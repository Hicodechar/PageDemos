
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,9/9] x86: convert dma_map_ops to support mapping a __pfn_t. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,9/9] x86: convert dma_map_ops to support mapping a __pfn_t.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=320">Dan Williams</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 5, 2015, 9:19 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150605211955.20751.15047.stgit@dwillia2-desk3.amr.corp.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6558161/mbox/"
   >mbox</a>
|
   <a href="/patch/6558161/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6558161/">/patch/6558161/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 38FA0C0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jun 2015 21:23:43 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id BB9572081A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jun 2015 21:23:41 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 06B1020818
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jun 2015 21:23:40 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754614AbbFEVXa (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 5 Jun 2015 17:23:30 -0400
Received: from mga02.intel.com ([134.134.136.20]:64447 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932742AbbFEVWh (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 5 Jun 2015 17:22:37 -0400
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
	by orsmga101.jf.intel.com with ESMTP; 05 Jun 2015 14:22:37 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.13,560,1427785200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;737936566&quot;
Received: from dwillia2-desk3.jf.intel.com (HELO
	dwillia2-desk3.amr.corp.intel.com) ([10.23.232.36])
	by fmsmga002.fm.intel.com with ESMTP; 05 Jun 2015 14:22:36 -0700
Subject: [PATCH v4 9/9] x86: convert dma_map_ops to support mapping a
	__pfn_t.
From: Dan Williams &lt;dan.j.williams@intel.com&gt;
To: linux-kernel@vger.kernel.org
Cc: axboe@kernel.dk, boaz@plexistor.com, david@fromorbit.com,
	linux-arch@vger.kernel.org, linux-fsdevel@vger.kernel.org,
	arnd@arndb.de, ross.zwisler@linux.intel.com,
	Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;,
	benh@kernel.crashing.org, linux-nvdimm@lists.01.org,
	heiko.carstens@de.ibm.com, hch@lst.de, tj@kernel.org,
	paulus@samba.org, hpa@zytor.com, schwidefsky@de.ibm.com,
	willy@linux.intel.com, akpm@linux-foundation.org,
	torvalds@linux-foundation.org, mingo@kernel.org
Date: Fri, 05 Jun 2015 17:19:55 -0400
Message-ID: &lt;20150605211955.20751.15047.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
In-Reply-To: &lt;20150605205052.20751.77149.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
References: &lt;20150605205052.20751.77149.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
User-Agent: StGit/0.17.1-8-g92dd
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - June 5, 2015, 9:19 p.m.</div>
<pre class="content">
As long as a dma_map_sg() implementation avoids sg_page() conversions it
can support scatterlists that carry &quot;page-less&quot; __pfn_t entries.
However, a couple implementations require that __pfn_t_has_page() is
always true. The Xen swiotlb implementation&#39;s entanglements with ARM and
the Calgary MMUs requirement to have a pre-existing virtual mapping make
them unable to support this conversion (i.e. these now have &#39;depends on
!HAVE_DMA_PFN&#39;).

Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;
<span class="signed-off-by">Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
---
 arch/x86/Kconfig                         |    2 ++
 arch/x86/kernel/amd_gart_64.c            |   22 +++++++++++++++++-----
 arch/x86/kernel/pci-nommu.c              |   22 +++++++++++++++++-----
 arch/x86/kernel/pci-swiotlb.c            |    4 ++++
 arch/x86/pci/sta2x11-fixup.c             |    4 ++++
 drivers/iommu/amd_iommu.c                |   21 ++++++++++++++++-----
 drivers/iommu/intel-iommu.c              |   22 +++++++++++++++++-----
 drivers/pci/Kconfig                      |    2 +-
 include/asm-generic/dma-mapping-common.h |    4 ++--
 include/linux/scatterlist.h              |    4 ++--
 include/linux/swiotlb.h                  |    4 ++++
 lib/swiotlb.c                            |   20 +++++++++++++++-----
 12 files changed, 101 insertions(+), 30 deletions(-)


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - June 9, 2015, 6:58 a.m.</div>
<pre class="content">
On Fri, Jun 05, 2015 at 05:19:55PM -0400, Dan Williams wrote:
<span class="quote">&gt; As long as a dma_map_sg() implementation avoids sg_page() conversions it</span>
<span class="quote">&gt; can support scatterlists that carry &quot;page-less&quot; __pfn_t entries.</span>
<span class="quote">&gt; However, a couple implementations require that __pfn_t_has_page() is</span>
<span class="quote">&gt; always true. The Xen swiotlb implementation&#39;s entanglements with ARM and</span>
<span class="quote">&gt; the Calgary MMUs requirement to have a pre-existing virtual mapping make</span>
<span class="quote">&gt; them unable to support this conversion (i.e. these now have &#39;depends on</span>
<span class="quote">&gt; !HAVE_DMA_PFN&#39;).</span>

That&#39;s why we really need a whole kernel conversion and not just a piecemail
one.  Given how trivial this patch is that doesn&#39;t look like a too big
task ayway.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=3407">Konrad Rzeszutek Wilk</a> - June 9, 2015, 1:47 p.m.</div>
<pre class="content">
On Tue, Jun 09, 2015 at 08:58:54AM +0200, Christoph Hellwig wrote:
<span class="quote">&gt; On Fri, Jun 05, 2015 at 05:19:55PM -0400, Dan Williams wrote:</span>
<span class="quote">&gt; &gt; As long as a dma_map_sg() implementation avoids sg_page() conversions it</span>
<span class="quote">&gt; &gt; can support scatterlists that carry &quot;page-less&quot; __pfn_t entries.</span>
<span class="quote">&gt; &gt; However, a couple implementations require that __pfn_t_has_page() is</span>
<span class="quote">&gt; &gt; always true. The Xen swiotlb implementation&#39;s entanglements with ARM and</span>
<span class="quote">&gt; &gt; the Calgary MMUs requirement to have a pre-existing virtual mapping make</span>
<span class="quote">&gt; &gt; them unable to support this conversion (i.e. these now have &#39;depends on</span>
<span class="quote">&gt; &gt; !HAVE_DMA_PFN&#39;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s why we really need a whole kernel conversion and not just a piecemail</span>
<span class="quote">&gt; one.  Given how trivial this patch is that doesn&#39;t look like a too big</span>
<span class="quote">&gt; task ayway.</span>

Aye, and the SWIOTLB (baremetal), Xen SWIOTLB (x86), Xen SWIOTLB (ARM)
can surely be easily tested by the Xen folks if you have patches. Please
just CC the xen-devel@lists.xenproject.org on the patches and shout
out for testing help.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 130d1a4c2efc..2fd7690ed0e2 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -796,6 +796,7 @@</span> <span class="p_context"> config CALGARY_IOMMU</span>
 	bool &quot;IBM Calgary IOMMU support&quot;
 	select SWIOTLB
 	depends on X86_64 &amp;&amp; PCI
<span class="p_add">+	depends on !HAVE_DMA_PFN</span>
 	---help---
 	  Support for hardware IOMMUs in IBM&#39;s xSeries x366 and x460
 	  systems. Needed to run systems with more than 3GB of memory
<span class="p_chunk">@@ -1436,6 +1437,7 @@</span> <span class="p_context"> config X86_PMEM_DMA</span>
 	depends on !HIGHMEM
 	def_bool DEV_PFN
 	select HAVE_KMAP_PFN
<span class="p_add">+	select HAVE_DMA_PFN</span>
 
 config HIGHPTE
 	bool &quot;Allocate 3rd-level pagetables from highmem&quot;
<span class="p_header">diff --git a/arch/x86/kernel/amd_gart_64.c b/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_header">index 8e3842fc8bea..8fad83c8dfd2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_chunk">@@ -239,13 +239,13 @@</span> <span class="p_context"> static dma_addr_t dma_map_area(struct device *dev, dma_addr_t phys_mem,</span>
 }
 
 /* Map a single area into the IOMMU */
<span class="p_del">-static dma_addr_t gart_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				unsigned long offset, size_t size,</span>
<span class="p_del">-				enum dma_data_direction dir,</span>
<span class="p_del">-				struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t gart_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+			       unsigned long offset, size_t size,</span>
<span class="p_add">+			       enum dma_data_direction dir,</span>
<span class="p_add">+			       struct dma_attrs *attrs)</span>
 {
 	unsigned long bus;
<span class="p_del">-	phys_addr_t paddr = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t paddr = __pfn_t_to_phys(pfn) + offset;</span>
 
 	if (!dev)
 		dev = &amp;x86_dma_fallback_dev;
<span class="p_chunk">@@ -259,6 +259,14 @@</span> <span class="p_context"> static dma_addr_t gart_map_page(struct device *dev, struct page *page,</span>
 	return bus;
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t gart_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return gart_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Free a DMA mapping.
  */
<span class="p_chunk">@@ -699,7 +707,11 @@</span> <span class="p_context"> static __init int init_amd_gatt(struct agp_kern_info *info)</span>
 static struct dma_map_ops gart_dma_ops = {
 	.map_sg				= gart_map_sg,
 	.unmap_sg			= gart_unmap_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn			= gart_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page			= gart_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page			= gart_unmap_page,
 	.alloc				= gart_alloc_coherent,
 	.free				= gart_free_coherent,
<span class="p_header">diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">index da15918d1c81..876dacfbabf6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_chunk">@@ -25,12 +25,12 @@</span> <span class="p_context"> check_addr(char *name, struct device *hwdev, dma_addr_t bus, size_t size)</span>
 	return 1;
 }
 
<span class="p_del">-static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				 unsigned long offset, size_t size,</span>
<span class="p_del">-				 enum dma_data_direction dir,</span>
<span class="p_del">-				 struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t nommu_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				unsigned long offset, size_t size,</span>
<span class="p_add">+				enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="p_add">+	dma_addr_t bus = __pfn_t_to_phys(pfn) + offset;</span>
 	WARN_ON(size == 0);
 	if (!check_addr(&quot;map_single&quot;, dev, bus, size))
 		return DMA_ERROR_CODE;
<span class="p_chunk">@@ -38,6 +38,14 @@</span> <span class="p_context"> static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
 	return bus;
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t nommu_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return nommu_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Map a set of buffers described by scatterlist in streaming
  * mode for DMA.  This is the scatter-gather version of the
  * above pci_map_single interface.  Here the scatter gather list
<span class="p_chunk">@@ -92,7 +100,11 @@</span> <span class="p_context"> struct dma_map_ops nommu_dma_ops = {</span>
 	.alloc			= dma_generic_alloc_coherent,
 	.free			= dma_generic_free_coherent,
 	.map_sg			= nommu_map_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn		= nommu_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page		= nommu_map_page,
<span class="p_add">+#endif</span>
 	.sync_single_for_device = nommu_sync_single_for_device,
 	.sync_sg_for_device	= nommu_sync_sg_for_device,
 	.is_phys		= 1,
<span class="p_header">diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">index 77dd0ad58be4..5351eb8c8f7f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_chunk">@@ -48,7 +48,11 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = swiotlb_sync_sg_for_device,
 	.map_sg = swiotlb_map_sg_attrs,
 	.unmap_sg = swiotlb_unmap_sg_attrs,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = swiotlb_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = swiotlb_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = swiotlb_unmap_page,
 	.dma_supported = NULL,
 };
<span class="p_header">diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">index 5ceda85b8687..d1c6e3808bb5 100644</span>
<span class="p_header">--- a/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">+++ b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_chunk">@@ -182,7 +182,11 @@</span> <span class="p_context"> static void *sta2x11_swiotlb_alloc_coherent(struct device *dev,</span>
 static struct dma_map_ops sta2x11_dma_ops = {
 	.alloc = sta2x11_swiotlb_alloc_coherent,
 	.free = x86_swiotlb_free_coherent,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = swiotlb_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = swiotlb_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = swiotlb_unmap_page,
 	.map_sg = swiotlb_map_sg_attrs,
 	.unmap_sg = swiotlb_unmap_sg_attrs,
<span class="p_header">diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="p_header">index e43d48956dea..ee8f70224b73 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu.c</span>
<span class="p_chunk">@@ -2754,16 +2754,15 @@</span> <span class="p_context"> static void __unmap_single(struct dma_ops_domain *dma_dom,</span>
 /*
  * The exported map_single function for dma_ops.
  */
<span class="p_del">-static dma_addr_t map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-			   unsigned long offset, size_t size,</span>
<span class="p_del">-			   enum dma_data_direction dir,</span>
<span class="p_del">-			   struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t map_pfn(struct device *dev, __pfn_t pfn, unsigned long offset,</span>
<span class="p_add">+		size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+		struct dma_attrs *attrs)</span>
 {
 	unsigned long flags;
 	struct protection_domain *domain;
 	dma_addr_t addr;
 	u64 dma_mask;
<span class="p_del">-	phys_addr_t paddr = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t paddr = __pfn_t_to_phys(pfn) + offset;</span>
 
 	INC_STATS_COUNTER(cnt_map_single);
 
<span class="p_chunk">@@ -2788,6 +2787,14 @@</span> <span class="p_context"> out:</span>
 	spin_unlock_irqrestore(&amp;domain-&gt;lock, flags);
 
 	return addr;
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __maybe_unused dma_addr_t map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+		unsigned long offset, size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+		struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return map_pfn(dev, page_to_pfn_t(page), offset, size, dir, attrs);</span>
 }
 
 /*
<span class="p_chunk">@@ -3062,7 +3069,11 @@</span> <span class="p_context"> static void __init prealloc_protection_domains(void)</span>
 static struct dma_map_ops amd_iommu_dma_ops = {
 	.alloc = alloc_coherent,
 	.free = free_coherent,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = unmap_page,
 	.map_sg = map_sg,
 	.unmap_sg = unmap_sg,
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 9b9ada71e0d3..6d9a0f85b827 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -3086,15 +3086,23 @@</span> <span class="p_context"> error:</span>
 	return 0;
 }
 
<span class="p_del">-static dma_addr_t intel_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				 unsigned long offset, size_t size,</span>
<span class="p_del">-				 enum dma_data_direction dir,</span>
<span class="p_del">-				 struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t intel_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				unsigned long offset, size_t size,</span>
<span class="p_add">+				enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	return __intel_map_single(dev, page_to_phys(page) + offset, size,</span>
<span class="p_add">+	return __intel_map_single(dev, __pfn_t_to_phys(pfn) + offset, size,</span>
 				  dir, *dev-&gt;dma_mask);
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t intel_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return intel_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void flush_unmaps(void)
 {
 	int i, j;
<span class="p_chunk">@@ -3380,7 +3388,11 @@</span> <span class="p_context"> struct dma_map_ops intel_dma_ops = {</span>
 	.free = intel_free_coherent,
 	.map_sg = intel_map_sg,
 	.unmap_sg = intel_unmap_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = intel_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = intel_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = intel_unmap_page,
 	.mapping_error = intel_mapping_error,
 };
<span class="p_header">diff --git a/drivers/pci/Kconfig b/drivers/pci/Kconfig</span>
<span class="p_header">index 7a8f1c5e65af..e56b04e24ec6 100644</span>
<span class="p_header">--- a/drivers/pci/Kconfig</span>
<span class="p_header">+++ b/drivers/pci/Kconfig</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> config PCI_STUB</span>
 
 config XEN_PCIDEV_FRONTEND
         tristate &quot;Xen PCI Frontend&quot;
<span class="p_del">-        depends on PCI &amp;&amp; X86 &amp;&amp; XEN</span>
<span class="p_add">+        depends on PCI &amp;&amp; X86 &amp;&amp; XEN &amp;&amp; !HAVE_DMA_PFN</span>
         select PCI_XEN
 	select XEN_XENBUS_FRONTEND
         default y
<span class="p_header">diff --git a/include/asm-generic/dma-mapping-common.h b/include/asm-generic/dma-mapping-common.h</span>
<span class="p_header">index 7305efb1bac6..e031b079ce4e 100644</span>
<span class="p_header">--- a/include/asm-generic/dma-mapping-common.h</span>
<span class="p_header">+++ b/include/asm-generic/dma-mapping-common.h</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,</span>
 	kmemcheck_mark_initialized(ptr, size);
 	BUG_ON(!valid_dma_direction(dir));
 #ifdef CONFIG_HAVE_DMA_PFN
<span class="p_del">-	addr = ops-&gt;map_pfn(dev, page_to_pfn_typed(virt_to_page(ptr)),</span>
<span class="p_add">+	addr = ops-&gt;map_pfn(dev, page_to_pfn_t(virt_to_page(ptr)),</span>
 			     (unsigned long)ptr &amp; ~PAGE_MASK, size,
 			     dir, attrs);
 #else
<span class="p_chunk">@@ -99,7 +99,7 @@</span> <span class="p_context"> static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,</span>
 				      enum dma_data_direction dir)
 {
 	kmemcheck_mark_initialized(page_address(page) + offset, size);
<span class="p_del">-	return dma_map_pfn(dev, page_to_pfn_typed(page), offset, size, dir);</span>
<span class="p_add">+	return dma_map_pfn(dev, page_to_pfn_t(page), offset, size, dir);</span>
 }
 #else
 static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
<span class="p_header">diff --git a/include/linux/scatterlist.h b/include/linux/scatterlist.h</span>
<span class="p_header">index 49054374646e..20334222d0c9 100644</span>
<span class="p_header">--- a/include/linux/scatterlist.h</span>
<span class="p_header">+++ b/include/linux/scatterlist.h</span>
<span class="p_chunk">@@ -158,9 +158,9 @@</span> <span class="p_context"> static inline struct page *sg_page(struct scatterlist *sg)</span>
 	return page;
 }
 
<span class="p_del">-static inline unsigned long sg_pfn(struct scatterlist *sg)</span>
<span class="p_add">+static inline __pfn_t sg_pfn(struct scatterlist *sg)</span>
 {
<span class="p_del">-	return __pfn_t_to_pfn(sg-&gt;pfn);</span>
<span class="p_add">+	return sg-&gt;pfn;</span>
 }
 
 /**
<span class="p_header">diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h</span>
<span class="p_header">index e7a018eaf3a2..5093fc8d2825 100644</span>
<span class="p_header">--- a/include/linux/swiotlb.h</span>
<span class="p_header">+++ b/include/linux/swiotlb.h</span>
<span class="p_chunk">@@ -66,6 +66,10 @@</span> <span class="p_context"> extern dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 				   unsigned long offset, size_t size,
 				   enum dma_data_direction dir,
 				   struct dma_attrs *attrs);
<span class="p_add">+extern dma_addr_t swiotlb_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				  unsigned long offset, size_t size,</span>
<span class="p_add">+				  enum dma_data_direction dir,</span>
<span class="p_add">+				  struct dma_attrs *attrs);</span>
 extern void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 			       size_t size, enum dma_data_direction dir,
 			       struct dma_attrs *attrs);
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 341268841b31..5ab7566735ba 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -727,12 +727,12 @@</span> <span class="p_context"> swiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,</span>
  * Once the device is given the dma address, the device owns this memory until
  * either swiotlb_unmap_page or swiotlb_dma_sync_single is performed.
  */
<span class="p_del">-dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-			    unsigned long offset, size_t size,</span>
<span class="p_del">-			    enum dma_data_direction dir,</span>
<span class="p_del">-			    struct dma_attrs *attrs)</span>
<span class="p_add">+dma_addr_t swiotlb_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+			   unsigned long offset, size_t size,</span>
<span class="p_add">+			   enum dma_data_direction dir,</span>
<span class="p_add">+			   struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	phys_addr_t map, phys = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t map, phys = __pfn_t_to_phys(pfn) + offset;</span>
 	dma_addr_t dev_addr = phys_to_dma(dev, phys);
 
 	BUG_ON(dir == DMA_NONE);
<span class="p_chunk">@@ -763,6 +763,16 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 
 	return dev_addr;
 }
<span class="p_add">+EXPORT_SYMBOL_GPL(swiotlb_map_pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+			    unsigned long offset, size_t size,</span>
<span class="p_add">+			    enum dma_data_direction dir,</span>
<span class="p_add">+			    struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swiotlb_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL_GPL(swiotlb_map_page);
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



