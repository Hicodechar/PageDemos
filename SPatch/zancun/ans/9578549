
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared mappings - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared mappings</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 17, 2017, 12:18 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;c9c8cafe-baa7-05b4-34ea-1dfa5523a85f@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9578549/mbox/"
   >mbox</a>
|
   <a href="/patch/9578549/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9578549/">/patch/9578549/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E6FFC600C5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 00:19:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C6EA92868F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 00:19:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B7BE12869E; Fri, 17 Feb 2017 00:19:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EBE012868F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 00:19:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753869AbdBQATJ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Feb 2017 19:19:09 -0500
Received: from userp1040.oracle.com ([156.151.31.81]:25812 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751801AbdBQATH (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Feb 2017 19:19:07 -0500
Received: from userv0022.oracle.com (userv0022.oracle.com [156.151.31.74])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id v1H0IsLB029337
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Fri, 17 Feb 2017 00:18:54 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by userv0022.oracle.com (8.14.4/8.14.4) with ESMTP id v1H0IsME031809
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Fri, 17 Feb 2017 00:18:54 GMT
Received: from abhmp0009.oracle.com (abhmp0009.oracle.com [141.146.116.15])
	by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id
	v1H0Iqsq027775; Fri, 17 Feb 2017 00:18:52 GMT
Received: from [192.168.1.164] (/50.188.161.229)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 16 Feb 2017 16:18:52 -0800
Subject: Re: [PATCH] userfaultfd: hugetlbfs: add UFFDIO_COPY support for
	shared mappings
To: Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
References: &lt;1487195210-12839-1-git-send-email-mike.kravetz@oracle.com&gt;
	&lt;20170216184100.GS25530@redhat.com&gt;
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Mike Rapoport &lt;rppt@linux.vnet.ibm.com&gt;,
	&quot;Dr. David Alan Gilbert&quot; &lt;dgilbert@redhat.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Pavel Emelyanov &lt;xemul@parallels.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;
Message-ID: &lt;c9c8cafe-baa7-05b4-34ea-1dfa5523a85f@oracle.com&gt;
Date: Thu, 16 Feb 2017 16:18:51 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101
	Thunderbird/45.7.0
MIME-Version: 1.0
In-Reply-To: &lt;20170216184100.GS25530@redhat.com&gt;
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: 7bit
X-Source-IP: userv0022.oracle.com [156.151.31.74]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Feb. 17, 2017, 12:18 a.m.</div>
<pre class="content">
On 02/16/2017 10:41 AM, Andrea Arcangeli wrote:
<span class="quote">&gt; On Wed, Feb 15, 2017 at 01:46:50PM -0800, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; index d0d1d08..41f6c51 100644</span>
<span class="quote">&gt;&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; @@ -4029,6 +4029,18 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
<span class="quote">&gt;&gt;  	__SetPageUptodate(page);</span>
<span class="quote">&gt;&gt;  	set_page_huge_active(page);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * If shared, add to page cache</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (dst_vma-&gt;vm_flags &amp; VM_SHARED) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Minor nitpick, this could be a:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;       int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (int faster than bool here as VM_SHARED won&#39;t have to be converted into 0|1)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; @@ -386,7 +413,8 @@ static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
<span class="quote">&gt;&gt;  		goto out_unlock;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	err = -EINVAL;</span>
<span class="quote">&gt;&gt; -	if (!vma_is_shmem(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt;&gt; +	if (!vma_is_shmem(dst_vma) &amp;&amp; !is_vm_hugetlb_page(dst_vma) &amp;&amp;</span>
<span class="quote">&gt;&gt; +	    dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt;&gt;  		goto out_unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Other minor nitpick, this could have been faster as:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      if (vma_is_anonymous(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thinking twice, the only case we need to rule out is shmem_zero_setup</span>
<span class="quote">&gt; (it&#39;s not anon vmas can be really VM_SHARED or they wouldn&#39;t be anon</span>
<span class="quote">&gt; vmas in the first place) so even the above is superfluous because</span>
<span class="quote">&gt; shmem_zero_setup does:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	vma-&gt;vm_ops = &amp;shmem_vm_ops;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I would turn it into:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      /*</span>
<span class="quote">&gt;       * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but</span>
<span class="quote">&gt;       * it will overwrite vm_ops, so vma_is_anonymous must return false.</span>
<span class="quote">&gt;       */</span>
<span class="quote">&gt;      if (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>

Thanks Andrea, I incorporated your suggestions into a new version of the patch. 
While changing (dst_vma-&gt;vm_flags &amp; VM_SHARED) to integers, I noticed an issue
in the error path of __mcopy_atomic_hugetlb().
<span class="quote">
&gt;                */</span>
<span class="quote">&gt; -             ClearPagePrivate(page);</span>
<span class="quote">&gt; +             if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="quote">&gt; +                     SetPagePrivate(page);</span>
<span class="quote">&gt; +             else</span>
<span class="quote">&gt; +                     ClearPagePrivate(page);</span>
<span class="quote">&gt;               put_page(page);</span>

We can not use dst_vma here as it may be different than the vma for which the
page was originally allocated, or even NULL.  Remember, we may drop mmap_sem
and look up dst_vma again.  Therefore, we need to save the value of
(dst_vma-&gt;vm_flags &amp; VM_SHARED) for the vma which was used when the page was
allocated.  This change as well as your suggestions are in the patch below:

From e13d3b4ab24f2c423a8b0d645f0ea715c285880d Mon Sep 17 00:00:00 2001
<span class="from">From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
Date: Wed, 15 Feb 2017 13:02:41 -0800
Subject: [PATCH] userfaultfd: hugetlbfs: add UFFDIO_COPY support for shared
 mappings

When userfaultfd hugetlbfs support was originally added, it followed
the pattern of anon mappings and did not support any vmas marked
VM_SHARED.  As such, support was only added for private mappings.

Remove this limitation and support shared mappings.  The primary
functional change required is adding pages to the page cache.  More
subtle changes are required for huge page reservation handling in
error paths.  A lengthy comment in the code describes the reservation
handling.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Mike Rapoport &lt;rppt@linux.vnet.ibm.com&gt;
Cc: &quot;Dr. David Alan Gilbert&quot; &lt;dgilbert@redhat.com&gt;
Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: Pavel Emelyanov &lt;xemul@parallels.com&gt;
---
 mm/hugetlb.c     | 26 ++++++++++++++++++--
 mm/userfaultfd.c | 74 ++++++++++++++++++++++++++++++++++++++++++++------------
 2 files changed, 82 insertions(+), 18 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index d0d1d08..2e0e815 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3993,6 +3993,7 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 			    unsigned long src_addr,
 			    struct page **pagep)
 {
<span class="p_add">+	int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;</span>
 	struct hstate *h = hstate_vma(dst_vma);
 	pte_t _dst_pte;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -4029,6 +4030,18 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	__SetPageUptodate(page);
 	set_page_huge_active(page);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If shared, add to page cache</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (vm_shared) {</span>
<span class="p_add">+		struct address_space *mapping = dst_vma-&gt;vm_file-&gt;f_mapping;</span>
<span class="p_add">+		pgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = huge_add_to_page_cache(page, mapping, idx);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out_release_nounlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	ptl = huge_pte_lockptr(h, dst_mm, dst_pte);
 	spin_lock(ptl);
 
<span class="p_chunk">@@ -4036,8 +4049,12 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	if (!huge_pte_none(huge_ptep_get(dst_pte)))
 		goto out_release_unlock;
 
<span class="p_del">-	ClearPagePrivate(page);</span>
<span class="p_del">-	hugepage_add_new_anon_rmap(page, dst_vma, dst_addr);</span>
<span class="p_add">+	if (vm_shared) {</span>
<span class="p_add">+		page_dup_rmap(page, true);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		ClearPagePrivate(page);</span>
<span class="p_add">+		hugepage_add_new_anon_rmap(page, dst_vma, dst_addr);</span>
<span class="p_add">+	}</span>
 
 	_dst_pte = make_huge_pte(dst_vma, page, dst_vma-&gt;vm_flags &amp; VM_WRITE);
 	if (dst_vma-&gt;vm_flags &amp; VM_WRITE)
<span class="p_chunk">@@ -4054,11 +4071,16 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 	update_mmu_cache(dst_vma, dst_addr, dst_pte);
 
 	spin_unlock(ptl);
<span class="p_add">+	if (vm_shared)</span>
<span class="p_add">+		unlock_page(page);</span>
 	ret = 0;
 out:
 	return ret;
 out_release_unlock:
 	spin_unlock(ptl);
<span class="p_add">+out_release_nounlock:</span>
<span class="p_add">+	if (vm_shared)</span>
<span class="p_add">+		unlock_page(page);</span>
 	put_page(page);
 	goto out;
 }
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index b861cf9..2fc0e76 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -154,6 +154,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 					      unsigned long len,
 					      bool zeropage)
 {
<span class="p_add">+	int vm_alloc_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;</span>
<span class="p_add">+	int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;</span>
 	ssize_t err;
 	pte_t *dst_pte;
 	unsigned long src_addr, dst_addr;
<span class="p_chunk">@@ -211,14 +213,14 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 			goto out_unlock;
 
 		/*
<span class="p_del">-		 * Make sure the vma is not shared, that the remaining dst</span>
<span class="p_del">-		 * range is both valid and fully within a single existing vma.</span>
<span class="p_add">+		 * Make sure the remaining dst range is both valid and</span>
<span class="p_add">+		 * fully within a single existing vma.</span>
 		 */
<span class="p_del">-		if (dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_del">-			goto out_unlock;</span>
 		if (dst_start &lt; dst_vma-&gt;vm_start ||
 		    dst_start + len &gt; dst_vma-&gt;vm_end)
 			goto out_unlock;
<span class="p_add">+</span>
<span class="p_add">+		vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;</span>
 	}
 
 	if (WARN_ON(dst_addr &amp; (vma_hpagesize - 1) ||
<span class="p_chunk">@@ -226,11 +228,13 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 		goto out_unlock;
 
 	/*
<span class="p_del">-	 * Ensure the dst_vma has a anon_vma.</span>
<span class="p_add">+	 * If not shared, ensure the dst_vma has a anon_vma.</span>
 	 */
 	err = -ENOMEM;
<span class="p_del">-	if (unlikely(anon_vma_prepare(dst_vma)))</span>
<span class="p_del">-		goto out_unlock;</span>
<span class="p_add">+	if (!vm_shared) {</span>
<span class="p_add">+		if (unlikely(anon_vma_prepare(dst_vma)))</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+	}</span>
 
 	h = hstate_vma(dst_vma);
 
<span class="p_chunk">@@ -267,6 +271,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 						dst_addr, src_addr, &amp;page);
 
 		mutex_unlock(&amp;hugetlb_fault_mutex_table[hash]);
<span class="p_add">+		vm_alloc_shared = vm_shared;</span>
 
 		cond_resched();
 
<span class="p_chunk">@@ -306,18 +311,49 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	if (page) {
 		/*
 		 * We encountered an error and are about to free a newly
<span class="p_del">-		 * allocated huge page.  It is possible that there was a</span>
<span class="p_del">-		 * reservation associated with the page that has been</span>
<span class="p_del">-		 * consumed.  See the routine restore_reserve_on_error</span>
<span class="p_del">-		 * for details.  Unfortunately, we can not call</span>
<span class="p_del">-		 * restore_reserve_on_error now as it would require holding</span>
<span class="p_del">-		 * mmap_sem.  Clear the PagePrivate flag so that the global</span>
<span class="p_add">+		 * allocated huge page.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Reservation handling is very subtle, and is different for</span>
<span class="p_add">+		 * private and shared mappings.  See the routine</span>
<span class="p_add">+		 * restore_reserve_on_error for details.  Unfortunately, we</span>
<span class="p_add">+		 * can not call restore_reserve_on_error now as it would</span>
<span class="p_add">+		 * require holding mmap_sem.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If a reservation for the page existed in the reservation</span>
<span class="p_add">+		 * map of a private mapping, the map was modified to indicate</span>
<span class="p_add">+		 * the reservation was consumed when the page was allocated.</span>
<span class="p_add">+		 * We clear the PagePrivate flag now so that the global</span>
 		 * reserve count will not be incremented in free_huge_page.
 		 * The reservation map will still indicate the reservation
 		 * was consumed and possibly prevent later page allocation.
<span class="p_del">-		 * This is better than leaking a global reservation.</span>
<span class="p_add">+		 * This is better than leaking a global reservation.  If no</span>
<span class="p_add">+		 * reservation existed, it is still safe to clear PagePrivate</span>
<span class="p_add">+		 * as no adjustments to reservation counts were made during</span>
<span class="p_add">+		 * allocation.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The reservation map for shared mappings indicates which</span>
<span class="p_add">+		 * pages have reservations.  When a huge page is allocated</span>
<span class="p_add">+		 * for an address with a reservation, no change is made to</span>
<span class="p_add">+		 * the reserve map.  In this case PagePrivate will be set</span>
<span class="p_add">+		 * to indicate that the global reservation count should be</span>
<span class="p_add">+		 * incremented when the page is freed.  This is the desired</span>
<span class="p_add">+		 * behavior.  However, when a huge page is allocated for an</span>
<span class="p_add">+		 * address without a reservation a reservation entry is added</span>
<span class="p_add">+		 * to the reservation map, and PagePrivate will not be set.</span>
<span class="p_add">+		 * When the page is freed, the global reserve count will NOT</span>
<span class="p_add">+		 * be incremented and it will appear as though we have leaked</span>
<span class="p_add">+		 * reserved page.  In this case, set PagePrivate so that the</span>
<span class="p_add">+		 * global reserve count will be incremented to match the</span>
<span class="p_add">+		 * reservation map entry which was created.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Note that vm_alloc_shared is based on the flags of the vma</span>
<span class="p_add">+		 * for which the page was originally allocated.  dst_vma could</span>
<span class="p_add">+		 * be different or NULL on error.</span>
 		 */
<span class="p_del">-		ClearPagePrivate(page);</span>
<span class="p_add">+		if (vm_alloc_shared)</span>
<span class="p_add">+			SetPagePrivate(page);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			ClearPagePrivate(page);</span>
 		put_page(page);
 	}
 	BUG_ON(copied &lt; 0);
<span class="p_chunk">@@ -386,8 +422,14 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 		goto out_unlock;
 
 	err = -EINVAL;
<span class="p_del">-	if (!vma_is_shmem(dst_vma) &amp;&amp; dst_vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but</span>
<span class="p_add">+	 * it will overwrite vm_ops, so vma_is_anonymous must return false.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &amp;&amp;</span>
<span class="p_add">+	    dst_vma-&gt;vm_flags &amp; VM_SHARED))</span>
 		goto out_unlock;
<span class="p_add">+</span>
 	if (dst_start &lt; dst_vma-&gt;vm_start ||
 	    dst_start + len &gt; dst_vma-&gt;vm_end)
 		goto out_unlock;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



