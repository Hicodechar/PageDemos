
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PTI,v3,10/10] x86/pti: Put the LDT in its own PGD if PTI is on - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PTI,v3,10/10] x86/pti: Put the LDT in its own PGD if PTI is on</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 12, 2017, 3:56 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;6e2fc833b964e8dc18af406f3fb86e0fc7fbf736.1513035461.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10107787/mbox/"
   >mbox</a>
|
   <a href="/patch/10107787/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10107787/">/patch/10107787/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	156F3602B3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Dec 2017 15:57:55 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0627129B50
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Dec 2017 15:57:55 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EF3A629B65; Tue, 12 Dec 2017 15:57:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AC5D029B50
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Dec 2017 15:57:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752710AbdLLP5u (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 Dec 2017 10:57:50 -0500
Received: from mail.kernel.org ([198.145.29.99]:44620 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752595AbdLLP45 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 Dec 2017 10:56:57 -0500
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 9C05F2190A;
	Tue, 12 Dec 2017 15:56:56 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 9C05F2190A
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Brian Gerst &lt;brgerst@gmail.com&gt;, David Laight &lt;David.Laight@aculab.com&gt;,
	Kees Cook &lt;keescook@chromium.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;
Subject: [PATCH PTI v3 10/10] x86/pti: Put the LDT in its own PGD if PTI is
	on
Date: Tue, 12 Dec 2017 07:56:45 -0800
Message-Id: &lt;6e2fc833b964e8dc18af406f3fb86e0fc7fbf736.1513035461.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.13.6
In-Reply-To: &lt;24c898b4f44fdf8c22d93703850fb384ef87cfdc.1513035461.git.luto@kernel.org&gt;
References: &lt;24c898b4f44fdf8c22d93703850fb384ef87cfdc.1513035461.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1513035461.git.luto@kernel.org&gt;
References: &lt;cover.1513035461.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 12, 2017, 3:56 p.m.</div>
<pre class="content">
With PTI on, we need the LDT to be in the usermode tables somewhere,
and the LDT is per-mm.

tglx had a hack to have a per-cpu LDT and context switch it, but it
was probably insanely slow due to the required TLB flushes.

Instead, take advantage of the fact that we have an address space
hole that gives us a completely unused pgd and make that pgd be
per-mm.  We can put the LDT in it.

This has a down side: the LDT isn&#39;t (currently) randomized, and an
attack that can write the LDT is instant root due to call gates
(thanks, AMD, for leaving call gates in AMD64 but designing them
wrong so they&#39;re only useful for exploits).  We could mitigate this
by making the LDT read-only or randomizing it, either of which is
strightforward on top of this patch.

This will significantly slow down LDT users, but that shouldn&#39;t
matter for important workloads -- the LDT is only used by DOSEMU(2),
Wine, and very old libc implementations.

Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 Documentation/x86/x86_64/mm.txt         |   3 +-
 arch/x86/include/asm/mmu_context.h      |  48 +++++++++-
 arch/x86/include/asm/pgtable_64_types.h |   4 +
 arch/x86/include/asm/processor.h        |  23 +++--
 arch/x86/kernel/ldt.c                   | 155 ++++++++++++++++++++++++++++++--
 arch/x86/mm/dump_pagetables.c           |  12 +++
 6 files changed, 225 insertions(+), 20 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 15, 2017, 10:54 p.m.</div>
<pre class="content">
On Tue, 12 Dec 2017, Andy Lutomirski wrote:
<span class="quote">&gt; +/* This is a multiple of PAGE_SIZE. */</span>
<span class="quote">&gt; +#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void *ldt_slot_va(int slot)</span>

How is that supposed to compile w/o warnings? Want&#39;s to be inline ....

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 16, 2017, 12:39 a.m.</div>
<pre class="content">
On Tue, 12 Dec 2017, Andy Lutomirski wrote:
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +	return -EINVAL;</span>

Errm. What&#39;s the point of that? Breaking non PTI?
<span class="quote">
&gt;  	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);</span>
<span class="quote">&gt;  	if (!new_ldt) {</span>
<span class="quote">&gt; @@ -155,8 +266,17 @@ int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,</span>
<span class="quote">&gt;  	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt;  	finalize_ldt_struct(new_ldt);</span>
<span class="quote">&gt; +	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="quote">&gt; +	if (retval)</span>
<span class="quote">&gt; +		goto out_free;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mm-&gt;context.ldt = new_ldt;</span>
<span class="quote">&gt; +	goto out_unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_free:</span>
<span class="quote">&gt; +	free_ldt_pgtables(mm);</span>
<span class="quote">&gt; +	free_ldt_struct(new_ldt);</span>
<span class="quote">&gt; +	return retval;</span>

Leaks old_mm-&gt;context_lock;

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 16, 2017, 6:41 a.m.</div>
<pre class="content">
On Fri, Dec 15, 2017 at 2:54 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Tue, 12 Dec 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; +/* This is a multiple of PAGE_SIZE. */</span>
<span class="quote">&gt;&gt; +#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void *ldt_slot_va(int slot)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; How is that supposed to compile w/o warnings? Want&#39;s to be inline ....</span>

Good question.  But it does compile for me and apparently for the 0day
bot without warnings.  Fixed in my tree.

On Fri, Dec 15, 2017 at 4:39 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Tue, 12 Dec 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     return 0;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +     return -EINVAL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Errm. What&#39;s the point of that? Breaking non PTI?</span>

That&#39;s embarrassing.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;       new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);</span>
<span class="quote">&gt;&gt;       if (!new_ldt) {</span>
<span class="quote">&gt;&gt; @@ -155,8 +266,17 @@ int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;       memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,</span>
<span class="quote">&gt;&gt;              new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt;&gt;       finalize_ldt_struct(new_ldt);</span>
<span class="quote">&gt;&gt; +     retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="quote">&gt;&gt; +     if (retval)</span>
<span class="quote">&gt;&gt; +             goto out_free;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       mm-&gt;context.ldt = new_ldt;</span>
<span class="quote">&gt;&gt; +     goto out_unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +out_free:</span>
<span class="quote">&gt;&gt; +     free_ldt_pgtables(mm);</span>
<span class="quote">&gt;&gt; +     free_ldt_struct(new_ldt);</span>
<span class="quote">&gt;&gt; +     return retval;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Leaks old_mm-&gt;context_lock;</span>
<span class="quote">&gt;</span>

Indeed.

I&#39;m going to go test the failure paths better tomorrow.  Meanwhile,
all three issues should be fixed in my tree.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 6a28aeaccd53..0a9925881e37 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT remap for PTI</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_chunk">@@ -28,7 +29,7 @@</span> <span class="p_context"> Virtual memory map with 5 level page tables:</span>
 hole caused by [56:63] sign extension
 ff00000000000000 - ff0fffffffffffff (=52 bits) guard hole, reserved for hypervisor
 ff10000000000000 - ff8fffffffffffff (=55 bits) direct mapping of all phys. memory
<span class="p_del">-ff90000000000000 - ff9fffffffffffff (=52 bits) hole</span>
<span class="p_add">+ff90000000000000 - ff9fffffffffffff (=49 bits) LDT remap for PTI</span>
 ffa0000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space
 ffd2000000000000 - ffd3ffffffffffff (=49 bits) hole
 ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5e1a1ecb65c6..6088f508b623 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -52,13 +52,33 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 */
 	struct desc_struct *entries;
 	unsigned int nr_entries;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).  We use two slots so that we can allocate</span>
<span class="p_add">+	 * and map, and enable a new LDT without invalidating the mapping</span>
<span class="p_add">+	 * of an older, still-in-use LDT.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * slot will be -1 if this LDT doesn&#39;t have an alias mapping.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
 int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline int init_new_context_ldt(struct task_struct *tsk,
 				       struct mm_struct *mm)
<span class="p_chunk">@@ -90,10 +110,31 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Whoops -- either the new LDT isn&#39;t mapped</span>
<span class="p_add">+				 * (if slot == -1) or is mapped into a bogus</span>
<span class="p_add">+				 * slot (if slot &gt; 1).</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page table isolation is enabled, ldt-&gt;entries</span>
<span class="p_add">+			 * will not be mapped in the userspace pagetables.</span>
<span class="p_add">+			 * Tell the CPU to access the LDT through the alias</span>
<span class="p_add">+			 * at ldt_slot_va(ldt-&gt;slot).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -185,6 +226,7 @@</span> <span class="p_context"> static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6e2f50843c7d..10ea949df97f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -81,10 +81,14 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define VMALLOC_SIZE_TB _AC(12800, UL)
 #define __VMALLOC_BASE	_AC(0xffa0000000000000, UL)
 #define __VMEMMAP_BASE	_AC(0xffd4000000000000, UL)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-112, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #else
 #define VMALLOC_SIZE_TB	_AC(32, UL)
 #define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)
 #define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-3, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #endif
 #ifdef CONFIG_RANDOMIZE_MEMORY
 #define VMALLOC_START	vmalloc_base
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index ae5615b03def..fe5e024b3a20 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -46,13 +47,11 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -89,10 +88,124 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 		return NULL;
 	}
 
<span class="p_add">+	/* The new LDT isn&#39;t aliased for PTI yet. */</span>
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
<span class="p_add">+</span>
 	new_ldt-&gt;nr_entries = num_entries;
 	return new_ldt;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="p_add">+ * usermode tables for the given mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="p_add">+ * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="p_add">+ * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="p_add">+ * access the freed slot.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="p_add">+ * it useful, and the flush would slow down modify_ldt().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	bool is_vmalloc;</span>
<span class="p_add">+	bool had_top_level_entry;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any given ldt_struct should have map_ldt_struct() called at most</span>
<span class="p_add">+	 * once.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+		pte_t pte, *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Treat the PTI LDT range as a *userspace* range.</span>
<span class="p_add">+		 * get_locked_pte() will allocate all needed pagetables</span>
<span class="p_add">+		 * and account for them in this mm.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="p_add">+		if (!ptep)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>
<span class="p_add">+			      set_pte_at(mm, va, ptep, pte);</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="p_add">+		 * Sync the pgd to the usermode tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_mm_range(mm,</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot),</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="p_add">+			   0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return -EINVAL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -134,17 +247,15 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	int retval = 0;
 
 	mutex_init(&amp;mm-&gt;context.lock);
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+</span>
 	old_mm = current-&gt;mm;
<span class="p_del">-	if (!old_mm) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm)</span>
 		return 0;
<span class="p_del">-	}</span>
 
 	mutex_lock(&amp;old_mm-&gt;context.lock);
<span class="p_del">-	if (!old_mm-&gt;context.ldt) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm-&gt;context.ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
 	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);
 	if (!new_ldt) {
<span class="p_chunk">@@ -155,8 +266,17 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval)</span>
<span class="p_add">+		goto out_free;</span>
 
 	mm-&gt;context.ldt = new_ldt;
<span class="p_add">+	goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+	free_ldt_struct(new_ldt);</span>
<span class="p_add">+	return retval;</span>
 
 out_unlock:
 	mutex_unlock(&amp;old_mm-&gt;context.lock);
<span class="p_chunk">@@ -174,6 +294,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -286,6 +411,18 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we are using PTI, map the new LDT into the userspace pagetables.</span>
<span class="p_add">+	 * If there is already an LDT, use the other slot so that other CPUs</span>
<span class="p_add">+	 * will continue to use the old LDT until install_ldt() switches</span>
<span class="p_add">+	 * them over to the new LDT.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);
 	error = 0;
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index e5a2df886130..878b3fdc00a5 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -50,12 +50,18 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 #ifdef CONFIG_X86_64
 	KERNEL_SPACE_NR,
 	LOW_KERNEL_NR,
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 	VMALLOC_START_NR,
 	VMEMMAP_START_NR,
 #ifdef CONFIG_KASAN
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 # ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
 # endif
<span class="p_chunk">@@ -79,12 +85,18 @@</span> <span class="p_context"> static struct addr_marker address_markers[] = {</span>
 #ifdef CONFIG_X86_64
 	{ 0x8000000000000000UL, &quot;Kernel Space&quot; },
 	{ 0/* PAGE_OFFSET */,   &quot;Low Kernel Mapping&quot; },
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	{ LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
<span class="p_add">+#endif</span>
 	{ 0/* VMALLOC_START */, &quot;vmalloc() Area&quot; },
 	{ 0/* VMEMMAP_START */, &quot;Vmemmap&quot; },
 #ifdef CONFIG_KASAN
 	{ KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },
 	{ KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },
 #endif
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	{ LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
<span class="p_add">+#endif</span>
 # ifdef CONFIG_X86_ESPFIX64
 	{ ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },
 # endif

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



