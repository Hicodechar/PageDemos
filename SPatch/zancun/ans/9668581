
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv2,8/8] x86/mm: Allow to have userspace mappings above 47-bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv2,8/8] x86/mm: Allow to have userspace mappings above 47-bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 6, 2017, 11:24 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170406232442.9822-1-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9668581/mbox/"
   >mbox</a>
|
   <a href="/patch/9668581/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9668581/">/patch/9668581/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D3615601EB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 23:25:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F0E4C285BD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 23:25:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E3FA3285EA; Thu,  6 Apr 2017 23:25:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E16C8285BD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 23:25:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933343AbdDFXZf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 6 Apr 2017 19:25:35 -0400
Received: from mga09.intel.com ([134.134.136.24]:15487 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755303AbdDFXZZ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 6 Apr 2017 19:25:25 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
	d=intel.com; i=@intel.com; q=dns/txt; s=intel;
	t=1491521125; x=1523057125;
	h=from:to:cc:subject:date:message-id:in-reply-to: references;
	bh=FouVqLy+amxIbGQQTS0SPNjAlz1O0isb692PVtHitVo=;
	b=P9MtmHY/n/+z8BzzzhbyV4uF5byC57L8c0bxcUID8wvpCIrjXoISd8Vf
	tF4DwPNgmAnOMxh+Ti5MxdlCz9yEBA==;
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	06 Apr 2017 16:25:24 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.37,161,1488873600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;842999837&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by FMSMGA003.fm.intel.com with ESMTP; 06 Apr 2017 16:25:20 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 604E2BA; Fri,  7 Apr 2017 02:24:49 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
Subject: [PATCHv2 8/8] x86/mm: Allow to have userspace mappings above 47-bits
Date: Fri,  7 Apr 2017 02:24:42 +0300
Message-Id: &lt;20170406232442.9822-1-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170406232137.uk7y2knbkcsru4pi@black.fi.intel.com&gt;
References: &lt;20170406232137.uk7y2knbkcsru4pi@black.fi.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - April 6, 2017, 11:24 p.m.</div>
<pre class="content">
On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

To mitigate this, we are not going to allocate virtual address space
above 47-bit by default.

But userspace can ask for allocation from full address space by
specifying hint address (with or without MAP_FIXED) above 47-bits.

If hint address set above 47-bit, but MAP_FIXED is not specified, we try
to look for unmapped area by specified address. If it&#39;s already
occupied, we look for unmapped area in *full* address space, rather than
from 47-bit window.

This approach helps to easily make application&#39;s memory allocator aware
about large address space without manually tracking allocated virtual
address space.

One important case we need to handle here is interaction with MPX.
MPX (without MAWA( extension cannot handle addresses above 47-bit, so we
need to make sure that MPX cannot be enabled we already have VMA above
the boundary and forbid creating such VMAs once MPX is enabled.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
Cc: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
---
 arch/x86/include/asm/elf.h       |  2 +-
 arch/x86/include/asm/mpx.h       |  9 +++++++++
 arch/x86/include/asm/processor.h | 10 +++++++---
 arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-
 arch/x86/mm/hugetlbpage.c        | 27 ++++++++++++++++++++++++---
 arch/x86/mm/mmap.c               |  2 +-
 arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-
 7 files changed, 101 insertions(+), 10 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 7, 2017, 11:32 a.m.</div>
<pre class="content">
On 04/07/2017 02:24 AM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; leads to crashes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; above 47-bit by default.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt; from 47-bit window.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt; address space.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; One important case we need to handle here is interaction with MPX.</span>
<span class="quote">&gt; MPX (without MAWA( extension cannot handle addresses above 47-bit, so we</span>
<span class="quote">&gt; need to make sure that MPX cannot be enabled we already have VMA above</span>
<span class="quote">&gt; the boundary and forbid creating such VMAs once MPX is enabled.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; Cc: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/elf.h       |  2 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/mpx.h       |  9 +++++++++</span>
<span class="quote">&gt;  arch/x86/include/asm/processor.h | 10 +++++++---</span>
<span class="quote">&gt;  arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  arch/x86/mm/hugetlbpage.c        | 27 ++++++++++++++++++++++++---</span>
<span class="quote">&gt;  arch/x86/mm/mmap.c               |  2 +-</span>
<span class="quote">&gt;  arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  7 files changed, 101 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; index d4d3ed456cb7..67260dbe1688 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; @@ -250,7 +250,7 @@ extern int force_personality32;</span>
<span class="quote">&gt;     the loader.  We need to make sure that it is out of the way of the program</span>
<span class="quote">&gt;     that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="quote">&gt; +#define ELF_ET_DYN_BASE		(DEFAULT_MAP_WINDOW / 3 * 2)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* This yields a mask that user programs can use to figure out what</span>
<span class="quote">&gt;     instruction set this CPU supports.  This could be done in user space,</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; @@ -73,6 +73,9 @@ static inline void mpx_mm_init(struct mm_struct *mm)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		      unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long flags);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -94,6 +97,12 @@ static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  				    unsigned long start, unsigned long end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="quote">&gt; +		unsigned long len, unsigned long flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return addr;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif /* CONFIG_X86_INTEL_MPX */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #endif /* _ASM_X86_MPX_H */</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; index 3cada998a402..a98395e89ac6 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; @@ -795,6 +795,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;  #define IA32_PAGE_OFFSET	PAGE_OFFSET</span>
<span class="quote">&gt;  #define TASK_SIZE		PAGE_OFFSET</span>
<span class="quote">&gt;  #define TASK_SIZE_MAX		TASK_SIZE</span>
<span class="quote">&gt; +#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
<span class="quote">&gt;  #define STACK_TOP		TASK_SIZE</span>
<span class="quote">&gt;  #define STACK_TOP_MAX		STACK_TOP</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -834,7 +835,10 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;   * particular problem by preventing anything from being mapped</span>
<span class="quote">&gt;   * at the maximum canonical address.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt; +#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define DEFAULT_MAP_WINDOW	(test_thread_flag(TIF_ADDR32) ? \</span>
<span class="quote">&gt; +				IA32_PAGE_OFFSET : ((1UL &lt;&lt; 47) - PAGE_SIZE))</span>

That fixes 32-bit, but we need to adjust some places, AFAICS, I&#39;ll
point them below.
<span class="quote">
&gt;</span>
<span class="quote">&gt;  /* This decides where the kernel will search for a free chunk of vm</span>
<span class="quote">&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt; @@ -847,7 +851,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;  #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \</span>
<span class="quote">&gt;  					IA32_PAGE_OFFSET : TASK_SIZE_MAX)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define STACK_TOP		TASK_SIZE</span>
<span class="quote">&gt; +#define STACK_TOP		DEFAULT_MAP_WINDOW</span>
<span class="quote">&gt;  #define STACK_TOP_MAX		TASK_SIZE_MAX</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #define INIT_THREAD  {						\</span>
<span class="quote">&gt; @@ -870,7 +874,7 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
<span class="quote">&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt; -#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="quote">&gt; +#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; index 207b8f2582c7..593a31e93812 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; @@ -21,6 +21,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/compat.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/ia32.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Align a virtual address to avoid aliasing in the I$ on AMD F15h.</span>
<span class="quote">&gt; @@ -132,6 +133,10 @@ arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;  	unsigned long begin, end;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt;  		return addr;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -151,7 +156,16 @@ arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
<span class="quote">&gt;  	info.flags = 0;</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = begin;</span>
<span class="quote">&gt; -	info.high_limit = end;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		info.high_limit = min(end, TASK_SIZE);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>

That looks not working.
`end&#39; is choosed between tasksize_32bit() and tasksize_64bit().
Which is ~4Gb or 47-bit. So, info.high_limit will never go
above DEFAULT_MAP_WINDOW with this min().

Can we move this logic into find_start_end()?

May it be something like:
if (in_compat_syscall())
   *end = tasksize_32bit();
else if (addr &gt; task_size_64bit())
   *end = TASK_SIZE_MAX;
else
   *end = tasksize_64bit();

In my point of view, it could be even simpler if we add a parameter
to task_size_64bit():

#define TASK_SIZE_47BIT ((1UL &lt;&lt; 47) - PAGE_SIZE))

unsigned long task_size_64bit(int full_addr_space)
{
    return (full_addr_space) ? TASK_SIZE_MAX : TASK_SIZE_47BIT;
}
<span class="quote">
&gt; +</span>
<span class="quote">&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>
<span class="quote">&gt; @@ -171,6 +185,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; @@ -195,6 +213,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>

Hmm, looks like we do need in_compat_syscall() as you did
because x32 mmap() syscall has 8 byte parameter.
Maybe worth a comment.

Anyway, maybe something like that:
if (addr &gt; tasksize_64bit() &amp;&amp; !in_compat_syscall())
    info.high_limit += TASK_SIZE_MAX - tasksize_64bit();

This way it&#39;s more readable and clear because we don&#39;t
need to keep in mind that TIF_ADDR32 flag, while reading.
<span class="quote">

&gt; +</span>
<span class="quote">&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; index 302f43fd9c28..9a0b89252c52 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; @@ -18,6 +18,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/elf.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #if 0	/* This is just for testing */</span>
<span class="quote">&gt;  struct page *</span>
<span class="quote">&gt; @@ -87,23 +88,38 @@ static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
<span class="quote">&gt;  	info.low_limit = get_mmap_base(1);</span>
<span class="quote">&gt;  	info.high_limit = in_compat_syscall() ?</span>
<span class="quote">&gt;  		tasksize_32bit() : tasksize_64bit();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;  	info.align_offset = 0;</span>
<span class="quote">&gt;  	return vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
<span class="quote">&gt; -		unsigned long addr0, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;  		unsigned long pgoff, unsigned long flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hstate *h = hstate_file(file);</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt; -	unsigned long addr;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	info.flags = VM_UNMAPPED_AREA_TOPDOWN;</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;  	info.align_offset = 0;</span>
<span class="quote">&gt;  	addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt; @@ -118,7 +134,7 @@ static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
<span class="quote">&gt;  		VM_BUG_ON(addr != -ENOMEM);</span>
<span class="quote">&gt;  		info.flags = 0;</span>
<span class="quote">&gt;  		info.low_limit = TASK_UNMAPPED_BASE;</span>
<span class="quote">&gt; -		info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt; +		info.high_limit = DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;  		addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -135,6 +151,11 @@ hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	if (len &amp; ~huge_page_mask(h))</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; index 19ad095b41df..d63232a31945 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; @@ -44,7 +44,7 @@ unsigned long tasksize_32bit(void)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  unsigned long tasksize_64bit(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return TASK_SIZE_MAX;</span>
<span class="quote">&gt; +	return DEFAULT_MAP_WINDOW;</span>

My suggestion about new parameter is above, but at least
we need to omit depending on TIF_ADDR32 here and return
64-bit size independent of flag value:

#define TASK_SIZE_47BIT ((1UL &lt;&lt; 47) - PAGE_SIZE))
unsigned long task_size_64bit(void)
{
    return TASK_SIZE_47BIT;
}

Because for 32-bit ELFs it would be always 4Gb in your
case, while 32-bit ELFs can do 64-bit syscalls.
<span class="quote">
&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static unsigned long stack_maxrandom_size(unsigned long task_size)</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; index cd44ae727df7..a26a1b373fd0 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; @@ -355,10 +355,19 @@ int mpx_enable_management(void)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	bd_base = mpx_get_bounds_dir();</span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="quote">&gt; +	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="quote">&gt; +		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="quote">&gt; +				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="quote">&gt; +				current-&gt;comm, current-&gt;pid);</span>
<span class="quote">&gt; +		ret = -ENXIO;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	mm-&gt;context.bd_addr = bd_base;</span>
<span class="quote">&gt;  	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)</span>
<span class="quote">&gt;  		ret = -ENXIO;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1038,3 +1047,25 @@ void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (ret)</span>
<span class="quote">&gt;  		force_sig(SIGSEGV, current);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="quote">&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +	if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="quote">&gt; +	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - April 13, 2017, 11:30 a.m.</div>
<pre class="content">
Here&#39;s updated version the fourth and the last bunch of of patches that brings
initial 5-level paging enabling.

Please review and consider applying.

The situation with assembly hasn&#39;t changed much. I still not see a way to get
it work.

In this version I&#39;ve included patch to fix comment in return_from_SYSCALL_64,
fixed bug in coverting startup_64 to C and updated the patch which allows to
opt-in full address space.

Kirill A. Shutemov (9):
  x86/asm: Fix comment in return_from_SYSCALL_64
  x86/boot/64: Rewrite startup_64 in C
  x86/boot/64: Rename init_level4_pgt and early_level4_pgt
  x86/boot/64: Add support of additional page table level during early
    boot
  x86/mm: Add sync_global_pgds() for configuration with 5-level paging
  x86/mm: Make kernel_physical_mapping_init() support 5-level paging
  x86/mm: Add support for 5-level paging for KASLR
  x86: Enable 5-level paging support
  x86/mm: Allow to have userspace mappings above 47-bits

 arch/x86/Kconfig                            |   5 +
 arch/x86/boot/compressed/head_64.S          |  23 ++++-
 arch/x86/entry/entry_64.S                   |   3 +-
 arch/x86/include/asm/elf.h                  |   4 +-
 arch/x86/include/asm/mpx.h                  |   9 ++
 arch/x86/include/asm/pgtable.h              |   2 +-
 arch/x86/include/asm/pgtable_64.h           |   6 +-
 arch/x86/include/asm/processor.h            |  11 ++-
 arch/x86/include/uapi/asm/processor-flags.h |   2 +
 arch/x86/kernel/espfix_64.c                 |   2 +-
 arch/x86/kernel/head64.c                    | 137 +++++++++++++++++++++++++---
 arch/x86/kernel/head_64.S                   | 134 +++++++--------------------
 arch/x86/kernel/machine_kexec_64.c          |   2 +-
 arch/x86/kernel/sys_x86_64.c                |  30 +++++-
 arch/x86/mm/dump_pagetables.c               |   2 +-
 arch/x86/mm/hugetlbpage.c                   |  27 +++++-
 arch/x86/mm/init_64.c                       | 104 +++++++++++++++++++--
 arch/x86/mm/kasan_init_64.c                 |  12 +--
 arch/x86/mm/kaslr.c                         |  81 ++++++++++++----
 arch/x86/mm/mmap.c                          |   6 +-
 arch/x86/mm/mpx.c                           |  33 ++++++-
 arch/x86/realmode/init.c                    |   2 +-
 arch/x86/xen/Kconfig                        |   1 +
 arch/x86/xen/mmu.c                          |  18 ++--
 arch/x86/xen/xen-pvh.S                      |   2 +-
 25 files changed, 470 insertions(+), 188 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index d4d3ed456cb7..67260dbe1688 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(DEFAULT_MAP_WINDOW / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_header">diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="p_header">index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mpx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mpx.h</span>
<span class="p_chunk">@@ -73,6 +73,9 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 }
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
 		      unsigned long start, unsigned long end);
<span class="p_add">+</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags);</span>
 #else
 static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)
 {
<span class="p_chunk">@@ -94,6 +97,12 @@</span> <span class="p_context"> static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
 				    unsigned long start, unsigned long end)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="p_add">+		unsigned long len, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_X86_INTEL_MPX */
 
 #endif /* _ASM_X86_MPX_H */
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 3cada998a402..a98395e89ac6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -795,6 +795,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
<span class="p_add">+#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
<span class="p_chunk">@@ -834,7 +835,10 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
<span class="p_del">-#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="p_add">+#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFAULT_MAP_WINDOW	(test_thread_flag(TIF_ADDR32) ? \</span>
<span class="p_add">+				IA32_PAGE_OFFSET : ((1UL &lt;&lt; 47) - PAGE_SIZE))</span>
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
<span class="p_chunk">@@ -847,7 +851,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		DEFAULT_MAP_WINDOW</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -870,7 +874,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * space during mmap&#39;s.
  */
 #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
<span class="p_del">-#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(DEFAULT_MAP_WINDOW)</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 207b8f2582c7..593a31e93812 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/compat.h&gt;
 #include &lt;asm/ia32.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 /*
  * Align a virtual address to avoid aliasing in the I$ on AMD F15h.
<span class="p_chunk">@@ -132,6 +133,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	struct vm_unmapped_area_info info;
 	unsigned long begin, end;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (flags &amp; MAP_FIXED)
 		return addr;
 
<span class="p_chunk">@@ -151,7 +156,16 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = begin;
<span class="p_del">-	info.high_limit = end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = min(end, TASK_SIZE);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_chunk">@@ -171,6 +185,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
<span class="p_chunk">@@ -195,6 +213,14 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 302f43fd9c28..9a0b89252c52 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/elf.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 #if 0	/* This is just for testing */
 struct page *
<span class="p_chunk">@@ -87,23 +88,38 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.low_limit = get_mmap_base(1);
 	info.high_limit = in_compat_syscall() ?
 		tasksize_32bit() : tasksize_64bit();
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
 }
 
 static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,
<span class="p_del">-		unsigned long addr0, unsigned long len,</span>
<span class="p_add">+		unsigned long addr, unsigned long len,</span>
 		unsigned long pgoff, unsigned long flags)
 {
 	struct hstate *h = hstate_file(file);
 	struct vm_unmapped_area_info info;
<span class="p_del">-	unsigned long addr;</span>
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -118,7 +134,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = DEFAULT_MAP_WINDOW;</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -135,6 +151,11 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_add">+</span>
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index 19ad095b41df..d63232a31945 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> unsigned long tasksize_32bit(void)</span>
 
 unsigned long tasksize_64bit(void)
 {
<span class="p_del">-	return TASK_SIZE_MAX;</span>
<span class="p_add">+	return DEFAULT_MAP_WINDOW;</span>
 }
 
 static unsigned long stack_maxrandom_size(unsigned long task_size)
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index cd44ae727df7..a26a1b373fd0 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -355,10 +355,19 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 */
 	bd_base = mpx_get_bounds_dir();
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+</span>
<span class="p_add">+	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="p_add">+	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="p_add">+		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="p_add">+				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="p_add">+				current-&gt;comm, current-&gt;pid);</span>
<span class="p_add">+		ret = -ENXIO;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
<span class="p_del">-</span>
<span class="p_add">+out:</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -1038,3 +1047,25 @@</span> <span class="p_context"> void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (ret)
 		force_sig(SIGSEGV, current);
 }
<span class="p_add">+</span>
<span class="p_add">+/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (flags &amp; MAP_FIXED)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="p_add">+	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



