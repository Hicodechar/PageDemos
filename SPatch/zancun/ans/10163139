
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6,16/36] nds32: DMA mapping API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6,16/36] nds32: DMA mapping API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 15, 2018, 5:53 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;ecd1742c718c5b45983486cf9dfb44adb6e06923.1515766253.git.green.hu@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10163139/mbox/"
   >mbox</a>
|
   <a href="/patch/10163139/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10163139/">/patch/10163139/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BD13E60245 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 06:05:30 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A10B5286C5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 06:05:30 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 92CF0288DE; Mon, 15 Jan 2018 06:05:30 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A1BB0286C5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 06:05:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933980AbeAOGF2 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 15 Jan 2018 01:05:28 -0500
Received: from mail-pf0-f193.google.com ([209.85.192.193]:41835 &quot;EHLO
	mail-pf0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S932821AbeAOFzW (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 15 Jan 2018 00:55:22 -0500
Received: by mail-pf0-f193.google.com with SMTP id j3so7559968pfh.8;
	Sun, 14 Jan 2018 21:55:22 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:in-reply-to:references;
	bh=UsNv4ziPPHmjMoJuaa2XevflsQ+7d75mIruQ4H8p+3w=;
	b=qERL2/iOW2wmrgbuTyWJTZUt7D1DtaUykaFPqAPlCbKHTItJE4olpujw3zIP4VQHkM
	OdiQdGtH46gNeai4BfQ3p1lgcdO70+GtT0yaa0iYfP4IyV57POQyTu5yHE+7pMAPhj2P
	MetBdbLPokJJk4oD7IUg1Vej5ruF+ifg1pfS7o4jCwvSd+iq/mK14Chrdg1t6/9CVq9w
	4Y5gmq+ReHtQfZHZFjtcc1EcyhUhuY4AK1oMVWP+SI7M+Q2jssVDEyWoGxEYEkiTp35a
	uy5lxlt/+f+/2JshEW1NVZ2sJPkXj5bw7s9fbJnfEB1+9s36sH+3DsjGRLKqhY7huEBo
	9o0A==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references:in-reply-to:references;
	bh=UsNv4ziPPHmjMoJuaa2XevflsQ+7d75mIruQ4H8p+3w=;
	b=bOkD6txGTxXrtlCjrR+KfGUiRMxGtL2H4vui6ANYNbkVaWNNvQmf3Sx9BZ7RNAUeBq
	EXWCtq3TkwvoCIXNA0SE2Q0Os/QQub6SoCFnrR8Gj/K9Yq+NWFXwAUR0rhPUncQ+xz5K
	mpIQeM6nzaOMBs8obZgwVXlC5hw8qeQqfMHHJfAJLdzY7arka1Fwqt5yGpSXGCd66t2J
	zdDczV576bjR+op8Ns72wqeE5WmTMZgC6JZQ2UkjjXM8dgCME74nwAbybP29HjX/FNKh
	b2tN5YXmc7AjNh1LB0W4HWY699OsqNTMLdzih2YuSAe6cxivo+MbSkAy6cH8J/KcT+yv
	gLZw==
X-Gm-Message-State: AKGB3mLk7NsdrGtWIkVQTFUyE3HmnggJy+g2wHC/je5ICExoI2BQzFy+
	VaMJodKE8iA849MKkfPXLf0=
X-Google-Smtp-Source: ACJfBotNPPkuO9iFW87bDHvDMEGcuKQWZyzdaa28FgowCvEb67aVQNsmBdDGvz8unmbhAkgb6e1CDQ==
X-Received: by 10.99.44.14 with SMTP id s14mr26449678pgs.452.1515995721565; 
	Sun, 14 Jan 2018 21:55:21 -0800 (PST)
Received: from app09.andestech.com ([118.163.51.199])
	by smtp.gmail.com with ESMTPSA id
	h13sm49614291pfi.40.2018.01.14.21.55.16
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Sun, 14 Jan 2018 21:55:20 -0800 (PST)
From: Greentime Hu &lt;green.hu@gmail.com&gt;
To: greentime@andestech.com, linux-kernel@vger.kernel.org,
	arnd@arndb.de, linux-arch@vger.kernel.org, tglx@linutronix.de,
	jason@lakedaemon.net, marc.zyngier@arm.com, robh+dt@kernel.org,
	netdev@vger.kernel.org, deanbo422@gmail.com,
	devicetree@vger.kernel.org, viro@zeniv.linux.org.uk,
	dhowells@redhat.com, will.deacon@arm.com,
	daniel.lezcano@linaro.org, linux-serial@vger.kernel.org,
	geert.uytterhoeven@gmail.com, linus.walleij@linaro.org,
	mark.rutland@arm.com, greg@kroah.com, ren_guo@c-sky.com,
	rdunlap@infradead.org, davem@davemloft.net, jonas@southpole.se,
	stefan.kristiansson@saunalahti.fi, shorne@gmail.com
Cc: green.hu@gmail.com, Vincent Chen &lt;vincentc@andestech.com&gt;
Subject: [PATCH v6 16/36] nds32: DMA mapping API
Date: Mon, 15 Jan 2018 13:53:24 +0800
Message-Id: &lt;ecd1742c718c5b45983486cf9dfb44adb6e06923.1515766253.git.green.hu@gmail.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;cover.1515766253.git.green.hu@gmail.com&gt;
References: &lt;cover.1515766253.git.green.hu@gmail.com&gt;
In-Reply-To: &lt;cover.1515766253.git.green.hu@gmail.com&gt;
References: &lt;cover.1515766253.git.green.hu@gmail.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 15, 2018, 5:53 a.m.</div>
<pre class="content">
<span class="from">From: Greentime Hu &lt;greentime@andestech.com&gt;</span>

This patch adds support for the DMA mapping API. It uses dma_map_ops for
flexibility.
<span class="signed-off-by">
Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
---
 arch/nds32/include/asm/dma-mapping.h |   14 ++
 arch/nds32/kernel/dma.c              |  459 ++++++++++++++++++++++++++++++++++
 2 files changed, 473 insertions(+)
 create mode 100644 arch/nds32/include/asm/dma-mapping.h
 create mode 100644 arch/nds32/kernel/dma.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 18, 2018, 10:26 a.m.</div>
<pre class="content">
On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:
<span class="quote">&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch adds support for the DMA mapping API. It uses dma_map_ops for</span>
<span class="quote">&gt; flexibility.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>

I&#39;m still unhappy about the way the cache flushes are done here as discussed
before. It&#39;s not a show-stopped, but no Ack from me.

        Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 23, 2018, 8:23 a.m.</div>
<pre class="content">
Hi, Arnd:

2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:
<span class="quote">&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds support for the DMA mapping API. It uses dma_map_ops for</span>
<span class="quote">&gt;&gt; flexibility.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m still unhappy about the way the cache flushes are done here as discussed</span>
<span class="quote">&gt; before. It&#39;s not a show-stopped, but no Ack from me.</span>

How about this implementation?

static void
nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,
                              size_t size, enum dma_data_direction dir)
{
        switch (direction) {
        case DMA_TO_DEVICE:     /* writeback only */
                break;
        case DMA_FROM_DEVICE:   /* invalidate only */
        case DMA_BIDIRECTIONAL: /* writeback and invalidate */
                cpu_dma_inval_range(start, end);
                break;
        default:
                BUG();
        }
}

static void
nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,
                                 size_t size, enum dma_data_direction dir)
{
        switch (direction) {
        case DMA_FROM_DEVICE:   /* invalidate only */
                break;
        case DMA_TO_DEVICE:     /* writeback only */
        case DMA_BIDIRECTIONAL: /* writeback and invalidate */
                cpu_dma_wb_range(start, end);
                break;
        default:
                BUG();
        }
}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 23, 2018, 11:52 a.m.</div>
<pre class="content">
Hi, Arnd:

2018-01-23 16:23 GMT+08:00 Greentime Hu &lt;green.hu@gmail.com&gt;:
<span class="quote">&gt; Hi, Arnd:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch adds support for the DMA mapping API. It uses dma_map_ops for</span>
<span class="quote">&gt;&gt;&gt; flexibility.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m still unhappy about the way the cache flushes are done here as discussed</span>
<span class="quote">&gt;&gt; before. It&#39;s not a show-stopped, but no Ack from me.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; How about this implementation?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static void</span>
<span class="quote">&gt; nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;                               size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         switch (direction) {</span>
<span class="quote">&gt;         case DMA_TO_DEVICE:     /* writeback only */</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         case DMA_FROM_DEVICE:   /* invalidate only */</span>
<span class="quote">&gt;         case DMA_BIDIRECTIONAL: /* writeback and invalidate */</span>
<span class="quote">&gt;                 cpu_dma_inval_range(start, end);</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         default:</span>
<span class="quote">&gt;                 BUG();</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static void</span>
<span class="quote">&gt; nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;                                  size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         switch (direction) {</span>
<span class="quote">&gt;         case DMA_FROM_DEVICE:   /* invalidate only */</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         case DMA_TO_DEVICE:     /* writeback only */</span>
<span class="quote">&gt;         case DMA_BIDIRECTIONAL: /* writeback and invalidate */</span>
<span class="quote">&gt;                 cpu_dma_wb_range(start, end);</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         default:</span>
<span class="quote">&gt;                 BUG();</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; }</span>

I am not sure if I understand it correctly.
I list all the combinations.

RAM to DEVICE
    before DMA =&gt; writeback cache
    after DMA =&gt; nop

DEVICE to RAM
    before DMA =&gt; nop
    after DMA =&gt; invalidate cache

static void consistent_sync(void *vaddr, size_t size, int direction, int master)
{
        unsigned long start = (unsigned long)vaddr;
        unsigned long end = start + size;

        if (master == FOR_CPU) {
                switch (direction) {
                case DMA_TO_DEVICE:
                        break;
                case DMA_FROM_DEVICE:
                case DMA_BIDIRECTIONAL:
                        cpu_dma_inval_range(start, end);
                        break;
                default:
                        BUG();
                }
        } else {
                /* FOR_DEVICE */
                switch (direction) {
                case DMA_FROM_DEVICE:
                        break;
                case DMA_TO_DEVICE:
                case DMA_BIDIRECTIONAL:
                        cpu_dma_wb_range(start, end);
                        break;
                default:
                        BUG();
                }
        }
}

static void
nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,
                              size_t size, enum dma_data_direction dir)
{
        consistent_sync((void *)phys_to_virt(handle), size, dir, FOR_CPU);
}

static void
nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,
                                 size_t size, enum dma_data_direction dir)
{
        consistent_sync((void *)phys_to_virt(handle), size, dir, FOR_DEVICE);
}

static dma_addr_t nds32_dma_map_page(struct device *dev, struct page *page,
                                     unsigned long offset, size_t size,
                                     enum dma_data_direction dir,
                                     unsigned long attrs)
{
        if (!(attrs &amp; DMA_ATTR_SKIP_CPU_SYNC))
                consistent_sync((void *)(page_address(page) + offset),
size, dir, FOR_DEVICE);
        return page_to_phys(page) + offset;
}

static void nds32_dma_unmap_page(struct device *dev, dma_addr_t handle,
                                 size_t size, enum dma_data_direction dir,
                                 unsigned long attrs)
{
        if (!(attrs &amp; DMA_ATTR_SKIP_CPU_SYNC))
                consistent_sync(phys_to_virt(handle), size, dir, FOR_CPU);
}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 24, 2018, 11:36 a.m.</div>
<pre class="content">
On Tue, Jan 23, 2018 at 12:52 PM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:
<span class="quote">&gt; Hi, Arnd:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 2018-01-23 16:23 GMT+08:00 Greentime Hu &lt;green.hu@gmail.com&gt;:</span>
<span class="quote">&gt;&gt; Hi, Arnd:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch adds support for the DMA mapping API. It uses dma_map_ops for</span>
<span class="quote">&gt;&gt;&gt;&gt; flexibility.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I&#39;m still unhappy about the way the cache flushes are done here as discussed</span>
<span class="quote">&gt;&gt;&gt; before. It&#39;s not a show-stopped, but no Ack from me.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; How about this implementation?</span>
<span class="quote">
&gt; I am not sure if I understand it correctly.</span>
<span class="quote">&gt; I list all the combinations.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; RAM to DEVICE</span>
<span class="quote">&gt;     before DMA =&gt; writeback cache</span>
<span class="quote">&gt;     after DMA =&gt; nop</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; DEVICE to RAM</span>
<span class="quote">&gt;     before DMA =&gt; nop</span>
<span class="quote">&gt;     after DMA =&gt; invalidate cache</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static void consistent_sync(void *vaddr, size_t size, int direction, int master)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         unsigned long start = (unsigned long)vaddr;</span>
<span class="quote">&gt;         unsigned long end = start + size;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (master == FOR_CPU) {</span>
<span class="quote">&gt;                 switch (direction) {</span>
<span class="quote">&gt;                 case DMA_TO_DEVICE:</span>
<span class="quote">&gt;                         break;</span>
<span class="quote">&gt;                 case DMA_FROM_DEVICE:</span>
<span class="quote">&gt;                 case DMA_BIDIRECTIONAL:</span>
<span class="quote">&gt;                         cpu_dma_inval_range(start, end);</span>
<span class="quote">&gt;                         break;</span>
<span class="quote">&gt;                 default:</span>
<span class="quote">&gt;                         BUG();</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;         } else {</span>
<span class="quote">&gt;                 /* FOR_DEVICE */</span>
<span class="quote">&gt;                 switch (direction) {</span>
<span class="quote">&gt;                 case DMA_FROM_DEVICE:</span>
<span class="quote">&gt;                         break;</span>
<span class="quote">&gt;                 case DMA_TO_DEVICE:</span>
<span class="quote">&gt;                 case DMA_BIDIRECTIONAL:</span>
<span class="quote">&gt;                         cpu_dma_wb_range(start, end);</span>
<span class="quote">&gt;                         break;</span>
<span class="quote">&gt;                 default:</span>
<span class="quote">&gt;                         BUG();</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; }</span>

That looks reasonable enough, but it does depend on a number of factors,
and the dma-mapping.h implementation is not just about cache flushes.

As I don&#39;t know the microarchitecture, can you answer these questions:

- are caches always write-back, or could they be write-through?
- can the cache be shared with another CPU or a device?
- if the cache is shared, is it always coherent, never coherent, or
either of them?
- could the same memory be visible at different physical addresses
  and have conflicting caches?
- is the CPU physical address always the same as the address visible to the
  device?
- are there devices that can only see a subset of the physical memory?
- can there be an IOMMU?
- are there write-buffers in the CPU that might need to get flushed before
  flushing the cache?
- could cache lines be loaded speculatively or with read-ahead while
  a buffer is owned by a device?

        Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 25, 2018, 3:45 a.m.</div>
<pre class="content">
Hi, Arnd:

2018-01-24 19:36 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:
<span class="quote">&gt; On Tue, Jan 23, 2018 at 12:52 PM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Hi, Arnd:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 2018-01-23 16:23 GMT+08:00 Greentime Hu &lt;green.hu@gmail.com&gt;:</span>
<span class="quote">&gt;&gt;&gt; Hi, Arnd:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; 2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; This patch adds support for the DMA mapping API. It uses dma_map_ops for</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; flexibility.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; I&#39;m still unhappy about the way the cache flushes are done here as discussed</span>
<span class="quote">&gt;&gt;&gt;&gt; before. It&#39;s not a show-stopped, but no Ack from me.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; How about this implementation?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; I am not sure if I understand it correctly.</span>
<span class="quote">&gt;&gt; I list all the combinations.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; RAM to DEVICE</span>
<span class="quote">&gt;&gt;     before DMA =&gt; writeback cache</span>
<span class="quote">&gt;&gt;     after DMA =&gt; nop</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; DEVICE to RAM</span>
<span class="quote">&gt;&gt;     before DMA =&gt; nop</span>
<span class="quote">&gt;&gt;     after DMA =&gt; invalidate cache</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; static void consistent_sync(void *vaddr, size_t size, int direction, int master)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;         unsigned long start = (unsigned long)vaddr;</span>
<span class="quote">&gt;&gt;         unsigned long end = start + size;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;         if (master == FOR_CPU) {</span>
<span class="quote">&gt;&gt;                 switch (direction) {</span>
<span class="quote">&gt;&gt;                 case DMA_TO_DEVICE:</span>
<span class="quote">&gt;&gt;                         break;</span>
<span class="quote">&gt;&gt;                 case DMA_FROM_DEVICE:</span>
<span class="quote">&gt;&gt;                 case DMA_BIDIRECTIONAL:</span>
<span class="quote">&gt;&gt;                         cpu_dma_inval_range(start, end);</span>
<span class="quote">&gt;&gt;                         break;</span>
<span class="quote">&gt;&gt;                 default:</span>
<span class="quote">&gt;&gt;                         BUG();</span>
<span class="quote">&gt;&gt;                 }</span>
<span class="quote">&gt;&gt;         } else {</span>
<span class="quote">&gt;&gt;                 /* FOR_DEVICE */</span>
<span class="quote">&gt;&gt;                 switch (direction) {</span>
<span class="quote">&gt;&gt;                 case DMA_FROM_DEVICE:</span>
<span class="quote">&gt;&gt;                         break;</span>
<span class="quote">&gt;&gt;                 case DMA_TO_DEVICE:</span>
<span class="quote">&gt;&gt;                 case DMA_BIDIRECTIONAL:</span>
<span class="quote">&gt;&gt;                         cpu_dma_wb_range(start, end);</span>
<span class="quote">&gt;&gt;                         break;</span>
<span class="quote">&gt;&gt;                 default:</span>
<span class="quote">&gt;&gt;                         BUG();</span>
<span class="quote">&gt;&gt;                 }</span>
<span class="quote">&gt;&gt;         }</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That looks reasonable enough, but it does depend on a number of factors,</span>
<span class="quote">&gt; and the dma-mapping.h implementation is not just about cache flushes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As I don&#39;t know the microarchitecture, can you answer these questions:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; - are caches always write-back, or could they be write-through?</span>
Yes, we can config it to write-back or write-through.
<span class="quote">
&gt; - can the cache be shared with another CPU or a device?</span>
No, we don&#39;t support it.
<span class="quote">
&gt; - if the cache is shared, is it always coherent, never coherent, or</span>
<span class="quote">&gt; either of them?</span>
We don&#39;t support SMP and the device will access memory through bus. I
think the cache is not shared.
<span class="quote">
&gt; - could the same memory be visible at different physical addresses</span>
<span class="quote">&gt;   and have conflicting caches?</span>
We currently don&#39;t have such kind of SoC memory map.
<span class="quote">
&gt; - is the CPU physical address always the same as the address visible to the</span>
<span class="quote">&gt;   device?</span>
Yes, it is always the same unless the CPU uses local memory. The
physical address of local memory will overlap the original bus
address.
I think the local memory case can be ignored because we don&#39;t use it for now.
<span class="quote">
&gt; - are there devices that can only see a subset of the physical memory?</span>
All devices are able to see the whole physical memory in our current
SoC, but I think other SoC may support such kind of HW behavior.
<span class="quote">
&gt; - can there be an IOMMU?</span>
No.
<span class="quote">
&gt; - are there write-buffers in the CPU that might need to get flushed before</span>
<span class="quote">&gt;   flushing the cache?</span>
Yes, there are write-buffers in front of CPU caches but it should be
transparent to SW. We don&#39;t need to flush it.
<span class="quote">
&gt; - could cache lines be loaded speculatively or with read-ahead while</span>
<span class="quote">&gt;   a buffer is owned by a device?</span>
No.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 25, 2018, 10:42 a.m.</div>
<pre class="content">
On Thu, Jan 25, 2018 at 4:45 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:
<span class="quote">&gt; 2018-01-24 19:36 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt; On Tue, Jan 23, 2018 at 12:52 PM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; 2018-01-23 16:23 GMT+08:00 Greentime Hu &lt;green.hu@gmail.com&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt; 2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That looks reasonable enough, but it does depend on a number of factors,</span>
<span class="quote">&gt;&gt; and the dma-mapping.h implementation is not just about cache flushes.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; As I don&#39;t know the microarchitecture, can you answer these questions:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - are caches always write-back, or could they be write-through?</span>
<span class="quote">&gt; Yes, we can config it to write-back or write-through.</span>

Ok. If a WT-cache is common enough, you could optimize for that
case by skipping the explicit writeback here and just doing a synchronizing
instruction. Usually if the cache is configurable, one would pick the
writeback option though, so it&#39;s probably not important.
<span class="quote">
&gt;&gt; - is the CPU physical address always the same as the address visible to the</span>
<span class="quote">&gt;&gt;   device?</span>
<span class="quote">&gt; Yes, it is always the same unless the CPU uses local memory. The</span>
<span class="quote">&gt; physical address of local memory will overlap the original bus</span>
<span class="quote">&gt; address.</span>
<span class="quote">&gt; I think the local memory case can be ignored because we don&#39;t use it for now.</span>

Ok, makes sense.
<span class="quote">
&gt;&gt; - are there devices that can only see a subset of the physical memory?</span>
<span class="quote">&gt; All devices are able to see the whole physical memory in our current</span>
<span class="quote">&gt; SoC, but I think other SoC may support such kind of HW behavior.</span>

This is one area that might need a more complex implementation then,
depending on what devices are used in other SoCs. For network or
storage devices, it&#39;s usually sufficient to configure a DMA mask
from the &quot;dma-ranges&quot; property of the parent bus in the device tree,
the kernel code will then use bounce buffers.

For other types of drivers, using the streaming DMA interfaces
can require using the swiotlb helper that performs the bounce
buffering at in place of the cache operations. With a bit of luck,
you won&#39;t ever need to worry about it, just mentioning it here in
case you run into that problem later.

The consistent_sync() implementaiton you showed earlier should be
good enough then.  With that change,
<span class="acked-by">
Acked-by: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Jan. 25, 2018, 1:48 p.m.</div>
<pre class="content">
Hi, Arnd:

2018-01-25 18:42 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:
<span class="quote">&gt; On Thu, Jan 25, 2018 at 4:45 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; 2018-01-24 19:36 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jan 23, 2018 at 12:52 PM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; 2018-01-23 16:23 GMT+08:00 Greentime Hu &lt;green.hu@gmail.com&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; 2018-01-18 18:26 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On Mon, Jan 15, 2018 at 6:53 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That looks reasonable enough, but it does depend on a number of factors,</span>
<span class="quote">&gt;&gt;&gt; and the dma-mapping.h implementation is not just about cache flushes.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; As I don&#39;t know the microarchitecture, can you answer these questions:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; - are caches always write-back, or could they be write-through?</span>
<span class="quote">&gt;&gt; Yes, we can config it to write-back or write-through.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok. If a WT-cache is common enough, you could optimize for that</span>
<span class="quote">&gt; case by skipping the explicit writeback here and just doing a synchronizing</span>
<span class="quote">&gt; instruction. Usually if the cache is configurable, one would pick the</span>
<span class="quote">&gt; writeback option though, so it&#39;s probably not important.</span>

Thank you for this suggestion.
We have optimized in cpu_dcache_wb_range() and it will be called from
cpu_dma_wb_range().
It will do nothing if it is a write-through config cache.
<span class="quote">
&gt;&gt;&gt; - is the CPU physical address always the same as the address visible to the</span>
<span class="quote">&gt;&gt;&gt;   device?</span>
<span class="quote">&gt;&gt; Yes, it is always the same unless the CPU uses local memory. The</span>
<span class="quote">&gt;&gt; physical address of local memory will overlap the original bus</span>
<span class="quote">&gt;&gt; address.</span>
<span class="quote">&gt;&gt; I think the local memory case can be ignored because we don&#39;t use it for now.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok, makes sense.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt; - are there devices that can only see a subset of the physical memory?</span>
<span class="quote">&gt;&gt; All devices are able to see the whole physical memory in our current</span>
<span class="quote">&gt;&gt; SoC, but I think other SoC may support such kind of HW behavior.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is one area that might need a more complex implementation then,</span>
<span class="quote">&gt; depending on what devices are used in other SoCs. For network or</span>
<span class="quote">&gt; storage devices, it&#39;s usually sufficient to configure a DMA mask</span>
<span class="quote">&gt; from the &quot;dma-ranges&quot; property of the parent bus in the device tree,</span>
<span class="quote">&gt; the kernel code will then use bounce buffers.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For other types of drivers, using the streaming DMA interfaces</span>
<span class="quote">&gt; can require using the swiotlb helper that performs the bounce</span>
<span class="quote">&gt; buffering at in place of the cache operations. With a bit of luck,</span>
<span class="quote">&gt; you won&#39;t ever need to worry about it, just mentioning it here in</span>
<span class="quote">&gt; case you run into that problem later.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The consistent_sync() implementaiton you showed earlier should be</span>
<span class="quote">&gt; good enough then.  With that change,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Acked-by: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>

Thank you. :)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/nds32/include/asm/dma-mapping.h b/arch/nds32/include/asm/dma-mapping.h</span>
new file mode 100644
<span class="p_header">index 0000000..2dd47d24</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -0,0 +1,14 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+// Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+#define ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct dma_map_ops nds32_dma_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;nds32_dma_ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/kernel/dma.c b/arch/nds32/kernel/dma.c</span>
new file mode 100644
<span class="p_header">index 0000000..9bd1dc7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/kernel/dma.c</span>
<span class="p_chunk">@@ -0,0 +1,459 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+// Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/scatterlist.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/io.h&gt;</span>
<span class="p_add">+#include &lt;linux/cache.h&gt;</span>
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;asm/proc-fns.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the page table (2MB) covering uncached, DMA consistent allocations</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *consistent_pte;</span>
<span class="p_add">+static DEFINE_RAW_SPINLOCK(consistent_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * VM region handling support.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This should become something generic, handling VM region allocations for</span>
<span class="p_add">+ * vmalloc and similar (ioremap, module space, etc).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * I envisage vmalloc()&#39;s supporting vm_struct becoming:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_struct {</span>
<span class="p_add">+ *    struct vm_region	region;</span>
<span class="p_add">+ *    unsigned long	flags;</span>
<span class="p_add">+ *    struct page	**pages;</span>
<span class="p_add">+ *    unsigned int	nr_pages;</span>
<span class="p_add">+ *    unsigned long	phys_addr;</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * get_vm_area() would then call vm_region_alloc with an appropriate</span>
<span class="p_add">+ * struct vm_region head (eg):</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_region vmalloc_head = {</span>
<span class="p_add">+ *	.vm_list	= LIST_HEAD_INIT(vmalloc_head.vm_list),</span>
<span class="p_add">+ *	.vm_start	= VMALLOC_START,</span>
<span class="p_add">+ *	.vm_end		= VMALLOC_END,</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * However, vmalloc_head.vm_start is variable (typically, it is dependent on</span>
<span class="p_add">+ * the amount of RAM found at boot time.)  I would imagine that get_vm_area()</span>
<span class="p_add">+ * would have to initialise this each time prior to calling vm_region_alloc().</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct arch_vm_region {</span>
<span class="p_add">+	struct list_head vm_list;</span>
<span class="p_add">+	unsigned long vm_start;</span>
<span class="p_add">+	unsigned long vm_end;</span>
<span class="p_add">+	struct page *vm_pages;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region consistent_head = {</span>
<span class="p_add">+	.vm_list = LIST_HEAD_INIT(consistent_head.vm_list),</span>
<span class="p_add">+	.vm_start = CONSISTENT_BASE,</span>
<span class="p_add">+	.vm_end = CONSISTENT_END,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_alloc(struct arch_vm_region *head,</span>
<span class="p_add">+					      size_t size, int gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = head-&gt;vm_start, end = head-&gt;vm_end - size;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct arch_vm_region *c, *new;</span>
<span class="p_add">+</span>
<span class="p_add">+	new = kmalloc(sizeof(struct arch_vm_region), gfp);</span>
<span class="p_add">+	if (!new)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if ((addr + size) &lt; addr)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+		if ((addr + size) &lt;= c-&gt;vm_start)</span>
<span class="p_add">+			goto found;</span>
<span class="p_add">+		addr = c-&gt;vm_end;</span>
<span class="p_add">+		if (addr &gt; end)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+found:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Insert this entry _before_ the one we found.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	list_add_tail(&amp;new-&gt;vm_list, &amp;c-&gt;vm_list);</span>
<span class="p_add">+	new-&gt;vm_start = addr;</span>
<span class="p_add">+	new-&gt;vm_end = addr + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	return new;</span>
<span class="p_add">+</span>
<span class="p_add">+nospc:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	kfree(new);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_find(struct arch_vm_region *head,</span>
<span class="p_add">+					     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if (c-&gt;vm_start == addr)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	c = NULL;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: attrs is not used. */</span>
<span class="p_add">+static void *nds32_dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="p_add">+				      dma_addr_t * handle, gfp_t gfp,</span>
<span class="p_add">+				      unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long order;</span>
<span class="p_add">+	u64 mask = ~0ULL, limit;</span>
<span class="p_add">+	pgprot_t prot = pgprot_noncached(PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!consistent_pte) {</span>
<span class="p_add">+		pr_err(&quot;%s: not initialized\n&quot;, __func__);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dev) {</span>
<span class="p_add">+		mask = dev-&gt;coherent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Sanity check the DMA mask - it must be non-zero, and</span>
<span class="p_add">+		 * must be able to be satisfied by a DMA allocation.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mask == 0) {</span>
<span class="p_add">+			dev_warn(dev, &quot;coherent DMA mask is unset\n&quot;);</span>
<span class="p_add">+			goto no_page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Sanity check the allocation size.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+	limit = (mask + 1) &amp; ~mask;</span>
<span class="p_add">+	if ((limit &amp;&amp; size &gt;= limit) ||</span>
<span class="p_add">+	    size &gt;= (CONSISTENT_END - CONSISTENT_BASE)) {</span>
<span class="p_add">+		pr_warn(&quot;coherent allocation too big &quot;</span>
<span class="p_add">+			&quot;(requested %#x mask %#llx)\n&quot;, size, mask);</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	order = get_order(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mask != 0xffffffff)</span>
<span class="p_add">+		gfp |= GFP_DMA;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = alloc_pages(gfp, order);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate any data that might be lurking in the</span>
<span class="p_add">+	 * kernel direct-mapped region for device DMA.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		unsigned long kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		memset(page_address(page), 0, size);</span>
<span class="p_add">+		cpu_dma_wbinval_range(kaddr, kaddr + size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Allocate a virtual address in the consistent mapping region.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	c = vm_region_alloc(&amp;consistent_head, size,</span>
<span class="p_add">+			    gfp &amp; ~(__GFP_DMA | __GFP_HIGHMEM));</span>
<span class="p_add">+	if (c) {</span>
<span class="p_add">+		pte_t *pte = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+		struct page *end = page + (1 &lt;&lt; order);</span>
<span class="p_add">+</span>
<span class="p_add">+		c-&gt;vm_pages = page;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Set the &quot;dma handle&quot;</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		*handle = page_to_phys(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			BUG_ON(!pte_none(*pte));</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * x86 does not mark the pages reserved...</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			SetPageReserved(page);</span>
<span class="p_add">+			set_pte(pte, mk_pte(page, prot));</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+			pte++;</span>
<span class="p_add">+		} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Free the otherwise unused pages.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		while (page &lt; end) {</span>
<span class="p_add">+			__free_page(page);</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		return (void *)c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (page)</span>
<span class="p_add">+		__free_pages(page, order);</span>
<span class="p_add">+no_page:</span>
<span class="p_add">+	*handle = ~0;</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_free(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="p_add">+			   dma_addr_t handle, unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long flags, addr;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	c = vm_region_find(&amp;consistent_head, (unsigned long)cpu_addr);</span>
<span class="p_add">+	if (!c)</span>
<span class="p_add">+		goto no_area;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((c-&gt;vm_end - c-&gt;vm_start) != size) {</span>
<span class="p_add">+		pr_err(&quot;%s: freeing wrong coherent size (%ld != %d)\n&quot;,</span>
<span class="p_add">+		       __func__, c-&gt;vm_end - c-&gt;vm_start, size);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		size = c-&gt;vm_end - c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+	addr = c-&gt;vm_start;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte_t pte = ptep_get_and_clear(&amp;init_mm, addr, ptep);</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep++;</span>
<span class="p_add">+		addr += PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_none(pte) &amp;&amp; pte_present(pte)) {</span>
<span class="p_add">+			pfn = pte_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pfn_valid(pfn)) {</span>
<span class="p_add">+				struct page *page = pfn_to_page(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * x86 does not mark the pages reserved...</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				ClearPageReserved(page);</span>
<span class="p_add">+</span>
<span class="p_add">+				__free_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		pr_crit(&quot;%s: bad page in kernel page table\n&quot;, __func__);</span>
<span class="p_add">+	} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(c-&gt;vm_start, c-&gt;vm_end);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_del(&amp;c-&gt;vm_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(c);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+no_area:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	pr_err(&quot;%s: trying to free invalid coherent area: %p\n&quot;,</span>
<span class="p_add">+	       __func__, cpu_addr);</span>
<span class="p_add">+	dump_stack();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialise the consistent memory allocation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int __init consistent_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd = pgd_offset(&amp;init_mm, CONSISTENT_BASE);</span>
<span class="p_add">+		pmd = pmd_alloc(&amp;init_mm, pgd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pmd) {</span>
<span class="p_add">+			pr_err(&quot;%s: no pmd tables\n&quot;, __func__);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* The first level mapping may be created in somewhere.</span>
<span class="p_add">+		 * It&#39;s not necessary to warn here. */</span>
<span class="p_add">+		/* WARN_ON(!pmd_none(*pmd)); */</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_alloc_kernel(pmd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		consistent_pte = pte;</span>
<span class="p_add">+	} while (0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+core_initcall(consistent_init);</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction);</span>
<span class="p_add">+static dma_addr_t nds32_dma_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+				     unsigned long offset, size_t size,</span>
<span class="p_add">+				     enum dma_data_direction dir,</span>
<span class="p_add">+				     unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)(page_address(page) + offset), size, dir);</span>
<span class="p_add">+	return page_to_phys(page) + offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_page(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+				 unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync(phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make an area consistent for devices.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start = (unsigned long)vaddr;</span>
<span class="p_add">+	unsigned long end = start + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (direction) {</span>
<span class="p_add">+	case DMA_FROM_DEVICE:	/* invalidate only */</span>
<span class="p_add">+		cpu_dma_inval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_TO_DEVICE:	/* writeback only */</span>
<span class="p_add">+		cpu_dma_wb_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_BIDIRECTIONAL:	/* writeback and invalidate */</span>
<span class="p_add">+		cpu_dma_wbinval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int nds32_dma_map_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			    int nents, enum dma_data_direction dir,</span>
<span class="p_add">+			    unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		void *virt;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		struct page *page = sg_page(sg);</span>
<span class="p_add">+</span>
<span class="p_add">+		sg-&gt;dma_address = sg_phys(sg);</span>
<span class="p_add">+		pfn = page_to_pfn(page) + sg-&gt;offset / PAGE_SIZE;</span>
<span class="p_add">+		page = pfn_to_page(pfn);</span>
<span class="p_add">+		if (PageHighMem(page)) {</span>
<span class="p_add">+			virt = kmap_atomic(page);</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+			kunmap_atomic(virt);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (sg-&gt;offset &gt; PAGE_SIZE)</span>
<span class="p_add">+				panic(&quot;sg-&gt;offset:%08x &gt; PAGE_SIZE\n&quot;,</span>
<span class="p_add">+				      sg-&gt;offset);</span>
<span class="p_add">+			virt = page_address(page) + sg-&gt;offset;</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return nents;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			       int nhwentries, enum dma_data_direction dir,</span>
<span class="p_add">+			       unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+			      size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,</span>
<span class="p_add">+			  enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			     int nents, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct dma_map_ops nds32_dma_ops = {</span>
<span class="p_add">+	.alloc = nds32_dma_alloc_coherent,</span>
<span class="p_add">+	.free = nds32_dma_free,</span>
<span class="p_add">+	.map_page = nds32_dma_map_page,</span>
<span class="p_add">+	.unmap_page = nds32_dma_unmap_page,</span>
<span class="p_add">+	.map_sg = nds32_dma_map_sg,</span>
<span class="p_add">+	.unmap_sg = nds32_dma_unmap_sg,</span>
<span class="p_add">+	.sync_single_for_device = nds32_dma_sync_single_for_device,</span>
<span class="p_add">+	.sync_single_for_cpu = nds32_dma_sync_single_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_cpu = nds32_dma_sync_sg_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_device = nds32_dma_sync_sg_for_device,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(nds32_dma_ops);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



