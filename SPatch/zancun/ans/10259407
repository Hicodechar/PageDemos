
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[11/13] dma-direct: handle the memory encryption bit in common code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [11/13] dma-direct: handle the memory encryption bit in common code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 5, 2018, 5:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180305174655.9878-12-hch@lst.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10259407/mbox/"
   >mbox</a>
|
   <a href="/patch/10259407/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10259407/">/patch/10259407/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DB1EC60365 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Mar 2018 17:47:59 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C8BD1288C7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Mar 2018 17:47:59 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BC33F2897B; Mon,  5 Mar 2018 17:47:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 523C1288C7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Mar 2018 17:47:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752845AbeCERry (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 5 Mar 2018 12:47:54 -0500
Received: from bombadil.infradead.org ([198.137.202.133]:60732 &quot;EHLO
	bombadil.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752720AbeCERrV (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 5 Mar 2018 12:47:21 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=bombadil.20170209;
	h=References:In-Reply-To:Message-Id:
	Date:Subject:Cc:To:From:Sender:Reply-To:MIME-Version:Content-Type:
	Content-Transfer-Encoding:Content-ID:Content-Description:Resent-Date:
	Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:
	List-Help:List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=P89kwOinWN8nnEbEUjtekFlv/48SH5/fKvkOLhMQF8s=;
	b=U9ffUfsa3vLkP3L4o5heZ+Fct
	xocHbjExQmeliMxv/L2w51Bi5jo62k74IcXJ54udWJEY1VpDKwqFUeUTD0i6zikt+Q452ZD3uba5C
	10FwMkaXSO+78AgxI7VU/B+i9agUr5Rx0+0RTuXf+z/WFHxGwtUUagNZo33R/fmLwOi+Hl6D0QxN5
	PkOXmZUQAG10Dc+EAM4NnkjECFVbgjBa2ni/ESPsGzGP/whZ9AGT1HzQIOVUnYtZXjfo0mRtDGaNk
	PmiO8MjLZjXUhvkPB7pk5NNjV0i4R2CcKizD3AZhOdcmI9IoQyJS3JCN1eIl/j6QH/VYYR2lB3wn/
	i7ucEVC4A==;
Received: from [209.116.154.70] (helo=localhost)
	by bombadil.infradead.org with esmtpsa (Exim 4.89 #1 (Red Hat Linux))
	id 1esuCX-0002wv-Qm; Mon, 05 Mar 2018 17:47:06 +0000
From: Christoph Hellwig &lt;hch@lst.de&gt;
To: x86@kernel.org
Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;,
	Tom Lendacky &lt;thomas.lendacky@amd.com&gt;,
	David Woodhouse &lt;dwmw2@infradead.org&gt;, Muli Ben-Yehuda &lt;mulix@mulix.org&gt;,
	Jon Mason &lt;jdmason@kudzu.us&gt;, Joerg Roedel &lt;joro@8bytes.org&gt;,
	iommu@lists.linux-foundation.org, linux-kernel@vger.kernel.org
Subject: [PATCH 11/13] dma-direct: handle the memory encryption bit in
	common code
Date: Mon,  5 Mar 2018 09:46:53 -0800
Message-Id: &lt;20180305174655.9878-12-hch@lst.de&gt;
X-Mailer: git-send-email 2.14.2
In-Reply-To: &lt;20180305174655.9878-1-hch@lst.de&gt;
References: &lt;20180305174655.9878-1-hch@lst.de&gt;
X-SRS-Rewrite: SMTP reverse-path rewritten from &lt;hch@infradead.org&gt; by
	bombadil.infradead.org. See http://www.infradead.org/rpr.html
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - March 5, 2018, 5:46 p.m.</div>
<pre class="content">
Give the basic phys_to_dma and dma_to_phys helpers a __-prefix and add
the memory encryption mask to the non-prefixed versions.  Use the
__-prefixed versions directly instead of clearing the mask again in
various places.

With that in place the generic dma-direct routines can be used to
allocate non-encrypted bounce buffers, and the x86 SEV case can use
the generic swiotlb ops.
<span class="signed-off-by">
Signed-off-by: Christoph Hellwig &lt;hch@lst.de&gt;</span>
---
 arch/arm/include/asm/dma-direct.h                  |  4 +-
 arch/mips/cavium-octeon/dma-octeon.c               | 10 +--
 .../include/asm/mach-cavium-octeon/dma-coherence.h |  4 +-
 .../include/asm/mach-loongson64/dma-coherence.h    | 10 +--
 arch/mips/loongson64/common/dma-swiotlb.c          |  4 +-
 arch/powerpc/include/asm/dma-direct.h              |  4 +-
 arch/x86/Kconfig                                   |  2 +-
 arch/x86/include/asm/dma-direct.h                  | 25 +-------
 arch/x86/mm/mem_encrypt.c                          | 73 +---------------------
 arch/x86/pci/sta2x11-fixup.c                       |  6 +-
 include/linux/dma-direct.h                         | 21 ++++++-
 lib/dma-direct.c                                   | 21 +++++--
 lib/swiotlb.c                                      | 25 +++-----
 13 files changed, 70 insertions(+), 139 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - March 12, 2018, 6:29 p.m.</div>
<pre class="content">
On 3/5/2018 11:46 AM, Christoph Hellwig wrote:
<span class="quote">&gt; Give the basic phys_to_dma and dma_to_phys helpers a __-prefix and add</span>
<span class="quote">&gt; the memory encryption mask to the non-prefixed versions.  Use the</span>
<span class="quote">&gt; __-prefixed versions directly instead of clearing the mask again in</span>
<span class="quote">&gt; various places.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With that in place the generic dma-direct routines can be used to</span>
<span class="quote">&gt; allocate non-encrypted bounce buffers, and the x86 SEV case can use</span>
<span class="quote">&gt; the generic swiotlb ops.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Christoph Hellwig &lt;hch@lst.de&gt;</span>

So this patch results in my system failing to boot when SME is active.
I&#39;m investigating further to see why.  I&#39;ll follow up with more details
as I find them.

Additionally, when running with SME (not SEV), this is forcing all DMA
coherent allocations to be decrypted, when that isn&#39;t required with SME
(as long as the device can perform 48-bit or greater DMA).  So it may
be worth looking at only doing the decrypted allocations for SEV.

Thanks,
Tom
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/dma-direct.h                  |  4 +-</span>
<span class="quote">&gt;  arch/mips/cavium-octeon/dma-octeon.c               | 10 +--</span>
<span class="quote">&gt;  .../include/asm/mach-cavium-octeon/dma-coherence.h |  4 +-</span>
<span class="quote">&gt;  .../include/asm/mach-loongson64/dma-coherence.h    | 10 +--</span>
<span class="quote">&gt;  arch/mips/loongson64/common/dma-swiotlb.c          |  4 +-</span>
<span class="quote">&gt;  arch/powerpc/include/asm/dma-direct.h              |  4 +-</span>
<span class="quote">&gt;  arch/x86/Kconfig                                   |  2 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/dma-direct.h                  | 25 +-------</span>
<span class="quote">&gt;  arch/x86/mm/mem_encrypt.c                          | 73 +---------------------</span>
<span class="quote">&gt;  arch/x86/pci/sta2x11-fixup.c                       |  6 +-</span>
<span class="quote">&gt;  include/linux/dma-direct.h                         | 21 ++++++-</span>
<span class="quote">&gt;  lib/dma-direct.c                                   | 21 +++++--</span>
<span class="quote">&gt;  lib/swiotlb.c                                      | 25 +++-----</span>
<span class="quote">&gt;  13 files changed, 70 insertions(+), 139 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/dma-direct.h b/arch/arm/include/asm/dma-direct.h</span>
<span class="quote">&gt; index 5b0a8a421894..b67e5fc1fe43 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/dma-direct.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/dma-direct.h</span>
<span class="quote">&gt; @@ -2,13 +2,13 @@</span>
<span class="quote">&gt;  #ifndef ASM_ARM_DMA_DIRECT_H</span>
<span class="quote">&gt;  #define ASM_ARM_DMA_DIRECT_H 1</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int offset = paddr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;  	return pfn_to_dma(dev, __phys_to_pfn(paddr)) + offset;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt; +static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int offset = dev_addr &amp; ~PAGE_MASK;</span>
<span class="quote">&gt;  	return __pfn_to_phys(dma_to_pfn(dev, dev_addr)) + offset;</span>
<span class="quote">&gt; diff --git a/arch/mips/cavium-octeon/dma-octeon.c b/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="quote">&gt; index c7bb8a407041..7b335ab21697 100644</span>
<span class="quote">&gt; --- a/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="quote">&gt; +++ b/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="quote">&gt; @@ -10,7 +10,7 @@</span>
<span class="quote">&gt;   * IP32 changes by Ilya.</span>
<span class="quote">&gt;   * Copyright (C) 2010 Cavium Networks, Inc.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#include &lt;linux/dma-mapping.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/dma-direct.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/scatterlist.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/bootmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt; @@ -182,7 +182,7 @@ struct octeon_dma_map_ops {</span>
<span class="quote">&gt;  	phys_addr_t (*dma_to_phys)(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),</span>
<span class="quote">&gt;  						      struct octeon_dma_map_ops,</span>
<span class="quote">&gt; @@ -190,9 +190,9 @@ dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return ops-&gt;phys_to_dma(dev, paddr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -EXPORT_SYMBOL(phys_to_dma);</span>
<span class="quote">&gt; +EXPORT_SYMBOL(__phys_to_dma);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; +phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),</span>
<span class="quote">&gt;  						      struct octeon_dma_map_ops,</span>
<span class="quote">&gt; @@ -200,7 +200,7 @@ phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return ops-&gt;dma_to_phys(dev, daddr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -EXPORT_SYMBOL(dma_to_phys);</span>
<span class="quote">&gt; +EXPORT_SYMBOL(__dma_to_phys);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static struct octeon_dma_map_ops octeon_linear_dma_map_ops = {</span>
<span class="quote">&gt;  	.dma_map_ops = {</span>
<span class="quote">&gt; diff --git a/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h b/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="quote">&gt; index 138edf6b5b48..6eb1ee548b11 100644</span>
<span class="quote">&gt; --- a/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="quote">&gt; +++ b/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="quote">&gt; @@ -69,8 +69,8 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; -phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt; +dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; +phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct dma_map_ops;</span>
<span class="quote">&gt;  extern const struct dma_map_ops *octeon_pci_dma_map_ops;</span>
<span class="quote">&gt; diff --git a/arch/mips/include/asm/mach-loongson64/dma-coherence.h b/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="quote">&gt; index b1b575f5c6c1..64fc44dec0a8 100644</span>
<span class="quote">&gt; --- a/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="quote">&gt; +++ b/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="quote">&gt; @@ -25,13 +25,13 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -extern dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; -extern phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt; +extern dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; +extern phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt;  static inline dma_addr_t plat_map_dma_mem(struct device *dev, void *addr,</span>
<span class="quote">&gt;  					  size_t size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  #ifdef CONFIG_CPU_LOONGSON3</span>
<span class="quote">&gt; -	return phys_to_dma(dev, virt_to_phys(addr));</span>
<span class="quote">&gt; +	return __phys_to_dma(dev, virt_to_phys(addr));</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  	return virt_to_phys(addr) | 0x80000000;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -41,7 +41,7 @@ static inline dma_addr_t plat_map_dma_mem_page(struct device *dev,</span>
<span class="quote">&gt;  					       struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  #ifdef CONFIG_CPU_LOONGSON3</span>
<span class="quote">&gt; -	return phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt; +	return __phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  	return page_to_phys(page) | 0x80000000;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -51,7 +51,7 @@ static inline unsigned long plat_dma_addr_to_phys(struct device *dev,</span>
<span class="quote">&gt;  	dma_addr_t dma_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  #if defined(CONFIG_CPU_LOONGSON3) &amp;&amp; defined(CONFIG_64BIT)</span>
<span class="quote">&gt; -	return dma_to_phys(dev, dma_addr);</span>
<span class="quote">&gt; +	return __dma_to_phys(dev, dma_addr);</span>
<span class="quote">&gt;  #elif defined(CONFIG_CPU_LOONGSON2F) &amp;&amp; defined(CONFIG_64BIT)</span>
<span class="quote">&gt;  	return (dma_addr &gt; 0x8fffffff) ? dma_addr : (dma_addr &amp; 0x0fffffff);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/arch/mips/loongson64/common/dma-swiotlb.c b/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="quote">&gt; index 7bbcf89475f3..6a739f8ae110 100644</span>
<span class="quote">&gt; --- a/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="quote">&gt; +++ b/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="quote">&gt; @@ -63,7 +63,7 @@ static int loongson_dma_supported(struct device *dev, u64 mask)</span>
<span class="quote">&gt;  	return swiotlb_dma_supported(dev, mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long nid;</span>
<span class="quote">&gt;  #ifdef CONFIG_PHYS48_TO_HT40</span>
<span class="quote">&gt; @@ -75,7 +75,7 @@ dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  	return paddr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; +phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long nid;</span>
<span class="quote">&gt;  #ifdef CONFIG_PHYS48_TO_HT40</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/dma-direct.h b/arch/powerpc/include/asm/dma-direct.h</span>
<span class="quote">&gt; index a5b59c765426..7702875aabb7 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/dma-direct.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/dma-direct.h</span>
<span class="quote">&gt; @@ -17,12 +17,12 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return paddr + get_dma_offset(dev);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; +static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return daddr - get_dma_offset(dev);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index 7272bb3768d7..6e25ca4c86ee 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -54,7 +54,6 @@ config X86</span>
<span class="quote">&gt;  	select ARCH_HAS_FORTIFY_SOURCE</span>
<span class="quote">&gt;  	select ARCH_HAS_GCOV_PROFILE_ALL</span>
<span class="quote">&gt;  	select ARCH_HAS_KCOV			if X86_64</span>
<span class="quote">&gt; -	select ARCH_HAS_PHYS_TO_DMA</span>
<span class="quote">&gt;  	select ARCH_HAS_MEMBARRIER_SYNC_CORE</span>
<span class="quote">&gt;  	select ARCH_HAS_PMEM_API		if X86_64</span>
<span class="quote">&gt;  	select ARCH_HAS_REFCOUNT</span>
<span class="quote">&gt; @@ -691,6 +690,7 @@ config X86_SUPPORTS_MEMORY_FAILURE</span>
<span class="quote">&gt;  config STA2X11</span>
<span class="quote">&gt;  	bool &quot;STA2X11 Companion Chip Support&quot;</span>
<span class="quote">&gt;  	depends on X86_32_NON_STANDARD &amp;&amp; PCI</span>
<span class="quote">&gt; +	select ARCH_HAS_PHYS_TO_DMA</span>
<span class="quote">&gt;  	select X86_DEV_DMA_OPS</span>
<span class="quote">&gt;  	select X86_DMA_REMAP</span>
<span class="quote">&gt;  	select SWIOTLB</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/dma-direct.h b/arch/x86/include/asm/dma-direct.h</span>
<span class="quote">&gt; index 1295bc622ebe..1a19251eaac9 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/dma-direct.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/dma-direct.h</span>
<span class="quote">&gt; @@ -2,29 +2,8 @@</span>
<span class="quote">&gt;  #ifndef ASM_X86_DMA_DIRECT_H</span>
<span class="quote">&gt;  #define ASM_X86_DMA_DIRECT_H 1</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#include &lt;linux/mem_encrypt.h&gt;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifdef CONFIG_X86_DMA_REMAP /* Platform code defines bridge-specific code */</span>
<span class="quote">&gt;  bool dma_capable(struct device *dev, dma_addr_t addr, size_t size);</span>
<span class="quote">&gt; -dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; -phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (!dev-&gt;dma_mask)</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return __sme_set(paddr);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; +dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="quote">&gt; +phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return __sme_clr(daddr);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif /* CONFIG_X86_DMA_REMAP */</span>
<span class="quote">&gt;  #endif /* ASM_X86_DMA_DIRECT_H */</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; index 66beedc8fe3d..8bfc735bbdd7 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="quote">&gt; @@ -200,58 +200,6 @@ void __init sme_early_init(void)</span>
<span class="quote">&gt;  		swiotlb_force = SWIOTLB_FORCE;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void *sev_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt; -		       gfp_t gfp, unsigned long attrs)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	unsigned int order;</span>
<span class="quote">&gt; -	struct page *page;</span>
<span class="quote">&gt; -	void *vaddr = NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	order = get_order(size);</span>
<span class="quote">&gt; -	page = alloc_pages_node(dev_to_node(dev), gfp, order);</span>
<span class="quote">&gt; -	if (page) {</span>
<span class="quote">&gt; -		dma_addr_t addr;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Since we will be clearing the encryption bit, check the</span>
<span class="quote">&gt; -		 * mask with it already cleared.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		addr = __sme_clr(phys_to_dma(dev, page_to_phys(page)));</span>
<span class="quote">&gt; -		if ((addr + size) &gt; dev-&gt;coherent_dma_mask) {</span>
<span class="quote">&gt; -			__free_pages(page, get_order(size));</span>
<span class="quote">&gt; -		} else {</span>
<span class="quote">&gt; -			vaddr = page_address(page);</span>
<span class="quote">&gt; -			*dma_handle = addr;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (!vaddr)</span>
<span class="quote">&gt; -		vaddr = swiotlb_alloc_coherent(dev, size, dma_handle, gfp);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (!vaddr)</span>
<span class="quote">&gt; -		return NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* Clear the SME encryption bit for DMA use if not swiotlb area */</span>
<span class="quote">&gt; -	if (!is_swiotlb_buffer(dma_to_phys(dev, *dma_handle))) {</span>
<span class="quote">&gt; -		set_memory_decrypted((unsigned long)vaddr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; -		memset(vaddr, 0, PAGE_SIZE &lt;&lt; order);</span>
<span class="quote">&gt; -		*dma_handle = __sme_clr(*dma_handle);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return vaddr;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void sev_free(struct device *dev, size_t size, void *vaddr,</span>
<span class="quote">&gt; -		     dma_addr_t dma_handle, unsigned long attrs)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	/* Set the SME encryption bit for re-use if not swiotlb area */</span>
<span class="quote">&gt; -	if (!is_swiotlb_buffer(dma_to_phys(dev, dma_handle)))</span>
<span class="quote">&gt; -		set_memory_encrypted((unsigned long)vaddr,</span>
<span class="quote">&gt; -				     1 &lt;&lt; get_order(size));</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	swiotlb_free_coherent(dev, size, vaddr, dma_handle);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgprot_t old_prot, new_prot;</span>
<span class="quote">&gt; @@ -404,20 +352,6 @@ bool sev_active(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(sev_active);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static const struct dma_map_ops sev_dma_ops = {</span>
<span class="quote">&gt; -	.alloc                  = sev_alloc,</span>
<span class="quote">&gt; -	.free                   = sev_free,</span>
<span class="quote">&gt; -	.map_page               = swiotlb_map_page,</span>
<span class="quote">&gt; -	.unmap_page             = swiotlb_unmap_page,</span>
<span class="quote">&gt; -	.map_sg                 = swiotlb_map_sg_attrs,</span>
<span class="quote">&gt; -	.unmap_sg               = swiotlb_unmap_sg_attrs,</span>
<span class="quote">&gt; -	.sync_single_for_cpu    = swiotlb_sync_single_for_cpu,</span>
<span class="quote">&gt; -	.sync_single_for_device = swiotlb_sync_single_for_device,</span>
<span class="quote">&gt; -	.sync_sg_for_cpu        = swiotlb_sync_sg_for_cpu,</span>
<span class="quote">&gt; -	.sync_sg_for_device     = swiotlb_sync_sg_for_device,</span>
<span class="quote">&gt; -	.mapping_error          = swiotlb_dma_mapping_error,</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /* Architecture __weak replacement functions */</span>
<span class="quote">&gt;  void __init mem_encrypt_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -428,12 +362,11 @@ void __init mem_encrypt_init(void)</span>
<span class="quote">&gt;  	swiotlb_update_mem_attributes();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * With SEV, DMA operations cannot use encryption. New DMA ops</span>
<span class="quote">&gt; -	 * are required in order to mark the DMA areas as decrypted or</span>
<span class="quote">&gt; -	 * to use bounce buffers.</span>
<span class="quote">&gt; +	 * With SEV, DMA operations cannot use encryption, we need to use</span>
<span class="quote">&gt; +	 * SWIOTLB to bounce buffer DMA operation.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (sev_active())</span>
<span class="quote">&gt; -		dma_ops = &amp;sev_dma_ops;</span>
<span class="quote">&gt; +		dma_ops = &amp;swiotlb_dma_ops;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * With SEV, we need to unroll the rep string I/O instructions.</span>
<span class="quote">&gt; diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="quote">&gt; index eac58e03f43c..7a5bafb76d77 100644</span>
<span class="quote">&gt; --- a/arch/x86/pci/sta2x11-fixup.c</span>
<span class="quote">&gt; +++ b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="quote">&gt; @@ -207,11 +207,11 @@ bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; - * phys_to_dma - Return the DMA AMBA address used for this STA2x11 device</span>
<span class="quote">&gt; + * __phys_to_dma - Return the DMA AMBA address used for this STA2x11 device</span>
<span class="quote">&gt;   * @dev: device for a PCI device</span>
<span class="quote">&gt;   * @paddr: Physical address</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!dev-&gt;archdata.is_sta2x11)</span>
<span class="quote">&gt;  		return paddr;</span>
<span class="quote">&gt; @@ -223,7 +223,7 @@ dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;   * @dev: device for a PCI device</span>
<span class="quote">&gt;   * @daddr: STA2x11 AMBA DMA address</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; +phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!dev-&gt;archdata.is_sta2x11)</span>
<span class="quote">&gt;  		return daddr;</span>
<span class="quote">&gt; diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h</span>
<span class="quote">&gt; index bcdb1a3e4b1f..53ad6a47f513 100644</span>
<span class="quote">&gt; --- a/include/linux/dma-direct.h</span>
<span class="quote">&gt; +++ b/include/linux/dma-direct.h</span>
<span class="quote">&gt; @@ -3,18 +3,19 @@</span>
<span class="quote">&gt;  #define _LINUX_DMA_DIRECT_H 1</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;linux/dma-mapping.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mem_encrypt.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA</span>
<span class="quote">&gt;  #include &lt;asm/dma-direct.h&gt;</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; -static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	dma_addr_t dev_addr = (dma_addr_t)paddr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dev_addr - ((dma_addr_t)dev-&gt;dma_pfn_offset &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt; +static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	phys_addr_t paddr = (phys_addr_t)dev_addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -30,6 +31,22 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * If memory encryption is supported, phys_to_dma will set the memory encryption</span>
<span class="quote">&gt; + * bit in the DMA address, and dma_to_phys will clear it.  The raw __phys_to_dma</span>
<span class="quote">&gt; + * and __dma_to_phys versions should only be used on non-encrypted memory for</span>
<span class="quote">&gt; + * special occasions like DMA coherent buffers.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __sme_set(__phys_to_dma(dev, paddr));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __sme_clr(__dma_to_phys(dev, daddr));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_ARCH_HAS_DMA_MARK_CLEAN</span>
<span class="quote">&gt;  void dma_mark_clean(void *addr, size_t size);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/lib/dma-direct.c b/lib/dma-direct.c</span>
<span class="quote">&gt; index c9e8e21cb334..84f50b5982fc 100644</span>
<span class="quote">&gt; --- a/lib/dma-direct.c</span>
<span class="quote">&gt; +++ b/lib/dma-direct.c</span>
<span class="quote">&gt; @@ -9,6 +9,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/scatterlist.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/dma-contiguous.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/pfn.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/set_memory.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define DIRECT_MAPPING_ERROR		0</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -35,9 +36,13 @@ check_addr(struct device *dev, dma_addr_t dma_addr, size_t size,</span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Since we will be clearing the encryption bit, check the mask with it already</span>
<span class="quote">&gt; + * cleared.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt;  static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
<span class="quote">&gt; +	return __phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt; @@ -46,6 +51,7 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;  	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	int page_order = get_order(size);</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; +	void *ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* GFP_DMA32 and GFP_DMA are no ops without the corresponding zones: */</span>
<span class="quote">&gt;  	if (dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))</span>
<span class="quote">&gt; @@ -78,10 +84,11 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	*dma_handle = phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt; -	memset(page_address(page), 0, size);</span>
<span class="quote">&gt; -	return page_address(page);</span>
<span class="quote">&gt; +	*dma_handle = __phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt; +	ret = page_address(page);</span>
<span class="quote">&gt; +	set_memory_decrypted((unsigned long)ret, page_order);</span>
<span class="quote">&gt; +	memset(ret, 0, size);</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -92,9 +99,11 @@ void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="quote">&gt;  		dma_addr_t dma_addr, unsigned long attrs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +	unsigned int page_order = get_order(size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	set_memory_encrypted((unsigned long)cpu_addr, 1 &lt;&lt; page_order);</span>
<span class="quote">&gt;  	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))</span>
<span class="quote">&gt; -		free_pages((unsigned long)cpu_addr, get_order(size));</span>
<span class="quote">&gt; +		free_pages((unsigned long)cpu_addr, page_order);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt; index c43ec2271469..ca8eeaead925 100644</span>
<span class="quote">&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt; @@ -158,13 +158,6 @@ unsigned long swiotlb_size_or_default(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void __weak swiotlb_set_mem_attributes(void *vaddr, unsigned long size) { }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="quote">&gt; -static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="quote">&gt; -				      phys_addr_t address)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return __sme_clr(phys_to_dma(hwdev, address));</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /* Note that this doesn&#39;t work with highmem page */</span>
<span class="quote">&gt;  static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,</span>
<span class="quote">&gt;  				      volatile void *address)</span>
<span class="quote">&gt; @@ -622,7 +615,7 @@ map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;  		return SWIOTLB_MAP_ERROR;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt; +	start_dma_addr = __phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;  	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,</span>
<span class="quote">&gt;  				      dir, attrs);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -726,12 +719,12 @@ swiotlb_alloc_buffer(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;  		goto out_warn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	phys_addr = swiotlb_tbl_map_single(dev,</span>
<span class="quote">&gt; -			swiotlb_phys_to_dma(dev, io_tlb_start),</span>
<span class="quote">&gt; +			__phys_to_dma(dev, io_tlb_start),</span>
<span class="quote">&gt;  			0, size, DMA_FROM_DEVICE, 0);</span>
<span class="quote">&gt;  	if (phys_addr == SWIOTLB_MAP_ERROR)</span>
<span class="quote">&gt;  		goto out_warn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	*dma_handle = swiotlb_phys_to_dma(dev, phys_addr);</span>
<span class="quote">&gt; +	*dma_handle = __phys_to_dma(dev, phys_addr);</span>
<span class="quote">&gt;  	if (dma_coherent_ok(dev, *dma_handle, size))</span>
<span class="quote">&gt;  		goto out_unmap;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -867,10 +860,10 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  	map = map_single(dev, phys, size, dir, attrs);</span>
<span class="quote">&gt;  	if (map == SWIOTLB_MAP_ERROR) {</span>
<span class="quote">&gt;  		swiotlb_full(dev, size, dir, 1);</span>
<span class="quote">&gt; -		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +		return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
<span class="quote">&gt; +	dev_addr = __phys_to_dma(dev, map);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Ensure that the address returned is DMA&#39;ble */</span>
<span class="quote">&gt;  	if (dma_capable(dev, dev_addr, size))</span>
<span class="quote">&gt; @@ -879,7 +872,7 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  	attrs |= DMA_ATTR_SKIP_CPU_SYNC;</span>
<span class="quote">&gt;  	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt; +	return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1009,7 +1002,7 @@ swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
<span class="quote">&gt;  				sg_dma_len(sgl) = 0;</span>
<span class="quote">&gt;  				return 0;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; -			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
<span class="quote">&gt; +			sg-&gt;dma_address = __phys_to_dma(hwdev, map);</span>
<span class="quote">&gt;  		} else</span>
<span class="quote">&gt;  			sg-&gt;dma_address = dev_addr;</span>
<span class="quote">&gt;  		sg_dma_len(sg) = sg-&gt;length;</span>
<span class="quote">&gt; @@ -1073,7 +1066,7 @@ swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,</span>
<span class="quote">&gt;  int</span>
<span class="quote">&gt;  swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt; +	return (dma_addr == __phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1085,7 +1078,7 @@ swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
<span class="quote">&gt;  int</span>
<span class="quote">&gt;  swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt; +	return __phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_DMA_DIRECT_OPS</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - March 12, 2018, 7:48 p.m.</div>
<pre class="content">
On 3/12/2018 1:29 PM, Tom Lendacky wrote:
<span class="quote">&gt; On 3/5/2018 11:46 AM, Christoph Hellwig wrote:</span>
<span class="quote">&gt;&gt; Give the basic phys_to_dma and dma_to_phys helpers a __-prefix and add</span>
<span class="quote">&gt;&gt; the memory encryption mask to the non-prefixed versions.  Use the</span>
<span class="quote">&gt;&gt; __-prefixed versions directly instead of clearing the mask again in</span>
<span class="quote">&gt;&gt; various places.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; With that in place the generic dma-direct routines can be used to</span>
<span class="quote">&gt;&gt; allocate non-encrypted bounce buffers, and the x86 SEV case can use</span>
<span class="quote">&gt;&gt; the generic swiotlb ops.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Christoph Hellwig &lt;hch@lst.de&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So this patch results in my system failing to boot when SME is active.</span>
<span class="quote">&gt; I&#39;m investigating further to see why.  I&#39;ll follow up with more details</span>
<span class="quote">&gt; as I find them.</span>

Ok, I found one issue that allows this to work when the IOMMU isn&#39;t
enabled (see below).

But the bigger issue is when the IOMMU is enabled.  The IOMMU code uses
a common mapping routine to create the I/O page tables.  This routine
assumes that all memory being mapped is encrypted and therefore sets the
encryption bit in the I/O page tables.  With this patch, the call to
dma_alloc_direct() now returns un-encrypted memory which results in an
encryption mis-match.  I think keeping dma_alloc_direct() as it was prior
to this patch is the way to go.  It allows SME DMA allocations to remain
encrypted and avoids added complexity in the amd_iommu.c file.  This
would mean that SEV would still have special DMA operations (so that the
alloc/free can change the memory to un-encrypted).

What do you think?
<span class="quote">
&gt; </span>
<span class="quote">&gt; Additionally, when running with SME (not SEV), this is forcing all DMA</span>
<span class="quote">&gt; coherent allocations to be decrypted, when that isn&#39;t required with SME</span>
<span class="quote">&gt; (as long as the device can perform 48-bit or greater DMA).  So it may</span>
<span class="quote">&gt; be worth looking at only doing the decrypted allocations for SEV.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Tom</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/arm/include/asm/dma-direct.h                  |  4 +-</span>
<span class="quote">&gt;&gt;  arch/mips/cavium-octeon/dma-octeon.c               | 10 +--</span>
<span class="quote">&gt;&gt;  .../include/asm/mach-cavium-octeon/dma-coherence.h |  4 +-</span>
<span class="quote">&gt;&gt;  .../include/asm/mach-loongson64/dma-coherence.h    | 10 +--</span>
<span class="quote">&gt;&gt;  arch/mips/loongson64/common/dma-swiotlb.c          |  4 +-</span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/dma-direct.h              |  4 +-</span>
<span class="quote">&gt;&gt;  arch/x86/Kconfig                                   |  2 +-</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/dma-direct.h                  | 25 +-------</span>
<span class="quote">&gt;&gt;  arch/x86/mm/mem_encrypt.c                          | 73 +---------------------</span>
<span class="quote">&gt;&gt;  arch/x86/pci/sta2x11-fixup.c                       |  6 +-</span>
<span class="quote">&gt;&gt;  include/linux/dma-direct.h                         | 21 ++++++-</span>
<span class="quote">&gt;&gt;  lib/dma-direct.c                                   | 21 +++++--</span>
<span class="quote">&gt;&gt;  lib/swiotlb.c                                      | 25 +++-----</span>
<span class="quote">&gt;&gt;  13 files changed, 70 insertions(+), 139 deletions(-)</span>
<span class="quote">&gt;&gt;</span>

&lt; ... SNIP ... &gt;
<span class="quote">
&gt;&gt; diff --git a/lib/dma-direct.c b/lib/dma-direct.c</span>
<span class="quote">&gt;&gt; index c9e8e21cb334..84f50b5982fc 100644</span>
<span class="quote">&gt;&gt; --- a/lib/dma-direct.c</span>
<span class="quote">&gt;&gt; +++ b/lib/dma-direct.c</span>
<span class="quote">&gt;&gt; @@ -9,6 +9,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;linux/scatterlist.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/dma-contiguous.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;linux/pfn.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/set_memory.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define DIRECT_MAPPING_ERROR		0</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -35,9 +36,13 @@ check_addr(struct device *dev, dma_addr_t dma_addr, size_t size,</span>
<span class="quote">&gt;&gt;  	return true;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Since we will be clearing the encryption bit, check the mask with it already</span>
<span class="quote">&gt;&gt; + * cleared.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt;  static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	return phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
<span class="quote">&gt;&gt; +	return __phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;&gt; @@ -46,6 +51,7 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;&gt;  	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt;  	int page_order = get_order(size);</span>
<span class="quote">&gt;&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;&gt; +	void *ret;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/* GFP_DMA32 and GFP_DMA are no ops without the corresponding zones: */</span>
<span class="quote">&gt;&gt;  	if (dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))</span>
<span class="quote">&gt;&gt; @@ -78,10 +84,11 @@ void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (!page)</span>
<span class="quote">&gt;&gt;  		return NULL;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	*dma_handle = phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt;&gt; -	memset(page_address(page), 0, size);</span>
<span class="quote">&gt;&gt; -	return page_address(page);</span>
<span class="quote">&gt;&gt; +	*dma_handle = __phys_to_dma(dev, page_to_phys(page));</span>
<span class="quote">&gt;&gt; +	ret = page_address(page);</span>
<span class="quote">&gt;&gt; +	set_memory_decrypted((unsigned long)ret, page_order);</span>

The second parameter should be 1 &lt;&lt; page_order to get the number of
pages.

Thanks,
Tom
<span class="quote">
&gt;&gt; +	memset(ret, 0, size);</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; @@ -92,9 +99,11 @@ void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="quote">&gt;&gt;  		dma_addr_t dma_addr, unsigned long attrs)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt; +	unsigned int page_order = get_order(size);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +	set_memory_encrypted((unsigned long)cpu_addr, 1 &lt;&lt; page_order);</span>
<span class="quote">&gt;&gt;  	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))</span>
<span class="quote">&gt;&gt; -		free_pages((unsigned long)cpu_addr, get_order(size));</span>
<span class="quote">&gt;&gt; +		free_pages((unsigned long)cpu_addr, page_order);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;&gt; diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; index c43ec2271469..ca8eeaead925 100644</span>
<span class="quote">&gt;&gt; --- a/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; +++ b/lib/swiotlb.c</span>
<span class="quote">&gt;&gt; @@ -158,13 +158,6 @@ unsigned long swiotlb_size_or_default(void)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  void __weak swiotlb_set_mem_attributes(void *vaddr, unsigned long size) { }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="quote">&gt;&gt; -static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="quote">&gt;&gt; -				      phys_addr_t address)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	return __sme_clr(phys_to_dma(hwdev, address));</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  /* Note that this doesn&#39;t work with highmem page */</span>
<span class="quote">&gt;&gt;  static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,</span>
<span class="quote">&gt;&gt;  				      volatile void *address)</span>
<span class="quote">&gt;&gt; @@ -622,7 +615,7 @@ map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
<span class="quote">&gt;&gt;  		return SWIOTLB_MAP_ERROR;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt; +	start_dma_addr = __phys_to_dma(hwdev, io_tlb_start);</span>
<span class="quote">&gt;&gt;  	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,</span>
<span class="quote">&gt;&gt;  				      dir, attrs);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -726,12 +719,12 @@ swiotlb_alloc_buffer(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;&gt;  		goto out_warn;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	phys_addr = swiotlb_tbl_map_single(dev,</span>
<span class="quote">&gt;&gt; -			swiotlb_phys_to_dma(dev, io_tlb_start),</span>
<span class="quote">&gt;&gt; +			__phys_to_dma(dev, io_tlb_start),</span>
<span class="quote">&gt;&gt;  			0, size, DMA_FROM_DEVICE, 0);</span>
<span class="quote">&gt;&gt;  	if (phys_addr == SWIOTLB_MAP_ERROR)</span>
<span class="quote">&gt;&gt;  		goto out_warn;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	*dma_handle = swiotlb_phys_to_dma(dev, phys_addr);</span>
<span class="quote">&gt;&gt; +	*dma_handle = __phys_to_dma(dev, phys_addr);</span>
<span class="quote">&gt;&gt;  	if (dma_coherent_ok(dev, *dma_handle, size))</span>
<span class="quote">&gt;&gt;  		goto out_unmap;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -867,10 +860,10 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;&gt;  	map = map_single(dev, phys, size, dir, attrs);</span>
<span class="quote">&gt;&gt;  	if (map == SWIOTLB_MAP_ERROR) {</span>
<span class="quote">&gt;&gt;  		swiotlb_full(dev, size, dir, 1);</span>
<span class="quote">&gt;&gt; -		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;&gt; +		return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
<span class="quote">&gt;&gt; +	dev_addr = __phys_to_dma(dev, map);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/* Ensure that the address returned is DMA&#39;ble */</span>
<span class="quote">&gt;&gt;  	if (dma_capable(dev, dev_addr, size))</span>
<span class="quote">&gt;&gt; @@ -879,7 +872,7 @@ dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;&gt;  	attrs |= DMA_ATTR_SKIP_CPU_SYNC;</span>
<span class="quote">&gt;&gt;  	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;&gt; +	return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; @@ -1009,7 +1002,7 @@ swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
<span class="quote">&gt;&gt;  				sg_dma_len(sgl) = 0;</span>
<span class="quote">&gt;&gt;  				return 0;</span>
<span class="quote">&gt;&gt;  			}</span>
<span class="quote">&gt;&gt; -			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
<span class="quote">&gt;&gt; +			sg-&gt;dma_address = __phys_to_dma(hwdev, map);</span>
<span class="quote">&gt;&gt;  		} else</span>
<span class="quote">&gt;&gt;  			sg-&gt;dma_address = dev_addr;</span>
<span class="quote">&gt;&gt;  		sg_dma_len(sg) = sg-&gt;length;</span>
<span class="quote">&gt;&gt; @@ -1073,7 +1066,7 @@ swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,</span>
<span class="quote">&gt;&gt;  int</span>
<span class="quote">&gt;&gt;  swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt;&gt; +	return (dma_addr == __phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; @@ -1085,7 +1078,7 @@ swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
<span class="quote">&gt;&gt;  int</span>
<span class="quote">&gt;&gt;  swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt;&gt; +	return __phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_DMA_DIRECT_OPS</span>
<span class="quote">&gt;&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - March 13, 2018, 1:10 p.m.</div>
<pre class="content">
On Mon, Mar 12, 2018 at 02:48:51PM -0500, Tom Lendacky wrote:
<span class="quote">&gt; Ok, I found one issue that allows this to work when the IOMMU isn&#39;t</span>
<span class="quote">&gt; enabled (see below).</span>

Thanks, folded!
<span class="quote">
&gt; But the bigger issue is when the IOMMU is enabled.  The IOMMU code uses</span>
<span class="quote">&gt; a common mapping routine to create the I/O page tables.  This routine</span>
<span class="quote">&gt; assumes that all memory being mapped is encrypted and therefore sets the</span>
<span class="quote">&gt; encryption bit in the I/O page tables.  With this patch, the call to</span>
<span class="quote">&gt; dma_alloc_direct() now returns un-encrypted memory which results in an</span>
<span class="quote">&gt; encryption mis-match.  I think keeping dma_alloc_direct() as it was prior</span>
<span class="quote">&gt; to this patch is the way to go.  It allows SME DMA allocations to remain</span>
<span class="quote">&gt; encrypted and avoids added complexity in the amd_iommu.c file.  This</span>
<span class="quote">&gt; would mean that SEV would still have special DMA operations (so that the</span>
<span class="quote">&gt; alloc/free can change the memory to un-encrypted).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What do you think?</span>

In terms of logic you are right.  I still don&#39;t like keeping a just
slightly tweaked version of dma_alloc_direct around just for this, it
will be perpetually out of sync in terms of features and bug fixes.

What do you think about this version that does the decision at runtime:

	http://git.infradead.org/users/hch/misc.git/commitdiff/b89f24dc856595dc7610d672bf077195ab0dabf4

The full tree is available here for testing:

	git://git.infradead.org/users/hch/misc.git dma-direct-x86
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/dma-direct.h b/arch/arm/include/asm/dma-direct.h</span>
<span class="p_header">index 5b0a8a421894..b67e5fc1fe43 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/dma-direct.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/dma-direct.h</span>
<span class="p_chunk">@@ -2,13 +2,13 @@</span> <span class="p_context"></span>
 #ifndef ASM_ARM_DMA_DIRECT_H
 #define ASM_ARM_DMA_DIRECT_H 1
 
<span class="p_del">-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	unsigned int offset = paddr &amp; ~PAGE_MASK;
 	return pfn_to_dma(dev, __phys_to_pfn(paddr)) + offset;
 }
 
<span class="p_del">-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="p_add">+static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
 {
 	unsigned int offset = dev_addr &amp; ~PAGE_MASK;
 	return __pfn_to_phys(dma_to_pfn(dev, dev_addr)) + offset;
<span class="p_header">diff --git a/arch/mips/cavium-octeon/dma-octeon.c b/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="p_header">index c7bb8a407041..7b335ab21697 100644</span>
<span class="p_header">--- a/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="p_header">+++ b/arch/mips/cavium-octeon/dma-octeon.c</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"></span>
  * IP32 changes by Ilya.
  * Copyright (C) 2010 Cavium Networks, Inc.
  */
<span class="p_del">-#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-direct.h&gt;</span>
 #include &lt;linux/scatterlist.h&gt;
 #include &lt;linux/bootmem.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> struct octeon_dma_map_ops {</span>
 	phys_addr_t (*dma_to_phys)(struct device *dev, dma_addr_t daddr);
 };
 
<span class="p_del">-dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	struct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),
 						      struct octeon_dma_map_ops,
<span class="p_chunk">@@ -190,9 +190,9 @@</span> <span class="p_context"> dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 
 	return ops-&gt;phys_to_dma(dev, paddr);
 }
<span class="p_del">-EXPORT_SYMBOL(phys_to_dma);</span>
<span class="p_add">+EXPORT_SYMBOL(__phys_to_dma);</span>
 
<span class="p_del">-phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_add">+phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
 {
 	struct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),
 						      struct octeon_dma_map_ops,
<span class="p_chunk">@@ -200,7 +200,7 @@</span> <span class="p_context"> phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
 
 	return ops-&gt;dma_to_phys(dev, daddr);
 }
<span class="p_del">-EXPORT_SYMBOL(dma_to_phys);</span>
<span class="p_add">+EXPORT_SYMBOL(__dma_to_phys);</span>
 
 static struct octeon_dma_map_ops octeon_linear_dma_map_ops = {
 	.dma_map_ops = {
<span class="p_header">diff --git a/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h b/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="p_header">index 138edf6b5b48..6eb1ee548b11 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/mach-cavium-octeon/dma-coherence.h</span>
<span class="p_chunk">@@ -69,8 +69,8 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 	return addr + size - 1 &lt;= *dev-&gt;dma_mask;
 }
 
<span class="p_del">-dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_del">-phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="p_add">+dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_add">+phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
 
 struct dma_map_ops;
 extern const struct dma_map_ops *octeon_pci_dma_map_ops;
<span class="p_header">diff --git a/arch/mips/include/asm/mach-loongson64/dma-coherence.h b/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="p_header">index b1b575f5c6c1..64fc44dec0a8 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/mach-loongson64/dma-coherence.h</span>
<span class="p_chunk">@@ -25,13 +25,13 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 	return addr + size - 1 &lt;= *dev-&gt;dma_mask;
 }
 
<span class="p_del">-extern dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_del">-extern phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="p_add">+extern dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_add">+extern phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
 static inline dma_addr_t plat_map_dma_mem(struct device *dev, void *addr,
 					  size_t size)
 {
 #ifdef CONFIG_CPU_LOONGSON3
<span class="p_del">-	return phys_to_dma(dev, virt_to_phys(addr));</span>
<span class="p_add">+	return __phys_to_dma(dev, virt_to_phys(addr));</span>
 #else
 	return virt_to_phys(addr) | 0x80000000;
 #endif
<span class="p_chunk">@@ -41,7 +41,7 @@</span> <span class="p_context"> static inline dma_addr_t plat_map_dma_mem_page(struct device *dev,</span>
 					       struct page *page)
 {
 #ifdef CONFIG_CPU_LOONGSON3
<span class="p_del">-	return phys_to_dma(dev, page_to_phys(page));</span>
<span class="p_add">+	return __phys_to_dma(dev, page_to_phys(page));</span>
 #else
 	return page_to_phys(page) | 0x80000000;
 #endif
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> static inline unsigned long plat_dma_addr_to_phys(struct device *dev,</span>
 	dma_addr_t dma_addr)
 {
 #if defined(CONFIG_CPU_LOONGSON3) &amp;&amp; defined(CONFIG_64BIT)
<span class="p_del">-	return dma_to_phys(dev, dma_addr);</span>
<span class="p_add">+	return __dma_to_phys(dev, dma_addr);</span>
 #elif defined(CONFIG_CPU_LOONGSON2F) &amp;&amp; defined(CONFIG_64BIT)
 	return (dma_addr &gt; 0x8fffffff) ? dma_addr : (dma_addr &amp; 0x0fffffff);
 #else
<span class="p_header">diff --git a/arch/mips/loongson64/common/dma-swiotlb.c b/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="p_header">index 7bbcf89475f3..6a739f8ae110 100644</span>
<span class="p_header">--- a/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="p_header">+++ b/arch/mips/loongson64/common/dma-swiotlb.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> static int loongson_dma_supported(struct device *dev, u64 mask)</span>
 	return swiotlb_dma_supported(dev, mask);
 }
 
<span class="p_del">-dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	long nid;
 #ifdef CONFIG_PHYS48_TO_HT40
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 	return paddr;
 }
 
<span class="p_del">-phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_add">+phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
 {
 	long nid;
 #ifdef CONFIG_PHYS48_TO_HT40
<span class="p_header">diff --git a/arch/powerpc/include/asm/dma-direct.h b/arch/powerpc/include/asm/dma-direct.h</span>
<span class="p_header">index a5b59c765426..7702875aabb7 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/dma-direct.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/dma-direct.h</span>
<span class="p_chunk">@@ -17,12 +17,12 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 	return addr + size - 1 &lt;= *dev-&gt;dma_mask;
 }
 
<span class="p_del">-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	return paddr + get_dma_offset(dev);
 }
 
<span class="p_del">-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_add">+static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
 {
 	return daddr - get_dma_offset(dev);
 }
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 7272bb3768d7..6e25ca4c86ee 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -54,7 +54,6 @@</span> <span class="p_context"> config X86</span>
 	select ARCH_HAS_FORTIFY_SOURCE
 	select ARCH_HAS_GCOV_PROFILE_ALL
 	select ARCH_HAS_KCOV			if X86_64
<span class="p_del">-	select ARCH_HAS_PHYS_TO_DMA</span>
 	select ARCH_HAS_MEMBARRIER_SYNC_CORE
 	select ARCH_HAS_PMEM_API		if X86_64
 	select ARCH_HAS_REFCOUNT
<span class="p_chunk">@@ -691,6 +690,7 @@</span> <span class="p_context"> config X86_SUPPORTS_MEMORY_FAILURE</span>
 config STA2X11
 	bool &quot;STA2X11 Companion Chip Support&quot;
 	depends on X86_32_NON_STANDARD &amp;&amp; PCI
<span class="p_add">+	select ARCH_HAS_PHYS_TO_DMA</span>
 	select X86_DEV_DMA_OPS
 	select X86_DMA_REMAP
 	select SWIOTLB
<span class="p_header">diff --git a/arch/x86/include/asm/dma-direct.h b/arch/x86/include/asm/dma-direct.h</span>
<span class="p_header">index 1295bc622ebe..1a19251eaac9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/dma-direct.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/dma-direct.h</span>
<span class="p_chunk">@@ -2,29 +2,8 @@</span> <span class="p_context"></span>
 #ifndef ASM_X86_DMA_DIRECT_H
 #define ASM_X86_DMA_DIRECT_H 1
 
<span class="p_del">-#include &lt;linux/mem_encrypt.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_DMA_REMAP /* Platform code defines bridge-specific code */</span>
 bool dma_capable(struct device *dev, dma_addr_t addr, size_t size);
<span class="p_del">-dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_del">-phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
<span class="p_del">-#else</span>
<span class="p_del">-static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!dev-&gt;dma_mask)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __sme_set(paddr);</span>
<span class="p_del">-}</span>
<span class="p_add">+dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr);</span>
<span class="p_add">+phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr);</span>
 
<span class="p_del">-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __sme_clr(daddr);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_X86_DMA_REMAP */</span>
 #endif /* ASM_X86_DMA_DIRECT_H */
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 66beedc8fe3d..8bfc735bbdd7 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -200,58 +200,6 @@</span> <span class="p_context"> void __init sme_early_init(void)</span>
 		swiotlb_force = SWIOTLB_FORCE;
 }
 
<span class="p_del">-static void *sev_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="p_del">-		       gfp_t gfp, unsigned long attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned int order;</span>
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	void *vaddr = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	order = get_order(size);</span>
<span class="p_del">-	page = alloc_pages_node(dev_to_node(dev), gfp, order);</span>
<span class="p_del">-	if (page) {</span>
<span class="p_del">-		dma_addr_t addr;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Since we will be clearing the encryption bit, check the</span>
<span class="p_del">-		 * mask with it already cleared.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		addr = __sme_clr(phys_to_dma(dev, page_to_phys(page)));</span>
<span class="p_del">-		if ((addr + size) &gt; dev-&gt;coherent_dma_mask) {</span>
<span class="p_del">-			__free_pages(page, get_order(size));</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			vaddr = page_address(page);</span>
<span class="p_del">-			*dma_handle = addr;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!vaddr)</span>
<span class="p_del">-		vaddr = swiotlb_alloc_coherent(dev, size, dma_handle, gfp);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!vaddr)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Clear the SME encryption bit for DMA use if not swiotlb area */</span>
<span class="p_del">-	if (!is_swiotlb_buffer(dma_to_phys(dev, *dma_handle))) {</span>
<span class="p_del">-		set_memory_decrypted((unsigned long)vaddr, 1 &lt;&lt; order);</span>
<span class="p_del">-		memset(vaddr, 0, PAGE_SIZE &lt;&lt; order);</span>
<span class="p_del">-		*dma_handle = __sme_clr(*dma_handle);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return vaddr;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void sev_free(struct device *dev, size_t size, void *vaddr,</span>
<span class="p_del">-		     dma_addr_t dma_handle, unsigned long attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Set the SME encryption bit for re-use if not swiotlb area */</span>
<span class="p_del">-	if (!is_swiotlb_buffer(dma_to_phys(dev, dma_handle)))</span>
<span class="p_del">-		set_memory_encrypted((unsigned long)vaddr,</span>
<span class="p_del">-				     1 &lt;&lt; get_order(size));</span>
<span class="p_del">-</span>
<span class="p_del">-	swiotlb_free_coherent(dev, size, vaddr, dma_handle);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)
 {
 	pgprot_t old_prot, new_prot;
<span class="p_chunk">@@ -404,20 +352,6 @@</span> <span class="p_context"> bool sev_active(void)</span>
 }
 EXPORT_SYMBOL(sev_active);
 
<span class="p_del">-static const struct dma_map_ops sev_dma_ops = {</span>
<span class="p_del">-	.alloc                  = sev_alloc,</span>
<span class="p_del">-	.free                   = sev_free,</span>
<span class="p_del">-	.map_page               = swiotlb_map_page,</span>
<span class="p_del">-	.unmap_page             = swiotlb_unmap_page,</span>
<span class="p_del">-	.map_sg                 = swiotlb_map_sg_attrs,</span>
<span class="p_del">-	.unmap_sg               = swiotlb_unmap_sg_attrs,</span>
<span class="p_del">-	.sync_single_for_cpu    = swiotlb_sync_single_for_cpu,</span>
<span class="p_del">-	.sync_single_for_device = swiotlb_sync_single_for_device,</span>
<span class="p_del">-	.sync_sg_for_cpu        = swiotlb_sync_sg_for_cpu,</span>
<span class="p_del">-	.sync_sg_for_device     = swiotlb_sync_sg_for_device,</span>
<span class="p_del">-	.mapping_error          = swiotlb_dma_mapping_error,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 /* Architecture __weak replacement functions */
 void __init mem_encrypt_init(void)
 {
<span class="p_chunk">@@ -428,12 +362,11 @@</span> <span class="p_context"> void __init mem_encrypt_init(void)</span>
 	swiotlb_update_mem_attributes();
 
 	/*
<span class="p_del">-	 * With SEV, DMA operations cannot use encryption. New DMA ops</span>
<span class="p_del">-	 * are required in order to mark the DMA areas as decrypted or</span>
<span class="p_del">-	 * to use bounce buffers.</span>
<span class="p_add">+	 * With SEV, DMA operations cannot use encryption, we need to use</span>
<span class="p_add">+	 * SWIOTLB to bounce buffer DMA operation.</span>
 	 */
 	if (sev_active())
<span class="p_del">-		dma_ops = &amp;sev_dma_ops;</span>
<span class="p_add">+		dma_ops = &amp;swiotlb_dma_ops;</span>
 
 	/*
 	 * With SEV, we need to unroll the rep string I/O instructions.
<span class="p_header">diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">index eac58e03f43c..7a5bafb76d77 100644</span>
<span class="p_header">--- a/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">+++ b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_chunk">@@ -207,11 +207,11 @@</span> <span class="p_context"> bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 }
 
 /**
<span class="p_del">- * phys_to_dma - Return the DMA AMBA address used for this STA2x11 device</span>
<span class="p_add">+ * __phys_to_dma - Return the DMA AMBA address used for this STA2x11 device</span>
  * @dev: device for a PCI device
  * @paddr: Physical address
  */
<span class="p_del">-dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	if (!dev-&gt;archdata.is_sta2x11)
 		return paddr;
<span class="p_chunk">@@ -223,7 +223,7 @@</span> <span class="p_context"> dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
  * @dev: device for a PCI device
  * @daddr: STA2x11 AMBA DMA address
  */
<span class="p_del">-phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_add">+phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
 {
 	if (!dev-&gt;archdata.is_sta2x11)
 		return daddr;
<span class="p_header">diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h</span>
<span class="p_header">index bcdb1a3e4b1f..53ad6a47f513 100644</span>
<span class="p_header">--- a/include/linux/dma-direct.h</span>
<span class="p_header">+++ b/include/linux/dma-direct.h</span>
<span class="p_chunk">@@ -3,18 +3,19 @@</span> <span class="p_context"></span>
 #define _LINUX_DMA_DIRECT_H 1
 
 #include &lt;linux/dma-mapping.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 #ifdef CONFIG_ARCH_HAS_PHYS_TO_DMA
 #include &lt;asm/dma-direct.h&gt;
 #else
<span class="p_del">-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+static inline dma_addr_t __phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
 {
 	dma_addr_t dev_addr = (dma_addr_t)paddr;
 
 	return dev_addr - ((dma_addr_t)dev-&gt;dma_pfn_offset &lt;&lt; PAGE_SHIFT);
 }
 
<span class="p_del">-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="p_add">+static inline phys_addr_t __dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
 {
 	phys_addr_t paddr = (phys_addr_t)dev_addr;
 
<span class="p_chunk">@@ -30,6 +31,22 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 }
 #endif /* !CONFIG_ARCH_HAS_PHYS_TO_DMA */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If memory encryption is supported, phys_to_dma will set the memory encryption</span>
<span class="p_add">+ * bit in the DMA address, and dma_to_phys will clear it.  The raw __phys_to_dma</span>
<span class="p_add">+ * and __dma_to_phys versions should only be used on non-encrypted memory for</span>
<span class="p_add">+ * special occasions like DMA coherent buffers.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __sme_set(__phys_to_dma(dev, paddr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __sme_clr(__dma_to_phys(dev, daddr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_ARCH_HAS_DMA_MARK_CLEAN
 void dma_mark_clean(void *addr, size_t size);
 #else
<span class="p_header">diff --git a/lib/dma-direct.c b/lib/dma-direct.c</span>
<span class="p_header">index c9e8e21cb334..84f50b5982fc 100644</span>
<span class="p_header">--- a/lib/dma-direct.c</span>
<span class="p_header">+++ b/lib/dma-direct.c</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/scatterlist.h&gt;
 #include &lt;linux/dma-contiguous.h&gt;
 #include &lt;linux/pfn.h&gt;
<span class="p_add">+#include &lt;linux/set_memory.h&gt;</span>
 
 #define DIRECT_MAPPING_ERROR		0
 
<span class="p_chunk">@@ -35,9 +36,13 @@</span> <span class="p_context"> check_addr(struct device *dev, dma_addr_t dma_addr, size_t size,</span>
 	return true;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Since we will be clearing the encryption bit, check the mask with it already</span>
<span class="p_add">+ * cleared.</span>
<span class="p_add">+ */</span>
 static bool dma_coherent_ok(struct device *dev, phys_addr_t phys, size_t size)
 {
<span class="p_del">-	return phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
<span class="p_add">+	return __phys_to_dma(dev, phys) + size - 1 &lt;= dev-&gt;coherent_dma_mask;</span>
 }
 
 void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
<span class="p_chunk">@@ -46,6 +51,7 @@</span> <span class="p_context"> void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
 	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;
 	int page_order = get_order(size);
 	struct page *page = NULL;
<span class="p_add">+	void *ret;</span>
 
 	/* GFP_DMA32 and GFP_DMA are no ops without the corresponding zones: */
 	if (dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(ARCH_ZONE_DMA_BITS))
<span class="p_chunk">@@ -78,10 +84,11 @@</span> <span class="p_context"> void *dma_direct_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
 
 	if (!page)
 		return NULL;
<span class="p_del">-</span>
<span class="p_del">-	*dma_handle = phys_to_dma(dev, page_to_phys(page));</span>
<span class="p_del">-	memset(page_address(page), 0, size);</span>
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	*dma_handle = __phys_to_dma(dev, page_to_phys(page));</span>
<span class="p_add">+	ret = page_address(page);</span>
<span class="p_add">+	set_memory_decrypted((unsigned long)ret, page_order);</span>
<span class="p_add">+	memset(ret, 0, size);</span>
<span class="p_add">+	return ret;</span>
 }
 
 /*
<span class="p_chunk">@@ -92,9 +99,11 @@</span> <span class="p_context"> void dma_direct_free(struct device *dev, size_t size, void *cpu_addr,</span>
 		dma_addr_t dma_addr, unsigned long attrs)
 {
 	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;
<span class="p_add">+	unsigned int page_order = get_order(size);</span>
 
<span class="p_add">+	set_memory_encrypted((unsigned long)cpu_addr, 1 &lt;&lt; page_order);</span>
 	if (!dma_release_from_contiguous(dev, virt_to_page(cpu_addr), count))
<span class="p_del">-		free_pages((unsigned long)cpu_addr, get_order(size));</span>
<span class="p_add">+		free_pages((unsigned long)cpu_addr, page_order);</span>
 }
 
 static dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index c43ec2271469..ca8eeaead925 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -158,13 +158,6 @@</span> <span class="p_context"> unsigned long swiotlb_size_or_default(void)</span>
 
 void __weak swiotlb_set_mem_attributes(void *vaddr, unsigned long size) { }
 
<span class="p_del">-/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="p_del">-static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="p_del">-				      phys_addr_t address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __sme_clr(phys_to_dma(hwdev, address));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /* Note that this doesn&#39;t work with highmem page */
 static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,
 				      volatile void *address)
<span class="p_chunk">@@ -622,7 +615,7 @@</span> <span class="p_context"> map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
 		return SWIOTLB_MAP_ERROR;
 	}
 
<span class="p_del">-	start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
<span class="p_add">+	start_dma_addr = __phys_to_dma(hwdev, io_tlb_start);</span>
 	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,
 				      dir, attrs);
 }
<span class="p_chunk">@@ -726,12 +719,12 @@</span> <span class="p_context"> swiotlb_alloc_buffer(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
 		goto out_warn;
 
 	phys_addr = swiotlb_tbl_map_single(dev,
<span class="p_del">-			swiotlb_phys_to_dma(dev, io_tlb_start),</span>
<span class="p_add">+			__phys_to_dma(dev, io_tlb_start),</span>
 			0, size, DMA_FROM_DEVICE, 0);
 	if (phys_addr == SWIOTLB_MAP_ERROR)
 		goto out_warn;
 
<span class="p_del">-	*dma_handle = swiotlb_phys_to_dma(dev, phys_addr);</span>
<span class="p_add">+	*dma_handle = __phys_to_dma(dev, phys_addr);</span>
 	if (dma_coherent_ok(dev, *dma_handle, size))
 		goto out_unmap;
 
<span class="p_chunk">@@ -867,10 +860,10 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 	map = map_single(dev, phys, size, dir, attrs);
 	if (map == SWIOTLB_MAP_ERROR) {
 		swiotlb_full(dev, size, dir, 1);
<span class="p_del">-		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+		return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 	}
 
<span class="p_del">-	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
<span class="p_add">+	dev_addr = __phys_to_dma(dev, map);</span>
 
 	/* Ensure that the address returned is DMA&#39;ble */
 	if (dma_capable(dev, dev_addr, size))
<span class="p_chunk">@@ -879,7 +872,7 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 	attrs |= DMA_ATTR_SKIP_CPU_SYNC;
 	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);
 
<span class="p_del">-	return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+	return __phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 }
 
 /*
<span class="p_chunk">@@ -1009,7 +1002,7 @@</span> <span class="p_context"> swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
 				sg_dma_len(sgl) = 0;
 				return 0;
 			}
<span class="p_del">-			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
<span class="p_add">+			sg-&gt;dma_address = __phys_to_dma(hwdev, map);</span>
 		} else
 			sg-&gt;dma_address = dev_addr;
 		sg_dma_len(sg) = sg-&gt;length;
<span class="p_chunk">@@ -1073,7 +1066,7 @@</span> <span class="p_context"> swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,</span>
 int
 swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)
 {
<span class="p_del">-	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="p_add">+	return (dma_addr == __phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
 }
 
 /*
<span class="p_chunk">@@ -1085,7 +1078,7 @@</span> <span class="p_context"> swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)</span>
 int
 swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
<span class="p_del">-	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="p_add">+	return __phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
 }
 
 #ifdef CONFIG_DMA_DIRECT_OPS

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



