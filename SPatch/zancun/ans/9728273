
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v1,10/11] mm/kasan: support dynamic shadow memory free - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v1,10/11] mm/kasan: support dynamic shadow memory free</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=39721">Joonsoo Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 16, 2017, 1:16 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1494897409-14408-11-git-send-email-iamjoonsoo.kim@lge.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9728273/mbox/"
   >mbox</a>
|
   <a href="/patch/9728273/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9728273/">/patch/9728273/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9C91060380 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 01:18:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 81D7F289E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 01:18:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 73E2C289EB; Tue, 16 May 2017 01:18:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AA0F5289E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 01:18:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751529AbdEPBSp (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 15 May 2017 21:18:45 -0400
Received: from mail-pf0-f196.google.com ([209.85.192.196]:32809 &quot;EHLO
	mail-pf0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751408AbdEPBSO (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 15 May 2017 21:18:14 -0400
Received: by mail-pf0-f196.google.com with SMTP id f27so7515372pfe.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 15 May 2017 18:18:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=x6GGl2oT8UKtltStomx4ri1cTPzcwKViPryiXtH0II4=;
	b=efd0kUdL87tBsuXxWHaLv6MGOgpQDqIvY1iY5ShLV3hh28W8pxYP2blObQp4SVHHUl
	0K9YluW9yoQZCPCufObuKaYVn1OZCDqEBeCrnK8bAwVlVYTDrcLlETcdyiZPbh//Zt+C
	41FAxtCI9EwjRYs0it5Pf3Yv3Oo/me3lfoX6ThCPgVZuM9/mwEx0mO61BJdDvmQNnk/i
	gercGc6eTkUgnyBfhHHGZo2TsCQoucuz9IHnniEaouD2QRB9qTNdIvEYidprfLTNPb29
	YSuzw1J3Eq7IeDWuRZ498CkwsA3hGjWfKlXvmuKeiUg2Ehp0PSGGtYezT86lKzq9O5B1
	eA5A==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=x6GGl2oT8UKtltStomx4ri1cTPzcwKViPryiXtH0II4=;
	b=X5oRkkQVR+LlXnojT2Q69xzRRuCVJ0pG8rFaf9AJC8wU8BOsro3uVKfepSqQQ4vis2
	8cCZWWTDYvSXGiBXoZ6ofrE+vNnsiiDuDynmLsNzk71GWVnnhcX70IRmcEfUBto+Fah1
	udfkUd/qtIFWJR/Wo8qVcDBqUyq55qbM98XA4lMBecFSsoM3kHB5y6PCmtbMG6vgus5S
	azdeghq+tEOQw+pL26u9ZpQSGl8LaAfyMc9G9dd/zzUIQRlptIKNyEdvvw4ku4PHWsFG
	988GCUS/Vxv+ECPJceUfGdRqvb7+VdrhiR8nqt6ilJIGk2y7lF6FaGq0lvHsKMHcK8GV
	iySQ==
X-Gm-Message-State: AODbwcB+ZP7qFj/odDh9kDGT5L6SkkPHoFXKWFjzkw5oLlr3QDVLKhg7
	0zhekPNDbujsiw==
X-Received: by 10.84.136.131 with SMTP id 3mr12240505pll.181.1494897494036; 
	Mon, 15 May 2017 18:18:14 -0700 (PDT)
Received: from localhost.localdomain ([124.56.155.17])
	by smtp.gmail.com with ESMTPSA id
	c7sm25648186pfk.103.2017.05.15.18.18.09
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Mon, 15 May 2017 18:18:12 -0700 (PDT)
From: js1304@gmail.com
X-Google-Original-From: iamjoonsoo.kim@lge.com
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;,
	Alexander Potapenko &lt;glider@google.com&gt;,
	Dmitry Vyukov &lt;dvyukov@google.com&gt;, kasan-dev@googlegroups.com,
	linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, kernel-team@lge.com,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;
Subject: [PATCH v1 10/11] mm/kasan: support dynamic shadow memory free
Date: Tue, 16 May 2017 10:16:48 +0900
Message-Id: &lt;1494897409-14408-11-git-send-email-iamjoonsoo.kim@lge.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1494897409-14408-1-git-send-email-iamjoonsoo.kim@lge.com&gt;
References: &lt;1494897409-14408-1-git-send-email-iamjoonsoo.kim@lge.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=39721">Joonsoo Kim</a> - May 16, 2017, 1:16 a.m.</div>
<pre class="content">
<span class="from">From: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;</span>

On-demand alloc/map the shadow memory isn&#39;t sufficient to save
memory consumption since shadow memory would be populated
for all the memory range in the long running system. This patch
implements dynamic shadow memory unmap/free to solve this problem.

Since shadow memory is populated in order-3 page unit, we can also
unmap/free in order-3 page unit. Therefore, this patch inserts
a hook in buddy allocator to detect free of order-3 page.

Note that unmapping need to flush TLBs in all cpus so actual
unmap/free is delegate to the workqueue.
<span class="signed-off-by">
Signed-off-by: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;</span>
---
 include/linux/kasan.h |   4 ++
 mm/kasan/kasan.c      | 134 ++++++++++++++++++++++++++++++++++++++++++++++++++
 mm/page_alloc.c       |  10 ++++
 3 files changed, 148 insertions(+)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/kasan.h b/include/linux/kasan.h</span>
<span class="p_header">index c8ef665..9e44cf6 100644</span>
<span class="p_header">--- a/include/linux/kasan.h</span>
<span class="p_header">+++ b/include/linux/kasan.h</span>
<span class="p_chunk">@@ -87,6 +87,8 @@</span> <span class="p_context"> int kasan_stack_alloc(const void *address, size_t size);</span>
 void kasan_stack_free(const void *addr, size_t size);
 int kasan_slab_page_alloc(const void *address, size_t size, gfp_t flags);
 void kasan_slab_page_free(const void *addr, size_t size);
<span class="p_add">+bool kasan_free_buddy(struct page *page, unsigned int order,</span>
<span class="p_add">+			unsigned int max_order);</span>
 
 void kasan_unpoison_task_stack(struct task_struct *task);
 void kasan_unpoison_stack_above_sp_to(const void *watermark);
<span class="p_chunk">@@ -140,6 +142,8 @@</span> <span class="p_context"> static inline void kasan_stack_free(const void *addr, size_t size) {}</span>
 static inline int kasan_slab_page_alloc(const void *address, size_t size,
 					gfp_t flags) { return 0; }
 static inline void kasan_slab_page_free(const void *addr, size_t size) {}
<span class="p_add">+static inline bool kasan_free_buddy(struct page *page, unsigned int order,</span>
<span class="p_add">+			unsigned int max_order) { return false; }</span>
 
 static inline void kasan_unpoison_task_stack(struct task_struct *task) {}
 static inline void kasan_unpoison_stack_above_sp_to(const void *watermark) {}
<span class="p_header">diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c</span>
<span class="p_header">index 8d59cf0..e5612be 100644</span>
<span class="p_header">--- a/mm/kasan/kasan.c</span>
<span class="p_header">+++ b/mm/kasan/kasan.c</span>
<span class="p_chunk">@@ -36,13 +36,19 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/bug.h&gt;
<span class="p_add">+#include &lt;linux/page-isolation.h&gt;</span>
 #include &lt;asm/cacheflush.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/sections.h&gt;</span>
 
 #include &quot;kasan.h&quot;
 #include &quot;../slab.h&quot;
 #include &quot;../internal.h&quot;
 
 static DEFINE_SPINLOCK(shadow_lock);
<span class="p_add">+static LIST_HEAD(unmap_list);</span>
<span class="p_add">+static void kasan_unmap_shadow_workfn(struct work_struct *work);</span>
<span class="p_add">+static DECLARE_WORK(kasan_unmap_shadow_work, kasan_unmap_shadow_workfn);</span>
 
 void kasan_enable_current(void)
 {
<span class="p_chunk">@@ -241,6 +247,125 @@</span> <span class="p_context"> static int kasan_map_shadow(const void *addr, size_t size, gfp_t flags)</span>
 	return err;
 }
 
<span class="p_add">+static int kasan_unmap_shadow_pte(pte_t *ptep, pgtable_t token,</span>
<span class="p_add">+			unsigned long addr, void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct list_head *list = data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kasan_black_shadow(ptep))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (addr &gt;= (unsigned long)_text &amp;&amp; addr &lt; (unsigned long)_end)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = *ptep;</span>
<span class="p_add">+	page = pfn_to_page(pte_pfn(pte));</span>
<span class="p_add">+	list_add(&amp;page-&gt;lru, list);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pfn_pte(PFN_DOWN(__pa(kasan_black_page)), PAGE_KERNEL);</span>
<span class="p_add">+	pte = pte_wrprotect(pte);</span>
<span class="p_add">+	set_pte_at(&amp;init_mm, addr, ptep, pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void kasan_unmap_shadow_workfn(struct work_struct *work)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page, *next;</span>
<span class="p_add">+	LIST_HEAD(list);</span>
<span class="p_add">+	LIST_HEAD(shadow_list);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned int order;</span>
<span class="p_add">+	unsigned long shadow_addr, shadow_size;</span>
<span class="p_add">+	unsigned long tlb_start = ULONG_MAX, tlb_end = 0;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;shadow_lock, flags);</span>
<span class="p_add">+	list_splice_init(&amp;unmap_list, &amp;list);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;shadow_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (list_empty(&amp;list))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(page, next, &amp;list, lru) {</span>
<span class="p_add">+		order = page_private(page);</span>
<span class="p_add">+		post_alloc_hook(page, order, GFP_NOWAIT);</span>
<span class="p_add">+		set_page_private(page, order);</span>
<span class="p_add">+</span>
<span class="p_add">+		shadow_addr = (unsigned long)kasan_mem_to_shadow(</span>
<span class="p_add">+						page_address(page));</span>
<span class="p_add">+		shadow_size = PAGE_SIZE &lt;&lt; (order - KASAN_SHADOW_SCALE_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+		tlb_start = min(shadow_addr, tlb_start);</span>
<span class="p_add">+		tlb_end = max(shadow_addr + shadow_size, tlb_end);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_cache_vunmap(shadow_addr, shadow_addr + shadow_size);</span>
<span class="p_add">+		err = apply_to_page_range(&amp;init_mm, shadow_addr, shadow_size,</span>
<span class="p_add">+				kasan_unmap_shadow_pte, &amp;shadow_list);</span>
<span class="p_add">+		if (err) {</span>
<span class="p_add">+			pr_err(&quot;invalid shadow entry is found&quot;);</span>
<span class="p_add">+			list_del(&amp;page-&gt;lru);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	flush_tlb_kernel_range(tlb_start, tlb_end);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(page, next, &amp;list, lru) {</span>
<span class="p_add">+		list_del(&amp;page-&gt;lru);</span>
<span class="p_add">+		__free_pages(page, page_private(page));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	list_for_each_entry_safe(page, next, &amp;shadow_list, lru) {</span>
<span class="p_add">+		list_del(&amp;page-&gt;lru);</span>
<span class="p_add">+		__free_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool kasan_unmap_shadow(struct page *page, unsigned int order,</span>
<span class="p_add">+			unsigned int max_order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+	unsigned long shadow_addr, shadow_size;</span>
<span class="p_add">+	unsigned long count = 0;</span>
<span class="p_add">+	LIST_HEAD(list);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	int mt;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (order &lt; KASAN_SHADOW_SCALE_SHIFT)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (max_order != (KASAN_SHADOW_SCALE_SHIFT + 1))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	shadow_addr = (unsigned long)kasan_mem_to_shadow(page_address(page));</span>
<span class="p_add">+	shadow_size = PAGE_SIZE &lt;&lt; (order - KASAN_SHADOW_SCALE_SHIFT);</span>
<span class="p_add">+	err = apply_to_page_range(&amp;init_mm, shadow_addr, shadow_size,</span>
<span class="p_add">+				kasan_exist_shadow_pte, &amp;count);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		pr_err(&quot;checking shadow entry is failed&quot;);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!count)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	zone = page_zone(page);</span>
<span class="p_add">+	mt = get_pageblock_migratetype(page);</span>
<span class="p_add">+	if (!is_migrate_isolate(mt))</span>
<span class="p_add">+		__mod_zone_freepage_state(zone, -(1UL &lt;&lt; order), mt);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_page_private(page, order);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;shadow_lock, flags);</span>
<span class="p_add">+	list_add(&amp;page-&gt;lru, &amp;unmap_list);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;shadow_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	schedule_work(&amp;kasan_unmap_shadow_work);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * All functions below always inlined so compiler could
  * perform better optimizations in each of __asan_loadX/__assn_storeX
<span class="p_chunk">@@ -601,6 +726,15 @@</span> <span class="p_context"> void kasan_free_pages(struct page *page, unsigned int order)</span>
 	}
 }
 
<span class="p_add">+bool kasan_free_buddy(struct page *page, unsigned int order,</span>
<span class="p_add">+			unsigned int max_order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kasan_pshadow_inited())</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return kasan_unmap_shadow(page, order, max_order);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
  * For larger allocations larger redzones are used.
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 3b175c3..4a6f722 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -797,6 +797,12 @@</span> <span class="p_context"> static inline void __free_one_page(struct page *page,</span>
 
 	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);
 
<span class="p_add">+#ifdef CONFIG_KASAN</span>
<span class="p_add">+	/* Suppress merging at initial attempt to unmap shadow memory */</span>
<span class="p_add">+	max_order = min_t(unsigned int,</span>
<span class="p_add">+			KASAN_SHADOW_SCALE_SHIFT + 1, max_order);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	VM_BUG_ON(!zone_is_initialized(zone));
 	VM_BUG_ON_PAGE(page-&gt;flags &amp; PAGE_FLAGS_CHECK_AT_PREP, page);
 
<span class="p_chunk">@@ -832,6 +838,10 @@</span> <span class="p_context"> static inline void __free_one_page(struct page *page,</span>
 		pfn = combined_pfn;
 		order++;
 	}
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(kasan_free_buddy(page, order, max_order)))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	if (max_order &lt; MAX_ORDER) {
 		/* If we are here, it means order is &gt;= pageblock_order.
 		 * We want to prevent merge between freepages on isolate

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



