
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,05/11] mm: thp: enable thp migration in generic path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,05/11] mm: thp: enable thp migration in generic path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 20, 2017, 8:47 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170420204752.79703-6-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9691299/mbox/"
   >mbox</a>
|
   <a href="/patch/9691299/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9691299/">/patch/9691299/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	94320601D4 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 21:00:24 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 82CB226B41
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 21:00:24 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 76E0626E47; Thu, 20 Apr 2017 21:00:24 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 426A326B41
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 21:00:23 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1033669AbdDTVAN (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 20 Apr 2017 17:00:13 -0400
Received: from out3-smtp.messagingengine.com ([66.111.4.27]:33059 &quot;EHLO
	out3-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S947556AbdDTVAJ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 20 Apr 2017 17:00:09 -0400
X-Greylist: delayed 731 seconds by postgrey-1.27 at vger.kernel.org;
	Thu, 20 Apr 2017 17:00:09 EDT
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by mailout.nyi.internal (Postfix) with ESMTP id 2ED1522057;
	Thu, 20 Apr 2017 16:47:58 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute3.internal (MEProxy); Thu, 20 Apr 2017 16:47:58 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-me-sender:x-me-sender:x-sasl-enc:x-sasl-enc; s=fm1; bh=PmB8Pt
	vL7UR4JAkmVNYa2hzK92nAa8/I6QmyBNKMIsc=; b=rOG7hXY0R2HjNU+oY9eCAO
	pnyHVFmAbybEhEvZ1wVJKTnQKKrurATRlOIvYnbbx4PDrrWyNM8fnE4ZpOw7FCvN
	t1YZFGSV6dsW9VByspPR46FoZLAx9C7jnwj5NwEqcenIvK9zmEYZyPyMPszLAgry
	YM+NJ86AshcyI69Ao13QVxDhu+WLb4tNC/Ok1Beddiu8y/T4wivRWteNA6/4FASQ
	pKlSChBaC9xMJNZsTBx/RoDEvXCJIzpZgKjawXDfedNE1QbWcmNCuHdqEx0rmARN
	L1IMu1j1shMUWUjlyAPxkYRB+g9t2R9Fh/yvDevwHNu3m+Utaab+rhQo7PYzCbhg
	==
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-me-sender:x-me-sender:x-sasl-enc
	:x-sasl-enc; s=fm1; bh=PmB8PtvL7UR4JAkmVNYa2hzK92nAa8/I6QmyBNKMI
	sc=; b=jgUxKgLYHwI4XG7KUG0prGQbZiPdIOPZ7FhAl8ah1dZOGj2baWTUx0o+M
	5CERCxN5pYM11sijNCjrYQSusO08Zmg2pN/bb0kBWmZp3dOBx6RqlBAl4slchEJi
	swilOFe6aI/iew/PpFwV+8kbPjx1massVJq9YBZn18t7OZtVFSj16XSB7iDmrLhe
	i2Fs8uZlt/7ZYRrwsv7KGhXKD+uAjv0XC2/n/1kisooi32GOVJXi8EHOJsCrZxF6
	EOc7zMouJ/lleZENOuoR2qYuFLCPnxTiW8GPNP3jkTUSuOsAgCzV9lgVN39SFqFR
	7w0kz38+pj30UN2q201M6tkqvB7JQ==
X-ME-Sender: &lt;xms:fh75WDIn3g9vB__1NTqyS9-8ilrP35bqzYApoL6RyYgP9H0EKGvfjQ&gt;
X-Sasl-enc: p8r8q8jtuMsSkYgZRkVPfbq0QHGIBS85jZrWBS3wW2C3 1492721277
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id C1C5B24457;
	Thu, 20 Apr 2017 16:47:57 -0400 (EDT)
From: Zi Yan &lt;zi.yan@sent.com&gt;
To: n-horiguchi@ah.jp.nec.com, kirill.shutemov@linux.intel.com,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: akpm@linux-foundation.org, minchan@kernel.org, vbabka@suse.cz,
	mgorman@techsingularity.net, mhocko@kernel.org,
	khandual@linux.vnet.ibm.com, zi.yan@cs.rutgers.edu, dnellans@nvidia.com
Subject: [PATCH v5 05/11] mm: thp: enable thp migration in generic path
Date: Thu, 20 Apr 2017 16:47:46 -0400
Message-Id: &lt;20170420204752.79703-6-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170420204752.79703-1-zi.yan@sent.com&gt;
References: &lt;20170420204752.79703-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - April 20, 2017, 8:47 p.m.</div>
<pre class="content">
<span class="from">From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>

This patch adds thp migration&#39;s core code, including conversions
between a PMD entry and a swap entry, setting PMD migration entry,
removing PMD migration entry, and waiting on PMD migration entries.

This patch makes it possible to support thp migration.
If you fail to allocate a destination page as a thp, you just split
the source thp as we do now, and then enter the normal page migration.
If you succeed to allocate destination thp, you enter thp migration.
Subsequent patches actually enable thp migration for each caller of
page migration by allowing its get_new_page() callback to
allocate thps.

ChangeLog v1 -&gt; v2:
- support pte-mapped thp, doubly-mapped thp
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

ChangeLog v2 -&gt; v3:
- use page_vma_mapped_walk()
- use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in
  set_pmd_migration_entry()

ChangeLog v3 -&gt; v4:
- factor out the code of removing pte pgtable page in zap_huge_pmd()

ChangeLog v4 -&gt; v5:
- remove unnecessary PTE-mapped THP code in remove_migration_pmd()
  and set_pmd_migration_entry()
- restructure the code in zap_huge_pmd() to avoid factoring out
  the pte pgtable page code
- in zap_huge_pmd(), check that PMD swap entries are migration entries
- change author information
<span class="signed-off-by">
Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
---
 arch/x86/include/asm/pgtable_64.h |  2 +
 include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++++-
 mm/huge_memory.c                  | 84 ++++++++++++++++++++++++++++++++++++---
 mm/migrate.c                      | 30 +++++++++++++-
 mm/page_vma_mapped.c              | 13 ++++--
 mm/pgtable-generic.c              |  3 +-
 mm/rmap.c                         | 11 +++++
 7 files changed, 200 insertions(+), 12 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 770b5ae271ed..bd0252630bb3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -187,7 +187,9 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \
 					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
<span class="p_add">+#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
<span class="p_add">+#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
 
 extern int kern_addr_valid(unsigned long addr);
 extern void cleanup_highmap(void);
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3e7eec..c543c6f25e8f 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -103,7 +103,8 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_del">-	BUG_ON(!PageLocked(page));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="p_add">+</span>
 	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,
 			page_to_pfn(page));
 }
<span class="p_chunk">@@ -126,7 +127,7 @@</span> <span class="p_context"> static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
 	 * Any use of migration entries may only occur while the
 	 * corresponding page is locked
 	 */
<span class="p_del">-	BUG_ON(!PageLocked(p));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(p)));</span>
 	return p;
 }
 
<span class="p_chunk">@@ -163,6 +164,70 @@</span> <span class="p_context"> static inline int is_write_migration_entry(swp_entry_t entry)</span>
 
 #endif
 
<span class="p_add">+struct page_vma_mapped_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="p_add">+	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="p_add">+	return __swp_entry_to_pmd(arch_entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return (pmd_t){{ 0 }};</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MEMORY_FAILURE
 
 extern atomic_long_t num_poisoned_pages __read_mostly;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 0db1f1c90aad..7406d88445bf 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1633,10 +1633,23 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		struct page *page = NULL;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pmd_present(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+			migration = 1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (PageAnon(page)) {
 			zap_deposited_table(tlb-&gt;mm, pmd);
 			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);
<span class="p_chunk">@@ -1645,8 +1658,10 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 				zap_deposited_table(tlb-&gt;mm, pmd);
 			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);
 		}
<span class="p_add">+</span>
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -2669,3 +2684,62 @@</span> <span class="p_context"> static int __init split_huge_pages_debugfs(void)</span>
 }
 late_initcall(split_huge_pages_debugfs);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	pmd_t pmdval;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="p_add">+			address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+	if (pmd_dirty(pmdval))</span>
<span class="p_add">+		set_page_dirty(page);</span>
<span class="p_add">+	entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="p_add">+	pmdval = swp_entry_to_pmd(entry);</span>
<span class="p_add">+	set_pmd_at(mm, address, pvmw-&gt;pmd, pmdval);</span>
<span class="p_add">+	page_remove_rmap(page, true);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="p_add">+			address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="p_add">+	pmd_t pmde;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+	get_page(new);</span>
<span class="p_add">+	pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	if (is_write_migration_entry(entry))</span>
<span class="p_add">+		pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+	page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="p_add">+	set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="p_add">+	flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+		mlock_vma_page(new);</span>
<span class="p_add">+	update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 5cfe3c27bcbe..bbc856264b69 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -214,6 +214,13 @@</span> <span class="p_context"> static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
 			new = page - pvmw.page-&gt;index +
 				linear_page_index(vma, pvmw.address);
 
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (!pvmw.pte &amp;&amp; pvmw.page) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);</span>
<span class="p_add">+			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		get_page(new);
 		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));
 		if (pte_swp_soft_dirty(*pvmw.pte))
<span class="p_chunk">@@ -327,6 +334,27 @@</span> <span class="p_context"> void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
 	__migration_entry_wait(mm, pte, ptl);
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pmd_lock(mm, pmd);</span>
<span class="p_add">+	if (!is_pmd_migration_entry(*pmd))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="p_add">+	if (!get_page_unless_zero(page))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	wait_on_page_locked(page);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLOCK
 /* Returns true if all buffers are successfully locked */
 static bool buffer_migrate_lock_buffers(struct buffer_head *head,
<span class="p_chunk">@@ -1085,7 +1113,7 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 		goto out;
 	}
 
<span class="p_del">-	if (unlikely(PageTransHuge(page))) {</span>
<span class="p_add">+	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
 		lock_page(page);
 		rc = split_huge_page(page);
 		unlock_page(page);
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index de9c40d7304a..e209a12d8722 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -137,16 +137,23 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	if (!pud_present(*pud))
 		return false;
 	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);
<span class="p_del">-	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
 		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);
<span class="p_del">-		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="p_del">-			return not_found(pvmw);</span>
 		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {
 			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)
 				return not_found(pvmw);
 			if (pmd_page(*pvmw-&gt;pmd) != page)
 				return not_found(pvmw);
 			return true;
<span class="p_add">+		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="p_add">+				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+				if (migration_entry_to_page(entry) != page)</span>
<span class="p_add">+					return not_found(pvmw);</span>
<span class="p_add">+				return true;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			return not_found(pvmw);</span>
 		} else {
 			/* THP pmd was split under us: handle on pte level */
 			spin_unlock(pvmw-&gt;ptl);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index c99d9512a45b..1175f6a24fdb 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -124,7 +124,8 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 {
 	pmd_t pmd;
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="p_add">+	VM_BUG_ON((pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="p_add">+			   !pmd_devmap(*pmdp)) || !pmd_present(*pmdp));</span>
 	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return pmd;
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index b0c6b20dca74..b9505f15c099 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1302,6 +1302,7 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	bool ret = true;
 	enum ttu_flags flags = (enum ttu_flags)arg;
 
<span class="p_add">+</span>
 	/* munlock has nothing to gain from examining un-locked vmas */
 	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))
 		return true;
<span class="p_chunk">@@ -1312,6 +1313,16 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	}
 
 	while (page_vma_mapped_walk(&amp;pvmw)) {
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (flags &amp; TTU_MIGRATION) {</span>
<span class="p_add">+			if (!pvmw.pte &amp;&amp; page) {</span>
<span class="p_add">+				VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page),</span>
<span class="p_add">+						page);</span>
<span class="p_add">+				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/*
 		 * If the page is mlock()d, we cannot swap it out.
 		 * If it&#39;s recently referenced (perhaps page_referenced

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



