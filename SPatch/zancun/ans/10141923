
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[5/6] mm, hugetlb: further simplify hugetlb allocation API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [5/6] mm, hugetlb: further simplify hugetlb allocation API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 3, 2018, 9:32 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180103093213.26329-6-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10141923/mbox/"
   >mbox</a>
|
   <a href="/patch/10141923/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10141923/">/patch/10141923/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C08A860594 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:33:05 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A745328E6C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:33:05 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9C6132902E; Wed,  3 Jan 2018 09:33:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BB63428E6C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:33:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752137AbeACJcm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 3 Jan 2018 04:32:42 -0500
Received: from mail-pg0-f66.google.com ([74.125.83.66]:36806 &quot;EHLO
	mail-pg0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751933AbeACJci (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 3 Jan 2018 04:32:38 -0500
Received: by mail-pg0-f66.google.com with SMTP id 21so505615pgg.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 03 Jan 2018 01:32:38 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=3OTMJCYeWCKksWWL6Lh3ERhmvOWQb+VxNDzWqTgpjHo=;
	b=RdfEuDw6bdfZlPNLsDSrLjIlR2BxP7493/YfZKTwsli2M5EZ8bd6bFV5Q5ZOMxBfio
	oKDnHcrP+xVVjtCPC0ZqdxVmYJYjaLfaLFfFAP2NcresE3M1g0IEE90fSN2EPSFlrDqw
	0DW1JCS36LerfPzSSo13Lk/rYmkwCrLcBsm0/fen3fwCPC6m3YicRg3XD6n7BkoypUYk
	bu2TDlPpkmMCVQlt4POBWfem3kzNFQgIMhxF7Cq17Tjtpz29n6EmNOSmUMvHD/TqmP33
	Rm9J+iIBNxso4z1o8bQ4EA2vDgI8L2xQJKAirRzTYiRGgCmYltGzwXNnw8Sz51sMfLQF
	jnrQ==
X-Gm-Message-State: AKGB3mLPlbvTZuqZaBKRDGReZ7AFu3BKn+njnVr192i4PCd8mp6imLlo
	BUx/Zd67j6yNpkj8SmtVByg=
X-Google-Smtp-Source: ACJfBos4yWLPDvPelwN+UjfztXv4oPW3gOTSsS5PcQvYHhcUsTKC42LDF7B2K4SSdB8pu4oH6htTHw==
X-Received: by 10.98.246.18 with SMTP id x18mr850203pfh.219.1514971957575;
	Wed, 03 Jan 2018 01:32:37 -0800 (PST)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	c2sm1790628pgq.48.2018.01.03.01.32.35
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 03 Jan 2018 01:32:37 -0800 (PST)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: &lt;linux-mm@kvack.org&gt;, Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 5/6] mm, hugetlb: further simplify hugetlb allocation API
Date: Wed,  3 Jan 2018 10:32:12 +0100
Message-Id: &lt;20180103093213.26329-6-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20180103093213.26329-1-mhocko@kernel.org&gt;
References: &lt;20180103093213.26329-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Jan. 3, 2018, 9:32 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Hugetlb allocator has several layer of allocation functions depending
and the purpose of the allocation. There are two allocators depending
on whether the page can be allocated from the page allocator or we need
a contiguous allocator. This is currently opencoded in alloc_fresh_huge_page
which is the only path that might allocate giga pages which require the
later allocator. Create alloc_fresh_huge_page which hides this
implementation detail and use it in all callers which hardcoded the
buddy allocator path (__hugetlb_alloc_buddy_huge_page). This shouldn&#39;t
introduce any funtional change because both migration and surplus
allocators exlude giga pages explicitly.

While we are at it let&#39;s do some renaming. The current scheme is not
consistent and overly painfull to read and understand. Get rid of prefix
underscores from most functions. There is no real reason to make names
longer.
* alloc_fresh_huge_page is the new layer to abstract underlying
  allocator
* __hugetlb_alloc_buddy_huge_page becomes shorter and neater
  alloc_buddy_huge_page.
* Former alloc_fresh_huge_page becomes alloc_pool_huge_page because we put
  the new page directly to the pool
* alloc_surplus_huge_page can drop the opencoded prep_new_huge_page code
  as it uses alloc_fresh_huge_page now
* others lose their excessive prefix underscores to make names shorter
<span class="reviewed-by">
Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 mm/hugetlb.c | 78 ++++++++++++++++++++++++++++++++----------------------------
 1 file changed, 42 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=178067">Dan Rue</a> - Feb. 21, 2018, 4:24 a.m.</div>
<pre class="content">
On Wed, Jan 03, 2018 at 10:32:12AM +0100, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hugetlb allocator has several layer of allocation functions depending</span>
<span class="quote">&gt; and the purpose of the allocation. There are two allocators depending</span>
<span class="quote">&gt; on whether the page can be allocated from the page allocator or we need</span>
<span class="quote">&gt; a contiguous allocator. This is currently opencoded in alloc_fresh_huge_page</span>
<span class="quote">&gt; which is the only path that might allocate giga pages which require the</span>
<span class="quote">&gt; later allocator. Create alloc_fresh_huge_page which hides this</span>
<span class="quote">&gt; implementation detail and use it in all callers which hardcoded the</span>
<span class="quote">&gt; buddy allocator path (__hugetlb_alloc_buddy_huge_page). This shouldn&#39;t</span>
<span class="quote">&gt; introduce any funtional change because both migration and surplus</span>
<span class="quote">&gt; allocators exlude giga pages explicitly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; While we are at it let&#39;s do some renaming. The current scheme is not</span>
<span class="quote">&gt; consistent and overly painfull to read and understand. Get rid of prefix</span>
<span class="quote">&gt; underscores from most functions. There is no real reason to make names</span>
<span class="quote">&gt; longer.</span>
<span class="quote">&gt; * alloc_fresh_huge_page is the new layer to abstract underlying</span>
<span class="quote">&gt;   allocator</span>
<span class="quote">&gt; * __hugetlb_alloc_buddy_huge_page becomes shorter and neater</span>
<span class="quote">&gt;   alloc_buddy_huge_page.</span>
<span class="quote">&gt; * Former alloc_fresh_huge_page becomes alloc_pool_huge_page because we put</span>
<span class="quote">&gt;   the new page directly to the pool</span>
<span class="quote">&gt; * alloc_surplus_huge_page can drop the opencoded prep_new_huge_page code</span>
<span class="quote">&gt;   as it uses alloc_fresh_huge_page now</span>
<span class="quote">&gt; * others lose their excessive prefix underscores to make names shorter</span>

Hi Michal -

We (Linaro) run the libhugetlbfs test suite continuously against
mainline and recently (Feb 1), the &#39;counters&#39; test started failing on
with the following error:

    root@localhost:~# mount_point=&quot;/mnt/hugetlb/&quot;
    root@localhost:~# echo 200 &gt; /proc/sys/vm/nr_hugepages
    root@localhost:~# mkdir -p &quot;${mount_point}&quot;
    root@localhost:~# mount -t hugetlbfs hugetlbfs &quot;${mount_point}&quot;
    root@localhost:~# export LD_LIBRARY_PATH=/root/libhugetlbfs/libhugetlbfs-2.20/obj64
    root@localhost:~# /root/libhugetlbfs/libhugetlbfs-2.20/tests/obj64/counters
    Starting testcase &quot;/root/libhugetlbfs/libhugetlbfs-2.20/tests/obj64/counters&quot;, pid 3319
    Base pool size: 0
    Clean...
    FAIL    Line 326: Bad HugePages_Total: expected 0, actual 1

Line 326 refers to the test source @
https://github.com/libhugetlbfs/libhugetlbfs/blob/master/tests/counters.c#L326

I bisected the failure to this commit. The problem is seen on multiple
architectures (tested x86-64 and arm64).

Thanks,
Dan
<span class="quote">
&gt; </span>
<span class="quote">&gt; Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; Reviewed-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  mm/hugetlb.c | 78 ++++++++++++++++++++++++++++++++----------------------------</span>
<span class="quote">&gt;  1 file changed, 42 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 7dc80cbe8e89..60acd3e93a95 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1378,7 +1378,7 @@ pgoff_t __basepage_index(struct page *page)</span>
<span class="quote">&gt;  	return (index &lt;&lt; compound_order(page_head)) + compound_idx;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="quote">&gt; +static struct page *alloc_buddy_huge_page(struct hstate *h,</span>
<span class="quote">&gt;  		gfp_t gfp_mask, int nid, nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int order = huge_page_order(h);</span>
<span class="quote">&gt; @@ -1396,34 +1396,49 @@ static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Common helper to allocate a fresh hugetlb page. All specific allocators</span>
<span class="quote">&gt; + * should use this function to get new hugetlb pages</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static struct page *alloc_fresh_huge_page(struct hstate *h,</span>
<span class="quote">&gt; +		gfp_t gfp_mask, int nid, nodemask_t *nmask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (hstate_is_gigantic(h))</span>
<span class="quote">&gt; +		page = alloc_gigantic_page(h, gfp_mask, nid, nmask);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		page = alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="quote">&gt; +				nid, nmask);</span>
<span class="quote">&gt; +	if (!page)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (hstate_is_gigantic(h))</span>
<span class="quote">&gt; +		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="quote">&gt; +	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return page;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Allocates a fresh page to the hugetlb allocator pool in the node interleaved</span>
<span class="quote">&gt;   * manner.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
<span class="quote">&gt; +static int alloc_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	int nr_nodes, node;</span>
<span class="quote">&gt;  	gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {</span>
<span class="quote">&gt; -		if (hstate_is_gigantic(h))</span>
<span class="quote">&gt; -			page = alloc_gigantic_page(h, gfp_mask,</span>
<span class="quote">&gt; -					node, nodes_allowed);</span>
<span class="quote">&gt; -		else</span>
<span class="quote">&gt; -			page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="quote">&gt; -					node, nodes_allowed);</span>
<span class="quote">&gt; +		page = alloc_fresh_huge_page(h, gfp_mask, node, nodes_allowed);</span>
<span class="quote">&gt;  		if (page)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (hstate_is_gigantic(h))</span>
<span class="quote">&gt; -		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="quote">&gt; -	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="quote">&gt;  	put_page(page); /* free it into the hugepage allocator */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt; @@ -1537,7 +1552,7 @@ int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Allocates a fresh surplus page from the page allocator.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt; +static struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  		int nid, nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; @@ -1550,7 +1565,7 @@ static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="quote">&gt; +	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1567,16 +1582,8 @@ static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  		put_page(page);</span>
<span class="quote">&gt;  		page = NULL;</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		int r_nid;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  		h-&gt;surplus_huge_pages++;</span>
<span class="quote">&gt; -		h-&gt;nr_huge_pages++;</span>
<span class="quote">&gt; -		INIT_LIST_HEAD(&amp;page-&gt;lru);</span>
<span class="quote">&gt; -		r_nid = page_to_nid(page);</span>
<span class="quote">&gt; -		set_compound_page_dtor(page, HUGETLB_PAGE_DTOR);</span>
<span class="quote">&gt; -		set_hugetlb_cgroup(page, NULL);</span>
<span class="quote">&gt; -		h-&gt;nr_huge_pages_node[r_nid]++;</span>
<span class="quote">&gt; -		h-&gt;surplus_huge_pages_node[r_nid]++;</span>
<span class="quote">&gt; +		h-&gt;nr_huge_pages_node[page_to_nid(page)]++;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  out_unlock:</span>
<span class="quote">&gt; @@ -1585,7 +1592,7 @@ static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt; +static struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  		int nid, nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; @@ -1593,7 +1600,7 @@ static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	if (hstate_is_gigantic(h))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="quote">&gt; +	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1601,7 +1608,6 @@ static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	 * We do not account these pages as surplus because they are only</span>
<span class="quote">&gt;  	 * temporary and will be released properly on the last reference</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="quote">&gt;  	SetPageHugeTemporary(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt; @@ -1611,7 +1617,7 @@ static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="quote">&gt;   * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static</span>
<span class="quote">&gt; -struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="quote">&gt; +struct page *alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma, unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; @@ -1621,7 +1627,7 @@ struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="quote">&gt;  	nodemask_t *nodemask;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	nid = huge_node(vma, addr, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="quote">&gt; -	page = __alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="quote">&gt; +	page = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="quote">&gt;  	mpol_cond_put(mpol);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt; @@ -1642,7 +1648,7 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt; -		page = __alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
<span class="quote">&gt; +		page = alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1665,7 +1671,7 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	return __alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
<span class="quote">&gt; +	return alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1693,7 +1699,7 @@ static int gather_surplus_pages(struct hstate *h, int delta)</span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	for (i = 0; i &lt; needed; i++) {</span>
<span class="quote">&gt; -		page = __alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
<span class="quote">&gt; +		page = alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
<span class="quote">&gt;  				NUMA_NO_NODE, NULL);</span>
<span class="quote">&gt;  		if (!page) {</span>
<span class="quote">&gt;  			alloc_ok = false;</span>
<span class="quote">&gt; @@ -2030,7 +2036,7 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);</span>
<span class="quote">&gt;  	if (!page) {</span>
<span class="quote">&gt;  		spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
<span class="quote">&gt; +		page = alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
<span class="quote">&gt;  		if (!page)</span>
<span class="quote">&gt;  			goto out_uncharge_cgroup;</span>
<span class="quote">&gt;  		if (!avoid_reserve &amp;&amp; vma_has_reserves(vma, gbl_chg)) {</span>
<span class="quote">&gt; @@ -2170,7 +2176,7 @@ static void __init hugetlb_hstate_alloc_pages(struct hstate *h)</span>
<span class="quote">&gt;  		if (hstate_is_gigantic(h)) {</span>
<span class="quote">&gt;  			if (!alloc_bootmem_huge_page(h))</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt; -		} else if (!alloc_fresh_huge_page(h,</span>
<span class="quote">&gt; +		} else if (!alloc_pool_huge_page(h,</span>
<span class="quote">&gt;  					 &amp;node_states[N_MEMORY]))</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt; @@ -2290,7 +2296,7 @@ static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
<span class="quote">&gt;  	 * First take pages out of surplus state.  Then make up the</span>
<span class="quote">&gt;  	 * remaining difference by allocating fresh huge pages.</span>
<span class="quote">&gt;  	 *</span>
<span class="quote">&gt; -	 * We might race with __alloc_surplus_huge_page() here and be unable</span>
<span class="quote">&gt; +	 * We might race with alloc_surplus_huge_page() here and be unable</span>
<span class="quote">&gt;  	 * to convert a surplus huge page to a normal huge page. That is</span>
<span class="quote">&gt;  	 * not critical, though, it just means the overall size of the</span>
<span class="quote">&gt;  	 * pool might be one hugepage larger than it needs to be, but</span>
<span class="quote">&gt; @@ -2313,7 +2319,7 @@ static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
<span class="quote">&gt;  		/* yield cpu to avoid soft lockup */</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		ret = alloc_fresh_huge_page(h, nodes_allowed);</span>
<span class="quote">&gt; +		ret = alloc_pool_huge_page(h, nodes_allowed);</span>
<span class="quote">&gt;  		spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  		if (!ret)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt; @@ -2333,7 +2339,7 @@ static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
<span class="quote">&gt;  	 * By placing pages into the surplus state independent of the</span>
<span class="quote">&gt;  	 * overcommit value, we are allowing the surplus pool size to</span>
<span class="quote">&gt;  	 * exceed overcommit. There are few sane options here. Since</span>
<span class="quote">&gt; -	 * __alloc_surplus_huge_page() is checking the global counter,</span>
<span class="quote">&gt; +	 * alloc_surplus_huge_page() is checking the global counter,</span>
<span class="quote">&gt;  	 * though, we&#39;ll note that we&#39;re not allowed to exceed surplus</span>
<span class="quote">&gt;  	 * and won&#39;t grow the pool anywhere else. Not until one of the</span>
<span class="quote">&gt;  	 * sysctls are changed, or the surplus pages go out of use.</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.15.1</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Feb. 21, 2018, 9:55 a.m.</div>
<pre class="content">
On Tue 20-02-18 22:24:57, Dan Rue wrote:
<span class="quote">&gt; On Wed, Jan 03, 2018 at 10:32:12AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hugetlb allocator has several layer of allocation functions depending</span>
<span class="quote">&gt; &gt; and the purpose of the allocation. There are two allocators depending</span>
<span class="quote">&gt; &gt; on whether the page can be allocated from the page allocator or we need</span>
<span class="quote">&gt; &gt; a contiguous allocator. This is currently opencoded in alloc_fresh_huge_page</span>
<span class="quote">&gt; &gt; which is the only path that might allocate giga pages which require the</span>
<span class="quote">&gt; &gt; later allocator. Create alloc_fresh_huge_page which hides this</span>
<span class="quote">&gt; &gt; implementation detail and use it in all callers which hardcoded the</span>
<span class="quote">&gt; &gt; buddy allocator path (__hugetlb_alloc_buddy_huge_page). This shouldn&#39;t</span>
<span class="quote">&gt; &gt; introduce any funtional change because both migration and surplus</span>
<span class="quote">&gt; &gt; allocators exlude giga pages explicitly.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; While we are at it let&#39;s do some renaming. The current scheme is not</span>
<span class="quote">&gt; &gt; consistent and overly painfull to read and understand. Get rid of prefix</span>
<span class="quote">&gt; &gt; underscores from most functions. There is no real reason to make names</span>
<span class="quote">&gt; &gt; longer.</span>
<span class="quote">&gt; &gt; * alloc_fresh_huge_page is the new layer to abstract underlying</span>
<span class="quote">&gt; &gt;   allocator</span>
<span class="quote">&gt; &gt; * __hugetlb_alloc_buddy_huge_page becomes shorter and neater</span>
<span class="quote">&gt; &gt;   alloc_buddy_huge_page.</span>
<span class="quote">&gt; &gt; * Former alloc_fresh_huge_page becomes alloc_pool_huge_page because we put</span>
<span class="quote">&gt; &gt;   the new page directly to the pool</span>
<span class="quote">&gt; &gt; * alloc_surplus_huge_page can drop the opencoded prep_new_huge_page code</span>
<span class="quote">&gt; &gt;   as it uses alloc_fresh_huge_page now</span>
<span class="quote">&gt; &gt; * others lose their excessive prefix underscores to make names shorter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Michal -</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We (Linaro) run the libhugetlbfs test suite continuously against</span>
<span class="quote">&gt; mainline and recently (Feb 1), the &#39;counters&#39; test started failing on</span>
<span class="quote">&gt; with the following error:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     root@localhost:~# mount_point=&quot;/mnt/hugetlb/&quot;</span>
<span class="quote">&gt;     root@localhost:~# echo 200 &gt; /proc/sys/vm/nr_hugepages</span>
<span class="quote">&gt;     root@localhost:~# mkdir -p &quot;${mount_point}&quot;</span>
<span class="quote">&gt;     root@localhost:~# mount -t hugetlbfs hugetlbfs &quot;${mount_point}&quot;</span>
<span class="quote">&gt;     root@localhost:~# export LD_LIBRARY_PATH=/root/libhugetlbfs/libhugetlbfs-2.20/obj64</span>
<span class="quote">&gt;     root@localhost:~# /root/libhugetlbfs/libhugetlbfs-2.20/tests/obj64/counters</span>
<span class="quote">&gt;     Starting testcase &quot;/root/libhugetlbfs/libhugetlbfs-2.20/tests/obj64/counters&quot;, pid 3319</span>
<span class="quote">&gt;     Base pool size: 0</span>
<span class="quote">&gt;     Clean...</span>
<span class="quote">&gt;     FAIL    Line 326: Bad HugePages_Total: expected 0, actual 1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Line 326 refers to the test source @</span>
<span class="quote">&gt; https://github.com/libhugetlbfs/libhugetlbfs/blob/master/tests/counters.c#L326</span>

Thanks for the report. I am fighting to get hugetlb tests working. My
previous deployment is gone and the new git snapshot fails to build. I
will look into it further but ...
<span class="quote">
&gt; I bisected the failure to this commit. The problem is seen on multiple</span>
<span class="quote">&gt; architectures (tested x86-64 and arm64).</span>

The patch shouldn&#39;t have introduced any functional changes IIRC. But let
me have a look
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 7dc80cbe8e89..60acd3e93a95 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1378,7 +1378,7 @@</span> <span class="p_context"> pgoff_t __basepage_index(struct page *page)</span>
 	return (index &lt;&lt; compound_order(page_head)) + compound_idx;
 }
 
<span class="p_del">-static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_add">+static struct page *alloc_buddy_huge_page(struct hstate *h,</span>
 		gfp_t gfp_mask, int nid, nodemask_t *nmask)
 {
 	int order = huge_page_order(h);
<span class="p_chunk">@@ -1396,34 +1396,49 @@</span> <span class="p_context"> static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
 	return page;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Common helper to allocate a fresh hugetlb page. All specific allocators</span>
<span class="p_add">+ * should use this function to get new hugetlb pages</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct page *alloc_fresh_huge_page(struct hstate *h,</span>
<span class="p_add">+		gfp_t gfp_mask, int nid, nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		page = alloc_gigantic_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		page = alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_add">+				nid, nmask);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="p_add">+	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Allocates a fresh page to the hugetlb allocator pool in the node interleaved
  * manner.
  */
<span class="p_del">-static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
<span class="p_add">+static int alloc_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
 {
 	struct page *page;
 	int nr_nodes, node;
 	gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
 
 	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_del">-		if (hstate_is_gigantic(h))</span>
<span class="p_del">-			page = alloc_gigantic_page(h, gfp_mask,</span>
<span class="p_del">-					node, nodes_allowed);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_del">-					node, nodes_allowed);</span>
<span class="p_add">+		page = alloc_fresh_huge_page(h, gfp_mask, node, nodes_allowed);</span>
 		if (page)
 			break;
<span class="p_del">-</span>
 	}
 
 	if (!page)
 		return 0;
 
<span class="p_del">-	if (hstate_is_gigantic(h))</span>
<span class="p_del">-		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="p_del">-	prep_new_huge_page(h, page, page_to_nid(page));</span>
 	put_page(page); /* free it into the hugepage allocator */
 
 	return 1;
<span class="p_chunk">@@ -1537,7 +1552,7 @@</span> <span class="p_context"> int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 /*
  * Allocates a fresh surplus page from the page allocator.
  */
<span class="p_del">-static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+static struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		int nid, nodemask_t *nmask)
 {
 	struct page *page = NULL;
<span class="p_chunk">@@ -1550,7 +1565,7 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		goto out_unlock;
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
 	if (!page)
 		goto out_unlock;
 
<span class="p_chunk">@@ -1567,16 +1582,8 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		put_page(page);
 		page = NULL;
 	} else {
<span class="p_del">-		int r_nid;</span>
<span class="p_del">-</span>
 		h-&gt;surplus_huge_pages++;
<span class="p_del">-		h-&gt;nr_huge_pages++;</span>
<span class="p_del">-		INIT_LIST_HEAD(&amp;page-&gt;lru);</span>
<span class="p_del">-		r_nid = page_to_nid(page);</span>
<span class="p_del">-		set_compound_page_dtor(page, HUGETLB_PAGE_DTOR);</span>
<span class="p_del">-		set_hugetlb_cgroup(page, NULL);</span>
<span class="p_del">-		h-&gt;nr_huge_pages_node[r_nid]++;</span>
<span class="p_del">-		h-&gt;surplus_huge_pages_node[r_nid]++;</span>
<span class="p_add">+		h-&gt;nr_huge_pages_node[page_to_nid(page)]++;</span>
 	}
 
 out_unlock:
<span class="p_chunk">@@ -1585,7 +1592,7 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	return page;
 }
 
<span class="p_del">-static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+static struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		int nid, nodemask_t *nmask)
 {
 	struct page *page;
<span class="p_chunk">@@ -1593,7 +1600,7 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	if (hstate_is_gigantic(h))
 		return NULL;
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
 	if (!page)
 		return NULL;
 
<span class="p_chunk">@@ -1601,7 +1608,6 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	 * We do not account these pages as surplus because they are only
 	 * temporary and will be released properly on the last reference
 	 */
<span class="p_del">-	prep_new_huge_page(h, page, page_to_nid(page));</span>
 	SetPageHugeTemporary(page);
 
 	return page;
<span class="p_chunk">@@ -1611,7 +1617,7 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
  * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.
  */
 static
<span class="p_del">-struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="p_add">+struct page *alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 		struct vm_area_struct *vma, unsigned long addr)
 {
 	struct page *page;
<span class="p_chunk">@@ -1621,7 +1627,7 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 	nodemask_t *nodemask;
 
 	nid = huge_node(vma, addr, gfp_mask, &amp;mpol, &amp;nodemask);
<span class="p_del">-	page = __alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="p_add">+	page = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
 	mpol_cond_put(mpol);
 
 	return page;
<span class="p_chunk">@@ -1642,7 +1648,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_del">-		page = __alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
<span class="p_add">+		page = alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
 
 	return page;
 }
<span class="p_chunk">@@ -1665,7 +1671,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	return __alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
<span class="p_add">+	return alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
 }
 
 /*
<span class="p_chunk">@@ -1693,7 +1699,7 @@</span> <span class="p_context"> static int gather_surplus_pages(struct hstate *h, int delta)</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = __alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
<span class="p_add">+		page = alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
 				NUMA_NO_NODE, NULL);
 		if (!page) {
 			alloc_ok = false;
<span class="p_chunk">@@ -2030,7 +2036,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);
 	if (!page) {
 		spin_unlock(&amp;hugetlb_lock);
<span class="p_del">-		page = __alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
<span class="p_add">+		page = alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
 		if (!page)
 			goto out_uncharge_cgroup;
 		if (!avoid_reserve &amp;&amp; vma_has_reserves(vma, gbl_chg)) {
<span class="p_chunk">@@ -2170,7 +2176,7 @@</span> <span class="p_context"> static void __init hugetlb_hstate_alloc_pages(struct hstate *h)</span>
 		if (hstate_is_gigantic(h)) {
 			if (!alloc_bootmem_huge_page(h))
 				break;
<span class="p_del">-		} else if (!alloc_fresh_huge_page(h,</span>
<span class="p_add">+		} else if (!alloc_pool_huge_page(h,</span>
 					 &amp;node_states[N_MEMORY]))
 			break;
 		cond_resched();
<span class="p_chunk">@@ -2290,7 +2296,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * First take pages out of surplus state.  Then make up the
 	 * remaining difference by allocating fresh huge pages.
 	 *
<span class="p_del">-	 * We might race with __alloc_surplus_huge_page() here and be unable</span>
<span class="p_add">+	 * We might race with alloc_surplus_huge_page() here and be unable</span>
 	 * to convert a surplus huge page to a normal huge page. That is
 	 * not critical, though, it just means the overall size of the
 	 * pool might be one hugepage larger than it needs to be, but
<span class="p_chunk">@@ -2313,7 +2319,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 		/* yield cpu to avoid soft lockup */
 		cond_resched();
 
<span class="p_del">-		ret = alloc_fresh_huge_page(h, nodes_allowed);</span>
<span class="p_add">+		ret = alloc_pool_huge_page(h, nodes_allowed);</span>
 		spin_lock(&amp;hugetlb_lock);
 		if (!ret)
 			goto out;
<span class="p_chunk">@@ -2333,7 +2339,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * By placing pages into the surplus state independent of the
 	 * overcommit value, we are allowing the surplus pool size to
 	 * exceed overcommit. There are few sane options here. Since
<span class="p_del">-	 * __alloc_surplus_huge_page() is checking the global counter,</span>
<span class="p_add">+	 * alloc_surplus_huge_page() is checking the global counter,</span>
 	 * though, we&#39;ll note that we&#39;re not allowed to exceed surplus
 	 * and won&#39;t grow the pool anywhere else. Not until one of the
 	 * sysctls are changed, or the surplus pages go out of use.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



