
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv6,5/7] mm: make compound_head() robust - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv6,5/7] mm: make compound_head() robust</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 15, 2015, 10:28 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1442312895-124384-6-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7183611/mbox/"
   >mbox</a>
|
   <a href="/patch/7183611/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7183611/">/patch/7183611/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 92992BEEC1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 15 Sep 2015 10:30:08 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id B37A2206E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 15 Sep 2015 10:30:06 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 3858E206EF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 15 Sep 2015 10:30:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753001AbbIOK37 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 15 Sep 2015 06:29:59 -0400
Received: from mga02.intel.com ([134.134.136.20]:3364 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752771AbbIOK3a (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 15 Sep 2015 06:29:30 -0400
Received: from orsmga001.jf.intel.com ([10.7.209.18])
	by orsmga101.jf.intel.com with ESMTP; 15 Sep 2015 03:29:29 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,535,1437462000&quot;; 
	d=&quot;scan&#39;208,217&quot;;a=&quot;769333576&quot;
Received: from black.fi.intel.com ([10.237.72.93])
	by orsmga001.jf.intel.com with ESMTP; 15 Sep 2015 03:29:25 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id B39E36B8; Tue, 15 Sep 2015 13:28:50 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	&quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
Subject: [PATCHv6 5/7] mm: make compound_head() robust
Date: Tue, 15 Sep 2015 13:28:13 +0300
Message-Id: &lt;1442312895-124384-6-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.5.1
In-Reply-To: &lt;1442312895-124384-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1442312895-124384-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Sept. 15, 2015, 10:28 a.m.</div>
<pre class="content">
Hugh has pointed that compound_head() call can be unsafe in some
context. There&#39;s one example:

	CPU0					CPU1

isolate_migratepages_block()
  page_count()
    compound_head()
      !!PageTail() == true
					put_page()
					  tail-&gt;first_page = NULL
      head = tail-&gt;first_page
					alloc_pages(__GFP_COMP)
					   prep_compound_page()
					     tail-&gt;first_page = head
					     __SetPageTail(p);
      !!PageTail() == true
    &lt;head == NULL dereferencing&gt;

The race is pure theoretical. I don&#39;t it&#39;s possible to trigger it in
practice. But who knows.

We can fix the race by changing how encode PageTail() and compound_head()
within struct page to be able to update them in one shot.

The patch introduces page-&gt;compound_head into third double word block in
front of compound_dtor and compound_order. Bit 0 encodes PageTail() and
the rest bits are pointer to head page if bit zero is set.

The patch moves page-&gt;pmd_huge_pte out of word, just in case if an
architecture defines pgtable_t into something what can have the bit 0
set.

hugetlb_cgroup uses page-&gt;lru.next in the second tail page to store
pointer struct hugetlb_cgroup. The patch switch it to use page-&gt;private
in the second tail page instead. The space is free since -&gt;first_page is
removed from the union.

The patch also opens possibility to remove HUGETLB_CGROUP_MIN_ORDER
limitation, since there&#39;s now space in first tail page to store struct
hugetlb_cgroup pointer. But that&#39;s out of scope of the patch.

That means page-&gt;compound_head shares storage space with:

 - page-&gt;lru.next;
 - page-&gt;next;
 - page-&gt;rcu_head.next;

That&#39;s too long list to be absolutely sure, but looks like nobody uses
bit 0 of the word.

page-&gt;rcu_head.next guaranteed[1] to have bit 0 clean as long as we use
call_rcu(), call_rcu_bh(), call_rcu_sched(), or call_srcu(). But future
call_rcu_lazy() is not allowed as it makes use of the bit and we can
get false positive PageTail().

[1] http://lkml.kernel.org/g/20150827163634.GD4029@linux.vnet.ibm.com
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
---
 Documentation/vm/split_page_table_lock |  4 +-
 arch/xtensa/configs/iss_defconfig      |  1 -
 include/linux/hugetlb_cgroup.h         |  4 +-
 include/linux/mm.h                     | 53 ++--------------------
 include/linux/mm_types.h               | 22 ++++++++--
 include/linux/page-flags.h             | 80 ++++++++--------------------------
 mm/Kconfig                             | 12 -----
 mm/debug.c                             |  5 ---
 mm/huge_memory.c                       |  3 +-
 mm/hugetlb.c                           |  8 +---
 mm/hugetlb_cgroup.c                    |  2 +-
 mm/internal.h                          |  4 +-
 mm/memory-failure.c                    |  7 ---
 mm/page_alloc.c                        | 46 +++++++++++--------
 mm/swap.c                              |  4 +-
 15 files changed, 80 insertions(+), 175 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=61">Paul E. McKenney</a> - Sept. 15, 2015, 3:45 p.m.</div>
<pre class="content">
On Tue, Sep 15, 2015 at 01:28:13PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; Hugh has pointed that compound_head() call can be unsafe in some</span>
<span class="quote">&gt; context. There&#39;s one example:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0					CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; isolate_migratepages_block()</span>
<span class="quote">&gt;   page_count()</span>
<span class="quote">&gt;     compound_head()</span>
<span class="quote">&gt;       !!PageTail() == true</span>
<span class="quote">&gt; 					put_page()</span>
<span class="quote">&gt; 					  tail-&gt;first_page = NULL</span>
<span class="quote">&gt;       head = tail-&gt;first_page</span>
<span class="quote">&gt; 					alloc_pages(__GFP_COMP)</span>
<span class="quote">&gt; 					   prep_compound_page()</span>
<span class="quote">&gt; 					     tail-&gt;first_page = head</span>
<span class="quote">&gt; 					     __SetPageTail(p);</span>
<span class="quote">&gt;       !!PageTail() == true</span>
<span class="quote">&gt;     &lt;head == NULL dereferencing&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The race is pure theoretical. I don&#39;t it&#39;s possible to trigger it in</span>
<span class="quote">&gt; practice. But who knows.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can fix the race by changing how encode PageTail() and compound_head()</span>
<span class="quote">&gt; within struct page to be able to update them in one shot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch introduces page-&gt;compound_head into third double word block in</span>
<span class="quote">&gt; front of compound_dtor and compound_order. Bit 0 encodes PageTail() and</span>
<span class="quote">&gt; the rest bits are pointer to head page if bit zero is set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch moves page-&gt;pmd_huge_pte out of word, just in case if an</span>
<span class="quote">&gt; architecture defines pgtable_t into something what can have the bit 0</span>
<span class="quote">&gt; set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hugetlb_cgroup uses page-&gt;lru.next in the second tail page to store</span>
<span class="quote">&gt; pointer struct hugetlb_cgroup. The patch switch it to use page-&gt;private</span>
<span class="quote">&gt; in the second tail page instead. The space is free since -&gt;first_page is</span>
<span class="quote">&gt; removed from the union.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch also opens possibility to remove HUGETLB_CGROUP_MIN_ORDER</span>
<span class="quote">&gt; limitation, since there&#39;s now space in first tail page to store struct</span>
<span class="quote">&gt; hugetlb_cgroup pointer. But that&#39;s out of scope of the patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That means page-&gt;compound_head shares storage space with:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - page-&gt;lru.next;</span>
<span class="quote">&gt;  - page-&gt;next;</span>
<span class="quote">&gt;  - page-&gt;rcu_head.next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s too long list to be absolutely sure, but looks like nobody uses</span>
<span class="quote">&gt; bit 0 of the word.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; page-&gt;rcu_head.next guaranteed[1] to have bit 0 clean as long as we use</span>
<span class="quote">&gt; call_rcu(), call_rcu_bh(), call_rcu_sched(), or call_srcu(). But future</span>
<span class="quote">&gt; call_rcu_lazy() is not allowed as it makes use of the bit and we can</span>
<span class="quote">&gt; get false positive PageTail().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/g/20150827163634.GD4029@linux.vnet.ibm.com</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Reviewed-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>

From the viewpoint of relying on the bottom bit of the rcu_head
structure&#39;s -&gt;next field being zero in the case where only call_rcu()
is used, and never kfree_rcu() or possible future call_rcu_lazy()
functions:
<span class="acked-by">
Acked-by: Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="quote">
&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; Cc: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/vm/split_page_table_lock |  4 +-</span>
<span class="quote">&gt;  arch/xtensa/configs/iss_defconfig      |  1 -</span>
<span class="quote">&gt;  include/linux/hugetlb_cgroup.h         |  4 +-</span>
<span class="quote">&gt;  include/linux/mm.h                     | 53 ++--------------------</span>
<span class="quote">&gt;  include/linux/mm_types.h               | 22 ++++++++--</span>
<span class="quote">&gt;  include/linux/page-flags.h             | 80 ++++++++--------------------------</span>
<span class="quote">&gt;  mm/Kconfig                             | 12 -----</span>
<span class="quote">&gt;  mm/debug.c                             |  5 ---</span>
<span class="quote">&gt;  mm/huge_memory.c                       |  3 +-</span>
<span class="quote">&gt;  mm/hugetlb.c                           |  8 +---</span>
<span class="quote">&gt;  mm/hugetlb_cgroup.c                    |  2 +-</span>
<span class="quote">&gt;  mm/internal.h                          |  4 +-</span>
<span class="quote">&gt;  mm/memory-failure.c                    |  7 ---</span>
<span class="quote">&gt;  mm/page_alloc.c                        | 46 +++++++++++--------</span>
<span class="quote">&gt;  mm/swap.c                              |  4 +-</span>
<span class="quote">&gt;  15 files changed, 80 insertions(+), 175 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/vm/split_page_table_lock b/Documentation/vm/split_page_table_lock</span>
<span class="quote">&gt; index 6dea4fd5c961..62842a857dab 100644</span>
<span class="quote">&gt; --- a/Documentation/vm/split_page_table_lock</span>
<span class="quote">&gt; +++ b/Documentation/vm/split_page_table_lock</span>
<span class="quote">&gt; @@ -54,8 +54,8 @@ everything required is done by pgtable_page_ctor() and pgtable_page_dtor(),</span>
<span class="quote">&gt;  which must be called on PTE table allocation / freeing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  Make sure the architecture doesn&#39;t use slab allocator for page table</span>
<span class="quote">&gt; -allocation: slab uses page-&gt;slab_cache and page-&gt;first_page for its pages.</span>
<span class="quote">&gt; -These fields share storage with page-&gt;ptl.</span>
<span class="quote">&gt; +allocation: slab uses page-&gt;slab_cache for its pages.</span>
<span class="quote">&gt; +This field shares storage with page-&gt;ptl.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  PMD split lock only makes sense if you have more than two page table</span>
<span class="quote">&gt;  levels.</span>
<span class="quote">&gt; diff --git a/arch/xtensa/configs/iss_defconfig b/arch/xtensa/configs/iss_defconfig</span>
<span class="quote">&gt; index f3dfe0d921c2..44c6764d9146 100644</span>
<span class="quote">&gt; --- a/arch/xtensa/configs/iss_defconfig</span>
<span class="quote">&gt; +++ b/arch/xtensa/configs/iss_defconfig</span>
<span class="quote">&gt; @@ -169,7 +169,6 @@ CONFIG_FLATMEM_MANUAL=y</span>
<span class="quote">&gt;  # CONFIG_SPARSEMEM_MANUAL is not set</span>
<span class="quote">&gt;  CONFIG_FLATMEM=y</span>
<span class="quote">&gt;  CONFIG_FLAT_NODE_MEM_MAP=y</span>
<span class="quote">&gt; -CONFIG_PAGEFLAGS_EXTENDED=y</span>
<span class="quote">&gt;  CONFIG_SPLIT_PTLOCK_CPUS=4</span>
<span class="quote">&gt;  # CONFIG_PHYS_ADDR_T_64BIT is not set</span>
<span class="quote">&gt;  CONFIG_ZONE_DMA_FLAG=1</span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb_cgroup.h b/include/linux/hugetlb_cgroup.h</span>
<span class="quote">&gt; index bcc853eccc85..75e34b900748 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb_cgroup.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb_cgroup.h</span>
<span class="quote">&gt; @@ -32,7 +32,7 @@ static inline struct hugetlb_cgroup *hugetlb_cgroup_from_page(struct page *page)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; -	return (struct hugetlb_cgroup *)page[2].lru.next;</span>
<span class="quote">&gt; +	return (struct hugetlb_cgroup *)page[2].private;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline</span>
<span class="quote">&gt; @@ -42,7 +42,7 @@ int set_hugetlb_cgroup(struct page *page, struct hugetlb_cgroup *h_cg)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt; -	page[2].lru.next = (void *)h_cg;</span>
<span class="quote">&gt; +	page[2].private	= (unsigned long)h_cg;</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index 107ba3798d08..17154f5e1c2a 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -426,46 +426,6 @@ static inline void compound_unlock_irqrestore(struct page *page,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline struct page *compound_head_by_tail(struct page *tail)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct page *head = tail-&gt;first_page;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * page-&gt;first_page may be a dangling pointer to an old</span>
<span class="quote">&gt; -	 * compound page, so recheck that it is still a tail</span>
<span class="quote">&gt; -	 * page before returning.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	smp_rmb();</span>
<span class="quote">&gt; -	if (likely(PageTail(tail)))</span>
<span class="quote">&gt; -		return head;</span>
<span class="quote">&gt; -	return tail;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Since either compound page could be dismantled asynchronously in THP</span>
<span class="quote">&gt; - * or we access asynchronously arbitrary positioned struct page, there</span>
<span class="quote">&gt; - * would be tail flag race. To handle this race, we should call</span>
<span class="quote">&gt; - * smp_rmb() before checking tail flag. compound_head_by_tail() did it.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline struct page *compound_head(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (unlikely(PageTail(page)))</span>
<span class="quote">&gt; -		return compound_head_by_tail(page);</span>
<span class="quote">&gt; -	return page;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * If we access compound page synchronously such as access to</span>
<span class="quote">&gt; - * allocated page, there is no need to handle tail flag race, so we can</span>
<span class="quote">&gt; - * check tail flag directly without any synchronization primitive.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline struct page *compound_head_fast(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (unlikely(PageTail(page)))</span>
<span class="quote">&gt; -		return page-&gt;first_page;</span>
<span class="quote">&gt; -	return page;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * The atomic page-&gt;_mapcount, starts from -1: so that transitions</span>
<span class="quote">&gt;   * both from it and to it can be tracked, using atomic_inc_and_test</span>
<span class="quote">&gt; @@ -514,7 +474,7 @@ static inline void get_huge_page_tail(struct page *page)</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageTail(page), page);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);</span>
<span class="quote">&gt; -	if (compound_tail_refcounted(page-&gt;first_page))</span>
<span class="quote">&gt; +	if (compound_tail_refcounted(compound_head(page)))</span>
<span class="quote">&gt;  		atomic_inc(&amp;page-&gt;_mapcount);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -537,13 +497,7 @@ static inline struct page *virt_to_head_page(const void *x)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = virt_to_page(x);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * We don&#39;t need to worry about synchronization of tail flag</span>
<span class="quote">&gt; -	 * when we call virt_to_head_page() since it is only called for</span>
<span class="quote">&gt; -	 * already allocated page and this page won&#39;t be freed until</span>
<span class="quote">&gt; -	 * this virt_to_head_page() is finished. So use _fast variant.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	return compound_head_fast(page);</span>
<span class="quote">&gt; +	return compound_head(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1561,8 +1515,7 @@ static inline bool ptlock_init(struct page *page)</span>
<span class="quote">&gt;  	 * with 0. Make sure nobody took it in use in between.</span>
<span class="quote">&gt;  	 *</span>
<span class="quote">&gt;  	 * It can happen if arch try to use slab for page table allocation:</span>
<span class="quote">&gt; -	 * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail</span>
<span class="quote">&gt; -	 * pages), which share storage with page-&gt;ptl.</span>
<span class="quote">&gt; +	 * slab code uses page-&gt;slab_cache, which share storage with page-&gt;ptl.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);</span>
<span class="quote">&gt;  	if (!ptlock_alloc(page))</span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index d8a43524a45c..385604afbafa 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -111,7 +111,13 @@ struct page {</span>
<span class="quote">&gt;  		};</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	/* Third double word block */</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Third double word block</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * WARNING: bit 0 of the first word encode PageTail(). That means</span>
<span class="quote">&gt; +	 * the rest users of the storage space MUST NOT use the bit to</span>
<span class="quote">&gt; +	 * avoid collision and false-positive PageTail().</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	union {</span>
<span class="quote">&gt;  		struct list_head lru;	/* Pageout list, eg. active_list</span>
<span class="quote">&gt;  					 * protected by zone-&gt;lru_lock !</span>
<span class="quote">&gt; @@ -132,14 +138,23 @@ struct page {</span>
<span class="quote">&gt;  		struct rcu_head rcu_head;	/* Used by SLAB</span>
<span class="quote">&gt;  						 * when destroying via RCU</span>
<span class="quote">&gt;  						 */</span>
<span class="quote">&gt; -		/* First tail page of compound page */</span>
<span class="quote">&gt; +		/* Tail pages of compound page */</span>
<span class="quote">&gt;  		struct {</span>
<span class="quote">&gt; +			unsigned long compound_head; /* If bit zero is set */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* First tail page only */</span>
<span class="quote">&gt;  			unsigned short int compound_dtor;</span>
<span class="quote">&gt;  			unsigned short int compound_order;</span>
<span class="quote">&gt;  		};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt; -		pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="quote">&gt; +		struct {</span>
<span class="quote">&gt; +			unsigned long __pad;	/* do not overlay pmd_huge_pte</span>
<span class="quote">&gt; +						 * with compound_head to avoid</span>
<span class="quote">&gt; +						 * possible bit 0 collision.</span>
<span class="quote">&gt; +						 */</span>
<span class="quote">&gt; +			pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="quote">&gt; +		};</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -160,7 +175,6 @@ struct page {</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */</span>
<span class="quote">&gt; -		struct page *first_page;	/* Compound tail pages */</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="quote">&gt; index 416509e26d6d..4fdfb5ed4b43 100644</span>
<span class="quote">&gt; --- a/include/linux/page-flags.h</span>
<span class="quote">&gt; +++ b/include/linux/page-flags.h</span>
<span class="quote">&gt; @@ -86,12 +86,7 @@ enum pageflags {</span>
<span class="quote">&gt;  	PG_private,		/* If pagecache, has fs-private data */</span>
<span class="quote">&gt;  	PG_private_2,		/* If pagecache, has fs aux data */</span>
<span class="quote">&gt;  	PG_writeback,		/* Page is under writeback */</span>
<span class="quote">&gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt;  	PG_head,		/* A head page */</span>
<span class="quote">&gt; -	PG_tail,		/* A tail page */</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	PG_compound,		/* A compound page */</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  	PG_swapcache,		/* Swap page: swp_entry_t in private */</span>
<span class="quote">&gt;  	PG_mappedtodisk,	/* Has blocks allocated on-disk */</span>
<span class="quote">&gt;  	PG_reclaim,		/* To be reclaimed asap */</span>
<span class="quote">&gt; @@ -398,85 +393,46 @@ static inline void set_page_writeback_keepwrite(struct page *page)</span>
<span class="quote">&gt;  	test_set_page_writeback_keepwrite(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * System with lots of page flags available. This allows separate</span>
<span class="quote">&gt; - * flags for PageHead() and PageTail() checks of compound pages so that bit</span>
<span class="quote">&gt; - * tests can be used in performance sensitive paths. PageCompound is</span>
<span class="quote">&gt; - * generally not used in hot code paths except arch/powerpc/mm/init_64.c</span>
<span class="quote">&gt; - * and arch/powerpc/kvm/book3s_64_vio_hv.c which use it to detect huge pages</span>
<span class="quote">&gt; - * and avoid handling those in real mode.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt;  __PAGEFLAG(Head, head) CLEARPAGEFLAG(Head, head)</span>
<span class="quote">&gt; -__PAGEFLAG(Tail, tail)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline int PageCompound(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return page-&gt;flags &amp; ((1L &lt;&lt; PG_head) | (1L &lt;&lt; PG_tail));</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; -static inline void ClearPageCompound(struct page *page)</span>
<span class="quote">&gt; +static inline int PageTail(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(!PageHead(page));</span>
<span class="quote">&gt; -	ClearPageHead(page);</span>
<span class="quote">&gt; +	return READ_ONCE(page-&gt;compound_head) &amp; 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Reduce page flag use as much as possible by overlapping</span>
<span class="quote">&gt; - * compound page flags with the flags used for page cache pages. Possible</span>
<span class="quote">&gt; - * because PageCompound is always set for compound pages and not for</span>
<span class="quote">&gt; - * pages on the LRU and/or pagecache.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -TESTPAGEFLAG(Compound, compound)</span>
<span class="quote">&gt; -__SETPAGEFLAG(Head, compound)  __CLEARPAGEFLAG(Head, compound)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * PG_reclaim is used in combination with PG_compound to mark the</span>
<span class="quote">&gt; - * head and tail of a compound page. This saves one page flag</span>
<span class="quote">&gt; - * but makes it impossible to use compound pages for the page cache.</span>
<span class="quote">&gt; - * The PG_reclaim bit would have to be used for reclaim or readahead</span>
<span class="quote">&gt; - * if compound pages enter the page cache.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * PG_compound &amp; PG_reclaim	=&gt; Tail page</span>
<span class="quote">&gt; - * PG_compound &amp; ~PG_reclaim	=&gt; Head page</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -#define PG_head_mask ((1L &lt;&lt; PG_compound))</span>
<span class="quote">&gt; -#define PG_head_tail_mask ((1L &lt;&lt; PG_compound) | (1L &lt;&lt; PG_reclaim))</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline int PageHead(struct page *page)</span>
<span class="quote">&gt; +static inline void set_compound_head(struct page *page, struct page *head)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_mask);</span>
<span class="quote">&gt; +	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline int PageTail(struct page *page)</span>
<span class="quote">&gt; +static inline void clear_compound_head(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_tail_mask);</span>
<span class="quote">&gt; +	WRITE_ONCE(page-&gt;compound_head, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline void __SetPageTail(struct page *page)</span>
<span class="quote">&gt; +static inline struct page *compound_head(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	page-&gt;flags |= PG_head_tail_mask;</span>
<span class="quote">&gt; +	unsigned long head = READ_ONCE(page-&gt;compound_head);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(head &amp; 1))</span>
<span class="quote">&gt; +		return (struct page *) (head - 1);</span>
<span class="quote">&gt; +	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline void __ClearPageTail(struct page *page)</span>
<span class="quote">&gt; +static inline int PageCompound(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	page-&gt;flags &amp;= ~PG_head_tail_mask;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; +	return PageHead(page) || PageTail(page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;  static inline void ClearPageCompound(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON((page-&gt;flags &amp; PG_head_tail_mask) != (1 &lt;&lt; PG_compound));</span>
<span class="quote">&gt; -	clear_bit(PG_compound, &amp;page-&gt;flags);</span>
<span class="quote">&gt; +	BUG_ON(!PageHead(page));</span>
<span class="quote">&gt; +	ClearPageHead(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -#endif /* !PAGEFLAGS_EXTENDED */</span>
<span class="quote">&gt; +#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt;  int PageHuge(struct page *page);</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index 0d9fdcd01e47..97a4e06b15c0 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -200,18 +200,6 @@ config MEMORY_HOTREMOVE</span>
<span class="quote">&gt;  	depends on MEMORY_HOTPLUG &amp;&amp; ARCH_ENABLE_MEMORY_HOTREMOVE</span>
<span class="quote">&gt;  	depends on MIGRATION</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -#</span>
<span class="quote">&gt; -# If we have space for more page flags then we can enable additional</span>
<span class="quote">&gt; -# optimizations and functionality.</span>
<span class="quote">&gt; -#</span>
<span class="quote">&gt; -# Regular Sparsemem takes page flag bits for the sectionid if it does not</span>
<span class="quote">&gt; -# use a virtual memmap. Disable extended page flags for 32 bit platforms</span>
<span class="quote">&gt; -# that require the use of a sectionid in the page flags.</span>
<span class="quote">&gt; -#</span>
<span class="quote">&gt; -config PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt; -	def_bool y</span>
<span class="quote">&gt; -	depends on 64BIT || SPARSEMEM_VMEMMAP || !SPARSEMEM</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  # Heavily threaded applications may benefit from splitting the mm-wide</span>
<span class="quote">&gt;  # page_table_lock, so that faults on different parts of the user address</span>
<span class="quote">&gt;  # space can be handled with less contention: split it at this NR_CPUS.</span>
<span class="quote">&gt; diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="quote">&gt; index 6c1b3ea61bfd..594073e9f840 100644</span>
<span class="quote">&gt; --- a/mm/debug.c</span>
<span class="quote">&gt; +++ b/mm/debug.c</span>
<span class="quote">&gt; @@ -25,12 +25,7 @@ static const struct trace_print_flags pageflag_names[] = {</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_private,		&quot;private&quot;	},</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_private_2,		&quot;private_2&quot;	},</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_writeback,		&quot;writeback&quot;	},</span>
<span class="quote">&gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_head,		&quot;head&quot;		},</span>
<span class="quote">&gt; -	{1UL &lt;&lt; PG_tail,		&quot;tail&quot;		},</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	{1UL &lt;&lt; PG_compound,		&quot;compound&quot;	},</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_swapcache,		&quot;swapcache&quot;	},</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_mappedtodisk,	&quot;mappedtodisk&quot;	},</span>
<span class="quote">&gt;  	{1UL &lt;&lt; PG_reclaim,		&quot;reclaim&quot;	},</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 4b06b8db9df2..55ef91c491d2 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1755,8 +1755,7 @@ static void __split_huge_page_refcount(struct page *page,</span>
<span class="quote">&gt;  				      (1L &lt;&lt; PG_unevictable)));</span>
<span class="quote">&gt;  		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -		/* clear PageTail before overwriting first_page */</span>
<span class="quote">&gt; -		smp_wmb();</span>
<span class="quote">&gt; +		clear_compound_head(page_tail);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (page_is_young(page))</span>
<span class="quote">&gt;  			set_page_young(page_tail);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 713c87352100..8f751e6eebee 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1001,9 +1001,8 @@ static void destroy_compound_gigantic_page(struct page *page,</span>
<span class="quote">&gt;  	struct page *p = page + 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	for (i = 1; i &lt; nr_pages; i++, p = mem_map_next(p, page, i)) {</span>
<span class="quote">&gt; -		__ClearPageTail(p);</span>
<span class="quote">&gt; +		clear_compound_head(p);</span>
<span class="quote">&gt;  		set_page_refcounted(p);</span>
<span class="quote">&gt; -		p-&gt;first_page = NULL;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	set_compound_order(page, 0);</span>
<span class="quote">&gt; @@ -1276,10 +1275,7 @@ static void prep_compound_gigantic_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		__ClearPageReserved(p);</span>
<span class="quote">&gt;  		set_page_count(p, 0);</span>
<span class="quote">&gt; -		p-&gt;first_page = page;</span>
<span class="quote">&gt; -		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="quote">&gt; -		smp_wmb();</span>
<span class="quote">&gt; -		__SetPageTail(p);</span>
<span class="quote">&gt; +		set_compound_head(p, page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c</span>
<span class="quote">&gt; index 6e0057439a46..6a4426372698 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb_cgroup.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb_cgroup.c</span>
<span class="quote">&gt; @@ -384,7 +384,7 @@ void __init hugetlb_cgroup_file_init(void)</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Add cgroup control files only if the huge page consists</span>
<span class="quote">&gt;  		 * of more than two normal pages. This is because we use</span>
<span class="quote">&gt; -		 * page[2].lru.next for storing cgroup details.</span>
<span class="quote">&gt; +		 * page[2].private for storing cgroup details.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		if (huge_page_order(h) &gt;= HUGETLB_CGROUP_MIN_ORDER)</span>
<span class="quote">&gt;  			__hugetlb_cgroup_file_init(hstate_index(h));</span>
<span class="quote">&gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt; index bc0fa9a69e46..26027c62577b 100644</span>
<span class="quote">&gt; --- a/mm/internal.h</span>
<span class="quote">&gt; +++ b/mm/internal.h</span>
<span class="quote">&gt; @@ -61,9 +61,9 @@ static inline void __get_page_tail_foll(struct page *page,</span>
<span class="quote">&gt;  	 * speculative page access (like in</span>
<span class="quote">&gt;  	 * page_cache_get_speculative()) on tail pages.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt; +	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt;  	if (get_page_head)</span>
<span class="quote">&gt; -		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="quote">&gt; +		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>
<span class="quote">&gt;  	get_huge_page_tail(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt; index 95882692e747..8006e582015a 100644</span>
<span class="quote">&gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt; @@ -775,8 +775,6 @@ static int me_huge_page(struct page *p, unsigned long pfn)</span>
<span class="quote">&gt;  #define lru		(1UL &lt;&lt; PG_lru)</span>
<span class="quote">&gt;  #define swapbacked	(1UL &lt;&lt; PG_swapbacked)</span>
<span class="quote">&gt;  #define head		(1UL &lt;&lt; PG_head)</span>
<span class="quote">&gt; -#define tail		(1UL &lt;&lt; PG_tail)</span>
<span class="quote">&gt; -#define compound	(1UL &lt;&lt; PG_compound)</span>
<span class="quote">&gt;  #define slab		(1UL &lt;&lt; PG_slab)</span>
<span class="quote">&gt;  #define reserved	(1UL &lt;&lt; PG_reserved)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -799,12 +797,7 @@ static struct page_state {</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	{ slab,		slab,		MF_MSG_SLAB,	me_kernel },</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt;  	{ head,		head,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -	{ tail,		tail,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	{ compound,	compound,	MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	{ sc|dirty,	sc|dirty,	MF_MSG_DIRTY_SWAPCACHE,	me_swapcache_dirty },</span>
<span class="quote">&gt;  	{ sc|dirty,	sc,		MF_MSG_CLEAN_SWAPCACHE,	me_swapcache_clean },</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index ab1232292348..acd2e9346d6e 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -445,15 +445,15 @@ out:</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Higher-order pages are called &quot;compound pages&quot;.  They are structured thusly:</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The first PAGE_SIZE page is called the &quot;head page&quot;.</span>
<span class="quote">&gt; + * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;.</span>
<span class="quote">&gt; + * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded</span>
<span class="quote">&gt; + * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * All pages have PG_compound set.  All tail pages have their -&gt;first_page</span>
<span class="quote">&gt; - * pointing at the head page.</span>
<span class="quote">&gt; + * The first tail page&#39;s -&gt;compound_dtor holds the offset in array of compound</span>
<span class="quote">&gt; + * page destructors. See compound_page_dtors.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The first tail page&#39;s -&gt;lru.next holds the address of the compound page&#39;s</span>
<span class="quote">&gt; - * put_page() function.  Its -&gt;lru.prev holds the order of allocation.</span>
<span class="quote">&gt; + * The first tail page&#39;s -&gt;compound_order holds the order of allocation.</span>
<span class="quote">&gt;   * This usage means that zero-order pages may not be compound.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -473,10 +473,7 @@ void prep_compound_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt;  	for (i = 1; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt;  		struct page *p = page + i;</span>
<span class="quote">&gt;  		set_page_count(p, 0);</span>
<span class="quote">&gt; -		p-&gt;first_page = page;</span>
<span class="quote">&gt; -		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="quote">&gt; -		smp_wmb();</span>
<span class="quote">&gt; -		__SetPageTail(p);</span>
<span class="quote">&gt; +		set_compound_head(p, page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -855,17 +852,30 @@ static void free_one_page(struct zone *zone,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int free_tail_pages_check(struct page *head_page, struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (!IS_ENABLED(CONFIG_DEBUG_VM))</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; +	int ret = 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We rely page-&gt;lru.next never has bit 0 set, unless the page</span>
<span class="quote">&gt; +	 * is PageTail(). Let&#39;s make sure that&#39;s true even for poisoned -&gt;lru.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	BUILD_BUG_ON((unsigned long)LIST_POISON1 &amp; 1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {</span>
<span class="quote">&gt; +		ret = 0;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (unlikely(!PageTail(page))) {</span>
<span class="quote">&gt;  		bad_page(page, &quot;PageTail not set&quot;, 0);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	if (unlikely(page-&gt;first_page != head_page)) {</span>
<span class="quote">&gt; -		bad_page(page, &quot;first_page not consistent&quot;, 0);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; +	if (unlikely(compound_head(page) != head_page)) {</span>
<span class="quote">&gt; +		bad_page(page, &quot;compound_head not consistent&quot;, 0);</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; +	ret = 0;</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	clear_compound_head(page);</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static void __meminit __init_single_page(struct page *page, unsigned long pfn,</span>
<span class="quote">&gt; @@ -913,6 +923,8 @@ static void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	/* Avoid false-positive PageTail() */</span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;pfn_to_page(pfn)-&gt;lru);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; index 983f692a47fd..39395fb549c0 100644</span>
<span class="quote">&gt; --- a/mm/swap.c</span>
<span class="quote">&gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; @@ -201,7 +201,7 @@ out_put_single:</span>
<span class="quote">&gt;  				__put_single_page(page);</span>
<span class="quote">&gt;  			return;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="quote">&gt; +		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * We can release the refcount taken by</span>
<span class="quote">&gt;  		 * get_page_unless_zero() now that</span>
<span class="quote">&gt; @@ -262,7 +262,7 @@ static void put_compound_page(struct page *page)</span>
<span class="quote">&gt;  	 *  Case 3 is possible, as we may race with</span>
<span class="quote">&gt;  	 *  __split_huge_page_refcount tearing down a THP page.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	page_head = compound_head_by_tail(page);</span>
<span class="quote">&gt; +	page_head = compound_head(page);</span>
<span class="quote">&gt;  	if (!__compound_tail_refcounted(page_head))</span>
<span class="quote">&gt;  		put_unrefcounted_compound_page(page_head, page);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.5.1</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/split_page_table_lock b/Documentation/vm/split_page_table_lock</span>
<span class="p_header">index 6dea4fd5c961..62842a857dab 100644</span>
<span class="p_header">--- a/Documentation/vm/split_page_table_lock</span>
<span class="p_header">+++ b/Documentation/vm/split_page_table_lock</span>
<span class="p_chunk">@@ -54,8 +54,8 @@</span> <span class="p_context"> everything required is done by pgtable_page_ctor() and pgtable_page_dtor(),</span>
 which must be called on PTE table allocation / freeing.
 
 Make sure the architecture doesn&#39;t use slab allocator for page table
<span class="p_del">-allocation: slab uses page-&gt;slab_cache and page-&gt;first_page for its pages.</span>
<span class="p_del">-These fields share storage with page-&gt;ptl.</span>
<span class="p_add">+allocation: slab uses page-&gt;slab_cache for its pages.</span>
<span class="p_add">+This field shares storage with page-&gt;ptl.</span>
 
 PMD split lock only makes sense if you have more than two page table
 levels.
<span class="p_header">diff --git a/arch/xtensa/configs/iss_defconfig b/arch/xtensa/configs/iss_defconfig</span>
<span class="p_header">index f3dfe0d921c2..44c6764d9146 100644</span>
<span class="p_header">--- a/arch/xtensa/configs/iss_defconfig</span>
<span class="p_header">+++ b/arch/xtensa/configs/iss_defconfig</span>
<span class="p_chunk">@@ -169,7 +169,6 @@</span> <span class="p_context"> CONFIG_FLATMEM_MANUAL=y</span>
 # CONFIG_SPARSEMEM_MANUAL is not set
 CONFIG_FLATMEM=y
 CONFIG_FLAT_NODE_MEM_MAP=y
<span class="p_del">-CONFIG_PAGEFLAGS_EXTENDED=y</span>
 CONFIG_SPLIT_PTLOCK_CPUS=4
 # CONFIG_PHYS_ADDR_T_64BIT is not set
 CONFIG_ZONE_DMA_FLAG=1
<span class="p_header">diff --git a/include/linux/hugetlb_cgroup.h b/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">index bcc853eccc85..75e34b900748 100644</span>
<span class="p_header">--- a/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">+++ b/include/linux/hugetlb_cgroup.h</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"> static inline struct hugetlb_cgroup *hugetlb_cgroup_from_page(struct page *page)</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return NULL;
<span class="p_del">-	return (struct hugetlb_cgroup *)page[2].lru.next;</span>
<span class="p_add">+	return (struct hugetlb_cgroup *)page[2].private;</span>
 }
 
 static inline
<span class="p_chunk">@@ -42,7 +42,7 @@</span> <span class="p_context"> int set_hugetlb_cgroup(struct page *page, struct hugetlb_cgroup *h_cg)</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return -1;
<span class="p_del">-	page[2].lru.next = (void *)h_cg;</span>
<span class="p_add">+	page[2].private	= (unsigned long)h_cg;</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 107ba3798d08..17154f5e1c2a 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -426,46 +426,6 @@</span> <span class="p_context"> static inline void compound_unlock_irqrestore(struct page *page,</span>
 #endif
 }
 
<span class="p_del">-static inline struct page *compound_head_by_tail(struct page *tail)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *head = tail-&gt;first_page;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * page-&gt;first_page may be a dangling pointer to an old</span>
<span class="p_del">-	 * compound page, so recheck that it is still a tail</span>
<span class="p_del">-	 * page before returning.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	smp_rmb();</span>
<span class="p_del">-	if (likely(PageTail(tail)))</span>
<span class="p_del">-		return head;</span>
<span class="p_del">-	return tail;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Since either compound page could be dismantled asynchronously in THP</span>
<span class="p_del">- * or we access asynchronously arbitrary positioned struct page, there</span>
<span class="p_del">- * would be tail flag race. To handle this race, we should call</span>
<span class="p_del">- * smp_rmb() before checking tail flag. compound_head_by_tail() did it.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct page *compound_head(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (unlikely(PageTail(page)))</span>
<span class="p_del">-		return compound_head_by_tail(page);</span>
<span class="p_del">-	return page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * If we access compound page synchronously such as access to</span>
<span class="p_del">- * allocated page, there is no need to handle tail flag race, so we can</span>
<span class="p_del">- * check tail flag directly without any synchronization primitive.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct page *compound_head_fast(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (unlikely(PageTail(page)))</span>
<span class="p_del">-		return page-&gt;first_page;</span>
<span class="p_del">-	return page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * The atomic page-&gt;_mapcount, starts from -1: so that transitions
  * both from it and to it can be tracked, using atomic_inc_and_test
<span class="p_chunk">@@ -514,7 +474,7 @@</span> <span class="p_context"> static inline void get_huge_page_tail(struct page *page)</span>
 	VM_BUG_ON_PAGE(!PageTail(page), page);
 	VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);
 	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);
<span class="p_del">-	if (compound_tail_refcounted(page-&gt;first_page))</span>
<span class="p_add">+	if (compound_tail_refcounted(compound_head(page)))</span>
 		atomic_inc(&amp;page-&gt;_mapcount);
 }
 
<span class="p_chunk">@@ -537,13 +497,7 @@</span> <span class="p_context"> static inline struct page *virt_to_head_page(const void *x)</span>
 {
 	struct page *page = virt_to_page(x);
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t need to worry about synchronization of tail flag</span>
<span class="p_del">-	 * when we call virt_to_head_page() since it is only called for</span>
<span class="p_del">-	 * already allocated page and this page won&#39;t be freed until</span>
<span class="p_del">-	 * this virt_to_head_page() is finished. So use _fast variant.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return compound_head_fast(page);</span>
<span class="p_add">+	return compound_head(page);</span>
 }
 
 /*
<span class="p_chunk">@@ -1561,8 +1515,7 @@</span> <span class="p_context"> static inline bool ptlock_init(struct page *page)</span>
 	 * with 0. Make sure nobody took it in use in between.
 	 *
 	 * It can happen if arch try to use slab for page table allocation:
<span class="p_del">-	 * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail</span>
<span class="p_del">-	 * pages), which share storage with page-&gt;ptl.</span>
<span class="p_add">+	 * slab code uses page-&gt;slab_cache, which share storage with page-&gt;ptl.</span>
 	 */
 	VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);
 	if (!ptlock_alloc(page))
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index d8a43524a45c..385604afbafa 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -111,7 +111,13 @@</span> <span class="p_context"> struct page {</span>
 		};
 	};
 
<span class="p_del">-	/* Third double word block */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Third double word block</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * WARNING: bit 0 of the first word encode PageTail(). That means</span>
<span class="p_add">+	 * the rest users of the storage space MUST NOT use the bit to</span>
<span class="p_add">+	 * avoid collision and false-positive PageTail().</span>
<span class="p_add">+	 */</span>
 	union {
 		struct list_head lru;	/* Pageout list, eg. active_list
 					 * protected by zone-&gt;lru_lock !
<span class="p_chunk">@@ -132,14 +138,23 @@</span> <span class="p_context"> struct page {</span>
 		struct rcu_head rcu_head;	/* Used by SLAB
 						 * when destroying via RCU
 						 */
<span class="p_del">-		/* First tail page of compound page */</span>
<span class="p_add">+		/* Tail pages of compound page */</span>
 		struct {
<span class="p_add">+			unsigned long compound_head; /* If bit zero is set */</span>
<span class="p_add">+</span>
<span class="p_add">+			/* First tail page only */</span>
 			unsigned short int compound_dtor;
 			unsigned short int compound_order;
 		};
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; USE_SPLIT_PMD_PTLOCKS
<span class="p_del">-		pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			unsigned long __pad;	/* do not overlay pmd_huge_pte</span>
<span class="p_add">+						 * with compound_head to avoid</span>
<span class="p_add">+						 * possible bit 0 collision.</span>
<span class="p_add">+						 */</span>
<span class="p_add">+			pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="p_add">+		};</span>
 #endif
 	};
 
<span class="p_chunk">@@ -160,7 +175,6 @@</span> <span class="p_context"> struct page {</span>
 #endif
 #endif
 		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */
<span class="p_del">-		struct page *first_page;	/* Compound tail pages */</span>
 	};
 
 #ifdef CONFIG_MEMCG
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index 416509e26d6d..4fdfb5ed4b43 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -86,12 +86,7 @@</span> <span class="p_context"> enum pageflags {</span>
 	PG_private,		/* If pagecache, has fs-private data */
 	PG_private_2,		/* If pagecache, has fs aux data */
 	PG_writeback,		/* Page is under writeback */
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	PG_head,		/* A head page */
<span class="p_del">-	PG_tail,		/* A tail page */</span>
<span class="p_del">-#else</span>
<span class="p_del">-	PG_compound,		/* A compound page */</span>
<span class="p_del">-#endif</span>
 	PG_swapcache,		/* Swap page: swp_entry_t in private */
 	PG_mappedtodisk,	/* Has blocks allocated on-disk */
 	PG_reclaim,		/* To be reclaimed asap */
<span class="p_chunk">@@ -398,85 +393,46 @@</span> <span class="p_context"> static inline void set_page_writeback_keepwrite(struct page *page)</span>
 	test_set_page_writeback_keepwrite(page);
 }
 
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="p_del">-/*</span>
<span class="p_del">- * System with lots of page flags available. This allows separate</span>
<span class="p_del">- * flags for PageHead() and PageTail() checks of compound pages so that bit</span>
<span class="p_del">- * tests can be used in performance sensitive paths. PageCompound is</span>
<span class="p_del">- * generally not used in hot code paths except arch/powerpc/mm/init_64.c</span>
<span class="p_del">- * and arch/powerpc/kvm/book3s_64_vio_hv.c which use it to detect huge pages</span>
<span class="p_del">- * and avoid handling those in real mode.</span>
<span class="p_del">- */</span>
 __PAGEFLAG(Head, head) CLEARPAGEFLAG(Head, head)
<span class="p_del">-__PAGEFLAG(Tail, tail)</span>
 
<span class="p_del">-static inline int PageCompound(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return page-&gt;flags &amp; ((1L &lt;&lt; PG_head) | (1L &lt;&lt; PG_tail));</span>
<span class="p_del">-</span>
<span class="p_del">-}</span>
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-static inline void ClearPageCompound(struct page *page)</span>
<span class="p_add">+static inline int PageTail(struct page *page)</span>
 {
<span class="p_del">-	BUG_ON(!PageHead(page));</span>
<span class="p_del">-	ClearPageHead(page);</span>
<span class="p_add">+	return READ_ONCE(page-&gt;compound_head) &amp; 1;</span>
 }
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
 
<span class="p_del">-#else</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Reduce page flag use as much as possible by overlapping</span>
<span class="p_del">- * compound page flags with the flags used for page cache pages. Possible</span>
<span class="p_del">- * because PageCompound is always set for compound pages and not for</span>
<span class="p_del">- * pages on the LRU and/or pagecache.</span>
<span class="p_del">- */</span>
<span class="p_del">-TESTPAGEFLAG(Compound, compound)</span>
<span class="p_del">-__SETPAGEFLAG(Head, compound)  __CLEARPAGEFLAG(Head, compound)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * PG_reclaim is used in combination with PG_compound to mark the</span>
<span class="p_del">- * head and tail of a compound page. This saves one page flag</span>
<span class="p_del">- * but makes it impossible to use compound pages for the page cache.</span>
<span class="p_del">- * The PG_reclaim bit would have to be used for reclaim or readahead</span>
<span class="p_del">- * if compound pages enter the page cache.</span>
<span class="p_del">- *</span>
<span class="p_del">- * PG_compound &amp; PG_reclaim	=&gt; Tail page</span>
<span class="p_del">- * PG_compound &amp; ~PG_reclaim	=&gt; Head page</span>
<span class="p_del">- */</span>
<span class="p_del">-#define PG_head_mask ((1L &lt;&lt; PG_compound))</span>
<span class="p_del">-#define PG_head_tail_mask ((1L &lt;&lt; PG_compound) | (1L &lt;&lt; PG_reclaim))</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int PageHead(struct page *page)</span>
<span class="p_add">+static inline void set_compound_head(struct page *page, struct page *head)</span>
 {
<span class="p_del">-	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_mask);</span>
<span class="p_add">+	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);</span>
 }
 
<span class="p_del">-static inline int PageTail(struct page *page)</span>
<span class="p_add">+static inline void clear_compound_head(struct page *page)</span>
 {
<span class="p_del">-	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_tail_mask);</span>
<span class="p_add">+	WRITE_ONCE(page-&gt;compound_head, 0);</span>
 }
 
<span class="p_del">-static inline void __SetPageTail(struct page *page)</span>
<span class="p_add">+static inline struct page *compound_head(struct page *page)</span>
 {
<span class="p_del">-	page-&gt;flags |= PG_head_tail_mask;</span>
<span class="p_add">+	unsigned long head = READ_ONCE(page-&gt;compound_head);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(head &amp; 1))</span>
<span class="p_add">+		return (struct page *) (head - 1);</span>
<span class="p_add">+	return page;</span>
 }
 
<span class="p_del">-static inline void __ClearPageTail(struct page *page)</span>
<span class="p_add">+static inline int PageCompound(struct page *page)</span>
 {
<span class="p_del">-	page-&gt;flags &amp;= ~PG_head_tail_mask;</span>
<span class="p_del">-}</span>
<span class="p_add">+	return PageHead(page) || PageTail(page);</span>
 
<span class="p_add">+}</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
<span class="p_del">-	BUG_ON((page-&gt;flags &amp; PG_head_tail_mask) != (1 &lt;&lt; PG_compound));</span>
<span class="p_del">-	clear_bit(PG_compound, &amp;page-&gt;flags);</span>
<span class="p_add">+	BUG_ON(!PageHead(page));</span>
<span class="p_add">+	ClearPageHead(page);</span>
 }
 #endif
 
<span class="p_del">-#endif /* !PAGEFLAGS_EXTENDED */</span>
<span class="p_add">+#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
 
 #ifdef CONFIG_HUGETLB_PAGE
 int PageHuge(struct page *page);
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 0d9fdcd01e47..97a4e06b15c0 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -200,18 +200,6 @@</span> <span class="p_context"> config MEMORY_HOTREMOVE</span>
 	depends on MEMORY_HOTPLUG &amp;&amp; ARCH_ENABLE_MEMORY_HOTREMOVE
 	depends on MIGRATION
 
<span class="p_del">-#</span>
<span class="p_del">-# If we have space for more page flags then we can enable additional</span>
<span class="p_del">-# optimizations and functionality.</span>
<span class="p_del">-#</span>
<span class="p_del">-# Regular Sparsemem takes page flag bits for the sectionid if it does not</span>
<span class="p_del">-# use a virtual memmap. Disable extended page flags for 32 bit platforms</span>
<span class="p_del">-# that require the use of a sectionid in the page flags.</span>
<span class="p_del">-#</span>
<span class="p_del">-config PAGEFLAGS_EXTENDED</span>
<span class="p_del">-	def_bool y</span>
<span class="p_del">-	depends on 64BIT || SPARSEMEM_VMEMMAP || !SPARSEMEM</span>
<span class="p_del">-</span>
 # Heavily threaded applications may benefit from splitting the mm-wide
 # page_table_lock, so that faults on different parts of the user address
 # space can be handled with less contention: split it at this NR_CPUS.
<span class="p_header">diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="p_header">index 6c1b3ea61bfd..594073e9f840 100644</span>
<span class="p_header">--- a/mm/debug.c</span>
<span class="p_header">+++ b/mm/debug.c</span>
<span class="p_chunk">@@ -25,12 +25,7 @@</span> <span class="p_context"> static const struct trace_print_flags pageflag_names[] = {</span>
 	{1UL &lt;&lt; PG_private,		&quot;private&quot;	},
 	{1UL &lt;&lt; PG_private_2,		&quot;private_2&quot;	},
 	{1UL &lt;&lt; PG_writeback,		&quot;writeback&quot;	},
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	{1UL &lt;&lt; PG_head,		&quot;head&quot;		},
<span class="p_del">-	{1UL &lt;&lt; PG_tail,		&quot;tail&quot;		},</span>
<span class="p_del">-#else</span>
<span class="p_del">-	{1UL &lt;&lt; PG_compound,		&quot;compound&quot;	},</span>
<span class="p_del">-#endif</span>
 	{1UL &lt;&lt; PG_swapcache,		&quot;swapcache&quot;	},
 	{1UL &lt;&lt; PG_mappedtodisk,	&quot;mappedtodisk&quot;	},
 	{1UL &lt;&lt; PG_reclaim,		&quot;reclaim&quot;	},
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 4b06b8db9df2..55ef91c491d2 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1755,8 +1755,7 @@</span> <span class="p_context"> static void __split_huge_page_refcount(struct page *page,</span>
 				      (1L &lt;&lt; PG_unevictable)));
 		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);
 
<span class="p_del">-		/* clear PageTail before overwriting first_page */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_add">+		clear_compound_head(page_tail);</span>
 
 		if (page_is_young(page))
 			set_page_young(page_tail);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 713c87352100..8f751e6eebee 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1001,9 +1001,8 @@</span> <span class="p_context"> static void destroy_compound_gigantic_page(struct page *page,</span>
 	struct page *p = page + 1;
 
 	for (i = 1; i &lt; nr_pages; i++, p = mem_map_next(p, page, i)) {
<span class="p_del">-		__ClearPageTail(p);</span>
<span class="p_add">+		clear_compound_head(p);</span>
 		set_page_refcounted(p);
<span class="p_del">-		p-&gt;first_page = NULL;</span>
 	}
 
 	set_compound_order(page, 0);
<span class="p_chunk">@@ -1276,10 +1275,7 @@</span> <span class="p_context"> static void prep_compound_gigantic_page(struct page *page, unsigned long order)</span>
 		 */
 		__ClearPageReserved(p);
 		set_page_count(p, 0);
<span class="p_del">-		p-&gt;first_page = page;</span>
<span class="p_del">-		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_del">-		__SetPageTail(p);</span>
<span class="p_add">+		set_compound_head(p, page);</span>
 	}
 }
 
<span class="p_header">diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c</span>
<span class="p_header">index 6e0057439a46..6a4426372698 100644</span>
<span class="p_header">--- a/mm/hugetlb_cgroup.c</span>
<span class="p_header">+++ b/mm/hugetlb_cgroup.c</span>
<span class="p_chunk">@@ -384,7 +384,7 @@</span> <span class="p_context"> void __init hugetlb_cgroup_file_init(void)</span>
 		/*
 		 * Add cgroup control files only if the huge page consists
 		 * of more than two normal pages. This is because we use
<span class="p_del">-		 * page[2].lru.next for storing cgroup details.</span>
<span class="p_add">+		 * page[2].private for storing cgroup details.</span>
 		 */
 		if (huge_page_order(h) &gt;= HUGETLB_CGROUP_MIN_ORDER)
 			__hugetlb_cgroup_file_init(hstate_index(h));
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index bc0fa9a69e46..26027c62577b 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -61,9 +61,9 @@</span> <span class="p_context"> static inline void __get_page_tail_foll(struct page *page,</span>
 	 * speculative page access (like in
 	 * page_cache_get_speculative()) on tail pages.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
 	if (get_page_head)
<span class="p_del">-		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="p_add">+		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>
 	get_huge_page_tail(page);
 }
 
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index 95882692e747..8006e582015a 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -775,8 +775,6 @@</span> <span class="p_context"> static int me_huge_page(struct page *p, unsigned long pfn)</span>
 #define lru		(1UL &lt;&lt; PG_lru)
 #define swapbacked	(1UL &lt;&lt; PG_swapbacked)
 #define head		(1UL &lt;&lt; PG_head)
<span class="p_del">-#define tail		(1UL &lt;&lt; PG_tail)</span>
<span class="p_del">-#define compound	(1UL &lt;&lt; PG_compound)</span>
 #define slab		(1UL &lt;&lt; PG_slab)
 #define reserved	(1UL &lt;&lt; PG_reserved)
 
<span class="p_chunk">@@ -799,12 +797,7 @@</span> <span class="p_context"> static struct page_state {</span>
 	 */
 	{ slab,		slab,		MF_MSG_SLAB,	me_kernel },
 
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	{ head,		head,		MF_MSG_HUGE,		me_huge_page },
<span class="p_del">-	{ tail,		tail,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="p_del">-#else</span>
<span class="p_del">-	{ compound,	compound,	MF_MSG_HUGE,		me_huge_page },</span>
<span class="p_del">-#endif</span>
 
 	{ sc|dirty,	sc|dirty,	MF_MSG_DIRTY_SWAPCACHE,	me_swapcache_dirty },
 	{ sc|dirty,	sc,		MF_MSG_CLEAN_SWAPCACHE,	me_swapcache_clean },
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index ab1232292348..acd2e9346d6e 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -445,15 +445,15 @@</span> <span class="p_context"> out:</span>
 /*
  * Higher-order pages are called &quot;compound pages&quot;.  They are structured thusly:
  *
<span class="p_del">- * The first PAGE_SIZE page is called the &quot;head page&quot;.</span>
<span class="p_add">+ * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.</span>
  *
<span class="p_del">- * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;.</span>
<span class="p_add">+ * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded</span>
<span class="p_add">+ * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.</span>
  *
<span class="p_del">- * All pages have PG_compound set.  All tail pages have their -&gt;first_page</span>
<span class="p_del">- * pointing at the head page.</span>
<span class="p_add">+ * The first tail page&#39;s -&gt;compound_dtor holds the offset in array of compound</span>
<span class="p_add">+ * page destructors. See compound_page_dtors.</span>
  *
<span class="p_del">- * The first tail page&#39;s -&gt;lru.next holds the address of the compound page&#39;s</span>
<span class="p_del">- * put_page() function.  Its -&gt;lru.prev holds the order of allocation.</span>
<span class="p_add">+ * The first tail page&#39;s -&gt;compound_order holds the order of allocation.</span>
  * This usage means that zero-order pages may not be compound.
  */
 
<span class="p_chunk">@@ -473,10 +473,7 @@</span> <span class="p_context"> void prep_compound_page(struct page *page, unsigned long order)</span>
 	for (i = 1; i &lt; nr_pages; i++) {
 		struct page *p = page + i;
 		set_page_count(p, 0);
<span class="p_del">-		p-&gt;first_page = page;</span>
<span class="p_del">-		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_del">-		__SetPageTail(p);</span>
<span class="p_add">+		set_compound_head(p, page);</span>
 	}
 }
 
<span class="p_chunk">@@ -855,17 +852,30 @@</span> <span class="p_context"> static void free_one_page(struct zone *zone,</span>
 
 static int free_tail_pages_check(struct page *head_page, struct page *page)
 {
<span class="p_del">-	if (!IS_ENABLED(CONFIG_DEBUG_VM))</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+	int ret = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We rely page-&gt;lru.next never has bit 0 set, unless the page</span>
<span class="p_add">+	 * is PageTail(). Let&#39;s make sure that&#39;s true even for poisoned -&gt;lru.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON((unsigned long)LIST_POISON1 &amp; 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	if (unlikely(!PageTail(page))) {
 		bad_page(page, &quot;PageTail not set&quot;, 0);
<span class="p_del">-		return 1;</span>
<span class="p_add">+		goto out;</span>
 	}
<span class="p_del">-	if (unlikely(page-&gt;first_page != head_page)) {</span>
<span class="p_del">-		bad_page(page, &quot;first_page not consistent&quot;, 0);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+	if (unlikely(compound_head(page) != head_page)) {</span>
<span class="p_add">+		bad_page(page, &quot;compound_head not consistent&quot;, 0);</span>
<span class="p_add">+		goto out;</span>
 	}
<span class="p_del">-	return 0;</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	clear_compound_head(page);</span>
<span class="p_add">+	return ret;</span>
 }
 
 static void __meminit __init_single_page(struct page *page, unsigned long pfn,
<span class="p_chunk">@@ -913,6 +923,8 @@</span> <span class="p_context"> static void init_reserved_page(unsigned long pfn)</span>
 #else
 static inline void init_reserved_page(unsigned long pfn)
 {
<span class="p_add">+	/* Avoid false-positive PageTail() */</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;pfn_to_page(pfn)-&gt;lru);</span>
 }
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index 983f692a47fd..39395fb549c0 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> out_put_single:</span>
 				__put_single_page(page);
 			return;
 		}
<span class="p_del">-		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="p_add">+		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
 		/*
 		 * We can release the refcount taken by
 		 * get_page_unless_zero() now that
<span class="p_chunk">@@ -262,7 +262,7 @@</span> <span class="p_context"> static void put_compound_page(struct page *page)</span>
 	 *  Case 3 is possible, as we may race with
 	 *  __split_huge_page_refcount tearing down a THP page.
 	 */
<span class="p_del">-	page_head = compound_head_by_tail(page);</span>
<span class="p_add">+	page_head = compound_head(page);</span>
 	if (!__compound_tail_refcounted(page_head))
 		put_unrefcounted_compound_page(page_head, page);
 	else

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



