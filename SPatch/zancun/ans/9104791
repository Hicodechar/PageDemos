
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/asm changes for v4.7 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/asm changes for v4.7</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 16, 2016, 5:19 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160516171914.GA5238@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9104791/mbox/"
   >mbox</a>
|
   <a href="/patch/9104791/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9104791/">/patch/9104791/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id A9D19BF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:19:58 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id A408120268
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:19:49 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id B0F5020138
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:19:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754128AbcEPRT3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 16 May 2016 13:19:29 -0400
Received: from mail-wm0-f67.google.com ([74.125.82.67]:33012 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753183AbcEPRTX (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 16 May 2016 13:19:23 -0400
Received: by mail-wm0-f67.google.com with SMTP id r12so19602188wme.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 16 May 2016 10:19:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=HJvrFxA8Ijew5BFvQX8rtMugih3SuRbkKqBIx7SdYHA=;
	b=GrmUeezImMBh4SzlJ9FKADOcZ5fDHCGVmmVgdJCMUJALRj6VQkKRyYk3vcj9oZR5mN
	VDmlkT8wtLao9h6EB7Rystj+CO6u6FE2JmvEOnCyB7jCONiWDXEjKS/OLLK5RHIVdtGV
	fCMspKKEaCcSH2o8Q79LZIKM85gfPbUXAfRD8jHu+Xj9feo1Kgx/qLFE08oB2hz+cK9A
	FVXKeoQwI8Po+s6e/a7XxqiGueBsjp5cqY2ig8k/QqASkGGUdrJpmfUNnziLrbKK8YS+
	v04C/9wcfP/jdKn8qpxpOHNV01DiQaHrF94V7EQrkfhQJ15/mPPv96sZPD5bI4J/+MHa
	aBZg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=HJvrFxA8Ijew5BFvQX8rtMugih3SuRbkKqBIx7SdYHA=;
	b=ABPJlBMqC8UDMMEREJ4p1I48I5DGuJqQExsW6Js7lcvUuhbYUWYYHG2hKFekdMjrqp
	kPRE3dfD139vMf1dEUVRVkUFNqsu6K/1qyLWyLetzCQfHM5zFRTmJZC+KJMQV2ch8uBa
	aii1IAtPpEm0Hpfh9h3NBhEWhMikreaUEXVLHZOZDywku4tes+4i1/2s6PKbC0XNqnrP
	r0mxHB1f0cFFtjyBRVWJ5oBVYi+n1Wn4Lff391G1vfFeRhjxRjGj6jg3o8Ab/5cJPGyT
	IZ3GwLvY+nj85Ib4OO0j2deKkyNSXDDX6roBsP5vhjHGzqCn9b945OHgdLVpL9JH62w+
	CN4Q==
X-Gm-Message-State: AOPr4FX4M8JUo0IqX6Jg7+g6ENrP91evc2NfPxmnmd2XnEEjwkXg2LwpnRGKF573VINbtw==
X-Received: by 10.194.143.51 with SMTP id sb19mr31410925wjb.98.1463419160053;
	Mon, 16 May 2016 10:19:20 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	f11sm19536943wmf.22.2016.05.16.10.19.17
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 16 May 2016 10:19:18 -0700 (PDT)
Date: Mon, 16 May 2016 19:19:15 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86/asm changes for v4.7
Message-ID: &lt;20160516171914.GA5238@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.2 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - May 16, 2016, 5:19 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-asm-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-asm-for-linus

   # HEAD: 4afd0565552c87f23834db9121dd9cf6955d0b43 x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS

The main changes in this cycle were:

 - MSR access API fixes and enhancements (Andy Lutomirski)

 - early exception handling improvements (Andy Lutomirski)

 - user-space FS/GS prctl usage fixes and improvements (Andy Lutomirski)

 - Remove the cpu_has_*() APIs and replace them with equivalents (Borislav Petkov)

 - task switch micro-optimization (Brian Gerst)

 - 32-bit entry code simplification (Denys Vlasenko)

 - enhance PAT handling in enumated CPUs (Toshi Kani)

 - ... and lots of other cleanups/fixlets.

  out-of-topic modifications in x86-asm-for-linus:
  --------------------------------------------------
  arch/ia64/include/asm/iommu.h      # 62436a4d36c9: x86/cpufeature: Remove cpu_h
  drivers/cpufreq/longhaul.c         # 93984fbd4e33: x86/cpufeature: Replace cpu_
  drivers/gpu/drm/drm_cache.c        # 906bf7fda2c9: x86/cpufeature: Remove cpu_h
  drivers/gpu/drm/i915/i915_gem.c    # 568a58e5dfbc: x86/mm/pat, x86/cpufeature: 
  drivers/gpu/drm/i915/i915_gem_execbuffer.c# 906bf7fda2c9: x86/cpufeature: Remove cpu_h
  drivers/input/joystick/analog.c    # 59e21e3d00e6: x86/cpufeature: Replace cpu_
  drivers/iommu/irq_remapping.c      # 93984fbd4e33: x86/cpufeature: Replace cpu_
  drivers/lguest/x86/core.c          # c109bf95992b: x86/cpufeature: Remove cpu_h
  drivers/net/hamradio/baycom_epp.c  # 59e21e3d00e6: x86/cpufeature: Replace cpu_
  drivers/staging/unisys/visorbus/visorchipset.c# 0c9f3536cc71: x86/cpufeature: Remove cpu_h
  include/asm-generic/vmlinux.lds.h  # 91ed140d6c1e: x86/asm: Make sure verify_cp
  tools/testing/selftests/x86/Makefile# 0051202f6ad5: selftests/x86: Test the FSBA
  tools/testing/selftests/x86/fsgsbase.c# 0051202f6ad5: selftests/x86: Test the FSBA
  tools/testing/selftests/x86/ldt_gdt.c# d63f4b5269e9: selftests/x86/ldt_gdt: Test 

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (24):
      selftests/x86: Test the FSBASE/GSBASE API and context switching
      x86/arch_prctl: Fix ARCH_GET_FS and ARCH_GET_GS
      x86/cpu: Probe the behavior of nulling out a segment at boot time
      x86/asm, sched/x86: Rewrite the FS and GS context switch code
      x86/cpu: Move X86_BUG_ESPFIX initialization to generic_identify()
      x86/vdso: Remove direct HPET access through the vDSO
      x86/head: Pass a real pt_regs and trapnr to early_fixup_exception()
      x86/head: Move the early NMI fixup into C
      x86/head: Move early exception panic code into early_fixup_exception()
      x86/traps: Enable all exception handler callbacks early
      x86/paravirt: Add _safe to the read_ms()r and write_msr() PV callbacks
      x86/msr: Carry on after a non-&quot;safe&quot; MSR access fails
      x86/paravirt: Add paravirt_{read,write}_msr()
      x86/paravirt: Make &quot;unsafe&quot; MSR accesses unsafe even if PARAVIRT=y
      x86/msr: Set the return value to zero when native_rdmsr_safe() fails
      x86/extable: Add a comment about early exception handlers
      x86/asm: Stop depending on ptrace.h in alternative.h
      x86/asm: Make asm/alternative.h safe from assembly
      x86/segments/64: When loadsegment(fs, ...) fails, clear the base
      x86/segments/64: When load_gs_index fails, clear the base
      x86/arch_prctl/64: Remove FSBASE/GSBASE &lt; 4G optimization
      x86/asm/64: Rename thread_struct&#39;s fs and gs to fsbase and gsbase
      x86/tls: Synchronize segment registers in set_thread_area()
      selftests/x86/ldt_gdt: Test set_thread_area() deletion of an active segment

Borislav Petkov (30):
      x86/cpu: Do the feature test first in enable_sep_cpu()
      x86/mm/pat, x86/cpufeature: Remove cpu_has_pat
      x86/cpufeature: Remove cpu_has_arch_perfmon
      x86/cpufeature: Remove cpu_has_hypervisor
      x86/cpufeature: Remove cpu_has_osxsave
      x86/cpufeature: Remove cpu_has_x2apic
      x86/cpufeature: Remove cpu_has_gbpages
      x86/cpufeature: Remove cpu_has_clflush
      x86/cpufeature: Remove cpu_has_xmm2
      x86/cpufeature: Remove cpu_has_pge
      x86/cpufeature: Remove cpu_has_pse
      x86/cpu: Add Erratum 88 detection on AMD
      x86/entry/64: Make gs_change a local label
      x86/cpufeature: Replace cpu_has_avx2 with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_aes with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_avx with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_xmm with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_fpu with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_tsc with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_apic with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_fxsr with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_xsave with boot_cpu_has() usage
      x86/cpufeature: Replace cpu_has_xsaves with boot_cpu_has() usage
      x86/fpu/regset: Replace static_cpu_has() usage with boot_cpu_has()
      x86/cpu: Simplify extended APIC ID detection on AMD
      x86/tsc: Do not check X86_FEATURE_CONSTANT_TSC in notifier call
      x86/tsc: Save an indentation level in recalibrate_cpu_khz()
      x86/fpu: Remove check_fpu() indirection
      x86/fpu: Get rid of x87 math exception helpers
      x86/asm: Make sure verify_cpu() has a good stack

Brian Gerst (3):
      x86/entry, sched/x86: Don&#39;t save/restore EFLAGS on task switch
      x86/entry/32: Remove GET_THREAD_INFO() from entry code
      x86/entry/32: Remove asmlinkage_protect()

Christoph Hellwig (1):
      x86/syscalls: Wire up compat readv2/writev2 syscalls

Denys Vlasenko (1):
      x86/asm/entry/32: Simplify pushes of zeroed pt_regs-&gt;REGs

Dmitry Safonov (1):
      x86/entry: Rename is_{ia32,x32}_task() to in_{ia32,x32}_syscall()

Ingo Molnar (1):
      x86/cpufeature: Fix build bug caused by merge artifact with the removal of cpu_has_hypervisor

Mateusz Guzik (1):
      x86/arch_prctl/64: Restore accidentally removed put_cpu() in ARCH_SET_GS

Toshi Kani (8):
      x86/mm/pat: Add support of non-default PAT MSR setting
      x86/mm/pat: Add pat_disable() interface
      x86/mm/pat: Replace cpu_has_pat with boot_cpu_has()
      x86/mtrr: Fix Xorg crashes in Qemu sessions
      x86/mtrr: Fix PAT init handling when MTRR is disabled
      x86/xen, pat: Remove PAT table init code from Xen
      x86/pat: Document the PAT initialization sequence
      x86/mm/pat: Fix BUG_ON() in mmap_mem() on QEMU/i386


 Documentation/x86/pat.txt                      |  32 ++
 arch/ia64/include/asm/iommu.h                  |   1 -
 arch/x86/crypto/aesni-intel_glue.c             |   2 +-
 arch/x86/crypto/camellia_aesni_avx2_glue.c     |   5 +-
 arch/x86/crypto/camellia_aesni_avx_glue.c      |   4 +-
 arch/x86/crypto/chacha20_glue.c                |   3 +-
 arch/x86/crypto/poly1305_glue.c                |   5 +-
 arch/x86/crypto/serpent_avx2_glue.c            |   2 +-
 arch/x86/crypto/serpent_sse2_glue.c            |   2 +-
 arch/x86/crypto/sha1_ssse3_glue.c              |   2 +-
 arch/x86/crypto/sha256_ssse3_glue.c            |   2 +-
 arch/x86/crypto/sha512_ssse3_glue.c            |   2 +-
 arch/x86/entry/common.c                        |   2 +-
 arch/x86/entry/entry_32.S                      |   7 -
 arch/x86/entry/entry_64.S                      |  21 +-
 arch/x86/entry/entry_64_compat.S               |  45 ++-
 arch/x86/entry/syscalls/syscall_32.tbl         |   4 +-
 arch/x86/entry/syscalls/syscall_64.tbl         |   2 +
 arch/x86/entry/vdso/vclock_gettime.c           |  15 -
 arch/x86/entry/vdso/vdso-layout.lds.S          |   5 +-
 arch/x86/entry/vdso/vma.c                      |  11 -
 arch/x86/events/core.c                         |   2 +-
 arch/x86/events/intel/cstate.c                 |   2 +-
 arch/x86/events/intel/uncore.c                 |   2 +-
 arch/x86/ia32/ia32_signal.c                    |   2 +-
 arch/x86/include/asm/alternative.h             |  35 +--
 arch/x86/include/asm/apic.h                    |   4 +-
 arch/x86/include/asm/clocksource.h             |   9 +-
 arch/x86/include/asm/compat.h                  |   4 +-
 arch/x86/include/asm/cpufeature.h              |  25 --
 arch/x86/include/asm/cpufeatures.h             |   3 +
 arch/x86/include/asm/elf.h                     |   6 +-
 arch/x86/include/asm/hugetlb.h                 |   2 +-
 arch/x86/include/asm/irq_work.h                |   2 +-
 arch/x86/include/asm/kgdb.h                    |   2 +
 arch/x86/include/asm/linkage.h                 |  34 ---
 arch/x86/include/asm/msr.h                     |  20 +-
 arch/x86/include/asm/mtrr.h                    |   6 +-
 arch/x86/include/asm/paravirt.h                |  45 +--
 arch/x86/include/asm/paravirt_types.h          |  14 +-
 arch/x86/include/asm/pat.h                     |   2 +-
 arch/x86/include/asm/pgtable.h                 |   2 +-
 arch/x86/include/asm/processor.h               |  11 +-
 arch/x86/include/asm/segment.h                 |  49 ++-
 arch/x86/include/asm/setup.h                   |   1 +
 arch/x86/include/asm/switch_to.h               |   4 +-
 arch/x86/include/asm/text-patching.h           |  40 +++
 arch/x86/include/asm/thread_info.h             |   2 +-
 arch/x86/include/asm/tlbflush.h                |   2 +-
 arch/x86/include/asm/tsc.h                     |   2 +-
 arch/x86/include/asm/uaccess.h                 |   2 +-
 arch/x86/include/asm/xor_32.h                  |   2 +-
 arch/x86/include/asm/xor_avx.h                 |   4 +-
 arch/x86/kernel/acpi/boot.c                    |   8 +-
 arch/x86/kernel/alternative.c                  |   1 +
 arch/x86/kernel/apic/apic.c                    |  32 +-
 arch/x86/kernel/apic/apic_noop.c               |   4 +-
 arch/x86/kernel/apic/io_apic.c                 |   2 +-
 arch/x86/kernel/apic/ipi.c                     |   2 +-
 arch/x86/kernel/apic/vector.c                  |   2 +-
 arch/x86/kernel/cpu/amd.c                      |  20 +-
 arch/x86/kernel/cpu/common.c                   |  82 +++--
 arch/x86/kernel/cpu/cyrix.c                    |   2 +-
 arch/x86/kernel/cpu/intel.c                    |  12 +-
 arch/x86/kernel/cpu/mcheck/mce_intel.c         |   2 +-
 arch/x86/kernel/cpu/mcheck/therm_throt.c       |   2 +-
 arch/x86/kernel/cpu/mtrr/cyrix.c               |   4 +-
 arch/x86/kernel/cpu/mtrr/generic.c             |  28 +-
 arch/x86/kernel/cpu/mtrr/main.c                |  13 +-
 arch/x86/kernel/cpu/mtrr/mtrr.h                |   1 +
 arch/x86/kernel/cpu/vmware.c                   |   2 +-
 arch/x86/kernel/devicetree.c                   |   2 +-
 arch/x86/kernel/fpu/bugs.c                     |  16 +-
 arch/x86/kernel/fpu/core.c                     |  50 +---
 arch/x86/kernel/fpu/init.c                     |  16 +-
 arch/x86/kernel/fpu/regset.c                   |  25 +-
 arch/x86/kernel/fpu/xstate.c                   |  18 +-
 arch/x86/kernel/head_32.S                      | 116 +++----
 arch/x86/kernel/head_64.S                      | 103 +++----
 arch/x86/kernel/hpet.c                         |   1 -
 arch/x86/kernel/jump_label.c                   |   1 +
 arch/x86/kernel/kgdb.c                         |   1 +
 arch/x86/kernel/kprobes/core.c                 |   1 +
 arch/x86/kernel/kprobes/opt.c                  |   1 +
 arch/x86/kernel/kvm.c                          |   2 +-
 arch/x86/kernel/module.c                       |   1 +
 arch/x86/kernel/paravirt.c                     |   6 +-
 arch/x86/kernel/process_64.c                   | 241 +++++++--------
 arch/x86/kernel/ptrace.c                       |  50 +---
 arch/x86/kernel/signal.c                       |   6 +-
 arch/x86/kernel/smpboot.c                      |   2 +-
 arch/x86/kernel/tce_64.c                       |   2 +-
 arch/x86/kernel/tls.c                          |  42 +++
 arch/x86/kernel/traps.c                        |   1 +
 arch/x86/kernel/tsc.c                          |  33 +-
 arch/x86/kernel/uprobes.c                      |   2 +-
 arch/x86/kvm/cpuid.c                           |   2 +-
 arch/x86/kvm/mmu.c                             |   3 +-
 arch/x86/kvm/svm.c                             |   2 +-
 arch/x86/kvm/trace.h                           |   3 +-
 arch/x86/kvm/vmx.c                             |   2 +-
 arch/x86/kvm/x86.c                             |  16 +-
 arch/x86/lib/usercopy_32.c                     |   4 +-
 arch/x86/mm/extable.c                          |  96 ++++--
 arch/x86/mm/hugetlbpage.c                      |   4 +-
 arch/x86/mm/init.c                             |   8 +-
 arch/x86/mm/init_32.c                          |   2 +-
 arch/x86/mm/init_64.c                          |   4 +-
 arch/x86/mm/ioremap.c                          |   4 +-
 arch/x86/mm/pageattr.c                         |   4 +-
 arch/x86/mm/pat.c                              | 109 ++++---
 arch/x86/oprofile/nmi_int.c                    |   6 +-
 arch/x86/oprofile/op_model_ppro.c              |   2 +-
 arch/x86/pci/xen.c                             |   2 +-
 arch/x86/power/hibernate_32.c                  |   2 +-
 arch/x86/xen/enlighten.c                       |  40 ++-
 drivers/cpufreq/longhaul.c                     |   2 +-
 drivers/gpu/drm/drm_cache.c                    |   6 +-
 drivers/gpu/drm/i915/i915_gem.c                |   2 +-
 drivers/gpu/drm/i915/i915_gem_execbuffer.c     |   2 +-
 drivers/input/joystick/analog.c                |   6 +-
 drivers/iommu/irq_remapping.c                  |   2 +-
 drivers/lguest/x86/core.c                      |   2 +-
 drivers/net/hamradio/baycom_epp.c              |   8 +-
 drivers/staging/unisys/visorbus/visorchipset.c |   2 +-
 include/asm-generic/vmlinux.lds.h              |   4 +-
 tools/testing/selftests/x86/Makefile           |   1 +
 tools/testing/selftests/x86/fsgsbase.c         | 398 +++++++++++++++++++++++++
 tools/testing/selftests/x86/ldt_gdt.c          | 250 ++++++++++++++++
 129 files changed, 1614 insertions(+), 860 deletions(-)
 create mode 100644 arch/x86/include/asm/text-patching.h
 create mode 100644 tools/testing/selftests/x86/fsgsbase.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/pat.txt b/Documentation/x86/pat.txt</span>
<span class="p_header">index 54944c71b819..2a4ee6302122 100644</span>
<span class="p_header">--- a/Documentation/x86/pat.txt</span>
<span class="p_header">+++ b/Documentation/x86/pat.txt</span>
<span class="p_chunk">@@ -196,3 +196,35 @@</span> <span class="p_context"> Another, more verbose way of getting PAT related debug messages is with</span>
 &quot;debugpat&quot; boot parameter. With this parameter, various debug messages are
 printed to dmesg log.
 
<span class="p_add">+PAT Initialization</span>
<span class="p_add">+------------------</span>
<span class="p_add">+</span>
<span class="p_add">+The following table describes how PAT is initialized under various</span>
<span class="p_add">+configurations. The PAT MSR must be updated by Linux in order to support WC</span>
<span class="p_add">+and WT attributes. Otherwise, the PAT MSR has the value programmed in it</span>
<span class="p_add">+by the firmware. Note, Xen enables WC attribute in the PAT MSR for guests.</span>
<span class="p_add">+</span>
<span class="p_add">+ MTRR PAT   Call Sequence               PAT State  PAT MSR</span>
<span class="p_add">+ =========================================================</span>
<span class="p_add">+ E    E     MTRR -&gt; PAT init            Enabled    OS</span>
<span class="p_add">+ E    D     MTRR -&gt; PAT init            Disabled    -</span>
<span class="p_add">+ D    E     MTRR -&gt; PAT disable         Disabled   BIOS</span>
<span class="p_add">+ D    D     MTRR -&gt; PAT disable         Disabled    -</span>
<span class="p_add">+ -    np/E  PAT  -&gt; PAT disable         Disabled   BIOS</span>
<span class="p_add">+ -    np/D  PAT  -&gt; PAT disable         Disabled    -</span>
<span class="p_add">+ E    !P/E  MTRR -&gt; PAT init            Disabled   BIOS</span>
<span class="p_add">+ D    !P/E  MTRR -&gt; PAT disable         Disabled   BIOS</span>
<span class="p_add">+ !M   !P/E  MTRR stub -&gt; PAT disable    Disabled   BIOS</span>
<span class="p_add">+</span>
<span class="p_add">+ Legend</span>
<span class="p_add">+ ------------------------------------------------</span>
<span class="p_add">+ E         Feature enabled in CPU</span>
<span class="p_add">+ D	   Feature disabled/unsupported in CPU</span>
<span class="p_add">+ np	   &quot;nopat&quot; boot option specified</span>
<span class="p_add">+ !P	   CONFIG_X86_PAT option unset</span>
<span class="p_add">+ !M	   CONFIG_MTRR option unset</span>
<span class="p_add">+ Enabled   PAT state set to enabled</span>
<span class="p_add">+ Disabled  PAT state set to disabled</span>
<span class="p_add">+ OS        PAT initializes PAT MSR with OS setting</span>
<span class="p_add">+ BIOS      PAT keeps PAT MSR with BIOS setting</span>
<span class="p_add">+</span>
<span class="p_header">diff --git a/arch/ia64/include/asm/iommu.h b/arch/ia64/include/asm/iommu.h</span>
<span class="p_header">index 105c93b00b1b..1d1212901ae7 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/iommu.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/iommu.h</span>
<span class="p_chunk">@@ -1,7 +1,6 @@</span> <span class="p_context"></span>
 #ifndef _ASM_IA64_IOMMU_H
 #define _ASM_IA64_IOMMU_H 1
 
<span class="p_del">-#define cpu_has_x2apic 0</span>
 /* 10 seconds */
 #define DMAR_OPERATION_TIMEOUT (((cycles_t) local_cpu_data-&gt;itc_freq)*10)
 
<span class="p_header">diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c</span>
<span class="p_header">index 064c7e2bd7c8..5b7fa1471007 100644</span>
<span class="p_header">--- a/arch/x86/crypto/aesni-intel_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/aesni-intel_glue.c</span>
<span class="p_chunk">@@ -1477,7 +1477,7 @@</span> <span class="p_context"> static int __init aesni_init(void)</span>
 	}
 	aesni_ctr_enc_tfm = aesni_ctr_enc;
 #ifdef CONFIG_AS_AVX
<span class="p_del">-	if (cpu_has_avx) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_AVX)) {</span>
 		/* optimize performance of ctr mode encryption transform */
 		aesni_ctr_enc_tfm = aesni_ctr_enc_avx_tfm;
 		pr_info(&quot;AES CTR mode by8 optimization enabled\n&quot;);
<span class="p_header">diff --git a/arch/x86/crypto/camellia_aesni_avx2_glue.c b/arch/x86/crypto/camellia_aesni_avx2_glue.c</span>
<span class="p_header">index d84456924563..60907c139c4e 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia_aesni_avx2_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia_aesni_avx2_glue.c</span>
<span class="p_chunk">@@ -562,7 +562,10 @@</span> <span class="p_context"> static int __init camellia_aesni_init(void)</span>
 {
 	const char *feature_name;
 
<span class="p_del">-	if (!cpu_has_avx2 || !cpu_has_avx || !cpu_has_aes || !cpu_has_osxsave) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_AVX) ||</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_AVX2) ||</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_AES) ||</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_OSXSAVE)) {</span>
 		pr_info(&quot;AVX2 or AES-NI instructions are not detected.\n&quot;);
 		return -ENODEV;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/camellia_aesni_avx_glue.c b/arch/x86/crypto/camellia_aesni_avx_glue.c</span>
<span class="p_header">index 93d8f295784e..d96429da88eb 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia_aesni_avx_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia_aesni_avx_glue.c</span>
<span class="p_chunk">@@ -554,7 +554,9 @@</span> <span class="p_context"> static int __init camellia_aesni_init(void)</span>
 {
 	const char *feature_name;
 
<span class="p_del">-	if (!cpu_has_avx || !cpu_has_aes || !cpu_has_osxsave) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_AVX) ||</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_AES) ||</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_OSXSAVE)) {</span>
 		pr_info(&quot;AVX or AES-NI instructions are not detected.\n&quot;);
 		return -ENODEV;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/chacha20_glue.c b/arch/x86/crypto/chacha20_glue.c</span>
<span class="p_header">index 8baaff5af0b5..2d5c2e0bd939 100644</span>
<span class="p_header">--- a/arch/x86/crypto/chacha20_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/chacha20_glue.c</span>
<span class="p_chunk">@@ -129,7 +129,8 @@</span> <span class="p_context"> static int __init chacha20_simd_mod_init(void)</span>
 		return -ENODEV;
 
 #ifdef CONFIG_AS_AVX2
<span class="p_del">-	chacha20_use_avx2 = cpu_has_avx &amp;&amp; cpu_has_avx2 &amp;&amp;</span>
<span class="p_add">+	chacha20_use_avx2 = boot_cpu_has(X86_FEATURE_AVX) &amp;&amp;</span>
<span class="p_add">+			    boot_cpu_has(X86_FEATURE_AVX2) &amp;&amp;</span>
 			    cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL);
 #endif
 	return crypto_register_alg(&amp;alg);
<span class="p_header">diff --git a/arch/x86/crypto/poly1305_glue.c b/arch/x86/crypto/poly1305_glue.c</span>
<span class="p_header">index 4264a3d59589..e32142bc071d 100644</span>
<span class="p_header">--- a/arch/x86/crypto/poly1305_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/poly1305_glue.c</span>
<span class="p_chunk">@@ -179,11 +179,12 @@</span> <span class="p_context"> static struct shash_alg alg = {</span>
 
 static int __init poly1305_simd_mod_init(void)
 {
<span class="p_del">-	if (!cpu_has_xmm2)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XMM2))</span>
 		return -ENODEV;
 
 #ifdef CONFIG_AS_AVX2
<span class="p_del">-	poly1305_use_avx2 = cpu_has_avx &amp;&amp; cpu_has_avx2 &amp;&amp;</span>
<span class="p_add">+	poly1305_use_avx2 = boot_cpu_has(X86_FEATURE_AVX) &amp;&amp;</span>
<span class="p_add">+			    boot_cpu_has(X86_FEATURE_AVX2) &amp;&amp;</span>
 			    cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL);
 	alg.descsize = sizeof(struct poly1305_simd_desc_ctx);
 	if (poly1305_use_avx2)
<span class="p_header">diff --git a/arch/x86/crypto/serpent_avx2_glue.c b/arch/x86/crypto/serpent_avx2_glue.c</span>
<span class="p_header">index 6d198342e2de..870f6d812a2d 100644</span>
<span class="p_header">--- a/arch/x86/crypto/serpent_avx2_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/serpent_avx2_glue.c</span>
<span class="p_chunk">@@ -538,7 +538,7 @@</span> <span class="p_context"> static int __init init(void)</span>
 {
 	const char *feature_name;
 
<span class="p_del">-	if (!cpu_has_avx2 || !cpu_has_osxsave) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_AVX2) || !boot_cpu_has(X86_FEATURE_OSXSAVE)) {</span>
 		pr_info(&quot;AVX2 instructions are not detected.\n&quot;);
 		return -ENODEV;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/serpent_sse2_glue.c b/arch/x86/crypto/serpent_sse2_glue.c</span>
<span class="p_header">index 8943407e8917..644f97ab8cac 100644</span>
<span class="p_header">--- a/arch/x86/crypto/serpent_sse2_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/serpent_sse2_glue.c</span>
<span class="p_chunk">@@ -600,7 +600,7 @@</span> <span class="p_context"> static struct crypto_alg serpent_algs[10] = { {</span>
 
 static int __init serpent_sse2_init(void)
 {
<span class="p_del">-	if (!cpu_has_xmm2) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XMM2)) {</span>
 		printk(KERN_INFO &quot;SSE2 instructions are not detected.\n&quot;);
 		return -ENODEV;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/sha1_ssse3_glue.c b/arch/x86/crypto/sha1_ssse3_glue.c</span>
<span class="p_header">index dd14616b7739..1024e378a358 100644</span>
<span class="p_header">--- a/arch/x86/crypto/sha1_ssse3_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/sha1_ssse3_glue.c</span>
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> static struct shash_alg sha1_avx_alg = {</span>
 static bool avx_usable(void)
 {
 	if (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL)) {
<span class="p_del">-		if (cpu_has_avx)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_AVX))</span>
 			pr_info(&quot;AVX detected but unusable.\n&quot;);
 		return false;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/sha256_ssse3_glue.c b/arch/x86/crypto/sha256_ssse3_glue.c</span>
<span class="p_header">index 5f4d6086dc59..3ae0f43ebd37 100644</span>
<span class="p_header">--- a/arch/x86/crypto/sha256_ssse3_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/sha256_ssse3_glue.c</span>
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> static struct shash_alg sha256_avx_algs[] = { {</span>
 static bool avx_usable(void)
 {
 	if (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL)) {
<span class="p_del">-		if (cpu_has_avx)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_AVX))</span>
 			pr_info(&quot;AVX detected but unusable.\n&quot;);
 		return false;
 	}
<span class="p_header">diff --git a/arch/x86/crypto/sha512_ssse3_glue.c b/arch/x86/crypto/sha512_ssse3_glue.c</span>
<span class="p_header">index 34e5083d6f36..0b17c83d027d 100644</span>
<span class="p_header">--- a/arch/x86/crypto/sha512_ssse3_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/sha512_ssse3_glue.c</span>
<span class="p_chunk">@@ -151,7 +151,7 @@</span> <span class="p_context"> asmlinkage void sha512_transform_avx(u64 *digest, const char *data,</span>
 static bool avx_usable(void)
 {
 	if (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL)) {
<span class="p_del">-		if (cpu_has_avx)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_AVX))</span>
 			pr_info(&quot;AVX detected but unusable.\n&quot;);
 		return false;
 	}
<span class="p_header">diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c</span>
<span class="p_header">index e79d93d44ecd..ec138e538c44 100644</span>
<span class="p_header">--- a/arch/x86/entry/common.c</span>
<span class="p_header">+++ b/arch/x86/entry/common.c</span>
<span class="p_chunk">@@ -191,7 +191,7 @@</span> <span class="p_context"> long syscall_trace_enter_phase2(struct pt_regs *regs, u32 arch,</span>
 
 long syscall_trace_enter(struct pt_regs *regs)
 {
<span class="p_del">-	u32 arch = is_ia32_task() ? AUDIT_ARCH_I386 : AUDIT_ARCH_X86_64;</span>
<span class="p_add">+	u32 arch = in_ia32_syscall() ? AUDIT_ARCH_I386 : AUDIT_ARCH_X86_64;</span>
 	unsigned long phase1_result = syscall_trace_enter_phase1(regs, arch);
 
 	if (phase1_result == 0)
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 10868aa734dc..983e5d3a0d27 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -207,10 +207,7 @@</span> <span class="p_context"></span>
 ENTRY(ret_from_fork)
 	pushl	%eax
 	call	schedule_tail
<span class="p_del">-	GET_THREAD_INFO(%ebp)</span>
 	popl	%eax
<span class="p_del">-	pushl	$0x0202				# Reset kernel eflags</span>
<span class="p_del">-	popfl</span>
 
 	/* When we fork, we trace the syscall return in the child, too. */
 	movl    %esp, %eax
<span class="p_chunk">@@ -221,10 +218,7 @@</span> <span class="p_context"> END(ret_from_fork)</span>
 ENTRY(ret_from_kernel_thread)
 	pushl	%eax
 	call	schedule_tail
<span class="p_del">-	GET_THREAD_INFO(%ebp)</span>
 	popl	%eax
<span class="p_del">-	pushl	$0x0202				# Reset kernel eflags</span>
<span class="p_del">-	popfl</span>
 	movl	PT_EBP(%esp), %eax
 	call	*PT_EBX(%esp)
 	movl	$0, PT_EAX(%esp)
<span class="p_chunk">@@ -251,7 +245,6 @@</span> <span class="p_context"> ENDPROC(ret_from_kernel_thread)</span>
 ret_from_exception:
 	preempt_stop(CLBR_ANY)
 ret_from_intr:
<span class="p_del">-	GET_THREAD_INFO(%ebp)</span>
 #ifdef CONFIG_VM86
 	movl	PT_EFLAGS(%esp), %eax		# mix EFLAGS and CS
 	movb	PT_CS(%esp), %al
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 858b555e274b..9ee0da1807ed 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -372,9 +372,6 @@</span> <span class="p_context"> END(ptregs_\func)</span>
 ENTRY(ret_from_fork)
 	LOCK ; btr $TIF_FORK, TI_flags(%r8)
 
<span class="p_del">-	pushq	$0x0002</span>
<span class="p_del">-	popfq					/* reset kernel eflags */</span>
<span class="p_del">-</span>
 	call	schedule_tail			/* rdi: &#39;prev&#39; task parameter */
 
 	testb	$3, CS(%rsp)			/* from kernel_thread? */
<span class="p_chunk">@@ -781,19 +778,25 @@</span> <span class="p_context"> ENTRY(native_load_gs_index)</span>
 	pushfq
 	DISABLE_INTERRUPTS(CLBR_ANY &amp; ~CLBR_RDI)
 	SWAPGS
<span class="p_del">-gs_change:</span>
<span class="p_add">+.Lgs_change:</span>
 	movl	%edi, %gs
<span class="p_del">-2:	mfence					/* workaround */</span>
<span class="p_add">+2:	ALTERNATIVE &quot;&quot;, &quot;mfence&quot;, X86_BUG_SWAPGS_FENCE</span>
 	SWAPGS
 	popfq
 	ret
 END(native_load_gs_index)
 
<span class="p_del">-	_ASM_EXTABLE(gs_change, bad_gs)</span>
<span class="p_add">+	_ASM_EXTABLE(.Lgs_change, bad_gs)</span>
 	.section .fixup, &quot;ax&quot;
 	/* running with kernelgs */
 bad_gs:
 	SWAPGS					/* switch back to user gs */
<span class="p_add">+.macro ZAP_GS</span>
<span class="p_add">+	/* This can&#39;t be a string because the preprocessor needs to see it. */</span>
<span class="p_add">+	movl $__USER_DS, %eax</span>
<span class="p_add">+	movl %eax, %gs</span>
<span class="p_add">+.endm</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;ZAP_GS&quot;, X86_BUG_NULL_SEG</span>
 	xorl	%eax, %eax
 	movl	%eax, %gs
 	jmp	2b
<span class="p_chunk">@@ -1019,13 +1022,13 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	movl	%ecx, %eax			/* zero extend */
 	cmpq	%rax, RIP+8(%rsp)
 	je	.Lbstep_iret
<span class="p_del">-	cmpq	$gs_change, RIP+8(%rsp)</span>
<span class="p_add">+	cmpq	$.Lgs_change, RIP+8(%rsp)</span>
 	jne	.Lerror_entry_done
 
 	/*
<span class="p_del">-	 * hack: gs_change can fail with user gsbase.  If this happens, fix up</span>
<span class="p_add">+	 * hack: .Lgs_change can fail with user gsbase.  If this happens, fix up</span>
 	 * gsbase and proceed.  We&#39;ll fix up the exception and land in
<span class="p_del">-	 * gs_change&#39;s error handler with kernel gsbase.</span>
<span class="p_add">+	 * .Lgs_change&#39;s error handler with kernel gsbase.</span>
 	 */
 	jmp	.Lerror_entry_from_usermode_swapgs
 
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 847f2f0c31e5..e1721dafbcb1 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -72,24 +72,23 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_compat)</span>
 	pushfq				/* pt_regs-&gt;flags (except IF = 0) */
 	orl	$X86_EFLAGS_IF, (%rsp)	/* Fix saved flags */
 	pushq	$__USER32_CS		/* pt_regs-&gt;cs */
<span class="p_del">-	xorq    %r8,%r8</span>
<span class="p_del">-	pushq	%r8			/* pt_regs-&gt;ip = 0 (placeholder) */</span>
<span class="p_add">+	pushq	$0			/* pt_regs-&gt;ip = 0 (placeholder) */</span>
 	pushq	%rax			/* pt_regs-&gt;orig_ax */
 	pushq	%rdi			/* pt_regs-&gt;di */
 	pushq	%rsi			/* pt_regs-&gt;si */
 	pushq	%rdx			/* pt_regs-&gt;dx */
 	pushq	%rcx			/* pt_regs-&gt;cx */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r8  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r9  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r10 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r11 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r8  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r9  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r10 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r11 = 0 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
 	pushq   %rbp                    /* pt_regs-&gt;rbp (will be overwritten) */
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r12 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r13 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r14 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r15 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r12 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r13 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r14 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r15 = 0 */</span>
 	cld
 
 	/*
<span class="p_chunk">@@ -205,17 +204,16 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_compat)</span>
 	pushq	%rdx			/* pt_regs-&gt;dx */
 	pushq	%rbp			/* pt_regs-&gt;cx (stashed in bp) */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
<span class="p_del">-	xorq    %r8,%r8</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r8  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r9  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r10 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r11 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r8  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r9  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r10 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r11 = 0 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
 	pushq   %rbp                    /* pt_regs-&gt;rbp (will be overwritten) */
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r12 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r13 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r14 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r15 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r12 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r13 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r14 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r15 = 0 */</span>
 
 	/*
 	 * User mode is traced as though IRQs are on, and SYSENTER
<span class="p_chunk">@@ -316,11 +314,10 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 	pushq	%rdx			/* pt_regs-&gt;dx */
 	pushq	%rcx			/* pt_regs-&gt;cx */
 	pushq	$-ENOSYS		/* pt_regs-&gt;ax */
<span class="p_del">-	xorq    %r8,%r8</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r8  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r9  = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r10 = 0 */</span>
<span class="p_del">-	pushq   %r8                     /* pt_regs-&gt;r11 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r8  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r9  = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r10 = 0 */</span>
<span class="p_add">+	pushq   $0			/* pt_regs-&gt;r11 = 0 */</span>
 	pushq   %rbx                    /* pt_regs-&gt;rbx */
 	pushq   %rbp                    /* pt_regs-&gt;rbp */
 	pushq   %r12                    /* pt_regs-&gt;r12 */
<span class="p_header">diff --git a/arch/x86/entry/syscalls/syscall_32.tbl b/arch/x86/entry/syscalls/syscall_32.tbl</span>
<span class="p_header">index b30dd8154cc2..4cddd17153fb 100644</span>
<span class="p_header">--- a/arch/x86/entry/syscalls/syscall_32.tbl</span>
<span class="p_header">+++ b/arch/x86/entry/syscalls/syscall_32.tbl</span>
<span class="p_chunk">@@ -384,5 +384,5 @@</span> <span class="p_context"></span>
 375	i386	membarrier		sys_membarrier
 376	i386	mlock2			sys_mlock2
 377	i386	copy_file_range		sys_copy_file_range
<span class="p_del">-378	i386	preadv2			sys_preadv2</span>
<span class="p_del">-379	i386	pwritev2		sys_pwritev2</span>
<span class="p_add">+378	i386	preadv2			sys_preadv2			compat_sys_preadv2</span>
<span class="p_add">+379	i386	pwritev2		sys_pwritev2			compat_sys_pwritev2</span>
<span class="p_header">diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_header">index cac6d17ce5db..555263e385c9 100644</span>
<span class="p_header">--- a/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_header">+++ b/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_chunk">@@ -374,3 +374,5 @@</span> <span class="p_context"></span>
 543	x32	io_setup		compat_sys_io_setup
 544	x32	io_submit		compat_sys_io_submit
 545	x32	execveat		compat_sys_execveat/ptregs
<span class="p_add">+534	x32	preadv2			compat_sys_preadv2</span>
<span class="p_add">+535	x32	pwritev2		compat_sys_pwritev2</span>
<span class="p_header">diff --git a/arch/x86/entry/vdso/vclock_gettime.c b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">index 03c3eb77bfce..2f02d23a05ef 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_chunk">@@ -13,7 +13,6 @@</span> <span class="p_context"></span>
 
 #include &lt;uapi/linux/time.h&gt;
 #include &lt;asm/vgtod.h&gt;
<span class="p_del">-#include &lt;asm/hpet.h&gt;</span>
 #include &lt;asm/vvar.h&gt;
 #include &lt;asm/unistd.h&gt;
 #include &lt;asm/msr.h&gt;
<span class="p_chunk">@@ -28,16 +27,6 @@</span> <span class="p_context"> extern int __vdso_clock_gettime(clockid_t clock, struct timespec *ts);</span>
 extern int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz);
 extern time_t __vdso_time(time_t *t);
 
<span class="p_del">-#ifdef CONFIG_HPET_TIMER</span>
<span class="p_del">-extern u8 hpet_page</span>
<span class="p_del">-	__attribute__((visibility(&quot;hidden&quot;)));</span>
<span class="p_del">-</span>
<span class="p_del">-static notrace cycle_t vread_hpet(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return *(const volatile u32 *)(&amp;hpet_page + HPET_COUNTER);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #ifdef CONFIG_PARAVIRT_CLOCK
 extern u8 pvclock_page
 	__attribute__((visibility(&quot;hidden&quot;)));
<span class="p_chunk">@@ -195,10 +184,6 @@</span> <span class="p_context"> notrace static inline u64 vgetsns(int *mode)</span>
 
 	if (gtod-&gt;vclock_mode == VCLOCK_TSC)
 		cycles = vread_tsc();
<span class="p_del">-#ifdef CONFIG_HPET_TIMER</span>
<span class="p_del">-	else if (gtod-&gt;vclock_mode == VCLOCK_HPET)</span>
<span class="p_del">-		cycles = vread_hpet();</span>
<span class="p_del">-#endif</span>
 #ifdef CONFIG_PARAVIRT_CLOCK
 	else if (gtod-&gt;vclock_mode == VCLOCK_PVCLOCK)
 		cycles = vread_pvclock(mode);
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso-layout.lds.S b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">index 4158acc17df0..a708aa90b507 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> SECTIONS</span>
 	 * segment.
 	 */
 
<span class="p_del">-	vvar_start = . - 3 * PAGE_SIZE;</span>
<span class="p_add">+	vvar_start = . - 2 * PAGE_SIZE;</span>
 	vvar_page = vvar_start;
 
 	/* Place all vvars at the offsets in asm/vvar.h. */
<span class="p_chunk">@@ -35,8 +35,7 @@</span> <span class="p_context"> SECTIONS</span>
 #undef __VVAR_KERNEL_LDS
 #undef EMIT_VVAR
 
<span class="p_del">-	hpet_page = vvar_start + PAGE_SIZE;</span>
<span class="p_del">-	pvclock_page = vvar_start + 2 * PAGE_SIZE;</span>
<span class="p_add">+	pvclock_page = vvar_start + PAGE_SIZE;</span>
 
 	. = SIZEOF_HEADERS;
 
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 10f704584922..b3cf81333a54 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -18,7 +18,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/vdso.h&gt;
 #include &lt;asm/vvar.h&gt;
 #include &lt;asm/page.h&gt;
<span class="p_del">-#include &lt;asm/hpet.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/cpufeature.h&gt;
 
<span class="p_chunk">@@ -129,16 +128,6 @@</span> <span class="p_context"> static int vvar_fault(const struct vm_special_mapping *sm,</span>
 	if (sym_offset == image-&gt;sym_vvar_page) {
 		ret = vm_insert_pfn(vma, (unsigned long)vmf-&gt;virtual_address,
 				    __pa_symbol(&amp;__vvar_page) &gt;&gt; PAGE_SHIFT);
<span class="p_del">-	} else if (sym_offset == image-&gt;sym_hpet_page) {</span>
<span class="p_del">-#ifdef CONFIG_HPET_TIMER</span>
<span class="p_del">-		if (hpet_address &amp;&amp; vclock_was_used(VCLOCK_HPET)) {</span>
<span class="p_del">-			ret = vm_insert_pfn_prot(</span>
<span class="p_del">-				vma,</span>
<span class="p_del">-				(unsigned long)vmf-&gt;virtual_address,</span>
<span class="p_del">-				hpet_address &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-				pgprot_noncached(PAGE_READONLY));</span>
<span class="p_del">-		}</span>
<span class="p_del">-#endif</span>
 	} else if (sym_offset == image-&gt;sym_pvclock_page) {
 		struct pvclock_vsyscall_time_info *pvti =
 			pvclock_pvti_cpu0_va();
<span class="p_header">diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c</span>
<span class="p_header">index 041e442a3e28..54c17455600e 100644</span>
<span class="p_header">--- a/arch/x86/events/core.c</span>
<span class="p_header">+++ b/arch/x86/events/core.c</span>
<span class="p_chunk">@@ -1518,7 +1518,7 @@</span> <span class="p_context"> x86_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)</span>
 
 static void __init pmu_check_apic(void)
 {
<span class="p_del">-	if (cpu_has_apic)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC))</span>
 		return;
 
 	x86_pmu.apic = 0;
<span class="p_header">diff --git a/arch/x86/events/intel/cstate.c b/arch/x86/events/intel/cstate.c</span>
<span class="p_header">index 7946c4231169..d5045c8e2e63 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/cstate.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/cstate.c</span>
<span class="p_chunk">@@ -677,7 +677,7 @@</span> <span class="p_context"> static int __init cstate_pmu_init(void)</span>
 {
 	int err;
 
<span class="p_del">-	if (cpu_has_hypervisor)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))</span>
 		return -ENODEV;
 
 	err = cstate_init();
<span class="p_header">diff --git a/arch/x86/events/intel/uncore.c b/arch/x86/events/intel/uncore.c</span>
<span class="p_header">index 7012d18bb293..3f6d8b5672d5 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/uncore.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/uncore.c</span>
<span class="p_chunk">@@ -1383,7 +1383,7 @@</span> <span class="p_context"> static int __init intel_uncore_init(void)</span>
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
 		return -ENODEV;
 
<span class="p_del">-	if (cpu_has_hypervisor)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))</span>
 		return -ENODEV;
 
 	max_packages = topology_max_packages();
<span class="p_header">diff --git a/arch/x86/ia32/ia32_signal.c b/arch/x86/ia32/ia32_signal.c</span>
<span class="p_header">index 0552884da18d..2f29f4e407c3 100644</span>
<span class="p_header">--- a/arch/x86/ia32/ia32_signal.c</span>
<span class="p_header">+++ b/arch/x86/ia32/ia32_signal.c</span>
<span class="p_chunk">@@ -357,7 +357,7 @@</span> <span class="p_context"> int ia32_setup_rt_frame(int sig, struct ksignal *ksig,</span>
 		put_user_ex(ptr_to_compat(&amp;frame-&gt;uc), &amp;frame-&gt;puc);
 
 		/* Create the ucontext.  */
<span class="p_del">-		if (cpu_has_xsave)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 			put_user_ex(UC_FP_XSTATE, &amp;frame-&gt;uc.uc_flags);
 		else
 			put_user_ex(0, &amp;frame-&gt;uc.uc_flags);
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index 99afb665a004..e77a6443104f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -1,11 +1,12 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_ALTERNATIVE_H
 #define _ASM_X86_ALTERNATIVE_H
 
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/stddef.h&gt;
 #include &lt;linux/stringify.h&gt;
 #include &lt;asm/asm.h&gt;
<span class="p_del">-#include &lt;asm/ptrace.h&gt;</span>
 
 /*
  * Alternative inline assembly for SMP.
<span class="p_chunk">@@ -233,36 +234,6 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
  */
 #define ASM_NO_INPUT_CLOBBER(clbr...) &quot;i&quot; (0) : clbr
 
<span class="p_del">-struct paravirt_patch_site;</span>
<span class="p_del">-#ifdef CONFIG_PARAVIRT</span>
<span class="p_del">-void apply_paravirt(struct paravirt_patch_site *start,</span>
<span class="p_del">-		    struct paravirt_patch_site *end);</span>
<span class="p_del">-#else</span>
<span class="p_del">-static inline void apply_paravirt(struct paravirt_patch_site *start,</span>
<span class="p_del">-				  struct paravirt_patch_site *end)</span>
<span class="p_del">-{}</span>
<span class="p_del">-#define __parainstructions	NULL</span>
<span class="p_del">-#define __parainstructions_end	NULL</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-extern void *text_poke_early(void *addr, const void *opcode, size_t len);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Clear and restore the kernel write-protection flag on the local CPU.</span>
<span class="p_del">- * Allows the kernel to edit read-only pages.</span>
<span class="p_del">- * Side-effect: any interrupt handler running between save and restore will have</span>
<span class="p_del">- * the ability to write to read-only pages.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Warning:</span>
<span class="p_del">- * Code patching in the UP case is safe if NMIs and MCE handlers are stopped and</span>
<span class="p_del">- * no thread can be preempted in the instructions being modified (no iret to an</span>
<span class="p_del">- * invalid instruction possible) or if the instructions are changed from a</span>
<span class="p_del">- * consistent state to another consistent state atomically.</span>
<span class="p_del">- * On the local CPU you need to be protected again NMI or MCE handlers seeing an</span>
<span class="p_del">- * inconsistent instruction while you patch.</span>
<span class="p_del">- */</span>
<span class="p_del">-extern void *text_poke(void *addr, const void *opcode, size_t len);</span>
<span class="p_del">-extern int poke_int3_handler(struct pt_regs *regs);</span>
<span class="p_del">-extern void *text_poke_bp(void *addr, const void *opcode, size_t len, void *handler);</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
 
 #endif /* _ASM_X86_ALTERNATIVE_H */
<span class="p_header">diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h</span>
<span class="p_header">index 98f25bbafac4..bc27611fa58f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/apic.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/apic.h</span>
<span class="p_chunk">@@ -239,10 +239,10 @@</span> <span class="p_context"> extern void __init check_x2apic(void);</span>
 extern void x2apic_setup(void);
 static inline int x2apic_enabled(void)
 {
<span class="p_del">-	return cpu_has_x2apic &amp;&amp; apic_is_x2apic_enabled();</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_X2APIC) &amp;&amp; apic_is_x2apic_enabled();</span>
 }
 
<span class="p_del">-#define x2apic_supported()	(cpu_has_x2apic)</span>
<span class="p_add">+#define x2apic_supported()	(boot_cpu_has(X86_FEATURE_X2APIC))</span>
 #else /* !CONFIG_X86_X2APIC */
 static inline void check_x2apic(void) { }
 static inline void x2apic_setup(void) { }
<span class="p_header">diff --git a/arch/x86/include/asm/clocksource.h b/arch/x86/include/asm/clocksource.h</span>
<span class="p_header">index d194266acb28..eae33c7170c8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/clocksource.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/clocksource.h</span>
<span class="p_chunk">@@ -3,11 +3,10 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_CLOCKSOURCE_H
 #define _ASM_X86_CLOCKSOURCE_H
 
<span class="p_del">-#define VCLOCK_NONE	0  /* No vDSO clock available.	*/</span>
<span class="p_del">-#define VCLOCK_TSC	1  /* vDSO should use vread_tsc.	*/</span>
<span class="p_del">-#define VCLOCK_HPET	2  /* vDSO should use vread_hpet.	*/</span>
<span class="p_del">-#define VCLOCK_PVCLOCK	3 /* vDSO should use vread_pvclock. */</span>
<span class="p_del">-#define VCLOCK_MAX	3</span>
<span class="p_add">+#define VCLOCK_NONE	0	/* No vDSO clock available.		*/</span>
<span class="p_add">+#define VCLOCK_TSC	1	/* vDSO should use vread_tsc.		*/</span>
<span class="p_add">+#define VCLOCK_PVCLOCK	2	/* vDSO should use vread_pvclock.	*/</span>
<span class="p_add">+#define VCLOCK_MAX	2</span>
 
 struct arch_clocksource_data {
 	int vclock_mode;
<span class="p_header">diff --git a/arch/x86/include/asm/compat.h b/arch/x86/include/asm/compat.h</span>
<span class="p_header">index ebb102e1bbc7..5a3b2c119ed0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/compat.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/compat.h</span>
<span class="p_chunk">@@ -307,7 +307,7 @@</span> <span class="p_context"> static inline void __user *arch_compat_alloc_user_space(long len)</span>
 	return (void __user *)round_down(sp - len, 16);
 }
 
<span class="p_del">-static inline bool is_x32_task(void)</span>
<span class="p_add">+static inline bool in_x32_syscall(void)</span>
 {
 #ifdef CONFIG_X86_X32_ABI
 	if (task_pt_regs(current)-&gt;orig_ax &amp; __X32_SYSCALL_BIT)
<span class="p_chunk">@@ -318,7 +318,7 @@</span> <span class="p_context"> static inline bool is_x32_task(void)</span>
 
 static inline bool in_compat_syscall(void)
 {
<span class="p_del">-	return is_ia32_task() || is_x32_task();</span>
<span class="p_add">+	return in_ia32_syscall() || in_x32_syscall();</span>
 }
 #define in_compat_syscall in_compat_syscall	/* override the generic impl */
 
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index 3636ec06c887..07c942d84662 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -118,31 +118,6 @@</span> <span class="p_context"> extern const char * const x86_bug_flags[NBUGINTS*32];</span>
 	set_bit(bit, (unsigned long *)cpu_caps_set);	\
 } while (0)
 
<span class="p_del">-#define cpu_has_fpu		boot_cpu_has(X86_FEATURE_FPU)</span>
<span class="p_del">-#define cpu_has_pse		boot_cpu_has(X86_FEATURE_PSE)</span>
<span class="p_del">-#define cpu_has_tsc		boot_cpu_has(X86_FEATURE_TSC)</span>
<span class="p_del">-#define cpu_has_pge		boot_cpu_has(X86_FEATURE_PGE)</span>
<span class="p_del">-#define cpu_has_apic		boot_cpu_has(X86_FEATURE_APIC)</span>
<span class="p_del">-#define cpu_has_fxsr		boot_cpu_has(X86_FEATURE_FXSR)</span>
<span class="p_del">-#define cpu_has_xmm		boot_cpu_has(X86_FEATURE_XMM)</span>
<span class="p_del">-#define cpu_has_xmm2		boot_cpu_has(X86_FEATURE_XMM2)</span>
<span class="p_del">-#define cpu_has_aes		boot_cpu_has(X86_FEATURE_AES)</span>
<span class="p_del">-#define cpu_has_avx		boot_cpu_has(X86_FEATURE_AVX)</span>
<span class="p_del">-#define cpu_has_avx2		boot_cpu_has(X86_FEATURE_AVX2)</span>
<span class="p_del">-#define cpu_has_clflush		boot_cpu_has(X86_FEATURE_CLFLUSH)</span>
<span class="p_del">-#define cpu_has_gbpages		boot_cpu_has(X86_FEATURE_GBPAGES)</span>
<span class="p_del">-#define cpu_has_arch_perfmon	boot_cpu_has(X86_FEATURE_ARCH_PERFMON)</span>
<span class="p_del">-#define cpu_has_pat		boot_cpu_has(X86_FEATURE_PAT)</span>
<span class="p_del">-#define cpu_has_x2apic		boot_cpu_has(X86_FEATURE_X2APIC)</span>
<span class="p_del">-#define cpu_has_xsave		boot_cpu_has(X86_FEATURE_XSAVE)</span>
<span class="p_del">-#define cpu_has_xsaves		boot_cpu_has(X86_FEATURE_XSAVES)</span>
<span class="p_del">-#define cpu_has_osxsave		boot_cpu_has(X86_FEATURE_OSXSAVE)</span>
<span class="p_del">-#define cpu_has_hypervisor	boot_cpu_has(X86_FEATURE_HYPERVISOR)</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Do not add any more of those clumsy macros - use static_cpu_has() for</span>
<span class="p_del">- * fast paths and boot_cpu_has() otherwise!</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
 #if defined(CC_HAVE_ASM_GOTO) &amp;&amp; defined(CONFIG_X86_FAST_FEATURE_TESTS)
 /*
  * Static testing of CPU features.  Used the same as boot_cpu_has().
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 8f9afefd2dc5..7bfb6b70c745 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -294,6 +294,9 @@</span> <span class="p_context"></span>
 #define X86_BUG_FXSAVE_LEAK	X86_BUG(6) /* FXSAVE leaks FOP/FIP/FOP */
 #define X86_BUG_CLFLUSH_MONITOR	X86_BUG(7) /* AAI65, CLFLUSH required before MONITOR */
 #define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */
<span class="p_add">+#define X86_BUG_NULL_SEG	X86_BUG(9) /* Nulling a selector preserves the base */</span>
<span class="p_add">+#define X86_BUG_SWAPGS_FENCE	X86_BUG(10) /* SWAPGS without input dep on GS */</span>
<span class="p_add">+</span>
 
 #ifdef CONFIG_X86_32
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index 15340e36ddcb..fea7724141a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -176,7 +176,7 @@</span> <span class="p_context"> static inline void elf_common_init(struct thread_struct *t,</span>
 	regs-&gt;si = regs-&gt;di = regs-&gt;bp = 0;
 	regs-&gt;r8 = regs-&gt;r9 = regs-&gt;r10 = regs-&gt;r11 = 0;
 	regs-&gt;r12 = regs-&gt;r13 = regs-&gt;r14 = regs-&gt;r15 = 0;
<span class="p_del">-	t-&gt;fs = t-&gt;gs = 0;</span>
<span class="p_add">+	t-&gt;fsbase = t-&gt;gsbase = 0;</span>
 	t-&gt;fsindex = t-&gt;gsindex = 0;
 	t-&gt;ds = t-&gt;es = ds;
 }
<span class="p_chunk">@@ -226,8 +226,8 @@</span> <span class="p_context"> do {								\</span>
 	(pr_reg)[18] = (regs)-&gt;flags;				\
 	(pr_reg)[19] = (regs)-&gt;sp;				\
 	(pr_reg)[20] = (regs)-&gt;ss;				\
<span class="p_del">-	(pr_reg)[21] = current-&gt;thread.fs;			\</span>
<span class="p_del">-	(pr_reg)[22] = current-&gt;thread.gs;			\</span>
<span class="p_add">+	(pr_reg)[21] = current-&gt;thread.fsbase;			\</span>
<span class="p_add">+	(pr_reg)[22] = current-&gt;thread.gsbase;			\</span>
 	asm(&quot;movl %%ds,%0&quot; : &quot;=r&quot; (v)); (pr_reg)[23] = v;	\
 	asm(&quot;movl %%es,%0&quot; : &quot;=r&quot; (v)); (pr_reg)[24] = v;	\
 	asm(&quot;movl %%fs,%0&quot; : &quot;=r&quot; (v)); (pr_reg)[25] = v;	\
<span class="p_header">diff --git a/arch/x86/include/asm/hugetlb.h b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">index e6a8613fbfb0..3a106165e03a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_chunk">@@ -4,7 +4,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm-generic/hugetlb.h&gt;
 
<span class="p_del">-#define hugepages_supported() cpu_has_pse</span>
<span class="p_add">+#define hugepages_supported() boot_cpu_has(X86_FEATURE_PSE)</span>
 
 static inline int is_hugepage_only_range(struct mm_struct *mm,
 					 unsigned long addr,
<span class="p_header">diff --git a/arch/x86/include/asm/irq_work.h b/arch/x86/include/asm/irq_work.h</span>
<span class="p_header">index d0afb05c84fc..f70604125286 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/irq_work.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/irq_work.h</span>
<span class="p_chunk">@@ -5,7 +5,7 @@</span> <span class="p_context"></span>
 
 static inline bool arch_irq_work_has_interrupt(void)
 {
<span class="p_del">-	return cpu_has_apic;</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_APIC);</span>
 }
 
 #endif /* _ASM_IRQ_WORK_H */
<span class="p_header">diff --git a/arch/x86/include/asm/kgdb.h b/arch/x86/include/asm/kgdb.h</span>
<span class="p_header">index 332f98c9111f..22a8537eb780 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kgdb.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kgdb.h</span>
<span class="p_chunk">@@ -6,6 +6,8 @@</span> <span class="p_context"></span>
  * Copyright (C) 2008 Wind River Systems, Inc.
  */
 
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
 /*
  * BUFMAX defines the maximum number of characters in inbound/outbound
  * buffers at least NUMREGBYTES*2 are needed for register packets
<span class="p_header">diff --git a/arch/x86/include/asm/linkage.h b/arch/x86/include/asm/linkage.h</span>
<span class="p_header">index 79327e9483a3..0ccb26dda126 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/linkage.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/linkage.h</span>
<span class="p_chunk">@@ -8,40 +8,6 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_X86_32
 #define asmlinkage CPP_ASMLINKAGE __attribute__((regparm(0)))
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Make sure the compiler doesn&#39;t do anything stupid with the</span>
<span class="p_del">- * arguments on the stack - they are owned by the *caller*, not</span>
<span class="p_del">- * the callee. This just fools gcc into not spilling into them,</span>
<span class="p_del">- * and keeps it from doing tailcall recursion and/or using the</span>
<span class="p_del">- * stack slots for temporaries, since they are live and &quot;used&quot;</span>
<span class="p_del">- * all the way to the end of the function.</span>
<span class="p_del">- *</span>
<span class="p_del">- * NOTE! On x86-64, all the arguments are in registers, so this</span>
<span class="p_del">- * only matters on a 32-bit kernel.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define asmlinkage_protect(n, ret, args...) \</span>
<span class="p_del">-	__asmlinkage_protect##n(ret, ##args)</span>
<span class="p_del">-#define __asmlinkage_protect_n(ret, args...) \</span>
<span class="p_del">-	__asm__ __volatile__ (&quot;&quot; : &quot;=r&quot; (ret) : &quot;0&quot; (ret), ##args)</span>
<span class="p_del">-#define __asmlinkage_protect0(ret) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret)</span>
<span class="p_del">-#define __asmlinkage_protect1(ret, arg1) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1))</span>
<span class="p_del">-#define __asmlinkage_protect2(ret, arg1, arg2) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1), &quot;m&quot; (arg2))</span>
<span class="p_del">-#define __asmlinkage_protect3(ret, arg1, arg2, arg3) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1), &quot;m&quot; (arg2), &quot;m&quot; (arg3))</span>
<span class="p_del">-#define __asmlinkage_protect4(ret, arg1, arg2, arg3, arg4) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1), &quot;m&quot; (arg2), &quot;m&quot; (arg3), \</span>
<span class="p_del">-			      &quot;m&quot; (arg4))</span>
<span class="p_del">-#define __asmlinkage_protect5(ret, arg1, arg2, arg3, arg4, arg5) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1), &quot;m&quot; (arg2), &quot;m&quot; (arg3), \</span>
<span class="p_del">-			      &quot;m&quot; (arg4), &quot;m&quot; (arg5))</span>
<span class="p_del">-#define __asmlinkage_protect6(ret, arg1, arg2, arg3, arg4, arg5, arg6) \</span>
<span class="p_del">-	__asmlinkage_protect_n(ret, &quot;m&quot; (arg1), &quot;m&quot; (arg2), &quot;m&quot; (arg3), \</span>
<span class="p_del">-			      &quot;m&quot; (arg4), &quot;m&quot; (arg5), &quot;m&quot; (arg6))</span>
<span class="p_del">-</span>
 #endif /* CONFIG_X86_32 */
 
 #ifdef __ASSEMBLY__
<span class="p_header">diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h</span>
<span class="p_header">index 7a79ee2778b3..7dc1d8fef7fd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/msr.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/msr.h</span>
<span class="p_chunk">@@ -84,7 +84,10 @@</span> <span class="p_context"> static inline unsigned long long native_read_msr(unsigned int msr)</span>
 {
 	DECLARE_ARGS(val, low, high);
 
<span class="p_del">-	asm volatile(&quot;rdmsr&quot; : EAX_EDX_RET(val, low, high) : &quot;c&quot; (msr));</span>
<span class="p_add">+	asm volatile(&quot;1: rdmsr\n&quot;</span>
<span class="p_add">+		     &quot;2:\n&quot;</span>
<span class="p_add">+		     _ASM_EXTABLE_HANDLE(1b, 2b, ex_handler_rdmsr_unsafe)</span>
<span class="p_add">+		     : EAX_EDX_RET(val, low, high) : &quot;c&quot; (msr));</span>
 	if (msr_tracepoint_active(__tracepoint_read_msr))
 		do_trace_read_msr(msr, EAX_EDX_VAL(val, low, high), 0);
 	return EAX_EDX_VAL(val, low, high);
<span class="p_chunk">@@ -98,7 +101,10 @@</span> <span class="p_context"> static inline unsigned long long native_read_msr_safe(unsigned int msr,</span>
 	asm volatile(&quot;2: rdmsr ; xor %[err],%[err]\n&quot;
 		     &quot;1:\n\t&quot;
 		     &quot;.section .fixup,\&quot;ax\&quot;\n\t&quot;
<span class="p_del">-		     &quot;3:  mov %[fault],%[err] ; jmp 1b\n\t&quot;</span>
<span class="p_add">+		     &quot;3: mov %[fault],%[err]\n\t&quot;</span>
<span class="p_add">+		     &quot;xorl %%eax, %%eax\n\t&quot;</span>
<span class="p_add">+		     &quot;xorl %%edx, %%edx\n\t&quot;</span>
<span class="p_add">+		     &quot;jmp 1b\n\t&quot;</span>
 		     &quot;.previous\n\t&quot;
 		     _ASM_EXTABLE(2b, 3b)
 		     : [err] &quot;=r&quot; (*err), EAX_EDX_RET(val, low, high)
<span class="p_chunk">@@ -108,10 +114,14 @@</span> <span class="p_context"> static inline unsigned long long native_read_msr_safe(unsigned int msr,</span>
 	return EAX_EDX_VAL(val, low, high);
 }
 
<span class="p_del">-static inline void native_write_msr(unsigned int msr,</span>
<span class="p_del">-				    unsigned low, unsigned high)</span>
<span class="p_add">+/* Can be uninlined because referenced by paravirt */</span>
<span class="p_add">+notrace static inline void native_write_msr(unsigned int msr,</span>
<span class="p_add">+					    unsigned low, unsigned high)</span>
 {
<span class="p_del">-	asm volatile(&quot;wrmsr&quot; : : &quot;c&quot; (msr), &quot;a&quot;(low), &quot;d&quot; (high) : &quot;memory&quot;);</span>
<span class="p_add">+	asm volatile(&quot;1: wrmsr\n&quot;</span>
<span class="p_add">+		     &quot;2:\n&quot;</span>
<span class="p_add">+		     _ASM_EXTABLE_HANDLE(1b, 2b, ex_handler_wrmsr_unsafe)</span>
<span class="p_add">+		     : : &quot;c&quot; (msr), &quot;a&quot;(low), &quot;d&quot; (high) : &quot;memory&quot;);</span>
 	if (msr_tracepoint_active(__tracepoint_read_msr))
 		do_trace_write_msr(msr, ((u64)high &lt;&lt; 32 | low), 0);
 }
<span class="p_header">diff --git a/arch/x86/include/asm/mtrr.h b/arch/x86/include/asm/mtrr.h</span>
<span class="p_header">index b94f6f64e23d..dbff1456d215 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mtrr.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mtrr.h</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_MTRR_H
 
 #include &lt;uapi/asm/mtrr.h&gt;
<span class="p_add">+#include &lt;asm/pat.h&gt;</span>
 
 
 /*
<span class="p_chunk">@@ -83,9 +84,12 @@</span> <span class="p_context"> static inline int mtrr_trim_uncached_memory(unsigned long end_pfn)</span>
 static inline void mtrr_centaur_report_mcr(int mcr, u32 lo, u32 hi)
 {
 }
<span class="p_add">+static inline void mtrr_bp_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pat_disable(&quot;MTRRs disabled, skipping PAT initialization too.&quot;);</span>
<span class="p_add">+}</span>
 
 #define mtrr_ap_init() do {} while (0)
<span class="p_del">-#define mtrr_bp_init() do {} while (0)</span>
 #define set_mtrr_aps_delayed_init() do {} while (0)
 #define mtrr_aps_init() do {} while (0)
 #define mtrr_bp_restore() do {} while (0)
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 601f1b8f9961..3c731413f1de 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -130,21 +130,31 @@</span> <span class="p_context"> static inline void wbinvd(void)</span>
 
 #define get_kernel_rpl()  (pv_info.kernel_rpl)
 
<span class="p_del">-static inline u64 paravirt_read_msr(unsigned msr, int *err)</span>
<span class="p_add">+static inline u64 paravirt_read_msr(unsigned msr)</span>
 {
<span class="p_del">-	return PVOP_CALL2(u64, pv_cpu_ops.read_msr, msr, err);</span>
<span class="p_add">+	return PVOP_CALL1(u64, pv_cpu_ops.read_msr, msr);</span>
 }
 
<span class="p_del">-static inline int paravirt_write_msr(unsigned msr, unsigned low, unsigned high)</span>
<span class="p_add">+static inline void paravirt_write_msr(unsigned msr,</span>
<span class="p_add">+				      unsigned low, unsigned high)</span>
 {
<span class="p_del">-	return PVOP_CALL3(int, pv_cpu_ops.write_msr, msr, low, high);</span>
<span class="p_add">+	return PVOP_VCALL3(pv_cpu_ops.write_msr, msr, low, high);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u64 paravirt_read_msr_safe(unsigned msr, int *err)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return PVOP_CALL2(u64, pv_cpu_ops.read_msr_safe, msr, err);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int paravirt_write_msr_safe(unsigned msr,</span>
<span class="p_add">+					  unsigned low, unsigned high)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return PVOP_CALL3(int, pv_cpu_ops.write_msr_safe, msr, low, high);</span>
 }
 
<span class="p_del">-/* These should all do BUG_ON(_err), but our headers are too tangled. */</span>
 #define rdmsr(msr, val1, val2)			\
 do {						\
<span class="p_del">-	int _err;				\</span>
<span class="p_del">-	u64 _l = paravirt_read_msr(msr, &amp;_err);	\</span>
<span class="p_add">+	u64 _l = paravirt_read_msr(msr);	\</span>
 	val1 = (u32)_l;				\
 	val2 = _l &gt;&gt; 32;			\
 } while (0)
<span class="p_chunk">@@ -156,8 +166,7 @@</span> <span class="p_context"> do {						\</span>
 
 #define rdmsrl(msr, val)			\
 do {						\
<span class="p_del">-	int _err;				\</span>
<span class="p_del">-	val = paravirt_read_msr(msr, &amp;_err);	\</span>
<span class="p_add">+	val = paravirt_read_msr(msr);		\</span>
 } while (0)
 
 static inline void wrmsrl(unsigned msr, u64 val)
<span class="p_chunk">@@ -165,23 +174,23 @@</span> <span class="p_context"> static inline void wrmsrl(unsigned msr, u64 val)</span>
 	wrmsr(msr, (u32)val, (u32)(val&gt;&gt;32));
 }
 
<span class="p_del">-#define wrmsr_safe(msr, a, b)	paravirt_write_msr(msr, a, b)</span>
<span class="p_add">+#define wrmsr_safe(msr, a, b)	paravirt_write_msr_safe(msr, a, b)</span>
 
 /* rdmsr with exception handling */
<span class="p_del">-#define rdmsr_safe(msr, a, b)			\</span>
<span class="p_del">-({						\</span>
<span class="p_del">-	int _err;				\</span>
<span class="p_del">-	u64 _l = paravirt_read_msr(msr, &amp;_err);	\</span>
<span class="p_del">-	(*a) = (u32)_l;				\</span>
<span class="p_del">-	(*b) = _l &gt;&gt; 32;			\</span>
<span class="p_del">-	_err;					\</span>
<span class="p_add">+#define rdmsr_safe(msr, a, b)				\</span>
<span class="p_add">+({							\</span>
<span class="p_add">+	int _err;					\</span>
<span class="p_add">+	u64 _l = paravirt_read_msr_safe(msr, &amp;_err);	\</span>
<span class="p_add">+	(*a) = (u32)_l;					\</span>
<span class="p_add">+	(*b) = _l &gt;&gt; 32;				\</span>
<span class="p_add">+	_err;						\</span>
 })
 
 static inline int rdmsrl_safe(unsigned msr, unsigned long long *p)
 {
 	int err;
 
<span class="p_del">-	*p = paravirt_read_msr(msr, &amp;err);</span>
<span class="p_add">+	*p = paravirt_read_msr_safe(msr, &amp;err);</span>
 	return err;
 }
 
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index e8c2326478c8..b4a23eafa1b9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -155,10 +155,16 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 	void (*cpuid)(unsigned int *eax, unsigned int *ebx,
 		      unsigned int *ecx, unsigned int *edx);
 
<span class="p_del">-	/* MSR, PMC and TSR operations.</span>
<span class="p_del">-	   err = 0/-EFAULT.  wrmsr returns 0/-EFAULT. */</span>
<span class="p_del">-	u64 (*read_msr)(unsigned int msr, int *err);</span>
<span class="p_del">-	int (*write_msr)(unsigned int msr, unsigned low, unsigned high);</span>
<span class="p_add">+	/* Unsafe MSR operations.  These will warn or panic on failure. */</span>
<span class="p_add">+	u64 (*read_msr)(unsigned int msr);</span>
<span class="p_add">+	void (*write_msr)(unsigned int msr, unsigned low, unsigned high);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Safe MSR operations.</span>
<span class="p_add">+	 * read sets err to 0 or -EIO.  write returns 0 or -EIO.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	u64 (*read_msr_safe)(unsigned int msr, int *err);</span>
<span class="p_add">+	int (*write_msr_safe)(unsigned int msr, unsigned low, unsigned high);</span>
 
 	u64 (*read_pmc)(int counter);
 
<span class="p_header">diff --git a/arch/x86/include/asm/pat.h b/arch/x86/include/asm/pat.h</span>
<span class="p_header">index ca6c228d5e62..0b1ff4c1c14e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pat.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pat.h</span>
<span class="p_chunk">@@ -5,8 +5,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable_types.h&gt;
 
 bool pat_enabled(void);
<span class="p_add">+void pat_disable(const char *reason);</span>
 extern void pat_init(void);
<span class="p_del">-void pat_init_cache_modes(u64);</span>
 
 extern int reserve_memtype(u64 start, u64 end,
 		enum page_cache_mode req_pcm, enum page_cache_mode *ret_pcm);
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 97f3242e133c..f86491a7bc9d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -183,7 +183,7 @@</span> <span class="p_context"> static inline int pmd_trans_huge(pmd_t pmd)</span>
 
 static inline int has_transparent_hugepage(void)
 {
<span class="p_del">-	return cpu_has_pse;</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_PSE);</span>
 }
 
 #ifdef __HAVE_ARCH_PTE_DEVMAP
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9264476f3d57..9251aa962721 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -388,9 +388,16 @@</span> <span class="p_context"> struct thread_struct {</span>
 	unsigned long		ip;
 #endif
 #ifdef CONFIG_X86_64
<span class="p_del">-	unsigned long		fs;</span>
<span class="p_add">+	unsigned long		fsbase;</span>
<span class="p_add">+	unsigned long		gsbase;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * XXX: this could presumably be unsigned short.  Alternatively,</span>
<span class="p_add">+	 * 32-bit kernels could be taught to use fsindex instead.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned long fs;</span>
<span class="p_add">+	unsigned long gs;</span>
 #endif
<span class="p_del">-	unsigned long		gs;</span>
 
 	/* Save middle states of ptrace breakpoints */
 	struct perf_event	*ptrace_bps[HBP_NUM];
<span class="p_header">diff --git a/arch/x86/include/asm/segment.h b/arch/x86/include/asm/segment.h</span>
<span class="p_header">index 7d5a1929d76b..1549caa098f0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/segment.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/segment.h</span>
<span class="p_chunk">@@ -2,6 +2,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_SEGMENT_H
 
 #include &lt;linux/const.h&gt;
<span class="p_add">+#include &lt;asm/alternative.h&gt;</span>
 
 /*
  * Constructor for a conventional segment GDT (or LDT) entry.
<span class="p_chunk">@@ -207,13 +208,6 @@</span> <span class="p_context"></span>
 #define __USER_CS			(GDT_ENTRY_DEFAULT_USER_CS*8 + 3)
 #define __PER_CPU_SEG			(GDT_ENTRY_PER_CPU*8 + 3)
 
<span class="p_del">-/* TLS indexes for 64-bit - hardcoded in arch_prctl(): */</span>
<span class="p_del">-#define FS_TLS				0</span>
<span class="p_del">-#define GS_TLS				1</span>
<span class="p_del">-</span>
<span class="p_del">-#define GS_TLS_SEL			((GDT_ENTRY_TLS_MIN+GS_TLS)*8 + 3)</span>
<span class="p_del">-#define FS_TLS_SEL			((GDT_ENTRY_TLS_MIN+FS_TLS)*8 + 3)</span>
<span class="p_del">-</span>
 #endif
 
 #ifndef CONFIG_PARAVIRT
<span class="p_chunk">@@ -249,10 +243,13 @@</span> <span class="p_context"> extern const char early_idt_handler_array[NUM_EXCEPTION_VECTORS][EARLY_IDT_HANDL</span>
 #endif
 
 /*
<span class="p_del">- * Load a segment. Fall back on loading the zero</span>
<span class="p_del">- * segment if something goes wrong..</span>
<span class="p_add">+ * Load a segment. Fall back on loading the zero segment if something goes</span>
<span class="p_add">+ * wrong.  This variant assumes that loading zero fully clears the segment.</span>
<span class="p_add">+ * This is always the case on Intel CPUs and, even on 64-bit AMD CPUs, any</span>
<span class="p_add">+ * failure to fully clear the cached descriptor is only observable for</span>
<span class="p_add">+ * FS and GS.</span>
  */
<span class="p_del">-#define loadsegment(seg, value)						\</span>
<span class="p_add">+#define __loadsegment_simple(seg, value)				\</span>
 do {									\
 	unsigned short __val = (value);					\
 									\
<span class="p_chunk">@@ -269,6 +266,38 @@</span> <span class="p_context"> do {									\</span>
 		     : &quot;+r&quot; (__val) : : &quot;memory&quot;);			\
 } while (0)
 
<span class="p_add">+#define __loadsegment_ss(value) __loadsegment_simple(ss, (value))</span>
<span class="p_add">+#define __loadsegment_ds(value) __loadsegment_simple(ds, (value))</span>
<span class="p_add">+#define __loadsegment_es(value) __loadsegment_simple(es, (value))</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * On 32-bit systems, the hidden parts of FS and GS are unobservable if</span>
<span class="p_add">+ * the selector is NULL, so there&#39;s no funny business here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __loadsegment_fs(value) __loadsegment_simple(fs, (value))</span>
<span class="p_add">+#define __loadsegment_gs(value) __loadsegment_simple(gs, (value))</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __loadsegment_fs(unsigned short value)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;						\n&quot;</span>
<span class="p_add">+		     &quot;1:	movw %0, %%fs			\n&quot;</span>
<span class="p_add">+		     &quot;2:					\n&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+		     _ASM_EXTABLE_HANDLE(1b, 2b, ex_handler_clear_fs)</span>
<span class="p_add">+</span>
<span class="p_add">+		     : : &quot;rm&quot; (value) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* __loadsegment_gs is intentionally undefined.  Use load_gs_index instead. */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define loadsegment(seg, value) __loadsegment_ ## seg (value)</span>
<span class="p_add">+</span>
 /*
  * Save a segment register away:
  */
<span class="p_header">diff --git a/arch/x86/include/asm/setup.h b/arch/x86/include/asm/setup.h</span>
<span class="p_header">index 11af24e09c8a..ac1d5da14734 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/setup.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/setup.h</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"></span>
 #define COMMAND_LINE_SIZE 2048
 
 #include &lt;linux/linkage.h&gt;
<span class="p_add">+#include &lt;asm/page_types.h&gt;</span>
 
 #ifdef __i386__
 
<span class="p_header">diff --git a/arch/x86/include/asm/switch_to.h b/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">index 751bf4b7bf11..8f321a1b03a1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/switch_to.h</span>
<span class="p_chunk">@@ -39,8 +39,7 @@</span> <span class="p_context"> do {									\</span>
 	 */								\
 	unsigned long ebx, ecx, edx, esi, edi;				\
 									\
<span class="p_del">-	asm volatile(&quot;pushfl\n\t&quot;		/* save    flags */	\</span>
<span class="p_del">-		     &quot;pushl %%ebp\n\t&quot;		/* save    EBP   */	\</span>
<span class="p_add">+	asm volatile(&quot;pushl %%ebp\n\t&quot;		/* save    EBP   */	\</span>
 		     &quot;movl %%esp,%[prev_sp]\n\t&quot;	/* save    ESP   */ \
 		     &quot;movl %[next_sp],%%esp\n\t&quot;	/* restore ESP   */ \
 		     &quot;movl $1f,%[prev_ip]\n\t&quot;	/* save    EIP   */	\
<span class="p_chunk">@@ -49,7 +48,6 @@</span> <span class="p_context"> do {									\</span>
 		     &quot;jmp __switch_to\n&quot;	/* regparm call  */	\
 		     &quot;1:\t&quot;						\
 		     &quot;popl %%ebp\n\t&quot;		/* restore EBP   */	\
<span class="p_del">-		     &quot;popfl\n&quot;			/* restore flags */	\</span>
 									\
 		     /* output parameters */				\
 		     : [prev_sp] &quot;=m&quot; (prev-&gt;thread.sp),		\
<span class="p_header">diff --git a/arch/x86/include/asm/text-patching.h b/arch/x86/include/asm/text-patching.h</span>
new file mode 100644
<span class="p_header">index 000000000000..90395063383c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/text-patching.h</span>
<span class="p_chunk">@@ -0,0 +1,40 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_TEXT_PATCHING_H</span>
<span class="p_add">+#define _ASM_X86_TEXT_PATCHING_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/stddef.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct paravirt_patch_site;</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT</span>
<span class="p_add">+void apply_paravirt(struct paravirt_patch_site *start,</span>
<span class="p_add">+		    struct paravirt_patch_site *end);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void apply_paravirt(struct paravirt_patch_site *start,</span>
<span class="p_add">+				  struct paravirt_patch_site *end)</span>
<span class="p_add">+{}</span>
<span class="p_add">+#define __parainstructions	NULL</span>
<span class="p_add">+#define __parainstructions_end	NULL</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+extern void *text_poke_early(void *addr, const void *opcode, size_t len);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clear and restore the kernel write-protection flag on the local CPU.</span>
<span class="p_add">+ * Allows the kernel to edit read-only pages.</span>
<span class="p_add">+ * Side-effect: any interrupt handler running between save and restore will have</span>
<span class="p_add">+ * the ability to write to read-only pages.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Warning:</span>
<span class="p_add">+ * Code patching in the UP case is safe if NMIs and MCE handlers are stopped and</span>
<span class="p_add">+ * no thread can be preempted in the instructions being modified (no iret to an</span>
<span class="p_add">+ * invalid instruction possible) or if the instructions are changed from a</span>
<span class="p_add">+ * consistent state to another consistent state atomically.</span>
<span class="p_add">+ * On the local CPU you need to be protected again NMI or MCE handlers seeing an</span>
<span class="p_add">+ * inconsistent instruction while you patch.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void *text_poke(void *addr, const void *opcode, size_t len);</span>
<span class="p_add">+extern int poke_int3_handler(struct pt_regs *regs);</span>
<span class="p_add">+extern void *text_poke_bp(void *addr, const void *opcode, size_t len, void *handler);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_TEXT_PATCHING_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">index ffae84df8a93..30c133ac05cd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -255,7 +255,7 @@</span> <span class="p_context"> static inline bool test_and_clear_restore_sigmask(void)</span>
 	return true;
 }
 
<span class="p_del">-static inline bool is_ia32_task(void)</span>
<span class="p_add">+static inline bool in_ia32_syscall(void)</span>
 {
 #ifdef CONFIG_X86_32
 	return true;
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 1fde8d580a5b..4e5be94e079a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -181,7 +181,7 @@</span> <span class="p_context"> static inline void __native_flush_tlb_single(unsigned long addr)</span>
 
 static inline void __flush_tlb_all(void)
 {
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PGE))</span>
 		__flush_tlb_global();
 	else
 		__flush_tlb();
<span class="p_header">diff --git a/arch/x86/include/asm/tsc.h b/arch/x86/include/asm/tsc.h</span>
<span class="p_header">index 174c4212780a..7428697c5b8d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tsc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tsc.h</span>
<span class="p_chunk">@@ -22,7 +22,7 @@</span> <span class="p_context"> extern void disable_TSC(void);</span>
 static inline cycles_t get_cycles(void)
 {
 #ifndef CONFIG_X86_TSC
<span class="p_del">-	if (!cpu_has_tsc)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC))</span>
 		return 0;
 #endif
 
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">index a969ae607be8..d794fd1f582f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -110,7 +110,7 @@</span> <span class="p_context"> struct exception_table_entry {</span>
 
 extern int fixup_exception(struct pt_regs *regs, int trapnr);
 extern bool ex_has_fault_handler(unsigned long ip);
<span class="p_del">-extern int early_fixup_exception(unsigned long *ip);</span>
<span class="p_add">+extern void early_fixup_exception(struct pt_regs *regs, int trapnr);</span>
 
 /*
  * These are the main single-value transfer routines.  They automatically
<span class="p_header">diff --git a/arch/x86/include/asm/xor_32.h b/arch/x86/include/asm/xor_32.h</span>
<span class="p_header">index c54beb44c4c1..635eac543922 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/xor_32.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/xor_32.h</span>
<span class="p_chunk">@@ -550,7 +550,7 @@</span> <span class="p_context"> static struct xor_block_template xor_block_pIII_sse = {</span>
 #define XOR_TRY_TEMPLATES				\
 do {							\
 	AVX_XOR_SPEED;					\
<span class="p_del">-	if (cpu_has_xmm) {				\</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XMM)) {				\</span>
 		xor_speed(&amp;xor_block_pIII_sse);		\
 		xor_speed(&amp;xor_block_sse_pf64);		\
 	} else if (boot_cpu_has(X86_FEATURE_MMX)) {	\
<span class="p_header">diff --git a/arch/x86/include/asm/xor_avx.h b/arch/x86/include/asm/xor_avx.h</span>
<span class="p_header">index 7c0a517ec751..22a7b1870a31 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/xor_avx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/xor_avx.h</span>
<span class="p_chunk">@@ -167,12 +167,12 @@</span> <span class="p_context"> static struct xor_block_template xor_block_avx = {</span>
 
 #define AVX_XOR_SPEED \
 do { \
<span class="p_del">-	if (cpu_has_avx &amp;&amp; cpu_has_osxsave) \</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_AVX) &amp;&amp; boot_cpu_has(X86_FEATURE_OSXSAVE)) \</span>
 		xor_speed(&amp;xor_block_avx); \
 } while (0)
 
 #define AVX_SELECT(FASTEST) \
<span class="p_del">-	(cpu_has_avx &amp;&amp; cpu_has_osxsave ? &amp;xor_block_avx : FASTEST)</span>
<span class="p_add">+	(boot_cpu_has(X86_FEATURE_AVX) &amp;&amp; boot_cpu_has(X86_FEATURE_OSXSAVE) ? &amp;xor_block_avx : FASTEST)</span>
 
 #else
 
<span class="p_header">diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c</span>
<span class="p_header">index 8c2f1ef6ca23..2522e564269e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/acpi/boot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/acpi/boot.c</span>
<span class="p_chunk">@@ -136,7 +136,7 @@</span> <span class="p_context"> static int __init acpi_parse_madt(struct acpi_table_header *table)</span>
 {
 	struct acpi_table_madt *madt = NULL;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -EINVAL;
 
 	madt = (struct acpi_table_madt *)table;
<span class="p_chunk">@@ -951,7 +951,7 @@</span> <span class="p_context"> static int __init early_acpi_parse_madt_lapic_addr_ovr(void)</span>
 {
 	int count;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -ENODEV;
 
 	/*
<span class="p_chunk">@@ -979,7 +979,7 @@</span> <span class="p_context"> static int __init acpi_parse_madt_lapic_entries(void)</span>
 	int ret;
 	struct acpi_subtable_proc madt_proc[2];
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -ENODEV;
 
 	/*
<span class="p_chunk">@@ -1125,7 +1125,7 @@</span> <span class="p_context"> static int __init acpi_parse_madt_ioapic_entries(void)</span>
 	if (acpi_disabled || acpi_noirq)
 		return -ENODEV;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -ENODEV;
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c</span>
<span class="p_header">index 25f909362b7a..5cb272a7a5a3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/alternative.c</span>
<span class="p_header">+++ b/arch/x86/kernel/alternative.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/stop_machine.h&gt;
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/kdebug.h&gt;
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/sections.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">index d356987a04e9..60078a67d7e3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic.c</span>
<span class="p_chunk">@@ -607,7 +607,7 @@</span> <span class="p_context"> static void __init lapic_cal_handler(struct clock_event_device *dev)</span>
 	long tapic = apic_read(APIC_TMCCT);
 	unsigned long pm = acpi_pm_read_early();
 
<span class="p_del">-	if (cpu_has_tsc)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_TSC))</span>
 		tsc = rdtsc();
 
 	switch (lapic_cal_loops++) {
<span class="p_chunk">@@ -668,7 +668,7 @@</span> <span class="p_context"> calibrate_by_pmtimer(long deltapm, long *delta, long *deltatsc)</span>
 	*delta = (long)res;
 
 	/* Correct the tsc counter value */
<span class="p_del">-	if (cpu_has_tsc) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_TSC)) {</span>
 		res = (((u64)(*deltatsc)) * pm_100ms);
 		do_div(res, deltapm);
 		apic_printk(APIC_VERBOSE, &quot;TSC delta adjusted to &quot;
<span class="p_chunk">@@ -760,7 +760,7 @@</span> <span class="p_context"> static int __init calibrate_APIC_clock(void)</span>
 	apic_printk(APIC_VERBOSE, &quot;..... calibration result: %u\n&quot;,
 		    lapic_timer_frequency);
 
<span class="p_del">-	if (cpu_has_tsc) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_TSC)) {</span>
 		apic_printk(APIC_VERBOSE, &quot;..... CPU clock speed is &quot;
 			    &quot;%ld.%04ld MHz.\n&quot;,
 			    (deltatsc / LAPIC_CAL_LOOPS) / (1000000 / HZ),
<span class="p_chunk">@@ -1085,7 +1085,7 @@</span> <span class="p_context"> void lapic_shutdown(void)</span>
 {
 	unsigned long flags;
 
<span class="p_del">-	if (!cpu_has_apic &amp;&amp; !apic_from_smp_config())</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; !apic_from_smp_config())</span>
 		return;
 
 	local_irq_save(flags);
<span class="p_chunk">@@ -1134,7 +1134,7 @@</span> <span class="p_context"> void __init init_bsp_APIC(void)</span>
 	 * Don&#39;t do the setup now if we have a SMP BIOS as the
 	 * through-I/O-APIC virtual wire mode might be active.
 	 */
<span class="p_del">-	if (smp_found_config || !cpu_has_apic)</span>
<span class="p_add">+	if (smp_found_config || !boot_cpu_has(X86_FEATURE_APIC))</span>
 		return;
 
 	/*
<span class="p_chunk">@@ -1227,7 +1227,7 @@</span> <span class="p_context"> void setup_local_APIC(void)</span>
 	unsigned long long tsc = 0, ntsc;
 	long long max_loops = cpu_khz ? cpu_khz : 1000000;
 
<span class="p_del">-	if (cpu_has_tsc)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_TSC))</span>
 		tsc = rdtsc();
 
 	if (disable_apic) {
<span class="p_chunk">@@ -1311,7 +1311,7 @@</span> <span class="p_context"> void setup_local_APIC(void)</span>
 			break;
 		}
 		if (queued) {
<span class="p_del">-			if (cpu_has_tsc &amp;&amp; cpu_khz) {</span>
<span class="p_add">+			if (boot_cpu_has(X86_FEATURE_TSC) &amp;&amp; cpu_khz) {</span>
 				ntsc = rdtsc();
 				max_loops = (cpu_khz &lt;&lt; 10) - (ntsc - tsc);
 			} else
<span class="p_chunk">@@ -1445,7 +1445,7 @@</span> <span class="p_context"> static void __x2apic_disable(void)</span>
 {
 	u64 msr;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return;
 
 	rdmsrl(MSR_IA32_APICBASE, msr);
<span class="p_chunk">@@ -1561,7 +1561,7 @@</span> <span class="p_context"> void __init check_x2apic(void)</span>
 		pr_info(&quot;x2apic: enabled by BIOS, switching to x2apic ops\n&quot;);
 		x2apic_mode = 1;
 		x2apic_state = X2APIC_ON;
<span class="p_del">-	} else if (!cpu_has_x2apic) {</span>
<span class="p_add">+	} else if (!boot_cpu_has(X86_FEATURE_X2APIC)) {</span>
 		x2apic_state = X2APIC_DISABLED;
 	}
 }
<span class="p_chunk">@@ -1632,7 +1632,7 @@</span> <span class="p_context"> void __init enable_IR_x2apic(void)</span>
  */
 static int __init detect_init_APIC(void)
 {
<span class="p_del">-	if (!cpu_has_apic) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		pr_info(&quot;No local APIC present\n&quot;);
 		return -1;
 	}
<span class="p_chunk">@@ -1711,14 +1711,14 @@</span> <span class="p_context"> static int __init detect_init_APIC(void)</span>
 		goto no_apic;
 	case X86_VENDOR_INTEL:
 		if (boot_cpu_data.x86 == 6 || boot_cpu_data.x86 == 15 ||
<span class="p_del">-		    (boot_cpu_data.x86 == 5 &amp;&amp; cpu_has_apic))</span>
<span class="p_add">+		    (boot_cpu_data.x86 == 5 &amp;&amp; boot_cpu_has(X86_FEATURE_APIC)))</span>
 			break;
 		goto no_apic;
 	default:
 		goto no_apic;
 	}
 
<span class="p_del">-	if (!cpu_has_apic) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		/*
 		 * Over-ride BIOS and try to enable the local APIC only if
 		 * &quot;lapic&quot; specified.
<span class="p_chunk">@@ -2233,19 +2233,19 @@</span> <span class="p_context"> int __init APIC_init_uniprocessor(void)</span>
 		return -1;
 	}
 #ifdef CONFIG_X86_64
<span class="p_del">-	if (!cpu_has_apic) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		disable_apic = 1;
 		pr_info(&quot;Apic disabled by BIOS\n&quot;);
 		return -1;
 	}
 #else
<span class="p_del">-	if (!smp_found_config &amp;&amp; !cpu_has_apic)</span>
<span class="p_add">+	if (!smp_found_config &amp;&amp; !boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -1;
 
 	/*
 	 * Complain if the BIOS pretends there is one.
 	 */
<span class="p_del">-	if (!cpu_has_apic &amp;&amp;</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC) &amp;&amp;</span>
 	    APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid])) {
 		pr_err(&quot;BIOS bug, local APIC 0x%x not detected!...\n&quot;,
 			boot_cpu_physical_apicid);
<span class="p_chunk">@@ -2426,7 +2426,7 @@</span> <span class="p_context"> static void apic_pm_activate(void)</span>
 static int __init init_lapic_sysfs(void)
 {
 	/* XXX: remove suspend/resume procs if !apic_pm_state.active? */
<span class="p_del">-	if (cpu_has_apic)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC))</span>
 		register_syscore_ops(&amp;lapic_syscore_ops);
 
 	return 0;
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic_noop.c b/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_header">index 331a7a07c48f..13d19ed58514 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_chunk">@@ -100,13 +100,13 @@</span> <span class="p_context"> static void noop_vector_allocation_domain(int cpu, struct cpumask *retmask,</span>
 
 static u32 noop_apic_read(u32 reg)
 {
<span class="p_del">-	WARN_ON_ONCE((cpu_has_apic &amp;&amp; !disable_apic));</span>
<span class="p_add">+	WARN_ON_ONCE(boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; !disable_apic);</span>
 	return 0;
 }
 
 static void noop_apic_write(u32 reg, u32 v)
 {
<span class="p_del">-	WARN_ON_ONCE(cpu_has_apic &amp;&amp; !disable_apic);</span>
<span class="p_add">+	WARN_ON_ONCE(boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; !disable_apic);</span>
 }
 
 struct apic apic_noop = {
<span class="p_header">diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_header">index fdb0fbfb1197..84e33ff5a6d5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_chunk">@@ -1454,7 +1454,7 @@</span> <span class="p_context"> void native_disable_io_apic(void)</span>
 		ioapic_write_entry(ioapic_i8259.apic, ioapic_i8259.pin, entry);
 	}
 
<span class="p_del">-	if (cpu_has_apic || apic_from_smp_config())</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC) || apic_from_smp_config())</span>
 		disconnect_bsp_APIC(ioapic_i8259.pin != -1);
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/apic/ipi.c b/arch/x86/kernel/apic/ipi.c</span>
<span class="p_header">index 28bde88b0085..2a0f225afebd 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/ipi.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/ipi.c</span>
<span class="p_chunk">@@ -230,7 +230,7 @@</span> <span class="p_context"> int safe_smp_processor_id(void)</span>
 {
 	int apicid, cpuid;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return 0;
 
 	apicid = hard_smp_processor_id();
<span class="p_header">diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">index ef495511f019..a5e400afc563 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/vector.c</span>
<span class="p_chunk">@@ -944,7 +944,7 @@</span> <span class="p_context"> static int __init print_ICs(void)</span>
 	print_PIC();
 
 	/* don&#39;t print out if apic is not there */
<span class="p_del">-	if (!cpu_has_apic &amp;&amp; !apic_from_smp_config())</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; !apic_from_smp_config())</span>
 		return 0;
 
 	print_local_APICs(show_lapic);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index 7b76eb67a9b3..c343a54bed39 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -565,14 +565,17 @@</span> <span class="p_context"> static void early_init_amd(struct cpuinfo_x86 *c)</span>
 	 * can safely set X86_FEATURE_EXTD_APICID unconditionally for families
 	 * after 16h.
 	 */
<span class="p_del">-	if (cpu_has_apic &amp;&amp; c-&gt;x86 &gt; 0x16) {</span>
<span class="p_del">-		set_cpu_cap(c, X86_FEATURE_EXTD_APICID);</span>
<span class="p_del">-	} else if (cpu_has_apic &amp;&amp; c-&gt;x86 &gt;= 0xf) {</span>
<span class="p_del">-		/* check CPU config space for extended APIC ID */</span>
<span class="p_del">-		unsigned int val;</span>
<span class="p_del">-		val = read_pci_config(0, 24, 0, 0x68);</span>
<span class="p_del">-		if ((val &amp; ((1 &lt;&lt; 17) | (1 &lt;&lt; 18))) == ((1 &lt;&lt; 17) | (1 &lt;&lt; 18)))</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC)) {</span>
<span class="p_add">+		if (c-&gt;x86 &gt; 0x16)</span>
 			set_cpu_cap(c, X86_FEATURE_EXTD_APICID);
<span class="p_add">+		else if (c-&gt;x86 &gt;= 0xf) {</span>
<span class="p_add">+			/* check CPU config space for extended APIC ID */</span>
<span class="p_add">+			unsigned int val;</span>
<span class="p_add">+</span>
<span class="p_add">+			val = read_pci_config(0, 24, 0, 0x68);</span>
<span class="p_add">+			if ((val &gt;&gt; 17 &amp; 0x3) == 0x3)</span>
<span class="p_add">+				set_cpu_cap(c, X86_FEATURE_EXTD_APICID);</span>
<span class="p_add">+		}</span>
 	}
 #endif
 
<span class="p_chunk">@@ -628,6 +631,7 @@</span> <span class="p_context"> static void init_amd_k8(struct cpuinfo_x86 *c)</span>
 	 */
 	msr_set_bit(MSR_K7_HWCR, 6);
 #endif
<span class="p_add">+	set_cpu_bug(c, X86_BUG_SWAPGS_FENCE);</span>
 }
 
 static void init_amd_gh(struct cpuinfo_x86 *c)
<span class="p_chunk">@@ -746,7 +750,7 @@</span> <span class="p_context"> static void init_amd(struct cpuinfo_x86 *c)</span>
 	if (c-&gt;x86 &gt;= 0xf)
 		set_cpu_cap(c, X86_FEATURE_K8);
 
<span class="p_del">-	if (cpu_has_xmm2) {</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_XMM2)) {</span>
 		/* MFENCE stops RDTSC speculation */
 		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);
 	}
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 8394b3d1f94f..088106140c4b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -430,7 +430,7 @@</span> <span class="p_context"> void load_percpu_segment(int cpu)</span>
 #ifdef CONFIG_X86_32
 	loadsegment(fs, __KERNEL_PERCPU);
 #else
<span class="p_del">-	loadsegment(gs, 0);</span>
<span class="p_add">+	__loadsegment_simple(gs, 0);</span>
 	wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
 #endif
 	load_stack_canary_segment();
<span class="p_chunk">@@ -862,30 +862,34 @@</span> <span class="p_context"> static void detect_nopl(struct cpuinfo_x86 *c)</span>
 #else
 	set_cpu_cap(c, X86_FEATURE_NOPL);
 #endif
<span class="p_add">+}</span>
 
<span class="p_add">+static void detect_null_seg_behavior(struct cpuinfo_x86 *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 	/*
<span class="p_del">-	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt</span>
<span class="p_del">-	 * systems that run Linux at CPL &gt; 0 may or may not have the</span>
<span class="p_del">-	 * issue, but, even if they have the issue, there&#39;s absolutely</span>
<span class="p_del">-	 * nothing we can do about it because we can&#39;t use the real IRET</span>
<span class="p_del">-	 * instruction.</span>
<span class="p_add">+	 * Empirically, writing zero to a segment selector on AMD does</span>
<span class="p_add">+	 * not clear the base, whereas writing zero to a segment</span>
<span class="p_add">+	 * selector on Intel does clear the base.  Intel&#39;s behavior</span>
<span class="p_add">+	 * allows slightly faster context switches in the common case</span>
<span class="p_add">+	 * where GS is unused by the prev and next threads.</span>
 	 *
<span class="p_del">-	 * NB: For the time being, only 32-bit kernels support</span>
<span class="p_del">-	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose</span>
<span class="p_del">-	 * whether to apply espfix using paravirt hooks.  If any</span>
<span class="p_del">-	 * non-paravirt system ever shows up that does *not* have the</span>
<span class="p_del">-	 * ESPFIX issue, we can change this.</span>
<span class="p_add">+	 * Since neither vendor documents this anywhere that I can see,</span>
<span class="p_add">+	 * detect it directly instead of hardcoding the choice by</span>
<span class="p_add">+	 * vendor.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * I&#39;ve designated AMD&#39;s behavior as the &quot;bug&quot; because it&#39;s</span>
<span class="p_add">+	 * counterintuitive and less friendly.</span>
 	 */
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-#ifdef CONFIG_PARAVIRT</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		extern void native_iret(void);</span>
<span class="p_del">-		if (pv_cpu_ops.iret == native_iret)</span>
<span class="p_del">-			set_cpu_bug(c, X86_BUG_ESPFIX);</span>
<span class="p_del">-	} while (0);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	set_cpu_bug(c, X86_BUG_ESPFIX);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned long old_base, tmp;</span>
<span class="p_add">+	rdmsrl(MSR_FS_BASE, old_base);</span>
<span class="p_add">+	wrmsrl(MSR_FS_BASE, 1);</span>
<span class="p_add">+	loadsegment(fs, 0);</span>
<span class="p_add">+	rdmsrl(MSR_FS_BASE, tmp);</span>
<span class="p_add">+	if (tmp != 0)</span>
<span class="p_add">+		set_cpu_bug(c, X86_BUG_NULL_SEG);</span>
<span class="p_add">+	wrmsrl(MSR_FS_BASE, old_base);</span>
 #endif
 }
 
<span class="p_chunk">@@ -921,6 +925,33 @@</span> <span class="p_context"> static void generic_identify(struct cpuinfo_x86 *c)</span>
 	get_model_name(c); /* Default name */
 
 	detect_nopl(c);
<span class="p_add">+</span>
<span class="p_add">+	detect_null_seg_behavior(c);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ESPFIX is a strange bug.  All real CPUs have it.  Paravirt</span>
<span class="p_add">+	 * systems that run Linux at CPL &gt; 0 may or may not have the</span>
<span class="p_add">+	 * issue, but, even if they have the issue, there&#39;s absolutely</span>
<span class="p_add">+	 * nothing we can do about it because we can&#39;t use the real IRET</span>
<span class="p_add">+	 * instruction.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * NB: For the time being, only 32-bit kernels support</span>
<span class="p_add">+	 * X86_BUG_ESPFIX as such.  64-bit kernels directly choose</span>
<span class="p_add">+	 * whether to apply espfix using paravirt hooks.  If any</span>
<span class="p_add">+	 * non-paravirt system ever shows up that does *not* have the</span>
<span class="p_add">+	 * ESPFIX issue, we can change this.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+# ifdef CONFIG_PARAVIRT</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		extern void native_iret(void);</span>
<span class="p_add">+		if (pv_cpu_ops.iret == native_iret)</span>
<span class="p_add">+			set_cpu_bug(c, X86_BUG_ESPFIX);</span>
<span class="p_add">+	} while (0);</span>
<span class="p_add">+# else</span>
<span class="p_add">+	set_cpu_bug(c, X86_BUG_ESPFIX);</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
 }
 
 static void x86_init_cache_qos(struct cpuinfo_x86 *c)
<span class="p_chunk">@@ -1076,12 +1107,12 @@</span> <span class="p_context"> void enable_sep_cpu(void)</span>
 	struct tss_struct *tss;
 	int cpu;
 
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_SEP))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	cpu = get_cpu();
 	tss = &amp;per_cpu(cpu_tss, cpu);
 
<span class="p_del">-	if (!boot_cpu_has(X86_FEATURE_SEP))</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
 	/*
 	 * We cache MSR_IA32_SYSENTER_CS&#39;s value in the TSS&#39;s ss1 field --
 	 * see the big comment in struct x86_hw_tss&#39;s definition.
<span class="p_chunk">@@ -1096,7 +1127,6 @@</span> <span class="p_context"> void enable_sep_cpu(void)</span>
 
 	wrmsr(MSR_IA32_SYSENTER_EIP, (unsigned long)entry_SYSENTER_32, 0);
 
<span class="p_del">-out:</span>
 	put_cpu();
 }
 #endif
<span class="p_chunk">@@ -1528,7 +1558,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	pr_info(&quot;Initializing CPU#%d\n&quot;, cpu);
 
 	if (cpu_feature_enabled(X86_FEATURE_VME) ||
<span class="p_del">-	    cpu_has_tsc ||</span>
<span class="p_add">+	    boot_cpu_has(X86_FEATURE_TSC) ||</span>
 	    boot_cpu_has(X86_FEATURE_DE))
 		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/cyrix.c b/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_header">index 6adef9cac23e..bd9dcd6b712d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/cyrix.c</span>
<span class="p_chunk">@@ -333,7 +333,7 @@</span> <span class="p_context"> static void init_cyrix(struct cpuinfo_x86 *c)</span>
 		switch (dir0_lsn) {
 		case 0xd:  /* either a 486SLC or DLC w/o DEVID */
 			dir0_msn = 0;
<span class="p_del">-			p = Cx486_name[(cpu_has_fpu ? 1 : 0)];</span>
<span class="p_add">+			p = Cx486_name[!!boot_cpu_has(X86_FEATURE_FPU)];</span>
 			break;
 
 		case 0xe:  /* a 486S A step */
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index 1f7fdb91a818..1d5582259b20 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -152,9 +152,9 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 	 *  the TLB when any changes are made to any of the page table entries.
 	 *  The operating system must reload CR3 to cause the TLB to be flushed&quot;
 	 *
<span class="p_del">-	 * As a result cpu_has_pge() in arch/x86/include/asm/tlbflush.h should</span>
<span class="p_del">-	 * be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE</span>
<span class="p_del">-	 * to be modified</span>
<span class="p_add">+	 * As a result, boot_cpu_has(X86_FEATURE_PGE) in arch/x86/include/asm/tlbflush.h</span>
<span class="p_add">+	 * should be false so that __flush_tlb_all() causes CR3 insted of CR4.PGE</span>
<span class="p_add">+	 * to be modified.</span>
 	 */
 	if (c-&gt;x86 == 5 &amp;&amp; c-&gt;x86_model == 9) {
 		pr_info(&quot;Disabling PGE capability bit\n&quot;);
<span class="p_chunk">@@ -281,7 +281,7 @@</span> <span class="p_context"> static void intel_workarounds(struct cpuinfo_x86 *c)</span>
 	 * integrated APIC (see 11AP erratum in &quot;Pentium Processor
 	 * Specification Update&quot;).
 	 */
<span class="p_del">-	if (cpu_has_apic &amp;&amp; (c-&gt;x86&lt;&lt;8 | c-&gt;x86_model&lt;&lt;4) == 0x520 &amp;&amp;</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; (c-&gt;x86&lt;&lt;8 | c-&gt;x86_model&lt;&lt;4) == 0x520 &amp;&amp;</span>
 	    (c-&gt;x86_mask &lt; 0x6 || c-&gt;x86_mask == 0xb))
 		set_cpu_bug(c, X86_BUG_11AP);
 
<span class="p_chunk">@@ -456,7 +456,7 @@</span> <span class="p_context"> static void init_intel(struct cpuinfo_x86 *c)</span>
 			set_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);
 	}
 
<span class="p_del">-	if (cpu_has_xmm2)</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_XMM2))</span>
 		set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);
 
 	if (boot_cpu_has(X86_FEATURE_DS)) {
<span class="p_chunk">@@ -468,7 +468,7 @@</span> <span class="p_context"> static void init_intel(struct cpuinfo_x86 *c)</span>
 			set_cpu_cap(c, X86_FEATURE_PEBS);
 	}
 
<span class="p_del">-	if (c-&gt;x86 == 6 &amp;&amp; cpu_has_clflush &amp;&amp;</span>
<span class="p_add">+	if (c-&gt;x86 == 6 &amp;&amp; boot_cpu_has(X86_FEATURE_CLFLUSH) &amp;&amp;</span>
 	    (c-&gt;x86_model == 29 || c-&gt;x86_model == 46 || c-&gt;x86_model == 47))
 		set_cpu_bug(c, X86_BUG_CLFLUSH_MONITOR);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/mce_intel.c b/arch/x86/kernel/cpu/mcheck/mce_intel.c</span>
<span class="p_header">index 1e8bb6c94f14..1defb8ea882c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/mce_intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/mce_intel.c</span>
<span class="p_chunk">@@ -84,7 +84,7 @@</span> <span class="p_context"> static int cmci_supported(int *banks)</span>
 	 */
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
 		return 0;
<span class="p_del">-	if (!cpu_has_apic || lapic_get_maxlvt() &lt; 6)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC) || lapic_get_maxlvt() &lt; 6)</span>
 		return 0;
 	rdmsrl(MSR_IA32_MCG_CAP, cap);
 	*banks = min_t(unsigned, MAX_NR_BANKS, cap &amp; 0xff);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/therm_throt.c b/arch/x86/kernel/cpu/mcheck/therm_throt.c</span>
<span class="p_header">index ac780cad3b86..6b9dc4d18ccc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/therm_throt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/therm_throt.c</span>
<span class="p_chunk">@@ -450,7 +450,7 @@</span> <span class="p_context"> asmlinkage __visible void smp_trace_thermal_interrupt(struct pt_regs *regs)</span>
 /* Thermal monitoring depends on APIC, ACPI and clock modulation */
 static int intel_thermal_supported(struct cpuinfo_x86 *c)
 {
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return 0;
 	if (!cpu_has(c, X86_FEATURE_ACPI) || !cpu_has(c, X86_FEATURE_ACC))
 		return 0;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/cyrix.c b/arch/x86/kernel/cpu/mtrr/cyrix.c</span>
<span class="p_header">index f8c81ba0b465..b1086f79e57e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/cyrix.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/cyrix.c</span>
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> static void prepare_set(void)</span>
 	u32 cr0;
 
 	/*  Save value of CR4 and clear Page Global Enable (bit 7)  */
<span class="p_del">-	if (cpu_has_pge) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
 		cr4 = __read_cr4();
 		__write_cr4(cr4 &amp; ~X86_CR4_PGE);
 	}
<span class="p_chunk">@@ -170,7 +170,7 @@</span> <span class="p_context"> static void post_set(void)</span>
 	write_cr0(read_cr0() &amp; ~X86_CR0_CD);
 
 	/* Restore value of CR4 */
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE))</span>
 		__write_cr4(cr4);
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/generic.c b/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_header">index 19f57360dfd2..16e37a2581ac 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/generic.c</span>
<span class="p_chunk">@@ -444,11 +444,24 @@</span> <span class="p_context"> static void __init print_mtrr_state(void)</span>
 		pr_debug(&quot;TOM2: %016llx aka %lldM\n&quot;, mtrr_tom2, mtrr_tom2&gt;&gt;20);
 }
 
<span class="p_add">+/* PAT setup for BP. We need to go through sync steps here */</span>
<span class="p_add">+void __init mtrr_bp_pat_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	prepare_set();</span>
<span class="p_add">+</span>
<span class="p_add">+	pat_init();</span>
<span class="p_add">+</span>
<span class="p_add">+	post_set();</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Grab all of the MTRR state for this CPU into *state */
 bool __init get_mtrr_state(void)
 {
 	struct mtrr_var_range *vrs;
<span class="p_del">-	unsigned long flags;</span>
 	unsigned lo, dummy;
 	unsigned int i;
 
<span class="p_chunk">@@ -481,15 +494,6 @@</span> <span class="p_context"> bool __init get_mtrr_state(void)</span>
 
 	mtrr_state_set = 1;
 
<span class="p_del">-	/* PAT setup for BP. We need to go through sync steps here */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-	prepare_set();</span>
<span class="p_del">-</span>
<span class="p_del">-	pat_init();</span>
<span class="p_del">-</span>
<span class="p_del">-	post_set();</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
 	return !!(mtrr_state.enabled &amp; MTRR_STATE_MTRR_ENABLED);
 }
 
<span class="p_chunk">@@ -741,7 +745,7 @@</span> <span class="p_context"> static void prepare_set(void) __acquires(set_atomicity_lock)</span>
 	wbinvd();
 
 	/* Save value of CR4 and clear Page Global Enable (bit 7) */
<span class="p_del">-	if (cpu_has_pge) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
 		cr4 = __read_cr4();
 		__write_cr4(cr4 &amp; ~X86_CR4_PGE);
 	}
<span class="p_chunk">@@ -771,7 +775,7 @@</span> <span class="p_context"> static void post_set(void) __releases(set_atomicity_lock)</span>
 	write_cr0(read_cr0() &amp; ~X86_CR0_CD);
 
 	/* Restore value of CR4 */
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE))</span>
 		__write_cr4(cr4);
 	raw_spin_unlock(&amp;set_atomicity_lock);
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/main.c b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">index 10f8d4796240..7d393ecdeee6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_chunk">@@ -752,6 +752,9 @@</span> <span class="p_context"> void __init mtrr_bp_init(void)</span>
 			/* BIOS may override */
 			__mtrr_enabled = get_mtrr_state();
 
<span class="p_add">+			if (mtrr_enabled())</span>
<span class="p_add">+				mtrr_bp_pat_init();</span>
<span class="p_add">+</span>
 			if (mtrr_cleanup(phys_addr)) {
 				changed_by_mtrr_cleanup = 1;
 				mtrr_if-&gt;set_all();
<span class="p_chunk">@@ -759,8 +762,16 @@</span> <span class="p_context"> void __init mtrr_bp_init(void)</span>
 		}
 	}
 
<span class="p_del">-	if (!mtrr_enabled())</span>
<span class="p_add">+	if (!mtrr_enabled()) {</span>
 		pr_info(&quot;MTRR: Disabled\n&quot;);
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * PAT initialization relies on MTRR&#39;s rendezvous handler.</span>
<span class="p_add">+		 * Skip PAT init until the handler can initialize both</span>
<span class="p_add">+		 * features independently.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pat_disable(&quot;MTRRs disabled, skipping PAT initialization too.&quot;);</span>
<span class="p_add">+	}</span>
 }
 
 void mtrr_ap_init(void)
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/mtrr.h b/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_header">index 951884dcc433..6c7ced07d16d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_chunk">@@ -52,6 +52,7 @@</span> <span class="p_context"> void set_mtrr_prepare_save(struct set_mtrr_context *ctxt);</span>
 void fill_mtrr_var_range(unsigned int index,
 		u32 base_lo, u32 base_hi, u32 mask_lo, u32 mask_hi);
 bool get_mtrr_state(void);
<span class="p_add">+void mtrr_bp_pat_init(void);</span>
 
 extern void set_mtrr_ops(const struct mtrr_ops *ops);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_header">index 364e58346897..8cac429b6a1d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/vmware.c</span>
<span class="p_chunk">@@ -94,7 +94,7 @@</span> <span class="p_context"> static void __init vmware_platform_setup(void)</span>
  */
 static uint32_t __init vmware_platform(void)
 {
<span class="p_del">-	if (cpu_has_hypervisor) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {</span>
 		unsigned int eax;
 		unsigned int hyper_vendor_id[3];
 
<span class="p_header">diff --git a/arch/x86/kernel/devicetree.c b/arch/x86/kernel/devicetree.c</span>
<span class="p_header">index 1f4acd68b98b..3fe45f84ced4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/devicetree.c</span>
<span class="p_header">+++ b/arch/x86/kernel/devicetree.c</span>
<span class="p_chunk">@@ -151,7 +151,7 @@</span> <span class="p_context"> static void __init dtb_lapic_setup(void)</span>
 		return;
 
 	/* Did the boot loader setup the local APIC ? */
<span class="p_del">-	if (!cpu_has_apic) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		if (apic_force_enable(r.start))
 			return;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/fpu/bugs.c b/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_header">index dd9ca9b60ff3..aad34aafc0e0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_chunk">@@ -21,11 +21,15 @@</span> <span class="p_context"> static double __initdata y = 3145727.0;</span>
  * We should really only care about bugs here
  * anyway. Not features.
  */
<span class="p_del">-static void __init check_fpu(void)</span>
<span class="p_add">+void __init fpu__init_check_bugs(void)</span>
 {
 	u32 cr0_saved;
 	s32 fdiv_bug;
 
<span class="p_add">+	/* kernel_fpu_begin/end() relies on patched alternative instructions. */</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	/* We might have CR0::TS set already, clear it: */
 	cr0_saved = read_cr0();
 	write_cr0(cr0_saved &amp; ~X86_CR0_TS);
<span class="p_chunk">@@ -59,13 +63,3 @@</span> <span class="p_context"> static void __init check_fpu(void)</span>
 		pr_warn(&quot;Hmm, FPU with FDIV bug\n&quot;);
 	}
 }
<span class="p_del">-</span>
<span class="p_del">-void __init fpu__init_check_bugs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * kernel_fpu_begin/end() in check_fpu() relies on the patched</span>
<span class="p_del">-	 * alternative instructions.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (cpu_has_fpu)</span>
<span class="p_del">-		check_fpu();</span>
<span class="p_del">-}</span>
<span class="p_header">diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c</span>
<span class="p_header">index 8e37cc8a539a..97027545a72d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/core.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/core.c</span>
<span class="p_chunk">@@ -217,14 +217,14 @@</span> <span class="p_context"> static inline void fpstate_init_fstate(struct fregs_state *fp)</span>
 
 void fpstate_init(union fpregs_state *state)
 {
<span class="p_del">-	if (!cpu_has_fpu) {</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_FPU)) {</span>
 		fpstate_init_soft(&amp;state-&gt;soft);
 		return;
 	}
 
 	memset(state, 0, xstate_size);
 
<span class="p_del">-	if (cpu_has_fxsr)</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_FXSR))</span>
 		fpstate_init_fxstate(&amp;state-&gt;fxsave);
 	else
 		fpstate_init_fstate(&amp;state-&gt;fsave);
<span class="p_chunk">@@ -237,7 +237,7 @@</span> <span class="p_context"> int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu)</span>
 	dst_fpu-&gt;fpregs_active = 0;
 	dst_fpu-&gt;last_cpu = -1;
 
<span class="p_del">-	if (!src_fpu-&gt;fpstate_active || !cpu_has_fpu)</span>
<span class="p_add">+	if (!src_fpu-&gt;fpstate_active || !static_cpu_has(X86_FEATURE_FPU))</span>
 		return 0;
 
 	WARN_ON_FPU(src_fpu != &amp;current-&gt;thread.fpu);
<span class="p_chunk">@@ -506,33 +506,6 @@</span> <span class="p_context"> void fpu__clear(struct fpu *fpu)</span>
  * x87 math exception handling:
  */
 
<span class="p_del">-static inline unsigned short get_fpu_cwd(struct fpu *fpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (cpu_has_fxsr) {</span>
<span class="p_del">-		return fpu-&gt;state.fxsave.cwd;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		return (unsigned short)fpu-&gt;state.fsave.cwd;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline unsigned short get_fpu_swd(struct fpu *fpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (cpu_has_fxsr) {</span>
<span class="p_del">-		return fpu-&gt;state.fxsave.swd;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		return (unsigned short)fpu-&gt;state.fsave.swd;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline unsigned short get_fpu_mxcsr(struct fpu *fpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (cpu_has_xmm) {</span>
<span class="p_del">-		return fpu-&gt;state.fxsave.mxcsr;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		return MXCSR_DEFAULT;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 int fpu__exception_code(struct fpu *fpu, int trap_nr)
 {
 	int err;
<span class="p_chunk">@@ -547,10 +520,15 @@</span> <span class="p_context"> int fpu__exception_code(struct fpu *fpu, int trap_nr)</span>
 		 * so if this combination doesn&#39;t produce any single exception,
 		 * then we have a bad program that isn&#39;t synchronizing its FPU usage
 		 * and it will suffer the consequences since we won&#39;t be able to
<span class="p_del">-		 * fully reproduce the context of the exception</span>
<span class="p_add">+		 * fully reproduce the context of the exception.</span>
 		 */
<span class="p_del">-		cwd = get_fpu_cwd(fpu);</span>
<span class="p_del">-		swd = get_fpu_swd(fpu);</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_FXSR)) {</span>
<span class="p_add">+			cwd = fpu-&gt;state.fxsave.cwd;</span>
<span class="p_add">+			swd = fpu-&gt;state.fxsave.swd;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			cwd = (unsigned short)fpu-&gt;state.fsave.cwd;</span>
<span class="p_add">+			swd = (unsigned short)fpu-&gt;state.fsave.swd;</span>
<span class="p_add">+		}</span>
 
 		err = swd &amp; ~cwd;
 	} else {
<span class="p_chunk">@@ -560,7 +538,11 @@</span> <span class="p_context"> int fpu__exception_code(struct fpu *fpu, int trap_nr)</span>
 		 * unmasked exception was caught we must mask the exception mask bits
 		 * at 0x1f80, and then use these to mask the exception bits at 0x3f.
 		 */
<span class="p_del">-		unsigned short mxcsr = get_fpu_mxcsr(fpu);</span>
<span class="p_add">+		unsigned short mxcsr = MXCSR_DEFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_XMM))</span>
<span class="p_add">+			mxcsr = fpu-&gt;state.fxsave.mxcsr;</span>
<span class="p_add">+</span>
 		err = ~(mxcsr &gt;&gt; 7) &amp; mxcsr;
 	}
 
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 54c86fffbf9f..aacfd7a82cec 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -29,22 +29,22 @@</span> <span class="p_context"> static void fpu__init_cpu_generic(void)</span>
 	unsigned long cr0;
 	unsigned long cr4_mask = 0;
 
<span class="p_del">-	if (cpu_has_fxsr)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_FXSR))</span>
 		cr4_mask |= X86_CR4_OSFXSR;
<span class="p_del">-	if (cpu_has_xmm)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XMM))</span>
 		cr4_mask |= X86_CR4_OSXMMEXCPT;
 	if (cr4_mask)
 		cr4_set_bits(cr4_mask);
 
 	cr0 = read_cr0();
 	cr0 &amp;= ~(X86_CR0_TS|X86_CR0_EM); /* clear TS and EM */
<span class="p_del">-	if (!cpu_has_fpu)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU))</span>
 		cr0 |= X86_CR0_EM;
 	write_cr0(cr0);
 
 	/* Flush out any pending x87 state: */
 #ifdef CONFIG_MATH_EMULATION
<span class="p_del">-	if (!cpu_has_fpu)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU))</span>
 		fpstate_init_soft(&amp;current-&gt;thread.fpu.state.soft);
 	else
 #endif
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> static void fpu__init_system_early_generic(struct cpuinfo_x86 *c)</span>
 	}
 
 #ifndef CONFIG_MATH_EMULATION
<span class="p_del">-	if (!cpu_has_fpu) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU)) {</span>
 		pr_emerg(&quot;x86/fpu: Giving up, no FPU found and no math emulation present\n&quot;);
 		for (;;)
 			asm volatile(&quot;hlt&quot;);
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> static void __init fpu__init_system_mxcsr(void)</span>
 {
 	unsigned int mask = 0;
 
<span class="p_del">-	if (cpu_has_fxsr) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_FXSR)) {</span>
 		/* Static because GCC does not get 16-byte stack alignment right: */
 		static struct fxregs_state fxregs __initdata;
 
<span class="p_chunk">@@ -212,7 +212,7 @@</span> <span class="p_context"> static void __init fpu__init_system_xstate_size_legacy(void)</span>
 	 * fpu__init_system_xstate().
 	 */
 
<span class="p_del">-	if (!cpu_has_fpu) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU)) {</span>
 		/*
 		 * Disable xsave as we do not support it if i387
 		 * emulation is enabled.
<span class="p_chunk">@@ -221,7 +221,7 @@</span> <span class="p_context"> static void __init fpu__init_system_xstate_size_legacy(void)</span>
 		setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);
 		xstate_size = sizeof(struct swregs_state);
 	} else {
<span class="p_del">-		if (cpu_has_fxsr)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_FXSR))</span>
 			xstate_size = sizeof(struct fxregs_state);
 		else
 			xstate_size = sizeof(struct fregs_state);
<span class="p_header">diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c</span>
<span class="p_header">index 8bd1c003942a..81422dfb152b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/regset.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/regset.c</span>
<span class="p_chunk">@@ -21,7 +21,10 @@</span> <span class="p_context"> int regset_xregset_fpregs_active(struct task_struct *target, const struct user_r</span>
 {
 	struct fpu *target_fpu = &amp;target-&gt;thread.fpu;
 
<span class="p_del">-	return (cpu_has_fxsr &amp;&amp; target_fpu-&gt;fpstate_active) ? regset-&gt;n : 0;</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_FXSR) &amp;&amp; target_fpu-&gt;fpstate_active)</span>
<span class="p_add">+		return regset-&gt;n;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return 0;</span>
 }
 
 int xfpregs_get(struct task_struct *target, const struct user_regset *regset,
<span class="p_chunk">@@ -30,7 +33,7 @@</span> <span class="p_context"> int xfpregs_get(struct task_struct *target, const struct user_regset *regset,</span>
 {
 	struct fpu *fpu = &amp;target-&gt;thread.fpu;
 
<span class="p_del">-	if (!cpu_has_fxsr)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FXSR))</span>
 		return -ENODEV;
 
 	fpu__activate_fpstate_read(fpu);
<span class="p_chunk">@@ -47,7 +50,7 @@</span> <span class="p_context"> int xfpregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	struct fpu *fpu = &amp;target-&gt;thread.fpu;
 	int ret;
 
<span class="p_del">-	if (!cpu_has_fxsr)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FXSR))</span>
 		return -ENODEV;
 
 	fpu__activate_fpstate_write(fpu);
<span class="p_chunk">@@ -65,7 +68,7 @@</span> <span class="p_context"> int xfpregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	 * update the header bits in the xsave header, indicating the
 	 * presence of FP and SSE state.
 	 */
<span class="p_del">-	if (cpu_has_xsave)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		fpu-&gt;state.xsave.header.xfeatures |= XFEATURE_MASK_FPSSE;
 
 	return ret;
<span class="p_chunk">@@ -79,7 +82,7 @@</span> <span class="p_context"> int xstateregs_get(struct task_struct *target, const struct user_regset *regset,</span>
 	struct xregs_state *xsave;
 	int ret;
 
<span class="p_del">-	if (!cpu_has_xsave)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		return -ENODEV;
 
 	fpu__activate_fpstate_read(fpu);
<span class="p_chunk">@@ -108,7 +111,7 @@</span> <span class="p_context"> int xstateregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	struct xregs_state *xsave;
 	int ret;
 
<span class="p_del">-	if (!cpu_has_xsave)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		return -ENODEV;
 
 	fpu__activate_fpstate_write(fpu);
<span class="p_chunk">@@ -275,10 +278,10 @@</span> <span class="p_context"> int fpregs_get(struct task_struct *target, const struct user_regset *regset,</span>
 
 	fpu__activate_fpstate_read(fpu);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU))</span>
 		return fpregs_soft_get(target, regset, pos, count, kbuf, ubuf);
 
<span class="p_del">-	if (!cpu_has_fxsr)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FXSR))</span>
 		return user_regset_copyout(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,
 					   &amp;fpu-&gt;state.fsave, 0,
 					   -1);
<span class="p_chunk">@@ -306,10 +309,10 @@</span> <span class="p_context"> int fpregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	fpu__activate_fpstate_write(fpu);
 	fpstate_sanitize_xstate(fpu);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FPU))</span>
 		return fpregs_soft_set(target, regset, pos, count, kbuf, ubuf);
 
<span class="p_del">-	if (!cpu_has_fxsr)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_FXSR))</span>
 		return user_regset_copyin(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,
 					  &amp;fpu-&gt;state.fsave, 0,
 					  -1);
<span class="p_chunk">@@ -325,7 +328,7 @@</span> <span class="p_context"> int fpregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	 * update the header bit in the xsave header, indicating the
 	 * presence of FP.
 	 */
<span class="p_del">-	if (cpu_has_xsave)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		fpu-&gt;state.xsave.header.xfeatures |= XFEATURE_MASK_FP;
 	return ret;
 }
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index b48ef35b28d4..4ea2a59483c7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -190,7 +190,7 @@</span> <span class="p_context"> void fpstate_sanitize_xstate(struct fpu *fpu)</span>
  */
 void fpu__init_cpu_xstate(void)
 {
<span class="p_del">-	if (!cpu_has_xsave || !xfeatures_mask)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask)</span>
 		return;
 
 	cr4_set_bits(X86_CR4_OSXSAVE);
<span class="p_chunk">@@ -280,7 +280,7 @@</span> <span class="p_context"> static void __init setup_xstate_comp(void)</span>
 	xstate_comp_offsets[0] = 0;
 	xstate_comp_offsets[1] = offsetof(struct fxregs_state, xmm_space);
 
<span class="p_del">-	if (!cpu_has_xsaves) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVES)) {</span>
 		for (i = FIRST_EXTENDED_XFEATURE; i &lt; XFEATURE_MAX; i++) {
 			if (xfeature_enabled(i)) {
 				xstate_comp_offsets[i] = xstate_offsets[i];
<span class="p_chunk">@@ -316,13 +316,13 @@</span> <span class="p_context"> static void __init setup_init_fpu_buf(void)</span>
 	WARN_ON_FPU(!on_boot_cpu);
 	on_boot_cpu = 0;
 
<span class="p_del">-	if (!cpu_has_xsave)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		return;
 
 	setup_xstate_features();
 	print_xstate_features();
 
<span class="p_del">-	if (cpu_has_xsaves) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVES)) {</span>
 		init_fpstate.xsave.header.xcomp_bv = (u64)1 &lt;&lt; 63 | xfeatures_mask;
 		init_fpstate.xsave.header.xfeatures = xfeatures_mask;
 	}
<span class="p_chunk">@@ -417,7 +417,7 @@</span> <span class="p_context"> static int xfeature_size(int xfeature_nr)</span>
  */
 static int using_compacted_format(void)
 {
<span class="p_del">-	return cpu_has_xsaves;</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_XSAVES);</span>
 }
 
 static void __xstate_dump_leaves(void)
<span class="p_chunk">@@ -549,7 +549,7 @@</span> <span class="p_context"> static unsigned int __init calculate_xstate_size(void)</span>
 	unsigned int eax, ebx, ecx, edx;
 	unsigned int calculated_xstate_size;
 
<span class="p_del">-	if (!cpu_has_xsaves) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVES)) {</span>
 		/*
 		 * - CPUID function 0DH, sub-function 0:
 		 *    EBX enumerates the size (in bytes) required by
<span class="p_chunk">@@ -630,7 +630,7 @@</span> <span class="p_context"> void __init fpu__init_system_xstate(void)</span>
 	WARN_ON_FPU(!on_boot_cpu);
 	on_boot_cpu = 0;
 
<span class="p_del">-	if (!cpu_has_xsave) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE)) {</span>
 		pr_info(&quot;x86/fpu: Legacy x87 FPU detected.\n&quot;);
 		return;
 	}
<span class="p_chunk">@@ -667,7 +667,7 @@</span> <span class="p_context"> void __init fpu__init_system_xstate(void)</span>
 	pr_info(&quot;x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using &#39;%s&#39; format.\n&quot;,
 		xfeatures_mask,
 		xstate_size,
<span class="p_del">-		cpu_has_xsaves ? &quot;compacted&quot; : &quot;standard&quot;);</span>
<span class="p_add">+		boot_cpu_has(X86_FEATURE_XSAVES) ? &quot;compacted&quot; : &quot;standard&quot;);</span>
 }
 
 /*
<span class="p_chunk">@@ -678,7 +678,7 @@</span> <span class="p_context"> void fpu__resume_cpu(void)</span>
 	/*
 	 * Restore XCR0 on xsave capable CPUs:
 	 */
<span class="p_del">-	if (cpu_has_xsave)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S</span>
<span class="p_header">index af1112980dd4..6f8902b0d151 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_32.S</span>
<span class="p_chunk">@@ -555,62 +555,53 @@</span> <span class="p_context"> ENDPROC(early_idt_handler_array)</span>
 	 */
 	cld
 
<span class="p_del">-	cmpl $2,(%esp)		# X86_TRAP_NMI</span>
<span class="p_del">-	je .Lis_nmi		# Ignore NMI</span>
<span class="p_del">-</span>
<span class="p_del">-	cmpl $2,%ss:early_recursion_flag</span>
<span class="p_del">-	je hlt_loop</span>
 	incl %ss:early_recursion_flag
 
<span class="p_del">-	push %eax		# 16(%esp)</span>
<span class="p_del">-	push %ecx		# 12(%esp)</span>
<span class="p_del">-	push %edx		#  8(%esp)</span>
<span class="p_del">-	push %ds		#  4(%esp)</span>
<span class="p_del">-	push %es		#  0(%esp)</span>
<span class="p_del">-	movl $(__KERNEL_DS),%eax</span>
<span class="p_del">-	movl %eax,%ds</span>
<span class="p_del">-	movl %eax,%es</span>
<span class="p_del">-</span>
<span class="p_del">-	cmpl $(__KERNEL_CS),32(%esp)</span>
<span class="p_del">-	jne 10f</span>
<span class="p_add">+	/* The vector number is in pt_regs-&gt;gs */</span>
 
<span class="p_del">-	leal 28(%esp),%eax	# Pointer to %eip</span>
<span class="p_del">-	call early_fixup_exception</span>
<span class="p_del">-	andl %eax,%eax</span>
<span class="p_del">-	jnz ex_entry		/* found an exception entry */</span>
<span class="p_del">-</span>
<span class="p_del">-10:</span>
<span class="p_del">-#ifdef CONFIG_PRINTK</span>
<span class="p_del">-	xorl %eax,%eax</span>
<span class="p_del">-	movw %ax,2(%esp)	/* clean up the segment values on some cpus */</span>
<span class="p_del">-	movw %ax,6(%esp)</span>
<span class="p_del">-	movw %ax,34(%esp)</span>
<span class="p_del">-	leal  40(%esp),%eax</span>
<span class="p_del">-	pushl %eax		/* %esp before the exception */</span>
<span class="p_del">-	pushl %ebx</span>
<span class="p_del">-	pushl %ebp</span>
<span class="p_del">-	pushl %esi</span>
<span class="p_del">-	pushl %edi</span>
<span class="p_del">-	movl %cr2,%eax</span>
<span class="p_del">-	pushl %eax</span>
<span class="p_del">-	pushl (20+6*4)(%esp)	/* trapno */</span>
<span class="p_del">-	pushl $fault_msg</span>
<span class="p_del">-	call printk</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	call dump_stack</span>
<span class="p_del">-hlt_loop:</span>
<span class="p_del">-	hlt</span>
<span class="p_del">-	jmp hlt_loop</span>
<span class="p_del">-</span>
<span class="p_del">-ex_entry:</span>
<span class="p_del">-	pop %es</span>
<span class="p_del">-	pop %ds</span>
<span class="p_del">-	pop %edx</span>
<span class="p_del">-	pop %ecx</span>
<span class="p_del">-	pop %eax</span>
<span class="p_del">-	decl %ss:early_recursion_flag</span>
<span class="p_del">-.Lis_nmi:</span>
<span class="p_del">-	addl $8,%esp		/* drop vector number and error code */</span>
<span class="p_add">+	cld</span>
<span class="p_add">+	pushl	%fs		/* pt_regs-&gt;fs */</span>
<span class="p_add">+	movw	$0, 2(%esp)	/* clear high bits (some CPUs leave garbage) */</span>
<span class="p_add">+	pushl	%es		/* pt_regs-&gt;es */</span>
<span class="p_add">+	movw	$0, 2(%esp)	/* clear high bits (some CPUs leave garbage) */</span>
<span class="p_add">+	pushl	%ds		/* pt_regs-&gt;ds */</span>
<span class="p_add">+	movw	$0, 2(%esp)	/* clear high bits (some CPUs leave garbage) */</span>
<span class="p_add">+	pushl	%eax		/* pt_regs-&gt;ax */</span>
<span class="p_add">+	pushl	%ebp		/* pt_regs-&gt;bp */</span>
<span class="p_add">+	pushl	%edi		/* pt_regs-&gt;di */</span>
<span class="p_add">+	pushl	%esi		/* pt_regs-&gt;si */</span>
<span class="p_add">+	pushl	%edx		/* pt_regs-&gt;dx */</span>
<span class="p_add">+	pushl	%ecx		/* pt_regs-&gt;cx */</span>
<span class="p_add">+	pushl	%ebx		/* pt_regs-&gt;bx */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Fix up DS and ES */</span>
<span class="p_add">+	movl	$(__KERNEL_DS), %ecx</span>
<span class="p_add">+	movl	%ecx, %ds</span>
<span class="p_add">+	movl	%ecx, %es</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Load the vector number into EDX */</span>
<span class="p_add">+	movl	PT_GS(%esp), %edx</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Load GS into pt_regs-&gt;gs and clear high bits */</span>
<span class="p_add">+	movw	%gs, PT_GS(%esp)</span>
<span class="p_add">+	movw	$0, PT_GS+2(%esp)</span>
<span class="p_add">+</span>
<span class="p_add">+	movl	%esp, %eax	/* args are pt_regs (EAX), trapnr (EDX) */</span>
<span class="p_add">+	call	early_fixup_exception</span>
<span class="p_add">+</span>
<span class="p_add">+	popl	%ebx		/* pt_regs-&gt;bx */</span>
<span class="p_add">+	popl	%ecx		/* pt_regs-&gt;cx */</span>
<span class="p_add">+	popl	%edx		/* pt_regs-&gt;dx */</span>
<span class="p_add">+	popl	%esi		/* pt_regs-&gt;si */</span>
<span class="p_add">+	popl	%edi		/* pt_regs-&gt;di */</span>
<span class="p_add">+	popl	%ebp		/* pt_regs-&gt;bp */</span>
<span class="p_add">+	popl	%eax		/* pt_regs-&gt;ax */</span>
<span class="p_add">+	popl	%ds		/* pt_regs-&gt;ds */</span>
<span class="p_add">+	popl	%es		/* pt_regs-&gt;es */</span>
<span class="p_add">+	popl	%fs		/* pt_regs-&gt;fs */</span>
<span class="p_add">+	popl	%gs		/* pt_regs-&gt;gs */</span>
<span class="p_add">+	decl	%ss:early_recursion_flag</span>
<span class="p_add">+	addl	$4, %esp	/* pop pt_regs-&gt;orig_ax */</span>
 	iret
 ENDPROC(early_idt_handler_common)
 
<span class="p_chunk">@@ -647,10 +638,14 @@</span> <span class="p_context"> ENDPROC(early_idt_handler_common)</span>
 	popl %eax
 #endif
 	iret
<span class="p_add">+</span>
<span class="p_add">+hlt_loop:</span>
<span class="p_add">+	hlt</span>
<span class="p_add">+	jmp hlt_loop</span>
 ENDPROC(ignore_int)
 __INITDATA
 	.align 4
<span class="p_del">-early_recursion_flag:</span>
<span class="p_add">+GLOBAL(early_recursion_flag)</span>
 	.long 0
 
 __REFDATA
<span class="p_chunk">@@ -715,19 +710,6 @@</span> <span class="p_context"> __INITRODATA</span>
 int_msg:
 	.asciz &quot;Unknown interrupt or fault at: %p %p %p\n&quot;
 
<span class="p_del">-fault_msg:</span>
<span class="p_del">-/* fault info: */</span>
<span class="p_del">-	.ascii &quot;BUG: Int %d: CR2 %p\n&quot;</span>
<span class="p_del">-/* regs pushed in early_idt_handler: */</span>
<span class="p_del">-	.ascii &quot;     EDI %p  ESI %p  EBP %p  EBX %p\n&quot;</span>
<span class="p_del">-	.ascii &quot;     ESP %p   ES %p   DS %p\n&quot;</span>
<span class="p_del">-	.ascii &quot;     EDX %p  ECX %p  EAX %p\n&quot;</span>
<span class="p_del">-/* fault frame: */</span>
<span class="p_del">-	.ascii &quot;     vec %p  err %p  EIP %p   CS %p  flg %p\n&quot;</span>
<span class="p_del">-	.ascii &quot;Stack: %p %p %p %p %p %p %p %p\n&quot;</span>
<span class="p_del">-	.ascii &quot;       %p %p %p %p %p %p %p %p\n&quot;</span>
<span class="p_del">-	.asciz &quot;       %p %p %p %p %p %p %p %p\n&quot;</span>
<span class="p_del">-</span>
 #include &quot;../../x86/xen/xen-head.S&quot;
 
 /*
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 22fbf9df61bb..5df831ef1442 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -20,6 +20,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor-flags.h&gt;
 #include &lt;asm/percpu.h&gt;
 #include &lt;asm/nops.h&gt;
<span class="p_add">+#include &quot;../entry/calling.h&quot;</span>
 
 #ifdef CONFIG_PARAVIRT
 #include &lt;asm/asm-offsets.h&gt;
<span class="p_chunk">@@ -64,6 +65,14 @@</span> <span class="p_context"> L3_START_KERNEL = pud_index(__START_KERNEL_map)</span>
 	 * tables and then reload them.
 	 */
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Setup stack for verify_cpu(). &quot;-8&quot; because stack_start is defined</span>
<span class="p_add">+	 * this way, see below. Our best guess is a NULL ptr for stack</span>
<span class="p_add">+	 * termination heuristics and we don&#39;t want to break anything which</span>
<span class="p_add">+	 * might depend on it (kgdb, ...).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	leaq	(__end_init_task - 8)(%rip), %rsp</span>
<span class="p_add">+</span>
 	/* Sanitize CPU configuration */
 	call verify_cpu
 
<span class="p_chunk">@@ -350,90 +359,48 @@</span> <span class="p_context"> ENDPROC(early_idt_handler_array)</span>
 	 */
 	cld
 
<span class="p_del">-	cmpl $2,(%rsp)		# X86_TRAP_NMI</span>
<span class="p_del">-	je .Lis_nmi		# Ignore NMI</span>
<span class="p_del">-</span>
<span class="p_del">-	cmpl $2,early_recursion_flag(%rip)</span>
<span class="p_del">-	jz  1f</span>
 	incl early_recursion_flag(%rip)
 
<span class="p_del">-	pushq %rax		# 64(%rsp)</span>
<span class="p_del">-	pushq %rcx		# 56(%rsp)</span>
<span class="p_del">-	pushq %rdx		# 48(%rsp)</span>
<span class="p_del">-	pushq %rsi		# 40(%rsp)</span>
<span class="p_del">-	pushq %rdi		# 32(%rsp)</span>
<span class="p_del">-	pushq %r8		# 24(%rsp)</span>
<span class="p_del">-	pushq %r9		# 16(%rsp)</span>
<span class="p_del">-	pushq %r10		#  8(%rsp)</span>
<span class="p_del">-	pushq %r11		#  0(%rsp)</span>
<span class="p_del">-</span>
<span class="p_del">-	cmpl $__KERNEL_CS,96(%rsp)</span>
<span class="p_del">-	jne 11f</span>
<span class="p_del">-</span>
<span class="p_del">-	cmpl $14,72(%rsp)	# Page fault?</span>
<span class="p_add">+	/* The vector number is currently in the pt_regs-&gt;di slot. */</span>
<span class="p_add">+	pushq %rsi				/* pt_regs-&gt;si */</span>
<span class="p_add">+	movq 8(%rsp), %rsi			/* RSI = vector number */</span>
<span class="p_add">+	movq %rdi, 8(%rsp)			/* pt_regs-&gt;di = RDI */</span>
<span class="p_add">+	pushq %rdx				/* pt_regs-&gt;dx */</span>
<span class="p_add">+	pushq %rcx				/* pt_regs-&gt;cx */</span>
<span class="p_add">+	pushq %rax				/* pt_regs-&gt;ax */</span>
<span class="p_add">+	pushq %r8				/* pt_regs-&gt;r8 */</span>
<span class="p_add">+	pushq %r9				/* pt_regs-&gt;r9 */</span>
<span class="p_add">+	pushq %r10				/* pt_regs-&gt;r10 */</span>
<span class="p_add">+	pushq %r11				/* pt_regs-&gt;r11 */</span>
<span class="p_add">+	pushq %rbx				/* pt_regs-&gt;bx */</span>
<span class="p_add">+	pushq %rbp				/* pt_regs-&gt;bp */</span>
<span class="p_add">+	pushq %r12				/* pt_regs-&gt;r12 */</span>
<span class="p_add">+	pushq %r13				/* pt_regs-&gt;r13 */</span>
<span class="p_add">+	pushq %r14				/* pt_regs-&gt;r14 */</span>
<span class="p_add">+	pushq %r15				/* pt_regs-&gt;r15 */</span>
<span class="p_add">+</span>
<span class="p_add">+	cmpq $14,%rsi		/* Page fault? */</span>
 	jnz 10f
<span class="p_del">-	GET_CR2_INTO(%rdi)	# can clobber any volatile register if pv</span>
<span class="p_add">+	GET_CR2_INTO(%rdi)	/* Can clobber any volatile register if pv */</span>
 	call early_make_pgtable
 	andl %eax,%eax
<span class="p_del">-	jz 20f			# All good</span>
<span class="p_add">+	jz 20f			/* All good */</span>
 
 10:
<span class="p_del">-	leaq 88(%rsp),%rdi	# Pointer to %rip</span>
<span class="p_add">+	movq %rsp,%rdi		/* RDI = pt_regs; RSI is already trapnr */</span>
 	call early_fixup_exception
<span class="p_del">-	andl %eax,%eax</span>
<span class="p_del">-	jnz 20f			# Found an exception entry</span>
<span class="p_del">-</span>
<span class="p_del">-11:</span>
<span class="p_del">-#ifdef CONFIG_EARLY_PRINTK</span>
<span class="p_del">-	GET_CR2_INTO(%r9)	# can clobber any volatile register if pv</span>
<span class="p_del">-	movl 80(%rsp),%r8d	# error code</span>
<span class="p_del">-	movl 72(%rsp),%esi	# vector number</span>
<span class="p_del">-	movl 96(%rsp),%edx	# %cs</span>
<span class="p_del">-	movq 88(%rsp),%rcx	# %rip</span>
<span class="p_del">-	xorl %eax,%eax</span>
<span class="p_del">-	leaq early_idt_msg(%rip),%rdi</span>
<span class="p_del">-	call early_printk</span>
<span class="p_del">-	cmpl $2,early_recursion_flag(%rip)</span>
<span class="p_del">-	jz  1f</span>
<span class="p_del">-	call dump_stack</span>
<span class="p_del">-#ifdef CONFIG_KALLSYMS	</span>
<span class="p_del">-	leaq early_idt_ripmsg(%rip),%rdi</span>
<span class="p_del">-	movq 40(%rsp),%rsi	# %rip again</span>
<span class="p_del">-	call __print_symbol</span>
<span class="p_del">-#endif</span>
<span class="p_del">-#endif /* EARLY_PRINTK */</span>
<span class="p_del">-1:	hlt</span>
<span class="p_del">-	jmp 1b</span>
<span class="p_del">-</span>
<span class="p_del">-20:	# Exception table entry found or page table generated</span>
<span class="p_del">-	popq %r11</span>
<span class="p_del">-	popq %r10</span>
<span class="p_del">-	popq %r9</span>
<span class="p_del">-	popq %r8</span>
<span class="p_del">-	popq %rdi</span>
<span class="p_del">-	popq %rsi</span>
<span class="p_del">-	popq %rdx</span>
<span class="p_del">-	popq %rcx</span>
<span class="p_del">-	popq %rax</span>
<span class="p_add">+</span>
<span class="p_add">+20:</span>
 	decl early_recursion_flag(%rip)
<span class="p_del">-.Lis_nmi:</span>
<span class="p_del">-	addq $16,%rsp		# drop vector number and error code</span>
<span class="p_del">-	INTERRUPT_RETURN</span>
<span class="p_add">+	jmp restore_regs_and_iret</span>
 ENDPROC(early_idt_handler_common)
 
 	__INITDATA
 
 	.balign 4
<span class="p_del">-early_recursion_flag:</span>
<span class="p_add">+GLOBAL(early_recursion_flag)</span>
 	.long 0
 
<span class="p_del">-#ifdef CONFIG_EARLY_PRINTK</span>
<span class="p_del">-early_idt_msg:</span>
<span class="p_del">-	.asciz &quot;PANIC: early exception %02lx rip %lx:%lx error %lx cr2 %lx\n&quot;</span>
<span class="p_del">-early_idt_ripmsg:</span>
<span class="p_del">-	.asciz &quot;RIP %s\n&quot;</span>
<span class="p_del">-#endif /* CONFIG_EARLY_PRINTK */</span>
<span class="p_del">-</span>
 #define NEXT_PAGE(name) \
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
<span class="p_header">diff --git a/arch/x86/kernel/hpet.c b/arch/x86/kernel/hpet.c</span>
<span class="p_header">index a1f0e4a5c47e..7282c2e3858e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/hpet.c</span>
<span class="p_header">+++ b/arch/x86/kernel/hpet.c</span>
<span class="p_chunk">@@ -773,7 +773,6 @@</span> <span class="p_context"> static struct clocksource clocksource_hpet = {</span>
 	.mask		= HPET_MASK,
 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
 	.resume		= hpet_resume_counter,
<span class="p_del">-	.archdata	= { .vclock_mode = VCLOCK_HPET },</span>
 };
 
 static int hpet_clocksource_register(void)
<span class="p_header">diff --git a/arch/x86/kernel/jump_label.c b/arch/x86/kernel/jump_label.c</span>
<span class="p_header">index e565e0e4d216..fc25f698d792 100644</span>
<span class="p_header">--- a/arch/x86/kernel/jump_label.c</span>
<span class="p_header">+++ b/arch/x86/kernel/jump_label.c</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/cpu.h&gt;
 #include &lt;asm/kprobes.h&gt;
 #include &lt;asm/alternative.h&gt;
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 
 #ifdef HAVE_JUMP_LABEL
 
<span class="p_header">diff --git a/arch/x86/kernel/kgdb.c b/arch/x86/kernel/kgdb.c</span>
<span class="p_header">index 2da6ee9ae69b..04cde527d728 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kgdb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kgdb.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/memory.h&gt;
 
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/apicdef.h&gt;
 #include &lt;asm/apic.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/kprobes/core.c b/arch/x86/kernel/kprobes/core.c</span>
<span class="p_header">index ae703acb85c1..38cf7a741250 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kprobes/core.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kprobes/core.c</span>
<span class="p_chunk">@@ -51,6 +51,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/ftrace.h&gt;
 #include &lt;linux/frame.h&gt;
 
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/kprobes/opt.c b/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_header">index 7b3b9d15c47a..4425f593f0ec 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kprobes/opt.c</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kallsyms.h&gt;
 #include &lt;linux/ftrace.h&gt;
 
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 807950860fb7..dc1207e2f193 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -522,7 +522,7 @@</span> <span class="p_context"> static noinline uint32_t __kvm_cpuid_base(void)</span>
 	if (boot_cpu_data.cpuid_level &lt; 0)
 		return 0;	/* So we don&#39;t blow up on old processors */
 
<span class="p_del">-	if (cpu_has_hypervisor)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))</span>
 		return hypervisor_cpuid_base(&quot;KVMKVMKVM\0\0\0&quot;, 0);
 
 	return 0;
<span class="p_header">diff --git a/arch/x86/kernel/module.c b/arch/x86/kernel/module.c</span>
<span class="p_header">index 005c03e93fc5..477ae806c2fa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/module.c</span>
<span class="p_header">+++ b/arch/x86/kernel/module.c</span>
<span class="p_chunk">@@ -31,6 +31,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/jump_label.h&gt;
 #include &lt;linux/random.h&gt;
 
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/setup.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c</span>
<span class="p_header">index f08ac28b8136..f9583917c7c4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt.c</span>
<span class="p_chunk">@@ -339,8 +339,10 @@</span> <span class="p_context"> __visible struct pv_cpu_ops pv_cpu_ops = {</span>
 	.write_cr8 = native_write_cr8,
 #endif
 	.wbinvd = native_wbinvd,
<span class="p_del">-	.read_msr = native_read_msr_safe,</span>
<span class="p_del">-	.write_msr = native_write_msr_safe,</span>
<span class="p_add">+	.read_msr = native_read_msr,</span>
<span class="p_add">+	.write_msr = native_write_msr,</span>
<span class="p_add">+	.read_msr_safe = native_read_msr_safe,</span>
<span class="p_add">+	.write_msr_safe = native_write_msr_safe,</span>
 	.read_pmc = native_read_pmc,
 	.load_tr_desc = native_load_tr_desc,
 	.set_ldt = native_set_ldt,
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index 6cbab31ac23a..6b16c36f0939 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -136,25 +136,6 @@</span> <span class="p_context"> void release_thread(struct task_struct *dead_task)</span>
 	}
 }
 
<span class="p_del">-static inline void set_32bit_tls(struct task_struct *t, int tls, u32 addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct user_desc ud = {</span>
<span class="p_del">-		.base_addr = addr,</span>
<span class="p_del">-		.limit = 0xfffff,</span>
<span class="p_del">-		.seg_32bit = 1,</span>
<span class="p_del">-		.limit_in_pages = 1,</span>
<span class="p_del">-		.useable = 1,</span>
<span class="p_del">-	};</span>
<span class="p_del">-	struct desc_struct *desc = t-&gt;thread.tls_array;</span>
<span class="p_del">-	desc += tls;</span>
<span class="p_del">-	fill_ldt(desc, &amp;ud);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u32 read_32bit_tls(struct task_struct *t, int tls)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return get_desc_base(&amp;t-&gt;thread.tls_array[tls]);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
 		unsigned long arg, struct task_struct *p, unsigned long tls)
 {
<span class="p_chunk">@@ -169,9 +150,9 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 	p-&gt;thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p-&gt;thread.gsindex);
<span class="p_del">-	p-&gt;thread.gs = p-&gt;thread.gsindex ? 0 : me-&gt;thread.gs;</span>
<span class="p_add">+	p-&gt;thread.gsbase = p-&gt;thread.gsindex ? 0 : me-&gt;thread.gsbase;</span>
 	savesegment(fs, p-&gt;thread.fsindex);
<span class="p_del">-	p-&gt;thread.fs = p-&gt;thread.fsindex ? 0 : me-&gt;thread.fs;</span>
<span class="p_add">+	p-&gt;thread.fsbase = p-&gt;thread.fsindex ? 0 : me-&gt;thread.fsbase;</span>
 	savesegment(es, p-&gt;thread.es);
 	savesegment(ds, p-&gt;thread.ds);
 	memset(p-&gt;thread.ptrace_bps, 0, sizeof(p-&gt;thread.ptrace_bps));
<span class="p_chunk">@@ -210,7 +191,7 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 	 */
 	if (clone_flags &amp; CLONE_SETTLS) {
 #ifdef CONFIG_IA32_EMULATION
<span class="p_del">-		if (is_ia32_task())</span>
<span class="p_add">+		if (in_ia32_syscall())</span>
 			err = do_set_thread_area(p, -1,
 				(struct user_desc __user *)tls, 0);
 		else
<span class="p_chunk">@@ -282,7 +263,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	struct fpu *next_fpu = &amp;next-&gt;fpu;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &amp;per_cpu(cpu_tss, cpu);
<span class="p_del">-	unsigned fsindex, gsindex;</span>
<span class="p_add">+	unsigned prev_fsindex, prev_gsindex;</span>
 	fpu_switch_t fpu_switch;
 
 	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);
<span class="p_chunk">@@ -292,8 +273,8 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	 *
 	 * (e.g. xen_load_tls())
 	 */
<span class="p_del">-	savesegment(fs, fsindex);</span>
<span class="p_del">-	savesegment(gs, gsindex);</span>
<span class="p_add">+	savesegment(fs, prev_fsindex);</span>
<span class="p_add">+	savesegment(gs, prev_gsindex);</span>
 
 	/*
 	 * Load TLS before restoring any segments so that segment loads
<span class="p_chunk">@@ -336,66 +317,104 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	 * Switch FS and GS.
 	 *
 	 * These are even more complicated than DS and ES: they have
<span class="p_del">-	 * 64-bit bases are that controlled by arch_prctl.  Those bases</span>
<span class="p_del">-	 * only differ from the values in the GDT or LDT if the selector</span>
<span class="p_del">-	 * is 0.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Loading the segment register resets the hidden base part of</span>
<span class="p_del">-	 * the register to 0 or the value from the GDT / LDT.  If the</span>
<span class="p_del">-	 * next base address zero, writing 0 to the segment register is</span>
<span class="p_del">-	 * much faster than using wrmsr to explicitly zero the base.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The thread_struct.fs and thread_struct.gs values are 0</span>
<span class="p_del">-	 * if the fs and gs bases respectively are not overridden</span>
<span class="p_del">-	 * from the values implied by fsindex and gsindex.  They</span>
<span class="p_del">-	 * are nonzero, and store the nonzero base addresses, if</span>
<span class="p_del">-	 * the bases are overridden.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * (fs != 0 &amp;&amp; fsindex != 0) || (gs != 0 &amp;&amp; gsindex != 0) should</span>
<span class="p_del">-	 * be impossible.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Therefore we need to reload the segment registers if either</span>
<span class="p_del">-	 * the old or new selector is nonzero, and we need to override</span>
<span class="p_del">-	 * the base address if next thread expects it to be overridden.</span>
<span class="p_add">+	 * 64-bit bases are that controlled by arch_prctl.  The bases</span>
<span class="p_add">+	 * don&#39;t necessarily match the selectors, as user code can do</span>
<span class="p_add">+	 * any number of things to cause them to be inconsistent.</span>
 	 *
<span class="p_del">-	 * This code is unnecessarily slow in the case where the old and</span>
<span class="p_del">-	 * new indexes are zero and the new base is nonzero -- it will</span>
<span class="p_del">-	 * unnecessarily write 0 to the selector before writing the new</span>
<span class="p_del">-	 * base address.</span>
<span class="p_add">+	 * We don&#39;t promise to preserve the bases if the selectors are</span>
<span class="p_add">+	 * nonzero.  We also don&#39;t promise to preserve the base if the</span>
<span class="p_add">+	 * selector is zero and the base doesn&#39;t match whatever was</span>
<span class="p_add">+	 * most recently passed to ARCH_SET_FS/GS.  (If/when the</span>
<span class="p_add">+	 * FSGSBASE instructions are enabled, we&#39;ll need to offer</span>
<span class="p_add">+	 * stronger guarantees.)</span>
 	 *
<span class="p_del">-	 * Note: This all depends on arch_prctl being the only way that</span>
<span class="p_del">-	 * user code can override the segment base.  Once wrfsbase and</span>
<span class="p_del">-	 * wrgsbase are enabled, most of this code will need to change.</span>
<span class="p_add">+	 * As an invariant,</span>
<span class="p_add">+	 * (fsbase != 0 &amp;&amp; fsindex != 0) || (gsbase != 0 &amp;&amp; gsindex != 0) is</span>
<span class="p_add">+	 * impossible.</span>
 	 */
<span class="p_del">-	if (unlikely(fsindex | next-&gt;fsindex | prev-&gt;fs)) {</span>
<span class="p_add">+	if (next-&gt;fsindex) {</span>
<span class="p_add">+		/* Loading a nonzero value into FS sets the index and base. */</span>
 		loadsegment(fs, next-&gt;fsindex);
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If user code wrote a nonzero value to FS, then it also</span>
<span class="p_del">-		 * cleared the overridden base address.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * XXX: if user code wrote 0 to FS and cleared the base</span>
<span class="p_del">-		 * address itself, we won&#39;t notice and we&#39;ll incorrectly</span>
<span class="p_del">-		 * restore the prior base address next time we reschdule</span>
<span class="p_del">-		 * the process.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (fsindex)</span>
<span class="p_del">-			prev-&gt;fs = 0;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (next-&gt;fsbase) {</span>
<span class="p_add">+			/* Next index is zero but next base is nonzero. */</span>
<span class="p_add">+			if (prev_fsindex)</span>
<span class="p_add">+				loadsegment(fs, 0);</span>
<span class="p_add">+			wrmsrl(MSR_FS_BASE, next-&gt;fsbase);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Next base and index are both zero. */</span>
<span class="p_add">+			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We don&#39;t know the previous base and can&#39;t</span>
<span class="p_add">+				 * find out without RDMSR.  Forcibly clear it.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				loadsegment(fs, __USER_DS);</span>
<span class="p_add">+				loadsegment(fs, 0);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * If the previous index is zero and ARCH_SET_FS</span>
<span class="p_add">+				 * didn&#39;t change the base, then the base is</span>
<span class="p_add">+				 * also zero and we don&#39;t need to do anything.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (prev-&gt;fsbase || prev_fsindex)</span>
<span class="p_add">+					loadsegment(fs, 0);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 	}
<span class="p_del">-	if (next-&gt;fs)</span>
<span class="p_del">-		wrmsrl(MSR_FS_BASE, next-&gt;fs);</span>
<span class="p_del">-	prev-&gt;fsindex = fsindex;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Save the old state and preserve the invariant.</span>
<span class="p_add">+	 * NB: if prev_fsindex == 0, then we can&#39;t reliably learn the base</span>
<span class="p_add">+	 * without RDMSR because Intel user code can zero it without telling</span>
<span class="p_add">+	 * us and AMD user code can program any 32-bit value without telling</span>
<span class="p_add">+	 * us.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (prev_fsindex)</span>
<span class="p_add">+		prev-&gt;fsbase = 0;</span>
<span class="p_add">+	prev-&gt;fsindex = prev_fsindex;</span>
 
<span class="p_del">-	if (unlikely(gsindex | next-&gt;gsindex | prev-&gt;gs)) {</span>
<span class="p_add">+	if (next-&gt;gsindex) {</span>
<span class="p_add">+		/* Loading a nonzero value into GS sets the index and base. */</span>
 		load_gs_index(next-&gt;gsindex);
<span class="p_del">-</span>
<span class="p_del">-		/* This works (and fails) the same way as fsindex above. */</span>
<span class="p_del">-		if (gsindex)</span>
<span class="p_del">-			prev-&gt;gs = 0;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (next-&gt;gsbase) {</span>
<span class="p_add">+			/* Next index is zero but next base is nonzero. */</span>
<span class="p_add">+			if (prev_gsindex)</span>
<span class="p_add">+				load_gs_index(0);</span>
<span class="p_add">+			wrmsrl(MSR_KERNEL_GS_BASE, next-&gt;gsbase);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Next base and index are both zero. */</span>
<span class="p_add">+			if (static_cpu_has_bug(X86_BUG_NULL_SEG)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We don&#39;t know the previous base and can&#39;t</span>
<span class="p_add">+				 * find out without RDMSR.  Forcibly clear it.</span>
<span class="p_add">+				 *</span>
<span class="p_add">+				 * This contains a pointless SWAPGS pair.</span>
<span class="p_add">+				 * Fixing it would involve an explicit check</span>
<span class="p_add">+				 * for Xen or a new pvop.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				load_gs_index(__USER_DS);</span>
<span class="p_add">+				load_gs_index(0);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * If the previous index is zero and ARCH_SET_GS</span>
<span class="p_add">+				 * didn&#39;t change the base, then the base is</span>
<span class="p_add">+				 * also zero and we don&#39;t need to do anything.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (prev-&gt;gsbase || prev_gsindex)</span>
<span class="p_add">+					load_gs_index(0);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 	}
<span class="p_del">-	if (next-&gt;gs)</span>
<span class="p_del">-		wrmsrl(MSR_KERNEL_GS_BASE, next-&gt;gs);</span>
<span class="p_del">-	prev-&gt;gsindex = gsindex;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Save the old state and preserve the invariant.</span>
<span class="p_add">+	 * NB: if prev_gsindex == 0, then we can&#39;t reliably learn the base</span>
<span class="p_add">+	 * without RDMSR because Intel user code can zero it without telling</span>
<span class="p_add">+	 * us and AMD user code can program any 32-bit value without telling</span>
<span class="p_add">+	 * us.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (prev_gsindex)</span>
<span class="p_add">+		prev-&gt;gsbase = 0;</span>
<span class="p_add">+	prev-&gt;gsindex = prev_gsindex;</span>
 
 	switch_fpu_finish(next_fpu, fpu_switch);
 
<span class="p_chunk">@@ -516,23 +535,11 @@</span> <span class="p_context"> long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)</span>
 		if (addr &gt;= TASK_SIZE_OF(task))
 			return -EPERM;
 		cpu = get_cpu();
<span class="p_del">-		/* handle small bases via the GDT because that&#39;s faster to</span>
<span class="p_del">-		   switch. */</span>
<span class="p_del">-		if (addr &lt;= 0xffffffff) {</span>
<span class="p_del">-			set_32bit_tls(task, GS_TLS, addr);</span>
<span class="p_del">-			if (doit) {</span>
<span class="p_del">-				load_TLS(&amp;task-&gt;thread, cpu);</span>
<span class="p_del">-				load_gs_index(GS_TLS_SEL);</span>
<span class="p_del">-			}</span>
<span class="p_del">-			task-&gt;thread.gsindex = GS_TLS_SEL;</span>
<span class="p_del">-			task-&gt;thread.gs = 0;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			task-&gt;thread.gsindex = 0;</span>
<span class="p_del">-			task-&gt;thread.gs = addr;</span>
<span class="p_del">-			if (doit) {</span>
<span class="p_del">-				load_gs_index(0);</span>
<span class="p_del">-				ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);</span>
<span class="p_del">-			}</span>
<span class="p_add">+		task-&gt;thread.gsindex = 0;</span>
<span class="p_add">+		task-&gt;thread.gsbase = addr;</span>
<span class="p_add">+		if (doit) {</span>
<span class="p_add">+			load_gs_index(0);</span>
<span class="p_add">+			ret = wrmsrl_safe(MSR_KERNEL_GS_BASE, addr);</span>
 		}
 		put_cpu();
 		break;
<span class="p_chunk">@@ -542,52 +549,30 @@</span> <span class="p_context"> long do_arch_prctl(struct task_struct *task, int code, unsigned long addr)</span>
 		if (addr &gt;= TASK_SIZE_OF(task))
 			return -EPERM;
 		cpu = get_cpu();
<span class="p_del">-		/* handle small bases via the GDT because that&#39;s faster to</span>
<span class="p_del">-		   switch. */</span>
<span class="p_del">-		if (addr &lt;= 0xffffffff) {</span>
<span class="p_del">-			set_32bit_tls(task, FS_TLS, addr);</span>
<span class="p_del">-			if (doit) {</span>
<span class="p_del">-				load_TLS(&amp;task-&gt;thread, cpu);</span>
<span class="p_del">-				loadsegment(fs, FS_TLS_SEL);</span>
<span class="p_del">-			}</span>
<span class="p_del">-			task-&gt;thread.fsindex = FS_TLS_SEL;</span>
<span class="p_del">-			task-&gt;thread.fs = 0;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			task-&gt;thread.fsindex = 0;</span>
<span class="p_del">-			task-&gt;thread.fs = addr;</span>
<span class="p_del">-			if (doit) {</span>
<span class="p_del">-				/* set the selector to 0 to not confuse</span>
<span class="p_del">-				   __switch_to */</span>
<span class="p_del">-				loadsegment(fs, 0);</span>
<span class="p_del">-				ret = wrmsrl_safe(MSR_FS_BASE, addr);</span>
<span class="p_del">-			}</span>
<span class="p_add">+		task-&gt;thread.fsindex = 0;</span>
<span class="p_add">+		task-&gt;thread.fsbase = addr;</span>
<span class="p_add">+		if (doit) {</span>
<span class="p_add">+			/* set the selector to 0 to not confuse __switch_to */</span>
<span class="p_add">+			loadsegment(fs, 0);</span>
<span class="p_add">+			ret = wrmsrl_safe(MSR_FS_BASE, addr);</span>
 		}
 		put_cpu();
 		break;
 	case ARCH_GET_FS: {
 		unsigned long base;
<span class="p_del">-		if (task-&gt;thread.fsindex == FS_TLS_SEL)</span>
<span class="p_del">-			base = read_32bit_tls(task, FS_TLS);</span>
<span class="p_del">-		else if (doit)</span>
<span class="p_add">+		if (doit)</span>
 			rdmsrl(MSR_FS_BASE, base);
 		else
<span class="p_del">-			base = task-&gt;thread.fs;</span>
<span class="p_add">+			base = task-&gt;thread.fsbase;</span>
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;
 	}
 	case ARCH_GET_GS: {
 		unsigned long base;
<span class="p_del">-		unsigned gsindex;</span>
<span class="p_del">-		if (task-&gt;thread.gsindex == GS_TLS_SEL)</span>
<span class="p_del">-			base = read_32bit_tls(task, GS_TLS);</span>
<span class="p_del">-		else if (doit) {</span>
<span class="p_del">-			savesegment(gs, gsindex);</span>
<span class="p_del">-			if (gsindex)</span>
<span class="p_del">-				rdmsrl(MSR_KERNEL_GS_BASE, base);</span>
<span class="p_del">-			else</span>
<span class="p_del">-				base = task-&gt;thread.gs;</span>
<span class="p_del">-		} else</span>
<span class="p_del">-			base = task-&gt;thread.gs;</span>
<span class="p_add">+		if (doit)</span>
<span class="p_add">+			rdmsrl(MSR_KERNEL_GS_BASE, base);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			base = task-&gt;thread.gsbase;</span>
 		ret = put_user(base, (unsigned long __user *)addr);
 		break;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c</span>
<span class="p_header">index 32e9d9cbb884..e60ef918f53d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ptrace.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ptrace.c</span>
<span class="p_chunk">@@ -303,29 +303,11 @@</span> <span class="p_context"> static int set_segment_reg(struct task_struct *task,</span>
 
 	switch (offset) {
 	case offsetof(struct user_regs_struct,fs):
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If this is setting fs as for normal 64-bit use but</span>
<span class="p_del">-		 * setting fs_base has implicitly changed it, leave it.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if ((value == FS_TLS_SEL &amp;&amp; task-&gt;thread.fsindex == 0 &amp;&amp;</span>
<span class="p_del">-		     task-&gt;thread.fs != 0) ||</span>
<span class="p_del">-		    (value == 0 &amp;&amp; task-&gt;thread.fsindex == FS_TLS_SEL &amp;&amp;</span>
<span class="p_del">-		     task-&gt;thread.fs == 0))</span>
<span class="p_del">-			break;</span>
 		task-&gt;thread.fsindex = value;
 		if (task == current)
 			loadsegment(fs, task-&gt;thread.fsindex);
 		break;
 	case offsetof(struct user_regs_struct,gs):
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If this is setting gs as for normal 64-bit use but</span>
<span class="p_del">-		 * setting gs_base has implicitly changed it, leave it.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if ((value == GS_TLS_SEL &amp;&amp; task-&gt;thread.gsindex == 0 &amp;&amp;</span>
<span class="p_del">-		     task-&gt;thread.gs != 0) ||</span>
<span class="p_del">-		    (value == 0 &amp;&amp; task-&gt;thread.gsindex == GS_TLS_SEL &amp;&amp;</span>
<span class="p_del">-		     task-&gt;thread.gs == 0))</span>
<span class="p_del">-			break;</span>
 		task-&gt;thread.gsindex = value;
 		if (task == current)
 			load_gs_index(task-&gt;thread.gsindex);
<span class="p_chunk">@@ -417,7 +399,7 @@</span> <span class="p_context"> static int putreg(struct task_struct *child,</span>
 		 * to set either thread.fs or thread.fsindex and the
 		 * corresponding GDT slot.
 		 */
<span class="p_del">-		if (child-&gt;thread.fs != value)</span>
<span class="p_add">+		if (child-&gt;thread.fsbase != value)</span>
 			return do_arch_prctl(child, ARCH_SET_FS, value);
 		return 0;
 	case offsetof(struct user_regs_struct,gs_base):
<span class="p_chunk">@@ -426,7 +408,7 @@</span> <span class="p_context"> static int putreg(struct task_struct *child,</span>
 		 */
 		if (value &gt;= TASK_SIZE_OF(child))
 			return -EIO;
<span class="p_del">-		if (child-&gt;thread.gs != value)</span>
<span class="p_add">+		if (child-&gt;thread.gsbase != value)</span>
 			return do_arch_prctl(child, ARCH_SET_GS, value);
 		return 0;
 #endif
<span class="p_chunk">@@ -453,31 +435,17 @@</span> <span class="p_context"> static unsigned long getreg(struct task_struct *task, unsigned long offset)</span>
 #ifdef CONFIG_X86_64
 	case offsetof(struct user_regs_struct, fs_base): {
 		/*
<span class="p_del">-		 * do_arch_prctl may have used a GDT slot instead of</span>
<span class="p_del">-		 * the MSR.  To userland, it appears the same either</span>
<span class="p_del">-		 * way, except the %fs segment selector might not be 0.</span>
<span class="p_add">+		 * XXX: This will not behave as expected if called on</span>
<span class="p_add">+		 * current or if fsindex != 0.</span>
 		 */
<span class="p_del">-		unsigned int seg = task-&gt;thread.fsindex;</span>
<span class="p_del">-		if (task-&gt;thread.fs != 0)</span>
<span class="p_del">-			return task-&gt;thread.fs;</span>
<span class="p_del">-		if (task == current)</span>
<span class="p_del">-			asm(&quot;movl %%fs,%0&quot; : &quot;=r&quot; (seg));</span>
<span class="p_del">-		if (seg != FS_TLS_SEL)</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		return get_desc_base(&amp;task-&gt;thread.tls_array[FS_TLS]);</span>
<span class="p_add">+		return task-&gt;thread.fsbase;</span>
 	}
 	case offsetof(struct user_regs_struct, gs_base): {
 		/*
<span class="p_del">-		 * Exactly the same here as the %fs handling above.</span>
<span class="p_add">+		 * XXX: This will not behave as expected if called on</span>
<span class="p_add">+		 * current or if fsindex != 0.</span>
 		 */
<span class="p_del">-		unsigned int seg = task-&gt;thread.gsindex;</span>
<span class="p_del">-		if (task-&gt;thread.gs != 0)</span>
<span class="p_del">-			return task-&gt;thread.gs;</span>
<span class="p_del">-		if (task == current)</span>
<span class="p_del">-			asm(&quot;movl %%gs,%0&quot; : &quot;=r&quot; (seg));</span>
<span class="p_del">-		if (seg != GS_TLS_SEL)</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		return get_desc_base(&amp;task-&gt;thread.tls_array[GS_TLS]);</span>
<span class="p_add">+		return task-&gt;thread.gsbase;</span>
 	}
 #endif
 	}
<span class="p_chunk">@@ -1266,7 +1234,7 @@</span> <span class="p_context"> long compat_arch_ptrace(struct task_struct *child, compat_long_t request,</span>
 			compat_ulong_t caddr, compat_ulong_t cdata)
 {
 #ifdef CONFIG_X86_X32_ABI
<span class="p_del">-	if (!is_ia32_task())</span>
<span class="p_add">+	if (!in_ia32_syscall())</span>
 		return x32_arch_ptrace(child, request, caddr, cdata);
 #endif
 #ifdef CONFIG_IA32_EMULATION
<span class="p_header">diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c</span>
<span class="p_header">index 548ddf7d6fd2..2ebcc60f0e14 100644</span>
<span class="p_header">--- a/arch/x86/kernel/signal.c</span>
<span class="p_header">+++ b/arch/x86/kernel/signal.c</span>
<span class="p_chunk">@@ -391,7 +391,7 @@</span> <span class="p_context"> static int __setup_rt_frame(int sig, struct ksignal *ksig,</span>
 		put_user_ex(&amp;frame-&gt;uc, &amp;frame-&gt;puc);
 
 		/* Create the ucontext.  */
<span class="p_del">-		if (cpu_has_xsave)</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 			put_user_ex(UC_FP_XSTATE, &amp;frame-&gt;uc.uc_flags);
 		else
 			put_user_ex(0, &amp;frame-&gt;uc.uc_flags);
<span class="p_chunk">@@ -442,7 +442,7 @@</span> <span class="p_context"> static unsigned long frame_uc_flags(struct pt_regs *regs)</span>
 {
 	unsigned long flags;
 
<span class="p_del">-	if (cpu_has_xsave)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		flags = UC_FP_XSTATE | UC_SIGCONTEXT_SS;
 	else
 		flags = UC_SIGCONTEXT_SS;
<span class="p_chunk">@@ -762,7 +762,7 @@</span> <span class="p_context"> handle_signal(struct ksignal *ksig, struct pt_regs *regs)</span>
 static inline unsigned long get_nr_restart_syscall(const struct pt_regs *regs)
 {
 #ifdef CONFIG_X86_64
<span class="p_del">-	if (is_ia32_task())</span>
<span class="p_add">+	if (in_ia32_syscall())</span>
 		return __NR_ia32_restart_syscall;
 #endif
 #ifdef CONFIG_X86_X32_ABI
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index a2065d3b3b39..1fe4130b14d9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -1231,7 +1231,7 @@</span> <span class="p_context"> static int __init smp_sanity_check(unsigned max_cpus)</span>
 	 * If we couldn&#39;t find a local APIC, then get out of here now!
 	 */
 	if (APIC_INTEGRATED(apic_version[boot_cpu_physical_apicid]) &amp;&amp;
<span class="p_del">-	    !cpu_has_apic) {</span>
<span class="p_add">+	    !boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		if (!disable_apic) {
 			pr_err(&quot;BIOS bug, local APIC #%d not detected!...\n&quot;,
 				boot_cpu_physical_apicid);
<span class="p_header">diff --git a/arch/x86/kernel/tce_64.c b/arch/x86/kernel/tce_64.c</span>
<span class="p_header">index ab40954e113e..f386bad0984e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tce_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tce_64.c</span>
<span class="p_chunk">@@ -40,7 +40,7 @@</span> <span class="p_context"></span>
 static inline void flush_tce(void* tceaddr)
 {
 	/* a single tce can&#39;t cross a cache line */
<span class="p_del">-	if (cpu_has_clflush)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_CLFLUSH))</span>
 		clflush(tceaddr);
 	else
 		wbinvd();
<span class="p_header">diff --git a/arch/x86/kernel/tls.c b/arch/x86/kernel/tls.c</span>
<span class="p_header">index 7fc5e843f247..9692a5e9fdab 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tls.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tls.c</span>
<span class="p_chunk">@@ -114,6 +114,7 @@</span> <span class="p_context"> int do_set_thread_area(struct task_struct *p, int idx,</span>
 		       int can_allocate)
 {
 	struct user_desc info;
<span class="p_add">+	unsigned short __maybe_unused sel, modified_sel;</span>
 
 	if (copy_from_user(&amp;info, u_info, sizeof(info)))
 		return -EFAULT;
<span class="p_chunk">@@ -141,6 +142,47 @@</span> <span class="p_context"> int do_set_thread_area(struct task_struct *p, int idx,</span>
 
 	set_tls_desc(p, idx, &amp;info, 1);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If DS, ES, FS, or GS points to the modified segment, forcibly</span>
<span class="p_add">+	 * refresh it.  Only needed on x86_64 because x86_32 reloads them</span>
<span class="p_add">+	 * on return to user mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	modified_sel = (idx &lt;&lt; 3) | 3;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p == current) {</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		savesegment(ds, sel);</span>
<span class="p_add">+		if (sel == modified_sel)</span>
<span class="p_add">+			loadsegment(ds, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+		savesegment(es, sel);</span>
<span class="p_add">+		if (sel == modified_sel)</span>
<span class="p_add">+			loadsegment(es, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+		savesegment(fs, sel);</span>
<span class="p_add">+		if (sel == modified_sel)</span>
<span class="p_add">+			loadsegment(fs, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+		savesegment(gs, sel);</span>
<span class="p_add">+		if (sel == modified_sel)</span>
<span class="p_add">+			load_gs_index(sel);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32_LAZY_GS</span>
<span class="p_add">+		savesegment(gs, sel);</span>
<span class="p_add">+		if (sel == modified_sel)</span>
<span class="p_add">+			loadsegment(gs, sel);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		if (p-&gt;thread.fsindex == modified_sel)</span>
<span class="p_add">+			p-&gt;thread.fsbase = info.base_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (p-&gt;thread.gsindex == modified_sel)</span>
<span class="p_add">+			p-&gt;thread.gsbase = info.base_addr;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 06cbe25861f1..d1590486204a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -51,6 +51,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/debugreg.h&gt;
 #include &lt;linux/atomic.h&gt;
<span class="p_add">+#include &lt;asm/text-patching.h&gt;</span>
 #include &lt;asm/ftrace.h&gt;
 #include &lt;asm/traps.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_header">diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c</span>
<span class="p_header">index c9c4c7ce3eb2..38ba6de56ede 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tsc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tsc.c</span>
<span class="p_chunk">@@ -36,7 +36,7 @@</span> <span class="p_context"> static int __read_mostly tsc_unstable;</span>
 
 /* native_sched_clock() is called before tsc_init(), so
    we must start with the TSC soft disabled to prevent
<span class="p_del">-   erroneous rdtsc usage on !cpu_has_tsc processors */</span>
<span class="p_add">+   erroneous rdtsc usage on !boot_cpu_has(X86_FEATURE_TSC) processors */</span>
 static int __read_mostly tsc_disabled = -1;
 
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
<span class="p_chunk">@@ -834,15 +834,15 @@</span> <span class="p_context"> int recalibrate_cpu_khz(void)</span>
 #ifndef CONFIG_SMP
 	unsigned long cpu_khz_old = cpu_khz;
 
<span class="p_del">-	if (cpu_has_tsc) {</span>
<span class="p_del">-		tsc_khz = x86_platform.calibrate_tsc();</span>
<span class="p_del">-		cpu_khz = tsc_khz;</span>
<span class="p_del">-		cpu_data(0).loops_per_jiffy =</span>
<span class="p_del">-			cpufreq_scale(cpu_data(0).loops_per_jiffy,</span>
<span class="p_del">-					cpu_khz_old, cpu_khz);</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	} else</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC))</span>
 		return -ENODEV;
<span class="p_add">+</span>
<span class="p_add">+	tsc_khz = x86_platform.calibrate_tsc();</span>
<span class="p_add">+	cpu_khz = tsc_khz;</span>
<span class="p_add">+	cpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,</span>
<span class="p_add">+						    cpu_khz_old, cpu_khz);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
 #else
 	return -ENODEV;
 #endif
<span class="p_chunk">@@ -922,9 +922,6 @@</span> <span class="p_context"> static int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,</span>
 	struct cpufreq_freqs *freq = data;
 	unsigned long *lpj;
 
<span class="p_del">-	if (cpu_has(&amp;cpu_data(freq-&gt;cpu), X86_FEATURE_CONSTANT_TSC))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
 	lpj = &amp;boot_cpu_data.loops_per_jiffy;
 #ifdef CONFIG_SMP
 	if (!(freq-&gt;flags &amp; CPUFREQ_CONST_LOOPS))
<span class="p_chunk">@@ -954,9 +951,9 @@</span> <span class="p_context"> static struct notifier_block time_cpufreq_notifier_block = {</span>
 	.notifier_call  = time_cpufreq_notifier
 };
 
<span class="p_del">-static int __init cpufreq_tsc(void)</span>
<span class="p_add">+static int __init cpufreq_register_tsc_scaling(void)</span>
 {
<span class="p_del">-	if (!cpu_has_tsc)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC))</span>
 		return 0;
 	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		return 0;
<span class="p_chunk">@@ -965,7 +962,7 @@</span> <span class="p_context"> static int __init cpufreq_tsc(void)</span>
 	return 0;
 }
 
<span class="p_del">-core_initcall(cpufreq_tsc);</span>
<span class="p_add">+core_initcall(cpufreq_register_tsc_scaling);</span>
 
 #endif /* CONFIG_CPU_FREQ */
 
<span class="p_chunk">@@ -1081,7 +1078,7 @@</span> <span class="p_context"> static void __init check_system_tsc_reliable(void)</span>
  */
 int unsynchronized_tsc(void)
 {
<span class="p_del">-	if (!cpu_has_tsc || tsc_unstable)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_unstable)</span>
 		return 1;
 
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -1205,7 +1202,7 @@</span> <span class="p_context"> static void tsc_refine_calibration_work(struct work_struct *work)</span>
 
 static int __init init_tsc_clocksource(void)
 {
<span class="p_del">-	if (!cpu_has_tsc || tsc_disabled &gt; 0 || !tsc_khz)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC) || tsc_disabled &gt; 0 || !tsc_khz)</span>
 		return 0;
 
 	if (tsc_clocksource_reliable)
<span class="p_chunk">@@ -1242,7 +1239,7 @@</span> <span class="p_context"> void __init tsc_init(void)</span>
 	u64 lpj;
 	int cpu;
 
<span class="p_del">-	if (!cpu_has_tsc) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_TSC)) {</span>
 		setup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/uprobes.c b/arch/x86/kernel/uprobes.c</span>
<span class="p_header">index bf4db6eaec8f..98b4dc87628b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/uprobes.c</span>
<span class="p_header">+++ b/arch/x86/kernel/uprobes.c</span>
<span class="p_chunk">@@ -516,7 +516,7 @@</span> <span class="p_context"> struct uprobe_xol_ops {</span>
 
 static inline int sizeof_long(void)
 {
<span class="p_del">-	return is_ia32_task() ? 4 : 8;</span>
<span class="p_add">+	return in_ia32_syscall() ? 4 : 8;</span>
 }
 
 static int default_pre_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="p_header">index bbbaa802d13e..769af907f824 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.c</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.c</span>
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
 		return 0;
 
 	/* Update OSXSAVE bit */
<span class="p_del">-	if (cpu_has_xsave &amp;&amp; best-&gt;function == 0x1) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE) &amp;&amp; best-&gt;function == 0x1) {</span>
 		best-&gt;ecx &amp;= ~F(OSXSAVE);
 		if (kvm_read_cr4_bits(vcpu, X86_CR4_OSXSAVE))
 			best-&gt;ecx |= F(OSXSAVE);
<span class="p_header">diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="p_header">index 1ff4dbb73fb7..4c6972f9ad1b 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu.c</span>
<span class="p_chunk">@@ -3844,7 +3844,8 @@</span> <span class="p_context"> reset_tdp_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,</span>
 		__reset_rsvds_bits_mask(vcpu, &amp;context-&gt;shadow_zero_check,
 					boot_cpu_data.x86_phys_bits,
 					context-&gt;shadow_root_level, false,
<span class="p_del">-					cpu_has_gbpages, true, true);</span>
<span class="p_add">+					boot_cpu_has(X86_FEATURE_GBPAGES),</span>
<span class="p_add">+					true, true);</span>
 	else
 		__reset_rsvds_bits_mask_ept(&amp;context-&gt;shadow_zero_check,
 					    boot_cpu_data.x86_phys_bits,
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index 31346a3f20a5..fafd720ce10a 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -1254,7 +1254,7 @@</span> <span class="p_context"> static void svm_vcpu_put(struct kvm_vcpu *vcpu)</span>
 	kvm_load_ldt(svm-&gt;host.ldt);
 #ifdef CONFIG_X86_64
 	loadsegment(fs, svm-&gt;host.fs);
<span class="p_del">-	wrmsrl(MSR_KERNEL_GS_BASE, current-&gt;thread.gs);</span>
<span class="p_add">+	wrmsrl(MSR_KERNEL_GS_BASE, current-&gt;thread.gsbase);</span>
 	load_gs_index(svm-&gt;host.gs);
 #else
 #ifdef CONFIG_X86_32_LAZY_GS
<span class="p_header">diff --git a/arch/x86/kvm/trace.h b/arch/x86/kvm/trace.h</span>
<span class="p_header">index 2f1ea2f61e1f..b72743c5668d 100644</span>
<span class="p_header">--- a/arch/x86/kvm/trace.h</span>
<span class="p_header">+++ b/arch/x86/kvm/trace.h</span>
<span class="p_chunk">@@ -809,8 +809,7 @@</span> <span class="p_context"> TRACE_EVENT(kvm_write_tsc_offset,</span>
 
 #define host_clocks					\
 	{VCLOCK_NONE, &quot;none&quot;},				\
<span class="p_del">-	{VCLOCK_TSC,  &quot;tsc&quot;},				\</span>
<span class="p_del">-	{VCLOCK_HPET, &quot;hpet&quot;}				\</span>
<span class="p_add">+	{VCLOCK_TSC,  &quot;tsc&quot;}				\</span>
 
 TRACE_EVENT(kvm_update_master_clock,
 	TP_PROTO(bool use_master_clock, unsigned int host_clock, bool offset_matched),
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 133679d520af..cb47fe3da292 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -3390,7 +3390,7 @@</span> <span class="p_context"> static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf)</span>
 		}
 	}
 
<span class="p_del">-	if (cpu_has_xsaves)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVES))</span>
 		rdmsrl(MSR_IA32_XSS, host_xss);
 
 	return 0;
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 9b7798c7b210..12f33e662382 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -2611,7 +2611,7 @@</span> <span class="p_context"> int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)</span>
 		r = KVM_MAX_MCE_BANKS;
 		break;
 	case KVM_CAP_XCRS:
<span class="p_del">-		r = cpu_has_xsave;</span>
<span class="p_add">+		r = boot_cpu_has(X86_FEATURE_XSAVE);</span>
 		break;
 	case KVM_CAP_TSC_CONTROL:
 		r = kvm_has_tsc_control;
<span class="p_chunk">@@ -3094,7 +3094,7 @@</span> <span class="p_context"> static void load_xsave(struct kvm_vcpu *vcpu, u8 *src)</span>
 
 	/* Set XSTATE_BV and possibly XCOMP_BV.  */
 	xsave-&gt;header.xfeatures = xstate_bv;
<span class="p_del">-	if (cpu_has_xsaves)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVES))</span>
 		xsave-&gt;header.xcomp_bv = host_xcr0 | XSTATE_COMPACTION_ENABLED;
 
 	/*
<span class="p_chunk">@@ -3121,7 +3121,7 @@</span> <span class="p_context"> static void load_xsave(struct kvm_vcpu *vcpu, u8 *src)</span>
 static void kvm_vcpu_ioctl_x86_get_xsave(struct kvm_vcpu *vcpu,
 					 struct kvm_xsave *guest_xsave)
 {
<span class="p_del">-	if (cpu_has_xsave) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE)) {</span>
 		memset(guest_xsave, 0, sizeof(struct kvm_xsave));
 		fill_xsave((u8 *) guest_xsave-&gt;region, vcpu);
 	} else {
<span class="p_chunk">@@ -3139,7 +3139,7 @@</span> <span class="p_context"> static int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,</span>
 	u64 xstate_bv =
 		*(u64 *)&amp;guest_xsave-&gt;region[XSAVE_HDR_OFFSET / sizeof(u32)];
 
<span class="p_del">-	if (cpu_has_xsave) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE)) {</span>
 		/*
 		 * Here we allow setting states that are not present in
 		 * CPUID leaf 0xD, index 0, EDX:EAX.  This is for compatibility
<span class="p_chunk">@@ -3160,7 +3160,7 @@</span> <span class="p_context"> static int kvm_vcpu_ioctl_x86_set_xsave(struct kvm_vcpu *vcpu,</span>
 static void kvm_vcpu_ioctl_x86_get_xcrs(struct kvm_vcpu *vcpu,
 					struct kvm_xcrs *guest_xcrs)
 {
<span class="p_del">-	if (!cpu_has_xsave) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE)) {</span>
 		guest_xcrs-&gt;nr_xcrs = 0;
 		return;
 	}
<span class="p_chunk">@@ -3176,7 +3176,7 @@</span> <span class="p_context"> static int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,</span>
 {
 	int i, r = 0;
 
<span class="p_del">-	if (!cpu_has_xsave)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		return -EINVAL;
 
 	if (guest_xcrs-&gt;nr_xcrs &gt; KVM_MAX_XCRS || guest_xcrs-&gt;flags)
<span class="p_chunk">@@ -5865,7 +5865,7 @@</span> <span class="p_context"> int kvm_arch_init(void *opaque)</span>
 
 	perf_register_guest_info_callbacks(&amp;kvm_guest_cbs);
 
<span class="p_del">-	if (cpu_has_xsave)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVE))</span>
 		host_xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);
 
 	kvm_lapic_init();
<span class="p_chunk">@@ -7293,7 +7293,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)</span>
 static void fx_init(struct kvm_vcpu *vcpu)
 {
 	fpstate_init(&amp;vcpu-&gt;arch.guest_fpu.state);
<span class="p_del">-	if (cpu_has_xsaves)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XSAVES))</span>
 		vcpu-&gt;arch.guest_fpu.state.xsave.header.xcomp_bv =
 			host_xcr0 | XSTATE_COMPACTION_ENABLED;
 
<span class="p_header">diff --git a/arch/x86/lib/usercopy_32.c b/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">index 91d93b95bd86..b559d9238781 100644</span>
<span class="p_header">--- a/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">+++ b/arch/x86/lib/usercopy_32.c</span>
<span class="p_chunk">@@ -612,7 +612,7 @@</span> <span class="p_context"> unsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,</span>
 {
 	stac();
 #ifdef CONFIG_X86_INTEL_USERCOPY
<span class="p_del">-	if (n &gt; 64 &amp;&amp; cpu_has_xmm2)</span>
<span class="p_add">+	if (n &gt; 64 &amp;&amp; static_cpu_has(X86_FEATURE_XMM2))</span>
 		n = __copy_user_zeroing_intel_nocache(to, from, n);
 	else
 		__copy_user_zeroing(to, from, n);
<span class="p_chunk">@@ -629,7 +629,7 @@</span> <span class="p_context"> unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *fr</span>
 {
 	stac();
 #ifdef CONFIG_X86_INTEL_USERCOPY
<span class="p_del">-	if (n &gt; 64 &amp;&amp; cpu_has_xmm2)</span>
<span class="p_add">+	if (n &gt; 64 &amp;&amp; static_cpu_has(X86_FEATURE_XMM2))</span>
 		n = __copy_user_intel_nocache(to, from, n);
 	else
 		__copy_user(to, from, n);
<span class="p_header">diff --git a/arch/x86/mm/extable.c b/arch/x86/mm/extable.c</span>
<span class="p_header">index 82447b3fba38..4bb53b89f3c5 100644</span>
<span class="p_header">--- a/arch/x86/mm/extable.c</span>
<span class="p_header">+++ b/arch/x86/mm/extable.c</span>
<span class="p_chunk">@@ -1,5 +1,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/module.h&gt;
 #include &lt;asm/uaccess.h&gt;
<span class="p_add">+#include &lt;asm/traps.h&gt;</span>
 
 typedef bool (*ex_handler_t)(const struct exception_table_entry *,
 			    struct pt_regs *, int);
<span class="p_chunk">@@ -42,6 +43,43 @@</span> <span class="p_context"> bool ex_handler_ext(const struct exception_table_entry *fixup,</span>
 }
 EXPORT_SYMBOL(ex_handler_ext);
 
<span class="p_add">+bool ex_handler_rdmsr_unsafe(const struct exception_table_entry *fixup,</span>
<span class="p_add">+			     struct pt_regs *regs, int trapnr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ONCE(1, &quot;unchecked MSR access error: RDMSR from 0x%x\n&quot;,</span>
<span class="p_add">+		  (unsigned int)regs-&gt;cx);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Pretend that the read succeeded and returned 0. */</span>
<span class="p_add">+	regs-&gt;ip = ex_fixup_addr(fixup);</span>
<span class="p_add">+	regs-&gt;ax = 0;</span>
<span class="p_add">+	regs-&gt;dx = 0;</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(ex_handler_rdmsr_unsafe);</span>
<span class="p_add">+</span>
<span class="p_add">+bool ex_handler_wrmsr_unsafe(const struct exception_table_entry *fixup,</span>
<span class="p_add">+			     struct pt_regs *regs, int trapnr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ONCE(1, &quot;unchecked MSR access error: WRMSR to 0x%x (tried to write 0x%08x%08x)\n&quot;,</span>
<span class="p_add">+		  (unsigned int)regs-&gt;cx,</span>
<span class="p_add">+		  (unsigned int)regs-&gt;dx, (unsigned int)regs-&gt;ax);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Pretend that the write succeeded. */</span>
<span class="p_add">+	regs-&gt;ip = ex_fixup_addr(fixup);</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(ex_handler_wrmsr_unsafe);</span>
<span class="p_add">+</span>
<span class="p_add">+bool ex_handler_clear_fs(const struct exception_table_entry *fixup,</span>
<span class="p_add">+			 struct pt_regs *regs, int trapnr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (static_cpu_has(X86_BUG_NULL_SEG))</span>
<span class="p_add">+		asm volatile (&quot;mov %0, %%fs&quot; : : &quot;rm&quot; (__USER_DS));</span>
<span class="p_add">+	asm volatile (&quot;mov %0, %%fs&quot; : : &quot;rm&quot; (0));</span>
<span class="p_add">+	return ex_handler_default(fixup, regs, trapnr);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(ex_handler_clear_fs);</span>
<span class="p_add">+</span>
 bool ex_has_fault_handler(unsigned long ip)
 {
 	const struct exception_table_entry *e;
<span class="p_chunk">@@ -82,24 +120,46 @@</span> <span class="p_context"> int fixup_exception(struct pt_regs *regs, int trapnr)</span>
 	return handler(e, regs, trapnr);
 }
 
<span class="p_add">+extern unsigned int early_recursion_flag;</span>
<span class="p_add">+</span>
 /* Restricted version used during very early boot */
<span class="p_del">-int __init early_fixup_exception(unsigned long *ip)</span>
<span class="p_add">+void __init early_fixup_exception(struct pt_regs *regs, int trapnr)</span>
 {
<span class="p_del">-	const struct exception_table_entry *e;</span>
<span class="p_del">-	unsigned long new_ip;</span>
<span class="p_del">-	ex_handler_t handler;</span>
<span class="p_del">-</span>
<span class="p_del">-	e = search_exception_tables(*ip);</span>
<span class="p_del">-	if (!e)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	new_ip  = ex_fixup_addr(e);</span>
<span class="p_del">-	handler = ex_fixup_handler(e);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* special handling not supported during early boot */</span>
<span class="p_del">-	if (handler != ex_handler_default)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	*ip = new_ip;</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	/* Ignore early NMIs. */</span>
<span class="p_add">+	if (trapnr == X86_TRAP_NMI)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (early_recursion_flag &gt; 2)</span>
<span class="p_add">+		goto halt_loop;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (regs-&gt;cs != __KERNEL_CS)</span>
<span class="p_add">+		goto fail;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The full exception fixup machinery is available as soon as</span>
<span class="p_add">+	 * the early IDT is loaded.  This means that it is the</span>
<span class="p_add">+	 * responsibility of extable users to either function correctly</span>
<span class="p_add">+	 * when handlers are invoked early or to simply avoid causing</span>
<span class="p_add">+	 * exceptions before they&#39;re ready to handle them.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This is better than filtering which handlers can be used,</span>
<span class="p_add">+	 * because refusing to call a handler here is guaranteed to</span>
<span class="p_add">+	 * result in a hard-to-debug panic.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Keep in mind that not all vectors actually get here.  Early</span>
<span class="p_add">+	 * fage faults, for example, are special.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (fixup_exception(regs, trapnr))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+fail:</span>
<span class="p_add">+	early_printk(&quot;PANIC: early exception 0x%02x IP %lx:%lx error %lx cr2 0x%lx\n&quot;,</span>
<span class="p_add">+		     (unsigned)trapnr, (unsigned long)regs-&gt;cs, regs-&gt;ip,</span>
<span class="p_add">+		     regs-&gt;orig_ax, read_cr2());</span>
<span class="p_add">+</span>
<span class="p_add">+	show_regs(regs);</span>
<span class="p_add">+</span>
<span class="p_add">+halt_loop:</span>
<span class="p_add">+	while (true)</span>
<span class="p_add">+		halt();</span>
 }
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 740d7ac03a55..14a95054d4e0 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -162,7 +162,7 @@</span> <span class="p_context"> static __init int setup_hugepagesz(char *opt)</span>
 	unsigned long ps = memparse(opt, &amp;opt);
 	if (ps == PMD_SIZE) {
 		hugetlb_add_hstate(PMD_SHIFT - PAGE_SHIFT);
<span class="p_del">-	} else if (ps == PUD_SIZE &amp;&amp; cpu_has_gbpages) {</span>
<span class="p_add">+	} else if (ps == PUD_SIZE &amp;&amp; boot_cpu_has(X86_FEATURE_GBPAGES)) {</span>
 		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 	} else {
 		printk(KERN_ERR &quot;hugepagesz: Unsupported page size %lu M\n&quot;,
<span class="p_chunk">@@ -177,7 +177,7 @@</span> <span class="p_context"> __setup(&quot;hugepagesz=&quot;, setup_hugepagesz);</span>
 static __init int gigantic_pages_init(void)
 {
 	/* With compaction or CMA we can allocate gigantic pages at runtime */
<span class="p_del">-	if (cpu_has_gbpages &amp;&amp; !size_to_hstate(1UL &lt;&lt; PUD_SHIFT))</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_GBPAGES) &amp;&amp; !size_to_hstate(1UL &lt;&lt; PUD_SHIFT))</span>
 		hugetlb_add_hstate(PUD_SHIFT - PAGE_SHIFT);
 	return 0;
 }
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 9d56f271d519..372aad2b3291 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -157,23 +157,23 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 	 * This will simplify cpa(), which otherwise needs to support splitting
 	 * large pages into small in interrupt context, etc.
 	 */
<span class="p_del">-	if (cpu_has_pse &amp;&amp; !debug_pagealloc_enabled())</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PSE) &amp;&amp; !debug_pagealloc_enabled())</span>
 		page_size_mask |= 1 &lt;&lt; PG_LEVEL_2M;
 #endif
 
 	/* Enable PSE if available */
<span class="p_del">-	if (cpu_has_pse)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PSE))</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	/* Enable PGE if available */
<span class="p_del">-	if (cpu_has_pge) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 		__supported_pte_mask |= _PAGE_GLOBAL;
 	} else
 		__supported_pte_mask &amp;= ~_PAGE_GLOBAL;
 
 	/* Enable 1 GB linear kernel mappings if available: */
<span class="p_del">-	if (direct_gbpages &amp;&amp; cpu_has_gbpages) {</span>
<span class="p_add">+	if (direct_gbpages &amp;&amp; boot_cpu_has(X86_FEATURE_GBPAGES)) {</span>
 		printk(KERN_INFO &quot;Using GB pages for direct mapping\n&quot;);
 		page_size_mask |= 1 &lt;&lt; PG_LEVEL_1G;
 	} else {
<span class="p_header">diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c</span>
<span class="p_header">index bd7a9b9e2e14..85af914e3d27 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_32.c</span>
<span class="p_chunk">@@ -284,7 +284,7 @@</span> <span class="p_context"> kernel_physical_mapping_init(unsigned long start,</span>
 	 */
 	mapping_iter = 1;
 
<span class="p_del">-	if (!cpu_has_pse)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PSE))</span>
 		use_pse = 0;
 
 repeat:
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 214afda97911..89d97477c1d9 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -1295,7 +1295,7 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)</span>
 	struct vmem_altmap *altmap = to_vmem_altmap(start);
 	int err;
 
<span class="p_del">-	if (cpu_has_pse)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PSE))</span>
 		err = vmemmap_populate_hugepages(start, end, node, altmap);
 	else if (altmap) {
 		pr_err_once(&quot;%s: no cpu support for altmap allocations\n&quot;,
<span class="p_chunk">@@ -1338,7 +1338,7 @@</span> <span class="p_context"> void register_page_bootmem_memmap(unsigned long section_nr,</span>
 		}
 		get_page_bootmem(section_nr, pud_page(*pud), MIX_SECTION_INFO);
 
<span class="p_del">-		if (!cpu_has_pse) {</span>
<span class="p_add">+		if (!boot_cpu_has(X86_FEATURE_PSE)) {</span>
 			next = (addr + PAGE_SIZE) &amp; PAGE_MASK;
 			pmd = pmd_offset(pud, addr);
 			if (pmd_none(*pmd))
<span class="p_header">diff --git a/arch/x86/mm/ioremap.c b/arch/x86/mm/ioremap.c</span>
<span class="p_header">index 0d8d53d1f5cc..f0894910bdd7 100644</span>
<span class="p_header">--- a/arch/x86/mm/ioremap.c</span>
<span class="p_header">+++ b/arch/x86/mm/ioremap.c</span>
<span class="p_chunk">@@ -378,7 +378,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(iounmap);</span>
 int __init arch_ioremap_pud_supported(void)
 {
 #ifdef CONFIG_X86_64
<span class="p_del">-	return cpu_has_gbpages;</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_GBPAGES);</span>
 #else
 	return 0;
 #endif
<span class="p_chunk">@@ -386,7 +386,7 @@</span> <span class="p_context"> int __init arch_ioremap_pud_supported(void)</span>
 
 int __init arch_ioremap_pmd_supported(void)
 {
<span class="p_del">-	return cpu_has_pse;</span>
<span class="p_add">+	return boot_cpu_has(X86_FEATURE_PSE);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 01be9ec3bf79..bbf462ff9745 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -1055,7 +1055,7 @@</span> <span class="p_context"> static int populate_pud(struct cpa_data *cpa, unsigned long start, pgd_t *pgd,</span>
 	/*
 	 * Map everything starting from the Gb boundary, possibly with 1G pages
 	 */
<span class="p_del">-	while (cpu_has_gbpages &amp;&amp; end - start &gt;= PUD_SIZE) {</span>
<span class="p_add">+	while (boot_cpu_has(X86_FEATURE_GBPAGES) &amp;&amp; end - start &gt;= PUD_SIZE) {</span>
 		set_pud(pud, __pud(cpa-&gt;pfn &lt;&lt; PAGE_SHIFT | _PAGE_PSE |
 				   massage_pgprot(pud_pgprot)));
 
<span class="p_chunk">@@ -1460,7 +1460,7 @@</span> <span class="p_context"> static int change_page_attr_set_clr(unsigned long *addr, int numpages,</span>
 	 * error case we fall back to cpa_flush_all (which uses
 	 * WBINVD):
 	 */
<span class="p_del">-	if (!ret &amp;&amp; cpu_has_clflush) {</span>
<span class="p_add">+	if (!ret &amp;&amp; boot_cpu_has(X86_FEATURE_CLFLUSH)) {</span>
 		if (cpa.flags &amp; (CPA_PAGES_ARRAY | CPA_ARRAY)) {
 			cpa_flush_array(addr, numpages, cache,
 					cpa.flags, pages);
<span class="p_header">diff --git a/arch/x86/mm/pat.c b/arch/x86/mm/pat.c</span>
<span class="p_header">index faec01e7a17d..fb0604f11eec 100644</span>
<span class="p_header">--- a/arch/x86/mm/pat.c</span>
<span class="p_header">+++ b/arch/x86/mm/pat.c</span>
<span class="p_chunk">@@ -40,11 +40,22 @@</span> <span class="p_context"></span>
 static bool boot_cpu_done;
 
 static int __read_mostly __pat_enabled = IS_ENABLED(CONFIG_X86_PAT);
<span class="p_add">+static void init_cache_modes(void);</span>
 
<span class="p_del">-static inline void pat_disable(const char *reason)</span>
<span class="p_add">+void pat_disable(const char *reason)</span>
 {
<span class="p_add">+	if (!__pat_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_done) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;x86/PAT: PAT cannot be disabled after initialization\n&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	__pat_enabled = 0;
 	pr_info(&quot;x86/PAT: %s\n&quot;, reason);
<span class="p_add">+</span>
<span class="p_add">+	init_cache_modes();</span>
 }
 
 static int __init nopat(char *str)
<span class="p_chunk">@@ -181,7 +192,7 @@</span> <span class="p_context"> static enum page_cache_mode pat_get_cache_mode(unsigned pat_val, char *msg)</span>
  * configuration.
  * Using lower indices is preferred, so we start with highest index.
  */
<span class="p_del">-void pat_init_cache_modes(u64 pat)</span>
<span class="p_add">+static void __init_cache_modes(u64 pat)</span>
 {
 	enum page_cache_mode cache;
 	char pat_msg[33];
<span class="p_chunk">@@ -202,14 +213,11 @@</span> <span class="p_context"> static void pat_bsp_init(u64 pat)</span>
 {
 	u64 tmp_pat;
 
<span class="p_del">-	if (!cpu_has_pat) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PAT)) {</span>
 		pat_disable(&quot;PAT not supported by CPU.&quot;);
 		return;
 	}
 
<span class="p_del">-	if (!pat_enabled())</span>
<span class="p_del">-		goto done;</span>
<span class="p_del">-</span>
 	rdmsrl(MSR_IA32_CR_PAT, tmp_pat);
 	if (!tmp_pat) {
 		pat_disable(&quot;PAT MSR is 0, disabled.&quot;);
<span class="p_chunk">@@ -218,16 +226,12 @@</span> <span class="p_context"> static void pat_bsp_init(u64 pat)</span>
 
 	wrmsrl(MSR_IA32_CR_PAT, pat);
 
<span class="p_del">-done:</span>
<span class="p_del">-	pat_init_cache_modes(pat);</span>
<span class="p_add">+	__init_cache_modes(pat);</span>
 }
 
 static void pat_ap_init(u64 pat)
 {
<span class="p_del">-	if (!pat_enabled())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!cpu_has_pat) {</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PAT)) {</span>
 		/*
 		 * If this happens we are on a secondary CPU, but switched to
 		 * PAT on the boot CPU. We have no way to undo PAT.
<span class="p_chunk">@@ -238,18 +242,32 @@</span> <span class="p_context"> static void pat_ap_init(u64 pat)</span>
 	wrmsrl(MSR_IA32_CR_PAT, pat);
 }
 
<span class="p_del">-void pat_init(void)</span>
<span class="p_add">+static void init_cache_modes(void)</span>
 {
<span class="p_del">-	u64 pat;</span>
<span class="p_del">-	struct cpuinfo_x86 *c = &amp;boot_cpu_data;</span>
<span class="p_add">+	u64 pat = 0;</span>
<span class="p_add">+	static int init_cm_done;</span>
 
<span class="p_del">-	if (!pat_enabled()) {</span>
<span class="p_add">+	if (init_cm_done)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PAT)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * CPU supports PAT. Set PAT table to be consistent with</span>
<span class="p_add">+		 * PAT MSR. This case supports &quot;nopat&quot; boot option, and</span>
<span class="p_add">+		 * virtual machine environments which support PAT without</span>
<span class="p_add">+		 * MTRRs. In specific, Xen has unique setup to PAT MSR.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If PAT MSR returns 0, it is considered invalid and emulates</span>
<span class="p_add">+		 * as No PAT.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		rdmsrl(MSR_IA32_CR_PAT, pat);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pat) {</span>
 		/*
 		 * No PAT. Emulate the PAT table that corresponds to the two
<span class="p_del">-		 * cache bits, PWT (Write Through) and PCD (Cache Disable). This</span>
<span class="p_del">-		 * setup is the same as the BIOS default setup when the system</span>
<span class="p_del">-		 * has PAT but the &quot;nopat&quot; boot option has been specified. This</span>
<span class="p_del">-		 * emulated PAT table is used when MSR_IA32_CR_PAT returns 0.</span>
<span class="p_add">+		 * cache bits, PWT (Write Through) and PCD (Cache Disable).</span>
<span class="p_add">+		 * This setup is also the same as the BIOS default setup.</span>
 		 *
 		 * PTE encoding:
 		 *
<span class="p_chunk">@@ -266,10 +284,36 @@</span> <span class="p_context"> void pat_init(void)</span>
 		 */
 		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, UC_MINUS) | PAT(3, UC) |
 		      PAT(4, WB) | PAT(5, WT) | PAT(6, UC_MINUS) | PAT(7, UC);
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	__init_cache_modes(pat);</span>
<span class="p_add">+</span>
<span class="p_add">+	init_cm_done = 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * pat_init - Initialize PAT MSR and PAT table</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function initializes PAT MSR and PAT table with an OS-defined value</span>
<span class="p_add">+ * to enable additional cache attributes, WC and WT.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function must be called on all CPUs using the specific sequence of</span>
<span class="p_add">+ * operations defined in Intel SDM. mtrr_rendezvous_handler() provides this</span>
<span class="p_add">+ * procedure for PAT.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void pat_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 pat;</span>
<span class="p_add">+	struct cpuinfo_x86 *c = &amp;boot_cpu_data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pat_enabled()) {</span>
<span class="p_add">+		init_cache_modes();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	} else if ((c-&gt;x86_vendor == X86_VENDOR_INTEL) &amp;&amp;</span>
<span class="p_del">-		   (((c-&gt;x86 == 0x6) &amp;&amp; (c-&gt;x86_model &lt;= 0xd)) ||</span>
<span class="p_del">-		    ((c-&gt;x86 == 0xf) &amp;&amp; (c-&gt;x86_model &lt;= 0x6)))) {</span>
<span class="p_add">+	if ((c-&gt;x86_vendor == X86_VENDOR_INTEL) &amp;&amp;</span>
<span class="p_add">+	    (((c-&gt;x86 == 0x6) &amp;&amp; (c-&gt;x86_model &lt;= 0xd)) ||</span>
<span class="p_add">+	     ((c-&gt;x86 == 0xf) &amp;&amp; (c-&gt;x86_model &lt;= 0x6)))) {</span>
 		/*
 		 * PAT support with the lower four entries. Intel Pentium 2,
 		 * 3, M, and 4 are affected by PAT errata, which makes the
<span class="p_chunk">@@ -734,25 +778,6 @@</span> <span class="p_context"> int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,</span>
 	if (file-&gt;f_flags &amp; O_DSYNC)
 		pcm = _PAGE_CACHE_MODE_UC_MINUS;
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * On the PPro and successors, the MTRRs are used to set</span>
<span class="p_del">-	 * memory types for physical addresses outside main memory,</span>
<span class="p_del">-	 * so blindly setting UC or PWT on those pages is wrong.</span>
<span class="p_del">-	 * For Pentiums and earlier, the surround logic should disable</span>
<span class="p_del">-	 * caching for the high addresses through the KEN pin, but</span>
<span class="p_del">-	 * we maintain the tradition of paranoia in this code.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!pat_enabled() &amp;&amp;</span>
<span class="p_del">-	    !(boot_cpu_has(X86_FEATURE_MTRR) ||</span>
<span class="p_del">-	      boot_cpu_has(X86_FEATURE_K6_MTRR) ||</span>
<span class="p_del">-	      boot_cpu_has(X86_FEATURE_CYRIX_ARR) ||</span>
<span class="p_del">-	      boot_cpu_has(X86_FEATURE_CENTAUR_MCR)) &amp;&amp;</span>
<span class="p_del">-	    (pfn &lt;&lt; PAGE_SHIFT) &gt;= __pa(high_memory)) {</span>
<span class="p_del">-		pcm = _PAGE_CACHE_MODE_UC;</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 	*vma_prot = __pgprot((pgprot_val(*vma_prot) &amp; ~_PAGE_CACHE_MASK) |
 			     cachemode2protval(pcm));
 	return 1;
<span class="p_header">diff --git a/arch/x86/oprofile/nmi_int.c b/arch/x86/oprofile/nmi_int.c</span>
<span class="p_header">index 0e07e0968c3a..28c04123b6dd 100644</span>
<span class="p_header">--- a/arch/x86/oprofile/nmi_int.c</span>
<span class="p_header">+++ b/arch/x86/oprofile/nmi_int.c</span>
<span class="p_chunk">@@ -636,7 +636,7 @@</span> <span class="p_context"> static int __init ppro_init(char **cpu_type)</span>
 	__u8 cpu_model = boot_cpu_data.x86_model;
 	struct op_x86_model_spec *spec = &amp;op_ppro_spec;	/* default */
 
<span class="p_del">-	if (force_cpu_type == arch_perfmon &amp;&amp; cpu_has_arch_perfmon)</span>
<span class="p_add">+	if (force_cpu_type == arch_perfmon &amp;&amp; boot_cpu_has(X86_FEATURE_ARCH_PERFMON))</span>
 		return 0;
 
 	/*
<span class="p_chunk">@@ -700,7 +700,7 @@</span> <span class="p_context"> int __init op_nmi_init(struct oprofile_operations *ops)</span>
 	char *cpu_type = NULL;
 	int ret = 0;
 
<span class="p_del">-	if (!cpu_has_apic)</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_APIC))</span>
 		return -ENODEV;
 
 	if (force_cpu_type == timer)
<span class="p_chunk">@@ -761,7 +761,7 @@</span> <span class="p_context"> int __init op_nmi_init(struct oprofile_operations *ops)</span>
 		if (cpu_type)
 			break;
 
<span class="p_del">-		if (!cpu_has_arch_perfmon)</span>
<span class="p_add">+		if (!boot_cpu_has(X86_FEATURE_ARCH_PERFMON))</span>
 			return -ENODEV;
 
 		/* use arch perfmon as fallback */
<span class="p_header">diff --git a/arch/x86/oprofile/op_model_ppro.c b/arch/x86/oprofile/op_model_ppro.c</span>
<span class="p_header">index d90528ea5412..350f7096baac 100644</span>
<span class="p_header">--- a/arch/x86/oprofile/op_model_ppro.c</span>
<span class="p_header">+++ b/arch/x86/oprofile/op_model_ppro.c</span>
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> static void ppro_setup_ctrs(struct op_x86_model_spec const *model,</span>
 	u64 val;
 	int i;
 
<span class="p_del">-	if (cpu_has_arch_perfmon) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_ARCH_PERFMON)) {</span>
 		union cpuid10_eax eax;
 		eax.full = cpuid_eax(0xa);
 
<span class="p_header">diff --git a/arch/x86/pci/xen.c b/arch/x86/pci/xen.c</span>
<span class="p_header">index beac4dfdade6..4bd08b0fc8ea 100644</span>
<span class="p_header">--- a/arch/x86/pci/xen.c</span>
<span class="p_header">+++ b/arch/x86/pci/xen.c</span>
<span class="p_chunk">@@ -445,7 +445,7 @@</span> <span class="p_context"> void __init xen_msi_init(void)</span>
 		uint32_t eax = cpuid_eax(xen_cpuid_base() + 4);
 
 		if (((eax &amp; XEN_HVM_CPUID_X2APIC_VIRT) &amp;&amp; x2apic_mode) ||
<span class="p_del">-		    ((eax &amp; XEN_HVM_CPUID_APIC_ACCESS_VIRT) &amp;&amp; cpu_has_apic))</span>
<span class="p_add">+		    ((eax &amp; XEN_HVM_CPUID_APIC_ACCESS_VIRT) &amp;&amp; boot_cpu_has(X86_FEATURE_APIC)))</span>
 			return;
 	}
 
<span class="p_header">diff --git a/arch/x86/power/hibernate_32.c b/arch/x86/power/hibernate_32.c</span>
<span class="p_header">index 291226b952a9..9f14bd34581d 100644</span>
<span class="p_header">--- a/arch/x86/power/hibernate_32.c</span>
<span class="p_header">+++ b/arch/x86/power/hibernate_32.c</span>
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> static int resume_physical_mapping_init(pgd_t *pgd_base)</span>
 			 * normal page tables.
 			 * NOTE: We can mark everything as executable here
 			 */
<span class="p_del">-			if (cpu_has_pse) {</span>
<span class="p_add">+			if (boot_cpu_has(X86_FEATURE_PSE)) {</span>
 				set_pmd(pmd, pfn_pmd(pfn, PAGE_KERNEL_LARGE_EXEC));
 				pfn += PTRS_PER_PTE;
 			} else {
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index 880862c7d9dd..6ab672233ac9 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -75,7 +75,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/mach_traps.h&gt;
 #include &lt;asm/mwait.h&gt;
 #include &lt;asm/pci_x86.h&gt;
<span class="p_del">-#include &lt;asm/pat.h&gt;</span>
 #include &lt;asm/cpu.h&gt;
 
 #ifdef CONFIG_ACPI
<span class="p_chunk">@@ -1093,6 +1092,26 @@</span> <span class="p_context"> static int xen_write_msr_safe(unsigned int msr, unsigned low, unsigned high)</span>
 	return ret;
 }
 
<span class="p_add">+static u64 xen_read_msr(unsigned int msr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This will silently swallow a #GP from RDMSR.  It may be worth</span>
<span class="p_add">+	 * changing that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	return xen_read_msr_safe(msr, &amp;err);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void xen_write_msr(unsigned int msr, unsigned low, unsigned high)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This will silently swallow a #GP from WRMSR.  It may be worth</span>
<span class="p_add">+	 * changing that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	xen_write_msr_safe(msr, low, high);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void xen_setup_shared_info(void)
 {
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
<span class="p_chunk">@@ -1223,8 +1242,11 @@</span> <span class="p_context"> static const struct pv_cpu_ops xen_cpu_ops __initconst = {</span>
 
 	.wbinvd = native_wbinvd,
 
<span class="p_del">-	.read_msr = xen_read_msr_safe,</span>
<span class="p_del">-	.write_msr = xen_write_msr_safe,</span>
<span class="p_add">+	.read_msr = xen_read_msr,</span>
<span class="p_add">+	.write_msr = xen_write_msr,</span>
<span class="p_add">+</span>
<span class="p_add">+	.read_msr_safe = xen_read_msr_safe,</span>
<span class="p_add">+	.write_msr_safe = xen_write_msr_safe,</span>
 
 	.read_pmc = xen_read_pmc,
 
<span class="p_chunk">@@ -1469,10 +1491,10 @@</span> <span class="p_context"> static void xen_pvh_set_cr_flags(int cpu)</span>
 	 * For BSP, PSE PGE are set in probe_page_size_mask(), for APs
 	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu__init_cpu().
 	*/
<span class="p_del">-	if (cpu_has_pse)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PSE))</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE))</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 }
 
<span class="p_chunk">@@ -1511,7 +1533,6 @@</span> <span class="p_context"> asmlinkage __visible void __init xen_start_kernel(void)</span>
 {
 	struct physdev_set_iopl set_iopl;
 	unsigned long initrd_start = 0;
<span class="p_del">-	u64 pat;</span>
 	int rc;
 
 	if (!xen_start_info)
<span class="p_chunk">@@ -1618,13 +1639,6 @@</span> <span class="p_context"> asmlinkage __visible void __init xen_start_kernel(void)</span>
 				   xen_start_info-&gt;nr_pages);
 	xen_reserve_special_pages();
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Modify the cache mode translation tables to match Xen&#39;s PAT</span>
<span class="p_del">-	 * configuration.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	rdmsrl(MSR_IA32_CR_PAT, pat);</span>
<span class="p_del">-	pat_init_cache_modes(pat);</span>
<span class="p_del">-</span>
 	/* keep using Xen gdt for now; no urgent need to change it */
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/drivers/cpufreq/longhaul.c b/drivers/cpufreq/longhaul.c</span>
<span class="p_header">index 0f6b229afcb9..247bfa8eaddb 100644</span>
<span class="p_header">--- a/drivers/cpufreq/longhaul.c</span>
<span class="p_header">+++ b/drivers/cpufreq/longhaul.c</span>
<span class="p_chunk">@@ -945,7 +945,7 @@</span> <span class="p_context"> static int __init longhaul_init(void)</span>
 	}
 #endif
 #ifdef CONFIG_X86_IO_APIC
<span class="p_del">-	if (cpu_has_apic) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC)) {</span>
 		printk(KERN_ERR PFX &quot;APIC detected. Longhaul is currently &quot;
 				&quot;broken in this configuration.\n&quot;);
 		return -ENODEV;
<span class="p_header">diff --git a/drivers/gpu/drm/drm_cache.c b/drivers/gpu/drm/drm_cache.c</span>
<span class="p_header">index 6743ff7dccfa..059f7c39c582 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/drm_cache.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/drm_cache.c</span>
<span class="p_chunk">@@ -72,7 +72,7 @@</span> <span class="p_context"> drm_clflush_pages(struct page *pages[], unsigned long num_pages)</span>
 {
 
 #if defined(CONFIG_X86)
<span class="p_del">-	if (cpu_has_clflush) {</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_CLFLUSH)) {</span>
 		drm_cache_flush_clflush(pages, num_pages);
 		return;
 	}
<span class="p_chunk">@@ -105,7 +105,7 @@</span> <span class="p_context"> void</span>
 drm_clflush_sg(struct sg_table *st)
 {
 #if defined(CONFIG_X86)
<span class="p_del">-	if (cpu_has_clflush) {</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_CLFLUSH)) {</span>
 		struct sg_page_iter sg_iter;
 
 		mb();
<span class="p_chunk">@@ -129,7 +129,7 @@</span> <span class="p_context"> void</span>
 drm_clflush_virt_range(void *addr, unsigned long length)
 {
 #if defined(CONFIG_X86)
<span class="p_del">-	if (cpu_has_clflush) {</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_CLFLUSH)) {</span>
 		const int size = boot_cpu_data.x86_clflush_size;
 		void *end = addr + length;
 		addr = (void *)(((unsigned long)addr) &amp; -size);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index dabc08987b5e..f2cb9a9539ee 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -1732,7 +1732,7 @@</span> <span class="p_context"> i915_gem_mmap_ioctl(struct drm_device *dev, void *data,</span>
 	if (args-&gt;flags &amp; ~(I915_MMAP_WC))
 		return -EINVAL;
 
<span class="p_del">-	if (args-&gt;flags &amp; I915_MMAP_WC &amp;&amp; !cpu_has_pat)</span>
<span class="p_add">+	if (args-&gt;flags &amp; I915_MMAP_WC &amp;&amp; !boot_cpu_has(X86_FEATURE_PAT))</span>
 		return -ENODEV;
 
 	obj = drm_gem_object_lookup(dev, file, args-&gt;handle);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_header">index 1328bc5021b4..b845f468dd74 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_chunk">@@ -488,7 +488,7 @@</span> <span class="p_context"> i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,</span>
 		ret = relocate_entry_cpu(obj, reloc, target_offset);
 	else if (obj-&gt;map_and_fenceable)
 		ret = relocate_entry_gtt(obj, reloc, target_offset);
<span class="p_del">-	else if (cpu_has_clflush)</span>
<span class="p_add">+	else if (static_cpu_has(X86_FEATURE_CLFLUSH))</span>
 		ret = relocate_entry_clflush(obj, reloc, target_offset);
 	else {
 		WARN_ONCE(1, &quot;Impossible case in relocation handling\n&quot;);
<span class="p_header">diff --git a/drivers/input/joystick/analog.c b/drivers/input/joystick/analog.c</span>
<span class="p_header">index 6f8b084e13d0..3d8ff09eba57 100644</span>
<span class="p_header">--- a/drivers/input/joystick/analog.c</span>
<span class="p_header">+++ b/drivers/input/joystick/analog.c</span>
<span class="p_chunk">@@ -143,9 +143,9 @@</span> <span class="p_context"> struct analog_port {</span>
 
 #include &lt;linux/i8253.h&gt;
 
<span class="p_del">-#define GET_TIME(x)	do { if (cpu_has_tsc) x = (unsigned int)rdtsc(); else x = get_time_pit(); } while (0)</span>
<span class="p_del">-#define DELTA(x,y)	(cpu_has_tsc ? ((y) - (x)) : ((x) - (y) + ((x) &lt; (y) ? PIT_TICK_RATE / HZ : 0)))</span>
<span class="p_del">-#define TIME_NAME	(cpu_has_tsc?&quot;TSC&quot;:&quot;PIT&quot;)</span>
<span class="p_add">+#define GET_TIME(x)	do { if (boot_cpu_has(X86_FEATURE_TSC)) x = (unsigned int)rdtsc(); else x = get_time_pit(); } while (0)</span>
<span class="p_add">+#define DELTA(x,y)	(boot_cpu_has(X86_FEATURE_TSC) ? ((y) - (x)) : ((x) - (y) + ((x) &lt; (y) ? PIT_TICK_RATE / HZ : 0)))</span>
<span class="p_add">+#define TIME_NAME	(boot_cpu_has(X86_FEATURE_TSC)?&quot;TSC&quot;:&quot;PIT&quot;)</span>
 static unsigned int get_time_pit(void)
 {
         unsigned long flags;
<span class="p_header">diff --git a/drivers/iommu/irq_remapping.c b/drivers/iommu/irq_remapping.c</span>
<span class="p_header">index 8adaaeae3268..49721b4e1975 100644</span>
<span class="p_header">--- a/drivers/iommu/irq_remapping.c</span>
<span class="p_header">+++ b/drivers/iommu/irq_remapping.c</span>
<span class="p_chunk">@@ -36,7 +36,7 @@</span> <span class="p_context"> static void irq_remapping_disable_io_apic(void)</span>
 	 * As this gets called during crash dump, keep this simple for
 	 * now.
 	 */
<span class="p_del">-	if (cpu_has_apic || apic_from_smp_config())</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_APIC) || apic_from_smp_config())</span>
 		disconnect_bsp_APIC(0);
 }
 
<span class="p_header">diff --git a/drivers/lguest/x86/core.c b/drivers/lguest/x86/core.c</span>
<span class="p_header">index adc162c7040d..6e9042e3d2a9 100644</span>
<span class="p_header">--- a/drivers/lguest/x86/core.c</span>
<span class="p_header">+++ b/drivers/lguest/x86/core.c</span>
<span class="p_chunk">@@ -603,7 +603,7 @@</span> <span class="p_context"> void __init lguest_arch_host_init(void)</span>
 	 * doing this.
 	 */
 	get_online_cpus();
<span class="p_del">-	if (cpu_has_pge) { /* We have a broader idea of &quot;global&quot;. */</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) { /* We have a broader idea of &quot;global&quot;. */</span>
 		/* Remember that this was originally set (for cleanup). */
 		cpu_had_pge = 1;
 		/*
<span class="p_header">diff --git a/drivers/net/hamradio/baycom_epp.c b/drivers/net/hamradio/baycom_epp.c</span>
<span class="p_header">index 72c9f1f352b4..7c7830722ea2 100644</span>
<span class="p_header">--- a/drivers/net/hamradio/baycom_epp.c</span>
<span class="p_header">+++ b/drivers/net/hamradio/baycom_epp.c</span>
<span class="p_chunk">@@ -635,10 +635,10 @@</span> <span class="p_context"> static int receive(struct net_device *dev, int cnt)</span>
 
 #ifdef __i386__
 #include &lt;asm/msr.h&gt;
<span class="p_del">-#define GETTICK(x)                                                \</span>
<span class="p_del">-({                                                                \</span>
<span class="p_del">-	if (cpu_has_tsc)                                          \</span>
<span class="p_del">-		x = (unsigned int)rdtsc();		  \</span>
<span class="p_add">+#define GETTICK(x)						\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_TSC))			\</span>
<span class="p_add">+		x = (unsigned int)rdtsc();			\</span>
 })
 #else /* __i386__ */
 #define GETTICK(x)
<span class="p_header">diff --git a/drivers/staging/unisys/visorbus/visorchipset.c b/drivers/staging/unisys/visorbus/visorchipset.c</span>
<span class="p_header">index 5fbda7b218c7..9cf4f8463c4e 100644</span>
<span class="p_header">--- a/drivers/staging/unisys/visorbus/visorchipset.c</span>
<span class="p_header">+++ b/drivers/staging/unisys/visorbus/visorchipset.c</span>
<span class="p_chunk">@@ -2425,7 +2425,7 @@</span> <span class="p_context"> static __init uint32_t visorutil_spar_detect(void)</span>
 {
 	unsigned int eax, ebx, ecx, edx;
 
<span class="p_del">-	if (cpu_has_hypervisor) {</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {</span>
 		/* check the ID */
 		cpuid(UNISYS_SPAR_LEAF_ID, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);
 		return  (ebx == UNISYS_SPAR_ID_EBX) &amp;&amp;
<span class="p_header">diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">index 339125bb4d2c..6a67ab94b553 100644</span>
<span class="p_header">--- a/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">+++ b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_chunk">@@ -245,7 +245,9 @@</span> <span class="p_context"></span>
 
 #define INIT_TASK_DATA(align)						\
 	. = ALIGN(align);						\
<span class="p_del">-	*(.data..init_task)</span>
<span class="p_add">+	VMLINUX_SYMBOL(__start_init_task) = .;				\</span>
<span class="p_add">+	*(.data..init_task)						\</span>
<span class="p_add">+	VMLINUX_SYMBOL(__end_init_task) = .;</span>
 
 /*
  * Read only Data
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index b47ebd170690..c73425de3cfe 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"> TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_sysc</span>
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault test_syscall_vdso unwind_vdso \
 			test_FCMOV test_FCOMI test_FISTTP \
 			vdso_restorer
<span class="p_add">+TARGETS_C_64BIT_ONLY := fsgsbase</span>
 
 TARGETS_C_32BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_32BIT_ONLY)
 TARGETS_C_64BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_64BIT_ONLY)
<span class="p_header">diff --git a/tools/testing/selftests/x86/fsgsbase.c b/tools/testing/selftests/x86/fsgsbase.c</span>
new file mode 100644
<span class="p_header">index 000000000000..5b2b4b3c634c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/fsgsbase.c</span>
<span class="p_chunk">@@ -0,0 +1,398 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * fsgsbase.c, an fsgsbase test</span>
<span class="p_add">+ * Copyright (c) 2014-2016 Andy Lutomirski</span>
<span class="p_add">+ * GPL v2</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;stdbool.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;sys/syscall.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;err.h&gt;</span>
<span class="p_add">+#include &lt;sys/user.h&gt;</span>
<span class="p_add">+#include &lt;asm/prctl.h&gt;</span>
<span class="p_add">+#include &lt;sys/prctl.h&gt;</span>
<span class="p_add">+#include &lt;signal.h&gt;</span>
<span class="p_add">+#include &lt;limits.h&gt;</span>
<span class="p_add">+#include &lt;sys/ucontext.h&gt;</span>
<span class="p_add">+#include &lt;sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/futex.h&gt;</span>
<span class="p_add">+#include &lt;pthread.h&gt;</span>
<span class="p_add">+#include &lt;asm/ldt.h&gt;</span>
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __x86_64__</span>
<span class="p_add">+# error This test is 64-bit only</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static volatile sig_atomic_t want_segv;</span>
<span class="p_add">+static volatile unsigned long segv_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+static int nerrs;</span>
<span class="p_add">+</span>
<span class="p_add">+static void sethandler(int sig, void (*handler)(int, siginfo_t *, void *),</span>
<span class="p_add">+		       int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct sigaction sa;</span>
<span class="p_add">+	memset(&amp;sa, 0, sizeof(sa));</span>
<span class="p_add">+	sa.sa_sigaction = handler;</span>
<span class="p_add">+	sa.sa_flags = SA_SIGINFO | flags;</span>
<span class="p_add">+	sigemptyset(&amp;sa.sa_mask);</span>
<span class="p_add">+	if (sigaction(sig, &amp;sa, 0))</span>
<span class="p_add">+		err(1, &quot;sigaction&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void clearhandler(int sig)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct sigaction sa;</span>
<span class="p_add">+	memset(&amp;sa, 0, sizeof(sa));</span>
<span class="p_add">+	sa.sa_handler = SIG_DFL;</span>
<span class="p_add">+	sigemptyset(&amp;sa.sa_mask);</span>
<span class="p_add">+	if (sigaction(sig, &amp;sa, 0))</span>
<span class="p_add">+		err(1, &quot;sigaction&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void sigsegv(int sig, siginfo_t *si, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ucontext_t *ctx = (ucontext_t*)ctx_void;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!want_segv) {</span>
<span class="p_add">+		clearhandler(SIGSEGV);</span>
<span class="p_add">+		return;  /* Crash cleanly. */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	want_segv = false;</span>
<span class="p_add">+	segv_addr = (unsigned long)si-&gt;si_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	ctx-&gt;uc_mcontext.gregs[REG_RIP] += 4;	/* Skip the faulting mov */</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+enum which_base { FS, GS };</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long read_base(enum which_base which)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long offset;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Unless we have FSGSBASE, there&#39;s no direct way to do this from</span>
<span class="p_add">+	 * user mode.  We can get at it indirectly using signals, though.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	want_segv = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	offset = 0;</span>
<span class="p_add">+	if (which == FS) {</span>
<span class="p_add">+		/* Use a constant-length instruction here. */</span>
<span class="p_add">+		asm volatile (&quot;mov %%fs:(%%rcx), %%rax&quot; : : &quot;c&quot; (offset) : &quot;rax&quot;);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		asm volatile (&quot;mov %%gs:(%%rcx), %%rax&quot; : : &quot;c&quot; (offset) : &quot;rax&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!want_segv)</span>
<span class="p_add">+		return segv_addr + offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If that didn&#39;t segfault, try the other end of the address space.</span>
<span class="p_add">+	 * Unless we get really unlucky and run into the vsyscall page, this</span>
<span class="p_add">+	 * is guaranteed to segfault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	offset = (ULONG_MAX &gt;&gt; 1) + 1;</span>
<span class="p_add">+	if (which == FS) {</span>
<span class="p_add">+		asm volatile (&quot;mov %%fs:(%%rcx), %%rax&quot;</span>
<span class="p_add">+			      : : &quot;c&quot; (offset) : &quot;rax&quot;);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		asm volatile (&quot;mov %%gs:(%%rcx), %%rax&quot;</span>
<span class="p_add">+			      : : &quot;c&quot; (offset) : &quot;rax&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!want_segv)</span>
<span class="p_add">+		return segv_addr + offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	abort();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void check_gs_value(unsigned long value)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long base;</span>
<span class="p_add">+	unsigned short sel;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tARCH_SET_GS to 0x%lx\n&quot;, value);</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, value) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (&quot;mov %%gs, %0&quot; : &quot;=rm&quot; (sel));</span>
<span class="p_add">+	base = read_base(GS);</span>
<span class="p_add">+	if (base == value) {</span>
<span class="p_add">+		printf(&quot;[OK]\tGSBASE was set as expected (selector 0x%hx)\n&quot;,</span>
<span class="p_add">+		       sel);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tGSBASE was not as expected: got 0x%lx (selector 0x%hx)\n&quot;,</span>
<span class="p_add">+		       base, sel);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_GET_GS, &amp;base) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_GET_GS&quot;);</span>
<span class="p_add">+	if (base == value) {</span>
<span class="p_add">+		printf(&quot;[OK]\tARCH_GET_GS worked as expected (selector 0x%hx)\n&quot;,</span>
<span class="p_add">+		       sel);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tARCH_GET_GS was not as expected: got 0x%lx (selector 0x%hx)\n&quot;,</span>
<span class="p_add">+		       base, sel);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mov_0_gs(unsigned long initial_base, bool schedule)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long base, arch_base;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tARCH_SET_GS to 0x%lx then mov 0 to %%gs%s\n&quot;, initial_base, schedule ? &quot; and schedule &quot; : &quot;&quot;);</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, initial_base) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (schedule)</span>
<span class="p_add">+		usleep(10);</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; (0));</span>
<span class="p_add">+	base = read_base(GS);</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_GET_GS, &amp;arch_base) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_GET_GS&quot;);</span>
<span class="p_add">+	if (base == arch_base) {</span>
<span class="p_add">+		printf(&quot;[OK]\tGSBASE is 0x%lx\n&quot;, base);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tGSBASE changed to 0x%lx but kernel reports 0x%lx\n&quot;, base, arch_base);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static volatile unsigned long remote_base;</span>
<span class="p_add">+static volatile bool remote_hard_zero;</span>
<span class="p_add">+static volatile unsigned int ftx;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ARCH_SET_FS/GS(0) may or may not program a selector of zero.  HARD_ZERO</span>
<span class="p_add">+ * means to force the selector to zero to improve test coverage.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define HARD_ZERO 0xa1fa5f343cb85fa4</span>
<span class="p_add">+</span>
<span class="p_add">+static void do_remote_base()</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long to_set = remote_base;</span>
<span class="p_add">+	bool hard_zero = false;</span>
<span class="p_add">+	if (to_set == HARD_ZERO) {</span>
<span class="p_add">+		to_set = 0;</span>
<span class="p_add">+		hard_zero = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, to_set) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hard_zero)</span>
<span class="p_add">+		asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)0));</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned short sel;</span>
<span class="p_add">+	asm volatile (&quot;mov %%gs, %0&quot; : &quot;=rm&quot; (sel));</span>
<span class="p_add">+	printf(&quot;\tother thread: ARCH_SET_GS(0x%lx)%s -- sel is 0x%hx\n&quot;,</span>
<span class="p_add">+	       to_set, hard_zero ? &quot; and clear gs&quot; : &quot;&quot;, sel);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void do_unexpected_base(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The goal here is to try to arrange for GS == 0, GSBASE !=</span>
<span class="p_add">+	 * 0, and for the the kernel the think that GSBASE == 0.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * To make the test as reliable as possible, this uses</span>
<span class="p_add">+	 * explicit descriptorss.  (This is not the only way.  This</span>
<span class="p_add">+	 * could use ARCH_SET_GS with a low, nonzero base, but the</span>
<span class="p_add">+	 * relevant side effect of ARCH_SET_GS could change.)</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Step 1: tell the kernel that we have GSBASE == 0. */</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, 0) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Step 2: change GSBASE without telling the kernel. */</span>
<span class="p_add">+	struct user_desc desc = {</span>
<span class="p_add">+		.entry_number    = 0,</span>
<span class="p_add">+		.base_addr       = 0xBAADF00D,</span>
<span class="p_add">+		.limit           = 0xfffff,</span>
<span class="p_add">+		.seg_32bit       = 1,</span>
<span class="p_add">+		.contents        = 0, /* Data, grow-up */</span>
<span class="p_add">+		.read_exec_only  = 0,</span>
<span class="p_add">+		.limit_in_pages  = 1,</span>
<span class="p_add">+		.seg_not_present = 0,</span>
<span class="p_add">+		.useable         = 0</span>
<span class="p_add">+	};</span>
<span class="p_add">+	if (syscall(SYS_modify_ldt, 1, &amp;desc, sizeof(desc)) == 0) {</span>
<span class="p_add">+		printf(&quot;\tother thread: using LDT slot 0\n&quot;);</span>
<span class="p_add">+		asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)0x7));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* No modify_ldt for us (configured out, perhaps) */</span>
<span class="p_add">+</span>
<span class="p_add">+		struct user_desc *low_desc = mmap(</span>
<span class="p_add">+			NULL, sizeof(desc),</span>
<span class="p_add">+			PROT_READ | PROT_WRITE,</span>
<span class="p_add">+			MAP_PRIVATE | MAP_ANONYMOUS | MAP_32BIT, -1, 0);</span>
<span class="p_add">+		memcpy(low_desc, &amp;desc, sizeof(desc));</span>
<span class="p_add">+</span>
<span class="p_add">+		low_desc-&gt;entry_number = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* 32-bit set_thread_area */</span>
<span class="p_add">+		long ret;</span>
<span class="p_add">+		asm volatile (&quot;int $0x80&quot;</span>
<span class="p_add">+			      : &quot;=a&quot; (ret) : &quot;a&quot; (243), &quot;b&quot; (low_desc)</span>
<span class="p_add">+			      : &quot;flags&quot;);</span>
<span class="p_add">+		memcpy(&amp;desc, low_desc, sizeof(desc));</span>
<span class="p_add">+		munmap(low_desc, sizeof(desc));</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret != 0) {</span>
<span class="p_add">+			printf(&quot;[NOTE]\tcould not create a segment -- test won&#39;t do anything\n&quot;);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		printf(&quot;\tother thread: using GDT slot %d\n&quot;, desc.entry_number);</span>
<span class="p_add">+		asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)((desc.entry_number &lt;&lt; 3) | 0x3)));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Step 3: set the selector back to zero.  On AMD chips, this will</span>
<span class="p_add">+	 * preserve GSBASE.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *threadproc(void *ctx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		while (ftx == 0)</span>
<span class="p_add">+			syscall(SYS_futex, &amp;ftx, FUTEX_WAIT, 0, NULL, NULL, 0);</span>
<span class="p_add">+		if (ftx == 3)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ftx == 1)</span>
<span class="p_add">+			do_remote_base();</span>
<span class="p_add">+		else if (ftx == 2)</span>
<span class="p_add">+			do_unexpected_base();</span>
<span class="p_add">+		else</span>
<span class="p_add">+			errx(1, &quot;helper thread got bad command&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+		ftx = 0;</span>
<span class="p_add">+		syscall(SYS_futex, &amp;ftx, FUTEX_WAKE, 0, NULL, NULL, 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_gs_and_switch_to(unsigned long local, unsigned long remote)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long base;</span>
<span class="p_add">+</span>
<span class="p_add">+	bool hard_zero = false;</span>
<span class="p_add">+	if (local == HARD_ZERO) {</span>
<span class="p_add">+		hard_zero = true;</span>
<span class="p_add">+		local = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tARCH_SET_GS(0x%lx)%s, then schedule to 0x%lx\n&quot;,</span>
<span class="p_add">+	       local, hard_zero ? &quot; and clear gs&quot; : &quot;&quot;, remote);</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, local) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+	if (hard_zero)</span>
<span class="p_add">+		asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)0));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (read_base(GS) != local) {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tGSBASE wasn&#39;t set as expected\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	remote_base = remote;</span>
<span class="p_add">+	ftx = 1;</span>
<span class="p_add">+	syscall(SYS_futex, &amp;ftx, FUTEX_WAKE, 0, NULL, NULL, 0);</span>
<span class="p_add">+	while (ftx != 0)</span>
<span class="p_add">+		syscall(SYS_futex, &amp;ftx, FUTEX_WAIT, 1, NULL, NULL, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	base = read_base(GS);</span>
<span class="p_add">+	if (base == local) {</span>
<span class="p_add">+		printf(&quot;[OK]\tGSBASE remained 0x%lx\n&quot;, local);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tGSBASE changed to 0x%lx\n&quot;, base);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void test_unexpected_base(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long base;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tARCH_SET_GS(0), clear gs, then manipulate GSBASE in a different thread\n&quot;);</span>
<span class="p_add">+	if (syscall(SYS_arch_prctl, ARCH_SET_GS, 0) != 0)</span>
<span class="p_add">+		err(1, &quot;ARCH_SET_GS&quot;);</span>
<span class="p_add">+	asm volatile (&quot;mov %0, %%gs&quot; : : &quot;rm&quot; ((unsigned short)0));</span>
<span class="p_add">+</span>
<span class="p_add">+	ftx = 2;</span>
<span class="p_add">+	syscall(SYS_futex, &amp;ftx, FUTEX_WAKE, 0, NULL, NULL, 0);</span>
<span class="p_add">+	while (ftx != 0)</span>
<span class="p_add">+		syscall(SYS_futex, &amp;ftx, FUTEX_WAIT, 1, NULL, NULL, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	base = read_base(GS);</span>
<span class="p_add">+	if (base == 0) {</span>
<span class="p_add">+		printf(&quot;[OK]\tGSBASE remained 0\n&quot;);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tGSBASE changed to 0x%lx\n&quot;, base);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int main()</span>
<span class="p_add">+{</span>
<span class="p_add">+	pthread_t thread;</span>
<span class="p_add">+</span>
<span class="p_add">+	sethandler(SIGSEGV, sigsegv, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	check_gs_value(0);</span>
<span class="p_add">+	check_gs_value(1);</span>
<span class="p_add">+	check_gs_value(0x200000000);</span>
<span class="p_add">+	check_gs_value(0);</span>
<span class="p_add">+	check_gs_value(0x200000000);</span>
<span class="p_add">+	check_gs_value(1);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (int sched = 0; sched &lt; 2; sched++) {</span>
<span class="p_add">+		mov_0_gs(0, !!sched);</span>
<span class="p_add">+		mov_0_gs(1, !!sched);</span>
<span class="p_add">+		mov_0_gs(0x200000000, !!sched);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Set up for multithreading. */</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu_set_t cpuset;</span>
<span class="p_add">+	CPU_ZERO(&amp;cpuset);</span>
<span class="p_add">+	CPU_SET(0, &amp;cpuset);</span>
<span class="p_add">+	if (sched_setaffinity(0, sizeof(cpuset), &amp;cpuset) != 0)</span>
<span class="p_add">+		err(1, &quot;sched_setaffinity to CPU 0&quot;);	/* should never fail */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pthread_create(&amp;thread, 0, threadproc, 0) != 0)</span>
<span class="p_add">+		err(1, &quot;pthread_create&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	static unsigned long bases_with_hard_zero[] = {</span>
<span class="p_add">+		0, HARD_ZERO, 1, 0x200000000,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	for (int local = 0; local &lt; 4; local++) {</span>
<span class="p_add">+		for (int remote = 0; remote &lt; 4; remote++) {</span>
<span class="p_add">+			set_gs_and_switch_to(bases_with_hard_zero[local],</span>
<span class="p_add">+					     bases_with_hard_zero[remote]);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	test_unexpected_base();</span>
<span class="p_add">+</span>
<span class="p_add">+	ftx = 3;  /* Kill the thread. */</span>
<span class="p_add">+	syscall(SYS_futex, &amp;ftx, FUTEX_WAKE, 0, NULL, NULL, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pthread_join(thread, NULL) != 0)</span>
<span class="p_add">+		err(1, &quot;pthread_join&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs == 0 ? 0 : 1;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/ldt_gdt.c b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">index 31a3035cd4eb..4af47079cf04 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_chunk">@@ -21,6 +21,9 @@</span> <span class="p_context"></span>
 #include &lt;pthread.h&gt;
 #include &lt;sched.h&gt;
 #include &lt;linux/futex.h&gt;
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+#include &lt;asm/prctl.h&gt;</span>
<span class="p_add">+#include &lt;sys/prctl.h&gt;</span>
 
 #define AR_ACCESSED		(1&lt;&lt;8)
 
<span class="p_chunk">@@ -44,6 +47,12 @@</span> <span class="p_context"></span>
 
 static int nerrs;
 
<span class="p_add">+/* Points to an array of 1024 ints, each holding its own index. */</span>
<span class="p_add">+static const unsigned int *counter_page;</span>
<span class="p_add">+static struct user_desc *low_user_desc;</span>
<span class="p_add">+static struct user_desc *low_user_desc_clear;  /* Use to delete GDT entry */</span>
<span class="p_add">+static int gdt_entry_num;</span>
<span class="p_add">+</span>
 static void check_invalid_segment(uint16_t index, int ldt)
 {
 	uint32_t has_limit = 0, has_ar = 0, limit, ar;
<span class="p_chunk">@@ -561,16 +570,257 @@</span> <span class="p_context"> static void do_exec_test(void)</span>
 	}
 }
 
<span class="p_add">+static void setup_counter_page(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int *page = mmap(NULL, 4096, PROT_READ | PROT_WRITE,</span>
<span class="p_add">+			 MAP_ANONYMOUS | MAP_PRIVATE | MAP_32BIT, -1, 0);</span>
<span class="p_add">+	if (page == MAP_FAILED)</span>
<span class="p_add">+		err(1, &quot;mmap&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (int i = 0; i &lt; 1024; i++)</span>
<span class="p_add">+		page[i] = i;</span>
<span class="p_add">+	counter_page = page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int invoke_set_thread_area(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+	asm volatile (&quot;int $0x80&quot;</span>
<span class="p_add">+		      : &quot;=a&quot; (ret), &quot;+m&quot; (low_user_desc) :</span>
<span class="p_add">+			&quot;a&quot; (243), &quot;b&quot; (low_user_desc)</span>
<span class="p_add">+		      : &quot;flags&quot;);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void setup_low_user_desc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	low_user_desc = mmap(NULL, 2 * sizeof(struct user_desc),</span>
<span class="p_add">+			     PROT_READ | PROT_WRITE,</span>
<span class="p_add">+			     MAP_ANONYMOUS | MAP_PRIVATE | MAP_32BIT, -1, 0);</span>
<span class="p_add">+	if (low_user_desc == MAP_FAILED)</span>
<span class="p_add">+		err(1, &quot;mmap&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	low_user_desc-&gt;entry_number	= -1;</span>
<span class="p_add">+	low_user_desc-&gt;base_addr	= (unsigned long)&amp;counter_page[1];</span>
<span class="p_add">+	low_user_desc-&gt;limit		= 0xfffff;</span>
<span class="p_add">+	low_user_desc-&gt;seg_32bit	= 1;</span>
<span class="p_add">+	low_user_desc-&gt;contents		= 0; /* Data, grow-up*/</span>
<span class="p_add">+	low_user_desc-&gt;read_exec_only	= 0;</span>
<span class="p_add">+	low_user_desc-&gt;limit_in_pages	= 1;</span>
<span class="p_add">+	low_user_desc-&gt;seg_not_present	= 0;</span>
<span class="p_add">+	low_user_desc-&gt;useable		= 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (invoke_set_thread_area() == 0) {</span>
<span class="p_add">+		gdt_entry_num = low_user_desc-&gt;entry_number;</span>
<span class="p_add">+		printf(&quot;[NOTE]\tset_thread_area is available; will use GDT index %d\n&quot;, gdt_entry_num);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[NOTE]\tset_thread_area is unavailable\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	low_user_desc_clear = low_user_desc + 1;</span>
<span class="p_add">+	low_user_desc_clear-&gt;entry_number = gdt_entry_num;</span>
<span class="p_add">+	low_user_desc_clear-&gt;read_exec_only = 1;</span>
<span class="p_add">+	low_user_desc_clear-&gt;seg_not_present = 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void test_gdt_invalidation(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!gdt_entry_num)</span>
<span class="p_add">+		return;	/* 64-bit only system -- we can&#39;t use set_thread_area */</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned short prev_sel;</span>
<span class="p_add">+	unsigned short sel;</span>
<span class="p_add">+	unsigned int eax;</span>
<span class="p_add">+	const char *result;</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	unsigned long saved_base;</span>
<span class="p_add">+	unsigned long new_base;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Test DS */</span>
<span class="p_add">+	invoke_set_thread_area();</span>
<span class="p_add">+	eax = 243;</span>
<span class="p_add">+	sel = (gdt_entry_num &lt;&lt; 3) | 3;</span>
<span class="p_add">+	asm volatile (&quot;movw %%ds, %[prev_sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[sel], %%ds\n\t&quot;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;pushl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movl %[arg1], %%ebx\n\t&quot;</span>
<span class="p_add">+		      &quot;int $0x80\n\t&quot;	/* Should invalidate ds */</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;popl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movw %%ds, %[sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[prev_sel], %%ds&quot;</span>
<span class="p_add">+		      : [prev_sel] &quot;=&amp;r&quot; (prev_sel), [sel] &quot;+r&quot; (sel),</span>
<span class="p_add">+			&quot;+a&quot; (eax)</span>
<span class="p_add">+		      : &quot;m&quot; (low_user_desc_clear),</span>
<span class="p_add">+			[arg1] &quot;r&quot; ((unsigned int)(unsigned long)low_user_desc_clear)</span>
<span class="p_add">+		      : &quot;flags&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sel != 0) {</span>
<span class="p_add">+		result = &quot;FAIL&quot;;</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		result = &quot;OK&quot;;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	printf(&quot;[%s]\tInvalidate DS with set_thread_area: new DS = 0x%hx\n&quot;,</span>
<span class="p_add">+	       result, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Test ES */</span>
<span class="p_add">+	invoke_set_thread_area();</span>
<span class="p_add">+	eax = 243;</span>
<span class="p_add">+	sel = (gdt_entry_num &lt;&lt; 3) | 3;</span>
<span class="p_add">+	asm volatile (&quot;movw %%es, %[prev_sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[sel], %%es\n\t&quot;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;pushl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movl %[arg1], %%ebx\n\t&quot;</span>
<span class="p_add">+		      &quot;int $0x80\n\t&quot;	/* Should invalidate es */</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;popl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movw %%es, %[sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[prev_sel], %%es&quot;</span>
<span class="p_add">+		      : [prev_sel] &quot;=&amp;r&quot; (prev_sel), [sel] &quot;+r&quot; (sel),</span>
<span class="p_add">+			&quot;+a&quot; (eax)</span>
<span class="p_add">+		      : &quot;m&quot; (low_user_desc_clear),</span>
<span class="p_add">+			[arg1] &quot;r&quot; ((unsigned int)(unsigned long)low_user_desc_clear)</span>
<span class="p_add">+		      : &quot;flags&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sel != 0) {</span>
<span class="p_add">+		result = &quot;FAIL&quot;;</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		result = &quot;OK&quot;;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	printf(&quot;[%s]\tInvalidate ES with set_thread_area: new ES = 0x%hx\n&quot;,</span>
<span class="p_add">+	       result, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Test FS */</span>
<span class="p_add">+	invoke_set_thread_area();</span>
<span class="p_add">+	eax = 243;</span>
<span class="p_add">+	sel = (gdt_entry_num &lt;&lt; 3) | 3;</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	syscall(SYS_arch_prctl, ARCH_GET_FS, &amp;saved_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	asm volatile (&quot;movw %%fs, %[prev_sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[sel], %%fs\n\t&quot;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;pushl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movl %[arg1], %%ebx\n\t&quot;</span>
<span class="p_add">+		      &quot;int $0x80\n\t&quot;	/* Should invalidate fs */</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;popl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movw %%fs, %[sel]\n\t&quot;</span>
<span class="p_add">+		      : [prev_sel] &quot;=&amp;r&quot; (prev_sel), [sel] &quot;+r&quot; (sel),</span>
<span class="p_add">+			&quot;+a&quot; (eax)</span>
<span class="p_add">+		      : &quot;m&quot; (low_user_desc_clear),</span>
<span class="p_add">+			[arg1] &quot;r&quot; ((unsigned int)(unsigned long)low_user_desc_clear)</span>
<span class="p_add">+		      : &quot;flags&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	syscall(SYS_arch_prctl, ARCH_GET_FS, &amp;new_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Restore FS/BASE for glibc */</span>
<span class="p_add">+	asm volatile (&quot;movw %[prev_sel], %%fs&quot; : : [prev_sel] &quot;rm&quot; (prev_sel));</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	if (saved_base)</span>
<span class="p_add">+		syscall(SYS_arch_prctl, ARCH_SET_FS, saved_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sel != 0) {</span>
<span class="p_add">+		result = &quot;FAIL&quot;;</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		result = &quot;OK&quot;;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	printf(&quot;[%s]\tInvalidate FS with set_thread_area: new FS = 0x%hx\n&quot;,</span>
<span class="p_add">+	       result, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	if (sel == 0 &amp;&amp; new_base != 0) {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tNew FSBASE was 0x%lx\n&quot;, new_base);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\tNew FSBASE was zero\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Test GS */</span>
<span class="p_add">+	invoke_set_thread_area();</span>
<span class="p_add">+	eax = 243;</span>
<span class="p_add">+	sel = (gdt_entry_num &lt;&lt; 3) | 3;</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	syscall(SYS_arch_prctl, ARCH_GET_GS, &amp;saved_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	asm volatile (&quot;movw %%gs, %[prev_sel]\n\t&quot;</span>
<span class="p_add">+		      &quot;movw %[sel], %%gs\n\t&quot;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;pushl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movl %[arg1], %%ebx\n\t&quot;</span>
<span class="p_add">+		      &quot;int $0x80\n\t&quot;	/* Should invalidate gs */</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		      &quot;popl %%ebx\n\t&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		      &quot;movw %%gs, %[sel]\n\t&quot;</span>
<span class="p_add">+		      : [prev_sel] &quot;=&amp;r&quot; (prev_sel), [sel] &quot;+r&quot; (sel),</span>
<span class="p_add">+			&quot;+a&quot; (eax)</span>
<span class="p_add">+		      : &quot;m&quot; (low_user_desc_clear),</span>
<span class="p_add">+			[arg1] &quot;r&quot; ((unsigned int)(unsigned long)low_user_desc_clear)</span>
<span class="p_add">+		      : &quot;flags&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	syscall(SYS_arch_prctl, ARCH_GET_GS, &amp;new_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Restore GS/BASE for glibc */</span>
<span class="p_add">+	asm volatile (&quot;movw %[prev_sel], %%gs&quot; : : [prev_sel] &quot;rm&quot; (prev_sel));</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	if (saved_base)</span>
<span class="p_add">+		syscall(SYS_arch_prctl, ARCH_SET_GS, saved_base);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sel != 0) {</span>
<span class="p_add">+		result = &quot;FAIL&quot;;</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		result = &quot;OK&quot;;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	printf(&quot;[%s]\tInvalidate GS with set_thread_area: new GS = 0x%hx\n&quot;,</span>
<span class="p_add">+	       result, sel);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	if (sel == 0 &amp;&amp; new_base != 0) {</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		printf(&quot;[FAIL]\tNew GSBASE was 0x%lx\n&quot;, new_base);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\tNew GSBASE was zero\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int main(int argc, char **argv)
 {
 	if (argc == 1 &amp;&amp; !strcmp(argv[0], &quot;ldt_gdt_test_exec&quot;))
 		return finish_exec_test();
 
<span class="p_add">+	setup_counter_page();</span>
<span class="p_add">+	setup_low_user_desc();</span>
<span class="p_add">+</span>
 	do_simple_tests();
 
 	do_multicpu_tests();
 
 	do_exec_test();
 
<span class="p_add">+	test_gdt_invalidation();</span>
<span class="p_add">+</span>
 	return nerrs ? 1 : 0;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



