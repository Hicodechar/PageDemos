
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,1/1] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,1/1] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 21, 2016, 12:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160921143534.0dd95fe7@thinkpad&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9343445/mbox/"
   >mbox</a>
|
   <a href="/patch/9343445/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9343445/">/patch/9343445/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C7459607D4 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 12:35:50 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B59612A626
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 12:35:50 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A552C2A633; Wed, 21 Sep 2016 12:35:50 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2E3992A626
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 12:35:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757138AbcIUMfq (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 21 Sep 2016 08:35:46 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:43648 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1754806AbcIUMfn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 21 Sep 2016 08:35:43 -0400
Received: from pps.filterd (m0098393.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.17/8.16.0.17) with SMTP id
	u8LCTIcw141692
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 21 Sep 2016 08:35:42 -0400
Received: from e06smtp15.uk.ibm.com (e06smtp15.uk.ibm.com [195.75.94.111])
	by mx0a-001b2d01.pphosted.com with ESMTP id 25k7bkbt4s-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 21 Sep 2016 08:35:42 -0400
Received: from localhost
	by e06smtp15.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;gerald.schaefer@de.ibm.com&gt;; 
	Wed, 21 Sep 2016 13:35:40 +0100
Received: from d06dlp01.portsmouth.uk.ibm.com (9.149.20.13)
	by e06smtp15.uk.ibm.com (192.168.101.145) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Wed, 21 Sep 2016 13:35:37 +0100
Received: from b06cxnps4075.portsmouth.uk.ibm.com
	(d06relay12.portsmouth.uk.ibm.com [9.149.109.197])
	by d06dlp01.portsmouth.uk.ibm.com (Postfix) with ESMTP id
	D0C5217D8067 for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 21 Sep 2016 13:37:35 +0100 (BST)
Received: from d06av06.portsmouth.uk.ibm.com (d06av06.portsmouth.uk.ibm.com
	[9.149.37.217])
	by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id u8LCZaQi50987224
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 21 Sep 2016 12:35:36 GMT
Received: from d06av06.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av06.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with
	ESMTP id u8LCZZeA024245
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 21 Sep 2016 08:35:36 -0400
Received: from thinkpad (dyn-9-152-212-72.boeblingen.de.ibm.com
	[9.152.212.72])
	by d06av06.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with
	ESMTP id u8LCZYwd024237; Wed, 21 Sep 2016 08:35:35 -0400
Date: Wed, 21 Sep 2016 14:35:34 +0200
From: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;, &lt;linux-mm@kvack.org&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	&quot;Aneesh Kumar K . V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt;,
	Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;
Subject: [PATCH v2 1/1] mm/hugetlb: fix memory offline with hugepage size &gt;
	memory block size
In-Reply-To: &lt;05d701d213d1$7fb70880$7f251980$@alibaba-inc.com&gt;
References: &lt;20160920155354.54403-1-gerald.schaefer@de.ibm.com&gt;
	&lt;20160920155354.54403-2-gerald.schaefer@de.ibm.com&gt;
	&lt;05d701d213d1$7fb70880$7f251980$@alibaba-inc.com&gt;
Organization: IBM Deutschland Research &amp; Development GmbH / Vorsitzende des
	Aufsichtsrats: Martina Koederitz / Geschaeftsfuehrung: Dirk
	Wittkopp / Sitz
	der Gesellschaft: Boeblingen / Registergericht: Amtsgericht Stuttgart,
	HRB 243294
X-Mailer: Claws Mail 3.9.0 (GTK+ 2.24.23; x86_64-redhat-linux-gnu)
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 16092112-0020-0000-0000-0000024F9120
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 16092112-0021-0000-0000-00003E464E1D
Message-Id: &lt;20160921143534.0dd95fe7@thinkpad&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2016-09-21_08:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1609020000
	definitions=main-1609210228
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 21, 2016, 12:35 p.m.</div>
<pre class="content">
dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a
list corruption and addressing exception when trying to set a memory
block offline that is part (but not the first part) of a hugetlb page
with a size &gt; memory block size.

When no other smaller hugetlb page sizes are present, the VM_BUG_ON()
will trigger directly. In the other case we will run into an addressing
exception later, because dissolve_free_huge_page() will not work on the
head page of the compound hugetlb page which will result in a NULL
hstate from page_hstate().

To fix this, first remove the VM_BUG_ON() because it is wrong, and then
use the compound head page in dissolve_free_huge_page().

Also change locking in dissolve_free_huge_page(), so that it only takes
the lock when actually removing a hugepage.
<span class="signed-off-by">
Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>
---
Changes in v2:
- Update comment in dissolve_free_huge_pages()
- Change locking in dissolve_free_huge_page()

 mm/hugetlb.c | 31 +++++++++++++++++++------------
 1 file changed, 19 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=158981">Rui Teng</a> - Sept. 21, 2016, 1:17 p.m.</div>
<pre class="content">
On 9/21/16 8:35 PM, Gerald Schaefer wrote:
<span class="quote">&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt; hstate from page_hstate().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Changes in v2:</span>
<span class="quote">&gt; - Update comment in dissolve_free_huge_pages()</span>
<span class="quote">&gt; - Change locking in dissolve_free_huge_page()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  mm/hugetlb.c | 31 +++++++++++++++++++------------</span>
<span class="quote">&gt;  1 file changed, 19 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 87e11d8..1522af8 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1441,23 +1441,30 @@ static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt; +	struct hstate *h;</span>
<span class="quote">&gt; +	int nid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (page_count(head))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	h = page_hstate(head);</span>
<span class="quote">&gt; +	nid = page_to_nid(head);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Dissolve free hugepages in a given pfn range. Used by memory hotplug to</span>
<span class="quote">&gt;   * make specified memory blocks removable from the system.</span>
<span class="quote">&gt; - * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="quote">&gt; + * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="quote">&gt; + * part of it lies within the given range.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1466,9 +1473,9 @@ void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
<span class="quote">&gt;  	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="quote">&gt; -		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; +		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="quote">&gt; +			dissolve_free_huge_page(pfn_to_page(pfn));</span>
How many times will dissolve_free_huge_page() be invoked in this loop?
For each pfn, it will be converted to the head page, and then the list
will be deleted repeatedly.
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 21, 2016, 3:13 p.m.</div>
<pre class="content">
On Wed, 21 Sep 2016 21:17:29 +0800
Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt; wrote:
<span class="quote">
&gt; &gt;  /*</span>
<span class="quote">&gt; &gt;   * Dissolve free hugepages in a given pfn range. Used by memory hotplug to</span>
<span class="quote">&gt; &gt;   * make specified memory blocks removable from the system.</span>
<span class="quote">&gt; &gt; - * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="quote">&gt; &gt; + * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="quote">&gt; &gt; + * part of it lies within the given range.</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; @@ -1466,9 +1473,9 @@ void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt; &gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt; &gt;  		return;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
<span class="quote">&gt; &gt;  	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="quote">&gt; &gt; -		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; &gt; +		if (pfn_to_page(pfn)))</span>
<span class="quote">&gt; &gt; +			pfn_to_page(pfn));</span>
<span class="quote">&gt; How many times will dissolve_free_huge_page() be invoked in this loop?</span>
<span class="quote">&gt; For each pfn, it will be converted to the head page, and then the list</span>
<span class="quote">&gt; will be deleted repeatedly.</span>

In the case where the memory block [start_pfn, end_pfn] is part of a
gigantic hugepage, dissolve_free_huge_page() will only be invoked once.

If there is only one gigantic hugepage pool, 1 &lt;&lt; minimum_order will be
larger than the memory block size, and the loop will stop after the first
invocation of dissolve_free_huge_page().

If there are additional hugepage pools, with hugepage sizes &lt; memory
block size, then it will loop as many times as 1 &lt;&lt; minimum_order fits
inside a memory block, e.g. 256 times with 1 MB minimum hugepage size
and 256 MB memory block size.

However, the PageHuge() check should always return false after the first
invocation of dissolve_free_huge_page(), since update_and_free_page()
will take care of resetting compound_dtor, and so there will also be
just one invocation.

The only case where there will be more than one invocation is the case
where we do not have any part of a gigantic hugepage inside the memory
block, but rather multiple &quot;normal sized&quot; hugepages. Then there will be
one invocation per hugepage, as opposed to one invocation per
&quot;1 &lt;&lt; minimum_order&quot; range as it was before the patch. So it also
improves the behaviour in the case where there is no gigantic page
involved.
<span class="quote">
&gt; &gt;  }</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Sept. 22, 2016, 7:58 a.m.</div>
<pre class="content">
<span class="quote">&gt; </span>
<span class="quote">&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt; hstate from page_hstate().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="acked-by">Acked-by: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 87e11d8..1522af8 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1441,23 +1441,30 @@</span> <span class="p_context"> static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
  */
 static void dissolve_free_huge_page(struct page *page)
 {
<span class="p_add">+	struct page *head = compound_head(page);</span>
<span class="p_add">+	struct hstate *h;</span>
<span class="p_add">+	int nid;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (page_count(head))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	h = page_hstate(head);</span>
<span class="p_add">+	nid = page_to_nid(head);</span>
<span class="p_add">+</span>
 	spin_lock(&amp;hugetlb_lock);
<span class="p_del">-	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="p_del">-		struct hstate *h = page_hstate(page);</span>
<span class="p_del">-		int nid = page_to_nid(page);</span>
<span class="p_del">-		list_del(&amp;page-&gt;lru);</span>
<span class="p_del">-		h-&gt;free_huge_pages--;</span>
<span class="p_del">-		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="p_del">-		h-&gt;max_huge_pages--;</span>
<span class="p_del">-		update_and_free_page(h, page);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	list_del(&amp;head-&gt;lru);</span>
<span class="p_add">+	h-&gt;free_huge_pages--;</span>
<span class="p_add">+	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="p_add">+	h-&gt;max_huge_pages--;</span>
<span class="p_add">+	update_and_free_page(h, head);</span>
 	spin_unlock(&amp;hugetlb_lock);
 }
 
 /*
  * Dissolve free hugepages in a given pfn range. Used by memory hotplug to
  * make specified memory blocks removable from the system.
<span class="p_del">- * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="p_add">+ * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="p_add">+ * part of it lies within the given range.</span>
  */
 void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)
 {
<span class="p_chunk">@@ -1466,9 +1473,9 @@</span> <span class="p_context"> void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 	if (!hugepages_supported())
 		return;
 
<span class="p_del">-	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
 	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)
<span class="p_del">-		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="p_add">+		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="p_add">+			dissolve_free_huge_page(pfn_to_page(pfn));</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



