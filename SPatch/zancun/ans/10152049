
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 3.16.53 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 3.16.53</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 9, 2018, 1:28 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180109132841.GB2748@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10152049/mbox/"
   >mbox</a>
|
   <a href="/patch/10152049/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10152049/">/patch/10152049/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B540860223 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jan 2018 13:29:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 82B1E2858C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jan 2018 13:29:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 74DA9289A8; Tue,  9 Jan 2018 13:29:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E8A312858C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jan 2018 13:29:05 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757462AbeAIN3D (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 9 Jan 2018 08:29:03 -0500
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:35516 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1753369AbeAIN2z (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 9 Jan 2018 08:28:55 -0500
Received: from ben by shadbolt.decadent.org.uk with local (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1eYtxJ-0002PT-Tq; Tue, 09 Jan 2018 13:28:49 +0000
Date: Tue, 9 Jan 2018 13:28:41 +0000
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, Jiri Slaby &lt;jslaby@suse.cz&gt;,
	stable@vger.kernel.org
Cc: lwn@lwn.net
Message-ID: &lt;20180109132841.GB2748@decadent.org.uk&gt;
MIME-Version: 1.0
Content-Type: multipart/signed; micalg=pgp-sha512;
	protocol=&quot;application/pgp-signature&quot;; boundary=&quot;YToU2i3Vx8H2dn7O&quot;
Content-Disposition: inline
X-Mailer: LinuxStableQueue (scripts by bwh)
User-Agent: Mutt/1.5.23 (2014-03-12)
X-SA-Exim-Connect-IP: &lt;locally generated&gt;
X-SA-Exim-Mail-From: ben@decadent.org.uk
Subject: Linux 3.16.53
X-SA-Exim-Version: 4.2.1 (built Mon, 26 Dec 2011 16:24:06 +0000)
X-SA-Exim-Scanned: Yes (on shadbolt.decadent.org.uk)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - Jan. 9, 2018, 1:28 p.m.</div>
<pre class="content">
I&#39;m announcing the release of the 3.16.53 kernel.

All users of the 3.16 kernel series should upgrade.

The updated 3.16.y git tree can be found at:
        https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git linux-3.16.y
and can be browsed at the normal kernel.org git web browser:
        https://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git

The diff from 3.16.52 is attached to this message.

Ben.

------------

 Documentation/kernel-parameters.txt         |  12 +
 Documentation/x86/tlb.txt                   |  75 +++++
 Makefile                                    |   2 +-
 arch/arm/include/asm/mmu_context.h          |   2 +
 arch/x86/boot/compressed/misc.h             |   1 +
 arch/x86/ia32/ia32entry.S                   |   7 +
 arch/x86/include/asm/alternative-asm.h      |  43 ++-
 arch/x86/include/asm/alternative.h          |  65 ++--
 arch/x86/include/asm/cmdline.h              |   2 +
 arch/x86/include/asm/cpufeature.h           |  34 +-
 arch/x86/include/asm/desc.h                 |   2 +-
 arch/x86/include/asm/hardirq.h              |   6 +-
 arch/x86/include/asm/hw_irq.h               |   2 +-
 arch/x86/include/asm/kaiser.h               | 141 +++++++++
 arch/x86/include/asm/mmu.h                  |   6 -
 arch/x86/include/asm/mmu_context.h          |  83 +----
 arch/x86/include/asm/pgtable.h              |  28 +-
 arch/x86/include/asm/pgtable_64.h           |  25 +-
 arch/x86/include/asm/pgtable_types.h        |  29 +-
 arch/x86/include/asm/processor.h            |  38 +--
 arch/x86/include/asm/smap.h                 |   4 +-
 arch/x86/include/asm/tlbflush.h             | 261 ++++++++++------
 arch/x86/include/asm/virtext.h              |   3 +-
 arch/x86/include/asm/vsyscall.h             |   2 +
 arch/x86/include/uapi/asm/processor-flags.h |   3 +-
 arch/x86/kernel/alternative.c               | 158 ++++++++--
 arch/x86/kernel/cpu/amd.c                   |   7 -
 arch/x86/kernel/cpu/bugs.c                  |   8 +
 arch/x86/kernel/cpu/common.c                | 113 +++++--
 arch/x86/kernel/cpu/intel.c                 |  26 --
 arch/x86/kernel/cpu/mcheck/mce.c            |   3 +-
 arch/x86/kernel/cpu/mcheck/p5.c             |   3 +-
 arch/x86/kernel/cpu/mcheck/winchip.c        |   3 +-
 arch/x86/kernel/cpu/perf_event.c            |   7 +-
 arch/x86/kernel/cpu/perf_event_intel_ds.c   |  56 +++-
 arch/x86/kernel/entry_32.S                  |   2 +-
 arch/x86/kernel/entry_64.S                  | 190 +++++++++--
 arch/x86/kernel/espfix_64.c                 |  10 +
 arch/x86/kernel/head_64.S                   |  35 ++-
 arch/x86/kernel/i387.c                      |   3 +-
 arch/x86/kernel/irq.c                       |   3 +-
 arch/x86/kernel/irqinit.c                   |   2 +-
 arch/x86/kernel/kvmclock.c                  |   5 +
 arch/x86/kernel/ldt.c                       |  27 +-
 arch/x86/kernel/paravirt_patch_64.c         |   2 -
 arch/x86/kernel/process.c                   |   7 +-
 arch/x86/kernel/process_64.c                |   2 +-
 arch/x86/kernel/reboot.c                    |   4 +
 arch/x86/kernel/setup.c                     |   7 +
 arch/x86/kernel/tracepoint.c                |   2 +
 arch/x86/kernel/vm86_32.c                   |   2 +-
 arch/x86/kernel/vsyscall_64.c               |  12 +-
 arch/x86/kernel/xsave.c                     |   3 +-
 arch/x86/kvm/vmx.c                          |   4 +-
 arch/x86/kvm/x86.c                          |   3 +-
 arch/x86/lib/clear_page_64.S                |   4 +-
 arch/x86/lib/cmdline.c                      | 165 +++++++++-
 arch/x86/lib/copy_page_64.S                 |   2 +-
 arch/x86/lib/copy_user_64.S                 |  15 +-
 arch/x86/lib/memcpy_64.S                    |   8 +-
 arch/x86/lib/memmove_64.S                   |   2 +-
 arch/x86/lib/memset_64.S                    |   8 +-
 arch/x86/mm/Makefile                        |   4 +-
 arch/x86/mm/init.c                          |   6 +-
 arch/x86/mm/init_64.c                       |  10 +
 arch/x86/mm/kaiser.c                        | 469 ++++++++++++++++++++++++++++
 arch/x86/mm/pageattr.c                      |  63 +++-
 arch/x86/mm/pgtable.c                       |  27 +-
 arch/x86/mm/tlb.c                           | 255 +++++++++------
 arch/x86/xen/enlighten.c                    |  10 +-
 drivers/lguest/x86/core.c                   |   5 +-
 drivers/vhost/vhost.c                       |   1 +
 include/asm-generic/vmlinux.lds.h           |   7 +
 include/linux/kaiser.h                      |  52 +++
 include/linux/mmu_context.h                 |   7 +
 include/linux/mmzone.h                      |   3 +-
 include/linux/percpu-defs.h                 |  32 +-
 init/main.c                                 |   2 +
 kernel/fork.c                               |   6 +
 kernel/sched/core.c                         |   4 +-
 mm/mmu_context.c                            |   2 +-
 mm/vmstat.c                                 |   1 +
 security/Kconfig                            |  10 +
 83 files changed, 2180 insertions(+), 585 deletions(-)

Aaron Lu (1):
      x86/irq: Do not substract irq_tlb_count from irq_call_count

Andy Lutomirski (18):
      x86: Clean up cr4 manipulation
      x86/mm: Add INVPCID helpers
      x86/mm: Add a &#39;noinvpcid&#39; boot option to turn off INVPCID
      x86/mm: If INVPCID is available, use it to flush global mappings
      sched/core: Add switch_mm_irqs_off() and use it in the scheduler
      x86/mm: Build arch/x86/mm/tlb.c even on !SMP
      x86/mm, sched/core: Uninline switch_mm()
      x86/mm, sched/core: Turn off IRQs in switch_mm()
      sched/core: Idle_task_exit() shouldn&#39;t use switch_mm_irqs_off()
      x86/vm86/32: Switch to flush_tlb_mm_range() in mark_screen_rdonly()
      x86/mm: Remove flush_tlb() and flush_tlb_current_task()
      x86/mm: Make flush_tlb_mm_range() more predictable
      x86/mm: Reimplement flush_tlb_page() using flush_tlb_mm_range()
      x86/mm: Remove the UP asm/tlbflush.h code, always use the (formerly) SMP code
      x86/mm: Disable PCID on 32-bit kernels
      x86/mm: Add the &#39;nopcid&#39; boot option to turn off PCID
      x86/mm: Enable CR4.PCIDE on supported systems
      x86/mm/64: Fix reboot interaction with CR4.PCIDE

Ben Hutchings (3):
      drivers/vhost: Fix mmu_context.h assumption
      x86: kvmclock: Disable use from vDSO if KPTI is enabled
      Linux 3.16.53

Borislav Petkov (10):
      x86/mm: Fix INVPCID asm constraint
      kaiser: Set _PAGE_USER of the vsyscall page
      x86/alternatives: Cleanup DPRINTK macro
      x86/alternatives: Add instruction padding
      x86/alternatives: Make JMPs more robust
      x86/alternatives: Use optimized NOPs for padding
      x86/kaiser: Rename and simplify X86_FEATURE_KAISER handling
      x86/kaiser: Check boottime cmdline params
      x86/kaiser: Reenable PARAVIRT
      x86/kaiser: Move feature detection up

Dave Hansen (9):
      x86/mm: Clean up the TLB flushing code
      x86/mm: Rip out complicated, out-of-date, buggy TLB flushing
      x86/mm: Fix missed global TLB flush stat
      x86/mm: New tunable for single vs full TLB flush
      x86/mm: Set TLB flush tunable to sane value (33)
      x86/boot: Fix early command-line parsing when matching at end
      x86/boot: Fix early command-line parsing when partial word matches
      x86/boot: Simplify early command line parsing
      x86/boot: Pass in size to early cmdline parsing

Hugh Dickins (5):
      kaiser: alloc_ldt_struct() use get_zeroed_page()
      kaiser: add &quot;nokaiser&quot; boot option, using ALTERNATIVE
      kaiser: use ALTERNATIVE instead of x86_cr3_pcid_noflush
      kaiser: asm/tlbflush.h handle noPGE at lower level
      kaiser: kaiser_flush_tlb_on_return_to_user() check PCID

Ingo Molnar (1):
      mm/mmu_context, sched/core: Fix mmu_context.h assumption

Jeremiah Mahler (1):
      x86/mm: Fix sparse &#39;tlb_single_page_flush_ceiling&#39; warning and make the variable read-mostly

Jiri Kosina (2):
      kaiser: disabled on Xen PV
      kaiser: x86: Fix NMI handling

Kees Cook (2):
      KPTI: Rename to PAGE_TABLE_ISOLATION
      KPTI: Report when enabled

Richard Fellner (1):
      KAISER: Kernel Address Isolation

Steven Rostedt (1):
      ARM: Hide finish_arch_post_lock_switch() from modules

Thomas Gleixner (1):
      x86/paravirt: Dont patch flush_tlb_single

Tom Lendacky (1):
      x86/boot: Add early cmdline parsing for options with arguments
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index 86c91be8647c..a0fd7c8052a1 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2225,8 +2225,12 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 
 	nointroute	[IA-64]
 
<span class="p_add">+	noinvpcid	[X86] Disable the INVPCID cpu feature.</span>
<span class="p_add">+</span>
 	nojitter	[IA-64] Disables jitter checking for ITC timers.
 
<span class="p_add">+	nopti		[X86-64] Disable KAISER isolation of kernel from user.</span>
<span class="p_add">+</span>
 	no-kvmclock	[X86,KVM] Disable paravirtualized KVM clock driver
 
 	no-kvmapf	[X86,KVM] Disable paravirtualized asynchronous page
<span class="p_chunk">@@ -2259,6 +2263,8 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	nopat		[X86] Disable PAT (page attribute table extension of
 			pagetables) support.
 
<span class="p_add">+	nopcid		[X86-64] Disable the PCID cpu feature.</span>
<span class="p_add">+</span>
 	norandmaps	Don&#39;t use address space randomization.  Equivalent to
 			echo 0 &gt; /proc/sys/kernel/randomize_va_space
 
<span class="p_chunk">@@ -2746,6 +2752,12 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_add">+	pti=		[X86_64]</span>
<span class="p_add">+			Control KAISER user/kernel address space isolation:</span>
<span class="p_add">+			on - enable</span>
<span class="p_add">+			off - disable</span>
<span class="p_add">+			auto - default setting</span>
<span class="p_add">+</span>
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
 			default number.
<span class="p_header">diff --git a/Documentation/x86/tlb.txt b/Documentation/x86/tlb.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..2b3a82e69151</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/x86/tlb.txt</span>
<span class="p_chunk">@@ -0,0 +1,75 @@</span> <span class="p_context"></span>
<span class="p_add">+When the kernel unmaps or modified the attributes of a range of</span>
<span class="p_add">+memory, it has two choices:</span>
<span class="p_add">+ 1. Flush the entire TLB with a two-instruction sequence.  This is</span>
<span class="p_add">+    a quick operation, but it causes collateral damage: TLB entries</span>
<span class="p_add">+    from areas other than the one we are trying to flush will be</span>
<span class="p_add">+    destroyed and must be refilled later, at some cost.</span>
<span class="p_add">+ 2. Use the invlpg instruction to invalidate a single page at a</span>
<span class="p_add">+    time.  This could potentialy cost many more instructions, but</span>
<span class="p_add">+    it is a much more precise operation, causing no collateral</span>
<span class="p_add">+    damage to other TLB entries.</span>
<span class="p_add">+</span>
<span class="p_add">+Which method to do depends on a few things:</span>
<span class="p_add">+ 1. The size of the flush being performed.  A flush of the entire</span>
<span class="p_add">+    address space is obviously better performed by flushing the</span>
<span class="p_add">+    entire TLB than doing 2^48/PAGE_SIZE individual flushes.</span>
<span class="p_add">+ 2. The contents of the TLB.  If the TLB is empty, then there will</span>
<span class="p_add">+    be no collateral damage caused by doing the global flush, and</span>
<span class="p_add">+    all of the individual flush will have ended up being wasted</span>
<span class="p_add">+    work.</span>
<span class="p_add">+ 3. The size of the TLB.  The larger the TLB, the more collateral</span>
<span class="p_add">+    damage we do with a full flush.  So, the larger the TLB, the</span>
<span class="p_add">+    more attrative an individual flush looks.  Data and</span>
<span class="p_add">+    instructions have separate TLBs, as do different page sizes.</span>
<span class="p_add">+ 4. The microarchitecture.  The TLB has become a multi-level</span>
<span class="p_add">+    cache on modern CPUs, and the global flushes have become more</span>
<span class="p_add">+    expensive relative to single-page flushes.</span>
<span class="p_add">+</span>
<span class="p_add">+There is obviously no way the kernel can know all these things,</span>
<span class="p_add">+especially the contents of the TLB during a given flush.  The</span>
<span class="p_add">+sizes of the flush will vary greatly depending on the workload as</span>
<span class="p_add">+well.  There is essentially no &quot;right&quot; point to choose.</span>
<span class="p_add">+</span>
<span class="p_add">+You may be doing too many individual invalidations if you see the</span>
<span class="p_add">+invlpg instruction (or instructions _near_ it) show up high in</span>
<span class="p_add">+profiles.  If you believe that individual invalidations being</span>
<span class="p_add">+called too often, you can lower the tunable:</span>
<span class="p_add">+</span>
<span class="p_add">+	/sys/debug/kernel/x86/tlb_single_page_flush_ceiling</span>
<span class="p_add">+</span>
<span class="p_add">+This will cause us to do the global flush for more cases.</span>
<span class="p_add">+Lowering it to 0 will disable the use of the individual flushes.</span>
<span class="p_add">+Setting it to 1 is a very conservative setting and it should</span>
<span class="p_add">+never need to be 0 under normal circumstances.</span>
<span class="p_add">+</span>
<span class="p_add">+Despite the fact that a single individual flush on x86 is</span>
<span class="p_add">+guaranteed to flush a full 2MB [1], hugetlbfs always uses the full</span>
<span class="p_add">+flushes.  THP is treated exactly the same as normal memory.</span>
<span class="p_add">+</span>
<span class="p_add">+You might see invlpg inside of flush_tlb_mm_range() show up in</span>
<span class="p_add">+profiles, or you can use the trace_tlb_flush() tracepoints. to</span>
<span class="p_add">+determine how long the flush operations are taking.</span>
<span class="p_add">+</span>
<span class="p_add">+Essentially, you are balancing the cycles you spend doing invlpg</span>
<span class="p_add">+with the cycles that you spend refilling the TLB later.</span>
<span class="p_add">+</span>
<span class="p_add">+You can measure how expensive TLB refills are by using</span>
<span class="p_add">+performance counters and &#39;perf stat&#39;, like this:</span>
<span class="p_add">+</span>
<span class="p_add">+perf stat -e</span>
<span class="p_add">+	cpu/event=0x8,umask=0x84,name=dtlb_load_misses_walk_duration/,</span>
<span class="p_add">+	cpu/event=0x8,umask=0x82,name=dtlb_load_misses_walk_completed/,</span>
<span class="p_add">+	cpu/event=0x49,umask=0x4,name=dtlb_store_misses_walk_duration/,</span>
<span class="p_add">+	cpu/event=0x49,umask=0x2,name=dtlb_store_misses_walk_completed/,</span>
<span class="p_add">+	cpu/event=0x85,umask=0x4,name=itlb_misses_walk_duration/,</span>
<span class="p_add">+	cpu/event=0x85,umask=0x2,name=itlb_misses_walk_completed/</span>
<span class="p_add">+</span>
<span class="p_add">+That works on an IvyBridge-era CPU (i5-3320M).  Different CPUs</span>
<span class="p_add">+may have differently-named counters, but they should at least</span>
<span class="p_add">+be there in some form.  You can use pmu-tools &#39;ocperf list&#39;</span>
<span class="p_add">+(https://github.com/andikleen/pmu-tools) to find the right</span>
<span class="p_add">+counters for a given CPU.</span>
<span class="p_add">+</span>
<span class="p_add">+1. A footnote in Intel&#39;s SDM &quot;4.10.4.2 Recommended Invalidation&quot;</span>
<span class="p_add">+   says: &quot;One execution of INVLPG is sufficient even for a page</span>
<span class="p_add">+   with size greater than 4 KBytes.&quot;</span>
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 5cf074eb43ed..4386eb2679ae 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 3
 PATCHLEVEL = 16
<span class="p_del">-SUBLEVEL = 52</span>
<span class="p_add">+SUBLEVEL = 53</span>
 EXTRAVERSION =
 NAME = Museum of Fishiegoodies
 
<span class="p_header">diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h</span>
<span class="p_header">index 9b32f76bb0dd..10f662498eb7 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"> static inline void check_and_switch_context(struct mm_struct *mm,</span>
 		cpu_switch_mm(mm-&gt;pgd, mm);
 }
 
<span class="p_add">+#ifndef MODULE</span>
 #define finish_arch_post_lock_switch \
 	finish_arch_post_lock_switch
 static inline void finish_arch_post_lock_switch(void)
<span class="p_chunk">@@ -82,6 +83,7 @@</span> <span class="p_context"> static inline void finish_arch_post_lock_switch(void)</span>
 		preempt_enable_no_resched();
 	}
 }
<span class="p_add">+#endif /* !MODULE */</span>
 
 #endif	/* CONFIG_MMU */
 
<span class="p_header">diff --git a/arch/x86/boot/compressed/misc.h b/arch/x86/boot/compressed/misc.h</span>
<span class="p_header">index 124312be129b..d527b9b12dd4 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/misc.h</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/misc.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
  */
 #undef CONFIG_PARAVIRT
 #undef CONFIG_PARAVIRT_SPINLOCKS
<span class="p_add">+#undef CONFIG_PAGE_TABLE_ISOLATION</span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/screen_info.h&gt;
<span class="p_header">diff --git a/arch/x86/ia32/ia32entry.S b/arch/x86/ia32/ia32entry.S</span>
<span class="p_header">index b74ac9c5710b..88ceb07b5a2b 100644</span>
<span class="p_header">--- a/arch/x86/ia32/ia32entry.S</span>
<span class="p_header">+++ b/arch/x86/ia32/ia32entry.S</span>
<span class="p_chunk">@@ -15,6 +15,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/irqflags.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/pgtable_types.h&gt;</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/err.h&gt;
 
<span class="p_chunk">@@ -121,6 +123,7 @@</span> <span class="p_context"> ENTRY(ia32_sysenter_target)</span>
 	CFI_DEF_CFA	rsp,0
 	CFI_REGISTER	rsp,rbp
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	movq	PER_CPU_VAR(kernel_stack), %rsp
 	addq	$(KERNEL_STACK_OFFSET),%rsp
 	/*
<span class="p_chunk">@@ -192,6 +195,7 @@</span> <span class="p_context"> ENTRY(ia32_sysenter_target)</span>
 	popq_cfi %rcx				/* User %esp */
 	CFI_REGISTER rsp,rcx
 	TRACE_IRQS_ON
<span class="p_add">+	SWITCH_USER_CR3</span>
 	ENABLE_INTERRUPTS_SYSEXIT32
 
 	CFI_RESTORE_STATE
<span class="p_chunk">@@ -296,6 +300,7 @@</span> <span class="p_context"> ENTRY(ia32_cstar_target)</span>
 	CFI_REGISTER	rip,rcx
 	/*CFI_REGISTER	rflags,r11*/
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	movl	%esp,%r8d
 	CFI_REGISTER	rsp,r8
 	movq	PER_CPU_VAR(kernel_stack),%rsp
<span class="p_chunk">@@ -350,6 +355,7 @@</span> <span class="p_context"> ENTRY(ia32_cstar_target)</span>
 	xorq	%r9,%r9
 	xorq	%r8,%r8
 	TRACE_IRQS_ON
<span class="p_add">+	SWITCH_USER_CR3</span>
 	movl RSP-ARGOFFSET(%rsp),%esp
 	CFI_RESTORE rsp
 	USERGS_SYSRET32
<span class="p_chunk">@@ -424,6 +430,7 @@</span> <span class="p_context"> ENTRY(ia32_syscall)</span>
 	PARAVIRT_ADJUST_EXCEPTION_FRAME
 	ASM_CLAC			/* Do this early to minimize exposure */
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	/*
 	 * No need to follow this irqs on/off section: the syscall
 	 * disabled irqs and here we enable it straight after entry:
<span class="p_header">diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_header">index 372231c22a47..524bddce0b76 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative-asm.h</span>
<span class="p_chunk">@@ -18,12 +18,53 @@</span> <span class="p_context"></span>
 	.endm
 #endif
 
<span class="p_del">-.macro altinstruction_entry orig alt feature orig_len alt_len</span>
<span class="p_add">+.macro altinstruction_entry orig alt feature orig_len alt_len pad_len</span>
 	.long \orig - .
 	.long \alt - .
 	.word \feature
 	.byte \orig_len
 	.byte \alt_len
<span class="p_add">+	.byte \pad_len</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro ALTERNATIVE oldinstr, newinstr, feature</span>
<span class="p_add">+140:</span>
<span class="p_add">+	\oldinstr</span>
<span class="p_add">+141:</span>
<span class="p_add">+	.skip -(((144f-143f)-(141b-140b)) &gt; 0) * ((144f-143f)-(141b-140b)),0x90</span>
<span class="p_add">+142:</span>
<span class="p_add">+</span>
<span class="p_add">+	.pushsection .altinstructions,&quot;a&quot;</span>
<span class="p_add">+	altinstruction_entry 140b,143f,\feature,142b-140b,144f-143f,142b-141b</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+</span>
<span class="p_add">+	.pushsection .altinstr_replacement,&quot;ax&quot;</span>
<span class="p_add">+143:</span>
<span class="p_add">+	\newinstr</span>
<span class="p_add">+144:</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro ALTERNATIVE_2 oldinstr, newinstr1, feature1, newinstr2, feature2</span>
<span class="p_add">+140:</span>
<span class="p_add">+	\oldinstr</span>
<span class="p_add">+141:</span>
<span class="p_add">+	.skip -(((144f-143f)-(141b-140b)) &gt; 0) * ((144f-143f)-(141b-140b)),0x90</span>
<span class="p_add">+	.skip -(((145f-144f)-(144f-143f)-(141b-140b)) &gt; 0) * ((145f-144f)-(144f-143f)-(141b-140b)),0x90</span>
<span class="p_add">+142:</span>
<span class="p_add">+</span>
<span class="p_add">+	.pushsection .altinstructions,&quot;a&quot;</span>
<span class="p_add">+	altinstruction_entry 140b,143f,\feature1,142b-140b,144f-143f,142b-141b</span>
<span class="p_add">+	altinstruction_entry 140b,144f,\feature2,142b-140b,145f-144f,142b-141b</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+</span>
<span class="p_add">+	.pushsection .altinstr_replacement,&quot;ax&quot;</span>
<span class="p_add">+143:</span>
<span class="p_add">+	\newinstr1</span>
<span class="p_add">+144:</span>
<span class="p_add">+	\newinstr2</span>
<span class="p_add">+145:</span>
<span class="p_add">+	.popsection</span>
 .endm
 
 #endif  /*  __ASSEMBLY__  */
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index 0a3f9c9f98d5..34310c03708a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -48,8 +48,9 @@</span> <span class="p_context"> struct alt_instr {</span>
 	s32 repl_offset;	/* offset to replacement instruction */
 	u16 cpuid;		/* cpuid bit set for replacement */
 	u8  instrlen;		/* length of original instruction */
<span class="p_del">-	u8  replacementlen;	/* length of new instruction, &lt;= instrlen */</span>
<span class="p_del">-};</span>
<span class="p_add">+	u8  replacementlen;	/* length of new instruction */</span>
<span class="p_add">+	u8  padlen;		/* length of build-time padding */</span>
<span class="p_add">+} __packed;</span>
 
 extern void alternative_instructions(void);
 extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);
<span class="p_chunk">@@ -76,50 +77,61 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 }
 #endif	/* CONFIG_SMP */
 
<span class="p_del">-#define OLDINSTR(oldinstr)	&quot;661:\n\t&quot; oldinstr &quot;\n662:\n&quot;</span>
<span class="p_add">+#define b_replacement(num)	&quot;664&quot;#num</span>
<span class="p_add">+#define e_replacement(num)	&quot;665&quot;#num</span>
 
<span class="p_del">-#define b_replacement(number)	&quot;663&quot;#number</span>
<span class="p_del">-#define e_replacement(number)	&quot;664&quot;#number</span>
<span class="p_add">+#define alt_end_marker		&quot;663&quot;</span>
<span class="p_add">+#define alt_slen		&quot;662b-661b&quot;</span>
<span class="p_add">+#define alt_pad_len		alt_end_marker&quot;b-662b&quot;</span>
<span class="p_add">+#define alt_total_slen		alt_end_marker&quot;b-661b&quot;</span>
<span class="p_add">+#define alt_rlen(num)		e_replacement(num)&quot;f-&quot;b_replacement(num)&quot;f&quot;</span>
 
<span class="p_del">-#define alt_slen &quot;662b-661b&quot;</span>
<span class="p_del">-#define alt_rlen(number) e_replacement(number)&quot;f-&quot;b_replacement(number)&quot;f&quot;</span>
<span class="p_add">+#define __OLDINSTR(oldinstr, num)					\</span>
<span class="p_add">+	&quot;661:\n\t&quot; oldinstr &quot;\n662:\n&quot;					\</span>
<span class="p_add">+	&quot;.skip -(((&quot; alt_rlen(num) &quot;)-(&quot; alt_slen &quot;)) &gt; 0) * &quot;		\</span>
<span class="p_add">+		&quot;((&quot; alt_rlen(num) &quot;)-(&quot; alt_slen &quot;)),0x90\n&quot;</span>
 
<span class="p_del">-#define ALTINSTR_ENTRY(feature, number)					      \</span>
<span class="p_add">+#define OLDINSTR(oldinstr, num)						\</span>
<span class="p_add">+	__OLDINSTR(oldinstr, num)					\</span>
<span class="p_add">+	alt_end_marker &quot;:\n&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Pad the second replacement alternative with additional NOPs if it is</span>
<span class="p_add">+ * additionally longer than the first replacement alternative.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define OLDINSTR_2(oldinstr, num1, num2)					\</span>
<span class="p_add">+	__OLDINSTR(oldinstr, num1)						\</span>
<span class="p_add">+	&quot;.skip -(((&quot; alt_rlen(num2) &quot;)-(&quot; alt_rlen(num1) &quot;)-(662b-661b)) &gt; 0) * &quot; \</span>
<span class="p_add">+		&quot;((&quot; alt_rlen(num2) &quot;)-(&quot; alt_rlen(num1) &quot;)-(662b-661b)),0x90\n&quot;  \</span>
<span class="p_add">+	alt_end_marker &quot;:\n&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ALTINSTR_ENTRY(feature, num)					      \</span>
 	&quot; .long 661b - .\n&quot;				/* label           */ \
<span class="p_del">-	&quot; .long &quot; b_replacement(number)&quot;f - .\n&quot;	/* new instruction */ \</span>
<span class="p_add">+	&quot; .long &quot; b_replacement(num)&quot;f - .\n&quot;		/* new instruction */ \</span>
 	&quot; .word &quot; __stringify(feature) &quot;\n&quot;		/* feature bit     */ \
<span class="p_del">-	&quot; .byte &quot; alt_slen &quot;\n&quot;				/* source len      */ \</span>
<span class="p_del">-	&quot; .byte &quot; alt_rlen(number) &quot;\n&quot;			/* replacement len */</span>
<span class="p_add">+	&quot; .byte &quot; alt_total_slen &quot;\n&quot;			/* source len      */ \</span>
<span class="p_add">+	&quot; .byte &quot; alt_rlen(num) &quot;\n&quot;			/* replacement len */ \</span>
<span class="p_add">+	&quot; .byte &quot; alt_pad_len &quot;\n&quot;			/* pad len */</span>
 
<span class="p_del">-#define DISCARD_ENTRY(number)				/* rlen &lt;= slen */    \</span>
<span class="p_del">-	&quot; .byte 0xff + (&quot; alt_rlen(number) &quot;) - (&quot; alt_slen &quot;)\n&quot;</span>
<span class="p_del">-</span>
<span class="p_del">-#define ALTINSTR_REPLACEMENT(newinstr, feature, number)	/* replacement */     \</span>
<span class="p_del">-	b_replacement(number)&quot;:\n\t&quot; newinstr &quot;\n&quot; e_replacement(number) &quot;:\n\t&quot;</span>
<span class="p_add">+#define ALTINSTR_REPLACEMENT(newinstr, feature, num)	/* replacement */     \</span>
<span class="p_add">+	b_replacement(num)&quot;:\n\t&quot; newinstr &quot;\n&quot; e_replacement(num) &quot;:\n\t&quot;</span>
 
 /* alternative assembly primitive: */
 #define ALTERNATIVE(oldinstr, newinstr, feature)			\
<span class="p_del">-	OLDINSTR(oldinstr)						\</span>
<span class="p_add">+	OLDINSTR(oldinstr, 1)						\</span>
 	&quot;.pushsection .altinstructions,\&quot;a\&quot;\n&quot;				\
 	ALTINSTR_ENTRY(feature, 1)					\
 	&quot;.popsection\n&quot;							\
<span class="p_del">-	&quot;.pushsection .discard,\&quot;aw\&quot;,@progbits\n&quot;			\</span>
<span class="p_del">-	DISCARD_ENTRY(1)						\</span>
<span class="p_del">-	&quot;.popsection\n&quot;							\</span>
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr, feature, 1)			\
 	&quot;.popsection&quot;
 
 #define ALTERNATIVE_2(oldinstr, newinstr1, feature1, newinstr2, feature2)\
<span class="p_del">-	OLDINSTR(oldinstr)						\</span>
<span class="p_add">+	OLDINSTR_2(oldinstr, 1, 2)					\</span>
 	&quot;.pushsection .altinstructions,\&quot;a\&quot;\n&quot;				\
 	ALTINSTR_ENTRY(feature1, 1)					\
 	ALTINSTR_ENTRY(feature2, 2)					\
 	&quot;.popsection\n&quot;							\
<span class="p_del">-	&quot;.pushsection .discard,\&quot;aw\&quot;,@progbits\n&quot;			\</span>
<span class="p_del">-	DISCARD_ENTRY(1)						\</span>
<span class="p_del">-	DISCARD_ENTRY(2)						\</span>
<span class="p_del">-	&quot;.popsection\n&quot;							\</span>
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr1, feature1, 1)			\
 	ALTINSTR_REPLACEMENT(newinstr2, feature2, 2)			\
<span class="p_chunk">@@ -146,6 +158,9 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 #define alternative(oldinstr, newinstr, feature)			\
 	asm volatile (ALTERNATIVE(oldinstr, newinstr, feature) : : : &quot;memory&quot;)
 
<span class="p_add">+#define alternative_2(oldinstr, newinstr1, feature1, newinstr2, feature2) \</span>
<span class="p_add">+	asm volatile(ALTERNATIVE_2(oldinstr, newinstr1, feature1, newinstr2, feature2) ::: &quot;memory&quot;)</span>
<span class="p_add">+</span>
 /*
  * Alternative inline assembly with input.
  *
<span class="p_header">diff --git a/arch/x86/include/asm/cmdline.h b/arch/x86/include/asm/cmdline.h</span>
<span class="p_header">index e01f7f7ccb0c..84ae170bc3d0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cmdline.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cmdline.h</span>
<span class="p_chunk">@@ -2,5 +2,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_CMDLINE_H
 
 int cmdline_find_option_bool(const char *cmdline_ptr, const char *option);
<span class="p_add">+int cmdline_find_option(const char *cmdline_ptr, const char *option,</span>
<span class="p_add">+			char *buffer, int bufsize);</span>
 
 #endif /* _ASM_X86_CMDLINE_H */
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index 51f75503541d..6b8fc931a973 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -186,6 +186,10 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_DTHERM	(7*32+ 7) /* Digital Thermal Sensor */
 #define X86_FEATURE_HW_PSTATE	(7*32+ 8) /* AMD HW-PState */
 #define X86_FEATURE_PROC_FEEDBACK (7*32+ 9) /* AMD ProcFeedbackInterface */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE (7*32+10) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Because the ALTERNATIVE scheme is for members of the X86_FEATURE club... */</span>
<span class="p_add">+#define X86_FEATURE_KAISER	(7*32+31) /* &quot;&quot; CONFIG_PAGE_TABLE_ISOLATION w/o nokaiser */</span>
 
 /* Virtualization flags: Linux defined, word 8 */
 #define X86_FEATURE_TPR_SHADOW  (8*32+ 0) /* Intel TPR Shadow */
<span class="p_chunk">@@ -389,6 +393,7 @@</span> <span class="p_context"> static __always_inline __pure bool __static_cpu_has(u16 bit)</span>
 			 &quot; .word %P0\n&quot;		/* 1: do replace */
 			 &quot; .byte 2b - 1b\n&quot;	/* source len */
 			 &quot; .byte 0\n&quot;		/* replacement len */
<span class="p_add">+			 &quot; .byte 0\n&quot;		/* pad len */</span>
 			 &quot;.previous\n&quot;
 			 /* skipping size check since replacement size = 0 */
 			 : : &quot;i&quot; (X86_FEATURE_ALWAYS) : : t_warn);
<span class="p_chunk">@@ -403,6 +408,7 @@</span> <span class="p_context"> static __always_inline __pure bool __static_cpu_has(u16 bit)</span>
 			 &quot; .word %P0\n&quot;		/* feature bit */
 			 &quot; .byte 2b - 1b\n&quot;	/* source len */
 			 &quot; .byte 0\n&quot;		/* replacement len */
<span class="p_add">+			 &quot; .byte 0\n&quot;		/* pad len */</span>
 			 &quot;.previous\n&quot;
 			 /* skipping size check since replacement size = 0 */
 			 : : &quot;i&quot; (bit) : : t_no);
<span class="p_chunk">@@ -428,6 +434,7 @@</span> <span class="p_context"> static __always_inline __pure bool __static_cpu_has(u16 bit)</span>
 			     &quot; .word %P1\n&quot;		/* feature bit */
 			     &quot; .byte 2b - 1b\n&quot;		/* source len */
 			     &quot; .byte 4f - 3f\n&quot;		/* replacement len */
<span class="p_add">+			     &quot; .byte 0\n&quot;		/* pad len */</span>
 			     &quot;.previous\n&quot;
 			     &quot;.section .discard,\&quot;aw\&quot;,@progbits\n&quot;
 			     &quot; .byte 0xff + (4f-3f) - (2b-1b)\n&quot; /* size check */
<span class="p_chunk">@@ -454,31 +461,30 @@</span> <span class="p_context"> static __always_inline __pure bool __static_cpu_has(u16 bit)</span>
 static __always_inline __pure bool _static_cpu_has_safe(u16 bit)
 {
 #ifdef CC_HAVE_ASM_GOTO
<span class="p_del">-/*</span>
<span class="p_del">- * We need to spell the jumps to the compiler because, depending on the offset,</span>
<span class="p_del">- * the replacement jump can be bigger than the original jump, and this we cannot</span>
<span class="p_del">- * have. Thus, we force the jump to the widest, 4-byte, signed relative</span>
<span class="p_del">- * offset even though the last would often fit in less bytes.</span>
<span class="p_del">- */</span>
<span class="p_del">-		asm_volatile_goto(&quot;1: .byte 0xe9\n .long %l[t_dynamic] - 2f\n&quot;</span>
<span class="p_add">+		asm_volatile_goto(&quot;1: jmp %l[t_dynamic]\n&quot;</span>
 			 &quot;2:\n&quot;
<span class="p_add">+			 &quot;.skip -(((5f-4f) - (2b-1b)) &gt; 0) * &quot;</span>
<span class="p_add">+			         &quot;((5f-4f) - (2b-1b)),0x90\n&quot;</span>
<span class="p_add">+			 &quot;3:\n&quot;</span>
 			 &quot;.section .altinstructions,\&quot;a\&quot;\n&quot;
 			 &quot; .long 1b - .\n&quot;		/* src offset */
<span class="p_del">-			 &quot; .long 3f - .\n&quot;		/* repl offset */</span>
<span class="p_add">+			 &quot; .long 4f - .\n&quot;		/* repl offset */</span>
 			 &quot; .word %P1\n&quot;			/* always replace */
<span class="p_del">-			 &quot; .byte 2b - 1b\n&quot;		/* src len */</span>
<span class="p_del">-			 &quot; .byte 4f - 3f\n&quot;		/* repl len */</span>
<span class="p_add">+			 &quot; .byte 3b - 1b\n&quot;		/* src len */</span>
<span class="p_add">+			 &quot; .byte 5f - 4f\n&quot;		/* repl len */</span>
<span class="p_add">+			 &quot; .byte 3b - 2b\n&quot;		/* pad len */</span>
 			 &quot;.previous\n&quot;
 			 &quot;.section .altinstr_replacement,\&quot;ax\&quot;\n&quot;
<span class="p_del">-			 &quot;3: .byte 0xe9\n .long %l[t_no] - 2b\n&quot;</span>
<span class="p_del">-			 &quot;4:\n&quot;</span>
<span class="p_add">+			 &quot;4: jmp %l[t_no]\n&quot;</span>
<span class="p_add">+			 &quot;5:\n&quot;</span>
 			 &quot;.previous\n&quot;
 			 &quot;.section .altinstructions,\&quot;a\&quot;\n&quot;
 			 &quot; .long 1b - .\n&quot;		/* src offset */
 			 &quot; .long 0\n&quot;			/* no replacement */
 			 &quot; .word %P0\n&quot;			/* feature bit */
<span class="p_del">-			 &quot; .byte 2b - 1b\n&quot;		/* src len */</span>
<span class="p_add">+			 &quot; .byte 3b - 1b\n&quot;		/* src len */</span>
 			 &quot; .byte 0\n&quot;			/* repl len */
<span class="p_add">+			 &quot; .byte 0\n&quot;			/* pad len */</span>
 			 &quot;.previous\n&quot;
 			 : : &quot;i&quot; (bit), &quot;i&quot; (X86_FEATURE_ALWAYS)
 			 : : t_dynamic, t_no);
<span class="p_chunk">@@ -498,6 +504,7 @@</span> <span class="p_context"> static __always_inline __pure bool _static_cpu_has_safe(u16 bit)</span>
 			     &quot; .word %P2\n&quot;		/* always replace */
 			     &quot; .byte 2b - 1b\n&quot;		/* source len */
 			     &quot; .byte 4f - 3f\n&quot;		/* replacement len */
<span class="p_add">+			     &quot; .byte 0\n&quot;		/* pad len */</span>
 			     &quot;.previous\n&quot;
 			     &quot;.section .discard,\&quot;aw\&quot;,@progbits\n&quot;
 			     &quot; .byte 0xff + (4f-3f) - (2b-1b)\n&quot; /* size check */
<span class="p_chunk">@@ -512,6 +519,7 @@</span> <span class="p_context"> static __always_inline __pure bool _static_cpu_has_safe(u16 bit)</span>
 			     &quot; .word %P1\n&quot;		/* feature bit */
 			     &quot; .byte 4b - 3b\n&quot;		/* src len */
 			     &quot; .byte 6f - 5f\n&quot;		/* repl len */
<span class="p_add">+			     &quot; .byte 0\n&quot;		/* pad len */</span>
 			     &quot;.previous\n&quot;
 			     &quot;.section .discard,\&quot;aw\&quot;,@progbits\n&quot;
 			     &quot; .byte 0xff + (6f-5f) - (4b-3b)\n&quot; /* size check */
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 69126184c609..2022e4181742 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> struct gdt_page {</span>
 	struct desc_struct gdt[GDT_ENTRIES];
 } __attribute__((aligned(PAGE_SIZE)));
 
<span class="p_del">-DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);</span>
<span class="p_add">+DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page);</span>
 
 static inline struct desc_struct *get_cpu_gdt_table(unsigned int cpu)
 {
<span class="p_header">diff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h</span>
<span class="p_header">index 230853da4ec0..07e1325ac8db 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hardirq.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hardirq.h</span>
<span class="p_chunk">@@ -21,12 +21,8 @@</span> <span class="p_context"> typedef struct {</span>
 #ifdef CONFIG_SMP
 	unsigned int irq_resched_count;
 	unsigned int irq_call_count;
<span class="p_del">-	/*</span>
<span class="p_del">-	 * irq_tlb_count is double-counted in irq_call_count, so it must be</span>
<span class="p_del">-	 * subtracted from irq_call_count when displaying irq_call_count</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	unsigned int irq_tlb_count;</span>
 #endif
<span class="p_add">+	unsigned int irq_tlb_count;</span>
 #ifdef CONFIG_X86_THERMAL_VECTOR
 	unsigned int irq_thermal_count;
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h</span>
<span class="p_header">index 4615906d83df..848d6f946203 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hw_irq.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hw_irq.h</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> extern void (*__initconst interrupt[NR_VECTORS-FIRST_EXTERNAL_VECTOR])(void);</span>
 #define VECTOR_RETRIGGERED	(-2)
 
 typedef int vector_irq_t[NR_VECTORS];
<span class="p_del">-DECLARE_PER_CPU(vector_irq_t, vector_irq);</span>
<span class="p_add">+DECLARE_PER_CPU_USER_MAPPED(vector_irq_t, vector_irq);</span>
 extern void setup_vector_irq(int cpu);
 
 #ifdef CONFIG_X86_IO_APIC
<span class="p_header">diff --git a/arch/x86/include/asm/kaiser.h b/arch/x86/include/asm/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..802bbbdfe143</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,141 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_KAISER_H</span>
<span class="p_add">+#define _ASM_X86_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/processor-flags.h&gt; /* For PCID constants */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This file includes the definitions for the KAISER feature.</span>
<span class="p_add">+ * KAISER is a counter measure against x86_64 side channel attacks on</span>
<span class="p_add">+ * the kernel virtual memory.  It has a shadow pgd for every process: the</span>
<span class="p_add">+ * shadow pgd has a minimalistic kernel-set mapped, but includes the whole</span>
<span class="p_add">+ * user memory. Within a kernel context switch, or when an interrupt is handled,</span>
<span class="p_add">+ * the pgd is switched to the normal one. When the system switches to user mode,</span>
<span class="p_add">+ * the shadow pgd is enabled. By this, the virtual memory caches are freed,</span>
<span class="p_add">+ * and the user may not attack the whole kernel memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A minimalistic kernel mapping holds the parts needed to be mapped in user</span>
<span class="p_add">+ * mode, such as the entry/exit functions of the user space, or the stacks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define KAISER_SHADOW_PGD_OFFSET 0x1000</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
<span class="p_add">+.macro _SWITCH_TO_KERNEL_CR3 reg</span>
<span class="p_add">+movq %cr3, \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */</span>
<span class="p_add">+ALTERNATIVE &quot;&quot;, &quot;bts $63, \reg&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+movq \reg, %cr3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro _SWITCH_TO_USER_CR3 reg regb</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * regb must be the low byte portion of reg: because we have arranged</span>
<span class="p_add">+ * for the low byte of the user PCID to serve as the high byte of NOFLUSH</span>
<span class="p_add">+ * (0x80 for each when PCID is enabled, or 0x00 when PCID and NOFLUSH are</span>
<span class="p_add">+ * not enabled): so that the one register can update both memory and cr3.</span>
<span class="p_add">+ */</span>
<span class="p_add">+movq %cr3, \reg</span>
<span class="p_add">+orq  PER_CPU_VAR(x86_cr3_pcid_user), \reg</span>
<span class="p_add">+js   9f</span>
<span class="p_add">+/* If PCID enabled, FLUSH this time, reset to NOFLUSH for next time */</span>
<span class="p_add">+movb \regb, PER_CPU_VAR(x86_cr3_pcid_user+7)</span>
<span class="p_add">+9:</span>
<span class="p_add">+movq \reg, %cr3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, &quot;pushq %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_KERNEL_CR3 %rax</span>
<span class="p_add">+popq %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_USER_CR3</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, &quot;pushq %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_USER_CR3 %rax %al</span>
<span class="p_add">+popq %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3_NO_STACK</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, \</span>
<span class="p_add">+	__stringify(movq %rax, PER_CPU_VAR(unsafe_stack_register_backup)), \</span>
<span class="p_add">+	X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_KERNEL_CR3 %rax</span>
<span class="p_add">+movq PER_CPU_VAR(unsafe_stack_register_backup), %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_USER_CR3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3_NO_STACK</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Upon kernel/user mode switch, it may happen that the address</span>
<span class="p_add">+ * space has to be switched before the registers have been</span>
<span class="p_add">+ * stored.  To change the address space, another register is</span>
<span class="p_add">+ * needed.  A register therefore has to be stored/restored.</span>
<span class="p_add">+*/</span>
<span class="p_add">+DECLARE_PER_CPU_USER_MAPPED(unsigned long, unsafe_stack_register_backup);</span>
<span class="p_add">+</span>
<span class="p_add">+DECLARE_PER_CPU(unsigned long, x86_cr3_pcid_user);</span>
<span class="p_add">+</span>
<span class="p_add">+extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];</span>
<span class="p_add">+</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+extern void __init kaiser_check_boottime_disable(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled	0</span>
<span class="p_add">+static inline void __init kaiser_check_boottime_disable(void) {}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Kaiser function prototypes are needed even when CONFIG_PAGE_TABLE_ISOLATION is not set,</span>
<span class="p_add">+ * so as to build with tests on kaiser_enabled instead of #ifdefs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_add_mapping - map a virtual memory part to the shadow (user) mapping</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ *  @flags: The mapping flags of the pages</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  The mapping is done on a global scope, so no bigger</span>
<span class="p_add">+ *  synchronization has to be done.  the pages have to be</span>
<span class="p_add">+ *  manually unmapped again when they are not needed any longer.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern int kaiser_add_mapping(unsigned long addr, unsigned long size, unsigned long flags);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_remove_mapping - unmap a virtual memory part of the shadow mapping</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_remove_mapping(unsigned long start, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_init - Initialize the shadow mapping</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Most parts of the shadow mapping can be mapped upon boot</span>
<span class="p_add">+ *  time.  Only per-process things like the thread stacks</span>
<span class="p_add">+ *  or a new LDT have to be mapped at runtime.  These boot-</span>
<span class="p_add">+ *  time mappings are permanent and never unmapped.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_KAISER_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="p_header">index b6b7bc3f5d26..3bc5c0b13cd7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -20,12 +20,6 @@</span> <span class="p_context"> typedef struct {</span>
 	void __user *vdso;
 } mm_context_t;
 
<span class="p_del">-#ifdef CONFIG_SMP</span>
 void leave_mm(int cpu);
<span class="p_del">-#else</span>
<span class="p_del">-static inline void leave_mm(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
 
 #endif /* _ASM_X86_MMU_H */
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 50f622dc0b1a..740e65890eaa 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -68,89 +68,16 @@</span> <span class="p_context"> void destroy_context(struct mm_struct *mm);</span>
 
 static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 {
<span class="p_del">-#ifdef CONFIG_SMP</span>
 	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)
 		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);
<span class="p_del">-#endif</span>
 }
 
<span class="p_del">-static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-			     struct task_struct *tsk)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned cpu = smp_processor_id();</span>
<span class="p_add">+extern void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+		      struct task_struct *tsk);</span>
 
<span class="p_del">-	if (likely(prev != next)) {</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.active_mm, next);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Re-load page tables.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * This logic has an ordering constraint:</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_del">-		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_del">-		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_del">-		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_del">-		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_del">-		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_del">-		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_del">-		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_del">-		 * execute full barriers to prevent this from happening.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_del">-		 * store to mm_cpumask and any operation that could load</span>
<span class="p_del">-		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_del">-		 * due to instruction fetches or for no reason at all,</span>
<span class="p_del">-		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_del">-		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_del">-		 * ordering guarantee we need.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		load_cr3(next-&gt;pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Stop flush ipis for the previous mm */</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, mm_cpumask(prev));</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Load the LDT, if the LDT is different: */</span>
<span class="p_del">-		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))</span>
<span class="p_del">-			load_mm_ldt(next);</span>
<span class="p_del">-	}</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-	  else {</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_del">-		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * On established mms, the mm_cpumask is only changed</span>
<span class="p_del">-			 * from irq context, from ptep_clear_flush() while in</span>
<span class="p_del">-			 * lazy tlb mode, and here. Irqs are blocked during</span>
<span class="p_del">-			 * schedule, protecting us from simultaneous changes.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * We were in lazy tlb mode and leave_mm disabled</span>
<span class="p_del">-			 * tlb flush IPI delivery. We must reload CR3</span>
<span class="p_del">-			 * to make sure to use no freed page tables.</span>
<span class="p_del">-			 *</span>
<span class="p_del">-			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_del">-			 * fills with respect to the mm_cpumask write.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			load_cr3(next-&gt;pgd);</span>
<span class="p_del">-			load_mm_ldt(next);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_add">+extern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			       struct task_struct *tsk);</span>
<span class="p_add">+#define switch_mm_irqs_off switch_mm_irqs_off</span>
 
 #define activate_mm(prev, next)			\
 do {						\
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index aa97a070f09f..1b5b34cba964 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -17,6 +17,12 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 #include &lt;asm/x86_init.h&gt;
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd);
 
 /*
<span class="p_chunk">@@ -641,7 +647,17 @@</span> <span class="p_context"> static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)</span>
 
 static inline int pgd_bad(pgd_t pgd)
 {
<span class="p_del">-	return (pgd_flags(pgd) &amp; ~_PAGE_USER) != _KERNPG_TABLE;</span>
<span class="p_add">+	pgdval_t ignore_flags = _PAGE_USER;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We set NX on KAISER pgds that map userspace memory so</span>
<span class="p_add">+	 * that userspace can not meaningfully use the kernel</span>
<span class="p_add">+	 * page table by accident; it will fault on the first</span>
<span class="p_add">+	 * instruction it tries to run.  See native_set_pgd().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pgd_flags(pgd) &amp; ~ignore_flags) != _KERNPG_TABLE;</span>
 }
 
 static inline int pgd_none(pgd_t pgd)
<span class="p_chunk">@@ -843,7 +859,15 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
  */
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
<span class="p_del">-       memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+	memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		/* Clone the shadow pgd part as well */</span>
<span class="p_add">+		memcpy(native_get_shadow_pgd(dst),</span>
<span class="p_add">+			native_get_shadow_pgd(src),</span>
<span class="p_add">+			count * sizeof(pgd_t));</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 3874693c0e53..ada2cb8adbc2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -106,9 +106,32 @@</span> <span class="p_context"> static inline void native_pud_clear(pud_t *pud)</span>
 	native_set_pud(pud, native_make_pud(0));
 }
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *native_get_shadow_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_VM</span>
<span class="p_add">+	/* linux/mmdebug.h may not have been included at this point */</span>
<span class="p_add">+	BUG_ON(!kaiser_enabled);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return (pgd_t *)((unsigned long)pgdp | (unsigned long)PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline pgd_t *native_get_shadow_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG_ON(1);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_del">-	*pgdp = pgd;</span>
<span class="p_add">+	*pgdp = kaiser_set_shadow_pgd(pgdp, pgd);</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index f216963760e5..c5d7703dc591 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -116,7 +116,7 @@</span> <span class="p_context"></span>
 #endif
 
 #define _PAGE_FILE	(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_FILE)
<span class="p_del">-#define _PAGE_PROTNONE	(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PROTNONE)</span>
<span class="p_add">+#define _PAGE_PROTNONE  (_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PROTNONE)</span>
 
 #define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\
 			 _PAGE_ACCESSED | _PAGE_DIRTY)
<span class="p_chunk">@@ -129,6 +129,33 @@</span> <span class="p_context"></span>
 			 _PAGE_SOFT_DIRTY | _PAGE_NUMA)
 #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE | _PAGE_NUMA)
 
<span class="p_add">+/* The ASID is the lower 12 bits of CR3 */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_MASK  (_AC((1&lt;&lt;12)-1,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Mask for all the PCID-related bits in CR3: */</span>
<span class="p_add">+#define X86_CR3_PCID_MASK       (X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_MASK)</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x0,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_PAGE_TABLE_ISOLATION) &amp;&amp; defined(CONFIG_X86_64)</span>
<span class="p_add">+/* Let X86_CR3_PCID_ASID_USER be usable for the X86_CR3_PCID_NOFLUSH bit */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER	(_AC(0x80,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x0,UL))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PCIDs are unsupported on 32-bit and none of these bits can be</span>
<span class="p_add">+ * set in CR3:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #define _PAGE_CACHE_MASK	(_PAGE_PCD | _PAGE_PWT)
 #define _PAGE_CACHE_WB		(0)
 #define _PAGE_CACHE_WC		(_PAGE_PWT)
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index a4ea02351f4d..4a2f66843288 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -72,7 +72,6 @@</span> <span class="p_context"> extern u16 __read_mostly tlb_lld_4k[NR_INFO];</span>
 extern u16 __read_mostly tlb_lld_2m[NR_INFO];
 extern u16 __read_mostly tlb_lld_4m[NR_INFO];
 extern u16 __read_mostly tlb_lld_1g[NR_INFO];
<span class="p_del">-extern s8  __read_mostly tlb_flushall_shift;</span>
 
 /*
  *  CPU type and hardware bug flags. Kept separately for each CPU.
<span class="p_chunk">@@ -283,7 +282,7 @@</span> <span class="p_context"> struct tss_struct {</span>
 
 } ____cacheline_aligned;
 
<span class="p_del">-DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);</span>
<span class="p_add">+DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, init_tss);</span>
 
 /*
  * Save the original ist values for checking stack pointers during debugging
<span class="p_chunk">@@ -579,39 +578,6 @@</span> <span class="p_context"> static inline void load_sp0(struct tss_struct *tss,</span>
 #define set_iopl_mask native_set_iopl_mask
 #endif /* CONFIG_PARAVIRT */
 
<span class="p_del">-/*</span>
<span class="p_del">- * Save the cr4 feature set we&#39;re using (ie</span>
<span class="p_del">- * Pentium 4MB enable and PPro Global page</span>
<span class="p_del">- * enable), so that any CPU&#39;s that boot up</span>
<span class="p_del">- * after us can get the correct flags.</span>
<span class="p_del">- */</span>
<span class="p_del">-extern unsigned long mmu_cr4_features;</span>
<span class="p_del">-extern u32 *trampoline_cr4_features;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void set_in_cr4(unsigned long mask)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long cr4;</span>
<span class="p_del">-</span>
<span class="p_del">-	mmu_cr4_features |= mask;</span>
<span class="p_del">-	if (trampoline_cr4_features)</span>
<span class="p_del">-		*trampoline_cr4_features = mmu_cr4_features;</span>
<span class="p_del">-	cr4 = read_cr4();</span>
<span class="p_del">-	cr4 |= mask;</span>
<span class="p_del">-	write_cr4(cr4);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void clear_in_cr4(unsigned long mask)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long cr4;</span>
<span class="p_del">-</span>
<span class="p_del">-	mmu_cr4_features &amp;= ~mask;</span>
<span class="p_del">-	if (trampoline_cr4_features)</span>
<span class="p_del">-		*trampoline_cr4_features = mmu_cr4_features;</span>
<span class="p_del">-	cr4 = read_cr4();</span>
<span class="p_del">-	cr4 &amp;= ~mask;</span>
<span class="p_del">-	write_cr4(cr4);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 typedef struct {
 	unsigned long		seg;
 } mm_segment_t;
<span class="p_chunk">@@ -930,7 +896,7 @@</span> <span class="p_context"> extern unsigned long KSTK_ESP(struct task_struct *task);</span>
 /*
  * User space RSP while inside the SYSCALL fast path
  */
<span class="p_del">-DECLARE_PER_CPU(unsigned long, old_rsp);</span>
<span class="p_add">+DECLARE_PER_CPU_USER_MAPPED(unsigned long, old_rsp);</span>
 
 #endif /* CONFIG_X86_64 */
 
<span class="p_header">diff --git a/arch/x86/include/asm/smap.h b/arch/x86/include/asm/smap.h</span>
<span class="p_header">index 8d3120f4e270..c56cb4f37be9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/smap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/smap.h</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 	662: __ASM_CLAC ;						\
 	.popsection ;							\
 	.pushsection .altinstructions, &quot;a&quot; ;				\
<span class="p_del">-	altinstruction_entry 661b, 662b, X86_FEATURE_SMAP, 3, 3 ;	\</span>
<span class="p_add">+	altinstruction_entry 661b, 662b, X86_FEATURE_SMAP, 3, 3, 0 ;	\</span>
 	.popsection
 
 #define ASM_STAC							\
<span class="p_chunk">@@ -42,7 +42,7 @@</span> <span class="p_context"></span>
 	662: __ASM_STAC ;						\
 	.popsection ;							\
 	.pushsection .altinstructions, &quot;a&quot; ;				\
<span class="p_del">-	altinstruction_entry 661b, 662b, X86_FEATURE_SMAP, 3, 3 ;	\</span>
<span class="p_add">+	altinstruction_entry 661b, 662b, X86_FEATURE_SMAP, 3, 3, 0 ;	\</span>
 	.popsection
 
 #else /* CONFIG_X86_SMAP */
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 5e4b0cc54e43..bd10f6775dfd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -6,6 +6,55 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/special_insns.h&gt;
<span class="p_add">+#include &lt;asm/smp.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
<span class="p_add">+			     unsigned long type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct { u64 d[2]; } desc = { { pcid, addr } };</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The memory clobber is because the whole point is to invalidate</span>
<span class="p_add">+	 * stale TLB entries and, especially if we&#39;re flushing global</span>
<span class="p_add">+	 * mappings, we don&#39;t want the compiler to reorder any subsequent</span>
<span class="p_add">+	 * memory accesses before the TLB flush.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The hex opcode is invpcid (%ecx), %eax in 32-bit mode and</span>
<span class="p_add">+	 * invpcid (%rcx), %rax in long mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	asm volatile (&quot;.byte 0x66, 0x0f, 0x38, 0x82, 0x01&quot;</span>
<span class="p_add">+		      : : &quot;m&quot; (desc), &quot;a&quot; (type), &quot;c&quot; (&amp;desc) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define INVPCID_TYPE_INDIV_ADDR		0</span>
<span class="p_add">+#define INVPCID_TYPE_SINGLE_CTXT	1</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_INCL_GLOBAL	2</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_NON_GLOBAL	3</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given pcid and addr, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_one(unsigned long pcid,</span>
<span class="p_add">+				     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, addr, INVPCID_TYPE_INDIV_ADDR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given PCID, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_single_context(unsigned long pcid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, 0, INVPCID_TYPE_SINGLE_CTXT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings, including globals, for all PCIDs. */</span>
<span class="p_add">+static inline void invpcid_flush_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_INCL_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for all PCIDs except globals. */</span>
<span class="p_add">+static inline void invpcid_flush_all_nonglobals(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_NON_GLOBAL);</span>
<span class="p_add">+}</span>
 
 #ifdef CONFIG_PARAVIRT
 #include &lt;asm/paravirt.h&gt;
<span class="p_chunk">@@ -15,6 +64,61 @@</span> <span class="p_context"></span>
 #define __flush_tlb_single(addr) __native_flush_tlb_single(addr)
 #endif
 
<span class="p_add">+/* Set in this cpu&#39;s CR4. */</span>
<span class="p_add">+static inline void cr4_set_bits(unsigned long mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long cr4;</span>
<span class="p_add">+</span>
<span class="p_add">+	cr4 = read_cr4();</span>
<span class="p_add">+	cr4 |= mask;</span>
<span class="p_add">+	write_cr4(cr4);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Clear in this cpu&#39;s CR4. */</span>
<span class="p_add">+static inline void cr4_clear_bits(unsigned long mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long cr4;</span>
<span class="p_add">+</span>
<span class="p_add">+	cr4 = read_cr4();</span>
<span class="p_add">+	cr4 &amp;= ~mask;</span>
<span class="p_add">+	write_cr4(cr4);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Save some of cr4 feature set we&#39;re using (e.g.  Pentium 4MB</span>
<span class="p_add">+ * enable and PPro Global page enable), so that any CPU&#39;s that boot</span>
<span class="p_add">+ * up after us can get the correct flags.  This should only be used</span>
<span class="p_add">+ * during boot on the boot cpu.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern unsigned long mmu_cr4_features;</span>
<span class="p_add">+extern u32 *trampoline_cr4_features;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mmu_cr4_features |= mask;</span>
<span class="p_add">+	if (trampoline_cr4_features)</span>
<span class="p_add">+		*trampoline_cr4_features = mmu_cr4_features;</span>
<span class="p_add">+	cr4_set_bits(mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Declare a couple of kaiser interfaces here for convenience,</span>
<span class="p_add">+ * to avoid the need for asm/kaiser.h in unexpected places.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+extern void kaiser_setup_pcid(void);</span>
<span class="p_add">+extern void kaiser_flush_tlb_on_return_to_user(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled 0</span>
<span class="p_add">+static inline void kaiser_setup_pcid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_flush_tlb_on_return_to_user(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline void __native_flush_tlb(void)
 {
 	/*
<span class="p_chunk">@@ -23,6 +127,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 	 * back:
 	 */
 	preempt_disable();
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		kaiser_flush_tlb_on_return_to_user();</span>
 	native_write_cr3(native_read_cr3());
 	preempt_enable();
 }
<span class="p_chunk">@@ -32,39 +138,84 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global_irq_disabled(void)</span>
 	unsigned long cr4;
 
 	cr4 = native_read_cr4();
<span class="p_del">-	/* clear PGE */</span>
<span class="p_del">-	native_write_cr4(cr4 &amp; ~X86_CR4_PGE);</span>
<span class="p_del">-	/* write old PGE again and flush TLBs */</span>
<span class="p_del">-	native_write_cr4(cr4);</span>
<span class="p_add">+	if (cr4 &amp; X86_CR4_PGE) {</span>
<span class="p_add">+		/* clear PGE and flush TLB of all entries */</span>
<span class="p_add">+		native_write_cr4(cr4 &amp; ~X86_CR4_PGE);</span>
<span class="p_add">+		/* restore PGE as it was before */</span>
<span class="p_add">+		native_write_cr4(cr4);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* do it with cr3, letting kaiser flush user PCID */</span>
<span class="p_add">+		__native_flush_tlb();</span>
<span class="p_add">+	}</span>
 }
 
 static inline void __native_flush_tlb_global(void)
 {
 	unsigned long flags;
 
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Using INVPCID is considerably faster than a pair of writes</span>
<span class="p_add">+		 * to CR4 sandwiched inside an IRQ flag save/restore.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+	 	 * Note, this works with CR4.PCIDE=0 or 1.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		invpcid_flush_all();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Read-modify-write to CR4 - protect it from preemption and
 	 * from interrupts. (Use the raw variant because this code can
 	 * be called from deep inside debugging code.)
 	 */
 	raw_local_irq_save(flags);
<span class="p_del">-</span>
 	__native_flush_tlb_global_irq_disabled();
<span class="p_del">-</span>
 	raw_local_irq_restore(flags);
 }
 
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_del">-	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SIMICS #GP&#39;s if you run INVPCID with type 2/3</span>
<span class="p_add">+	 * and X86_CR4_PCIDE clear.  Shame!</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The ASIDs used below are hard-coded.  But, we must not</span>
<span class="p_add">+	 * call invpcid(type=1/2) before CR4.PCIDE=1.  Just call</span>
<span class="p_add">+	 * invlpg in the case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE)) {</span>
<span class="p_add">+		if (kaiser_enabled)</span>
<span class="p_add">+			kaiser_flush_tlb_on_return_to_user();</span>
<span class="p_add">+		asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Flush the address out of both PCIDs. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * An optimization here might be to determine addresses</span>
<span class="p_add">+	 * that are only kernel-mapped and only flush the kernel</span>
<span class="p_add">+	 * ASID.  But, userspace flushes are probably much more</span>
<span class="p_add">+	 * important performance-wise.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Make sure to do only a single invpcid when KAISER is</span>
<span class="p_add">+	 * disabled and we have only a single ASID.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		invpcid_flush_one(X86_CR3_PCID_ASID_USER, addr);</span>
<span class="p_add">+	invpcid_flush_one(X86_CR3_PCID_ASID_KERN, addr);</span>
 }
 
 static inline void __flush_tlb_all(void)
 {
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_del">-		__flush_tlb_global();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		__flush_tlb();</span>
<span class="p_add">+	__flush_tlb_global();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: if we somehow had PCID but not PGE, then this wouldn&#39;t work --</span>
<span class="p_add">+	 * we&#39;d end up flushing kernel translations for the current ASID but</span>
<span class="p_add">+	 * we might fail to flush kernel translations for other cached ASIDs.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * To avoid this issue, we force PCID off if PGE is off.</span>
<span class="p_add">+	 */</span>
 }
 
 static inline void __flush_tlb_one(unsigned long addr)
<span class="p_chunk">@@ -78,7 +229,6 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
 /*
  * TLB flushing:
  *
<span class="p_del">- *  - flush_tlb() flushes the current mm struct TLBs</span>
  *  - flush_tlb_all() flushes all processes TLBs
  *  - flush_tlb_mm(mm) flushes the specified mm context TLB&#39;s
  *  - flush_tlb_page(vma, vmaddr) flushes one page
<span class="p_chunk">@@ -90,84 +240,6 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
  * and page-granular flushes are available only on i486 and up.
  */
 
<span class="p_del">-#ifndef CONFIG_SMP</span>
<span class="p_del">-</span>
<span class="p_del">-/* &quot;_up&quot; is for UniProcessor.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This is a helper for other header functions.  *Not* intended to be called</span>
<span class="p_del">- * directly.  All global TLB flushes need to either call this, or to bump the</span>
<span class="p_del">- * vm statistics themselves.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void __flush_tlb_up(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);</span>
<span class="p_del">-	__flush_tlb();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_all(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);</span>
<span class="p_del">-	__flush_tlb_all();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__flush_tlb_up();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void local_flush_tlb(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__flush_tlb_up();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (mm == current-&gt;active_mm)</span>
<span class="p_del">-		__flush_tlb_up();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_page(struct vm_area_struct *vma,</span>
<span class="p_del">-				  unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (vma-&gt;vm_mm == current-&gt;active_mm)</span>
<span class="p_del">-		__flush_tlb_one(addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_range(struct vm_area_struct *vma,</span>
<span class="p_del">-				   unsigned long start, unsigned long end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (vma-&gt;vm_mm == current-&gt;active_mm)</span>
<span class="p_del">-		__flush_tlb_up();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_mm_range(struct mm_struct *mm,</span>
<span class="p_del">-	   unsigned long start, unsigned long end, unsigned long vmflag)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (mm == current-&gt;active_mm)</span>
<span class="p_del">-		__flush_tlb_up();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void native_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="p_del">-					   struct mm_struct *mm,</span>
<span class="p_del">-					   unsigned long start,</span>
<span class="p_del">-					   unsigned long end)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void reset_lazy_tlbstate(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_del">-					  unsigned long end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	flush_tlb_all();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#else  /* SMP */</span>
<span class="p_del">-</span>
<span class="p_del">-#include &lt;asm/smp.h&gt;</span>
<span class="p_del">-</span>
 #define local_flush_tlb() __flush_tlb()
 
 #define flush_tlb_mm(mm)	flush_tlb_mm_range(mm, 0UL, TLB_FLUSH_ALL, 0UL)
<span class="p_chunk">@@ -176,13 +248,14 @@</span> <span class="p_context"> static inline void flush_tlb_kernel_range(unsigned long start,</span>
 		flush_tlb_mm_range(vma-&gt;vm_mm, start, end, vma-&gt;vm_flags)
 
 extern void flush_tlb_all(void);
<span class="p_del">-extern void flush_tlb_current_task(void);</span>
<span class="p_del">-extern void flush_tlb_page(struct vm_area_struct *, unsigned long);</span>
 extern void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned long vmflag);
 extern void flush_tlb_kernel_range(unsigned long start, unsigned long end);
 
<span class="p_del">-#define flush_tlb()	flush_tlb_current_task()</span>
<span class="p_add">+static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm_range(vma-&gt;vm_mm, a, a + PAGE_SIZE, VM_NONE);</span>
<span class="p_add">+}</span>
 
 void native_flush_tlb_others(const struct cpumask *cpumask,
 				struct mm_struct *mm,
<span class="p_chunk">@@ -203,8 +276,6 @@</span> <span class="p_context"> static inline void reset_lazy_tlbstate(void)</span>
 	this_cpu_write(cpu_tlbstate.active_mm, &amp;init_mm);
 }
 
<span class="p_del">-#endif	/* SMP */</span>
<span class="p_del">-</span>
 #ifndef CONFIG_PARAVIRT
 #define flush_tlb_others(mask, mm, start, end)	\
 	native_flush_tlb_others(mask, mm, start, end)
<span class="p_header">diff --git a/arch/x86/include/asm/virtext.h b/arch/x86/include/asm/virtext.h</span>
<span class="p_header">index 5da71c27cc59..f41e19ca717b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/virtext.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/virtext.h</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/vmx.h&gt;
 #include &lt;asm/svm.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 
 /*
  * VMX functions:
<span class="p_chunk">@@ -40,7 +41,7 @@</span> <span class="p_context"> static inline int cpu_has_vmx(void)</span>
 static inline void cpu_vmxoff(void)
 {
 	asm volatile (ASM_VMX_VMXOFF : : : &quot;cc&quot;);
<span class="p_del">-	write_cr4(read_cr4() &amp; ~X86_CR4_VMXE);</span>
<span class="p_add">+	cr4_clear_bits(X86_CR4_VMXE);</span>
 }
 
 static inline int cpu_vmx_enabled(void)
<span class="p_header">diff --git a/arch/x86/include/asm/vsyscall.h b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">index 2874be9aef0a..cbab442445ae 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 /* kernel space (writeable) */
 extern int vgetcpu_mode;
 extern struct timezone sys_tz;
<span class="p_add">+extern unsigned long vsyscall_pgprot;</span>
 
 #include &lt;asm/vvar.h&gt;
 
<span class="p_chunk">@@ -20,6 +21,7 @@</span> <span class="p_context"> extern void map_vsyscall(void);</span>
  * Returns true if handled.
  */
 extern bool emulate_vsyscall(struct pt_regs *regs, unsigned long address);
<span class="p_add">+extern bool vsyscall_enabled(void);</span>
 
 #ifdef CONFIG_X86_64
 
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 180a0c3c224d..bd4513b7b877 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -79,7 +79,8 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c</span>
<span class="p_header">index 703130f469ec..af397cc98d05 100644</span>
<span class="p_header">--- a/arch/x86/kernel/alternative.c</span>
<span class="p_header">+++ b/arch/x86/kernel/alternative.c</span>
<span class="p_chunk">@@ -52,10 +52,25 @@</span> <span class="p_context"> static int __init setup_noreplace_paravirt(char *str)</span>
 __setup(&quot;noreplace-paravirt&quot;, setup_noreplace_paravirt);
 #endif
 
<span class="p_del">-#define DPRINTK(fmt, ...)				\</span>
<span class="p_del">-do {							\</span>
<span class="p_del">-	if (debug_alternative)				\</span>
<span class="p_del">-		printk(KERN_DEBUG fmt, ##__VA_ARGS__);	\</span>
<span class="p_add">+#define DPRINTK(fmt, args...)						\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	if (debug_alternative)						\</span>
<span class="p_add">+		printk(KERN_DEBUG &quot;%s: &quot; fmt &quot;\n&quot;, __func__, ##args);	\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DUMP_BYTES(buf, len, fmt, args...)				\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	if (unlikely(debug_alternative)) {				\</span>
<span class="p_add">+		int j;							\</span>
<span class="p_add">+									\</span>
<span class="p_add">+		if (!(len))						\</span>
<span class="p_add">+			break;						\</span>
<span class="p_add">+									\</span>
<span class="p_add">+		printk(KERN_DEBUG fmt, ##args);				\</span>
<span class="p_add">+		for (j = 0; j &lt; (len) - 1; j++)				\</span>
<span class="p_add">+			printk(KERN_CONT &quot;%02hhx &quot;, buf[j]);		\</span>
<span class="p_add">+		printk(KERN_CONT &quot;%02hhx\n&quot;, buf[j]);			\</span>
<span class="p_add">+	}								\</span>
 } while (0)
 
 /*
<span class="p_chunk">@@ -243,12 +258,86 @@</span> <span class="p_context"> extern struct alt_instr __alt_instructions[], __alt_instructions_end[];</span>
 extern s32 __smp_locks[], __smp_locks_end[];
 void *text_poke_early(void *addr, const void *opcode, size_t len);
 
<span class="p_del">-/* Replace instructions with better alternatives for this CPU type.</span>
<span class="p_del">-   This runs before SMP is initialized to avoid SMP problems with</span>
<span class="p_del">-   self modifying code. This implies that asymmetric systems where</span>
<span class="p_del">-   APs have less capabilities than the boot processor are not handled.</span>
<span class="p_del">-   Tough. Make sure you disable such features by hand. */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Are we looking at a near JMP with a 1 or 4-byte displacement.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool is_jmp(const u8 opcode)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return opcode == 0xeb || opcode == 0xe9;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init_or_module</span>
<span class="p_add">+recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insnbuf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 *next_rip, *tgt_rip;</span>
<span class="p_add">+	s32 n_dspl, o_dspl;</span>
<span class="p_add">+	int repl_len;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (a-&gt;replacementlen != 5)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	o_dspl = *(s32 *)(insnbuf + 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* next_rip of the replacement JMP */</span>
<span class="p_add">+	next_rip = repl_insn + a-&gt;replacementlen;</span>
<span class="p_add">+	/* target rip of the replacement JMP */</span>
<span class="p_add">+	tgt_rip  = next_rip + o_dspl;</span>
<span class="p_add">+	n_dspl = tgt_rip - orig_insn;</span>
<span class="p_add">+</span>
<span class="p_add">+	DPRINTK(&quot;target RIP: %p, new_displ: 0x%x&quot;, tgt_rip, n_dspl);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tgt_rip - orig_insn &gt;= 0) {</span>
<span class="p_add">+		if (n_dspl - 2 &lt;= 127)</span>
<span class="p_add">+			goto two_byte_jmp;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			goto five_byte_jmp;</span>
<span class="p_add">+	/* negative offset */</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (((n_dspl - 2) &amp; 0xff) == (n_dspl - 2))</span>
<span class="p_add">+			goto two_byte_jmp;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			goto five_byte_jmp;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+two_byte_jmp:</span>
<span class="p_add">+	n_dspl -= 2;</span>
<span class="p_add">+</span>
<span class="p_add">+	insnbuf[0] = 0xeb;</span>
<span class="p_add">+	insnbuf[1] = (s8)n_dspl;</span>
<span class="p_add">+	add_nops(insnbuf + 2, 3);</span>
<span class="p_add">+</span>
<span class="p_add">+	repl_len = 2;</span>
<span class="p_add">+	goto done;</span>
<span class="p_add">+</span>
<span class="p_add">+five_byte_jmp:</span>
<span class="p_add">+	n_dspl -= 5;</span>
 
<span class="p_add">+	insnbuf[0] = 0xe9;</span>
<span class="p_add">+	*(s32 *)&amp;insnbuf[1] = n_dspl;</span>
<span class="p_add">+</span>
<span class="p_add">+	repl_len = 5;</span>
<span class="p_add">+</span>
<span class="p_add">+done:</span>
<span class="p_add">+</span>
<span class="p_add">+	DPRINTK(&quot;final displ: 0x%08x, JMP 0x%lx&quot;,</span>
<span class="p_add">+		n_dspl, (unsigned long)orig_insn + n_dspl + repl_len);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init_or_module optimize_nops(struct alt_instr *a, u8 *instr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	add_nops(instr + (a-&gt;instrlen - a-&gt;padlen), a-&gt;padlen);</span>
<span class="p_add">+</span>
<span class="p_add">+	DUMP_BYTES(instr, a-&gt;instrlen, &quot;%p: [%d:%d) optimized NOPs: &quot;,</span>
<span class="p_add">+		   instr, a-&gt;instrlen - a-&gt;padlen, a-&gt;padlen);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Replace instructions with better alternatives for this CPU type. This runs</span>
<span class="p_add">+ * before SMP is initialized to avoid SMP problems with self modifying code.</span>
<span class="p_add">+ * This implies that asymmetric systems where APs have less capabilities than</span>
<span class="p_add">+ * the boot processor are not handled. Tough. Make sure you disable such</span>
<span class="p_add">+ * features by hand.</span>
<span class="p_add">+ */</span>
 void __init_or_module apply_alternatives(struct alt_instr *start,
 					 struct alt_instr *end)
 {
<span class="p_chunk">@@ -256,10 +345,10 @@</span> <span class="p_context"> void __init_or_module apply_alternatives(struct alt_instr *start,</span>
 	u8 *instr, *replacement;
 	u8 insnbuf[MAX_PATCH_LEN];
 
<span class="p_del">-	DPRINTK(&quot;%s: alt table %p -&gt; %p\n&quot;, __func__, start, end);</span>
<span class="p_add">+	DPRINTK(&quot;alt table %p -&gt; %p&quot;, start, end);</span>
 	/*
 	 * The scan order should be from start to end. A later scanned
<span class="p_del">-	 * alternative code can overwrite a previous scanned alternative code.</span>
<span class="p_add">+	 * alternative code can overwrite previously scanned alternative code.</span>
 	 * Some kernel functions (e.g. memcpy, memset, etc) use this order to
 	 * patch code.
 	 *
<span class="p_chunk">@@ -267,29 +356,54 @@</span> <span class="p_context"> void __init_or_module apply_alternatives(struct alt_instr *start,</span>
 	 * order.
 	 */
 	for (a = start; a &lt; end; a++) {
<span class="p_add">+		int insnbuf_sz = 0;</span>
<span class="p_add">+</span>
 		instr = (u8 *)&amp;a-&gt;instr_offset + a-&gt;instr_offset;
 		replacement = (u8 *)&amp;a-&gt;repl_offset + a-&gt;repl_offset;
<span class="p_del">-		BUG_ON(a-&gt;replacementlen &gt; a-&gt;instrlen);</span>
 		BUG_ON(a-&gt;instrlen &gt; sizeof(insnbuf));
 		BUG_ON(a-&gt;cpuid &gt;= (NCAPINTS + NBUGINTS) * 32);
<span class="p_del">-		if (!boot_cpu_has(a-&gt;cpuid))</span>
<span class="p_add">+		if (!boot_cpu_has(a-&gt;cpuid)) {</span>
<span class="p_add">+			if (a-&gt;padlen &gt; 1)</span>
<span class="p_add">+				optimize_nops(a, instr);</span>
<span class="p_add">+</span>
 			continue;
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		DPRINTK(&quot;feat: %d*32+%d, old: (%p, len: %d), repl: (%p, len: %d)&quot;,</span>
<span class="p_add">+			a-&gt;cpuid &gt;&gt; 5,</span>
<span class="p_add">+			a-&gt;cpuid &amp; 0x1f,</span>
<span class="p_add">+			instr, a-&gt;instrlen,</span>
<span class="p_add">+			replacement, a-&gt;replacementlen);</span>
<span class="p_add">+</span>
<span class="p_add">+		DUMP_BYTES(instr, a-&gt;instrlen, &quot;%p: old_insn: &quot;, instr);</span>
<span class="p_add">+		DUMP_BYTES(replacement, a-&gt;replacementlen, &quot;%p: rpl_insn: &quot;, replacement);</span>
 
 		memcpy(insnbuf, replacement, a-&gt;replacementlen);
<span class="p_add">+		insnbuf_sz = a-&gt;replacementlen;</span>
 
 		/* 0xe8 is a relative jump; fix the offset. */
<span class="p_del">-		if (*insnbuf == 0xe8 &amp;&amp; a-&gt;replacementlen == 5)</span>
<span class="p_del">-		    *(s32 *)(insnbuf + 1) += replacement - instr;</span>
<span class="p_add">+		if (*insnbuf == 0xe8 &amp;&amp; a-&gt;replacementlen == 5) {</span>
<span class="p_add">+			*(s32 *)(insnbuf + 1) += replacement - instr;</span>
<span class="p_add">+			DPRINTK(&quot;Fix CALL offset: 0x%x, CALL 0x%lx&quot;,</span>
<span class="p_add">+				*(s32 *)(insnbuf + 1),</span>
<span class="p_add">+				(unsigned long)instr + *(s32 *)(insnbuf + 1) + 5);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (a-&gt;replacementlen &amp;&amp; is_jmp(replacement[0]))</span>
<span class="p_add">+			recompute_jump(a, instr, replacement, insnbuf);</span>
 
<span class="p_del">-		add_nops(insnbuf + a-&gt;replacementlen,</span>
<span class="p_del">-			 a-&gt;instrlen - a-&gt;replacementlen);</span>
<span class="p_add">+		if (a-&gt;instrlen &gt; a-&gt;replacementlen) {</span>
<span class="p_add">+			add_nops(insnbuf + a-&gt;replacementlen,</span>
<span class="p_add">+				 a-&gt;instrlen - a-&gt;replacementlen);</span>
<span class="p_add">+			insnbuf_sz += a-&gt;instrlen - a-&gt;replacementlen;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		DUMP_BYTES(insnbuf, insnbuf_sz, &quot;%p: final_insn: &quot;, instr);</span>
 
<span class="p_del">-		text_poke_early(instr, insnbuf, a-&gt;instrlen);</span>
<span class="p_add">+		text_poke_early(instr, insnbuf, insnbuf_sz);</span>
 	}
 }
 
 #ifdef CONFIG_SMP
<span class="p_del">-</span>
 static void alternatives_smp_lock(const s32 *start, const s32 *end,
 				  u8 *text, u8 *text_end)
 {
<span class="p_chunk">@@ -371,8 +485,8 @@</span> <span class="p_context"> void __init_or_module alternatives_smp_module_add(struct module *mod,</span>
 	smp-&gt;locks_end	= locks_end;
 	smp-&gt;text	= text;
 	smp-&gt;text_end	= text_end;
<span class="p_del">-	DPRINTK(&quot;%s: locks %p -&gt; %p, text %p -&gt; %p, name %s\n&quot;,</span>
<span class="p_del">-		__func__, smp-&gt;locks, smp-&gt;locks_end,</span>
<span class="p_add">+	DPRINTK(&quot;locks %p -&gt; %p, text %p -&gt; %p, name %s\n&quot;,</span>
<span class="p_add">+		smp-&gt;locks, smp-&gt;locks_end,</span>
 		smp-&gt;text, smp-&gt;text_end, smp-&gt;name);
 
 	list_add_tail(&amp;smp-&gt;next, &amp;smp_alt_modules);
<span class="p_chunk">@@ -440,7 +554,7 @@</span> <span class="p_context"> int alternatives_text_reserved(void *start, void *end)</span>
 
 	return 0;
 }
<span class="p_del">-#endif</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
 
 #ifdef CONFIG_PARAVIRT
 void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index 80e35e8522e4..6238499e51a2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -762,11 +762,6 @@</span> <span class="p_context"> static unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)</span>
 }
 #endif
 
<span class="p_del">-static void cpu_set_tlb_flushall_shift(struct cpuinfo_x86 *c)</span>
<span class="p_del">-{</span>
<span class="p_del">-	tlb_flushall_shift = 6;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)
 {
 	u32 ebx, eax, ecx, edx;
<span class="p_chunk">@@ -814,8 +809,6 @@</span> <span class="p_context"> static void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)</span>
 		tlb_lli_2m[ENTRIES] = eax &amp; mask;
 
 	tlb_lli_4m[ENTRIES] = tlb_lli_2m[ENTRIES] &gt;&gt; 1;
<span class="p_del">-</span>
<span class="p_del">-	cpu_set_tlb_flushall_shift(c);</span>
 }
 
 static const struct cpu_dev amd_cpu_dev = {
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index 03445346ee0a..4c7dd836304a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -65,6 +65,14 @@</span> <span class="p_context"> static void __init check_fpu(void)</span>
 
 void __init check_bugs(void)
 {
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Regardless of whether PCID is enumerated, the SDM says</span>
<span class="p_add">+	 * that it can&#39;t be enabled in 32-bit mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	identify_boot_cpu();
 #ifndef CONFIG_SMP
 	pr_info(&quot;CPU: &quot;);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index b0f4cfef81c4..274f5a7b27e7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -90,7 +90,7 @@</span> <span class="p_context"> static const struct cpu_dev default_cpu = {</span>
 
 static const struct cpu_dev *this_cpu = &amp;default_cpu;
 
<span class="p_del">-DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page) = { .gdt = {</span>
 #ifdef CONFIG_X86_64
 	/*
 	 * We need valid kernel segments for data and code in long mode too
<span class="p_chunk">@@ -163,6 +163,40 @@</span> <span class="p_context"> static int __init x86_xsaveopt_setup(char *s)</span>
 }
 __setup(&quot;noxsaveopt&quot;, x86_xsaveopt_setup);
 
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+static int __init x86_pcid_setup(char *s)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* require an exact match without trailing characters */</span>
<span class="p_add">+	if (strlen(s))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* do not emit a message if the feature is not present */</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
<span class="p_add">+	pr_info(&quot;nopcid: PCID feature disabled\n&quot;);</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;nopcid&quot;, x86_pcid_setup);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init x86_noinvpcid_setup(char *s)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* noinvpcid doesn&#39;t accept parameters */</span>
<span class="p_add">+	if (s)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* do not emit a message if the feature is not present */</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_INVPCID))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_INVPCID);</span>
<span class="p_add">+	pr_info(&quot;noinvpcid: INVPCID feature disabled\n&quot;);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+early_param(&quot;noinvpcid&quot;, x86_noinvpcid_setup);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;
<span class="p_chunk">@@ -270,7 +304,7 @@</span> <span class="p_context"> __setup(&quot;nosmep&quot;, setup_disable_smep);</span>
 static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_SMEP))
<span class="p_del">-		set_in_cr4(X86_CR4_SMEP);</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_SMEP);</span>
 }
 
 static __init int setup_disable_smap(char *arg)
<span class="p_chunk">@@ -289,13 +323,46 @@</span> <span class="p_context"> static __always_inline void setup_smap(struct cpuinfo_x86 *c)</span>
 
 	if (cpu_has(c, X86_FEATURE_SMAP)) {
 #ifdef CONFIG_X86_SMAP
<span class="p_del">-		set_in_cr4(X86_CR4_SMAP);</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_SMAP);</span>
 #else
<span class="p_del">-		clear_in_cr4(X86_CR4_SMAP);</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_SMAP);</span>
 #endif
 	}
 }
 
<span class="p_add">+static void setup_pcid(struct cpuinfo_x86 *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (cpu_has(c, X86_FEATURE_PCID)) {</span>
<span class="p_add">+		if (cpu_has(c, X86_FEATURE_PGE) || kaiser_enabled) {</span>
<span class="p_add">+			cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * INVPCID has two &quot;groups&quot; of types:</span>
<span class="p_add">+			 * 1/2: Invalidate an individual address</span>
<span class="p_add">+			 * 3/4: Invalidate all contexts</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * 1/2 take a PCID, but 3/4 do not.  So, 3/4</span>
<span class="p_add">+			 * ignore the PCID argument in the descriptor.</span>
<span class="p_add">+			 * But, we have to be careful not to call 1/2</span>
<span class="p_add">+			 * with an actual non-zero PCID in them before</span>
<span class="p_add">+			 * we do the above cr4_set_bits().</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (cpu_has(c, X86_FEATURE_INVPCID))</span>
<span class="p_add">+				set_cpu_cap(c, X86_FEATURE_INVPCID_SINGLE);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * flush_tlb_all(), as currently implemented, won&#39;t</span>
<span class="p_add">+			 * work if PCID is on but PGE is not.  Since that</span>
<span class="p_add">+			 * combination doesn&#39;t exist on real hardware, there&#39;s</span>
<span class="p_add">+			 * no reason to try to fully support it, but it&#39;s</span>
<span class="p_add">+			 * polite to avoid corrupting data if we&#39;re on</span>
<span class="p_add">+			 * an improperly configured VM.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			clear_cpu_cap(c, X86_FEATURE_PCID);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	kaiser_setup_pcid();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Some CPU features depend on higher CPUID levels, which may not always
  * be available due to CPUID level capping or broken virtualization
<span class="p_chunk">@@ -482,26 +549,17 @@</span> <span class="p_context"> u16 __read_mostly tlb_lld_2m[NR_INFO];</span>
 u16 __read_mostly tlb_lld_4m[NR_INFO];
 u16 __read_mostly tlb_lld_1g[NR_INFO];
 
<span class="p_del">-/*</span>
<span class="p_del">- * tlb_flushall_shift shows the balance point in replacing cr3 write</span>
<span class="p_del">- * with multiple &#39;invlpg&#39;. It will do this replacement when</span>
<span class="p_del">- *   flush_tlb_lines &lt;= active_lines/2^tlb_flushall_shift.</span>
<span class="p_del">- * If tlb_flushall_shift is -1, means the replacement will be disabled.</span>
<span class="p_del">- */</span>
<span class="p_del">-s8  __read_mostly tlb_flushall_shift = -1;</span>
<span class="p_del">-</span>
 void cpu_detect_tlb(struct cpuinfo_x86 *c)
 {
 	if (this_cpu-&gt;c_detect_tlb)
 		this_cpu-&gt;c_detect_tlb(c);
 
 	printk(KERN_INFO &quot;Last level iTLB entries: 4KB %d, 2MB %d, 4MB %d\n&quot;
<span class="p_del">-		&quot;Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n&quot;</span>
<span class="p_del">-		&quot;tlb_flushall_shift: %d\n&quot;,</span>
<span class="p_add">+		&quot;Last level dTLB entries: 4KB %d, 2MB %d, 4MB %d, 1GB %d\n&quot;,</span>
 		tlb_lli_4k[ENTRIES], tlb_lli_2m[ENTRIES],
 		tlb_lli_4m[ENTRIES], tlb_lld_4k[ENTRIES],
 		tlb_lld_2m[ENTRIES], tlb_lld_4m[ENTRIES],
<span class="p_del">-		tlb_lld_1g[ENTRIES], tlb_flushall_shift);</span>
<span class="p_add">+		tlb_lld_1g[ENTRIES]);</span>
 }
 
 void detect_ht(struct cpuinfo_x86 *c)
<span class="p_chunk">@@ -886,6 +944,9 @@</span> <span class="p_context"> static void identify_cpu(struct cpuinfo_x86 *c)</span>
 	setup_smep(c);
 	setup_smap(c);
 
<span class="p_add">+	/* Set up PCID */</span>
<span class="p_add">+	setup_pcid(c);</span>
<span class="p_add">+</span>
 	/*
 	 * The vendor-specific functions might have changed features.
 	 * Now we do &quot;generic changes.&quot;
<span class="p_chunk">@@ -1008,6 +1069,11 @@</span> <span class="p_context"> void identify_secondary_cpu(struct cpuinfo_x86 *c)</span>
 	BUG_ON(c == &amp;boot_cpu_data);
 	identify_cpu(c);
 #ifdef CONFIG_X86_32
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Regardless of whether PCID is enumerated, the SDM says</span>
<span class="p_add">+	 * that it can&#39;t be enabled in 32-bit mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	clear_cpu_cap(c, X86_FEATURE_PCID);</span>
 	enable_sep_cpu();
 #endif
 	mtrr_ap_init();
<span class="p_chunk">@@ -1114,7 +1180,7 @@</span> <span class="p_context"> static __init int setup_disablecpuid(char *arg)</span>
 }
 __setup(&quot;clearcpuid=&quot;, setup_disablecpuid);
 
<span class="p_del">-DEFINE_PER_CPU(unsigned long, kernel_stack) =</span>
<span class="p_add">+DEFINE_PER_CPU_USER_MAPPED(unsigned long, kernel_stack) =</span>
 	(unsigned long)&amp;init_thread_union - KERNEL_STACK_OFFSET + THREAD_SIZE;
 EXPORT_PER_CPU_SYMBOL(kernel_stack);
 
<span class="p_chunk">@@ -1155,7 +1221,7 @@</span> <span class="p_context"> static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
 	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
 };
 
<span class="p_del">-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(char, exception_stacks</span>
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 
 /* May not be marked __init: used by software suspend */
<span class="p_chunk">@@ -1277,6 +1343,15 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	int cpu;
 	int i;
 
<span class="p_add">+	if (!kaiser_enabled) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * secondary_startup_64() deferred setting PGE in cr4:</span>
<span class="p_add">+		 * probe_page_size_mask() sets it on the boot cpu,</span>
<span class="p_add">+		 * but it needs to be set on each secondary cpu.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PGE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Load microcode on this cpu if a valid microcode is available.
 	 * This is early microcode loading procedure.
<span class="p_chunk">@@ -1300,7 +1375,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 
 	pr_debug(&quot;Initializing CPU#%d\n&quot;, cpu);
 
<span class="p_del">-	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
<span class="p_add">+	cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
<span class="p_chunk">@@ -1385,7 +1460,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	printk(KERN_INFO &quot;Initializing CPU#%d\n&quot;, cpu);
 
 	if (cpu_has_vme || cpu_has_tsc || cpu_has_de)
<span class="p_del">-		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
 
 	load_current_idt();
 	switch_to_new_gdt(cpu);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index df64630d8053..cfbdac5c601e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -663,31 +663,6 @@</span> <span class="p_context"> static void intel_tlb_lookup(const unsigned char desc)</span>
 	}
 }
 
<span class="p_del">-static void intel_tlb_flushall_shift_set(struct cpuinfo_x86 *c)</span>
<span class="p_del">-{</span>
<span class="p_del">-	switch ((c-&gt;x86 &lt;&lt; 8) + c-&gt;x86_model) {</span>
<span class="p_del">-	case 0x60f: /* original 65 nm celeron/pentium/core2/xeon, &quot;Merom&quot;/&quot;Conroe&quot; */</span>
<span class="p_del">-	case 0x616: /* single-core 65 nm celeron/core2solo &quot;Merom-L&quot;/&quot;Conroe-L&quot; */</span>
<span class="p_del">-	case 0x617: /* current 45 nm celeron/core2/xeon &quot;Penryn&quot;/&quot;Wolfdale&quot; */</span>
<span class="p_del">-	case 0x61d: /* six-core 45 nm xeon &quot;Dunnington&quot; */</span>
<span class="p_del">-		tlb_flushall_shift = -1;</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	case 0x63a: /* Ivybridge */</span>
<span class="p_del">-		tlb_flushall_shift = 2;</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	case 0x61a: /* 45 nm nehalem, &quot;Bloomfield&quot; */</span>
<span class="p_del">-	case 0x61e: /* 45 nm nehalem, &quot;Lynnfield&quot; */</span>
<span class="p_del">-	case 0x625: /* 32 nm nehalem, &quot;Clarkdale&quot; */</span>
<span class="p_del">-	case 0x62c: /* 32 nm nehalem, &quot;Gulftown&quot; */</span>
<span class="p_del">-	case 0x62e: /* 45 nm nehalem-ex, &quot;Beckton&quot; */</span>
<span class="p_del">-	case 0x62f: /* 32 nm Xeon E7 */</span>
<span class="p_del">-	case 0x62a: /* SandyBridge */</span>
<span class="p_del">-	case 0x62d: /* SandyBridge, &quot;Romely-EP&quot; */</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		tlb_flushall_shift = 6;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void intel_detect_tlb(struct cpuinfo_x86 *c)
 {
 	int i, j, n;
<span class="p_chunk">@@ -712,7 +687,6 @@</span> <span class="p_context"> static void intel_detect_tlb(struct cpuinfo_x86 *c)</span>
 		for (j = 1 ; j &lt; 16 ; j++)
 			intel_tlb_lookup(desc[j]);
 	}
<span class="p_del">-	intel_tlb_flushall_shift_set(c);</span>
 }
 
 static const struct cpu_dev intel_cpu_dev = {
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/mce.c b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">index 8ae7e3f27b1b..72cb777b4625 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_chunk">@@ -43,6 +43,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/msr.h&gt;
 
<span class="p_chunk">@@ -1470,7 +1471,7 @@</span> <span class="p_context"> static void __mcheck_cpu_init_generic(void)</span>
 	bitmap_fill(all_banks, MAX_NR_BANKS);
 	machine_check_poll(MCP_UC | m_fl, &amp;all_banks);
 
<span class="p_del">-	set_in_cr4(X86_CR4_MCE);</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_MCE);</span>
 
 	rdmsrl(MSR_IA32_MCG_CAP, cap);
 	if (cap &amp; MCG_CTL_P)
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/p5.c b/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_header">index a3042989398c..30692ac88d1e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/smp.h&gt;
 
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/msr.h&gt;
 
<span class="p_chunk">@@ -59,7 +60,7 @@</span> <span class="p_context"> void intel_p5_mcheck_init(struct cpuinfo_x86 *c)</span>
 	       &quot;Intel old style machine check architecture supported.\n&quot;);
 
 	/* Enable MCE: */
<span class="p_del">-	set_in_cr4(X86_CR4_MCE);</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_MCE);</span>
 	printk(KERN_INFO
 	       &quot;Intel old style machine check reporting enabled on CPU#%d.\n&quot;,
 	       smp_processor_id());
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/winchip.c b/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_header">index 7dc5564d0cdf..590cc753ba8f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/msr.h&gt;
 
<span class="p_chunk">@@ -31,7 +32,7 @@</span> <span class="p_context"> void winchip_mcheck_init(struct cpuinfo_x86 *c)</span>
 	lo &amp;= ~(1&lt;&lt;4);	/* Enable MCE */
 	wrmsr(MSR_IDT_FCR1, lo, hi);
 
<span class="p_del">-	set_in_cr4(X86_CR4_MCE);</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_MCE);</span>
 
 	printk(KERN_INFO
 	       &quot;Winchip machine check reporting enabled on CPU#0.\n&quot;);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">index ec91ccee0661..33f549ccdcf1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_chunk">@@ -31,6 +31,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/nmi.h&gt;
 #include &lt;asm/smp.h&gt;
 #include &lt;asm/alternative.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/timer.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_chunk">@@ -1340,7 +1341,7 @@</span> <span class="p_context"> x86_pmu_notifier(struct notifier_block *self, unsigned long action, void *hcpu)</span>
 
 	case CPU_STARTING:
 		if (x86_pmu.attr_rdpmc)
<span class="p_del">-			set_in_cr4(X86_CR4_PCE);</span>
<span class="p_add">+			cr4_set_bits(X86_CR4_PCE);</span>
 		if (x86_pmu.cpu_starting)
 			x86_pmu.cpu_starting(cpu);
 		break;
<span class="p_chunk">@@ -1855,9 +1856,9 @@</span> <span class="p_context"> static void change_rdpmc(void *info)</span>
 	bool enable = !!(unsigned long)info;
 
 	if (enable)
<span class="p_del">-		set_in_cr4(X86_CR4_PCE);</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PCE);</span>
 	else
<span class="p_del">-		clear_in_cr4(X86_CR4_PCE);</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_PCE);</span>
 }
 
 static ssize_t set_attr_rdpmc(struct device *cdev,
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event_intel_ds.c b/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_header">index bae2bfa2af0e..711135df29d2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_chunk">@@ -2,11 +2,15 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/slab.h&gt;
 
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;asm/perf_event.h&gt;
 #include &lt;asm/insn.h&gt;
 
 #include &quot;perf_event.h&quot;
 
<span class="p_add">+static</span>
<span class="p_add">+DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
 /* The size of a BTS record in bytes: */
 #define BTS_RECORD_SIZE		24
 
<span class="p_chunk">@@ -256,6 +260,39 @@</span> <span class="p_context"> void fini_debug_store_on_cpu(int cpu)</span>
 
 static DEFINE_PER_CPU(void *, insn_buffer);
 
<span class="p_add">+static void *dsalloc(size_t size, gfp_t flags, int node)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	unsigned int order = get_order(size);</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = alloc_pages_node(node, flags | __GFP_ZERO, order);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr = (unsigned long)page_address(page);</span>
<span class="p_add">+	if (kaiser_add_mapping(addr, size, __PAGE_KERNEL) &lt; 0) {</span>
<span class="p_add">+		__free_pages(page, order);</span>
<span class="p_add">+		addr = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return (void *)addr;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return kmalloc_node(size, flags | __GFP_ZERO, node);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dsfree(const void *buffer, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (!buffer)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	kaiser_remove_mapping((unsigned long)buffer, size);</span>
<span class="p_add">+	free_pages((unsigned long)buffer, get_order(size));</span>
<span class="p_add">+#else</span>
<span class="p_add">+	kfree(buffer);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int alloc_pebs_buffer(int cpu)
 {
 	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;
<span class="p_chunk">@@ -266,7 +303,7 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (!x86_pmu.pebs)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
<span class="p_add">+	buffer = dsalloc(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
 	if (unlikely(!buffer))
 		return -ENOMEM;
 
<span class="p_chunk">@@ -277,7 +314,7 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (x86_pmu.intel_cap.pebs_format &lt; 2) {
 		ibuffer = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);
 		if (!ibuffer) {
<span class="p_del">-			kfree(buffer);</span>
<span class="p_add">+			dsfree(buffer, x86_pmu.pebs_buffer_size);</span>
 			return -ENOMEM;
 		}
 		per_cpu(insn_buffer, cpu) = ibuffer;
<span class="p_chunk">@@ -306,7 +343,7 @@</span> <span class="p_context"> static void release_pebs_buffer(int cpu)</span>
 	kfree(per_cpu(insn_buffer, cpu));
 	per_cpu(insn_buffer, cpu) = NULL;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;pebs_buffer_base);</span>
<span class="p_add">+	dsfree((void *)(unsigned long)ds-&gt;pebs_buffer_base, x86_pmu.pebs_buffer_size);</span>
 	ds-&gt;pebs_buffer_base = 0;
 }
 
<span class="p_chunk">@@ -320,7 +357,7 @@</span> <span class="p_context"> static int alloc_bts_buffer(int cpu)</span>
 	if (!x86_pmu.bts)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
<span class="p_add">+	buffer = dsalloc(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
 	if (unlikely(!buffer)) {
 		WARN_ONCE(1, &quot;%s: BTS buffer allocation failure\n&quot;, __func__);
 		return -ENOMEM;
<span class="p_chunk">@@ -346,19 +383,15 @@</span> <span class="p_context"> static void release_bts_buffer(int cpu)</span>
 	if (!ds || !x86_pmu.bts)
 		return;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;bts_buffer_base);</span>
<span class="p_add">+	dsfree((void *)(unsigned long)ds-&gt;bts_buffer_base, BTS_BUFFER_SIZE);</span>
 	ds-&gt;bts_buffer_base = 0;
 }
 
 static int alloc_ds_buffer(int cpu)
 {
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	struct debug_store *ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds = kzalloc_node(sizeof(*ds), GFP_KERNEL, node);</span>
<span class="p_del">-	if (unlikely(!ds))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	struct debug_store *ds = per_cpu_ptr(&amp;cpu_debug_store, cpu);</span>
 
<span class="p_add">+	memset(ds, 0, sizeof(*ds));</span>
 	per_cpu(cpu_hw_events, cpu).ds = ds;
 
 	return 0;
<span class="p_chunk">@@ -372,7 +405,6 @@</span> <span class="p_context"> static void release_ds_buffer(int cpu)</span>
 		return;
 
 	per_cpu(cpu_hw_events, cpu).ds = NULL;
<span class="p_del">-	kfree(ds);</span>
 }
 
 void release_ds_buffers(void)
<span class="p_header">diff --git a/arch/x86/kernel/entry_32.S b/arch/x86/kernel/entry_32.S</span>
<span class="p_header">index 2c123171944a..bfcc300d8b3c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/entry_32.S</span>
<span class="p_chunk">@@ -821,7 +821,7 @@</span> <span class="p_context"> ENTRY(simd_coprocessor_error)</span>
 661:	pushl_cfi $do_general_protection
 662:
 .section .altinstructions,&quot;a&quot;
<span class="p_del">-	altinstruction_entry 661b, 663f, X86_FEATURE_XMM, 662b-661b, 664f-663f</span>
<span class="p_add">+	altinstruction_entry 661b, 663f, X86_FEATURE_XMM, 662b-661b, 664f-663f, 0</span>
 .previous
 .section .altinstr_replacement,&quot;ax&quot;
 663:	pushl $do_simd_coprocessor_error
<span class="p_header">diff --git a/arch/x86/kernel/entry_64.S b/arch/x86/kernel/entry_64.S</span>
<span class="p_header">index 42a970d19ab3..5dc1043544ad 100644</span>
<span class="p_header">--- a/arch/x86/kernel/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/entry_64.S</span>
<span class="p_chunk">@@ -58,6 +58,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/context_tracking.h&gt;
 #include &lt;asm/smap.h&gt;
 #include &lt;asm/pgtable_types.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;linux/err.h&gt;
 
 /* Avoid __ASSEMBLER__&#39;ifying &lt;linux/audit.h&gt; just for this.  */
<span class="p_chunk">@@ -263,6 +264,7 @@</span> <span class="p_context"> ENDPROC(native_usergs_sysret64)</span>
 	testl $3, CS-RBP(%rsi)
 	je 1f
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 	/*
 	 * irq_count is used to check if a CPU is already on an interrupt stack
 	 * or not. While this is essentially redundant with preempt_count it is
<span class="p_chunk">@@ -284,6 +286,12 @@</span> <span class="p_context"> ENDPROC(native_usergs_sysret64)</span>
 	TRACE_IRQS_OFF
 	.endm
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Return: ebx=0: needs swapgs but not SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=1: needs neither swapgs nor SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=2: needs both swapgs and SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=3: needs SWITCH_USER_CR3 but not swapgs in paranoid_exit</span>
<span class="p_add">+ */</span>
 ENTRY(save_paranoid)
 	XCPT_FRAME 1 RDI+8
 	cld
<span class="p_chunk">@@ -309,7 +317,26 @@</span> <span class="p_context"> ENTRY(save_paranoid)</span>
 	js 1f	/* negative -&gt; in kernel */
 	SWAPGS
 	xorl %ebx,%ebx
<span class="p_del">-1:	ret</span>
<span class="p_add">+1:</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We might have come in between a swapgs and a SWITCH_KERNEL_CR3</span>
<span class="p_add">+	 * on entry, or between a SWITCH_USER_CR3 and a swapgs on exit.</span>
<span class="p_add">+	 * Do a conditional SWITCH_KERNEL_CR3: this could safely be done</span>
<span class="p_add">+	 * unconditionally, but we need to find out whether the reverse</span>
<span class="p_add">+	 * should be done on return (conveyed to paranoid_exit in %ebx).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	testl	$KAISER_SHADOW_PGD_OFFSET, %eax</span>
<span class="p_add">+	jz	2f</span>
<span class="p_add">+	orl	$2, %ebx</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	ret</span>
 	CFI_ENDPROC
 END(save_paranoid)
 
<span class="p_chunk">@@ -394,6 +421,7 @@</span> <span class="p_context"> ENTRY(system_call)</span>
 	CFI_REGISTER	rip,rcx
 	/*CFI_REGISTER	rflags,r11*/
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	/*
 	 * A hypervisor implementation might want to use a label
 	 * after the swapgs, so that it can do the swapgs
<span class="p_chunk">@@ -448,6 +476,14 @@</span> <span class="p_context"> GLOBAL(system_call_after_swapgs)</span>
 	CFI_REGISTER	rip,rcx
 	RESTORE_ARGS 1,-ARG_SKIP,0
 	/*CFI_REGISTER	rflags,r11*/
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This opens a window where we have a user CR3, but are</span>
<span class="p_add">+	 * running in the kernel.  This makes using the CS</span>
<span class="p_add">+	 * register useless for telling whether or not we need to</span>
<span class="p_add">+	 * switch CR3 in NMIs.  Normal interrupts are OK because</span>
<span class="p_add">+	 * they are off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
 	movq	PER_CPU_VAR(old_rsp), %rsp
 	USERGS_SYSRET64
 
<span class="p_chunk">@@ -820,6 +856,14 @@</span> <span class="p_context"> retint_swapgs:		/* return to user-space */</span>
 	 */
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_IRETQ
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This opens a window where we have a user CR3, but are</span>
<span class="p_add">+	 * running in the kernel.  This makes using the CS</span>
<span class="p_add">+	 * register useless for telling whether or not we need to</span>
<span class="p_add">+	 * switch CR3 in NMIs.  Normal interrupts are OK because</span>
<span class="p_add">+	 * they are off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	jmp restore_args
 
<span class="p_chunk">@@ -860,6 +904,7 @@</span> <span class="p_context"> ENTRY(native_iret)</span>
 	pushq_cfi %rax
 	pushq_cfi %rdi
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 	movq PER_CPU_VAR(espfix_waddr),%rdi
 	movq %rax,(0*8)(%rdi)	/* RAX */
 	movq (2*8)(%rsp),%rax	/* RIP */
<span class="p_chunk">@@ -875,6 +920,7 @@</span> <span class="p_context"> ENTRY(native_iret)</span>
 	andl $0xffff0000,%eax
 	popq_cfi %rdi
 	orq PER_CPU_VAR(espfix_stack),%rax
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	movq %rax,%rsp
 	popq_cfi %rax
<span class="p_chunk">@@ -1289,30 +1335,41 @@</span> <span class="p_context"> idtentry machine_check has_error_code=0 paranoid=1 do_sym=*machine_check_vector(</span>
 	 * is fundamentally NMI-unsafe. (we cannot change the soft and
 	 * hard flags at once, atomically)
 	 */
<span class="p_del">-</span>
<span class="p_del">-	/* ebx:	no swapgs flag */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * On entry: ebx=0: needs swapgs but not SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=1: needs neither swapgs nor SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=2: needs both swapgs and SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=3: needs SWITCH_USER_CR3 but not swapgs</span>
<span class="p_add">+ */</span>
 ENTRY(paranoid_exit)
 	DEFAULT_FRAME
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF_DEBUG
<span class="p_del">-	testl %ebx,%ebx				/* swapgs needed? */</span>
<span class="p_del">-	jnz paranoid_restore</span>
<span class="p_del">-	testl $3,CS(%rsp)</span>
<span class="p_del">-	jnz   paranoid_userspace</span>
<span class="p_del">-paranoid_swapgs:</span>
<span class="p_del">-	TRACE_IRQS_IRETQ 0</span>
<span class="p_del">-	SWAPGS_UNSAFE_STACK</span>
<span class="p_del">-	RESTORE_ALL 8</span>
<span class="p_del">-	jmp irq_return</span>
<span class="p_del">-paranoid_restore:</span>
<span class="p_add">+	movq	%rbx, %r12		/* paranoid_userspace uses %ebx */</span>
<span class="p_add">+	testl	$3, CS(%rsp)</span>
<span class="p_add">+	jnz	paranoid_userspace</span>
<span class="p_add">+paranoid_kernel:</span>
<span class="p_add">+	movq	%r12, %rbx		/* restore after paranoid_userspace */</span>
 	TRACE_IRQS_IRETQ_DEBUG 0
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* No ALTERNATIVE for X86_FEATURE_KAISER: paranoid_entry sets %ebx */</span>
<span class="p_add">+	testl	$2, %ebx			/* SWITCH_USER_CR3 needed? */</span>
<span class="p_add">+	jz	paranoid_exit_no_switch</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
<span class="p_add">+paranoid_exit_no_switch:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	testl	$1, %ebx			/* swapgs needed? */</span>
<span class="p_add">+	jnz	paranoid_exit_no_swapgs</span>
<span class="p_add">+	SWAPGS_UNSAFE_STACK</span>
<span class="p_add">+paranoid_exit_no_swapgs:</span>
 	RESTORE_ALL 8
<span class="p_del">-	jmp irq_return</span>
<span class="p_add">+	jmp	irq_return</span>
<span class="p_add">+</span>
 paranoid_userspace:
 	GET_THREAD_INFO(%rcx)
 	movl TI_flags(%rcx),%ebx
 	andl $_TIF_WORK_MASK,%ebx
<span class="p_del">-	jz paranoid_swapgs</span>
<span class="p_add">+	jz paranoid_kernel</span>
 	movq %rsp,%rdi			/* &amp;pt_regs */
 	call sync_regs
 	movq %rax,%rsp			/* switch stack for scheduling */
<span class="p_chunk">@@ -1361,6 +1418,13 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	movq_cfi r13, R13+8
 	movq_cfi r14, R14+8
 	movq_cfi r15, R15+8
<span class="p_add">+	/*</span>
<span class="p_add">+	 * error_entry() always returns with a kernel gsbase and</span>
<span class="p_add">+	 * CR3.  We must also have a kernel CR3/gsbase before</span>
<span class="p_add">+	 * calling TRACE_IRQS_*.  Just unconditionally switch to</span>
<span class="p_add">+	 * the kernel CR3 here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 	xorl %ebx,%ebx
 	testl $3,CS+8(%rsp)
 	je error_kernelspace
<span class="p_chunk">@@ -1497,6 +1561,10 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	 */
 
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	/*</span>
<span class="p_add">+	 * percpu variables are mapped with user CR3, so no need</span>
<span class="p_add">+	 * to switch CR3 here.</span>
<span class="p_add">+	 */</span>
 	cld
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(kernel_stack), %rsp
<span class="p_chunk">@@ -1531,12 +1599,34 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* Unconditionally use kernel CR3 for do_nmi() */</span>
<span class="p_add">+	/* %rax is saved above, so OK to clobber here */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
 	call	do_nmi
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Unconditionally restore CR3.  I know we return to</span>
<span class="p_add">+	 * kernel code that needs user CR3, but do we ever return</span>
<span class="p_add">+	 * to &quot;user mode&quot; where we need the kernel CR3?</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;popq %rax; movq %rax, %cr3&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/*
 	 * Return back to user mode.  We must *not* do the normal exit
<span class="p_del">-	 * work, because we don&#39;t want to enable interrupts.  Fortunately,</span>
<span class="p_del">-	 * do_nmi doesn&#39;t modify pt_regs.</span>
<span class="p_add">+	 * work, because we don&#39;t want to enable interrupts.  Do not</span>
<span class="p_add">+	 * switch to user CR3: we might be going back to kernel code</span>
<span class="p_add">+	 * that had a user CR3 set.</span>
 	 */
 	SWAPGS
 
<span class="p_chunk">@@ -1746,23 +1836,69 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	subq $ORIG_RAX-R15, %rsp
 	CFI_ADJUST_CFA_OFFSET ORIG_RAX-R15
 	/*
<span class="p_del">-	 * Use save_paranoid to handle SWAPGS, but no need to use paranoid_exit</span>
<span class="p_del">-	 * as we should not be calling schedule in NMI context.</span>
<span class="p_del">-	 * Even with normal interrupts enabled. An NMI should not be</span>
<span class="p_del">-	 * setting NEED_RESCHED or anything that normal interrupts and</span>
<span class="p_del">-	 * exceptions might do.</span>
<span class="p_add">+	 * Use the same approach as save_paranoid to handle SWAPGS, but</span>
<span class="p_add">+	 * without CR3 handling since we do that differently in NMIs.  No</span>
<span class="p_add">+	 * need to use paranoid_exit as we should not be calling schedule</span>
<span class="p_add">+	 * in NMI context.  Even with normal interrupts enabled. An NMI</span>
<span class="p_add">+	 * should not be setting NEED_RESCHED or anything that normal</span>
<span class="p_add">+	 * interrupts and exceptions might do.</span>
 	 */
<span class="p_del">-	call save_paranoid</span>
<span class="p_del">-	DEFAULT_FRAME 0</span>
<span class="p_add">+	cld</span>
<span class="p_add">+	movq	%rdi, RDI(%rsp)</span>
<span class="p_add">+	movq	%rsi, RSI(%rsp)</span>
<span class="p_add">+	movq_cfi rdx, RDX</span>
<span class="p_add">+	movq_cfi rcx, RCX</span>
<span class="p_add">+	movq_cfi rax, RAX</span>
<span class="p_add">+	movq	%r8, R8(%rsp)</span>
<span class="p_add">+	movq	%r9, R9(%rsp)</span>
<span class="p_add">+	movq	%r10, R10(%rsp)</span>
<span class="p_add">+	movq	%r11, R11(%rsp)</span>
<span class="p_add">+	movq_cfi rbx, RBX</span>
<span class="p_add">+	movq	%rbp, RBP(%rsp)</span>
<span class="p_add">+	movq	%r12, R12(%rsp)</span>
<span class="p_add">+	movq	%r13, R13(%rsp)</span>
<span class="p_add">+	movq	%r14, R14(%rsp)</span>
<span class="p_add">+	movq	%r15, R15(%rsp)</span>
<span class="p_add">+	movl	$1, %ebx</span>
<span class="p_add">+	movl	$MSR_GS_BASE, %ecx</span>
<span class="p_add">+	rdmsr</span>
<span class="p_add">+	testl	%edx, %edx</span>
<span class="p_add">+	js	1f				/* negative -&gt; in kernel */</span>
<span class="p_add">+	SWAPGS</span>
<span class="p_add">+	xorl %ebx,%ebx</span>
<span class="p_add">+1:</span>
<span class="p_add">+	movq	%rsp,%rdi</span>
<span class="p_add">+	movq	$-1,%rsi</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* Unconditionally use kernel CR3 for do_nmi() */</span>
<span class="p_add">+	/* %rax is saved above, so OK to clobber here */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	DEFAULT_FRAME 0		/* ???? */</span>
 
 	/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */
<span class="p_del">-	movq %rsp,%rdi</span>
<span class="p_del">-	movq $-1,%rsi</span>
<span class="p_del">-	call do_nmi</span>
<span class="p_add">+	call	do_nmi</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Unconditionally restore CR3.  We might be returning to</span>
<span class="p_add">+	 * kernel code that needs user CR3, like just just before</span>
<span class="p_add">+	 * a sysret.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;popq %rax; movq %rax, %cr3&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+#endif</span>
 
 	testl %ebx,%ebx				/* swapgs needed? */
 	jnz nmi_restore
 nmi_swapgs:
<span class="p_add">+	/* We fixed up CR3 above, so no need to switch it here */</span>
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
 
<span class="p_header">diff --git a/arch/x86/kernel/espfix_64.c b/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">index 94d857fb1033..a1944faa739c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/espfix.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * Note: we only need 6*8 = 48 bytes for the espfix stack, but round
<span class="p_chunk">@@ -129,6 +130,15 @@</span> <span class="p_context"> void __init init_espfix_bsp(void)</span>
 	/* Install the espfix pud into the kernel page directory */
 	pgd_p = &amp;init_level4_pgt[pgd_index(ESPFIX_BASE_ADDR)];
 	pgd_populate(&amp;init_mm, pgd_p, (pud_t *)espfix_pud_page);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Just copy the top-level PGD that is mapping the espfix</span>
<span class="p_add">+	 * area to ensure it is mapped into the shadow user page</span>
<span class="p_add">+	 * tables.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		set_pgd(native_get_shadow_pgd(pgd_p),</span>
<span class="p_add">+			__pgd(_KERNPG_TABLE | __pa((pud_t *)espfix_pud_page)));</span>
<span class="p_add">+	}</span>
 
 	/* Randomize the locations */
 	init_espfix_random();
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 761fd69df6d9..8c3cdb9ba14b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -183,8 +183,8 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	movq	$(init_level4_pgt - __START_KERNEL_map), %rax
 1:
 
<span class="p_del">-	/* Enable PAE mode and PGE */</span>
<span class="p_del">-	movl	$(X86_CR4_PAE | X86_CR4_PGE), %ecx</span>
<span class="p_add">+	/* Enable PAE and PSE, but defer PGE until kaiser_enabled is decided */</span>
<span class="p_add">+	movl	$(X86_CR4_PAE | X86_CR4_PSE), %ecx</span>
 	movq	%rcx, %cr4
 
 	/* Setup early boot stage 4 level pagetables. */
<span class="p_chunk">@@ -441,6 +441,27 @@</span> <span class="p_context"> ENDPROC(early_idt_handler_common)</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Each PGD needs to be 8k long and 8k aligned.  We do not</span>
<span class="p_add">+ * ever go out to userspace with these, so we do not</span>
<span class="p_add">+ * strictly *need* the second page, but this allows us to</span>
<span class="p_add">+ * have a single set_pgd() implementation that does not</span>
<span class="p_add">+ * need to worry about whether it has 4k or 8k to work</span>
<span class="p_add">+ * with.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures PGDs are 8k long:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KAISER_USER_PGD_FILL	512</span>
<span class="p_add">+/* This ensures they are 8k-aligned: */</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#define KAISER_USER_PGD_FILL	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -450,9 +471,10 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_level4_pgt)</span>
 	.fill	511,8,0
 	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(early_dynamic_pgts)
 	.fill	512*EARLY_DYNAMIC_PAGE_TABLES,8,0
<span class="p_chunk">@@ -460,16 +482,18 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #ifndef CONFIG_XEN
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_level4_pgt)</span>
 	.fill	512,8,0
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 #else
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_level4_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
 	.org    init_level4_pgt + L4_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
 	.org    init_level4_pgt + L4_START_KERNEL*8, 0
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
 	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_ident_pgt)
 	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
<span class="p_chunk">@@ -480,6 +504,7 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #endif
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_kernel_pgt)
 	.fill	L3_START_KERNEL,8,0
<span class="p_header">diff --git a/arch/x86/kernel/i387.c b/arch/x86/kernel/i387.c</span>
<span class="p_header">index 5d7acb77cc1d..99df035d7cb1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/i387.c</span>
<span class="p_header">+++ b/arch/x86/kernel/i387.c</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/sigcontext.h&gt;
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/math_emu.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/uaccess.h&gt;
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/i387.h&gt;
<span class="p_chunk">@@ -181,7 +182,7 @@</span> <span class="p_context"> void fpu_init(void)</span>
 	if (cpu_has_xmm)
 		cr4_mask |= X86_CR4_OSXMMEXCPT;
 	if (cr4_mask)
<span class="p_del">-		set_in_cr4(cr4_mask);</span>
<span class="p_add">+		cr4_set_bits(cr4_mask);</span>
 
 	cr0 = read_cr0();
 	cr0 &amp;= ~(X86_CR0_TS|X86_CR0_EM); /* clear TS and EM */
<span class="p_header">diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c</span>
<span class="p_header">index 37907756fc41..eedac3e88156 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq.c</span>
<span class="p_chunk">@@ -96,8 +96,7 @@</span> <span class="p_context"> int arch_show_interrupts(struct seq_file *p, int prec)</span>
 	seq_printf(p, &quot;  Rescheduling interrupts\n&quot;);
 	seq_printf(p, &quot;%*s: &quot;, prec, &quot;CAL&quot;);
 	for_each_online_cpu(j)
<span class="p_del">-		seq_printf(p, &quot;%10u &quot;, irq_stats(j)-&gt;irq_call_count -</span>
<span class="p_del">-					irq_stats(j)-&gt;irq_tlb_count);</span>
<span class="p_add">+		seq_printf(p, &quot;%10u &quot;, irq_stats(j)-&gt;irq_call_count);</span>
 	seq_printf(p, &quot;  Function call interrupts\n&quot;);
 	seq_printf(p, &quot;%*s: &quot;, prec, &quot;TLB&quot;);
 	for_each_online_cpu(j)
<span class="p_header">diff --git a/arch/x86/kernel/irqinit.c b/arch/x86/kernel/irqinit.c</span>
<span class="p_header">index 7f50156542fb..31045946cfe9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irqinit.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irqinit.c</span>
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> static struct irqaction irq2 = {</span>
 	.flags = IRQF_NO_THREAD,
 };
 
<span class="p_del">-DEFINE_PER_CPU(vector_irq_t, vector_irq) = {</span>
<span class="p_add">+DEFINE_PER_CPU_USER_MAPPED(vector_irq_t, vector_irq) = {</span>
 	[0 ... NR_VECTORS - 1] = VECTOR_UNDEFINED,
 };
 
<span class="p_header">diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">index a2de9bc7ac0b..206aa5a2afe0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvmclock.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/percpu.h&gt;
 #include &lt;linux/hardirq.h&gt;
 #include &lt;linux/memblock.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 
 #include &lt;asm/x86_init.h&gt;
 #include &lt;asm/reboot.h&gt;
<span class="p_chunk">@@ -281,6 +282,10 @@</span> <span class="p_context"> int __init kvm_setup_vsyscall_timeinfo(void)</span>
 	if (!hv_clock)
 		return 0;
 
<span class="p_add">+	/* FIXME: Need to add pvclock pages to user-space page tables */</span>
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	size = PAGE_ALIGN(sizeof(struct pvclock_vsyscall_time_info)*NR_CPUS);
 
 	preempt_disable();
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index 2bcc0525f1c1..7bfd9ae0d228 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/uaccess.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 
 #include &lt;asm/ldt.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_chunk">@@ -33,11 +34,21 @@</span> <span class="p_context"> static void flush_ldt(void *current_mm)</span>
 	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;size);
 }
 
<span class="p_add">+static void __free_ldt_struct(struct ldt_struct *ldt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ldt-&gt;size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_add">+		vfree(ldt-&gt;entries);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		free_page((unsigned long)ldt-&gt;entries);</span>
<span class="p_add">+	kfree(ldt);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* The caller must call finalize_ldt_struct on the result. LDT starts zeroed. */
 static struct ldt_struct *alloc_ldt_struct(int size)
 {
 	struct ldt_struct *new_ldt;
 	int alloc_size;
<span class="p_add">+	int ret;</span>
 
 	if (size &gt; LDT_ENTRIES)
 		return NULL;
<span class="p_chunk">@@ -58,14 +69,20 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(int size)</span>
 	if (alloc_size &gt; PAGE_SIZE)
 		new_ldt-&gt;entries = vzalloc(alloc_size);
 	else
<span class="p_del">-		new_ldt-&gt;entries = kzalloc(PAGE_SIZE, GFP_KERNEL);</span>
<span class="p_add">+		new_ldt-&gt;entries = (void *)get_zeroed_page(GFP_KERNEL);</span>
 
 	if (!new_ldt-&gt;entries) {
 		kfree(new_ldt);
 		return NULL;
 	}
 
<span class="p_add">+	ret = kaiser_add_mapping((unsigned long)new_ldt-&gt;entries, alloc_size,</span>
<span class="p_add">+				 __PAGE_KERNEL);</span>
 	new_ldt-&gt;size = size;
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		__free_ldt_struct(new_ldt);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
 	return new_ldt;
 }
 
<span class="p_chunk">@@ -91,12 +108,10 @@</span> <span class="p_context"> static void free_ldt_struct(struct ldt_struct *ldt)</span>
 	if (likely(!ldt))
 		return;
 
<span class="p_add">+	kaiser_remove_mapping((unsigned long)ldt-&gt;entries,</span>
<span class="p_add">+			      ldt-&gt;size * LDT_ENTRY_SIZE);</span>
 	paravirt_free_ldt(ldt-&gt;entries, ldt-&gt;size);
<span class="p_del">-	if (ldt-&gt;size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-		vfree(ldt-&gt;entries);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		kfree(ldt-&gt;entries);</span>
<span class="p_del">-	kfree(ldt);</span>
<span class="p_add">+	__free_ldt_struct(ldt);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index a1da6737ba5b..a91d9b9b4bde 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -9,7 +9,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_irq_ops, save_fl, &quot;pushfq; popq %rax&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;movq %cr2, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;movq %cr3, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;movq %rdi, %cr3&quot;);
<span class="p_del">-DEF_NATIVE(pv_mmu_ops, flush_tlb_single, &quot;invlpg (%rdi)&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 DEF_NATIVE(pv_cpu_ops, wbinvd, &quot;wbinvd&quot;);
 
<span class="p_chunk">@@ -57,7 +56,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
<span class="p_del">-		PATCH_SITE(pv_mmu_ops, flush_tlb_single);</span>
 		PATCH_SITE(pv_cpu_ops, wbinvd);
 
 	patch_site:
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index c98cff71760d..ee6014b2c43b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/fpu-internal.h&gt;
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/nmi.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 
 /*
  * per-CPU TSS segments. Threads are completely &#39;soft&#39; on Linux,
<span class="p_chunk">@@ -37,7 +38,7 @@</span> <span class="p_context"></span>
  * section. Since TSS&#39;s are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
<span class="p_del">-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;</span>
<span class="p_add">+__visible DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, init_tss) = INIT_TSS;</span>
 
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
<span class="p_chunk">@@ -139,7 +140,7 @@</span> <span class="p_context"> void flush_thread(void)</span>
 
 static void hard_disable_TSC(void)
 {
<span class="p_del">-	write_cr4(read_cr4() | X86_CR4_TSD);</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_TSD);</span>
 }
 
 void disable_TSC(void)
<span class="p_chunk">@@ -156,7 +157,7 @@</span> <span class="p_context"> void disable_TSC(void)</span>
 
 static void hard_enable_TSC(void)
 {
<span class="p_del">-	write_cr4(read_cr4() &amp; ~X86_CR4_TSD);</span>
<span class="p_add">+	cr4_clear_bits(X86_CR4_TSD);</span>
 }
 
 static void enable_TSC(void)
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index a6f51ad11d89..838d259bca74 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"></span>
 
 asmlinkage extern void ret_from_fork(void);
 
<span class="p_del">-__visible DEFINE_PER_CPU(unsigned long, old_rsp);</span>
<span class="p_add">+__visible DEFINE_PER_CPU_USER_MAPPED(unsigned long, old_rsp);</span>
 
 /* Prints also some state that isn&#39;t saved in the pt_regs */
 void __show_regs(struct pt_regs *regs, int all)
<span class="p_header">diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c</span>
<span class="p_header">index 5e8c0f1c99c4..281aa6bcafd9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/reboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/reboot.c</span>
<span class="p_chunk">@@ -92,6 +92,10 @@</span> <span class="p_context"> void __noreturn machine_real_restart(unsigned int type)</span>
 	load_cr3(initial_page_table);
 #else
 	write_cr3(real_mode_header-&gt;trampoline_pgd);
<span class="p_add">+</span>
<span class="p_add">+	/* Exiting long mode will fail if CR4.PCIDE is set. */</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_PCIDE);</span>
 #endif
 
 	/* Jump to the identity-mapped low memory code */
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index b1077b8ee7af..28373e51b94a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -110,6 +110,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/prom.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * max_low_pfn_mapped: highest direct mapped pfn under 4GB
<span class="p_chunk">@@ -1019,6 +1020,12 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 	 */
 	init_hypervisor_platform();
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This needs to happen right after XENPV is set on xen and</span>
<span class="p_add">+	 * kaiser_enabled is checked below in cleanup_highmap().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_check_boottime_disable();</span>
<span class="p_add">+</span>
 	x86_init.resources.probe_roms();
 
 	/* after parse_early_param, so could debug it */
<span class="p_header">diff --git a/arch/x86/kernel/tracepoint.c b/arch/x86/kernel/tracepoint.c</span>
<span class="p_header">index 1c113db9ed57..2bb5ee464df3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tracepoint.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tracepoint.c</span>
<span class="p_chunk">@@ -9,10 +9,12 @@</span> <span class="p_context"></span>
 #include &lt;linux/atomic.h&gt;
 
 atomic_t trace_idt_ctr = ATOMIC_INIT(0);
<span class="p_add">+__aligned(PAGE_SIZE)</span>
 struct desc_ptr trace_idt_descr = { NR_VECTORS * 16 - 1,
 				(unsigned long) trace_idt_table };
 
 /* No need to be aligned, but done to keep all IDTs defined the same way. */
<span class="p_add">+__aligned(PAGE_SIZE)</span>
 gate_desc trace_idt_table[NR_VECTORS] __page_aligned_bss;
 
 static int trace_irq_vector_refcount;
<span class="p_header">diff --git a/arch/x86/kernel/vm86_32.c b/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">index e8edcf52e069..3dd3508ad297 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vm86_32.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> static void mark_screen_rdonly(struct mm_struct *mm)</span>
 	pte_unmap_unlock(pte, ptl);
 out:
 	up_write(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	flush_tlb();</span>
<span class="p_add">+	flush_tlb_mm_range(mm, 0xA0000, 0xA0000 + 32*PAGE_SIZE, 0UL);</span>
 }
 
 
<span class="p_header">diff --git a/arch/x86/kernel/vsyscall_64.c b/arch/x86/kernel/vsyscall_64.c</span>
<span class="p_header">index e1e1e80fc6a6..5a615d801815 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vsyscall_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vsyscall_64.c</span>
<span class="p_chunk">@@ -55,6 +55,7 @@</span> <span class="p_context"></span>
 DEFINE_VVAR(int, vgetcpu_mode);
 
 static enum { EMULATE, NATIVE, NONE } vsyscall_mode = EMULATE;
<span class="p_add">+unsigned long vsyscall_pgprot = __PAGE_KERNEL_VSYSCALL;</span>
 
 static int __init vsyscall_setup(char *str)
 {
<span class="p_chunk">@@ -75,6 +76,11 @@</span> <span class="p_context"> static int __init vsyscall_setup(char *str)</span>
 }
 early_param(&quot;vsyscall&quot;, vsyscall_setup);
 
<span class="p_add">+bool vsyscall_enabled(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return vsyscall_mode != NONE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void warn_bad_vsyscall(const char *level, struct pt_regs *regs,
 			      const char *message)
 {
<span class="p_chunk">@@ -331,10 +337,10 @@</span> <span class="p_context"> void __init map_vsyscall(void)</span>
 	extern char __vsyscall_page;
 	unsigned long physaddr_vsyscall = __pa_symbol(&amp;__vsyscall_page);
 
<span class="p_add">+	if (vsyscall_mode != NATIVE)</span>
<span class="p_add">+		vsyscall_pgprot = __PAGE_KERNEL_VVAR;</span>
 	__set_fixmap(VSYSCALL_PAGE, physaddr_vsyscall,
<span class="p_del">-		     vsyscall_mode == NATIVE</span>
<span class="p_del">-		     ? PAGE_KERNEL_VSYSCALL</span>
<span class="p_del">-		     : PAGE_KERNEL_VVAR);</span>
<span class="p_add">+		     __pgprot(vsyscall_pgprot));</span>
 	BUILD_BUG_ON((unsigned long)__fix_to_virt(VSYSCALL_PAGE) !=
 		     (unsigned long)VSYSCALL_ADDR);
 }
<span class="p_header">diff --git a/arch/x86/kernel/xsave.c b/arch/x86/kernel/xsave.c</span>
<span class="p_header">index beddb0344d52..cfc2dbffa16c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/xsave.c</span>
<span class="p_header">+++ b/arch/x86/kernel/xsave.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/i387.h&gt;
 #include &lt;asm/fpu-internal.h&gt;
 #include &lt;asm/sigframe.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/xcr.h&gt;
 
 /*
<span class="p_chunk">@@ -450,7 +451,7 @@</span> <span class="p_context"> static void prepare_fx_sw_frame(void)</span>
  */
 static inline void xstate_enable(void)
 {
<span class="p_del">-	set_in_cr4(X86_CR4_OSXSAVE);</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_OSXSAVE);</span>
 	xsetbv(XCR_XFEATURE_ENABLED_MASK, pcntxt_mask);
 }
 
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index f9a6d8d68720..6f6f77a5145c 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -2738,7 +2738,7 @@</span> <span class="p_context"> static int hardware_enable(void *garbage)</span>
 		/* enable and lock */
 		wrmsrl(MSR_IA32_FEATURE_CONTROL, old | test_bits);
 	}
<span class="p_del">-	write_cr4(read_cr4() | X86_CR4_VMXE); /* FIXME: not cpu hotplug safe */</span>
<span class="p_add">+	cr4_set_bits(X86_CR4_VMXE);</span>
 
 	if (vmm_exclusive) {
 		kvm_cpu_vmxon(phys_addr);
<span class="p_chunk">@@ -2775,7 +2775,7 @@</span> <span class="p_context"> static void hardware_disable(void *garbage)</span>
 		vmclear_local_loaded_vmcss();
 		kvm_cpu_vmxoff();
 	}
<span class="p_del">-	write_cr4(read_cr4() &amp; ~X86_CR4_VMXE);</span>
<span class="p_add">+	cr4_clear_bits(X86_CR4_VMXE);</span>
 }
 
 static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 2d7ccb5a2bdd..d0847b33282f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -688,7 +688,8 @@</span> <span class="p_context"> int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)</span>
 			return 1;
 
 		/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */
<span class="p_del">-		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_MASK) || !is_long_mode(vcpu))</span>
<span class="p_add">+		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_ASID_MASK) ||</span>
<span class="p_add">+		    !is_long_mode(vcpu))</span>
 			return 1;
 	}
 
<span class="p_header">diff --git a/arch/x86/lib/clear_page_64.S b/arch/x86/lib/clear_page_64.S</span>
<span class="p_header">index f2145cfa12a6..38e57faefd71 100644</span>
<span class="p_header">--- a/arch/x86/lib/clear_page_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/clear_page_64.S</span>
<span class="p_chunk">@@ -67,7 +67,7 @@</span> <span class="p_context"> ENDPROC(clear_page)</span>
 	.previous
 	.section .altinstructions,&quot;a&quot;
 	altinstruction_entry clear_page,1b,X86_FEATURE_REP_GOOD,\
<span class="p_del">-			     .Lclear_page_end-clear_page, 2b-1b</span>
<span class="p_add">+			     .Lclear_page_end-clear_page, 2b-1b, 0</span>
 	altinstruction_entry clear_page,2b,X86_FEATURE_ERMS,   \
<span class="p_del">-			     .Lclear_page_end-clear_page,3b-2b</span>
<span class="p_add">+			     .Lclear_page_end-clear_page,3b-2b, 0</span>
 	.previous
<span class="p_header">diff --git a/arch/x86/lib/cmdline.c b/arch/x86/lib/cmdline.c</span>
<span class="p_header">index 422db000d727..3261abb21ef4 100644</span>
<span class="p_header">--- a/arch/x86/lib/cmdline.c</span>
<span class="p_header">+++ b/arch/x86/lib/cmdline.c</span>
<span class="p_chunk">@@ -21,12 +21,16 @@</span> <span class="p_context"> static inline int myisspace(u8 c)</span>
  * @option: option string to look for
  *
  * Returns the position of that @option (starts counting with 1)
<span class="p_del">- * or 0 on not found.</span>
<span class="p_add">+ * or 0 on not found.  @option will only be found if it is found</span>
<span class="p_add">+ * as an entire word in @cmdline.  For instance, if @option=&quot;car&quot;</span>
<span class="p_add">+ * then a cmdline which contains &quot;cart&quot; will not match.</span>
  */
<span class="p_del">-int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
<span class="p_add">+static int</span>
<span class="p_add">+__cmdline_find_option_bool(const char *cmdline, int max_cmdline_size,</span>
<span class="p_add">+			   const char *option)</span>
 {
 	char c;
<span class="p_del">-	int len, pos = 0, wstart = 0;</span>
<span class="p_add">+	int pos = 0, wstart = 0;</span>
 	const char *opptr = NULL;
 	enum {
 		st_wordstart = 0,	/* Start of word/after whitespace */
<span class="p_chunk">@@ -37,11 +41,11 @@</span> <span class="p_context"> int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
 	if (!cmdline)
 		return -1;      /* No command line */
 
<span class="p_del">-	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);</span>
<span class="p_del">-	if (!len)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	while (len--) {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This &#39;pos&#39; check ensures we do not overrun</span>
<span class="p_add">+	 * a non-NULL-terminated &#39;cmdline&#39;</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (pos &lt; max_cmdline_size) {</span>
 		c = *(char *)cmdline++;
 		pos++;
 
<span class="p_chunk">@@ -58,18 +62,35 @@</span> <span class="p_context"> int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
 			/* fall through */
 
 		case st_wordcmp:
<span class="p_del">-			if (!*opptr)</span>
<span class="p_add">+			if (!*opptr) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We matched all the way to the end of the</span>
<span class="p_add">+				 * option we were looking for.  If the</span>
<span class="p_add">+				 * command-line has a space _or_ ends, then</span>
<span class="p_add">+				 * we matched!</span>
<span class="p_add">+				 */</span>
 				if (!c || myisspace(c))
 					return wstart;
<span class="p_del">-				else</span>
<span class="p_del">-					state = st_wordskip;</span>
<span class="p_del">-			else if (!c)</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We hit the end of the option, but _not_</span>
<span class="p_add">+				 * the end of a word on the cmdline.  Not</span>
<span class="p_add">+				 * a match.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+			} else if (!c) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Hit the NULL terminator on the end of</span>
<span class="p_add">+				 * cmdline.</span>
<span class="p_add">+				 */</span>
 				return 0;
<span class="p_del">-			else if (c != *opptr++)</span>
<span class="p_del">-				state = st_wordskip;</span>
<span class="p_del">-			else if (!len)		/* last word and is matching */</span>
<span class="p_del">-				return wstart;</span>
<span class="p_del">-			break;</span>
<span class="p_add">+			} else if (c == *opptr++) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We are currently matching, so continue</span>
<span class="p_add">+				 * to the next character on the cmdline.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			state = st_wordskip;</span>
<span class="p_add">+			/* fall through */</span>
 
 		case st_wordskip:
 			if (!c)
<span class="p_chunk">@@ -82,3 +103,113 @@</span> <span class="p_context"> int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
 
 	return 0;	/* Buffer overrun */
 }
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Find a non-boolean option (i.e. option=argument). In accordance with</span>
<span class="p_add">+ * standard Linux practice, if this option is repeated, this returns the</span>
<span class="p_add">+ * last instance on the command line.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @cmdline: the cmdline string</span>
<span class="p_add">+ * @max_cmdline_size: the maximum size of cmdline</span>
<span class="p_add">+ * @option: option string to look for</span>
<span class="p_add">+ * @buffer: memory buffer to return the option argument</span>
<span class="p_add">+ * @bufsize: size of the supplied memory buffer</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns the length of the argument (regardless of if it was</span>
<span class="p_add">+ * truncated to fit in the buffer), or -1 on not found.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int</span>
<span class="p_add">+__cmdline_find_option(const char *cmdline, int max_cmdline_size,</span>
<span class="p_add">+		      const char *option, char *buffer, int bufsize)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char c;</span>
<span class="p_add">+	int pos = 0, len = -1;</span>
<span class="p_add">+	const char *opptr = NULL;</span>
<span class="p_add">+	char *bufptr = buffer;</span>
<span class="p_add">+	enum {</span>
<span class="p_add">+		st_wordstart = 0,	/* Start of word/after whitespace */</span>
<span class="p_add">+		st_wordcmp,	/* Comparing this word */</span>
<span class="p_add">+		st_wordskip,	/* Miscompare, skip */</span>
<span class="p_add">+		st_bufcpy,	/* Copying this to buffer */</span>
<span class="p_add">+	} state = st_wordstart;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cmdline)</span>
<span class="p_add">+		return -1;      /* No command line */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This &#39;pos&#39; check ensures we do not overrun</span>
<span class="p_add">+	 * a non-NULL-terminated &#39;cmdline&#39;</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (pos++ &lt; max_cmdline_size) {</span>
<span class="p_add">+		c = *(char *)cmdline++;</span>
<span class="p_add">+		if (!c)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (state) {</span>
<span class="p_add">+		case st_wordstart:</span>
<span class="p_add">+			if (myisspace(c))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			state = st_wordcmp;</span>
<span class="p_add">+			opptr = option;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_wordcmp:</span>
<span class="p_add">+			if ((c == &#39;=&#39;) &amp;&amp; !*opptr) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We matched all the way to the end of the</span>
<span class="p_add">+				 * option we were looking for, prepare to</span>
<span class="p_add">+				 * copy the argument.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				len = 0;</span>
<span class="p_add">+				bufptr = buffer;</span>
<span class="p_add">+				state = st_bufcpy;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			} else if (c == *opptr++) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We are currently matching, so continue</span>
<span class="p_add">+				 * to the next character on the cmdline.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			state = st_wordskip;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_wordskip:</span>
<span class="p_add">+			if (myisspace(c))</span>
<span class="p_add">+				state = st_wordstart;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_bufcpy:</span>
<span class="p_add">+			if (myisspace(c)) {</span>
<span class="p_add">+				state = st_wordstart;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Increment len, but don&#39;t overrun the</span>
<span class="p_add">+				 * supplied buffer and leave room for the</span>
<span class="p_add">+				 * NULL terminator.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (++len &lt; bufsize)</span>
<span class="p_add">+					*bufptr++ = c;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (bufsize)</span>
<span class="p_add">+		*bufptr = &#39;\0&#39;;</span>
<span class="p_add">+</span>
<span class="p_add">+	return len;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cmdline_find_option_bool(cmdline, COMMAND_LINE_SIZE, option);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int cmdline_find_option(const char *cmdline, const char *option, char *buffer,</span>
<span class="p_add">+			int bufsize)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cmdline_find_option(cmdline, COMMAND_LINE_SIZE, option,</span>
<span class="p_add">+				     buffer, bufsize);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/lib/copy_page_64.S b/arch/x86/lib/copy_page_64.S</span>
<span class="p_header">index 176cca67212b..f1ffdbb07755 100644</span>
<span class="p_header">--- a/arch/x86/lib/copy_page_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/copy_page_64.S</span>
<span class="p_chunk">@@ -106,5 +106,5 @@</span> <span class="p_context"> ENDPROC(copy_page)</span>
 	.previous
 	.section .altinstructions,&quot;a&quot;
 	altinstruction_entry copy_page, 1b, X86_FEATURE_REP_GOOD,	\
<span class="p_del">-		.Lcopy_page_end-copy_page, 2b-1b</span>
<span class="p_add">+		.Lcopy_page_end-copy_page, 2b-1b, 0</span>
 	.previous
<span class="p_header">diff --git a/arch/x86/lib/copy_user_64.S b/arch/x86/lib/copy_user_64.S</span>
<span class="p_header">index dee945d55594..fca1f3a32bf5 100644</span>
<span class="p_header">--- a/arch/x86/lib/copy_user_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/copy_user_64.S</span>
<span class="p_chunk">@@ -28,19 +28,18 @@</span> <span class="p_context"></span>
  */
 	.macro ALTERNATIVE_JUMP feature1,feature2,orig,alt1,alt2
 0:
<span class="p_del">-	.byte 0xe9	/* 32bit jump */</span>
<span class="p_del">-	.long \orig-1f	/* by default jump to orig */</span>
<span class="p_add">+	jmp \orig</span>
 1:
 	.section .altinstr_replacement,&quot;ax&quot;
<span class="p_del">-2:	.byte 0xe9			/* near jump with 32bit immediate */</span>
<span class="p_del">-	.long \alt1-1b /* offset */   /* or alternatively to alt1 */</span>
<span class="p_del">-3:	.byte 0xe9			/* near jump with 32bit immediate */</span>
<span class="p_del">-	.long \alt2-1b /* offset */   /* or alternatively to alt2 */</span>
<span class="p_add">+2:</span>
<span class="p_add">+	jmp \alt1</span>
<span class="p_add">+3:</span>
<span class="p_add">+	jmp \alt2</span>
 	.previous
 
 	.section .altinstructions,&quot;a&quot;
<span class="p_del">-	altinstruction_entry 0b,2b,\feature1,5,5</span>
<span class="p_del">-	altinstruction_entry 0b,3b,\feature2,5,5</span>
<span class="p_add">+	altinstruction_entry 0b,2b,\feature1,5,5,0</span>
<span class="p_add">+	altinstruction_entry 0b,3b,\feature2,5,5,0</span>
 	.previous
 	.endm
 
<span class="p_header">diff --git a/arch/x86/lib/memcpy_64.S b/arch/x86/lib/memcpy_64.S</span>
<span class="p_header">index 56313a326188..f7766e8a497d 100644</span>
<span class="p_header">--- a/arch/x86/lib/memcpy_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/memcpy_64.S</span>
<span class="p_chunk">@@ -199,8 +199,8 @@</span> <span class="p_context"> ENDPROC(__memcpy)</span>
 	 * only outcome...
 	 */
 	.section .altinstructions, &quot;a&quot;
<span class="p_del">-	altinstruction_entry memcpy,.Lmemcpy_c,X86_FEATURE_REP_GOOD,\</span>
<span class="p_del">-			     .Lmemcpy_e-.Lmemcpy_c,.Lmemcpy_e-.Lmemcpy_c</span>
<span class="p_del">-	altinstruction_entry memcpy,.Lmemcpy_c_e,X86_FEATURE_ERMS, \</span>
<span class="p_del">-			     .Lmemcpy_e_e-.Lmemcpy_c_e,.Lmemcpy_e_e-.Lmemcpy_c_e</span>
<span class="p_add">+	altinstruction_entry __memcpy,.Lmemcpy_c,X86_FEATURE_REP_GOOD,\</span>
<span class="p_add">+			     .Lmemcpy_e-.Lmemcpy_c,.Lmemcpy_e-.Lmemcpy_c,0</span>
<span class="p_add">+	altinstruction_entry __memcpy,.Lmemcpy_c_e,X86_FEATURE_ERMS, \</span>
<span class="p_add">+			     .Lmemcpy_e_e-.Lmemcpy_c_e,.Lmemcpy_e_e-.Lmemcpy_c_e,0</span>
 	.previous
<span class="p_header">diff --git a/arch/x86/lib/memmove_64.S b/arch/x86/lib/memmove_64.S</span>
<span class="p_header">index 65268a6104f4..d290a599996c 100644</span>
<span class="p_header">--- a/arch/x86/lib/memmove_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/memmove_64.S</span>
<span class="p_chunk">@@ -218,6 +218,6 @@</span> <span class="p_context"> ENTRY(memmove)</span>
 	altinstruction_entry .Lmemmove_begin_forward,		\
 		.Lmemmove_begin_forward_efs,X86_FEATURE_ERMS,	\
 		.Lmemmove_end_forward-.Lmemmove_begin_forward,	\
<span class="p_del">-		.Lmemmove_end_forward_efs-.Lmemmove_begin_forward_efs</span>
<span class="p_add">+		.Lmemmove_end_forward_efs-.Lmemmove_begin_forward_efs,0</span>
 	.previous
 ENDPROC(memmove)
<span class="p_header">diff --git a/arch/x86/lib/memset_64.S b/arch/x86/lib/memset_64.S</span>
<span class="p_header">index 2dcb3808cbda..aba502738043 100644</span>
<span class="p_header">--- a/arch/x86/lib/memset_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/memset_64.S</span>
<span class="p_chunk">@@ -147,8 +147,8 @@</span> <span class="p_context"> ENDPROC(__memset)</span>
          * feature to implement the right patch order.
 	 */
 	.section .altinstructions,&quot;a&quot;
<span class="p_del">-	altinstruction_entry memset,.Lmemset_c,X86_FEATURE_REP_GOOD,\</span>
<span class="p_del">-			     .Lfinal-memset,.Lmemset_e-.Lmemset_c</span>
<span class="p_del">-	altinstruction_entry memset,.Lmemset_c_e,X86_FEATURE_ERMS, \</span>
<span class="p_del">-			     .Lfinal-memset,.Lmemset_e_e-.Lmemset_c_e</span>
<span class="p_add">+	altinstruction_entry __memset,.Lmemset_c,X86_FEATURE_REP_GOOD,\</span>
<span class="p_add">+			     .Lfinal-__memset,.Lmemset_e-.Lmemset_c,0</span>
<span class="p_add">+	altinstruction_entry __memset,.Lmemset_c_e,X86_FEATURE_ERMS, \</span>
<span class="p_add">+			     .Lfinal-__memset,.Lmemset_e_e-.Lmemset_c_e,0</span>
 	.previous
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 6a19ad9f370d..8a8ac114358f 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_del">-	    pat.o pgtable.o physaddr.o gup.o setup_nx.o</span>
<span class="p_add">+	    pat.o pgtable.o physaddr.o gup.o setup_nx.o tlb.o</span>
 
 # Make sure __phys_addr has no stackprotector
 nostackp := $(call cc-option, -fno-stack-protector)
<span class="p_chunk">@@ -9,7 +9,6 @@</span> <span class="p_context"> CFLAGS_setup_nx.o		:= $(nostackp)</span>
 CFLAGS_fault.o := -I$(src)/../include/asm/trace
 
 obj-$(CONFIG_X86_PAT)		+= pat_rbtree.o
<span class="p_del">-obj-$(CONFIG_SMP)		+= tlb.o</span>
 
 obj-$(CONFIG_X86_32)		+= pgtable_32.o iomap_32.o
 
<span class="p_chunk">@@ -30,3 +29,4 @@</span> <span class="p_context"> obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
 obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o
 
 obj-$(CONFIG_MEMTEST)		+= memtest.o
<span class="p_add">+obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= kaiser.o</span>
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 89c43a1ce82b..471f1ff32eb1 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -137,11 +137,11 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 
 	/* Enable PSE if available */
 	if (cpu_has_pse)
<span class="p_del">-		set_in_cr4(X86_CR4_PSE);</span>
<span class="p_add">+		cr4_set_bits_and_update_boot(X86_CR4_PSE);</span>
 
 	/* Enable PGE if available */
<span class="p_del">-	if (cpu_has_pge) {</span>
<span class="p_del">-		set_in_cr4(X86_CR4_PGE);</span>
<span class="p_add">+	if (cpu_has_pge &amp;&amp; !kaiser_enabled) {</span>
<span class="p_add">+		cr4_set_bits_and_update_boot(X86_CR4_PGE);</span>
 		__supported_pte_mask |= _PAGE_GLOBAL;
 	}
 }
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 12e5ac7885f8..10a4b72dcc1b 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -396,6 +396,16 @@</span> <span class="p_context"> void __init cleanup_highmap(void)</span>
 			continue;
 		if (vaddr &lt; (unsigned long) _text || vaddr &gt; end)
 			set_pmd(pmd, __pmd(0));
<span class="p_add">+		else if (kaiser_enabled) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * level2_kernel_pgt is initialized with _PAGE_GLOBAL:</span>
<span class="p_add">+			 * clear that now.  This is not important, so long as</span>
<span class="p_add">+			 * CR4.PGE remains clear, but it removes an anomaly.</span>
<span class="p_add">+			 * Physical mapping setup below avoids _PAGE_GLOBAL</span>
<span class="p_add">+			 * by use of massage_pgprot() inside pfn_pte() etc.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_pmd(pmd, pmd_clear_flags(*pmd, _PAGE_GLOBAL));</span>
<span class="p_add">+		}</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/x86/mm/kaiser.c b/arch/x86/mm/kaiser.c</span>
new file mode 100644
<span class="p_header">index 000000000000..8d1019d176e2</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c</span>
<span class="p_chunk">@@ -0,0 +1,469 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/interrupt.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/ftrace.h&gt;</span>
<span class="p_add">+#include &lt;xen/xen.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Kernel/User page tables isolation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct mm_struct init_mm;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;	/* to verify its kaiser declarations */</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+#include &lt;asm/vsyscall.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_enabled __read_mostly = 1;</span>
<span class="p_add">+EXPORT_SYMBOL(kaiser_enabled);	/* for inlined TLB flush functions */</span>
<span class="p_add">+</span>
<span class="p_add">+__visible</span>
<span class="p_add">+DEFINE_PER_CPU_USER_MAPPED(unsigned long, unsafe_stack_register_backup);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These can have bit 63 set, so we can not just use a plain &quot;or&quot;</span>
<span class="p_add">+ * instruction to get their value or&#39;d into CR3.  It would take</span>
<span class="p_add">+ * another register.  So, we use a memory reference to these instead.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is also handy because systems that do not support PCIDs</span>
<span class="p_add">+ * just end up or&#39;ing a 0 into their CR3, which does no harm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+DEFINE_PER_CPU(unsigned long, x86_cr3_pcid_user);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * At runtime, the only things we map are some things for CPU</span>
<span class="p_add">+ * hotplug, and stacks for new processes.  No two CPUs will ever</span>
<span class="p_add">+ * be populating the same addresses, so we only need to ensure</span>
<span class="p_add">+ * that we protect between two CPUs trying to allocate and</span>
<span class="p_add">+ * populate the same page table page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only take this lock when doing a set_p[4um]d(), but it is not</span>
<span class="p_add">+ * needed for doing a set_pte().  We assume that only the *owner*</span>
<span class="p_add">+ * of a given allocation will be doing this for _their_</span>
<span class="p_add">+ * allocation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures that once a system has been running for a while</span>
<span class="p_add">+ * and there have been stacks all over and these page tables</span>
<span class="p_add">+ * are fully populated, there will be no further acquisitions of</span>
<span class="p_add">+ * this lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static DEFINE_SPINLOCK(shadow_table_allocation_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns -1 on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long get_pa_from_mapping(unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(vaddr);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We made all the kernel PGDs present in kaiser_init().</span>
<span class="p_add">+	 * We expect them to stay that way.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(pgd_none(*pgd));</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PGDs are either 512GB or 128TB on all x86_64</span>
<span class="p_add">+	 * configurations.  We don&#39;t handle these.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(pgd_large(*pgd));</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, vaddr);</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_large(*pud))</span>
<span class="p_add">+		return (pud_pfn(*pud) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PUD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, vaddr);</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_large(*pmd))</span>
<span class="p_add">+		return (pmd_pfn(*pmd) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PMD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, vaddr);</span>
<span class="p_add">+	if (pte_none(*pte)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pte_pfn(*pte) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PAGE_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a relatively normal page table walk, except that it</span>
<span class="p_add">+ * also tries to allocate page tables pages along the way.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *kaiser_pagetable_walk(unsigned long address, bool user)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pgd_t *pgd = native_get_shadow_pgd(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+	unsigned long prot = _KERNPG_TABLE;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;All shadow pgds should have been populated&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (user) {</span>
<span class="p_add">+		set_pgd(pgd, __pgd(pgd_val(*pgd) | _PAGE_USER));</span>
<span class="p_add">+		prot = _PAGE_TABLE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pud_none(*pud)) {</span>
<span class="p_add">+			set_pud(pud, __pud(prot | __pa(new_pmd_page)));</span>
<span class="p_add">+			__inc_zone_page_state(virt_to_page((void *)</span>
<span class="p_add">+						new_pmd_page), NR_KAISERTABLE);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pmd_none(*pmd)) {</span>
<span class="p_add">+			set_pmd(pmd, __pmd(prot | __pa(new_pte_page)));</span>
<span class="p_add">+			__inc_zone_page_state(virt_to_page((void *)</span>
<span class="p_add">+						new_pte_page), NR_KAISERTABLE);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pte_offset_kernel(pmd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int kaiser_add_user_map(const void *__start_addr, unsigned long size,</span>
<span class="p_add">+			       unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	unsigned long start_addr = (unsigned long )__start_addr;</span>
<span class="p_add">+	unsigned long address = start_addr &amp; PAGE_MASK;</span>
<span class="p_add">+	unsigned long end_addr = PAGE_ALIGN(start_addr + size);</span>
<span class="p_add">+	unsigned long target_address;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * It is convenient for callers to pass in __PAGE_KERNEL etc,</span>
<span class="p_add">+	 * and there is no actual harm from setting _PAGE_GLOBAL, so</span>
<span class="p_add">+	 * long as CR4.PGE is not set.  But it is nonetheless troubling</span>
<span class="p_add">+	 * to see Kaiser itself setting _PAGE_GLOBAL (now that &quot;nokaiser&quot;</span>
<span class="p_add">+	 * requires that not to be #defined to 0): so mask it off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flags &amp;= ~_PAGE_GLOBAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; address &lt; end_addr; address += PAGE_SIZE) {</span>
<span class="p_add">+		target_address = get_pa_from_mapping(address);</span>
<span class="p_add">+		if (target_address == -1) {</span>
<span class="p_add">+			ret = -EIO;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte = kaiser_pagetable_walk(address, flags &amp; _PAGE_USER);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (pte_none(*pte)) {</span>
<span class="p_add">+			set_pte(pte, __pte(flags | target_address));</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pte_t tmp;</span>
<span class="p_add">+			set_pte(&amp;tmp, __pte(flags | target_address));</span>
<span class="p_add">+			WARN_ON_ONCE(!pte_same(*pte, tmp));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int kaiser_add_user_map_ptrs(const void *start, const void *end, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long size = end - start;</span>
<span class="p_add">+</span>
<span class="p_add">+	return kaiser_add_user_map(start, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Ensure that the top level of the (shadow) page tables are</span>
<span class="p_add">+ * entirely populated.  This ensures that all processes that get</span>
<span class="p_add">+ * forked have the same entries.  This way, we do not have to</span>
<span class="p_add">+ * ever go set up new entries in older processes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: we never free these, so there are no updates to them</span>
<span class="p_add">+ * after this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init kaiser_init_all_pgds(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = native_get_shadow_pgd(pgd_offset_k((unsigned long )0));</span>
<span class="p_add">+	for (i = PTRS_PER_PGD / 2; i &lt; PTRS_PER_PGD; i++) {</span>
<span class="p_add">+		pgd_t new_pgd;</span>
<span class="p_add">+		pud_t *pud = pud_alloc_one(&amp;init_mm,</span>
<span class="p_add">+					   PAGE_OFFSET + i * PGDIR_SIZE);</span>
<span class="p_add">+		if (!pud) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		inc_zone_page_state(virt_to_page(pud), NR_KAISERTABLE);</span>
<span class="p_add">+		new_pgd = __pgd(_KERNPG_TABLE |__pa(pud));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure not to stomp on some other pgd entry.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pgd_none(pgd[i])) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, new_pgd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_early(start, size, flags) do {	\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map(start, size, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_ptrs_early(start, end, flags) do {		\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map_ptrs(start, end, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+void __init kaiser_check_boottime_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool enable = true;</span>
<span class="p_add">+	char arg[5];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (xen_pv_domain())</span>
<span class="p_add">+		goto silent_disable;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;pti&quot;, arg, sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0) {</span>
<span class="p_add">+		if (!strncmp(arg, &quot;on&quot;, 2))</span>
<span class="p_add">+			goto enable;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!strncmp(arg, &quot;off&quot;, 3))</span>
<span class="p_add">+			goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!strncmp(arg, &quot;auto&quot;, 4))</span>
<span class="p_add">+			goto skip;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nopti&quot;))</span>
<span class="p_add">+		goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+skip:</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)</span>
<span class="p_add">+		goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+enable:</span>
<span class="p_add">+	if (enable)</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_KAISER);</span>
<span class="p_add">+</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+disable:</span>
<span class="p_add">+	pr_info(&quot;disabled\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+silent_disable:</span>
<span class="p_add">+	kaiser_enabled = 0;</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_KAISER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * If anything in here fails, we will likely die on one of the</span>
<span class="p_add">+ * first kernel-&gt;user transitions and init will die.  But, we</span>
<span class="p_add">+ * will have most of the kernel up by then and should be able to</span>
<span class="p_add">+ * get a clean warning out of it.  If we BUG_ON() here, we run</span>
<span class="p_add">+ * the risk of being before we have good console output.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu, idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_init_all_pgds();</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		void *percpu_vaddr = __per_cpu_user_mapped_start +</span>
<span class="p_add">+				     per_cpu_offset(cpu);</span>
<span class="p_add">+		unsigned long percpu_sz = __per_cpu_user_mapped_end -</span>
<span class="p_add">+					  __per_cpu_user_mapped_start;</span>
<span class="p_add">+		kaiser_add_user_map_early(percpu_vaddr, percpu_sz,</span>
<span class="p_add">+					  __PAGE_KERNEL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Map the entry/exit text section, which is needed at</span>
<span class="p_add">+	 * switches from user to and from kernel.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__entry_text_start, __entry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX);</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__irqentry_text_start,</span>
<span class="p_add">+				       __irqentry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	kaiser_add_user_map_early((void *)idt_descr.address,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL_RO);</span>
<span class="p_add">+#ifdef CONFIG_TRACING</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;trace_idt_descr,</span>
<span class="p_add">+				  sizeof(trace_idt_descr),</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;trace_idt_table,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;debug_idt_descr, sizeof(debug_idt_descr),</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;debug_idt_table,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vsyscall_enabled())</span>
<span class="p_add">+		kaiser_add_user_map_early((void *)VSYSCALL_ADDR, PAGE_SIZE,</span>
<span class="p_add">+					  vsyscall_pgprot);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;enabled\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Add a mapping to the shadow mapping, and synchronize the mappings */</span>
<span class="p_add">+int kaiser_add_mapping(unsigned long addr, unsigned long size, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return kaiser_add_user_map((const void *)addr, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	extern void unmap_pud_range_nofree(pgd_t *pgd,</span>
<span class="p_add">+				unsigned long start, unsigned long end);</span>
<span class="p_add">+	unsigned long end = start + size;</span>
<span class="p_add">+	unsigned long addr, next;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	pgd = native_get_shadow_pgd(pgd_offset_k(start));</span>
<span class="p_add">+	for (addr = start; addr &lt; end; pgd++, addr = next) {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		unmap_pud_range_nofree(pgd, addr, next);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ * This returns true for user pages that need to get copied into</span>
<span class="p_add">+ * both the user and kernel copies of the page tables, and false</span>
<span class="p_add">+ * for kernel pages that should only be in the kernel copy.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool is_userspace_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((unsigned long)pgdp % PAGE_SIZE) &lt; (PAGE_SIZE / 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do we need to also populate the shadow pgd?  Check _PAGE_USER to</span>
<span class="p_add">+	 * skip cases like kexec and EFI which make temporary low mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgd.pgd &amp; _PAGE_USER) {</span>
<span class="p_add">+		if (is_userspace_pgd(pgdp)) {</span>
<span class="p_add">+			native_get_shadow_pgd(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Even if the entry is *mapping* userspace, ensure</span>
<span class="p_add">+			 * that userspace can not use it.  This way, if we</span>
<span class="p_add">+			 * get out to userspace running on the kernel CR3,</span>
<span class="p_add">+			 * userspace will crash instead of running.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (!pgd.pgd) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * pgd_clear() cannot check _PAGE_USER, and is even used to</span>
<span class="p_add">+		 * clear corrupted pgd entries: so just rely on cases like</span>
<span class="p_add">+		 * kexec and EFI never to be using pgd_clear().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!WARN_ON_ONCE((unsigned long)pgdp &amp; PAGE_SIZE) &amp;&amp;</span>
<span class="p_add">+		    is_userspace_pgd(pgdp))</span>
<span class="p_add">+			native_get_shadow_pgd(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_setup_pcid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long user_cr3 = KAISER_SHADOW_PGD_OFFSET;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		user_cr3 |= X86_CR3_PCID_USER_NOFLUSH;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * These variables are used by the entry/exit</span>
<span class="p_add">+	 * code to change PCID and pgd and TLB flushing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	this_cpu_write(x86_cr3_pcid_user, user_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make a note that this cpu will need to flush USER tlb on return to user.</span>
<span class="p_add">+ * If cpu does not have PCID, then the NOFLUSH bit will never have been set.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void kaiser_flush_tlb_on_return_to_user(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		this_cpu_write(x86_cr3_pcid_user,</span>
<span class="p_add">+			X86_CR3_PCID_USER_FLUSH | KAISER_SHADOW_PGD_OFFSET);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(kaiser_flush_tlb_on_return_to_user);</span>
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index e5545f2105f6..aa7cdf54ad85 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -52,6 +52,7 @@</span> <span class="p_context"> static DEFINE_SPINLOCK(cpa_lock);</span>
 #define CPA_FLUSHTLB 1
 #define CPA_ARRAY 2
 #define CPA_PAGES_ARRAY 4
<span class="p_add">+#define CPA_FREE_PAGETABLES 8</span>
 
 #ifdef CONFIG_PROC_FS
 static unsigned long direct_pages_count[PG_LEVEL_NUM];
<span class="p_chunk">@@ -672,10 +673,13 @@</span> <span class="p_context"> static int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
 	return 0;
 }
 
<span class="p_del">-static bool try_to_free_pte_page(pte_t *pte)</span>
<span class="p_add">+static bool try_to_free_pte_page(struct cpa_data *cpa, pte_t *pte)</span>
 {
 	int i;
 
<span class="p_add">+	if (!(cpa-&gt;flags &amp; CPA_FREE_PAGETABLES))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; PTRS_PER_PTE; i++)
 		if (!pte_none(pte[i]))
 			return false;
<span class="p_chunk">@@ -684,10 +688,13 @@</span> <span class="p_context"> static bool try_to_free_pte_page(pte_t *pte)</span>
 	return true;
 }
 
<span class="p_del">-static bool try_to_free_pmd_page(pmd_t *pmd)</span>
<span class="p_add">+static bool try_to_free_pmd_page(struct cpa_data *cpa, pmd_t *pmd)</span>
 {
 	int i;
 
<span class="p_add">+	if (!(cpa-&gt;flags &amp; CPA_FREE_PAGETABLES))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; PTRS_PER_PMD; i++)
 		if (!pmd_none(pmd[i]))
 			return false;
<span class="p_chunk">@@ -708,7 +715,9 @@</span> <span class="p_context"> static bool try_to_free_pud_page(pud_t *pud)</span>
 	return true;
 }
 
<span class="p_del">-static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)</span>
<span class="p_add">+static bool unmap_pte_range(struct cpa_data *cpa, pmd_t *pmd,</span>
<span class="p_add">+			    unsigned long start,</span>
<span class="p_add">+			    unsigned long end)</span>
 {
 	pte_t *pte = pte_offset_kernel(pmd, start);
 
<span class="p_chunk">@@ -719,22 +728,23 @@</span> <span class="p_context"> static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)</span>
 		pte++;
 	}
 
<span class="p_del">-	if (try_to_free_pte_page((pte_t *)pmd_page_vaddr(*pmd))) {</span>
<span class="p_add">+	if (try_to_free_pte_page(cpa, (pte_t *)pmd_page_vaddr(*pmd))) {</span>
 		pmd_clear(pmd);
 		return true;
 	}
 	return false;
 }
 
<span class="p_del">-static void __unmap_pmd_range(pud_t *pud, pmd_t *pmd,</span>
<span class="p_add">+static void __unmap_pmd_range(struct cpa_data *cpa, pud_t *pud, pmd_t *pmd,</span>
 			      unsigned long start, unsigned long end)
 {
<span class="p_del">-	if (unmap_pte_range(pmd, start, end))</span>
<span class="p_del">-		if (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))</span>
<span class="p_add">+	if (unmap_pte_range(cpa, pmd, start, end))</span>
<span class="p_add">+		if (try_to_free_pmd_page(cpa, (pmd_t *)pud_page_vaddr(*pud)))</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
<span class="p_add">+static void unmap_pmd_range(struct cpa_data *cpa, pud_t *pud,</span>
<span class="p_add">+			    unsigned long start, unsigned long end)</span>
 {
 	pmd_t *pmd = pmd_offset(pud, start);
 
<span class="p_chunk">@@ -745,7 +755,7 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 		unsigned long next_page = (start + PMD_SIZE) &amp; PMD_MASK;
 		unsigned long pre_end = min_t(unsigned long, end, next_page);
 
<span class="p_del">-		__unmap_pmd_range(pud, pmd, start, pre_end);</span>
<span class="p_add">+		__unmap_pmd_range(cpa, pud, pmd, start, pre_end);</span>
 
 		start = pre_end;
 		pmd++;
<span class="p_chunk">@@ -758,7 +768,8 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 		if (pmd_large(*pmd))
 			pmd_clear(pmd);
 		else
<span class="p_del">-			__unmap_pmd_range(pud, pmd, start, start + PMD_SIZE);</span>
<span class="p_add">+			__unmap_pmd_range(cpa, pud, pmd,</span>
<span class="p_add">+					  start, start + PMD_SIZE);</span>
 
 		start += PMD_SIZE;
 		pmd++;
<span class="p_chunk">@@ -768,17 +779,19 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 	 * 4K leftovers?
 	 */
 	if (start &lt; end)
<span class="p_del">-		return __unmap_pmd_range(pud, pmd, start, end);</span>
<span class="p_add">+		return __unmap_pmd_range(cpa, pud, pmd, start, end);</span>
 
 	/*
 	 * Try again to free the PMD page if haven&#39;t succeeded above.
 	 */
 	if (!pud_none(*pud))
<span class="p_del">-		if (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))</span>
<span class="p_add">+		if (try_to_free_pmd_page(cpa, (pmd_t *)pud_page_vaddr(*pud)))</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+static void __unmap_pud_range(struct cpa_data *cpa, pgd_t *pgd,</span>
<span class="p_add">+			      unsigned long start,</span>
<span class="p_add">+			      unsigned long end)</span>
 {
 	pud_t *pud = pud_offset(pgd, start);
 
<span class="p_chunk">@@ -789,7 +802,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 		unsigned long next_page = (start + PUD_SIZE) &amp; PUD_MASK;
 		unsigned long pre_end	= min_t(unsigned long, end, next_page);
 
<span class="p_del">-		unmap_pmd_range(pud, start, pre_end);</span>
<span class="p_add">+		unmap_pmd_range(cpa, pud, start, pre_end);</span>
 
 		start = pre_end;
 		pud++;
<span class="p_chunk">@@ -803,7 +816,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 		if (pud_large(*pud))
 			pud_clear(pud);
 		else
<span class="p_del">-			unmap_pmd_range(pud, start, start + PUD_SIZE);</span>
<span class="p_add">+			unmap_pmd_range(cpa, pud, start, start + PUD_SIZE);</span>
 
 		start += PUD_SIZE;
 		pud++;
<span class="p_chunk">@@ -813,7 +826,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 	 * 2M leftovers?
 	 */
 	if (start &lt; end)
<span class="p_del">-		unmap_pmd_range(pud, start, end);</span>
<span class="p_add">+		unmap_pmd_range(cpa, pud, start, end);</span>
 
 	/*
 	 * No need to try to free the PUD page because we&#39;ll free it in
<span class="p_chunk">@@ -821,6 +834,24 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 	 */
 }
 
<span class="p_add">+static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpa_data cpa = {</span>
<span class="p_add">+		.flags = CPA_FREE_PAGETABLES,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	__unmap_pud_range(&amp;cpa, pgd, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void unmap_pud_range_nofree(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpa_data cpa = {</span>
<span class="p_add">+		.flags = 0,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	__unmap_pud_range(&amp;cpa, pgd, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void unmap_pgd_range(pgd_t *root, unsigned long addr, unsigned long end)
 {
 	pgd_t *pgd_entry = root + pgd_index(addr);
<span class="p_header">diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c</span>
<span class="p_header">index 6fb6927f9e76..d230776e9b19 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c</span>
<span class="p_chunk">@@ -5,7 +5,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlb.h&gt;
 #include &lt;asm/fixmap.h&gt;
 
<span class="p_del">-#define PGALLOC_GFP GFP_KERNEL | __GFP_NOTRACK | __GFP_REPEAT | __GFP_ZERO</span>
<span class="p_add">+#define PGALLOC_GFP (GFP_KERNEL | __GFP_NOTRACK | __GFP_REPEAT | __GFP_ZERO)</span>
 
 #ifdef CONFIG_HIGHPTE
 #define PGALLOC_USER_GFP __GFP_HIGHMEM
<span class="p_chunk">@@ -271,12 +271,31 @@</span> <span class="p_context"> static void pgd_prepopulate_pmd(struct mm_struct *mm, pgd_t *pgd, pmd_t *pmds[])</span>
 	}
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one pgd, Kaiser acquires two pgds.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER	kaiser_enabled</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *_pgd_alloc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* No __GFP_REPEAT: to avoid page allocation stalls in order-1 case */</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP &amp; ~__GFP_REPEAT,</span>
<span class="p_add">+					 PGD_ALLOCATION_ORDER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void _pgd_free(pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	pgd_t *pgd;
 	pmd_t *pmds[PREALLOCATED_PMDS];
 
<span class="p_del">-	pgd = (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	pgd = _pgd_alloc();</span>
 
 	if (pgd == NULL)
 		goto out;
<span class="p_chunk">@@ -306,7 +325,7 @@</span> <span class="p_context"> pgd_t *pgd_alloc(struct mm_struct *mm)</span>
 out_free_pmds:
 	free_pmds(pmds);
 out_free_pgd:
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	_pgd_free(pgd);</span>
 out:
 	return NULL;
 }
<span class="p_chunk">@@ -316,7 +335,7 @@</span> <span class="p_context"> void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
 	pgd_mop_up_pmds(mm, pgd);
 	pgd_dtor(pgd);
 	paravirt_pgd_free(mm, pgd);
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	_pgd_free(pgd);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 3465445be664..efa4839c3b59 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -6,19 +6,20 @@</span> <span class="p_context"></span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/debugfs.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/cache.h&gt;
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/uv/uv.h&gt;
<span class="p_del">-#include &lt;linux/debugfs.h&gt;</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate)
 			= { &amp;init_mm, 0, };
 
 /*
<span class="p_del">- *	Smarter SMP flushing macros.</span>
<span class="p_add">+ *	TLB flushing, formerly SMP-only</span>
  *		c/o Linus Torvalds.
  *
  *	These mean you can really definitely utterly forget about
<span class="p_chunk">@@ -37,6 +38,36 @@</span> <span class="p_context"> struct flush_tlb_info {</span>
 	unsigned long flush_end;
 };
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3 = __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We reuse the same PCID for different tasks, so we must</span>
<span class="p_add">+		 * flush all the entries for the PCID out when we change tasks.</span>
<span class="p_add">+		 * Flush KERN below, flush USER when returning to userspace in</span>
<span class="p_add">+		 * kaiser&#39;s SWITCH_USER_CR3 (_SWITCH_TO_USER_CR3) macro.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * invpcid_flush_single_context(X86_CR3_PCID_ASID_USER) could</span>
<span class="p_add">+		 * do it here, but can only be used if X86_FEATURE_INVPCID is</span>
<span class="p_add">+		 * available - and many machines support pcid without invpcid.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If X86_CR3_PCID_KERN_FLUSH actually added something, then it</span>
<span class="p_add">+		 * would be needed in the write_cr3() below - if PCIDs enabled.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		BUILD_BUG_ON(X86_CR3_PCID_KERN_FLUSH);</span>
<span class="p_add">+		kaiser_flush_tlb_on_return_to_user();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * We cannot call mmdrop() because we are in interrupt context,
  * instead update mm-&gt;cpu_vm_mask.
<span class="p_chunk">@@ -48,11 +79,94 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		BUG();
 	if (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {
 		cpumask_clear_cpu(cpu, mm_cpumask(active_mm));
<span class="p_del">-		load_cr3(swapper_pg_dir);</span>
<span class="p_add">+		load_new_mm_cr3(swapper_pg_dir);</span>
 	}
 }
 EXPORT_SYMBOL_GPL(leave_mm);
 
<span class="p_add">+void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+	       struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	switch_mm_irqs_off(prev, next, tsk);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (likely(prev != next)) {</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.active_mm, next);</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Re-load page tables.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * This logic has an ordering constraint:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_add">+		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_add">+		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_add">+		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_add">+		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_add">+		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_add">+		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_add">+		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_add">+		 * execute full barriers to prevent this from happening.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_add">+		 * store to mm_cpumask and any operation that could load</span>
<span class="p_add">+		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_add">+		 * due to instruction fetches or for no reason at all,</span>
<span class="p_add">+		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_add">+		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_add">+		 * ordering guarantee we need.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		load_new_mm_cr3(next-&gt;pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Stop flush ipis for the previous mm */</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(prev));</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Load the LDT, if the LDT is different: */</span>
<span class="p_add">+		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))</span>
<span class="p_add">+			load_mm_ldt(next);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * On established mms, the mm_cpumask is only changed</span>
<span class="p_add">+			 * from irq context, from ptep_clear_flush() while in</span>
<span class="p_add">+			 * lazy tlb mode, and here. Irqs are blocked during</span>
<span class="p_add">+			 * schedule, protecting us from simultaneous changes.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We were in lazy tlb mode and leave_mm disabled</span>
<span class="p_add">+			 * tlb flush IPI delivery. We must reload CR3</span>
<span class="p_add">+			 * to make sure to use no freed page tables.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_add">+			 * fills with respect to the mm_cpumask write.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * The flush IPI assumes that a thread switch happens in this order:
  * [cpu0: the cpu that switches]
<span class="p_chunk">@@ -143,35 +257,37 @@</span> <span class="p_context"> void native_flush_tlb_others(const struct cpumask *cpumask,</span>
 	smp_call_function_many(cpumask, flush_tlb_func, &amp;info, 1);
 }
 
<span class="p_del">-void flush_tlb_current_task(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-</span>
<span class="p_del">-	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* This is an implicit full barrier that synchronizes with switch_mm. */</span>
<span class="p_del">-	local_flush_tlb();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)</span>
<span class="p_del">-		flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * See Documentation/x86/tlb.txt for details.  We choose 33</span>
<span class="p_add">+ * because it is large enough to cover the vast majority (at</span>
<span class="p_add">+ * least 95%) of allocations, and is small enough that we are</span>
<span class="p_add">+ * confident it will not cause too much overhead.  Each single</span>
<span class="p_add">+ * flush is about 100 ns, so this caps the maximum overhead at</span>
<span class="p_add">+ * _about_ 3,000 ns.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is in units of pages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;</span>
 
 void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned long vmflag)
 {
 	unsigned long addr;
<span class="p_del">-	unsigned act_entries, tlb_entries = 0;</span>
<span class="p_del">-	unsigned long nr_base_pages;</span>
<span class="p_add">+	/* do a global flush by default */</span>
<span class="p_add">+	unsigned long base_pages_to_flush = TLB_FLUSH_ALL;</span>
 
 	preempt_disable();
<span class="p_add">+</span>
<span class="p_add">+	if ((end != TLB_FLUSH_ALL) &amp;&amp; !(vmflag &amp; VM_HUGETLB))</span>
<span class="p_add">+		base_pages_to_flush = (end - start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	if (base_pages_to_flush &gt; tlb_single_page_flush_ceiling)</span>
<span class="p_add">+		base_pages_to_flush = TLB_FLUSH_ALL;</span>
<span class="p_add">+</span>
 	if (current-&gt;active_mm != mm) {
 		/* Synchronize with switch_mm. */
 		smp_mb();
 
<span class="p_del">-		goto flush_all;</span>
<span class="p_add">+		goto out;</span>
 	}
 
 	if (!current-&gt;mm) {
<span class="p_chunk">@@ -180,28 +296,14 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 		/* Synchronize with switch_mm. */
 		smp_mb();
 
<span class="p_del">-		goto flush_all;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (end == TLB_FLUSH_ALL || tlb_flushall_shift == -1</span>
<span class="p_del">-					|| vmflag &amp; VM_HUGETLB) {</span>
<span class="p_del">-		local_flush_tlb();</span>
<span class="p_del">-		goto flush_all;</span>
<span class="p_add">+		goto out;</span>
 	}
 
<span class="p_del">-	/* In modern CPU, last level tlb used for both data/ins */</span>
<span class="p_del">-	if (vmflag &amp; VM_EXEC)</span>
<span class="p_del">-		tlb_entries = tlb_lli_4k[ENTRIES];</span>
<span class="p_del">-	else</span>
<span class="p_del">-		tlb_entries = tlb_lld_4k[ENTRIES];</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Assume all of TLB entries was occupied by this task */</span>
<span class="p_del">-	act_entries = tlb_entries &gt;&gt; tlb_flushall_shift;</span>
<span class="p_del">-	act_entries = mm-&gt;total_vm &gt; act_entries ? act_entries : mm-&gt;total_vm;</span>
<span class="p_del">-	nr_base_pages = (end - start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* tlb_flushall_shift is on balance point, details in commit log */</span>
<span class="p_del">-	if (nr_base_pages &gt; act_entries) {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Both branches below are implicit full barriers (MOV to CR or</span>
<span class="p_add">+	 * INVLPG) that synchronize with switch_mm.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (base_pages_to_flush == TLB_FLUSH_ALL) {</span>
 		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 		local_flush_tlb();
 	} else {
<span class="p_chunk">@@ -210,44 +312,14 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 			__flush_tlb_single(addr);
 		}
<span class="p_del">-</span>
<span class="p_del">-		if (cpumask_any_but(mm_cpumask(mm),</span>
<span class="p_del">-				smp_processor_id()) &lt; nr_cpu_ids)</span>
<span class="p_del">-			flush_tlb_others(mm_cpumask(mm), mm, start, end);</span>
<span class="p_del">-		preempt_enable();</span>
<span class="p_del">-		return;</span>
 	}
<span class="p_del">-</span>
<span class="p_del">-flush_all:</span>
<span class="p_del">-	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)</span>
<span class="p_del">-		flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (current-&gt;active_mm == mm) {</span>
<span class="p_del">-		if (current-&gt;mm) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Implicit full barrier (INVLPG) that synchronizes</span>
<span class="p_del">-			 * with switch_mm.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			__flush_tlb_one(start);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			leave_mm(smp_processor_id());</span>
<span class="p_del">-</span>
<span class="p_del">-			/* Synchronize with switch_mm. */</span>
<span class="p_del">-			smp_mb();</span>
<span class="p_del">-		}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	if (base_pages_to_flush == TLB_FLUSH_ALL) {</span>
<span class="p_add">+		start = 0UL;</span>
<span class="p_add">+		end = TLB_FLUSH_ALL;</span>
 	}
<span class="p_del">-</span>
 	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)
<span class="p_del">-		flush_tlb_others(mm_cpumask(mm), mm, start, start + PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_add">+		flush_tlb_others(mm_cpumask(mm), mm, start, end);</span>
 	preempt_enable();
 }
 
<span class="p_chunk">@@ -277,32 +349,26 @@</span> <span class="p_context"> static void do_kernel_range_flush(void *info)</span>
 
 void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 {
<span class="p_del">-	unsigned act_entries;</span>
<span class="p_del">-	struct flush_tlb_info info;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* In modern CPU, last level tlb used for both data/ins */</span>
<span class="p_del">-	act_entries = tlb_lld_4k[ENTRIES];</span>
 
 	/* Balance as user space task&#39;s flush, a bit conservative */
<span class="p_del">-	if (end == TLB_FLUSH_ALL || tlb_flushall_shift == -1 ||</span>
<span class="p_del">-		(end - start) &gt;&gt; PAGE_SHIFT &gt; act_entries &gt;&gt; tlb_flushall_shift)</span>
<span class="p_del">-</span>
<span class="p_add">+	if (end == TLB_FLUSH_ALL ||</span>
<span class="p_add">+	    (end - start) &gt; tlb_single_page_flush_ceiling * PAGE_SIZE) {</span>
 		on_each_cpu(do_flush_tlb_all, NULL, 1);
<span class="p_del">-	else {</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		struct flush_tlb_info info;</span>
 		info.flush_start = start;
 		info.flush_end = end;
 		on_each_cpu(do_kernel_range_flush, &amp;info, 1);
 	}
 }
 
<span class="p_del">-#ifdef CONFIG_DEBUG_TLBFLUSH</span>
 static ssize_t tlbflush_read_file(struct file *file, char __user *user_buf,
 			     size_t count, loff_t *ppos)
 {
 	char buf[32];
 	unsigned int len;
 
<span class="p_del">-	len = sprintf(buf, &quot;%hd\n&quot;, tlb_flushall_shift);</span>
<span class="p_add">+	len = sprintf(buf, &quot;%ld\n&quot;, tlb_single_page_flush_ceiling);</span>
 	return simple_read_from_buffer(user_buf, count, ppos, buf, len);
 }
 
<span class="p_chunk">@@ -311,20 +377,20 @@</span> <span class="p_context"> static ssize_t tlbflush_write_file(struct file *file,</span>
 {
 	char buf[32];
 	ssize_t len;
<span class="p_del">-	s8 shift;</span>
<span class="p_add">+	int ceiling;</span>
 
 	len = min(count, sizeof(buf) - 1);
 	if (copy_from_user(buf, user_buf, len))
 		return -EFAULT;
 
 	buf[len] = &#39;\0&#39;;
<span class="p_del">-	if (kstrtos8(buf, 0, &amp;shift))</span>
<span class="p_add">+	if (kstrtoint(buf, 0, &amp;ceiling))</span>
 		return -EINVAL;
 
<span class="p_del">-	if (shift &lt; -1 || shift &gt;= BITS_PER_LONG)</span>
<span class="p_add">+	if (ceiling &lt; 0)</span>
 		return -EINVAL;
 
<span class="p_del">-	tlb_flushall_shift = shift;</span>
<span class="p_add">+	tlb_single_page_flush_ceiling = ceiling;</span>
 	return count;
 }
 
<span class="p_chunk">@@ -334,11 +400,10 @@</span> <span class="p_context"> static const struct file_operations fops_tlbflush = {</span>
 	.llseek = default_llseek,
 };
 
<span class="p_del">-static int __init create_tlb_flushall_shift(void)</span>
<span class="p_add">+static int __init create_tlb_single_page_flush_ceiling(void)</span>
 {
<span class="p_del">-	debugfs_create_file(&quot;tlb_flushall_shift&quot;, S_IRUSR | S_IWUSR,</span>
<span class="p_add">+	debugfs_create_file(&quot;tlb_single_page_flush_ceiling&quot;, S_IRUSR | S_IWUSR,</span>
 			    arch_debugfs_dir, NULL, &amp;fops_tlbflush);
 	return 0;
 }
<span class="p_del">-late_initcall(create_tlb_flushall_shift);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+late_initcall(create_tlb_single_page_flush_ceiling);</span>
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index e1e7f4f1fb80..10130e9afd6e 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -430,6 +430,12 @@</span> <span class="p_context"> static void __init xen_init_cpuid_mask(void)</span>
 		~((1 &lt;&lt; X86_FEATURE_MTRR) |  /* disable MTRR */
 		  (1 &lt;&lt; X86_FEATURE_ACC));   /* thermal monitoring */
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Xen PV would need some work to support PCID: CR3 handling as well</span>
<span class="p_add">+	 * as xen_flush_tlb_others() would need updating.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cpuid_leaf1_ecx_mask &amp;= ~(1 &lt;&lt; (X86_FEATURE_PCID % 32));  /* disable PCID */</span>
<span class="p_add">+</span>
 	if (!xen_initial_domain())
 		cpuid_leaf1_edx_mask &amp;=
 			~((1 &lt;&lt; X86_FEATURE_ACPI));  /* disable ACPI */
<span class="p_chunk">@@ -1526,10 +1532,10 @@</span> <span class="p_context"> static void xen_pvh_set_cr_flags(int cpu)</span>
 	 * set them here. For all, OSFXSR OSXMMEXCPT are set in fpu_init.
 	*/
 	if (cpu_has_pse)
<span class="p_del">-		set_in_cr4(X86_CR4_PSE);</span>
<span class="p_add">+		cr4_set_bits_and_update_boot(X86_CR4_PSE);</span>
 
 	if (cpu_has_pge)
<span class="p_del">-		set_in_cr4(X86_CR4_PGE);</span>
<span class="p_add">+		cr4_set_bits_and_update_boot(X86_CR4_PGE);</span>
 }
 
 /*
<span class="p_header">diff --git a/drivers/lguest/x86/core.c b/drivers/lguest/x86/core.c</span>
<span class="p_header">index 922a1acbf652..6adfd7ba4c97 100644</span>
<span class="p_header">--- a/drivers/lguest/x86/core.c</span>
<span class="p_header">+++ b/drivers/lguest/x86/core.c</span>
<span class="p_chunk">@@ -47,6 +47,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/lguest.h&gt;
 #include &lt;asm/uaccess.h&gt;
 #include &lt;asm/i387.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &quot;../lg.h&quot;
 
 static int cpu_had_pge;
<span class="p_chunk">@@ -452,9 +453,9 @@</span> <span class="p_context"> void lguest_arch_handle_trap(struct lg_cpu *cpu)</span>
 static void adjust_pge(void *on)
 {
 	if (on)
<span class="p_del">-		write_cr4(read_cr4() | X86_CR4_PGE);</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PGE);</span>
 	else
<span class="p_del">-		write_cr4(read_cr4() &amp; ~X86_CR4_PGE);</span>
<span class="p_add">+		cr4_clear_bits(X86_CR4_PGE);</span>
 }
 
 /*H:020
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index 1fe2c8115be0..a9dfe6bd70eb 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/vhost.h&gt;
 #include &lt;linux/uio.h&gt;
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;linux/mmu_context.h&gt;
 #include &lt;linux/miscdevice.h&gt;
 #include &lt;linux/mutex.h&gt;
<span class="p_header">diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">index c1c0b0cf39b4..4191efe77e36 100644</span>
<span class="p_header">--- a/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">+++ b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_chunk">@@ -689,7 +689,14 @@</span> <span class="p_context"></span>
  */
 #define PERCPU_INPUT(cacheline)						\
 	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
<span class="p_add">+	VMLINUX_SYMBOL(__per_cpu_user_mapped_start) = .;		\</span>
 	*(.data..percpu..first)						\
<span class="p_add">+	. = ALIGN(cacheline);						\</span>
<span class="p_add">+	*(.data..percpu..user_mapped)					\</span>
<span class="p_add">+	*(.data..percpu..user_mapped..shared_aligned)			\</span>
<span class="p_add">+	. = ALIGN(PAGE_SIZE);						\</span>
<span class="p_add">+	*(.data..percpu..user_mapped..page_aligned)			\</span>
<span class="p_add">+	VMLINUX_SYMBOL(__per_cpu_user_mapped_end) = .;			\</span>
 	. = ALIGN(PAGE_SIZE);						\
 	*(.data..percpu..page_aligned)					\
 	. = ALIGN(cacheline);						\
<span class="p_header">diff --git a/include/linux/kaiser.h b/include/linux/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..58c55b1589d0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,52 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _LINUX_KAISER_H</span>
<span class="p_add">+#define _LINUX_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int kaiser_map_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Map that page of kernel stack on which we enter from user context.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return kaiser_add_mapping((unsigned long)stack +</span>
<span class="p_add">+			THREAD_SIZE - PAGE_SIZE, PAGE_SIZE, __PAGE_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_unmap_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: may be called even when kaiser_map_thread_stack() failed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_remove_mapping((unsigned long)stack +</span>
<span class="p_add">+			THREAD_SIZE - PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These stubs are used whenever CONFIG_PAGE_TABLE_ISOLATION is off, which</span>
<span class="p_add">+ * includes architectures that support KAISER, but have it disabled.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int kaiser_add_mapping(unsigned long addr,</span>
<span class="p_add">+				     unsigned long size, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_remove_mapping(unsigned long start,</span>
<span class="p_add">+					 unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int kaiser_map_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_unmap_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+#endif /* _LINUX_KAISER_H */</span>
<span class="p_header">diff --git a/include/linux/mmu_context.h b/include/linux/mmu_context.h</span>
<span class="p_header">index 70fffeba7495..a4441784503b 100644</span>
<span class="p_header">--- a/include/linux/mmu_context.h</span>
<span class="p_header">+++ b/include/linux/mmu_context.h</span>
<span class="p_chunk">@@ -1,9 +1,16 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_MMU_CONTEXT_H
 #define _LINUX_MMU_CONTEXT_H
 
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+</span>
 struct mm_struct;
 
 void use_mm(struct mm_struct *mm);
 void unuse_mm(struct mm_struct *mm);
 
<span class="p_add">+/* Architectures that care about IRQ state in switch_mm can override this. */</span>
<span class="p_add">+#ifndef switch_mm_irqs_off</span>
<span class="p_add">+# define switch_mm_irqs_off switch_mm</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index c7eb42235fbe..9fe9377821a5 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -131,8 +131,9 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
<span class="p_del">-	NR_KERNEL_STACK,</span>
 	/* Second 128 byte cacheline */
<span class="p_add">+	NR_KERNEL_STACK,</span>
<span class="p_add">+	NR_KAISERTABLE,</span>
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
<span class="p_header">diff --git a/include/linux/percpu-defs.h b/include/linux/percpu-defs.h</span>
<span class="p_header">index dec01d6c3f80..c2d267b53d40 100644</span>
<span class="p_header">--- a/include/linux/percpu-defs.h</span>
<span class="p_header">+++ b/include/linux/percpu-defs.h</span>
<span class="p_chunk">@@ -1,6 +1,12 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_PERCPU_DEFS_H
 #define _LINUX_PERCPU_DEFS_H
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#define USER_MAPPED_SECTION &quot;..user_mapped&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define USER_MAPPED_SECTION &quot;&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Base implementations of per-CPU variable declarations and definitions, where
  * the section in which the variable is to be placed is provided by the
<span class="p_chunk">@@ -94,6 +100,12 @@</span> <span class="p_context"></span>
 #define DEFINE_PER_CPU(type, name)					\
 	DEFINE_PER_CPU_SECTION(type, name, &quot;&quot;)
 
<span class="p_add">+#define DECLARE_PER_CPU_USER_MAPPED(type, name)				\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_USER_MAPPED(type, name)				\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION)</span>
<span class="p_add">+</span>
 /*
  * Declaration/definition used for per-CPU variables that must come first in
  * the set of variables.
<span class="p_chunk">@@ -123,6 +135,14 @@</span> <span class="p_context"></span>
 	DEFINE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \
 	____cacheline_aligned_in_smp
 
<span class="p_add">+#define DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION PER_CPU_SHARED_ALIGNED_SECTION) \</span>
<span class="p_add">+	____cacheline_aligned_in_smp</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION PER_CPU_SHARED_ALIGNED_SECTION) \</span>
<span class="p_add">+	____cacheline_aligned_in_smp</span>
<span class="p_add">+</span>
 #define DECLARE_PER_CPU_ALIGNED(type, name)				\
 	DECLARE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)	\
 	____cacheline_aligned
<span class="p_chunk">@@ -141,11 +161,21 @@</span> <span class="p_context"></span>
 #define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)				\
 	DEFINE_PER_CPU_SECTION(type, name, &quot;..page_aligned&quot;)		\
 	__aligned(PAGE_SIZE)
<span class="p_add">+/*</span>
<span class="p_add">+ * Declaration/definition used for per-CPU variables that must be page aligned and need to be mapped in user mode.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION&quot;..page_aligned&quot;) \</span>
<span class="p_add">+	__aligned(PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION&quot;..page_aligned&quot;) \</span>
<span class="p_add">+	__aligned(PAGE_SIZE)</span>
 
 /*
  * Declaration/definition used for per-CPU variables that must be read mostly.
  */
<span class="p_del">-#define DECLARE_PER_CPU_READ_MOSTLY(type, name)			\</span>
<span class="p_add">+#define DECLARE_PER_CPU_READ_MOSTLY(type, name)				\</span>
 	DECLARE_PER_CPU_SECTION(type, name, &quot;..read_mostly&quot;)
 
 #define DEFINE_PER_CPU_READ_MOSTLY(type, name)				\
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 3861fe648fbd..0190bb249401 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -78,6 +78,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/context_tracking.h&gt;
 #include &lt;linux/random.h&gt;
 #include &lt;linux/list.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 
 #include &lt;asm/io.h&gt;
 #include &lt;asm/bugs.h&gt;
<span class="p_chunk">@@ -497,6 +498,7 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	percpu_init_late();
 	pgtable_init();
 	vmalloc_init();
<span class="p_add">+	kaiser_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 0a2f907b1302..315342c2aac4 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -58,6 +58,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/tsacct_kern.h&gt;
 #include &lt;linux/cn_proc.h&gt;
 #include &lt;linux/freezer.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/delayacct.h&gt;
 #include &lt;linux/taskstats_kern.h&gt;
 #include &lt;linux/random.h&gt;
<span class="p_chunk">@@ -158,6 +159,7 @@</span> <span class="p_context"> static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,</span>
 
 static inline void free_thread_info(struct thread_info *ti)
 {
<span class="p_add">+	kaiser_unmap_thread_stack(ti);</span>
 	free_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 # else
<span class="p_chunk">@@ -316,6 +318,10 @@</span> <span class="p_context"> static struct task_struct *dup_task_struct(struct task_struct *orig)</span>
 
 	tsk-&gt;stack = ti;
 
<span class="p_add">+	err = kaiser_map_thread_stack(tsk-&gt;stack);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		goto free_ti;</span>
<span class="p_add">+</span>
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index e6c9458f4b49..d776fb7d902a 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/init.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/highmem.h&gt;
<span class="p_del">-#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_context.h&gt;</span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/capability.h&gt;
 #include &lt;linux/completion.h&gt;
<span class="p_chunk">@@ -2374,7 +2374,7 @@</span> <span class="p_context"> context_switch(struct rq *rq, struct task_struct *prev,</span>
 		atomic_inc(&amp;oldmm-&gt;mm_count);
 		enter_lazy_tlb(oldmm, next);
 	} else
<span class="p_del">-		switch_mm(oldmm, mm, next);</span>
<span class="p_add">+		switch_mm_irqs_off(oldmm, mm, next);</span>
 
 	if (!prev-&gt;mm) {
 		prev-&gt;active_mm = NULL;
<span class="p_header">diff --git a/mm/mmu_context.c b/mm/mmu_context.c</span>
<span class="p_header">index f802c2d216a7..6f4d27c5bb32 100644</span>
<span class="p_header">--- a/mm/mmu_context.c</span>
<span class="p_header">+++ b/mm/mmu_context.c</span>
<span class="p_chunk">@@ -4,9 +4,9 @@</span> <span class="p_context"></span>
  */
 
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;linux/mmu_context.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_del">-#include &lt;linux/sched.h&gt;</span>
 
 #include &lt;asm/mmu_context.h&gt;
 
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 8c0c76083562..6c70f6647907 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -753,6 +753,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_slab_unreclaimable&quot;,
 	&quot;nr_page_table_pages&quot;,
 	&quot;nr_kernel_stack&quot;,
<span class="p_add">+	&quot;nr_overhead&quot;,</span>
 	&quot;nr_unstable&quot;,
 	&quot;nr_bounce&quot;,
 	&quot;nr_vmscan_write&quot;,
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index beb86b500adf..89e13fb728f5 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -30,6 +30,16 @@</span> <span class="p_context"> config SECURITY</span>
 
 	  If you are unsure how to answer this question, answer N.
 
<span class="p_add">+config PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool &quot;Remove the kernel mapping in user mode&quot;</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; SMP</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This enforces a strict kernel and user space isolation, in order</span>
<span class="p_add">+	  to close hardware side channels on kernel address information.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If you are unsure how to answer this question, answer Y.</span>
<span class="p_add">+</span>
 config SECURITYFS
 	bool &quot;Enable the securityfs filesystem&quot;
 	help

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



