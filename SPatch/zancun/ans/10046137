
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RESEND] mm, oom_reaper: gather each vma to prevent leaking TLB entry - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RESEND] mm, oom_reaper: gather each vma to prevent leaking TLB entry</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=84111">Wang Nan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 7, 2017, 9:54 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171107095453.179940-1-wangnan0@huawei.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10046137/mbox/"
   >mbox</a>
|
   <a href="/patch/10046137/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10046137/">/patch/10046137/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	819406031B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Nov 2017 09:59:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7122F28EDB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Nov 2017 09:59:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 646F628F07; Tue,  7 Nov 2017 09:59:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 87B0528EDB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Nov 2017 09:59:37 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756101AbdKGJ7c (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 7 Nov 2017 04:59:32 -0500
Received: from szxga06-in.huawei.com ([45.249.212.32]:39794 &quot;EHLO huawei.com&quot;
	rhost-flags-OK-FAIL-OK-FAIL) by vger.kernel.org with ESMTP
	id S1752666AbdKGJ73 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 7 Nov 2017 04:59:29 -0500
Received: from DGGEMS408-HUB.china.huawei.com (unknown [172.30.72.59])
	by Forcepoint Email with ESMTP id 3EC39E971F2D5;
	Tue,  7 Nov 2017 17:56:40 +0800 (CST)
Received: from linux-4hy3.site (10.107.193.248) by
	DGGEMS408-HUB.china.huawei.com (10.3.19.208) with Microsoft SMTP
	Server id 14.3.361.1; Tue, 7 Nov 2017 17:55:31 +0800
From: Wang Nan &lt;wangnan0@huawei.com&gt;
To: &lt;linux-mm@kvack.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	&lt;mhocko@kernel.org&gt;, &lt;will.deacon@arm.com&gt;
CC: Wang Nan &lt;wangnan0@huawei.com&gt;, Bob Liu &lt;liubo95@huawei.com&gt;,
	&quot;Andrew Morton&quot; &lt;akpm@linux-foundation.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, &quot;David Rientjes&quot; &lt;rientjes@google.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, &quot;Roman Gushchin&quot; &lt;guro@fb.com&gt;,
	Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [RESEND PATCH] mm,
	oom_reaper: gather each vma to prevent leaking TLB entry
Date: Tue, 7 Nov 2017 09:54:53 +0000
Message-ID: &lt;20171107095453.179940-1-wangnan0@huawei.com&gt;
X-Mailer: git-send-email 2.10.1
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [10.107.193.248]
X-CFilter-Loop: Reflected
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=84111">Wang Nan</a> - Nov. 7, 2017, 9:54 a.m.</div>
<pre class="content">
tlb_gather_mmu(&amp;tlb, mm, 0, -1) means gathering the whole virtual memory
space. In this case, tlb-&gt;fullmm is true. Some archs like arm64 doesn&#39;t
flush TLB when tlb-&gt;fullmm is true:

  commit 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;).

Which makes leaking of tlb entries.

Will clarifies his patch:
<span class="quote">
&gt; Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; pulling down a full mm because we won&#39;t ever assign that ASID to another mm</span>
<span class="quote">&gt; without doing TLB invalidation elsewhere (which actually just nukes the</span>
<span class="quote">&gt; whole TLB).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think that means that we could potentially not fault on a kernel uaccess,</span>
<span class="quote">&gt; because we could hit in the TLB.</span>

There could be a window between complete_signal() sending IPI to other
cores and all threads sharing this mm are really kicked off from cores.
In this window, the oom reaper may calls tlb_flush_mmu_tlbonly() to flush
TLB then frees pages. However, due to the above problem, the TLB entries
are not really flushed on arm64. Other threads are possible to access
these pages through TLB entries. Moreover, a copy_to_user() can also
write to these pages without generating page fault, causes use-after-free
bugs.

This patch gathers each vma instead of gathering full vm space.
In this case tlb-&gt;fullmm is not true. The behavior of oom reaper become
similar to munmapping before do_exit, which should be safe for all archs.
<span class="signed-off-by">
Signed-off-by: Wang Nan &lt;wangnan0@huawei.com&gt;</span>
Cc: Bob Liu &lt;liubo95@huawei.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: Roman Gushchin &lt;guro@fb.com&gt;
Cc: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
---
 mm/oom_kill.c | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 7, 2017, 10:09 a.m.</div>
<pre class="content">
On Tue 07-11-17 09:54:53, Wang Nan wrote:
<span class="quote">&gt; tlb_gather_mmu(&amp;tlb, mm, 0, -1) means gathering the whole virtual memory</span>
<span class="quote">&gt; space. In this case, tlb-&gt;fullmm is true. Some archs like arm64 doesn&#39;t</span>
<span class="quote">&gt; flush TLB when tlb-&gt;fullmm is true:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   commit 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which makes leaking of tlb entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will clarifies his patch:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; &gt; is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; &gt; pulling down a full mm because we won&#39;t ever assign that ASID to another mm</span>
<span class="quote">&gt; &gt; without doing TLB invalidation elsewhere (which actually just nukes the</span>
<span class="quote">&gt; &gt; whole TLB).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think that means that we could potentially not fault on a kernel uaccess,</span>
<span class="quote">&gt; &gt; because we could hit in the TLB.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There could be a window between complete_signal() sending IPI to other</span>
<span class="quote">&gt; cores and all threads sharing this mm are really kicked off from cores.</span>
<span class="quote">&gt; In this window, the oom reaper may calls tlb_flush_mmu_tlbonly() to flush</span>
<span class="quote">&gt; TLB then frees pages. However, due to the above problem, the TLB entries</span>
<span class="quote">&gt; are not really flushed on arm64. Other threads are possible to access</span>
<span class="quote">&gt; these pages through TLB entries. Moreover, a copy_to_user() can also</span>
<span class="quote">&gt; write to these pages without generating page fault, causes use-after-free</span>
<span class="quote">&gt; bugs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch gathers each vma instead of gathering full vm space.</span>
<span class="quote">&gt; In this case tlb-&gt;fullmm is not true. The behavior of oom reaper become</span>
<span class="quote">&gt; similar to munmapping before do_exit, which should be safe for all archs.</span>

This goes all the way down to when the reaper has been introduced.

Fixes: aac453635549 (&quot;mm, oom: introduce oom reaper&quot;)

Maybe we should even Cc stable because a memory corruption will be quite
hard to debug.
<span class="quote">
&gt; Signed-off-by: Wang Nan &lt;wangnan0@huawei.com&gt;</span>
<span class="quote">&gt; Cc: Bob Liu &lt;liubo95@huawei.com&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Roman Gushchin &lt;guro@fb.com&gt;</span>
<span class="quote">&gt; Cc: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;</span>
<span class="quote">&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Thanks!
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  mm/oom_kill.c | 7 ++++---</span>
<span class="quote">&gt;  1 file changed, 4 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; index dee0f75..18c5b35 100644</span>
<span class="quote">&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; @@ -532,7 +532,6 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt;  	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt;  		if (!can_madv_dontneed_vma(vma))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -547,11 +546,13 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  		 * we do not want to block exit_mmap by keeping mm ref</span>
<span class="quote">&gt;  		 * count elevated without a good reason.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; +		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="quote">&gt; +			tlb_gather_mmu(&amp;tlb, mm, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt;  			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt;  					 NULL);</span>
<span class="quote">&gt; +			tlb_finish_mmu(&amp;tlb, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt;  	pr_info(&quot;oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n&quot;,</span>
<span class="quote">&gt;  			task_pid_nr(tsk), tsk-&gt;comm,</span>
<span class="quote">&gt;  			K(get_mm_counter(mm, MM_ANONPAGES)),</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.10.1</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 10, 2017, 12:19 a.m.</div>
<pre class="content">
On Tue, Nov 07, 2017 at 09:54:53AM +0000, Wang Nan wrote:
<span class="quote">&gt; tlb_gather_mmu(&amp;tlb, mm, 0, -1) means gathering the whole virtual memory</span>
<span class="quote">&gt; space. In this case, tlb-&gt;fullmm is true. Some archs like arm64 doesn&#39;t</span>
<span class="quote">&gt; flush TLB when tlb-&gt;fullmm is true:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   commit 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which makes leaking of tlb entries.</span>

That means soft-dirty which has used tlb_gather_mmu with fullmm could be
broken via losing write-protection bit once it supports arm64 in future?

If so, it would be better to use TASK_SIZE rather than -1 in tlb_gather_mmu.
Of course, it&#39;s a off-topic.

However, I want to add a big fat comment in tlb_gather_mmu to warn &quot;TLB
flushing with (0, -1) can be skipped on some architectures&quot; so upcoming
users can care of.

Thanks.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Will clarifies his patch:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; &gt; is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; &gt; pulling down a full mm because we won&#39;t ever assign that ASID to another mm</span>
<span class="quote">&gt; &gt; without doing TLB invalidation elsewhere (which actually just nukes the</span>
<span class="quote">&gt; &gt; whole TLB).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think that means that we could potentially not fault on a kernel uaccess,</span>
<span class="quote">&gt; &gt; because we could hit in the TLB.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There could be a window between complete_signal() sending IPI to other</span>
<span class="quote">&gt; cores and all threads sharing this mm are really kicked off from cores.</span>
<span class="quote">&gt; In this window, the oom reaper may calls tlb_flush_mmu_tlbonly() to flush</span>
<span class="quote">&gt; TLB then frees pages. However, due to the above problem, the TLB entries</span>
<span class="quote">&gt; are not really flushed on arm64. Other threads are possible to access</span>
<span class="quote">&gt; these pages through TLB entries. Moreover, a copy_to_user() can also</span>
<span class="quote">&gt; write to these pages without generating page fault, causes use-after-free</span>
<span class="quote">&gt; bugs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch gathers each vma instead of gathering full vm space.</span>
<span class="quote">&gt; In this case tlb-&gt;fullmm is not true. The behavior of oom reaper become</span>
<span class="quote">&gt; similar to munmapping before do_exit, which should be safe for all archs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Wang Nan &lt;wangnan0@huawei.com&gt;</span>
<span class="quote">&gt; Cc: Bob Liu &lt;liubo95@huawei.com&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Roman Gushchin &lt;guro@fb.com&gt;</span>
<span class="quote">&gt; Cc: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;</span>
<span class="quote">&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  mm/oom_kill.c | 7 ++++---</span>
<span class="quote">&gt;  1 file changed, 4 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; index dee0f75..18c5b35 100644</span>
<span class="quote">&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; @@ -532,7 +532,6 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt;  	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt;  		if (!can_madv_dontneed_vma(vma))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -547,11 +546,13 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  		 * we do not want to block exit_mmap by keeping mm ref</span>
<span class="quote">&gt;  		 * count elevated without a good reason.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; +		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="quote">&gt; +			tlb_gather_mmu(&amp;tlb, mm, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt;  			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt;  					 NULL);</span>
<span class="quote">&gt; +			tlb_finish_mmu(&amp;tlb, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt;  	pr_info(&quot;oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n&quot;,</span>
<span class="quote">&gt;  			task_pid_nr(tsk), tsk-&gt;comm,</span>
<span class="quote">&gt;  			K(get_mm_counter(mm, MM_ANONPAGES)),</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.10.1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index dee0f75..18c5b35 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -532,7 +532,6 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 */
 	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);
 
<span class="p_del">-	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
 	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {
 		if (!can_madv_dontneed_vma(vma))
 			continue;
<span class="p_chunk">@@ -547,11 +546,13 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 		 * we do not want to block exit_mmap by keeping mm ref
 		 * count elevated without a good reason.
 		 */
<span class="p_del">-		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="p_add">+			tlb_gather_mmu(&amp;tlb, mm, vma-&gt;vm_start, vma-&gt;vm_end);</span>
 			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,
 					 NULL);
<span class="p_add">+			tlb_finish_mmu(&amp;tlb, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="p_add">+		}</span>
 	}
<span class="p_del">-	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
 	pr_info(&quot;oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n&quot;,
 			task_pid_nr(tsk), tsk-&gt;comm,
 			K(get_mm_counter(mm, MM_ANONPAGES)),

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



