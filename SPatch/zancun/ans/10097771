
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>LDT improvements - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    LDT improvements</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 7, 2017, 7:22 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;48fe5cf1382d6a95c7b1837415882edcc81a9781.1512631324.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10097771/mbox/"
   >mbox</a>
|
   <a href="/patch/10097771/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10097771/">/patch/10097771/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	47B7E60329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Dec 2017 07:22:31 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 38F83281F9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Dec 2017 07:22:31 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2DE2A287F4; Thu,  7 Dec 2017 07:22:31 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5F9E5281F9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Dec 2017 07:22:30 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752433AbdLGHW1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 7 Dec 2017 02:22:27 -0500
Received: from mail.kernel.org ([198.145.29.99]:45588 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752405AbdLGHWW (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 7 Dec 2017 02:22:22 -0500
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 68DC8219A0;
	Thu,  7 Dec 2017 07:22:22 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 68DC8219A0
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Brian Gerst &lt;brgerst@gmail.com&gt;, David Laight &lt;David.Laight@aculab.com&gt;,
	Kees Cook &lt;keescook@chromium.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH] LDT improvements
Date: Wed,  6 Dec 2017 23:22:21 -0800
Message-Id: &lt;48fe5cf1382d6a95c7b1837415882edcc81a9781.1512631324.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.13.6
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 7, 2017, 7:22 a.m.</div>
<pre class="content">
I think I like this approach.  I also think it might be nice to move the
whole cpu_entry_area into this new pgd range so that we can stop mucking
around with the fixmap.

TODO:
 - It crashes in ldt_gdt_64.  Not sure why.
 - 5-level docs aren&#39;t updated and the code is untested.
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 Documentation/x86/x86_64/mm.txt         |  11 ++--
 arch/x86/include/asm/mmu_context.h      |  33 +++++++++-
 arch/x86/include/asm/pgtable_64_types.h |   2 +
 arch/x86/include/asm/processor.h        |  23 +++++--
 arch/x86/kernel/ldt.c                   | 109 +++++++++++++++++++++++++++++++-
 5 files changed, 161 insertions(+), 17 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - Dec. 7, 2017, 12:43 p.m.</div>
<pre class="content">
On Wed, Dec 06, 2017 at 11:22:21PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; I think I like this approach.  I also think it might be nice to move the</span>
<span class="quote">&gt; whole cpu_entry_area into this new pgd range so that we can stop mucking</span>
<span class="quote">&gt; around with the fixmap.</span>

Yeah, and also, I don&#39;t like the idea of sacrificing a whole PGD
only for the LDT crap which is optional, even. Frankly - and this
is just me - I&#39;d make CONFIG_KERNEL_PAGE_TABLE_ISOLATION xor
CONFIG_MODIFY_LDT_SYSCALL and don&#39;t give a rat&#39;s *ss about the LDT.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 7, 2017, 5:08 p.m.</div>
<pre class="content">
On Thu, Dec 7, 2017 at 4:43 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:
<span class="quote">&gt; On Wed, Dec 06, 2017 at 11:22:21PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; I think I like this approach.  I also think it might be nice to move the</span>
<span class="quote">&gt;&gt; whole cpu_entry_area into this new pgd range so that we can stop mucking</span>
<span class="quote">&gt;&gt; around with the fixmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, and also, I don&#39;t like the idea of sacrificing a whole PGD</span>
<span class="quote">&gt; only for the LDT crap which is optional, even. Frankly - and this</span>
<span class="quote">&gt; is just me - I&#39;d make CONFIG_KERNEL_PAGE_TABLE_ISOLATION xor</span>
<span class="quote">&gt; CONFIG_MODIFY_LDT_SYSCALL and don&#39;t give a rat&#39;s *ss about the LDT.</span>

The PGD sacrifice doesn&#39;t bother me.  Putting a writable LDT map at a
constant address does bother me.  We could probably get away with RO
if we trapped and handled the nasty faults, but that could be very
problematic.

The version here:

https://git.kernel.org/pub/scm/linux/kernel/git/luto/linux.git/commit/?h=x86/pti&amp;id=a74d1009ac72a1f04ec5bcd338a4bdbe170ab776

actually seems to work.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 7, 2017, 5:23 p.m.</div>
<pre class="content">
On Thu, 7 Dec 2017, Andy Lutomirski wrote:
<span class="quote">
&gt; On Thu, Dec 7, 2017 at 4:43 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:</span>
<span class="quote">&gt; &gt; On Wed, Dec 06, 2017 at 11:22:21PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; I think I like this approach.  I also think it might be nice to move the</span>
<span class="quote">&gt; &gt;&gt; whole cpu_entry_area into this new pgd range so that we can stop mucking</span>
<span class="quote">&gt; &gt;&gt; around with the fixmap.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yeah, and also, I don&#39;t like the idea of sacrificing a whole PGD</span>
<span class="quote">&gt; &gt; only for the LDT crap which is optional, even. Frankly - and this</span>
<span class="quote">&gt; &gt; is just me - I&#39;d make CONFIG_KERNEL_PAGE_TABLE_ISOLATION xor</span>
<span class="quote">&gt; &gt; CONFIG_MODIFY_LDT_SYSCALL and don&#39;t give a rat&#39;s *ss about the LDT.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The PGD sacrifice doesn&#39;t bother me.  Putting a writable LDT map at a</span>
<span class="quote">&gt; constant address does bother me.  We could probably get away with RO</span>
<span class="quote">&gt; if we trapped and handled the nasty faults, but that could be very</span>
<span class="quote">&gt; problematic.</span>

Where is the problem? You can map it RO into user space with the USER bit
cleared. The kernel knows how to access the real stuff.
<span class="quote">
&gt; The version here:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; https://git.kernel.org/pub/scm/linux/kernel/git/luto/linux.git/commit/?h=x86/pti&amp;id=a74d1009ac72a1f04ec5bcd338a4bdbe170ab776</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; actually seems to work.</span>

The approach I&#39;ve taken is to create a VMA and map it into user space with
the USER bit cleared. A little bit more effort code wise, but that avoids
all the page table muck and keeps it straight attached to the process.

Will post once in a bit.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Dec. 7, 2017, 6:21 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Dec 7, 2017, at 9:23 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Thu, 7 Dec 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On Thu, Dec 7, 2017 at 4:43 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Wed, Dec 06, 2017 at 11:22:21PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; I think I like this approach.  I also think it might be nice to move the</span>
<span class="quote">&gt;&gt;&gt;&gt; whole cpu_entry_area into this new pgd range so that we can stop mucking</span>
<span class="quote">&gt;&gt;&gt;&gt; around with the fixmap.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Yeah, and also, I don&#39;t like the idea of sacrificing a whole PGD</span>
<span class="quote">&gt;&gt;&gt; only for the LDT crap which is optional, even. Frankly - and this</span>
<span class="quote">&gt;&gt;&gt; is just me - I&#39;d make CONFIG_KERNEL_PAGE_TABLE_ISOLATION xor</span>
<span class="quote">&gt;&gt;&gt; CONFIG_MODIFY_LDT_SYSCALL and don&#39;t give a rat&#39;s *ss about the LDT.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The PGD sacrifice doesn&#39;t bother me.  Putting a writable LDT map at a</span>
<span class="quote">&gt;&gt; constant address does bother me.  We could probably get away with RO</span>
<span class="quote">&gt;&gt; if we trapped and handled the nasty faults, but that could be very</span>
<span class="quote">&gt;&gt; problematic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Where is the problem? You can map it RO into user space with the USER bit</span>
<span class="quote">&gt; cleared. The kernel knows how to access the real stuff.</span>

Blows up when the CPU tries to set the accessed bit.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; The version here:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; https://git.kernel.org/pub/scm/linux/kernel/git/luto/linux.git/commit/?h=x86/pti&amp;id=a74d1009ac72a1f04ec5bcd338a4bdbe170ab776</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; actually seems to work.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The approach I&#39;ve taken is to create a VMA and map it into user space with</span>
<span class="quote">&gt; the USER bit cleared. A little bit more effort code wise, but that avoids</span>
<span class="quote">&gt; all the page table muck and keeps it straight attached to the process.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will post once in a bit.</span>

I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about putting it in our near anything that could pass an access_ok check, since we&#39;re totally screwed if the bad guys can figure out how to write to it.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    tglx</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Dec. 8, 2017, 7:34 a.m.</div>
<pre class="content">
* Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Dec 7, 2017, at 9:23 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; On Thu, 7 Dec 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; On Thu, Dec 7, 2017 at 4:43 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; On Wed, Dec 06, 2017 at 11:22:21PM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; I think I like this approach.  I also think it might be nice to move the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; whole cpu_entry_area into this new pgd range so that we can stop mucking</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; around with the fixmap.</span>
<span class="quote">&gt; &gt;&gt;&gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; Yeah, and also, I don&#39;t like the idea of sacrificing a whole PGD</span>
<span class="quote">&gt; &gt;&gt;&gt; only for the LDT crap which is optional, even. Frankly - and this</span>
<span class="quote">&gt; &gt;&gt;&gt; is just me - I&#39;d make CONFIG_KERNEL_PAGE_TABLE_ISOLATION xor</span>
<span class="quote">&gt; &gt;&gt;&gt; CONFIG_MODIFY_LDT_SYSCALL and don&#39;t give a rat&#39;s *ss about the LDT.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; The PGD sacrifice doesn&#39;t bother me.  Putting a writable LDT map at a</span>
<span class="quote">&gt; &gt;&gt; constant address does bother me.  We could probably get away with RO</span>
<span class="quote">&gt; &gt;&gt; if we trapped and handled the nasty faults, but that could be very</span>
<span class="quote">&gt; &gt;&gt; problematic.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Where is the problem? You can map it RO into user space with the USER bit</span>
<span class="quote">&gt; &gt; cleared. The kernel knows how to access the real stuff.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Blows up when the CPU tries to set the accessed bit.</span>

BTW., could we force the accessed bit to be always set, without breaking the ABI?
<span class="quote">
&gt; &gt; The approach I&#39;ve taken is to create a VMA and map it into user space with</span>
<span class="quote">&gt; &gt; the USER bit cleared. A little bit more effort code wise, but that avoids</span>
<span class="quote">&gt; &gt; all the page table muck and keeps it straight attached to the process.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Will post once in a bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt; totally screwed if the bad guys can figure out how to write to it.</span>

Hm, robustness of the LDT address wrt. access_ok() is a valid concern.

Can we have vmas with high addresses, in the vmalloc space for example?
IIRC the GPU code has precedents in that area.

Since this is x86-64, limitation of the vmalloc() space is not an issue.

I like Thomas&#39;s solution:

 - have the LDT in a regular mmap space vma (hence per process ASLR randomized), 
   but with the system bit set.

 - That would be an advantage even for non-PTI kernels, because mmap() is probably 
   more randomized than kmalloc().

 - It would also be a cleaner approach all around, and would avoid the fixmap
   complications and the scheduler muckery.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 8, 2017, 9:34 a.m.</div>
<pre class="content">
On Fri, 8 Dec 2017, Ingo Molnar wrote:
<span class="quote">&gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt; &gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt; &gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - have the LDT in a regular mmap space vma (hence per process ASLR randomized), </span>
<span class="quote">&gt;    but with the system bit set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - That would be an advantage even for non-PTI kernels, because mmap() is probably </span>
<span class="quote">&gt;    more randomized than kmalloc().</span>

Randomization is pointless as long as you can get the LDT address in user
space, i.e. w/o UMIP.
<span class="quote">
&gt;  - It would also be a cleaner approach all around, and would avoid the fixmap</span>
<span class="quote">&gt;    complications and the scheduler muckery.</span>

The error code of such an access is always 0x03. So I added a special
handler, which checks whether the address is in the LDT map range and
verifies that the access bit in the descriptor is 0. If that&#39;s the case it
sets it and returns. If not, the thing dies. That works.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Dec. 8, 2017, 9:44 a.m.</div>
<pre class="content">
* Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">
&gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt; &gt; &gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt; &gt; &gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt; &gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  - have the LDT in a regular mmap space vma (hence per process ASLR randomized), </span>
<span class="quote">&gt; &gt;    but with the system bit set.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  - That would be an advantage even for non-PTI kernels, because mmap() is probably </span>
<span class="quote">&gt; &gt;    more randomized than kmalloc().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Randomization is pointless as long as you can get the LDT address in user</span>
<span class="quote">&gt; space, i.e. w/o UMIP.</span>

But with UMIP unprivileged user-space won&#39;t be able to get the linear address of 
the LDT. Now it&#39;s written out in /proc/self/maps.
<span class="quote">
&gt; &gt;  - It would also be a cleaner approach all around, and would avoid the fixmap</span>
<span class="quote">&gt; &gt;    complications and the scheduler muckery.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt; sets it and returns. If not, the thing dies. That works.</span>

Are SMP races possible? For example two threads both triggering the accessed bit 
fault, but only one of them succeeding in setting it. The other thread should not 
die in this case, right?

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 8, 2017, 9:55 a.m.</div>
<pre class="content">
On Fri, 8 Dec 2017, Ingo Molnar wrote:
<span class="quote">&gt; * Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; &gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt; &gt; &gt; &gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt; &gt; &gt; &gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt; &gt; &gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;  - have the LDT in a regular mmap space vma (hence per process ASLR randomized), </span>
<span class="quote">&gt; &gt; &gt;    but with the system bit set.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;  - That would be an advantage even for non-PTI kernels, because mmap() is probably </span>
<span class="quote">&gt; &gt; &gt;    more randomized than kmalloc().</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Randomization is pointless as long as you can get the LDT address in user</span>
<span class="quote">&gt; &gt; space, i.e. w/o UMIP.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But with UMIP unprivileged user-space won&#39;t be able to get the linear address of </span>
<span class="quote">&gt; the LDT. Now it&#39;s written out in /proc/self/maps.</span>

We can expose it nameless like other VMAs, but then it&#39;s 128k sized so it
can be figured out. But when it&#39;s RO then it&#39;s not really a problem, even
the kernel can&#39;t write to it.
<span class="quote">
&gt; &gt; &gt;  - It would also be a cleaner approach all around, and would avoid the fixmap</span>
<span class="quote">&gt; &gt; &gt;    complications and the scheduler muckery.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt; &gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt; &gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt; &gt; sets it and returns. If not, the thing dies. That works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Are SMP races possible? For example two threads both triggering the accessed bit </span>
<span class="quote">&gt; fault, but only one of them succeeding in setting it. The other thread should not </span>
<span class="quote">&gt; die in this case, right?</span>

Right. I&#39;m trying to figure out whether there is a way to reliably detect
that write access bit mode.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Dec. 8, 2017, 11:31 a.m.</div>
<pre class="content">
* Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">
&gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; * Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt; &gt; &gt; &gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt;  - have the LDT in a regular mmap space vma (hence per process ASLR randomized), </span>
<span class="quote">&gt; &gt; &gt; &gt;    but with the system bit set.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt;  - That would be an advantage even for non-PTI kernels, because mmap() is probably </span>
<span class="quote">&gt; &gt; &gt; &gt;    more randomized than kmalloc().</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Randomization is pointless as long as you can get the LDT address in user</span>
<span class="quote">&gt; &gt; &gt; space, i.e. w/o UMIP.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But with UMIP unprivileged user-space won&#39;t be able to get the linear address of </span>
<span class="quote">&gt; &gt; the LDT. Now it&#39;s written out in /proc/self/maps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can expose it nameless like other VMAs, but then it&#39;s 128k sized so it</span>
<span class="quote">&gt; can be figured out. But when it&#39;s RO then it&#39;s not really a problem, even</span>
<span class="quote">&gt; the kernel can&#39;t write to it.</span>

Yeah, ok. I don&#39;t think we should hide it - if it&#39;s in the vma space it should be 
listed in the &#39;maps&#39; file, and with a descriptive name.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Dec. 8, 2017, 1:20 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Dec 8, 2017, at 1:34 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt;&gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about </span>
<span class="quote">&gt;&gt;&gt; putting it in our near anything that could pass an access_ok check, since we&#39;re </span>
<span class="quote">&gt;&gt;&gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt;&gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; - have the LDT in a regular mmap space vma (hence per process ASLR randomized), </span>
<span class="quote">&gt;&gt;   but with the system bit set.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; - That would be an advantage even for non-PTI kernels, because mmap() is probably </span>
<span class="quote">&gt;&gt;   more randomized than kmalloc().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Randomization is pointless as long as you can get the LDT address in user</span>
<span class="quote">&gt; space, i.e. w/o UMIP.</span>

You only get the LDT selector, not the address.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; - It would also be a cleaner approach all around, and would avoid the fixmap</span>
<span class="quote">&gt;&gt;   complications and the scheduler muckery.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt; sets it and returns. If not, the thing dies. That works.</span>

What if you are in kernel mode and try to return to a context with SS or CS pointing to a non-accessed segment?  Or what if you try to schedule to a context with fs or, worse, gs pointing to such a segment?
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    tglx</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=21121">David Laight</a> - Dec. 8, 2017, 1:55 p.m.</div>
<pre class="content">
<span class="from">From: Andy Lutomirski</span>
<span class="quote">&gt; Sent: 08 December 2017 13:20</span>
...
<span class="quote">&gt; &gt;&gt; - It would also be a cleaner approach all around, and would avoid the fixmap</span>
<span class="quote">&gt; &gt;&gt;   complications and the scheduler muckery.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt; &gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt; &gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt; &gt; sets it and returns. If not, the thing dies. That works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What if you are in kernel mode and try to return to a context with SS or CS pointing to a non-accessed</span>
<span class="quote">&gt; segment?</span>
<span class="quote">&gt; Or what if you try to schedule to a context with fs or, worse, gs pointing to such a segment?</span>

Well, the cpu will fault in kernel on the &#39;pop %xs&#39; or &#39;iret&#39; instruction.
These all (probably) happen on the kernel stack with the usergs loaded.
So the fault handler has to look at the opcodes and/or %pc value, sort
out the stack (etc) and then generate SIGSEGV.

I&#39;m not sure the kernel needs to know why the segment selector is invalid.

	David
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Dec. 8, 2017, 2:06 p.m.</div>
<pre class="content">
On Fri, Dec 08, 2017 at 05:20:00AM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt; &gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt; &gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt; &gt; sets it and returns. If not, the thing dies. That works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What if you are in kernel mode and try to return to a context with SS</span>
<span class="quote">&gt; or CS pointing to a non-accessed segment?  Or what if you try to</span>
<span class="quote">&gt; schedule to a context with fs or, worse, gs pointing to such a</span>
<span class="quote">&gt; segment?</span>

How would that be different from setting a &#39;crap&#39; GS in modify_ldt() and
then returning from the syscall? That is something we should be able to
deal with already, no?

Is this something ldt_gdt.c already tests? The current &quot;Test GS&quot; is in
test_gdt_invalidation() which seems to suggest not.

Could we get a testcase for the exact situation you worry about? I&#39;m not
sure I&#39;d trust myself to get it right, all this LDT magic is new to me.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 8, 2017, 4:33 p.m.</div>
<pre class="content">
On Fri, Dec 8, 2017 at 6:06 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Fri, Dec 08, 2017 at 05:20:00AM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; The error code of such an access is always 0x03. So I added a special</span>
<span class="quote">&gt;&gt; &gt; handler, which checks whether the address is in the LDT map range and</span>
<span class="quote">&gt;&gt; &gt; verifies that the access bit in the descriptor is 0. If that&#39;s the case it</span>
<span class="quote">&gt;&gt; &gt; sets it and returns. If not, the thing dies. That works.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What if you are in kernel mode and try to return to a context with SS</span>
<span class="quote">&gt;&gt; or CS pointing to a non-accessed segment?  Or what if you try to</span>
<span class="quote">&gt;&gt; schedule to a context with fs or, worse, gs pointing to such a</span>
<span class="quote">&gt;&gt; segment?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; How would that be different from setting a &#39;crap&#39; GS in modify_ldt() and</span>
<span class="quote">&gt; then returning from the syscall? That is something we should be able to</span>
<span class="quote">&gt; deal with already, no?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is this something ldt_gdt.c already tests? The current &quot;Test GS&quot; is in</span>
<span class="quote">&gt; test_gdt_invalidation() which seems to suggest not.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Could we get a testcase for the exact situation you worry about? I&#39;m not</span>
<span class="quote">&gt; sure I&#39;d trust myself to get it right, all this LDT magic is new to me.</span>

#GP on IRET is a failure, and we have disgusting code to handle it.
#PF on IRET would not be a failure -- it&#39;s a case where IRET should be
retried.  Our crap that fixes up #GP would get that wrong and leave us
with the wrong GSBASE.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 8, 2017, 4:38 p.m.</div>
<pre class="content">
On Fri, Dec 8, 2017 at 3:31 AM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; * Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt;&gt; &gt; * Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; On Fri, 8 Dec 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; &gt; I don&#39;t love mucking with user address space.  I&#39;m also quite nervous about</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; &gt; putting it in our near anything that could pass an access_ok check, since we&#39;re</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; &gt; totally screwed if the bad guys can figure out how to write to it.</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; Hm, robustness of the LDT address wrt. access_ok() is a valid concern.</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; Can we have vmas with high addresses, in the vmalloc space for example?</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; IIRC the GPU code has precedents in that area.</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; Since this is x86-64, limitation of the vmalloc() space is not an issue.</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt; I like Thomas&#39;s solution:</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;  - have the LDT in a regular mmap space vma (hence per process ASLR randomized),</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;    but with the system bit set.</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;  - That would be an advantage even for non-PTI kernels, because mmap() is probably</span>
<span class="quote">&gt;&gt; &gt; &gt; &gt;    more randomized than kmalloc().</span>
<span class="quote">&gt;&gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; Randomization is pointless as long as you can get the LDT address in user</span>
<span class="quote">&gt;&gt; &gt; &gt; space, i.e. w/o UMIP.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; But with UMIP unprivileged user-space won&#39;t be able to get the linear address of</span>
<span class="quote">&gt;&gt; &gt; the LDT. Now it&#39;s written out in /proc/self/maps.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; We can expose it nameless like other VMAs, but then it&#39;s 128k sized so it</span>
<span class="quote">&gt;&gt; can be figured out. But when it&#39;s RO then it&#39;s not really a problem, even</span>
<span class="quote">&gt;&gt; the kernel can&#39;t write to it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, ok. I don&#39;t think we should hide it - if it&#39;s in the vma space it should be</span>
<span class="quote">&gt; listed in the &#39;maps&#39; file, and with a descriptive name.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         Ingo</span>

Can we take a step back here?  I think there are four vaguely sane
ways to make the LDT work:

1. The way it is right now -- in vmalloc space.  The only real
downside is that it requires exposing that part of vmalloc space in
the user tables, which is a bit gross.

2. In some fixmap-like space, which is what my patch does, albeit
buggily.  This requires a PGD that we treat as per-mm, not per-cpu,
but that&#39;s not so bad.

3. In one of the user PGDs but above TASK_SIZE_MAX.  This is
functionally almost identical to #2, except that there&#39;s more concern
about exploits that write past TASK_SIZE_MAX.

4. In an actual vma.  I don&#39;t see the benefit of doing this at all --
it&#39;s just like #2 except way more error prone.  Hell, you have to make
sure that you can&#39;t munmap or mremap it, which isn&#39;t a consideration
at all with the other choices.

Why all the effort to make #4 work?  #1 is working fine right now, and
#2 is half-implemented.  #3 code-wise looks just like #2 except for
the choice of address and the interation with PTI&#39;s shitty PGD
handling.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=21121">David Laight</a> - Dec. 8, 2017, 4:46 p.m.</div>
<pre class="content">
<span class="from">From: Andy Lutomirski</span>
<span class="quote">&gt; Sent: 08 December 2017 16:34</span>
<span class="quote">
&gt; #GP on IRET is a failure, and we have disgusting code to handle it.</span>

Is that the trap in kernel space when the on-stack segment registers
are invalid?
Definitely needs horrid code...
<span class="quote">
&gt; #PF on IRET would not be a failure -- it&#39;s a case where IRET should be</span>
<span class="quote">&gt; retried.  Our crap that fixes up #GP would get that wrong and leave us</span>
<span class="quote">&gt; with the wrong GSBASE.</span>

If the user code page isn&#39;t present then the fault happens after the
return to user mode, not on the IRET instruction in kernel mode.
So it is not really any different to returning to a NOP at the end
of a resident page when the page following is absent.
(Or any other invalid %ip value.)

SWAPGS is a PITA, should have been SAVEGS, LOAD_KERNEL_GS, and READ_SAVED_GS.

	David
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 8, 2017, 4:47 p.m.</div>
<pre class="content">
On Fri, Dec 8, 2017 at 8:46 AM, David Laight &lt;David.Laight@aculab.com&gt; wrote:
<span class="quote">&gt; From: Andy Lutomirski</span>
<span class="quote">&gt;&gt; Sent: 08 December 2017 16:34</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; #GP on IRET is a failure, and we have disgusting code to handle it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is that the trap in kernel space when the on-stack segment registers</span>
<span class="quote">&gt; are invalid?</span>
<span class="quote">&gt; Definitely needs horrid code...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; #PF on IRET would not be a failure -- it&#39;s a case where IRET should be</span>
<span class="quote">&gt;&gt; retried.  Our crap that fixes up #GP would get that wrong and leave us</span>
<span class="quote">&gt;&gt; with the wrong GSBASE.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If the user code page isn&#39;t present then the fault happens after the</span>
<span class="quote">&gt; return to user mode, not on the IRET instruction in kernel mode.</span>
<span class="quote">&gt; So it is not really any different to returning to a NOP at the end</span>
<span class="quote">&gt; of a resident page when the page following is absent.</span>
<span class="quote">&gt; (Or any other invalid %ip value.)</span>

I mean: if the user CS or SS is not accessed and the LDT is RO, then
we get #PF on the IRET instruction, I think.  Dealing with that is
truly awful.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=21121">David Laight</a> - Dec. 8, 2017, 5:29 p.m.</div>
<pre class="content">
<span class="from">From: Andy Lutomirski</span>
<span class="quote">&gt; Sent: 08 December 2017 16:48</span>
...
<span class="quote">&gt; I mean: if the user CS or SS is not accessed and the LDT is RO, then</span>
<span class="quote">&gt; we get #PF on the IRET instruction, I think.  Dealing with that is</span>
<span class="quote">&gt; truly awful.</span>

Any fault in-kernel on the IRET is horrid.
Doesn&#39;t really matter which one.
Same goes for the &#39;pop %ds&#39; (etc) that tend to precede it.

	David
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 8, 2017, 5:37 p.m.</div>
<pre class="content">
On Fri, 8 Dec 2017, Andy Lutomirski wrote:
<span class="quote">&gt; Can we take a step back here?  I think there are four vaguely sane</span>
<span class="quote">&gt; ways to make the LDT work:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1. The way it is right now -- in vmalloc space.  The only real</span>
<span class="quote">&gt; downside is that it requires exposing that part of vmalloc space in</span>
<span class="quote">&gt; the user tables, which is a bit gross.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2. In some fixmap-like space, which is what my patch does, albeit</span>
<span class="quote">&gt; buggily.  This requires a PGD that we treat as per-mm, not per-cpu,</span>
<span class="quote">&gt; but that&#39;s not so bad.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3. In one of the user PGDs but above TASK_SIZE_MAX.  This is</span>
<span class="quote">&gt; functionally almost identical to #2, except that there&#39;s more concern</span>
<span class="quote">&gt; about exploits that write past TASK_SIZE_MAX.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 4. In an actual vma.  I don&#39;t see the benefit of doing this at all --</span>
<span class="quote">&gt; it&#39;s just like #2 except way more error prone.  Hell, you have to make</span>
<span class="quote">&gt; sure that you can&#39;t munmap or mremap it, which isn&#39;t a consideration</span>
<span class="quote">&gt; at all with the other choices.</span>

Why? You can unmap vdso or uprobe or whatever VMAs and you will simply
die. You get what you asked for.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 8, 2017, 5:42 p.m.</div>
<pre class="content">
On Fri, Dec 8, 2017 at 9:37 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Fri, 8 Dec 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; Can we take a step back here?  I think there are four vaguely sane</span>
<span class="quote">&gt;&gt; ways to make the LDT work:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 1. The way it is right now -- in vmalloc space.  The only real</span>
<span class="quote">&gt;&gt; downside is that it requires exposing that part of vmalloc space in</span>
<span class="quote">&gt;&gt; the user tables, which is a bit gross.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 2. In some fixmap-like space, which is what my patch does, albeit</span>
<span class="quote">&gt;&gt; buggily.  This requires a PGD that we treat as per-mm, not per-cpu,</span>
<span class="quote">&gt;&gt; but that&#39;s not so bad.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 3. In one of the user PGDs but above TASK_SIZE_MAX.  This is</span>
<span class="quote">&gt;&gt; functionally almost identical to #2, except that there&#39;s more concern</span>
<span class="quote">&gt;&gt; about exploits that write past TASK_SIZE_MAX.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 4. In an actual vma.  I don&#39;t see the benefit of doing this at all --</span>
<span class="quote">&gt;&gt; it&#39;s just like #2 except way more error prone.  Hell, you have to make</span>
<span class="quote">&gt;&gt; sure that you can&#39;t munmap or mremap it, which isn&#39;t a consideration</span>
<span class="quote">&gt;&gt; at all with the other choices.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why? You can unmap vdso or uprobe or whatever VMAs and you will simply</span>
<span class="quote">&gt; die. You get what you asked for.</span>

But if you unmap ldt and then map something else at the same VA, you&#39;re root.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Dec. 8, 2017, 5:48 p.m.</div>
<pre class="content">
On Fri, Dec 08, 2017 at 08:38:26AM -0800, Andy Lutomirski wrote:
<span class="quote">
&gt; 4. In an actual vma.  I don&#39;t see the benefit of doing this at all --</span>
<span class="quote">&gt; it&#39;s just like #2 except way more error prone.  Hell, you have to make</span>
<span class="quote">&gt; sure that you can&#39;t munmap or mremap it, which isn&#39;t a consideration</span>
<span class="quote">&gt; at all with the other choices.</span>

mremap is trivially disabled. I&#39;ve not tried munmap() yet, as long as it
just kills the process doing it we&#39;re good of course. Otherwise we need
an extra callback in do_munmap() which isn&#39;t too hard.
<span class="quote">
&gt; Why all the effort to make #4 work?</span>

Seemed like a sensible approach; I really dislike wasting an entire pmd
or whatever on a feature &#39;nobody&#39; ever uses anyway.
<span class="quote">
&gt; #1 is working fine right now</span>

doesn&#39;t work for pti in its current form.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 2d7d6590ade8..bfa44e1cb293 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,13 +12,15 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT range</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space (variable)</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space (variable)</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Virtual memory map with 5 level page tables:
<span class="p_chunk">@@ -39,8 +41,9 @@</span> <span class="p_context"> ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks</span>
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Architecture defines a 64-bit virtual address. Implementations can support
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5e1a1ecb65c6..eb87bbeddacc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -52,13 +52,29 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 */
 	struct desc_struct *entries;
 	unsigned int nr_entries;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
 int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline int init_new_context_ldt(struct task_struct *tsk,
 				       struct mm_struct *mm)
<span class="p_chunk">@@ -90,10 +106,20 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -185,6 +211,7 @@</span> <span class="p_context"> static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6d5f45dcd4a1..130f575f8d1e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -100,6 +100,8 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
 #define ESPFIX_PGD_ENTRY _AC(-2, UL)
 #define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-3, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #define EFI_VA_START	 ( -4 * (_AC(1, UL) &lt;&lt; 30))
 #define EFI_VA_END	 (-68 * (_AC(1, UL) &lt;&lt; 30))
 
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index ae5615b03def..a0008fb26ba2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -46,13 +47,12 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	__flush_tlb_all();</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -90,9 +90,93 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 	}
 
 	new_ldt-&gt;nr_entries = num_entries;
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
 	return new_ldt;
 }
 
<span class="p_add">+static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	bool is_vmalloc;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	bool awful_hack;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Both LDT slots are contained in a single PMD. */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+</span>
<span class="p_add">+	awful_hack = pgd_none(*pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	pmd = pmd_alloc(mm, pud, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!pmd)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	if (pte_alloc(mm, pmd, LDT_BASE_ADDR))</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (true) {</span>
<span class="p_add">+		/* awful hack -- pti_set_user_pgd is a mess */</span>
<span class="p_add">+		/* we can&#39;t even use the bool awful_hack because of pti_init_all_pgds(), which poops on our pgd. */</span>
<span class="p_add">+		kernel_to_user_pgdp(pgd)-&gt;pgd = pgd-&gt;pgd;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_offset_kernel(pmd, va);</span>
<span class="p_add">+		set_pte(pte, pfn_pte(pfn, __pgprot(__PAGE_KERNEL_RO &amp; ~_PAGE_GLOBAL)));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_mm_range(mm,</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot),</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="p_add">+			   0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return -EINVAL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -155,8 +239,17 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval)</span>
<span class="p_add">+		goto out_free;</span>
 
 	mm-&gt;context.ldt = new_ldt;
<span class="p_add">+	goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+	free_ldt_struct(new_ldt);</span>
<span class="p_add">+	return retval;</span>
 
 out_unlock:
 	mutex_unlock(&amp;old_mm-&gt;context.lock);
<span class="p_chunk">@@ -174,6 +267,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -285,6 +383,11 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



