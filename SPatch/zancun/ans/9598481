
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V5,6/6] proc: show MADV_FREE pages info in smaps - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V5,6/6] proc: show MADV_FREE pages info in smaps</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 1, 2017, 1:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170301133624.GF1124@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9598481/mbox/"
   >mbox</a>
|
   <a href="/patch/9598481/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9598481/">/patch/9598481/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EF336604DC for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  1 Mar 2017 14:13:55 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E439328563
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  1 Mar 2017 14:13:55 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D91BB2856B; Wed,  1 Mar 2017 14:13:55 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6C42328563
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  1 Mar 2017 14:13:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751950AbdCAONy (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 1 Mar 2017 09:13:54 -0500
Received: from mx2.suse.de ([195.135.220.15]:44357 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751882AbdCAONt (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 1 Mar 2017 09:13:49 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 09E8BABA2;
	Wed,  1 Mar 2017 13:36:26 +0000 (UTC)
Date: Wed, 1 Mar 2017 14:36:24 +0100
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Shaohua Li &lt;shli@fb.com&gt;
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Kernel-team@fb.com, minchan@kernel.org, hughd@google.com,
	hannes@cmpxchg.org, riel@redhat.com, mgorman@techsingularity.net,
	akpm@linux-foundation.org
Subject: Re: [PATCH V5 6/6] proc: show MADV_FREE pages info in smaps
Message-ID: &lt;20170301133624.GF1124@dhcp22.suse.cz&gt;
References: &lt;cover.1487965799.git.shli@fb.com&gt;
	&lt;89efde633559de1ec07444f2ef0f4963a97a2ce8.1487965799.git.shli@fb.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;89efde633559de1ec07444f2ef0f4963a97a2ce8.1487965799.git.shli@fb.com&gt;
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 1, 2017, 1:36 p.m.</div>
<pre class="content">
On Fri 24-02-17 13:31:49, Shaohua Li wrote:
<span class="quote">&gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; can get accurate accounting info about MADV_FREE pages.</span>

I have just got to test this patchset and noticed something that was a
bit surprising

madvise(mmap(len), len, MADV_FREE)
Size:             102400 kB
Rss:              102400 kB
Pss:              102400 kB
Shared_Clean:          0 kB
Shared_Dirty:          0 kB
Private_Clean:    102400 kB
Private_Dirty:         0 kB
Referenced:            0 kB
Anonymous:        102400 kB
LazyFree:         102368 kB

It took me a some time to realize that LazyFree is not accurate because
there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this
is an implementation detail which shouldn&#39;t be visible to the userspace.
Should we simply drain the pagevec? A crude way would be to simply
lru_add_drain_all after we are done with the given range. We can also
make this lru_lazyfree_pvecs specific but I am not sure this is worth
the additional code.
---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - March 1, 2017, 5:37 p.m.</div>
<pre class="content">
On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; bit surprising</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; Size:             102400 kB</span>
<span class="quote">&gt; Rss:              102400 kB</span>
<span class="quote">&gt; Pss:              102400 kB</span>
<span class="quote">&gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; Referenced:            0 kB</span>
<span class="quote">&gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; the additional code.</span>

Minchan&#39;s original patch includes a drain of pvec. I discard it because I think
it&#39;s not worth the effort. There aren&#39;t too many memory in the per-cpu vecs.
Like what you said, I doubt this is noticeable to userspace.

Thanks,
Shaohua
<span class="quote">

&gt; ---</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index dc5927c812d3..d2c318db16c9 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -474,7 +474,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	lru_add_drain_all();</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 1, 2017, 5:49 p.m.</div>
<pre class="content">
On Wed 01-03-17 09:37:10, Shaohua Li wrote:
<span class="quote">&gt; On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; &gt; bit surprising</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; &gt; Size:             102400 kB</span>
<span class="quote">&gt; &gt; Rss:              102400 kB</span>
<span class="quote">&gt; &gt; Pss:              102400 kB</span>
<span class="quote">&gt; &gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; &gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; &gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; &gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; &gt; Referenced:            0 kB</span>
<span class="quote">&gt; &gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; &gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; &gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; &gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; &gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; &gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; &gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; &gt; the additional code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Minchan&#39;s original patch includes a drain of pvec. I discard it because I think</span>
<span class="quote">&gt; it&#39;s not worth the effort. There aren&#39;t too many memory in the per-cpu vecs.</span>

but multiply that by the number of CPUs.
<span class="quote">
&gt; Like what you said, I doubt this is noticeable to userspace.</span>

maybe I wasn&#39;t clear enough. I&#39;ve noticed and I expect others would as
well. We really shouldn&#39;t leak implementation details like that. So I
_believe_ this should be fixed. Draining all pagevecs is rather coarse
but it is the simplest thing to do. If you do not want to fold this
into the original patch I can send a standalone one. Or do you have any
concerns about draining?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - March 1, 2017, 6:18 p.m.</div>
<pre class="content">
On Wed, Mar 01, 2017 at 06:49:56PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 01-03-17 09:37:10, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; &gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; &gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; &gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; &gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; &gt; &gt; bit surprising</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; &gt; &gt; Size:             102400 kB</span>
<span class="quote">&gt; &gt; &gt; Rss:              102400 kB</span>
<span class="quote">&gt; &gt; &gt; Pss:              102400 kB</span>
<span class="quote">&gt; &gt; &gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; &gt; &gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; &gt; &gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; &gt; &gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; &gt; &gt; Referenced:            0 kB</span>
<span class="quote">&gt; &gt; &gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; &gt; &gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; &gt; &gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; &gt; &gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; &gt; &gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; &gt; &gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; &gt; &gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; &gt; &gt; the additional code.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Minchan&#39;s original patch includes a drain of pvec. I discard it because I think</span>
<span class="quote">&gt; &gt; it&#39;s not worth the effort. There aren&#39;t too many memory in the per-cpu vecs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; but multiply that by the number of CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Like what you said, I doubt this is noticeable to userspace.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; maybe I wasn&#39;t clear enough. I&#39;ve noticed and I expect others would as</span>
<span class="quote">&gt; well. We really shouldn&#39;t leak implementation details like that. So I</span>
<span class="quote">&gt; _believe_ this should be fixed. Draining all pagevecs is rather coarse</span>
<span class="quote">&gt; but it is the simplest thing to do. If you do not want to fold this</span>
<span class="quote">&gt; into the original patch I can send a standalone one. Or do you have any</span>
<span class="quote">&gt; concerns about draining?</span>

No, no objection at all. Just doubt it&#39;s worthy. Looks nobody complains similar
issue, For exmaple, deactivate_file_page does the similar thing, then the smaps
&#39;Referenced&#39; could be inaccurate.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - March 1, 2017, 6:31 p.m.</div>
<pre class="content">
On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; bit surprising</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; Size:             102400 kB</span>
<span class="quote">&gt; Rss:              102400 kB</span>
<span class="quote">&gt; Pss:              102400 kB</span>
<span class="quote">&gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; Referenced:            0 kB</span>
<span class="quote">&gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; the additional code.</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index dc5927c812d3..d2c318db16c9 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -474,7 +474,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	lru_add_drain_all();</span>

A full drain on all CPUs is very expensive and IMO not justified for
some per-cpu fuzz factor in the stats. I&#39;d take hampering the stats
over hampering the syscall any day; only a subset of MADV_FREE users
will look at the stats.

And while the aggregate error can be large on machines with many CPUs
(notably the machines on which you absolutely don&#39;t want to send IPIs
to all cores each time a thread madvises some pages!), the pages of a
single process are not likely to be spread out across more than a few
CPUs. The error when reading a specific smaps should be completely ok.

In numbers: even if your process is madvising from 16 different CPUs,
the error in its smaps file will peak at 896K in the worst case. That
level of concurrency tends to come with much bigger memory quantities
for that amount of error to matter.

IMO this is a non-issue.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 1, 2017, 6:57 p.m.</div>
<pre class="content">
On Wed 01-03-17 13:31:49, Johannes Weiner wrote:
<span class="quote">&gt; On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; &gt; bit surprising</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; &gt; Size:             102400 kB</span>
<span class="quote">&gt; &gt; Rss:              102400 kB</span>
<span class="quote">&gt; &gt; Pss:              102400 kB</span>
<span class="quote">&gt; &gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; &gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; &gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; &gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; &gt; Referenced:            0 kB</span>
<span class="quote">&gt; &gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; &gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; &gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; &gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; &gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; &gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; &gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; &gt; the additional code.</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; &gt; index dc5927c812d3..d2c318db16c9 100644</span>
<span class="quote">&gt; &gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; &gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; &gt; @@ -474,7 +474,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt; &gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; +	lru_add_drain_all();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A full drain on all CPUs is very expensive and IMO not justified for</span>
<span class="quote">&gt; some per-cpu fuzz factor in the stats. I&#39;d take hampering the stats</span>
<span class="quote">&gt; over hampering the syscall any day; only a subset of MADV_FREE users</span>
<span class="quote">&gt; will look at the stats.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And while the aggregate error can be large on machines with many CPUs</span>
<span class="quote">&gt; (notably the machines on which you absolutely don&#39;t want to send IPIs</span>
<span class="quote">&gt; to all cores each time a thread madvises some pages!),</span>

I am not sure I understand. Where would we trigger IPIs?
lru_add_drain_all relies on workqueus.
<span class="quote">
&gt; the pages of a</span>
<span class="quote">&gt; single process are not likely to be spread out across more than a few</span>
<span class="quote">&gt; CPUs.</span>

Then we can simply only flushe lru_lazyfree_pvecs which should reduce
the unrelated noise from other pagevecs.
<span class="quote">
&gt; The error when reading a specific smaps should be completely ok.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In numbers: even if your process is madvising from 16 different CPUs,</span>
<span class="quote">&gt; the error in its smaps file will peak at 896K in the worst case. That</span>
<span class="quote">&gt; level of concurrency tends to come with much bigger memory quantities</span>
<span class="quote">&gt; for that amount of error to matter.</span>

It is still an unexpected behavior IMHO and an implementation detail
which leaks to the userspace.
<span class="quote"> 
&gt; IMO this is a non-issue.</span>

I will not insist if there is a general consensus on this and it is a
documented behavior, though.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - March 2, 2017, 7:39 a.m.</div>
<pre class="content">
On Wed, Mar 01, 2017 at 07:57:35PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 01-03-17 13:31:49, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri 24-02-17 13:31:49, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; show MADV_FREE pages info of each vma in smaps. The interface is for</span>
<span class="quote">&gt; &gt; &gt; &gt; diganose or monitoring purpose, userspace could use it to understand</span>
<span class="quote">&gt; &gt; &gt; &gt; what happens in the application. Since userspace could dirty MADV_FREE</span>
<span class="quote">&gt; &gt; &gt; &gt; pages without notice from kernel, this interface is the only place we</span>
<span class="quote">&gt; &gt; &gt; &gt; can get accurate accounting info about MADV_FREE pages.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I have just got to test this patchset and noticed something that was a</span>
<span class="quote">&gt; &gt; &gt; bit surprising</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; madvise(mmap(len), len, MADV_FREE)</span>
<span class="quote">&gt; &gt; &gt; Size:             102400 kB</span>
<span class="quote">&gt; &gt; &gt; Rss:              102400 kB</span>
<span class="quote">&gt; &gt; &gt; Pss:              102400 kB</span>
<span class="quote">&gt; &gt; &gt; Shared_Clean:          0 kB</span>
<span class="quote">&gt; &gt; &gt; Shared_Dirty:          0 kB</span>
<span class="quote">&gt; &gt; &gt; Private_Clean:    102400 kB</span>
<span class="quote">&gt; &gt; &gt; Private_Dirty:         0 kB</span>
<span class="quote">&gt; &gt; &gt; Referenced:            0 kB</span>
<span class="quote">&gt; &gt; &gt; Anonymous:        102400 kB</span>
<span class="quote">&gt; &gt; &gt; LazyFree:         102368 kB</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It took me a some time to realize that LazyFree is not accurate because</span>
<span class="quote">&gt; &gt; &gt; there are still pages on the per-cpu lru_lazyfree_pvecs. I believe this</span>
<span class="quote">&gt; &gt; &gt; is an implementation detail which shouldn&#39;t be visible to the userspace.</span>
<span class="quote">&gt; &gt; &gt; Should we simply drain the pagevec? A crude way would be to simply</span>
<span class="quote">&gt; &gt; &gt; lru_add_drain_all after we are done with the given range. We can also</span>
<span class="quote">&gt; &gt; &gt; make this lru_lazyfree_pvecs specific but I am not sure this is worth</span>
<span class="quote">&gt; &gt; &gt; the additional code.</span>
<span class="quote">&gt; &gt; &gt; ---</span>
<span class="quote">&gt; &gt; &gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; &gt; &gt; index dc5927c812d3..d2c318db16c9 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; &gt; &gt; @@ -474,7 +474,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt; &gt; &gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; &gt; &gt; -</span>
<span class="quote">&gt; &gt; &gt; +	lru_add_drain_all();</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A full drain on all CPUs is very expensive and IMO not justified for</span>
<span class="quote">&gt; &gt; some per-cpu fuzz factor in the stats. I&#39;d take hampering the stats</span>
<span class="quote">&gt; &gt; over hampering the syscall any day; only a subset of MADV_FREE users</span>
<span class="quote">&gt; &gt; will look at the stats.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And while the aggregate error can be large on machines with many CPUs</span>
<span class="quote">&gt; &gt; (notably the machines on which you absolutely don&#39;t want to send IPIs</span>
<span class="quote">&gt; &gt; to all cores each time a thread madvises some pages!),</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand. Where would we trigger IPIs?</span>
<span class="quote">&gt; lru_add_drain_all relies on workqueus.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; the pages of a</span>
<span class="quote">&gt; &gt; single process are not likely to be spread out across more than a few</span>
<span class="quote">&gt; &gt; CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then we can simply only flushe lru_lazyfree_pvecs which should reduce</span>
<span class="quote">&gt; the unrelated noise from other pagevecs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The error when reading a specific smaps should be completely ok.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In numbers: even if your process is madvising from 16 different CPUs,</span>
<span class="quote">&gt; &gt; the error in its smaps file will peak at 896K in the worst case. That</span>
<span class="quote">&gt; &gt; level of concurrency tends to come with much bigger memory quantities</span>
<span class="quote">&gt; &gt; for that amount of error to matter.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is still an unexpected behavior IMHO and an implementation detail</span>
<span class="quote">&gt; which leaks to the userspace.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; &gt; IMO this is a non-issue.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I will not insist if there is a general consensus on this and it is a</span>
<span class="quote">&gt; documented behavior, though. </span>

We cannot gurantee with that even draining because madvise_free can
miss some of pages easily with several conditions.
First of all, userspace can never know how many of pages are mapped
in there at the moment. As well, one of page in the range can be
swapped out or is going migrating, fail to try_lockpage and so on.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - March 2, 2017, 2:01 p.m.</div>
<pre class="content">
On Wed, Mar 01, 2017 at 07:57:35PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 01-03-17 13:31:49, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Wed, Mar 01, 2017 at 02:36:24PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -474,7 +474,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt; &gt; &gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; &gt; &gt; -</span>
<span class="quote">&gt; &gt; &gt; +	lru_add_drain_all();</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A full drain on all CPUs is very expensive and IMO not justified for</span>
<span class="quote">&gt; &gt; some per-cpu fuzz factor in the stats. I&#39;d take hampering the stats</span>
<span class="quote">&gt; &gt; over hampering the syscall any day; only a subset of MADV_FREE users</span>
<span class="quote">&gt; &gt; will look at the stats.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And while the aggregate error can be large on machines with many CPUs</span>
<span class="quote">&gt; &gt; (notably the machines on which you absolutely don&#39;t want to send IPIs</span>
<span class="quote">&gt; &gt; to all cores each time a thread madvises some pages!),</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand. Where would we trigger IPIs?</span>
<span class="quote">&gt; lru_add_drain_all relies on workqueus.</span>

Brainfart on my end, s,IPIs,sync work items,.

That doesn&#39;t change my point, though. These things are expensive, and
we had scalability issues with them in the past. See for example
4dd72b4a47a5 (&quot;mm: fadvise: avoid expensive remote LRU cache draining
after FADV_DONTNEED&quot;).
<span class="quote">
&gt; &gt; the pages of a</span>
<span class="quote">&gt; &gt; single process are not likely to be spread out across more than a few</span>
<span class="quote">&gt; &gt; CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then we can simply only flushe lru_lazyfree_pvecs which should reduce</span>
<span class="quote">&gt; the unrelated noise from other pagevecs.</span>

The problem isn&#39;t flushing other pagevecs once we&#39;re already scheduled
on a CPU, the problem is scheduling work on all cpus and then waiting
for completion.
<span class="quote">
&gt; &gt; The error when reading a specific smaps should be completely ok.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In numbers: even if your process is madvising from 16 different CPUs,</span>
<span class="quote">&gt; &gt; the error in its smaps file will peak at 896K in the worst case. That</span>
<span class="quote">&gt; &gt; level of concurrency tends to come with much bigger memory quantities</span>
<span class="quote">&gt; &gt; for that amount of error to matter.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is still an unexpected behavior IMHO and an implementation detail</span>
<span class="quote">&gt; which leaks to the userspace.</span>

We have per-cpu fuzz in every single vmstat counter. Look at
calculate_normal_threshold() in vmstat.c and the sample thresholds for
when per-cpu deltas are flushed. In the vast majority of machines, the
per-cpu error in these counters is much higher than what we get with
pagevecs holding back a few pages.

It&#39;s not that I think you&#39;re wrong: it *is* an implementation detail.
But we take a bit of incoherency from batching all over the place, so
it&#39;s a little odd to take a stand over this particular instance of it
- whether demanding that it&#39;d be fixed, or be documented, which would
only suggest to users that this is special when it really isn&#39;t etc.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 2, 2017, 4:30 p.m.</div>
<pre class="content">
On Thu 02-03-17 09:01:01, Johannes Weiner wrote:
<span class="quote">&gt; On Wed, Mar 01, 2017 at 07:57:35PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 01-03-17 13:31:49, Johannes Weiner wrote:</span>
[...]
<span class="quote">&gt; &gt; &gt; The error when reading a specific smaps should be completely ok.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In numbers: even if your process is madvising from 16 different CPUs,</span>
<span class="quote">&gt; &gt; &gt; the error in its smaps file will peak at 896K in the worst case. That</span>
<span class="quote">&gt; &gt; &gt; level of concurrency tends to come with much bigger memory quantities</span>
<span class="quote">&gt; &gt; &gt; for that amount of error to matter.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It is still an unexpected behavior IMHO and an implementation detail</span>
<span class="quote">&gt; &gt; which leaks to the userspace.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We have per-cpu fuzz in every single vmstat counter. Look at</span>
<span class="quote">&gt; calculate_normal_threshold() in vmstat.c and the sample thresholds for</span>
<span class="quote">&gt; when per-cpu deltas are flushed. In the vast majority of machines, the</span>
<span class="quote">&gt; per-cpu error in these counters is much higher than what we get with</span>
<span class="quote">&gt; pagevecs holding back a few pages.</span>

Yes but vmstat counters have a different usecase AFAIK. You mostly look
at those when debugging or watching the system. /proc/&lt;pid&gt;/smaps is
quite often used to do per task metrics which are then used for some
decision making so it should be less fuzzy if that is possible.
<span class="quote">
&gt; It&#39;s not that I think you&#39;re wrong: it *is* an implementation detail.</span>
<span class="quote">&gt; But we take a bit of incoherency from batching all over the place, so</span>
<span class="quote">&gt; it&#39;s a little odd to take a stand over this particular instance of it</span>
<span class="quote">&gt; - whether demanding that it&#39;d be fixed, or be documented, which would</span>
<span class="quote">&gt; only suggest to users that this is special when it really isn&#39;t etc.</span>

I am not aware of other counter printed in smaps that would suffer from
the same problem, but I haven&#39;t checked too deeply so I might be wrong. 

Anyway it seems that I am alone in my position so I will not insist.
If we have any bug report then we can still fix it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - March 4, 2017, 12:10 a.m.</div>
<pre class="content">
On Thu, 2 Mar 2017 17:30:54 +0100 Michal Hocko &lt;mhocko@kernel.org&gt; wrote:
<span class="quote">
&gt; &gt; It&#39;s not that I think you&#39;re wrong: it *is* an implementation detail.</span>
<span class="quote">&gt; &gt; But we take a bit of incoherency from batching all over the place, so</span>
<span class="quote">&gt; &gt; it&#39;s a little odd to take a stand over this particular instance of it</span>
<span class="quote">&gt; &gt; - whether demanding that it&#39;d be fixed, or be documented, which would</span>
<span class="quote">&gt; &gt; only suggest to users that this is special when it really isn&#39;t etc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not aware of other counter printed in smaps that would suffer from</span>
<span class="quote">&gt; the same problem, but I haven&#39;t checked too deeply so I might be wrong. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway it seems that I am alone in my position so I will not insist.</span>
<span class="quote">&gt; If we have any bug report then we can still fix it.</span>

A single lru_add_drain_all() right at the top level (in smaps_show()?)
won&#39;t kill us and should significantly improve this issue.  And it
might accidentally make some of the other smaps statistics more
accurate as well.

If not, can we please have a nice comment somewhere appropriate which
explains why LazyFree is inaccurate and why we chose to leave it that
way?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index dc5927c812d3..d2c318db16c9 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -474,7 +474,7 @@</span> <span class="p_context"> static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
 	madvise_free_page_range(&amp;tlb, vma, start, end);
 	mmu_notifier_invalidate_range_end(mm, start, end);
 	tlb_finish_mmu(&amp;tlb, start, end);
<span class="p_del">-</span>
<span class="p_add">+	lru_add_drain_all();</span>
 	return 0;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



