
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>arch, mm: introduce arch_tlb_gather_mmu_lazy (was: Re: [RESEND PATCH] mm, oom_reaper: gather each vma to prevent) leaking TLB entry - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    arch, mm: introduce arch_tlb_gather_mmu_lazy (was: Re: [RESEND PATCH] mm, oom_reaper: gather each vma to prevent) leaking TLB entry</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 10, 2017, 12:26 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171110122635.q26xdxytgdfjy5q3@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10053061/mbox/"
   >mbox</a>
|
   <a href="/patch/10053061/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10053061/">/patch/10053061/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7F6986035D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Nov 2017 12:26:44 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 77A872B27D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Nov 2017 12:26:44 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6C6892B28A; Fri, 10 Nov 2017 12:26:44 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 929AB2B27D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Nov 2017 12:26:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752410AbdKJM0j (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 10 Nov 2017 07:26:39 -0500
Received: from mx2.suse.de ([195.135.220.15]:37384 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750850AbdKJM0i (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 10 Nov 2017 07:26:38 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id B2B68AC98;
	Fri, 10 Nov 2017 12:26:36 +0000 (UTC)
Date: Fri, 10 Nov 2017 13:26:35 +0100
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Wang Nan &lt;wangnan0@huawei.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, will.deacon@arm.com,
	Bob Liu &lt;liubo95@huawei.com&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, Roman Gushchin &lt;guro@fb.com&gt;,
	Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy (was: Re:
	[RESEND PATCH] mm,
	oom_reaper: gather each vma to prevent) leaking TLB entry
Message-ID: &lt;20171110122635.q26xdxytgdfjy5q3@dhcp22.suse.cz&gt;
References: &lt;20171107095453.179940-1-wangnan0@huawei.com&gt;
	&lt;20171110001933.GA12421@bbox&gt;
	&lt;20171110101529.op6yaxtdke2p4bsh@dhcp22.suse.cz&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20171110101529.op6yaxtdke2p4bsh@dhcp22.suse.cz&gt;
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 10, 2017, 12:26 p.m.</div>
<pre class="content">
On Fri 10-11-17 11:15:29, Michal Hocko wrote:
<span class="quote">&gt; On Fri 10-11-17 09:19:33, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; On Tue, Nov 07, 2017 at 09:54:53AM +0000, Wang Nan wrote:</span>
<span class="quote">&gt; &gt; &gt; tlb_gather_mmu(&amp;tlb, mm, 0, -1) means gathering the whole virtual memory</span>
<span class="quote">&gt; &gt; &gt; space. In this case, tlb-&gt;fullmm is true. Some archs like arm64 doesn&#39;t</span>
<span class="quote">&gt; &gt; &gt; flush TLB when tlb-&gt;fullmm is true:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;   commit 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Which makes leaking of tlb entries.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That means soft-dirty which has used tlb_gather_mmu with fullmm could be</span>
<span class="quote">&gt; &gt; broken via losing write-protection bit once it supports arm64 in future?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If so, it would be better to use TASK_SIZE rather than -1 in tlb_gather_mmu.</span>
<span class="quote">&gt; &gt; Of course, it&#39;s a off-topic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I wouldn&#39;t play tricks like that. And maybe the API itself could be more</span>
<span class="quote">&gt; explicit. E.g. add a lazy parameter which would allow arch specific code</span>
<span class="quote">&gt; to not flush if it is sure that nobody can actually stumble over missed</span>
<span class="quote">&gt; flush. E.g. the following?</span>

This one has a changelog and even compiles on my crosscompile test
---
From 7f0fcd2cab379ddac5611b2a520cdca8a77a235b Mon Sep 17 00:00:00 2001
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Date: Fri, 10 Nov 2017 11:27:17 +0100
Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy

5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;) has
introduced an optimization to not flush tlb when we are tearing the
whole address space down. Will goes on to explain

: Basically, we tag each address space with an ASID (PCID on x86) which
: is resident in the TLB. This means we can elide TLB invalidation when
: pulling down a full mm because we won&#39;t ever assign that ASID to
: another mm without doing TLB invalidation elsewhere (which actually
: just nukes the whole TLB).

This all is nice but tlb_gather users are not aware of that and this can
actually cause some real problems. E.g. the oom_reaper tries to reap the
whole address space but it might race with threads accessing the memory [1].
It is possible that soft-dirty handling might suffer from the same
problem [2].

Introduce an explicit lazy variant tlb_gather_mmu_lazy which allows the
behavior arm64 implements for the fullmm case and replace it by an
explicit lazy flag in the mmu_gather structure. exit_mmap path is then
turned into the explicit lazy variant. Other architectures simply ignore
the flag.

[1] http://lkml.kernel.org/r/20171106033651.172368-1-wangnan0@huawei.com
[2] http://lkml.kernel.org/r/20171110001933.GA12421@bbox
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 arch/arm/include/asm/tlb.h   |  3 ++-
 arch/arm64/include/asm/tlb.h |  2 +-
 arch/ia64/include/asm/tlb.h  |  3 ++-
 arch/s390/include/asm/tlb.h  |  3 ++-
 arch/sh/include/asm/tlb.h    |  2 +-
 arch/um/include/asm/tlb.h    |  2 +-
 include/asm-generic/tlb.h    |  6 ++++--
 include/linux/mm_types.h     |  2 ++
 mm/memory.c                  | 17 +++++++++++++++--
 mm/mmap.c                    |  2 +-
 10 files changed, 31 insertions(+), 11 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 13, 2017, 12:28 a.m.</div>
<pre class="content">
On Fri, Nov 10, 2017 at 01:26:35PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 10-11-17 11:15:29, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Fri 10-11-17 09:19:33, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tue, Nov 07, 2017 at 09:54:53AM +0000, Wang Nan wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; tlb_gather_mmu(&amp;tlb, mm, 0, -1) means gathering the whole virtual memory</span>
<span class="quote">&gt; &gt; &gt; &gt; space. In this case, tlb-&gt;fullmm is true. Some archs like arm64 doesn&#39;t</span>
<span class="quote">&gt; &gt; &gt; &gt; flush TLB when tlb-&gt;fullmm is true:</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt;   commit 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;).</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Which makes leaking of tlb entries.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; That means soft-dirty which has used tlb_gather_mmu with fullmm could be</span>
<span class="quote">&gt; &gt; &gt; broken via losing write-protection bit once it supports arm64 in future?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If so, it would be better to use TASK_SIZE rather than -1 in tlb_gather_mmu.</span>
<span class="quote">&gt; &gt; &gt; Of course, it&#39;s a off-topic.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I wouldn&#39;t play tricks like that. And maybe the API itself could be more</span>
<span class="quote">&gt; &gt; explicit. E.g. add a lazy parameter which would allow arch specific code</span>
<span class="quote">&gt; &gt; to not flush if it is sure that nobody can actually stumble over missed</span>
<span class="quote">&gt; &gt; flush. E.g. the following?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This one has a changelog and even compiles on my crosscompile test</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; From 7f0fcd2cab379ddac5611b2a520cdca8a77a235b Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Date: Fri, 10 Nov 2017 11:27:17 +0100</span>
<span class="quote">&gt; Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;) has</span>
<span class="quote">&gt; introduced an optimization to not flush tlb when we are tearing the</span>
<span class="quote">&gt; whole address space down. Will goes on to explain</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; : Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; : is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; : pulling down a full mm because we won&#39;t ever assign that ASID to</span>
<span class="quote">&gt; : another mm without doing TLB invalidation elsewhere (which actually</span>
<span class="quote">&gt; : just nukes the whole TLB).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This all is nice but tlb_gather users are not aware of that and this can</span>
<span class="quote">&gt; actually cause some real problems. E.g. the oom_reaper tries to reap the</span>
<span class="quote">&gt; whole address space but it might race with threads accessing the memory [1].</span>
<span class="quote">&gt; It is possible that soft-dirty handling might suffer from the same</span>
<span class="quote">&gt; problem [2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Introduce an explicit lazy variant tlb_gather_mmu_lazy which allows the</span>
<span class="quote">&gt; behavior arm64 implements for the fullmm case and replace it by an</span>
<span class="quote">&gt; explicit lazy flag in the mmu_gather structure. exit_mmap path is then</span>
<span class="quote">&gt; turned into the explicit lazy variant. Other architectures simply ignore</span>
<span class="quote">&gt; the flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/r/20171106033651.172368-1-wangnan0@huawei.com</span>
<span class="quote">&gt; [2] http://lkml.kernel.org/r/20171110001933.GA12421@bbox</span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/tlb.h   |  3 ++-</span>
<span class="quote">&gt;  arch/arm64/include/asm/tlb.h |  2 +-</span>
<span class="quote">&gt;  arch/ia64/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt;  arch/s390/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt;  arch/sh/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt;  arch/um/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt;  include/asm-generic/tlb.h    |  6 ++++--</span>
<span class="quote">&gt;  include/linux/mm_types.h     |  2 ++</span>
<span class="quote">&gt;  mm/memory.c                  | 17 +++++++++++++++--</span>
<span class="quote">&gt;  mm/mmap.c                    |  2 +-</span>
<span class="quote">&gt;  10 files changed, 31 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; index d5562f9ce600..fe9042aee8e9 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; @@ -149,7 +149,8 @@ static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void</span>
<span class="quote">&gt;  arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="quote">&gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			bool lazy)</span>
<span class="quote">&gt; </span>

Thanks for the patch, Michal.
However, it would be nice to do it tranparently without asking
new flags from users.

When I read tlb_gather_mmu&#39;s description, fullmm is supposed to
be used only if there is no users and full address space.

That means we can do it API itself like this?

void arch_tlb_gather_mmu(...)

        tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 13, 2017, 9:51 a.m.</div>
<pre class="content">
On Mon 13-11-17 09:28:33, Minchan Kim wrote:
[...]
<span class="quote">&gt; Thanks for the patch, Michal.</span>
<span class="quote">&gt; However, it would be nice to do it tranparently without asking</span>
<span class="quote">&gt; new flags from users.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When I read tlb_gather_mmu&#39;s description, fullmm is supposed to</span>
<span class="quote">&gt; be used only if there is no users and full address space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That means we can do it API itself like this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void arch_tlb_gather_mmu(...)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;</span>

I do not have a strong opinion here. The optimization is quite subtle so
calling it explicitly sounds like a less surprising behavior to me
longterm. Note that I haven&#39;t checked all fullmm users.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 14, 2017, 1:45 a.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 10:51:07AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Mon 13-11-17 09:28:33, Minchan Kim wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Thanks for the patch, Michal.</span>
<span class="quote">&gt; &gt; However, it would be nice to do it tranparently without asking</span>
<span class="quote">&gt; &gt; new flags from users.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When I read tlb_gather_mmu&#39;s description, fullmm is supposed to</span>
<span class="quote">&gt; &gt; be used only if there is no users and full address space.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That means we can do it API itself like this?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; void arch_tlb_gather_mmu(...)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;         tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I do not have a strong opinion here. The optimization is quite subtle so</span>
<span class="quote">&gt; calling it explicitly sounds like a less surprising behavior to me</span>
<span class="quote">&gt; longterm. Note that I haven&#39;t checked all fullmm users.</span>

With description of tlb_gather_mmu and 4d6ddfa9242b, set fullmm to true
should guarantees there is *no users* of the mm_struct so I think
my suggestion is not about optimization but to keep the semantic
&quot;there should be no one who can access address space when entire
address space is destroyed&quot;.

If you want to be more explicit, we should add some description
about &quot;where can we use lazy mode&quot;. I think it should tell the
internal of some architecture for user to understand. I&#39;m not
sure it&#39;s worth although we can do it transparently.

I&#39;m not strong against with you approach, either.

Anyway, I think Wang Nan&#39;s patch is already broken.
http://lkml.kernel.org/r/%3C20171107095453.179940-1-wangnan0@huawei.com%3E

Because unmap_page_range(ie, zap_pte_range) can flush TLB forcefully
and free pages. However, the architecture code for TLB flush cannot
flush at all by wrong fullmm so other threads can write freed-page.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 14, 2017, 7:21 a.m.</div>
<pre class="content">
On Tue 14-11-17 10:45:49, Minchan Kim wrote:
[...]
<span class="quote">&gt; Anyway, I think Wang Nan&#39;s patch is already broken.</span>
<span class="quote">&gt; http://lkml.kernel.org/r/%3C20171107095453.179940-1-wangnan0@huawei.com%3E</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because unmap_page_range(ie, zap_pte_range) can flush TLB forcefully</span>
<span class="quote">&gt; and free pages. However, the architecture code for TLB flush cannot</span>
<span class="quote">&gt; flush at all by wrong fullmm so other threads can write freed-page.</span>

I am not sure I understand what you mean. How is that any different from
any other explicit partial madvise call?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 15, 2017, 12:12 a.m.</div>
<pre class="content">
On Tue, Nov 14, 2017 at 08:21:00AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Tue 14-11-17 10:45:49, Minchan Kim wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Anyway, I think Wang Nan&#39;s patch is already broken.</span>
<span class="quote">&gt; &gt; http://lkml.kernel.org/r/%3C20171107095453.179940-1-wangnan0@huawei.com%3E</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because unmap_page_range(ie, zap_pte_range) can flush TLB forcefully</span>
<span class="quote">&gt; &gt; and free pages. However, the architecture code for TLB flush cannot</span>
<span class="quote">&gt; &gt; flush at all by wrong fullmm so other threads can write freed-page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand what you mean. How is that any different from</span>
<span class="quote">&gt; any other explicit partial madvise call?</span>

Argh, I misread his code. Sorry for that.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 15, 2017, 8:14 a.m.</div>
<pre class="content">
On Mon 13-11-17 09:28:33, Minchan Kim wrote:
[...]
<span class="quote">&gt; void arch_tlb_gather_mmu(...)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;</span>

Sorry, I should have realized sooner but this will not work for the oom
reaper. It _can_ race with the final exit_mmap and run with mm_users == 0
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Nov. 15, 2017, 5:33 p.m.</div>
<pre class="content">
Hi Michal,

On Fri, Nov 10, 2017 at 01:26:35PM +0100, Michal Hocko wrote:
<span class="quote">&gt; From 7f0fcd2cab379ddac5611b2a520cdca8a77a235b Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Date: Fri, 10 Nov 2017 11:27:17 +0100</span>
<span class="quote">&gt; Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;) has</span>
<span class="quote">&gt; introduced an optimization to not flush tlb when we are tearing the</span>
<span class="quote">&gt; whole address space down. Will goes on to explain</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; : Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; : is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; : pulling down a full mm because we won&#39;t ever assign that ASID to</span>
<span class="quote">&gt; : another mm without doing TLB invalidation elsewhere (which actually</span>
<span class="quote">&gt; : just nukes the whole TLB).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This all is nice but tlb_gather users are not aware of that and this can</span>
<span class="quote">&gt; actually cause some real problems. E.g. the oom_reaper tries to reap the</span>
<span class="quote">&gt; whole address space but it might race with threads accessing the memory [1].</span>
<span class="quote">&gt; It is possible that soft-dirty handling might suffer from the same</span>
<span class="quote">&gt; problem [2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Introduce an explicit lazy variant tlb_gather_mmu_lazy which allows the</span>
<span class="quote">&gt; behavior arm64 implements for the fullmm case and replace it by an</span>
<span class="quote">&gt; explicit lazy flag in the mmu_gather structure. exit_mmap path is then</span>
<span class="quote">&gt; turned into the explicit lazy variant. Other architectures simply ignore</span>
<span class="quote">&gt; the flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/r/20171106033651.172368-1-wangnan0@huawei.com</span>
<span class="quote">&gt; [2] http://lkml.kernel.org/r/20171110001933.GA12421@bbox</span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/tlb.h   |  3 ++-</span>
<span class="quote">&gt;  arch/arm64/include/asm/tlb.h |  2 +-</span>
<span class="quote">&gt;  arch/ia64/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt;  arch/s390/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt;  arch/sh/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt;  arch/um/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt;  include/asm-generic/tlb.h    |  6 ++++--</span>
<span class="quote">&gt;  include/linux/mm_types.h     |  2 ++</span>
<span class="quote">&gt;  mm/memory.c                  | 17 +++++++++++++++--</span>
<span class="quote">&gt;  mm/mmap.c                    |  2 +-</span>
<span class="quote">&gt;  10 files changed, 31 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; index d5562f9ce600..fe9042aee8e9 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; @@ -149,7 +149,8 @@ static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void</span>
<span class="quote">&gt;  arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="quote">&gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			bool lazy)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	tlb-&gt;mm = mm;</span>
<span class="quote">&gt;  	tlb-&gt;fullmm = !(start | (end+1));</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; index ffdaea7954bb..7adde19b2bcc 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; @@ -43,7 +43,7 @@ static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="quote">&gt;  	 * The ASID allocator will either invalidate the ASID or mark</span>
<span class="quote">&gt;  	 * it as used.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (tlb-&gt;fullmm)</span>
<span class="quote">&gt; +	if (tlb-&gt;lazy)</span>
<span class="quote">&gt;  		return;</span>

This looks like the right idea, but I&#39;d rather make this check:

	if (tlb-&gt;fullmm &amp;&amp; tlb-&gt;lazy)

since the optimisation doesn&#39;t work for anything than tearing down the
entire address space.

Alternatively, I could actually go check MMF_UNSTABLE in tlb-&gt;mm, which
would save you having to add an extra flag in the first place, e.g.:

	if (tlb-&gt;fullmm &amp;&amp; !test_bit(MMF_UNSTABLE, &amp;tlb-&gt;mm-&gt;flags))

which is a nice one-liner.

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 16, 2017, 12:44 a.m.</div>
<pre class="content">
On Wed, Nov 15, 2017 at 09:14:52AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Mon 13-11-17 09:28:33, Minchan Kim wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; void arch_tlb_gather_mmu(...)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;         tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, I should have realized sooner but this will not work for the oom</span>
<span class="quote">&gt; reaper. It _can_ race with the final exit_mmap and run with mm_users == 0</span>

If someone see mm_users is zero, it means there is no user to access
address space by stale TLB. Am I missing something?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 16, 2017, 9:19 a.m.</div>
<pre class="content">
On Thu 16-11-17 09:44:57, Minchan Kim wrote:
<span class="quote">&gt; On Wed, Nov 15, 2017 at 09:14:52AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 13-11-17 09:28:33, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; void arch_tlb_gather_mmu(...)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;         tlb-&gt;fullmm = !(start | (end + 1)) &amp;&amp; atomic_read(&amp;mm-&gt;mm_users) == 0;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sorry, I should have realized sooner but this will not work for the oom</span>
<span class="quote">&gt; &gt; reaper. It _can_ race with the final exit_mmap and run with mm_users == 0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If someone see mm_users is zero, it means there is no user to access</span>
<span class="quote">&gt; address space by stale TLB. Am I missing something?</span>

You are probably right but changing the flushing policy in the middle of
the address space tear down makes me nervous. While this might work
right now, it is kind of tricky and it has some potential to kick us
back in future. Just note how the current arm64 optimization went
unnoticed because the the oom reaper is such a rare event that nobody
has actually noticed this. And I suspect that the likelyhood of failure
is very low even when applied for anybody to notice in the real life.

So I would very much like to make the behavior really explicit for
everybody to see what is going on there.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 16, 2017, 9:20 a.m.</div>
<pre class="content">
On Wed 15-11-17 17:33:32, Will Deacon wrote:
<span class="quote">&gt; Hi Michal,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Fri, Nov 10, 2017 at 01:26:35PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; From 7f0fcd2cab379ddac5611b2a520cdca8a77a235b Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; Date: Fri, 10 Nov 2017 11:27:17 +0100</span>
<span class="quote">&gt; &gt; Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;) has</span>
<span class="quote">&gt; &gt; introduced an optimization to not flush tlb when we are tearing the</span>
<span class="quote">&gt; &gt; whole address space down. Will goes on to explain</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; : Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; &gt; : is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; &gt; : pulling down a full mm because we won&#39;t ever assign that ASID to</span>
<span class="quote">&gt; &gt; : another mm without doing TLB invalidation elsewhere (which actually</span>
<span class="quote">&gt; &gt; : just nukes the whole TLB).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This all is nice but tlb_gather users are not aware of that and this can</span>
<span class="quote">&gt; &gt; actually cause some real problems. E.g. the oom_reaper tries to reap the</span>
<span class="quote">&gt; &gt; whole address space but it might race with threads accessing the memory [1].</span>
<span class="quote">&gt; &gt; It is possible that soft-dirty handling might suffer from the same</span>
<span class="quote">&gt; &gt; problem [2].</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Introduce an explicit lazy variant tlb_gather_mmu_lazy which allows the</span>
<span class="quote">&gt; &gt; behavior arm64 implements for the fullmm case and replace it by an</span>
<span class="quote">&gt; &gt; explicit lazy flag in the mmu_gather structure. exit_mmap path is then</span>
<span class="quote">&gt; &gt; turned into the explicit lazy variant. Other architectures simply ignore</span>
<span class="quote">&gt; &gt; the flag.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [1] http://lkml.kernel.org/r/20171106033651.172368-1-wangnan0@huawei.com</span>
<span class="quote">&gt; &gt; [2] http://lkml.kernel.org/r/20171110001933.GA12421@bbox</span>
<span class="quote">&gt; &gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/arm/include/asm/tlb.h   |  3 ++-</span>
<span class="quote">&gt; &gt;  arch/arm64/include/asm/tlb.h |  2 +-</span>
<span class="quote">&gt; &gt;  arch/ia64/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt; &gt;  arch/s390/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt; &gt;  arch/sh/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt; &gt;  arch/um/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt; &gt;  include/asm-generic/tlb.h    |  6 ++++--</span>
<span class="quote">&gt; &gt;  include/linux/mm_types.h     |  2 ++</span>
<span class="quote">&gt; &gt;  mm/memory.c                  | 17 +++++++++++++++--</span>
<span class="quote">&gt; &gt;  mm/mmap.c                    |  2 +-</span>
<span class="quote">&gt; &gt;  10 files changed, 31 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; index d5562f9ce600..fe9042aee8e9 100644</span>
<span class="quote">&gt; &gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; @@ -149,7 +149,8 @@ static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void</span>
<span class="quote">&gt; &gt;  arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="quote">&gt; &gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; &gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; &gt; +			bool lazy)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	tlb-&gt;mm = mm;</span>
<span class="quote">&gt; &gt;  	tlb-&gt;fullmm = !(start | (end+1));</span>
<span class="quote">&gt; &gt; diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; index ffdaea7954bb..7adde19b2bcc 100644</span>
<span class="quote">&gt; &gt; --- a/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; +++ b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; @@ -43,7 +43,7 @@ static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="quote">&gt; &gt;  	 * The ASID allocator will either invalidate the ASID or mark</span>
<span class="quote">&gt; &gt;  	 * it as used.</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; -	if (tlb-&gt;fullmm)</span>
<span class="quote">&gt; &gt; +	if (tlb-&gt;lazy)</span>
<span class="quote">&gt; &gt;  		return;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This looks like the right idea, but I&#39;d rather make this check:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (tlb-&gt;fullmm &amp;&amp; tlb-&gt;lazy)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; since the optimisation doesn&#39;t work for anything than tearing down the</span>
<span class="quote">&gt; entire address space.</span>

OK, that makes sense.
<span class="quote">
&gt; Alternatively, I could actually go check MMF_UNSTABLE in tlb-&gt;mm, which</span>
<span class="quote">&gt; would save you having to add an extra flag in the first place, e.g.:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (tlb-&gt;fullmm &amp;&amp; !test_bit(MMF_UNSTABLE, &amp;tlb-&gt;mm-&gt;flags))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; which is a nice one-liner.</span>

But that would make it oom_reaper specific. What about the softdirty
case Minchan has mentioned earlier?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Nov. 20, 2017, 2:24 p.m.</div>
<pre class="content">
On Thu, Nov 16, 2017 at 10:20:42AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 15-11-17 17:33:32, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; Hi Michal,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Fri, Nov 10, 2017 at 01:26:35PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; From 7f0fcd2cab379ddac5611b2a520cdca8a77a235b Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; &gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; &gt; Date: Fri, 10 Nov 2017 11:27:17 +0100</span>
<span class="quote">&gt; &gt; &gt; Subject: [PATCH] arch, mm: introduce arch_tlb_gather_mmu_lazy</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 5a7862e83000 (&quot;arm64: tlbflush: avoid flushing when fullmm == 1&quot;) has</span>
<span class="quote">&gt; &gt; &gt; introduced an optimization to not flush tlb when we are tearing the</span>
<span class="quote">&gt; &gt; &gt; whole address space down. Will goes on to explain</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; : Basically, we tag each address space with an ASID (PCID on x86) which</span>
<span class="quote">&gt; &gt; &gt; : is resident in the TLB. This means we can elide TLB invalidation when</span>
<span class="quote">&gt; &gt; &gt; : pulling down a full mm because we won&#39;t ever assign that ASID to</span>
<span class="quote">&gt; &gt; &gt; : another mm without doing TLB invalidation elsewhere (which actually</span>
<span class="quote">&gt; &gt; &gt; : just nukes the whole TLB).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This all is nice but tlb_gather users are not aware of that and this can</span>
<span class="quote">&gt; &gt; &gt; actually cause some real problems. E.g. the oom_reaper tries to reap the</span>
<span class="quote">&gt; &gt; &gt; whole address space but it might race with threads accessing the memory [1].</span>
<span class="quote">&gt; &gt; &gt; It is possible that soft-dirty handling might suffer from the same</span>
<span class="quote">&gt; &gt; &gt; problem [2].</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Introduce an explicit lazy variant tlb_gather_mmu_lazy which allows the</span>
<span class="quote">&gt; &gt; &gt; behavior arm64 implements for the fullmm case and replace it by an</span>
<span class="quote">&gt; &gt; &gt; explicit lazy flag in the mmu_gather structure. exit_mmap path is then</span>
<span class="quote">&gt; &gt; &gt; turned into the explicit lazy variant. Other architectures simply ignore</span>
<span class="quote">&gt; &gt; &gt; the flag.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; [1] http://lkml.kernel.org/r/20171106033651.172368-1-wangnan0@huawei.com</span>
<span class="quote">&gt; &gt; &gt; [2] http://lkml.kernel.org/r/20171110001933.GA12421@bbox</span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; &gt; ---</span>
<span class="quote">&gt; &gt; &gt;  arch/arm/include/asm/tlb.h   |  3 ++-</span>
<span class="quote">&gt; &gt; &gt;  arch/arm64/include/asm/tlb.h |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  arch/ia64/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt; &gt; &gt;  arch/s390/include/asm/tlb.h  |  3 ++-</span>
<span class="quote">&gt; &gt; &gt;  arch/sh/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  arch/um/include/asm/tlb.h    |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  include/asm-generic/tlb.h    |  6 ++++--</span>
<span class="quote">&gt; &gt; &gt;  include/linux/mm_types.h     |  2 ++</span>
<span class="quote">&gt; &gt; &gt;  mm/memory.c                  | 17 +++++++++++++++--</span>
<span class="quote">&gt; &gt; &gt;  mm/mmap.c                    |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  10 files changed, 31 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; index d5562f9ce600..fe9042aee8e9 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/arch/arm/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; @@ -149,7 +149,8 @@ static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt;  static inline void</span>
<span class="quote">&gt; &gt; &gt;  arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="quote">&gt; &gt; &gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; &gt; &gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; &gt; &gt; +			bool lazy)</span>
<span class="quote">&gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt;  	tlb-&gt;mm = mm;</span>
<span class="quote">&gt; &gt; &gt;  	tlb-&gt;fullmm = !(start | (end+1));</span>
<span class="quote">&gt; &gt; &gt; diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; index ffdaea7954bb..7adde19b2bcc 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/arch/arm64/include/asm/tlb.h</span>
<span class="quote">&gt; &gt; &gt; @@ -43,7 +43,7 @@ static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="quote">&gt; &gt; &gt;  	 * The ASID allocator will either invalidate the ASID or mark</span>
<span class="quote">&gt; &gt; &gt;  	 * it as used.</span>
<span class="quote">&gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; -	if (tlb-&gt;fullmm)</span>
<span class="quote">&gt; &gt; &gt; +	if (tlb-&gt;lazy)</span>
<span class="quote">&gt; &gt; &gt;  		return;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This looks like the right idea, but I&#39;d rather make this check:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (tlb-&gt;fullmm &amp;&amp; tlb-&gt;lazy)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; since the optimisation doesn&#39;t work for anything than tearing down the</span>
<span class="quote">&gt; &gt; entire address space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, that makes sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Alternatively, I could actually go check MMF_UNSTABLE in tlb-&gt;mm, which</span>
<span class="quote">&gt; &gt; would save you having to add an extra flag in the first place, e.g.:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (tlb-&gt;fullmm &amp;&amp; !test_bit(MMF_UNSTABLE, &amp;tlb-&gt;mm-&gt;flags))</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; which is a nice one-liner.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But that would make it oom_reaper specific. What about the softdirty</span>
<span class="quote">&gt; case Minchan has mentioned earlier?</span>

We don&#39;t (yet) support that on arm64, so we&#39;re ok for now. If we do grow
support for it, then I agree that we want a flag to identify the case where
the address space is going away and only elide the invalidation then.

Will
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="p_header">index d5562f9ce600..fe9042aee8e9 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/tlb.h</span>
<span class="p_chunk">@@ -149,7 +149,8 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 
 static inline void
 arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 	tlb-&gt;fullmm = !(start | (end+1));
<span class="p_header">diff --git a/arch/arm64/include/asm/tlb.h b/arch/arm64/include/asm/tlb.h</span>
<span class="p_header">index ffdaea7954bb..7adde19b2bcc 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> static inline void tlb_flush(struct mmu_gather *tlb)</span>
 	 * The ASID allocator will either invalidate the ASID or mark
 	 * it as used.
 	 */
<span class="p_del">-	if (tlb-&gt;fullmm)</span>
<span class="p_add">+	if (tlb-&gt;lazy)</span>
 		return;
 
 	/*
<span class="p_header">diff --git a/arch/ia64/include/asm/tlb.h b/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">index cbe5ac3699bf..50c440f5b7bc 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -169,7 +169,8 @@</span> <span class="p_context"> static inline void __tlb_alloc_page(struct mmu_gather *tlb)</span>
 
 static inline void
 arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 	tlb-&gt;max = ARRAY_SIZE(tlb-&gt;local);
<span class="p_header">diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h</span>
<span class="p_header">index 2eb8ff0d6fca..2310657b64c4 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/tlb.h</span>
<span class="p_chunk">@@ -49,7 +49,8 @@</span> <span class="p_context"> extern void tlb_remove_table(struct mmu_gather *tlb, void *table);</span>
 
 static inline void
 arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 	tlb-&gt;start = start;
<span class="p_header">diff --git a/arch/sh/include/asm/tlb.h b/arch/sh/include/asm/tlb.h</span>
<span class="p_header">index 51a8bc967e75..ae4c50a7c1ec 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/tlb.h</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> static inline void init_tlb_gather(struct mmu_gather *tlb)</span>
 
 static inline void
 arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-		unsigned long start, unsigned long end)</span>
<span class="p_add">+		unsigned long start, unsigned long end, bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 	tlb-&gt;start = start;
<span class="p_header">diff --git a/arch/um/include/asm/tlb.h b/arch/um/include/asm/tlb.h</span>
<span class="p_header">index 344d95619d03..f24af66d07a4 100644</span>
<span class="p_header">--- a/arch/um/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/um/include/asm/tlb.h</span>
<span class="p_chunk">@@ -46,7 +46,7 @@</span> <span class="p_context"> static inline void init_tlb_gather(struct mmu_gather *tlb)</span>
 
 static inline void
 arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-		unsigned long start, unsigned long end)</span>
<span class="p_add">+		unsigned long start, unsigned long end, bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 	tlb-&gt;start = start;
<span class="p_header">diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h</span>
<span class="p_header">index faddde44de8c..e6f0b8715e52 100644</span>
<span class="p_header">--- a/include/asm-generic/tlb.h</span>
<span class="p_header">+++ b/include/asm-generic/tlb.h</span>
<span class="p_chunk">@@ -101,7 +101,8 @@</span> <span class="p_context"> struct mmu_gather {</span>
 	unsigned int		fullmm : 1,
 	/* we have performed an operation which
 	 * requires a complete flush of the tlb */
<span class="p_del">-				need_flush_all : 1;</span>
<span class="p_add">+				need_flush_all : 1,</span>
<span class="p_add">+				lazy : 1;</span>
 
 	struct mmu_gather_batch *active;
 	struct mmu_gather_batch	local;
<span class="p_chunk">@@ -113,7 +114,8 @@</span> <span class="p_context"> struct mmu_gather {</span>
 #define HAVE_GENERIC_MMU_GATHER
 
 void arch_tlb_gather_mmu(struct mmu_gather *tlb,
<span class="p_del">-	struct mm_struct *mm, unsigned long start, unsigned long end);</span>
<span class="p_add">+	struct mm_struct *mm, unsigned long start, unsigned long end,</span>
<span class="p_add">+	bool lazy);</span>
 void tlb_flush_mmu(struct mmu_gather *tlb);
 void arch_tlb_finish_mmu(struct mmu_gather *tlb,
 			 unsigned long start, unsigned long end, bool force);
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 2a728317cba0..3208bea0356f 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -523,6 +523,8 @@</span> <span class="p_context"> static inline cpumask_t *mm_cpumask(struct mm_struct *mm)</span>
 struct mmu_gather;
 extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 				unsigned long start, unsigned long end);
<span class="p_add">+extern void tlb_gather_mmu_lazy(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="p_add">+				unsigned long start, unsigned long end);</span>
 extern void tlb_finish_mmu(struct mmu_gather *tlb,
 				unsigned long start, unsigned long end);
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 590709e84a43..7dfdd4d8224f 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -218,13 +218,15 @@</span> <span class="p_context"> static bool tlb_next_batch(struct mmu_gather *tlb)</span>
 }
 
 void arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
<span class="p_del">-				unsigned long start, unsigned long end)</span>
<span class="p_add">+				unsigned long start, unsigned long end,</span>
<span class="p_add">+				bool lazy)</span>
 {
 	tlb-&gt;mm = mm;
 
 	/* Is it from 0 to ~0? */
 	tlb-&gt;fullmm     = !(start | (end+1));
 	tlb-&gt;need_flush_all = 0;
<span class="p_add">+	tlb-&gt;lazy	= lazy;</span>
 	tlb-&gt;local.next = NULL;
 	tlb-&gt;local.nr   = 0;
 	tlb-&gt;local.max  = ARRAY_SIZE(tlb-&gt;__pages);
<span class="p_chunk">@@ -408,7 +410,18 @@</span> <span class="p_context"> void tlb_remove_table(struct mmu_gather *tlb, void *table)</span>
 void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 			unsigned long start, unsigned long end)
 {
<span class="p_del">-	arch_tlb_gather_mmu(tlb, mm, start, end);</span>
<span class="p_add">+	arch_tlb_gather_mmu(tlb, mm, start, end, false);</span>
<span class="p_add">+	inc_tlb_flush_pending(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* tlb_gather_mmu_lazy</span>
<span class="p_add">+ * 	Basically same as tlb_gather_mmu except it allows architectures to</span>
<span class="p_add">+ * 	skip tlb flushing if they can ensure that nobody will reuse tlb entries</span>
<span class="p_add">+ */</span>
<span class="p_add">+void tlb_gather_mmu_lazy(struct mmu_gather *tlb, struct mm_struct *mm,</span>
<span class="p_add">+			unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arch_tlb_gather_mmu(tlb, mm, start, end, true);</span>
 	inc_tlb_flush_pending(tlb-&gt;mm);
 }
 
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 680506faceae..43594a6a2eac 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2997,7 +2997,7 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 
 	lru_add_drain();
 	flush_cache_mm(mm);
<span class="p_del">-	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="p_add">+	tlb_gather_mmu_lazy(&amp;tlb, mm, 0, -1);</span>
 	/* update_hiwater_rss(mm) here? but nobody should be looking */
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



