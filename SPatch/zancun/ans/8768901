
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[03/10] mm/hugetlb: Protect follow_huge_(pud|pgd) functions from race - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [03/10] mm/hugetlb: Protect follow_huge_(pud|pgd) functions from race</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 7, 2016, 5:37 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1460007464-26726-4-git-send-email-khandual@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8768901/mbox/"
   >mbox</a>
|
   <a href="/patch/8768901/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8768901/">/patch/8768901/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 5EB45C0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Apr 2016 05:39:33 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 9D57920125
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Apr 2016 05:39:32 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C21CE20120
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Apr 2016 05:39:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755297AbcDGFiO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 7 Apr 2016 01:38:14 -0400
Received: from e28smtp08.in.ibm.com ([125.16.236.8]:51520 &quot;EHLO
	e28smtp08.in.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752431AbcDGFiH (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 7 Apr 2016 01:38:07 -0400
Received: from localhost
	by e28smtp08.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;khandual@linux.vnet.ibm.com&gt;;
	Thu, 7 Apr 2016 11:08:05 +0530
Received: from d28relay05.in.ibm.com (9.184.220.62)
	by e28smtp08.in.ibm.com (192.168.1.138) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Thu, 7 Apr 2016 11:08:02 +0530
X-IBM-Helo: d28relay05.in.ibm.com
X-IBM-MailFrom: khandual@linux.vnet.ibm.com
X-IBM-RcptTo: linux-kernel@vger.kernel.org
Received: from d28av03.in.ibm.com (d28av03.in.ibm.com [9.184.220.65])
	by d28relay05.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	u375cHI416777500
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 7 Apr 2016 11:08:18 +0530
Received: from d28av03.in.ibm.com (localhost [127.0.0.1])
	by d28av03.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	u375bl36006058
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 7 Apr 2016 11:07:59 +0530
Received: from localhost.in.ibm.com ([9.124.158.233])
	by d28av03.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	u375biFq005939; Thu, 7 Apr 2016 11:07:45 +0530
From: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linuxppc-dev@lists.ozlabs.org
Cc: hughd@google.com, kirill@shutemov.name, n-horiguchi@ah.jp.nec.com,
	akpm@linux-foundation.org, mgorman@techsingularity.net,
	dave.hansen@intel.com, aneesh.kumar@linux.vnet.ibm.com,
	mpe@ellerman.id.au
Subject: [PATCH 03/10] mm/hugetlb: Protect follow_huge_(pud|pgd) functions
	from race
Date: Thu,  7 Apr 2016 11:07:37 +0530
Message-Id: &lt;1460007464-26726-4-git-send-email-khandual@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1460007464-26726-1-git-send-email-khandual@linux.vnet.ibm.com&gt;
References: &lt;1460007464-26726-1-git-send-email-khandual@linux.vnet.ibm.com&gt;
X-TM-AS-MML: disable
x-cbid: 16040705-0029-0000-0000-00000C910DB3
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 7, 2016, 5:37 a.m.</div>
<pre class="content">
follow_huge_(pmd|pud|pgd) functions are used to walk the page table and
fetch the page struct during &#39;follow_page_mask&#39; call. There are possible
race conditions faced by these functions which arise out of simultaneous
calls of move_pages() and freeing of huge pages. This was fixed partly
by the previous commit e66f17ff7177 (&quot;mm/hugetlb: take page table lock
in follow_huge_pmd()&quot;) for only PMD based huge pages.

After implementing similar logic, functions like follow_huge_(pud|pgd)
are now safe from above mentioned race conditions and also can support
FOLL_GET. Generic version of the function &#39;follow_huge_addr&#39; has been
left as it is and its upto the architecture to decide on it.
<span class="signed-off-by">
Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
---
 include/linux/mm.h | 33 +++++++++++++++++++++++++++
 mm/hugetlb.c       | 67 ++++++++++++++++++++++++++++++++++++++++++++++--------
 2 files changed, 91 insertions(+), 9 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - April 7, 2016, 9:16 a.m.</div>
<pre class="content">
Hi Anshuman,

[auto build test ERROR on powerpc/next]
[also build test ERROR on v4.6-rc2 next-20160407]
[if your patch is applied to the wrong git tree, please drop us a note to help improving the system]

url:    https://github.com/0day-ci/linux/commits/Anshuman-Khandual/Enable-HugeTLB-page-migration-on-POWER/20160407-165841
base:   https://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux.git next
config: sparc64-allyesconfig (attached as .config)
reproduce:
        wget https://git.kernel.org/cgit/linux/kernel/git/wfg/lkp-tests.git/plain/sbin/make.cross -O ~/bin/make.cross
        chmod +x ~/bin/make.cross
        # save the attached .config to linux build tree
        make.cross ARCH=sparc64 

All error/warnings (new ones prefixed by &gt;&gt;):

   mm/hugetlb.c: In function &#39;follow_huge_pgd&#39;:
<span class="quote">&gt;&gt; mm/hugetlb.c:4395:3: error: implicit declaration of function &#39;pgd_page&#39; [-Werror=implicit-function-declaration]</span>
      page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);
      ^
<span class="quote">&gt;&gt; mm/hugetlb.c:4395:8: warning: assignment makes pointer from integer without a cast</span>
      page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);
           ^
   cc1: some warnings being treated as errors

vim +/pgd_page +4395 mm/hugetlb.c

  4389		 * make sure that the address range covered by this pgd is not
  4390		 * unmapped from other threads.
  4391		 */
  4392		if (!pgd_huge(*pgd))
  4393			goto out;
  4394		if (pgd_present(*pgd)) {
<span class="quote">&gt; 4395			page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>
  4396			if (flags &amp; FOLL_GET)
  4397				get_page(page);
  4398		} else {

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - April 7, 2016, 9:26 a.m.</div>
<pre class="content">
On 07/04/16 15:37, Anshuman Khandual wrote:
<span class="quote">&gt; follow_huge_(pmd|pud|pgd) functions are used to walk the page table and</span>
<span class="quote">&gt; fetch the page struct during &#39;follow_page_mask&#39; call. There are possible</span>
<span class="quote">&gt; race conditions faced by these functions which arise out of simultaneous</span>
<span class="quote">&gt; calls of move_pages() and freeing of huge pages. This was fixed partly</span>
<span class="quote">&gt; by the previous commit e66f17ff7177 (&quot;mm/hugetlb: take page table lock</span>
<span class="quote">&gt; in follow_huge_pmd()&quot;) for only PMD based huge pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After implementing similar logic, functions like follow_huge_(pud|pgd)</span>
<span class="quote">&gt; are now safe from above mentioned race conditions and also can support</span>
<span class="quote">&gt; FOLL_GET. Generic version of the function &#39;follow_huge_addr&#39; has been</span>
<span class="quote">&gt; left as it is and its upto the architecture to decide on it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/mm.h | 33 +++++++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/hugetlb.c       | 67 ++++++++++++++++++++++++++++++++++++++++++++++--------</span>
<span class="quote">&gt;  2 files changed, 91 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index ffcff53..734182a 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -1751,6 +1751,19 @@ static inline void pgtable_page_dtor(struct page *page)</span>
<span class="quote">&gt;  		NULL: pte_offset_kernel(pmd, address))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #if USE_SPLIT_PMD_PTLOCKS</span>

Do we still use USE_SPLIT_PMD_PTLOCKS? I think its good enough. with pgd&#39;s
we are likely to use the same locks and the split nature may not be really
split.
<span class="quote">
&gt; +static struct page *pgd_to_page(pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long mask = ~(PTRS_PER_PGD * sizeof(pgd_t) - 1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return virt_to_page((void *)((unsigned long) pgd &amp; mask));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct page *pud_to_page(pud_t *pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long mask = ~(PTRS_PER_PUD * sizeof(pud_t) - 1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return virt_to_page((void *)((unsigned long) pud &amp; mask));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static struct page *pmd_to_page(pmd_t *pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1758,6 +1771,16 @@ static struct page *pmd_to_page(pmd_t *pmd)</span>
<span class="quote">&gt;  	return virt_to_page((void *)((unsigned long) pmd &amp; mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline spinlock_t *pgd_lockptr(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return ptlock_ptr(pgd_to_page(pgd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return ptlock_ptr(pud_to_page(pud));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return ptlock_ptr(pmd_to_page(pmd));</span>
<span class="quote">&gt; @@ -1783,6 +1806,16 @@ static inline void pgtable_pmd_page_dtor(struct page *page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline spinlock_t *pgd_lockptr(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return &amp;mm-&gt;page_table_lock;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return &amp;mm-&gt;page_table_lock;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return &amp;mm-&gt;page_table_lock;</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 5ea3158..e84e479 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -4346,21 +4346,70 @@ struct page * __weak</span>
<span class="quote">&gt;  follow_huge_pud(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  		pud_t *pud, int flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (flags &amp; FOLL_GET)</span>
<span class="quote">&gt; -		return NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return pte_page(*(pte_t *)pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +retry:</span>
<span class="quote">&gt; +	ptl = pud_lockptr(mm, pud);</span>
<span class="quote">&gt; +	spin_lock(ptl);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * make sure that the address range covered by this pud is not</span>
<span class="quote">&gt; +	 * unmapped from other threads.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!pud_huge(*pud))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	if (pud_present(*pud)) {</span>
<span class="quote">&gt; +		page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt; +		if (flags &amp; FOLL_GET)</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pud))) {</span>
<span class="quote">&gt; +			spin_unlock(ptl);</span>
<span class="quote">&gt; +			__migration_entry_wait(mm, (pte_t *)pud, ptl);</span>
<span class="quote">&gt; +			goto retry;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * hwpoisoned entry is treated as no_page_table in</span>
<span class="quote">&gt; +		 * follow_page_mask().</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct page * __weak</span>
<span class="quote">&gt;  follow_huge_pgd(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  		pgd_t *pgd, int flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (flags &amp; FOLL_GET)</span>
<span class="quote">&gt; -		return NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return pte_page(*(pte_t *)pgd) +</span>
<span class="quote">&gt; -				((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +retry:</span>
<span class="quote">&gt; +	ptl = pgd_lockptr(mm, pgd);</span>
<span class="quote">&gt; +	spin_lock(ptl);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * make sure that the address range covered by this pgd is not</span>
<span class="quote">&gt; +	 * unmapped from other threads.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!pgd_huge(*pgd))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	if (pgd_present(*pgd)) {</span>
<span class="quote">&gt; +		page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt; +		if (flags &amp; FOLL_GET)</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pgd))) {</span>
<span class="quote">&gt; +			spin_unlock(ptl);</span>
<span class="quote">&gt; +			__migration_entry_wait(mm, (pte_t *)pgd, ptl);</span>
<span class="quote">&gt; +			goto retry;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * hwpoisoned entry is treated as no_page_table in</span>
<span class="quote">&gt; +		 * follow_page_mask().</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt; </span>


Balbir
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - April 7, 2016, 9:34 a.m.</div>
<pre class="content">
Hi Anshuman,

[auto build test ERROR on powerpc/next]
[also build test ERROR on v4.6-rc2 next-20160407]
[if your patch is applied to the wrong git tree, please drop us a note to help improving the system]

url:    https://github.com/0day-ci/linux/commits/Anshuman-Khandual/Enable-HugeTLB-page-migration-on-POWER/20160407-165841
base:   https://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux.git next
config: s390-allyesconfig (attached as .config)
reproduce:
        wget https://git.kernel.org/cgit/linux/kernel/git/wfg/lkp-tests.git/plain/sbin/make.cross -O ~/bin/make.cross
        chmod +x ~/bin/make.cross
        # save the attached .config to linux build tree
        make.cross ARCH=s390 

All errors (new ones prefixed by &gt;&gt;):

   mm/hugetlb.c: In function &#39;follow_huge_pud&#39;:
<span class="quote">&gt;&gt; mm/hugetlb.c:4360:3: error: implicit declaration of function &#39;pud_page&#39; [-Werror=implicit-function-declaration]</span>
      page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);
      ^
   mm/hugetlb.c:4360:8: warning: assignment makes pointer from integer without a cast
      page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);
           ^
   mm/hugetlb.c: In function &#39;follow_huge_pgd&#39;:
   mm/hugetlb.c:4395:3: error: implicit declaration of function &#39;pgd_page&#39; [-Werror=implicit-function-declaration]
      page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);
      ^
   mm/hugetlb.c:4395:8: warning: assignment makes pointer from integer without a cast
      page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);
           ^
   cc1: some warnings being treated as errors

vim +/pud_page +4360 mm/hugetlb.c

  4354		 * make sure that the address range covered by this pud is not
  4355		 * unmapped from other threads.
  4356		 */
  4357		if (!pud_huge(*pud))
  4358			goto out;
  4359		if (pud_present(*pud)) {
<span class="quote">&gt; 4360			page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
  4361			if (flags &amp; FOLL_GET)
  4362				get_page(page);
  4363		} else {

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 11, 2016, 5:39 a.m.</div>
<pre class="content">
On 04/07/2016 02:56 PM, Balbir Singh wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; On 07/04/16 15:37, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; &gt; follow_huge_(pmd|pud|pgd) functions are used to walk the page table and</span>
<span class="quote">&gt;&gt; &gt; fetch the page struct during &#39;follow_page_mask&#39; call. There are possible</span>
<span class="quote">&gt;&gt; &gt; race conditions faced by these functions which arise out of simultaneous</span>
<span class="quote">&gt;&gt; &gt; calls of move_pages() and freeing of huge pages. This was fixed partly</span>
<span class="quote">&gt;&gt; &gt; by the previous commit e66f17ff7177 (&quot;mm/hugetlb: take page table lock</span>
<span class="quote">&gt;&gt; &gt; in follow_huge_pmd()&quot;) for only PMD based huge pages.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; After implementing similar logic, functions like follow_huge_(pud|pgd)</span>
<span class="quote">&gt;&gt; &gt; are now safe from above mentioned race conditions and also can support</span>
<span class="quote">&gt;&gt; &gt; FOLL_GET. Generic version of the function &#39;follow_huge_addr&#39; has been</span>
<span class="quote">&gt;&gt; &gt; left as it is and its upto the architecture to decide on it.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;&gt; &gt; ---</span>
<span class="quote">&gt;&gt; &gt;  include/linux/mm.h | 33 +++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt; &gt;  mm/hugetlb.c       | 67 ++++++++++++++++++++++++++++++++++++++++++++++--------</span>
<span class="quote">&gt;&gt; &gt;  2 files changed, 91 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt;&gt; &gt; index ffcff53..734182a 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt;&gt; &gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt;&gt; &gt; @@ -1751,6 +1751,19 @@ static inline void pgtable_page_dtor(struct page *page)</span>
<span class="quote">&gt;&gt; &gt;  		NULL: pte_offset_kernel(pmd, address))</span>
<span class="quote">&gt;&gt; &gt;  </span>
<span class="quote">&gt;&gt; &gt;  #if USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt; Do we still use USE_SPLIT_PMD_PTLOCKS? I think its good enough. with pgd&#39;s</span>
<span class="quote">&gt; we are likely to use the same locks and the split nature may not be really</span>
<span class="quote">&gt; split.</span>
<span class="quote">&gt; </span>

Sorry Balbir, did not get what you asked. Can you please elaborate on
this ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 11, 2016, 6:04 a.m.</div>
<pre class="content">
On 04/07/2016 03:04 PM, kbuild test robot wrote:
<span class="quote">&gt; All errors (new ones prefixed by &gt;&gt;):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    mm/hugetlb.c: In function &#39;follow_huge_pud&#39;:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; mm/hugetlb.c:4360:3: error: implicit declaration of function &#39;pud_page&#39; [-Werror=implicit-function-declaration]</span>
<span class="quote">&gt;       page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;       ^</span>
<span class="quote">&gt;    mm/hugetlb.c:4360:8: warning: assignment makes pointer from integer without a cast</span>
<span class="quote">&gt;       page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;            ^</span>
<span class="quote">&gt;    mm/hugetlb.c: In function &#39;follow_huge_pgd&#39;:</span>
<span class="quote">&gt;    mm/hugetlb.c:4395:3: error: implicit declaration of function &#39;pgd_page&#39; [-Werror=implicit-function-declaration]</span>
<span class="quote">&gt;       page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>

Both the build errors here are because of the fact that pgd_page() is
not available for some platforms and config options. It got missed as
I ran only powerpc config options for build test purpose. My bad, will
fix it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - April 11, 2016, 12:46 p.m.</div>
<pre class="content">
On 11/04/16 15:39, Anshuman Khandual wrote:
<span class="quote">&gt; On 04/07/2016 02:56 PM, Balbir Singh wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On 07/04/16 15:37, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; follow_huge_(pmd|pud|pgd) functions are used to walk the page table and</span>
<span class="quote">&gt;&gt;&gt;&gt; fetch the page struct during &#39;follow_page_mask&#39; call. There are possible</span>
<span class="quote">&gt;&gt;&gt;&gt; race conditions faced by these functions which arise out of simultaneous</span>
<span class="quote">&gt;&gt;&gt;&gt; calls of move_pages() and freeing of huge pages. This was fixed partly</span>
<span class="quote">&gt;&gt;&gt;&gt; by the previous commit e66f17ff7177 (&quot;mm/hugetlb: take page table lock</span>
<span class="quote">&gt;&gt;&gt;&gt; in follow_huge_pmd()&quot;) for only PMD based huge pages.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; After implementing similar logic, functions like follow_huge_(pud|pgd)</span>
<span class="quote">&gt;&gt;&gt;&gt; are now safe from above mentioned race conditions and also can support</span>
<span class="quote">&gt;&gt;&gt;&gt; FOLL_GET. Generic version of the function &#39;follow_huge_addr&#39; has been</span>
<span class="quote">&gt;&gt;&gt;&gt; left as it is and its upto the architecture to decide on it.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;&gt;  include/linux/mm.h | 33 +++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/hugetlb.c       | 67 ++++++++++++++++++++++++++++++++++++++++++++++--------</span>
<span class="quote">&gt;&gt;&gt;&gt;  2 files changed, 91 insertions(+), 9 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt;&gt;&gt;&gt; index ffcff53..734182a 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -1751,6 +1751,19 @@ static inline void pgtable_page_dtor(struct page *page)</span>
<span class="quote">&gt;&gt;&gt;&gt;  		NULL: pte_offset_kernel(pmd, address))</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;  #if USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt;&gt; Do we still use USE_SPLIT_PMD_PTLOCKS? I think its good enough. with pgd&#39;s</span>
<span class="quote">&gt;&gt; we are likely to use the same locks and the split nature may not be really</span>
<span class="quote">&gt;&gt; split.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry Balbir, did not get what you asked. Can you please elaborate on</span>
<span class="quote">&gt; this ?</span>
<span class="quote">&gt; </span>

What I meant is that do we need SPLIT_PUD_PTLOCKS for example? I don&#39;t think we do

Balbir
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index ffcff53..734182a 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1751,6 +1751,19 @@</span> <span class="p_context"> static inline void pgtable_page_dtor(struct page *page)</span>
 		NULL: pte_offset_kernel(pmd, address))
 
 #if USE_SPLIT_PMD_PTLOCKS
<span class="p_add">+static struct page *pgd_to_page(pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long mask = ~(PTRS_PER_PGD * sizeof(pgd_t) - 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	return virt_to_page((void *)((unsigned long) pgd &amp; mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct page *pud_to_page(pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long mask = ~(PTRS_PER_PUD * sizeof(pud_t) - 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	return virt_to_page((void *)((unsigned long) pud &amp; mask));</span>
<span class="p_add">+}</span>
 
 static struct page *pmd_to_page(pmd_t *pmd)
 {
<span class="p_chunk">@@ -1758,6 +1771,16 @@</span> <span class="p_context"> static struct page *pmd_to_page(pmd_t *pmd)</span>
 	return virt_to_page((void *)((unsigned long) pmd &amp; mask));
 }
 
<span class="p_add">+static inline spinlock_t *pgd_lockptr(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptlock_ptr(pgd_to_page(pgd));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptlock_ptr(pud_to_page(pud));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
 {
 	return ptlock_ptr(pmd_to_page(pmd));
<span class="p_chunk">@@ -1783,6 +1806,16 @@</span> <span class="p_context"> static inline void pgtable_pmd_page_dtor(struct page *page)</span>
 
 #else
 
<span class="p_add">+static inline spinlock_t *pgd_lockptr(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;mm-&gt;page_table_lock;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;mm-&gt;page_table_lock;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
 {
 	return &amp;mm-&gt;page_table_lock;
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 5ea3158..e84e479 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4346,21 +4346,70 @@</span> <span class="p_context"> struct page * __weak</span>
 follow_huge_pud(struct mm_struct *mm, unsigned long address,
 		pud_t *pud, int flags)
 {
<span class="p_del">-	if (flags &amp; FOLL_GET)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	return pte_page(*(pte_t *)pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	ptl = pud_lockptr(mm, pud);</span>
<span class="p_add">+	spin_lock(ptl);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * make sure that the address range covered by this pud is not</span>
<span class="p_add">+	 * unmapped from other threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pud_huge(*pud))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	if (pud_present(*pud)) {</span>
<span class="p_add">+		page = pud_page(*pud) + ((address &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+		if (flags &amp; FOLL_GET)</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pud))) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			__migration_entry_wait(mm, (pte_t *)pud, ptl);</span>
<span class="p_add">+			goto retry;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * hwpoisoned entry is treated as no_page_table in</span>
<span class="p_add">+		 * follow_page_mask().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	return page;</span>
 }
 
 struct page * __weak
 follow_huge_pgd(struct mm_struct *mm, unsigned long address,
 		pgd_t *pgd, int flags)
 {
<span class="p_del">-	if (flags &amp; FOLL_GET)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	return pte_page(*(pte_t *)pgd) +</span>
<span class="p_del">-				((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	ptl = pgd_lockptr(mm, pgd);</span>
<span class="p_add">+	spin_lock(ptl);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * make sure that the address range covered by this pgd is not</span>
<span class="p_add">+	 * unmapped from other threads.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pgd_huge(*pgd))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	if (pgd_present(*pgd)) {</span>
<span class="p_add">+		page = pgd_page(*pgd) + ((address &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+		if (flags &amp; FOLL_GET)</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pgd))) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			__migration_entry_wait(mm, (pte_t *)pgd, ptl);</span>
<span class="p_add">+			goto retry;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * hwpoisoned entry is treated as no_page_table in</span>
<span class="p_add">+		 * follow_page_mask().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	return page;</span>
 }
 
 #ifdef CONFIG_MEMORY_FAILURE

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



