
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v1,05/11] mm: thp: check pmd migration entry in common path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v1,05/11] mm: thp: check pmd migration entry in common path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 3, 2016, 7:41 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1456990918-30906-6-git-send-email-n-horiguchi@ah.jp.nec.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8488611/mbox/"
   >mbox</a>
|
   <a href="/patch/8488611/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8488611/">/patch/8488611/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id AD2ACC0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 07:44:15 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id B11CA20165
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 07:44:14 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 931DF20035
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 07:44:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757033AbcCCHoJ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 3 Mar 2016 02:44:09 -0500
Received: from mail-pa0-f52.google.com ([209.85.220.52]:32998 &quot;EHLO
	mail-pa0-f52.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756724AbcCCHmT (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 3 Mar 2016 02:42:19 -0500
Received: by mail-pa0-f52.google.com with SMTP id fl4so10161376pad.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 02 Mar 2016 23:42:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=6AaYJSrp/JQuN/gC3/gLPE++3EI5wMV+eE5ZKIlPJcc=;
	b=0liy+JYq3O3JjYmZb3L7pGZ1Z8Zs7rlHJcyAdpV2CakicyrpAFpa+qBp8TctIpZvu2
	63Ll0jI5QAiEDNgopNATrTGO9hoaQvudc5Keu7PBtPuwJAabC1Av2erqNBMvzDfzHiSD
	FRdCkdPxXIiAjBweChrtYNcZD7lspx6E9QyKvUv05D92lLGUfGYiAQEgRh9r+tf9JWtd
	T7skcRUIcPZy5CGOiWad6XwBCWNcn1+q2dit9KlPJwTNTgLqt29785P3L70dQ9SoriMl
	tzNaaTzodiMZcxB0QbxlYhcThId9dXzthQglRzZKhQNC+n9FeHg1XwTTmkzr6mJnItg8
	IBcg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:from:to:cc:subject:date:message-id
	:in-reply-to:references;
	bh=6AaYJSrp/JQuN/gC3/gLPE++3EI5wMV+eE5ZKIlPJcc=;
	b=gGJhi1jPOiG15p5yd8XAzlDSXVo6GzWErEeeN/H9NyK4bxvyIKv+gnl9QCWRg+VsKZ
	DvC+UkCint6P8Qe8ZHwTIu4fBcyhTCGqm+lTJUbQDH2gNSiR1GRFYVsgvhSWoVNkherp
	jvHBQwQqbd0AP2kKAtBVj0FE7yRv6Q0RHRdsIi/940pHsXKwq7qEQQleCozLdCt6Y0Nr
	3Iof4lLPFBhFYdawQNhqWkgBwNB9lOMKN8XcZAzVabVegmDNDVBXuUQ7xkK6dFPNA1gJ
	kRloJP+0pM7CYMw0JbUo5jNxeSBMQo7K8smHvzVO8TkHwffMBl0davoI5O2tyXoAMWgy
	deug==
X-Gm-Message-State: AD7BkJKSpbW1XAis3jM13wg2LwIzQyqYHX0aOjJvtoqEapg+mbko9QfkdapBJSnnlG5Mmw==
X-Received: by 10.66.220.162 with SMTP id px2mr1711848pac.15.1456990939153; 
	Wed, 02 Mar 2016 23:42:19 -0800 (PST)
Received: from www9186uo.sakura.ne.jp (www9186uo.sakura.ne.jp.
	[153.121.56.200]) by smtp.gmail.com with ESMTPSA id
	by3sm58161135pab.39.2016.03.02.23.42.15
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 02 Mar 2016 23:42:18 -0800 (PST)
From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
To: linux-mm@kvack.org
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Pavel Emelyanov &lt;xemul@parallels.com&gt;, linux-kernel@vger.kernel.org,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Naoya Horiguchi &lt;nao.horiguchi@gmail.com&gt;
Subject: [PATCH v1 05/11] mm: thp: check pmd migration entry in common path
Date: Thu,  3 Mar 2016 16:41:52 +0900
Message-Id: &lt;1456990918-30906-6-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
X-Mailer: git-send-email 2.7.0
In-Reply-To: &lt;1456990918-30906-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
References: &lt;1456990918-30906-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - March 3, 2016, 7:41 a.m.</div>
<pre class="content">
If one of callers of page migration starts to handle thp, memory management code
start to see pmd migration entry, so we need to prepare for it before enabling.
This patch changes various code point which checks the status of given pmds in
order to prevent race between thp migration and the pmd-related works.
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
---
 arch/x86/mm/gup.c  |  3 +++
 fs/proc/task_mmu.c | 25 +++++++++++++--------
 mm/gup.c           |  8 +++++++
 mm/huge_memory.c   | 66 ++++++++++++++++++++++++++++++++++++++++++++++++------
 mm/memcontrol.c    |  2 ++
 mm/memory.c        |  5 +++++
 6 files changed, 93 insertions(+), 16 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 3, 2016, 10:50 a.m.</div>
<pre class="content">
On Thu, Mar 03, 2016 at 04:41:52PM +0900, Naoya Horiguchi wrote:
<span class="quote">&gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; order to prevent race between thp migration and the pmd-related works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/gup.c  |  3 +++</span>
<span class="quote">&gt;  fs/proc/task_mmu.c | 25 +++++++++++++--------</span>
<span class="quote">&gt;  mm/gup.c           |  8 +++++++</span>
<span class="quote">&gt;  mm/huge_memory.c   | 66 ++++++++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  mm/memcontrol.c    |  2 ++</span>
<span class="quote">&gt;  mm/memory.c        |  5 +++++</span>
<span class="quote">&gt;  6 files changed, 93 insertions(+), 16 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; index f8d0b5e..34c3d43 100644</span>
<span class="quote">&gt; --- v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c</span>
<span class="quote">&gt; +++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; @@ -10,6 +10,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -210,6 +211,8 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		if (pmd_none(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; +			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="quote">&gt; +				return 0;</span>

Hm. I&#39;ve expected to see bunch of pmd_none() to pmd_present() conversions.
That&#39;s seems a right way guard the code. Otherwise we wound need even more
checks once PMD-level swap is implemented.

I think we need to check for migration entires only if we have something
to do with migration. In all other cases pmd_present() should be enough to
bail out.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123921">Naoya Horiguchi</a> - March 3, 2016, 4:15 p.m.</div>
<pre class="content">
On Thu, Mar 03, 2016 at 01:50:58PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, Mar 03, 2016 at 04:41:52PM +0900, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; &gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; &gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; &gt; order to prevent race between thp migration and the pmd-related works.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/x86/mm/gup.c  |  3 +++</span>
<span class="quote">&gt; &gt;  fs/proc/task_mmu.c | 25 +++++++++++++--------</span>
<span class="quote">&gt; &gt;  mm/gup.c           |  8 +++++++</span>
<span class="quote">&gt; &gt;  mm/huge_memory.c   | 66 ++++++++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt; &gt;  mm/memcontrol.c    |  2 ++</span>
<span class="quote">&gt; &gt;  mm/memory.c        |  5 +++++</span>
<span class="quote">&gt; &gt;  6 files changed, 93 insertions(+), 16 deletions(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; index f8d0b5e..34c3d43 100644</span>
<span class="quote">&gt; &gt; --- v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; +++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; @@ -10,6 +10,7 @@</span>
<span class="quote">&gt; &gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  #include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; @@ -210,6 +211,8 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; &gt;  		if (pmd_none(pmd))</span>
<span class="quote">&gt; &gt;  			return 0;</span>
<span class="quote">&gt; &gt;  		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; &gt; +			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="quote">&gt; &gt; +				return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hm. I&#39;ve expected to see bunch of pmd_none() to pmd_present() conversions.</span>
<span class="quote">&gt; That&#39;s seems a right way guard the code. Otherwise we wound need even more</span>
<span class="quote">&gt; checks once PMD-level swap is implemented.</span>

Yes, I agree. I&#39;ll try some for this pmd_none/pmd_present issue.

Thanks,
Naoya
<span class="quote">
&gt;</span>
<span class="quote">&gt; I think we need to check for migration entires only if we have something</span>
<span class="quote">&gt; to do with migration. In all other cases pmd_present() should be enough to</span>
<span class="quote">&gt; bail out.</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="p_header">index f8d0b5e..34c3d43 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/arch/x86/mm/gup.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/memremap.h&gt;
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #include &lt;asm/pgtable.h&gt;
 
<span class="p_chunk">@@ -210,6 +211,8 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		if (pmd_none(pmd))
 			return 0;
 		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+				return 0;</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/fs/proc/task_mmu.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/fs/proc/task_mmu.c</span>
<span class="p_header">index fa95ab2..20205d4 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/fs/proc/task_mmu.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -907,6 +907,9 @@</span> <span class="p_context"> static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
 
 	ptl = pmd_trans_huge_lock(pmd, vma);
 	if (ptl) {
<span class="p_add">+		if (unlikely(is_pmd_migration_entry(*pmd)))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+</span>
 		if (cp-&gt;type == CLEAR_REFS_SOFT_DIRTY) {
 			clear_soft_dirty_pmd(vma, addr, pmd);
 			goto out;
<span class="p_chunk">@@ -1184,19 +1187,18 @@</span> <span class="p_context"> static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
 	if (ptl) {
 		u64 flags = 0, frame = 0;
 		pmd_t pmd = *pmdp;
<span class="p_add">+		struct page *page;</span>
 
 		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))
 			flags |= PM_SOFT_DIRTY;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Currently pmd for thp is always present because thp</span>
<span class="p_del">-		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="p_del">-		 * (split in such cases instead.)</span>
<span class="p_del">-		 * This if-check is just to prepare for future implementation.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (pmd_present(pmd)) {</span>
<span class="p_del">-			struct page *page = pmd_page(pmd);</span>
<span class="p_del">-</span>
<span class="p_add">+		if (is_pmd_migration_entry(pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+			frame = swp_type(entry) |</span>
<span class="p_add">+				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+		} else if (pmd_present(pmd)) {</span>
<span class="p_add">+			page = pmd_page(pmd);</span>
 			if (page_mapcount(page) == 1)
 				flags |= PM_MMAP_EXCLUSIVE;
 
<span class="p_chunk">@@ -1518,6 +1520,11 @@</span> <span class="p_context"> static int gather_pte_stats(pmd_t *pmd, unsigned long addr,</span>
 		pte_t huge_pte = *(pte_t *)pmd;
 		struct page *page;
 
<span class="p_add">+		if (unlikely(is_pmd_migration_entry(*pmd))) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		page = can_gather_numa_stats(huge_pte, vma, addr);
 		if (page)
 			gather_stats(page, md, pte_dirty(huge_pte),
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/mm/gup.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/gup.c</span>
<span class="p_header">index 36ca850..113930b 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/mm/gup.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/gup.c</span>
<span class="p_chunk">@@ -271,6 +271,11 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
 	}
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (flags &amp; FOLL_SPLIT) {
 		int ret;
 		page = pmd_page(*pmd);
<span class="p_chunk">@@ -1324,6 +1329,9 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 			return 0;
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/mm/huge_memory.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/huge_memory.c</span>
<span class="p_header">index c6d5406..7120036 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/mm/huge_memory.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1107,6 +1107,19 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		goto out_unlock;
 	}
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="p_add">+		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+			make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+			pmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (!vma_is_dax(vma)) {
 		/* thp accounting separate from pmd_devmap accounting */
 		src_page = pmd_page(pmd);
<span class="p_chunk">@@ -1284,6 +1297,9 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (unlikely(!pmd_same(*pmd, orig_pmd)))
 		goto out_unlock;
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(*pmd)))</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);
 	/*
<span class="p_chunk">@@ -1418,7 +1434,14 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		goto out;
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+		if (!is_migration_entry(entry))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 	if (flags &amp; FOLL_TOUCH)
 		touch_pmd(vma, addr, pmd);
<span class="p_chunk">@@ -1601,6 +1624,9 @@</span> <span class="p_context"> int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		goto out;
 	}
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(orig_pmd)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	/*
 	 * If other processes are mapping this page, we couldn&#39;t discard
<span class="p_chunk">@@ -1681,15 +1707,28 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		put_huge_zero_page();
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="p_add">+			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			migration = 1;</span>
<span class="p_add">+		}</span>
 		pte_free(tlb-&gt;mm, pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd));
 		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page(tlb, page);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page(tlb, page);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -1775,6 +1814,11 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			return ret;
 		}
 
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (!prot_numa || !pmd_protnone(*pmd)) {
 			entry = pmdp_huge_get_and_clear_notify(mm, addr, pmd);
 			entry = pmd_modify(entry, newprot);
<span class="p_chunk">@@ -3071,6 +3115,9 @@</span> <span class="p_context"> static void split_huge_pmd_address(struct vm_area_struct *vma,</span>
 	pmd = pmd_offset(pud, address);
 	if (!pmd_present(*pmd) || (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)))
 		return;
<span class="p_add">+	if (pmd_trans_huge(*pmd) &amp;&amp; is_pmd_migration_entry(*pmd))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	/*
 	 * Caller holds the mmap_sem write mode, so a huge pmd cannot
 	 * materialize from under us.
<span class="p_chunk">@@ -3151,6 +3198,11 @@</span> <span class="p_context"> static void freeze_page_vma(struct vm_area_struct *vma, struct page *page,</span>
 		return;
 	}
 	if (pmd_trans_huge(*pmd)) {
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (page == pmd_page(*pmd))
 			__split_huge_pmd_locked(vma, pmd, haddr, true);
 		spin_unlock(ptl);
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/mm/memcontrol.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/memcontrol.c</span>
<span class="p_header">index ae8b81c..1772043 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/mm/memcontrol.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4548,6 +4548,8 @@</span> <span class="p_context"> static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 	struct page *page = NULL;
 	enum mc_target_type ret = MC_TARGET_NONE;
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+		return ret;</span>
 	page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!page || !PageHead(page), page);
 	if (!(mc.flags &amp; MOVE_ANON))
<span class="p_header">diff --git v4.5-rc5-mmotm-2016-02-24-16-18/mm/memory.c v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/memory.c</span>
<span class="p_header">index 6c92a99..a04a685 100644</span>
<span class="p_header">--- v4.5-rc5-mmotm-2016-02-24-16-18/mm/memory.c</span>
<span class="p_header">+++ v4.5-rc5-mmotm-2016-02-24-16-18_patched/mm/memory.c</span>
<span class="p_chunk">@@ -3405,6 +3405,11 @@</span> <span class="p_context"> static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 			unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;
 
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="p_add">+				pmd_migration_entry_wait(mm, pmd);</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			if (pmd_protnone(orig_pmd))
 				return do_huge_pmd_numa_page(mm, vma, address,
 							     orig_pmd, pmd);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



