
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V4,1/3] mm/thp: Split out pmd collpase flush into a separate functions - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V4,1/3] mm/thp: Split out pmd collpase flush into a separate functions</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 12, 2015, 6:08 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1431410914-21102-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6385881/mbox/"
   >mbox</a>
|
   <a href="/patch/6385881/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6385881/">/patch/6385881/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id A874BBEEE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 06:09:31 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 8BBBE20328
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 06:09:30 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A8CA420395
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 06:09:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932192AbbELGJR (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 May 2015 02:09:17 -0400
Received: from e28smtp04.in.ibm.com ([122.248.162.4]:34534 &quot;EHLO
	e28smtp04.in.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752675AbbELGJJ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 May 2015 02:09:09 -0400
Received: from /spool/local
	by e28smtp04.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Tue, 12 May 2015 11:39:07 +0530
Received: from d28dlp02.in.ibm.com (9.184.220.127)
	by e28smtp04.in.ibm.com (192.168.1.134) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Tue, 12 May 2015 11:39:05 +0530
Received: from d28relay01.in.ibm.com (d28relay01.in.ibm.com [9.184.220.58])
	by d28dlp02.in.ibm.com (Postfix) with ESMTP id 8C7093940060
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 12 May 2015 11:39:04 +0530 (IST)
Received: from d28av03.in.ibm.com (d28av03.in.ibm.com [9.184.220.65])
	by d28relay01.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	t4C68lYc26083556
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 12 May 2015 11:38:47 +0530
Received: from d28av03.in.ibm.com (localhost [127.0.0.1])
	by d28av03.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	t4C5WnJr018483
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 12 May 2015 11:02:51 +0530
Received: from skywalker.in.ibm.com ([9.79.218.40])
	by d28av03.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	t4C5WnV9018419; Tue, 12 May 2015 11:02:49 +0530
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: benh@kernel.crashing.org, paulus@samba.org, mpe@ellerman.id.au,
	kirill.shutemov@linux.intel.com, aarcange@redhat.com,
	akpm@linux-foundation.org
Cc: linuxppc-dev@lists.ozlabs.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH V4 1/3] mm/thp: Split out pmd collpase flush into a separate
	functions
Date: Tue, 12 May 2015 11:38:32 +0530
Message-Id: &lt;1431410914-21102-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.1.4
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15051206-0013-0000-0000-00000522396E
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 12, 2015, 6:08 a.m.</div>
<pre class="content">
Architectures like ppc64 [1] need to do special things while clearing
pmd before a collapse. For them this operation is largely different
from a normal hugepage pte clear. Hence add a separate function
to clear pmd before collapse. After this patch pmdp_* functions
operate only on hugepage pte, and not on regular pmd_t values
pointing to page table.

[1] ppc64 needs to invalidate all the normal page pte mappings we
already have inserted in the hardware hash page table. But before
doing that we need to make sure there are no parallel hash page
table insert going on. So we need to do a kick_all_cpus_sync()
before flushing the older hash table entries. By moving this to
a separate function we capture these details and mention how it
is different from a hugepage pte clear.

This patch is a cleanup and only does code movement for clarity.
There should not be any change in functionality.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++
 arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------
 include/asm-generic/pgtable.h            | 19 ++++++++
 mm/huge_memory.c                         |  2 +-
 4 files changed, 65 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - May 12, 2015, 9:19 a.m.</div>
<pre class="content">
On Tue, May 12, 2015 at 11:38:32AM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Architectures like ppc64 [1] need to do special things while clearing</span>
<span class="quote">&gt; pmd before a collapse. For them this operation is largely different</span>
<span class="quote">&gt; from a normal hugepage pte clear. Hence add a separate function</span>
<span class="quote">&gt; to clear pmd before collapse. After this patch pmdp_* functions</span>
<span class="quote">&gt; operate only on hugepage pte, and not on regular pmd_t values</span>
<span class="quote">&gt; pointing to page table.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] ppc64 needs to invalidate all the normal page pte mappings we</span>
<span class="quote">&gt; already have inserted in the hardware hash page table. But before</span>
<span class="quote">&gt; doing that we need to make sure there are no parallel hash page</span>
<span class="quote">&gt; table insert going on. So we need to do a kick_all_cpus_sync()</span>
<span class="quote">&gt; before flushing the older hash table entries. By moving this to</span>
<span class="quote">&gt; a separate function we capture these details and mention how it</span>
<span class="quote">&gt; is different from a hugepage pte clear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch is a cleanup and only does code movement for clarity.</span>
<span class="quote">&gt; There should not be any change in functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>

For the patchset:
<span class="acked-by">
Acked-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">index 43e6ad424c7f..b8d99227a0ac 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_chunk">@@ -576,6 +576,10 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
 extern void pmdp_splitting_flush(struct vm_area_struct *vma,
 				 unsigned long address, pmd_t *pmdp);
 
<span class="p_add">+#define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="p_add">+extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long address, pmd_t *pmdp);</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">index 6bfadf1aa5cb..049d961802aa 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_chunk">@@ -560,41 +560,47 @@</span> <span class="p_context"> pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	pmd_t pmd;
 
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	if (pmd_trans_huge(*pmdp)) {</span>
<span class="p_del">-		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * khugepaged calls this for normal pmd</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = *pmdp;</span>
<span class="p_del">-		pmd_clear(pmdp);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_del">-		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_del">-		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_del">-		 * the PTE entries. The assumption here is that any low level</span>
<span class="p_del">-		 * page fault will see a none pmd and take the slow path that</span>
<span class="p_del">-		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_del">-		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_del">-		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_del">-		 * That means we could be modifying the page content as we</span>
<span class="p_del">-		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_del">-		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_del">-		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_del">-		 * function there.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		kick_all_cpus_sync();</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Now invalidate the hpte entries in the range</span>
<span class="p_del">-		 * covered by pmd. This make sure we take a</span>
<span class="p_del">-		 * fault and will find the pmd as none, which will</span>
<span class="p_del">-		 * result in a major fault which takes mmap_sem and</span>
<span class="p_del">-		 * hence wait for collapse to complete. Without this</span>
<span class="p_del">-		 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_del">-		 * the old content.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="p_add">+	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_add">+	return pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+			  pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = *pmdp;</span>
<span class="p_add">+	pmd_clear(pmdp);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_add">+	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_add">+	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_add">+	 * the PTE entries. The assumption here is that any low level</span>
<span class="p_add">+	 * page fault will see a none pmd and take the slow path that</span>
<span class="p_add">+	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_add">+	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_add">+	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_add">+	 * That means we could be modifying the page content as we</span>
<span class="p_add">+	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_add">+	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_add">+	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_add">+	 * function there.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kick_all_cpus_sync();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now invalidate the hpte entries in the range</span>
<span class="p_add">+	 * covered by pmd. This make sure we take a</span>
<span class="p_add">+	 * fault and will find the pmd as none, which will</span>
<span class="p_add">+	 * result in a major fault which takes mmap_sem and</span>
<span class="p_add">+	 * hence wait for collapse to complete. Without this</span>
<span class="p_add">+	 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_add">+	 * the old content.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
 	return pmd;
 }
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 39f1d6a2b04d..3e22b80d467a 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -189,6 +189,25 @@</span> <span class="p_context"> extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
 				 unsigned long address, pmd_t *pmdp);
 #endif
 
<span class="p_add">+#ifndef pmdp_collapse_flush</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address,</span>
<span class="p_add">+					pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address,</span>
<span class="p_add">+					pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return __pmd(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 078832cf3636..88f695a4e38b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2499,7 +2499,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * huge and small TLB entries for the same virtual address
 	 * to avoid the risk of CPU bugs in that area.
 	 */
<span class="p_del">-	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="p_add">+	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>
 	spin_unlock(pmd_ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



