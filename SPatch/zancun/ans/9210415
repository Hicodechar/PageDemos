
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[01/31] mm, vmstat: add infrastructure for per-node vmstats - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [01/31] mm, vmstat: add infrastructure for per-node vmstats</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 1, 2016, 8:01 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1467403299-25786-2-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9210415/mbox/"
   >mbox</a>
|
   <a href="/patch/9210415/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9210415/">/patch/9210415/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7299B607D6 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 20:11:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 61125286C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 20:11:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5301D286D4; Fri,  1 Jul 2016 20:11:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7A396286C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 20:11:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752638AbcGAULS (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 1 Jul 2016 16:11:18 -0400
Received: from outbound-smtp06.blacknight.com ([81.17.249.39]:44857 &quot;EHLO
	outbound-smtp06.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751966AbcGAUCm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 1 Jul 2016 16:02:42 -0400
Received: from mail.blacknight.com (pemlinmail04.blacknight.ie
	[81.17.254.17])
	by outbound-smtp06.blacknight.com (Postfix) with ESMTPS id B863798E36
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri,  1 Jul 2016 20:02:00 +0000 (UTC)
Received: (qmail 9016 invoked from network); 1 Jul 2016 20:02:00 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.231.136])
	by 81.17.254.9 with ESMTPA; 1 Jul 2016 20:02:00 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 01/31] mm, vmstat: add infrastructure for per-node vmstats
Date: Fri,  1 Jul 2016 21:01:09 +0100
Message-Id: &lt;1467403299-25786-2-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.6.4
In-Reply-To: &lt;1467403299-25786-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1467403299-25786-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - July 1, 2016, 8:01 p.m.</div>
<pre class="content">
VM statistic counters for reclaim decisions are zone-based.  If the kernel
is to reclaim on a per-node basis then we need to track per-node
statistics but there is no infrastructure for that.  The most notable
change is that the old node_page_state is renamed to
sum_zone_node_page_state.  The new node_page_state takes a pglist_data and
uses per-node stats but none exist yet.  There is some renaming such as
vm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming
of mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical
patch with no functional change.  There is a lot of similarity between the
node and zone helpers which is unfortunate but there was no obvious way of
reusing the code and maintaining type safety.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="acked-by">Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="acked-by">Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
---
 drivers/base/node.c    |  76 +++++++------
 include/linux/mm.h     |   5 +
 include/linux/mmzone.h |  13 +++
 include/linux/vmstat.h |  92 +++++++++++++---
 mm/page_alloc.c        |  10 +-
 mm/vmstat.c            | 282 +++++++++++++++++++++++++++++++++++++++++++++----
 mm/workingset.c        |   9 +-
 7 files changed, 411 insertions(+), 76 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - July 4, 2016, 11:50 p.m.</div>
<pre class="content">
On Fri, Jul 01, 2016 at 09:01:09PM +0100, Mel Gorman wrote:
<span class="quote">&gt; VM statistic counters for reclaim decisions are zone-based.  If the kernel</span>
<span class="quote">&gt; is to reclaim on a per-node basis then we need to track per-node</span>
<span class="quote">&gt; statistics but there is no infrastructure for that.  The most notable</span>
<span class="quote">&gt; change is that the old node_page_state is renamed to</span>
<span class="quote">&gt; sum_zone_node_page_state.  The new node_page_state takes a pglist_data and</span>
<span class="quote">&gt; uses per-node stats but none exist yet.  There is some renaming such as</span>
<span class="quote">&gt; vm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming</span>
<span class="quote">&gt; of mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical</span>
<span class="quote">&gt; patch with no functional change.  There is a lot of similarity between the</span>
<span class="quote">&gt; node and zone helpers which is unfortunate but there was no obvious way of</span>
<span class="quote">&gt; reusing the code and maintaining type safety.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; ---</span>

&lt;snip&gt;
<span class="quote">
&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 7997f52935c9..90b0737ee4be 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -86,8 +86,10 @@ void vm_events_fold_cpu(int cpu)</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * vm_stat contains the global counters</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="quote">&gt; -EXPORT_SYMBOL(vm_stat);</span>
<span class="quote">&gt; +atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="quote">&gt; +atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="quote">&gt; +EXPORT_SYMBOL(vm_zone_stat);</span>
<span class="quote">&gt; +EXPORT_SYMBOL(vm_node_stat);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -172,13 +174,17 @@ void refresh_zone_stat_thresholds(void)</span>
<span class="quote">&gt;  	int threshold;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_populated_zone(zone) {</span>
<span class="quote">&gt; +		struct pglist_data *pgdat = zone-&gt;zone_pgdat;</span>
<span class="quote">&gt;  		unsigned long max_drift, tolerate_drift;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		threshold = calculate_normal_threshold(zone);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		for_each_online_cpu(cpu)</span>
<span class="quote">&gt; +		for_each_online_cpu(cpu) {</span>
<span class="quote">&gt;  			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;stat_threshold</span>
<span class="quote">&gt;  							= threshold;</span>
<span class="quote">&gt; +			per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu)-&gt;stat_threshold</span>
<span class="quote">&gt; +							= threshold;</span>
<span class="quote">&gt; +		}</span>

I didn&#39;t see other patches yet so it might fix it then.

per_cpu_nodestats is per node not zone but it use per-zone threshold
and even overwritten by next zones. I don&#39;t think it&#39;s not intended.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index ed0ef0f69489..92d8e090c5b3 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -74,16 +74,16 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(i.totalram),
 		       nid, K(i.freeram),
 		       nid, K(i.totalram - i.freeram),
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_ANON) +</span>
<span class="p_del">-				node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_ANON) +</span>
<span class="p_del">-				node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_ANON)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_MLOCK)));</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_ANON) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_ANON) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_ANON)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_MLOCK)));</span>
 
 #ifdef CONFIG_HIGHMEM
 	n += sprintf(buf + n,
<span class="p_chunk">@@ -117,31 +117,31 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       &quot;Node %d ShmemPmdMapped: %8lu kB\n&quot;
 #endif
 			,
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_DIRTY)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_WRITEBACK)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_PAGES)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_FILE_MAPPED)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ANON_PAGES)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_DIRTY)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_WRITEBACK)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_PAGES)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_FILE_MAPPED)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ANON_PAGES)),</span>
 		       nid, K(i.sharedram),
<span class="p_del">-		       nid, node_page_state(nid, NR_KERNEL_STACK) *</span>
<span class="p_add">+		       nid, sum_zone_node_page_state(nid, NR_KERNEL_STACK) *</span>
 				THREAD_SIZE / 1024,
<span class="p_del">-		       nid, K(node_page_state(nid, NR_PAGETABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_UNSTABLE_NFS)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_BOUNCE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_WRITEBACK_TEMP)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_del">-				node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_PAGETABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_UNSTABLE_NFS)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_WRITEBACK_TEMP)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_add">+				sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_del">-		       nid, K(node_page_state(nid, NR_ANON_THPS) *</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_ANON_THPS) *</span>
 				       HPAGE_PMD_NR),
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SHMEM_THPS) *</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SHMEM_THPS) *</span>
 				       HPAGE_PMD_NR),
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SHMEM_PMDMAPPED) *</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SHMEM_PMDMAPPED) *</span>
 				       HPAGE_PMD_NR));
 #else
<span class="p_del">-		       nid, K(node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
<span class="p_add">+		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
 #endif
 	n += hugetlb_report_node_meminfo(nid, buf + n);
 	return n;
<span class="p_chunk">@@ -160,12 +160,12 @@</span> <span class="p_context"> static ssize_t node_read_numastat(struct device *dev,</span>
 		       &quot;interleave_hit %lu\n&quot;
 		       &quot;local_node %lu\n&quot;
 		       &quot;other_node %lu\n&quot;,
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_HIT),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_MISS),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_FOREIGN),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_INTERLEAVE_HIT),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_LOCAL),</span>
<span class="p_del">-		       node_page_state(dev-&gt;id, NUMA_OTHER));</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_HIT),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_MISS),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_FOREIGN),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_INTERLEAVE_HIT),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_LOCAL),</span>
<span class="p_add">+		       sum_zone_node_page_state(dev-&gt;id, NUMA_OTHER));</span>
 }
 static DEVICE_ATTR(numastat, S_IRUGO, node_read_numastat, NULL);
 
<span class="p_chunk">@@ -173,12 +173,18 @@</span> <span class="p_context"> static ssize_t node_read_vmstat(struct device *dev,</span>
 				struct device_attribute *attr, char *buf)
 {
 	int nid = dev-&gt;id;
<span class="p_add">+	struct pglist_data *pgdat = NODE_DATA(nid);</span>
 	int i;
 	int n = 0;
 
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++)
 		n += sprintf(buf+n, &quot;%s %lu\n&quot;, vmstat_text[i],
<span class="p_del">-			     node_page_state(nid, i));</span>
<span class="p_add">+			     sum_zone_node_page_state(nid, i));</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		n += sprintf(buf+n, &quot;%s %lu\n&quot;,</span>
<span class="p_add">+			     vmstat_text[i + NR_VM_ZONE_STAT_ITEMS],</span>
<span class="p_add">+			     node_page_state(pgdat, i));</span>
 
 	return n;
 }
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index b21e5f30378e..dd79aa2800a3 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -933,6 +933,11 @@</span> <span class="p_context"> static inline struct zone *page_zone(const struct page *page)</span>
 	return &amp;NODE_DATA(page_to_nid(page))-&gt;node_zones[page_zonenum(page)];
 }
 
<span class="p_add">+static inline pg_data_t *page_pgdat(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NODE_DATA(page_to_nid(page));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef SECTION_IN_PAGE_FLAGS
 static inline void set_page_section(struct page *page, unsigned long section)
 {
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index 19425e988bdc..078ecb81e209 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -160,6 +160,10 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_FREE_CMA_PAGES,
 	NR_VM_ZONE_STAT_ITEMS };
 
<span class="p_add">+enum node_stat_item {</span>
<span class="p_add">+	NR_VM_NODE_STAT_ITEMS</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /*
  * We do arithmetic on the LRU lists in various places in the code,
  * so it is important to keep the active lists LRU_ACTIVE higher in
<span class="p_chunk">@@ -267,6 +271,11 @@</span> <span class="p_context"> struct per_cpu_pageset {</span>
 #endif
 };
 
<span class="p_add">+struct per_cpu_nodestat {</span>
<span class="p_add">+	s8 stat_threshold;</span>
<span class="p_add">+	s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #endif /* !__GENERATING_BOUNDS.H */
 
 enum zone_type {
<span class="p_chunk">@@ -695,6 +704,10 @@</span> <span class="p_context"> typedef struct pglist_data {</span>
 	struct list_head split_queue;
 	unsigned long split_queue_len;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	/* Per-node vmstats */</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *per_cpu_nodestats;</span>
<span class="p_add">+	atomic_long_t		vm_stat[NR_VM_NODE_STAT_ITEMS];</span>
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)-&gt;node_present_pages)
<span class="p_header">diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h</span>
<span class="p_header">index d2da8e053210..d1744aa3ab9c 100644</span>
<span class="p_header">--- a/include/linux/vmstat.h</span>
<span class="p_header">+++ b/include/linux/vmstat.h</span>
<span class="p_chunk">@@ -106,20 +106,38 @@</span> <span class="p_context"> static inline void vm_events_fold_cpu(int cpu)</span>
 		zone_idx(zone), delta)
 
 /*
<span class="p_del">- * Zone based page accounting with per cpu differentials.</span>
<span class="p_add">+ * Zone and node-based page accounting with per cpu differentials.</span>
  */
<span class="p_del">-extern atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];</span>
<span class="p_add">+extern atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS];</span>
<span class="p_add">+extern atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS];</span>
 
 static inline void zone_page_state_add(long x, struct zone *zone,
 				 enum zone_stat_item item)
 {
 	atomic_long_add(x, &amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_add(x, &amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_add(x, &amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void node_page_state_add(long x, struct pglist_data *pgdat,</span>
<span class="p_add">+				 enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_add(x, &amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_add(x, &amp;vm_node_stat[item]);</span>
 }
 
 static inline unsigned long global_page_state(enum zone_stat_item item)
 {
<span class="p_del">-	long x = atomic_long_read(&amp;vm_stat[item]);</span>
<span class="p_add">+	long x = atomic_long_read(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	if (x &lt; 0)</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return x;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long global_node_page_state(enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long x = atomic_long_read(&amp;vm_node_stat[item]);</span>
 #ifdef CONFIG_SMP
 	if (x &lt; 0)
 		x = 0;
<span class="p_chunk">@@ -161,31 +179,44 @@</span> <span class="p_context"> static inline unsigned long zone_page_state_snapshot(struct zone *zone,</span>
 }
 
 #ifdef CONFIG_NUMA
<span class="p_del">-</span>
<span class="p_del">-extern unsigned long node_page_state(int node, enum zone_stat_item item);</span>
<span class="p_del">-</span>
<span class="p_add">+extern unsigned long sum_zone_node_page_state(int node,</span>
<span class="p_add">+						enum zone_stat_item item);</span>
<span class="p_add">+extern unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+						enum node_stat_item item);</span>
 #else
<span class="p_del">-</span>
<span class="p_del">-#define node_page_state(node, item) global_page_state(item)</span>
<span class="p_del">-</span>
<span class="p_add">+#define sum_zone_node_page_state(node, item) global_page_state(item)</span>
<span class="p_add">+#define node_page_state(node, item) global_node_page_state(item)</span>
 #endif /* CONFIG_NUMA */
 
 #define add_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, __d)
 #define sub_zone_page_state(__z, __i, __d) mod_zone_page_state(__z, __i, -(__d))
<span class="p_add">+#define add_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, __d)</span>
<span class="p_add">+#define sub_node_page_state(__p, __i, __d) mod_node_page_state(__p, __i, -(__d))</span>
 
 #ifdef CONFIG_SMP
 void __mod_zone_page_state(struct zone *, enum zone_stat_item item, long);
 void __inc_zone_page_state(struct page *, enum zone_stat_item);
 void __dec_zone_page_state(struct page *, enum zone_stat_item);
 
<span class="p_add">+void __mod_node_page_state(struct pglist_data *, enum node_stat_item item, long);</span>
<span class="p_add">+void __inc_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+void __dec_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+</span>
 void mod_zone_page_state(struct zone *, enum zone_stat_item, long);
 void inc_zone_page_state(struct page *, enum zone_stat_item);
 void dec_zone_page_state(struct page *, enum zone_stat_item);
 
<span class="p_add">+void mod_node_page_state(struct pglist_data *, enum node_stat_item, long);</span>
<span class="p_add">+void inc_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+void dec_node_page_state(struct page *, enum node_stat_item);</span>
<span class="p_add">+</span>
 extern void inc_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void inc_node_state(struct pglist_data *, enum node_stat_item);</span>
 extern void __inc_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void __inc_node_state(struct pglist_data *, enum node_stat_item);</span>
 extern void dec_zone_state(struct zone *, enum zone_stat_item);
 extern void __dec_zone_state(struct zone *, enum zone_stat_item);
<span class="p_add">+extern void __dec_node_state(struct pglist_data *, enum node_stat_item);</span>
 
 void quiet_vmstat(void);
 void cpu_vm_stats_fold(int cpu);
<span class="p_chunk">@@ -213,16 +244,34 @@</span> <span class="p_context"> static inline void __mod_zone_page_state(struct zone *zone,</span>
 	zone_page_state_add(delta, zone, item);
 }
 
<span class="p_add">+static inline void __mod_node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+			enum node_stat_item item, int delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	node_page_state_add(delta, pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_inc(&amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_inc(&amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_inc(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_inc(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_inc(&amp;vm_node_stat[item]);</span>
 }
 
 static inline void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	atomic_long_dec(&amp;zone-&gt;vm_stat[item]);
<span class="p_del">-	atomic_long_dec(&amp;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_dec(&amp;vm_zone_stat[item]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_dec(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+	atomic_long_dec(&amp;vm_node_stat[item]);</span>
 }
 
 static inline void __inc_zone_page_state(struct page *page,
<span class="p_chunk">@@ -231,12 +280,26 @@</span> <span class="p_context"> static inline void __inc_zone_page_state(struct page *page,</span>
 	__inc_zone_state(page_zone(page), item);
 }
 
<span class="p_add">+static inline void __inc_node_page_state(struct page *page,</span>
<span class="p_add">+			enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__inc_node_state(page_pgdat(page), item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 static inline void __dec_zone_page_state(struct page *page,
 			enum zone_stat_item item)
 {
 	__dec_zone_state(page_zone(page), item);
 }
 
<span class="p_add">+static inline void __dec_node_page_state(struct page *page,</span>
<span class="p_add">+			enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dec_node_state(page_pgdat(page), item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 /*
  * We only use atomic operations to update counters. So there is no need to
  * disable interrupts.
<span class="p_chunk">@@ -245,7 +308,12 @@</span> <span class="p_context"> static inline void __dec_zone_page_state(struct page *page,</span>
 #define dec_zone_page_state __dec_zone_page_state
 #define mod_zone_page_state __mod_zone_page_state
 
<span class="p_add">+#define inc_node_page_state __inc_node_page_state</span>
<span class="p_add">+#define dec_node_page_state __dec_node_page_state</span>
<span class="p_add">+#define mod_node_page_state __mod_node_page_state</span>
<span class="p_add">+</span>
 #define inc_zone_state __inc_zone_state
<span class="p_add">+#define inc_node_state __inc_node_state</span>
 #define dec_zone_state __dec_zone_state
 
 #define set_pgdat_percpu_threshold(pgdat, callback) { }
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 403c5dcd24da..34e46c02a406 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -4247,8 +4247,8 @@</span> <span class="p_context"> void si_meminfo_node(struct sysinfo *val, int nid)</span>
 	for (zone_type = 0; zone_type &lt; MAX_NR_ZONES; zone_type++)
 		managed_pages += pgdat-&gt;node_zones[zone_type].managed_pages;
 	val-&gt;totalram = managed_pages;
<span class="p_del">-	val-&gt;sharedram = node_page_state(nid, NR_SHMEM);</span>
<span class="p_del">-	val-&gt;freeram = node_page_state(nid, NR_FREE_PAGES);</span>
<span class="p_add">+	val-&gt;sharedram = sum_zone_node_page_state(nid, NR_SHMEM);</span>
<span class="p_add">+	val-&gt;freeram = sum_zone_node_page_state(nid, NR_FREE_PAGES);</span>
 #ifdef CONFIG_HIGHMEM
 	for (zone_type = 0; zone_type &lt; MAX_NR_ZONES; zone_type++) {
 		struct zone *zone = &amp;pgdat-&gt;node_zones[zone_type];
<span class="p_chunk">@@ -5373,6 +5373,11 @@</span> <span class="p_context"> static void __meminit setup_zone_pageset(struct zone *zone)</span>
 	zone-&gt;pageset = alloc_percpu(struct per_cpu_pageset);
 	for_each_possible_cpu(cpu)
 		zone_pageset_init(zone, cpu);
<span class="p_add">+</span>
<span class="p_add">+	if (!zone-&gt;zone_pgdat-&gt;per_cpu_nodestats) {</span>
<span class="p_add">+		zone-&gt;zone_pgdat-&gt;per_cpu_nodestats =</span>
<span class="p_add">+			alloc_percpu(struct per_cpu_nodestat);</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_chunk">@@ -6078,6 +6083,7 @@</span> <span class="p_context"> void __paginginit free_area_init_node(int nid, unsigned long *zones_size,</span>
 	reset_deferred_meminit(pgdat);
 	pgdat-&gt;node_id = nid;
 	pgdat-&gt;node_start_pfn = node_start_pfn;
<span class="p_add">+	pgdat-&gt;per_cpu_nodestats = NULL;</span>
 #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 	get_pfn_range_for_nid(nid, &amp;start_pfn, &amp;end_pfn);
 	pr_info(&quot;Initmem setup node %d [mem %#018Lx-%#018Lx]\n&quot;, nid,
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 7997f52935c9..90b0737ee4be 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -86,8 +86,10 @@</span> <span class="p_context"> void vm_events_fold_cpu(int cpu)</span>
  *
  * vm_stat contains the global counters
  */
<span class="p_del">-atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_del">-EXPORT_SYMBOL(vm_stat);</span>
<span class="p_add">+atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_add">+atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp;</span>
<span class="p_add">+EXPORT_SYMBOL(vm_zone_stat);</span>
<span class="p_add">+EXPORT_SYMBOL(vm_node_stat);</span>
 
 #ifdef CONFIG_SMP
 
<span class="p_chunk">@@ -172,13 +174,17 @@</span> <span class="p_context"> void refresh_zone_stat_thresholds(void)</span>
 	int threshold;
 
 	for_each_populated_zone(zone) {
<span class="p_add">+		struct pglist_data *pgdat = zone-&gt;zone_pgdat;</span>
 		unsigned long max_drift, tolerate_drift;
 
 		threshold = calculate_normal_threshold(zone);
 
<span class="p_del">-		for_each_online_cpu(cpu)</span>
<span class="p_add">+		for_each_online_cpu(cpu) {</span>
 			per_cpu_ptr(zone-&gt;pageset, cpu)-&gt;stat_threshold
 							= threshold;
<span class="p_add">+			per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu)-&gt;stat_threshold</span>
<span class="p_add">+							= threshold;</span>
<span class="p_add">+		}</span>
 
 		/*
 		 * Only set percpu_drift_mark if there is a danger that
<span class="p_chunk">@@ -238,6 +244,26 @@</span> <span class="p_context"> void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,</span>
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
<span class="p_add">+void __mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+				long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	long x;</span>
<span class="p_add">+	long t;</span>
<span class="p_add">+</span>
<span class="p_add">+	x = delta + __this_cpu_read(*p);</span>
<span class="p_add">+</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(x &gt; t || x &lt; -t)) {</span>
<span class="p_add">+		node_page_state_add(x, pgdat, item);</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__this_cpu_write(*p, x);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__mod_node_page_state);</span>
<span class="p_add">+</span>
 /*
  * Optimized increment and decrement functions.
  *
<span class="p_chunk">@@ -277,12 +303,34 @@</span> <span class="p_context"> void __inc_zone_state(struct zone *zone, enum zone_stat_item item)</span>
 	}
 }
 
<span class="p_add">+void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	s8 v, t;</span>
<span class="p_add">+</span>
<span class="p_add">+	v = __this_cpu_inc_return(*p);</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+	if (unlikely(v &gt; t)) {</span>
<span class="p_add">+		s8 overstep = t &gt;&gt; 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		node_page_state_add(v + overstep, pgdat, item);</span>
<span class="p_add">+		__this_cpu_write(*p, -overstep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
 {
 	__inc_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__inc_zone_page_state);
 
<span class="p_add">+void __inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__inc_node_state(page_pgdat(page), item);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__inc_node_page_state);</span>
<span class="p_add">+</span>
 void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 {
 	struct per_cpu_pageset __percpu *pcp = zone-&gt;pageset;
<span class="p_chunk">@@ -299,12 +347,34 @@</span> <span class="p_context"> void __dec_zone_state(struct zone *zone, enum zone_stat_item item)</span>
 	}
 }
 
<span class="p_add">+void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	s8 v, t;</span>
<span class="p_add">+</span>
<span class="p_add">+	v = __this_cpu_dec_return(*p);</span>
<span class="p_add">+	t = __this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+	if (unlikely(v &lt; - t)) {</span>
<span class="p_add">+		s8 overstep = t &gt;&gt; 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		node_page_state_add(v - overstep, pgdat, item);</span>
<span class="p_add">+		__this_cpu_write(*p, overstep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
 {
 	__dec_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__dec_zone_page_state);
 
<span class="p_add">+void __dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dec_node_state(page_pgdat(page), item);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__dec_node_page_state);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HAVE_CMPXCHG_LOCAL
 /*
  * If we have cmpxchg_local support then we do not need to incur the overhead
<span class="p_chunk">@@ -318,8 +388,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(__dec_zone_page_state);</span>
  *     1       Overstepping half of threshold
  *     -1      Overstepping minus half of threshold
 */
<span class="p_del">-static inline void mod_state(struct zone *zone, enum zone_stat_item item,</span>
<span class="p_del">-			     long delta, int overstep_mode)</span>
<span class="p_add">+static inline void mod_zone_state(struct zone *zone,</span>
<span class="p_add">+       enum zone_stat_item item, long delta, int overstep_mode)</span>
 {
 	struct per_cpu_pageset __percpu *pcp = zone-&gt;pageset;
 	s8 __percpu *p = pcp-&gt;vm_stat_diff + item;
<span class="p_chunk">@@ -359,26 +429,88 @@</span> <span class="p_context"> static inline void mod_state(struct zone *zone, enum zone_stat_item item,</span>
 void mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 			 long delta)
 {
<span class="p_del">-	mod_state(zone, item, delta, 0);</span>
<span class="p_add">+	mod_zone_state(zone, item, delta, 0);</span>
 }
 EXPORT_SYMBOL(mod_zone_page_state);
 
 void inc_zone_state(struct zone *zone, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(zone, item, 1, 1);</span>
<span class="p_add">+	mod_zone_state(zone, item, 1, 1);</span>
 }
 
 void inc_zone_page_state(struct page *page, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(page_zone(page), item, 1, 1);</span>
<span class="p_add">+	mod_zone_state(page_zone(page), item, 1, 1);</span>
 }
 EXPORT_SYMBOL(inc_zone_page_state);
 
 void dec_zone_page_state(struct page *page, enum zone_stat_item item)
 {
<span class="p_del">-	mod_state(page_zone(page), item, -1, -1);</span>
<span class="p_add">+	mod_zone_state(page_zone(page), item, -1, -1);</span>
 }
 EXPORT_SYMBOL(dec_zone_page_state);
<span class="p_add">+</span>
<span class="p_add">+static inline void mod_node_state(struct pglist_data *pgdat,</span>
<span class="p_add">+       enum node_stat_item item, int delta, int overstep_mode)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct per_cpu_nodestat __percpu *pcp = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+	s8 __percpu *p = pcp-&gt;vm_node_stat_diff + item;</span>
<span class="p_add">+	long o, n, t, z;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		z = 0;  /* overflow to node counters */</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The fetching of the stat_threshold is racy. We may apply</span>
<span class="p_add">+		 * a counter threshold to the wrong the cpu if we get</span>
<span class="p_add">+		 * rescheduled while executing here. However, the next</span>
<span class="p_add">+		 * counter update will apply the threshold again and</span>
<span class="p_add">+		 * therefore bring the counter under the threshold again.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Most of the time the thresholds are the same anyways</span>
<span class="p_add">+		 * for all cpus in a node.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		t = this_cpu_read(pcp-&gt;stat_threshold);</span>
<span class="p_add">+</span>
<span class="p_add">+		o = this_cpu_read(*p);</span>
<span class="p_add">+		n = delta + o;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (n &gt; t || n &lt; -t) {</span>
<span class="p_add">+			int os = overstep_mode * (t &gt;&gt; 1) ;</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Overflow must be added to node counters */</span>
<span class="p_add">+			z = n + os;</span>
<span class="p_add">+			n = -os;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (this_cpu_cmpxchg(*p, o, n) != o);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (z)</span>
<span class="p_add">+		node_page_state_add(z, pgdat, item);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+					long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(pgdat, item, delta, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mod_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(pgdat, item, 1, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(page_pgdat(page), item, 1, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mod_node_state(page_pgdat(page), item, -1, -1);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dec_node_page_state);</span>
 #else
 /*
  * Use interrupt disable to serialize counter updates
<span class="p_chunk">@@ -424,21 +556,69 @@</span> <span class="p_context"> void dec_zone_page_state(struct page *page, enum zone_stat_item item)</span>
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(dec_zone_page_state);
<span class="p_del">-#endif</span>
 
<span class="p_add">+void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__inc_node_state(pgdat, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void mod_node_page_state(struct pglist_data *pgdat, enum node_stat_item item,</span>
<span class="p_add">+					long delta)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__mod_node_page_state(pgdat, item, delta);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mod_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void inc_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct pglist_data *pgdat;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdat = page_pgdat(page);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__inc_node_state(pgdat, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(inc_node_page_state);</span>
<span class="p_add">+</span>
<span class="p_add">+void dec_node_page_state(struct page *page, enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	__dec_node_page_state(page, item);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dec_node_page_state);</span>
<span class="p_add">+#endif</span>
 
 /*
  * Fold a differential into the global counters.
  * Returns the number of counters updated.
  */
<span class="p_del">-static int fold_diff(int *diff)</span>
<span class="p_add">+static int fold_diff(int *zone_diff, int *node_diff)</span>
 {
 	int i;
 	int changes = 0;
 
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++)
<span class="p_del">-		if (diff[i]) {</span>
<span class="p_del">-			atomic_long_add(diff[i], &amp;vm_stat[i]);</span>
<span class="p_add">+		if (zone_diff[i]) {</span>
<span class="p_add">+			atomic_long_add(zone_diff[i], &amp;vm_zone_stat[i]);</span>
<span class="p_add">+			changes++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		if (node_diff[i]) {</span>
<span class="p_add">+			atomic_long_add(node_diff[i], &amp;vm_node_stat[i]);</span>
 			changes++;
 	}
 	return changes;
<span class="p_chunk">@@ -462,9 +642,11 @@</span> <span class="p_context"> static int fold_diff(int *diff)</span>
  */
 static int refresh_cpu_vm_stats(bool do_pagesets)
 {
<span class="p_add">+	struct pglist_data *pgdat;</span>
 	struct zone *zone;
 	int i;
<span class="p_del">-	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };</span>
 	int changes = 0;
 
 	for_each_populated_zone(zone) {
<span class="p_chunk">@@ -477,7 +659,7 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
 			if (v) {
 
 				atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-				global_diff[i] += v;</span>
<span class="p_add">+				global_zone_diff[i] += v;</span>
 #ifdef CONFIG_NUMA
 				/* 3 seconds idle till flush */
 				__this_cpu_write(p-&gt;expire, 3);
<span class="p_chunk">@@ -516,7 +698,22 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
 		}
 #endif
 	}
<span class="p_del">-	changes += fold_diff(global_diff);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+		struct per_cpu_nodestat __percpu *p = pgdat-&gt;per_cpu_nodestats;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++) {</span>
<span class="p_add">+			int v;</span>
<span class="p_add">+</span>
<span class="p_add">+			v = this_cpu_xchg(p-&gt;vm_node_stat_diff[i], 0);</span>
<span class="p_add">+			if (v) {</span>
<span class="p_add">+				atomic_long_add(v, &amp;pgdat-&gt;vm_stat[i]);</span>
<span class="p_add">+				global_node_diff[i] += v;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	changes += fold_diff(global_zone_diff, global_node_diff);</span>
 	return changes;
 }
 
<span class="p_chunk">@@ -527,9 +724,11 @@</span> <span class="p_context"> static int refresh_cpu_vm_stats(bool do_pagesets)</span>
  */
 void cpu_vm_stats_fold(int cpu)
 {
<span class="p_add">+	struct pglist_data *pgdat;</span>
 	struct zone *zone;
 	int i;
<span class="p_del">-	int global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_zone_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };</span>
<span class="p_add">+	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };</span>
 
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *p;
<span class="p_chunk">@@ -543,11 +742,27 @@</span> <span class="p_context"> void cpu_vm_stats_fold(int cpu)</span>
 				v = p-&gt;vm_stat_diff[i];
 				p-&gt;vm_stat_diff[i] = 0;
 				atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-				global_diff[i] += v;</span>
<span class="p_add">+				global_zone_diff[i] += v;</span>
 			}
 	}
 
<span class="p_del">-	fold_diff(global_diff);</span>
<span class="p_add">+	for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+		struct per_cpu_nodestat *p;</span>
<span class="p_add">+</span>
<span class="p_add">+		p = per_cpu_ptr(pgdat-&gt;per_cpu_nodestats, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+			if (p-&gt;vm_node_stat_diff[i]) {</span>
<span class="p_add">+				int v;</span>
<span class="p_add">+</span>
<span class="p_add">+				v = p-&gt;vm_node_stat_diff[i];</span>
<span class="p_add">+				p-&gt;vm_node_stat_diff[i] = 0;</span>
<span class="p_add">+				atomic_long_add(v, &amp;pgdat-&gt;vm_stat[i]);</span>
<span class="p_add">+				global_node_diff[i] += v;</span>
<span class="p_add">+			}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fold_diff(global_zone_diff, global_node_diff);</span>
 }
 
 /*
<span class="p_chunk">@@ -563,16 +778,19 @@</span> <span class="p_context"> void drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset)</span>
 			int v = pset-&gt;vm_stat_diff[i];
 			pset-&gt;vm_stat_diff[i] = 0;
 			atomic_long_add(v, &amp;zone-&gt;vm_stat[i]);
<span class="p_del">-			atomic_long_add(v, &amp;vm_stat[i]);</span>
<span class="p_add">+			atomic_long_add(v, &amp;vm_zone_stat[i]);</span>
 		}
 }
 #endif
 
 #ifdef CONFIG_NUMA
 /*
<span class="p_del">- * Determine the per node value of a stat item.</span>
<span class="p_add">+ * Determine the per node value of a stat item. This function</span>
<span class="p_add">+ * is called frequently in a NUMA machine, so try to be as</span>
<span class="p_add">+ * frugal as possible.</span>
  */
<span class="p_del">-unsigned long node_page_state(int node, enum zone_stat_item item)</span>
<span class="p_add">+unsigned long sum_zone_node_page_state(int node,</span>
<span class="p_add">+				 enum zone_stat_item item)</span>
 {
 	struct zone *zones = NODE_DATA(node)-&gt;node_zones;
 	int i;
<span class="p_chunk">@@ -584,6 +802,19 @@</span> <span class="p_context"> unsigned long node_page_state(int node, enum zone_stat_item item)</span>
 	return count;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Determine the per node value of a stat item.</span>
<span class="p_add">+ */</span>
<span class="p_add">+unsigned long node_page_state(struct pglist_data *pgdat,</span>
<span class="p_add">+				enum node_stat_item item)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long x = atomic_long_read(&amp;pgdat-&gt;vm_stat[item]);</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	if (x &lt; 0)</span>
<span class="p_add">+		x = 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return x;</span>
<span class="p_add">+}</span>
 #endif
 
 #ifdef CONFIG_COMPACTION
<span class="p_chunk">@@ -1287,6 +1518,7 @@</span> <span class="p_context"> static void *vmstat_start(struct seq_file *m, loff_t *pos)</span>
 	if (*pos &gt;= ARRAY_SIZE(vmstat_text))
 		return NULL;
 	stat_items_size = NR_VM_ZONE_STAT_ITEMS * sizeof(unsigned long) +
<span class="p_add">+			  NR_VM_NODE_STAT_ITEMS * sizeof(unsigned long) +</span>
 			  NR_VM_WRITEBACK_STAT_ITEMS * sizeof(unsigned long);
 
 #ifdef CONFIG_VM_EVENT_COUNTERS
<span class="p_chunk">@@ -1301,6 +1533,10 @@</span> <span class="p_context"> static void *vmstat_start(struct seq_file *m, loff_t *pos)</span>
 		v[i] = global_page_state(i);
 	v += NR_VM_ZONE_STAT_ITEMS;
 
<span class="p_add">+	for (i = 0; i &lt; NR_VM_NODE_STAT_ITEMS; i++)</span>
<span class="p_add">+		v[i] = global_node_page_state(i);</span>
<span class="p_add">+	v += NR_VM_NODE_STAT_ITEMS;</span>
<span class="p_add">+</span>
 	global_dirty_limits(v + NR_DIRTY_BG_THRESHOLD,
 			    v + NR_DIRTY_THRESHOLD);
 	v += NR_VM_WRITEBACK_STAT_ITEMS;
<span class="p_chunk">@@ -1390,7 +1626,7 @@</span> <span class="p_context"> int vmstat_refresh(struct ctl_table *table, int write,</span>
 	if (err)
 		return err;
 	for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++) {
<span class="p_del">-		val = atomic_long_read(&amp;vm_stat[i]);</span>
<span class="p_add">+		val = atomic_long_read(&amp;vm_zone_stat[i]);</span>
 		if (val &lt; 0) {
 			switch (i) {
 			case NR_ALLOC_BATCH:
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index 8252de4566e9..ba972ac2dfdd 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -351,12 +351,13 @@</span> <span class="p_context"> static unsigned long count_shadow_nodes(struct shrinker *shrinker,</span>
 	shadow_nodes = list_lru_shrink_count(&amp;workingset_shadow_nodes, sc);
 	local_irq_enable();
 
<span class="p_del">-	if (memcg_kmem_enabled())</span>
<span class="p_add">+	if (memcg_kmem_enabled()) {</span>
 		pages = mem_cgroup_node_nr_lru_pages(sc-&gt;memcg, sc-&gt;nid,
 						     LRU_ALL_FILE);
<span class="p_del">-	else</span>
<span class="p_del">-		pages = node_page_state(sc-&gt;nid, NR_ACTIVE_FILE) +</span>
<span class="p_del">-			node_page_state(sc-&gt;nid, NR_INACTIVE_FILE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pages = sum_zone_node_page_state(sc-&gt;nid, NR_ACTIVE_FILE) +</span>
<span class="p_add">+			sum_zone_node_page_state(sc-&gt;nid, NR_INACTIVE_FILE);</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * Active cache pages are limited to 50% of memory, and shadow

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



