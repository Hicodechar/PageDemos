
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V5,1/3] mm/thp: Split out pmd collpase flush into a separate functions - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V5,1/3] mm/thp: Split out pmd collpase flush into a separate functions</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 15, 2015, 3:42 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1431704550-19937-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6415251/mbox/"
   >mbox</a>
|
   <a href="/patch/6415251/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6415251/">/patch/6415251/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 14376C0432
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 15:43:03 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id EDCB020396
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 15:43:01 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A348F2049E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 15:43:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1946173AbbEOPm5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 15 May 2015 11:42:57 -0400
Received: from e28smtp05.in.ibm.com ([122.248.162.5]:49612 &quot;EHLO
	e28smtp05.in.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1422945AbbEOPmt (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 15 May 2015 11:42:49 -0400
Received: from /spool/local
	by e28smtp05.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Fri, 15 May 2015 21:12:46 +0530
Received: from d28dlp03.in.ibm.com (9.184.220.128)
	by e28smtp05.in.ibm.com (192.168.1.135) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Fri, 15 May 2015 21:12:44 +0530
Received: from d28relay04.in.ibm.com (d28relay04.in.ibm.com [9.184.220.61])
	by d28dlp03.in.ibm.com (Postfix) with ESMTP id E9BC3125805F
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 15 May 2015 21:14:56 +0530 (IST)
Received: from d28av04.in.ibm.com (d28av04.in.ibm.com [9.184.220.66])
	by d28relay04.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	t4FFghPc39911602
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 15 May 2015 21:12:43 +0530
Received: from d28av04.in.ibm.com (localhost [127.0.0.1])
	by d28av04.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	t4FFggM6019029
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 15 May 2015 21:12:43 +0530
Received: from skywalker.in.ibm.com ([9.79.194.198])
	by d28av04.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	t4FFgfLG018987; Fri, 15 May 2015 21:12:42 +0530
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: akpm@linux-foundation.org, benh@kernel.crashing.org,
	paulus@samba.org, mpe@ellerman.id.au,
	kirill.shutemov@linux.intel.com, aarcange@redhat.com,
	schwidefsky@de.ibm.com
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linuxppc-dev@lists.ozlabs.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH V5 1/3] mm/thp: Split out pmd collpase flush into a separate
	functions
Date: Fri, 15 May 2015 21:12:28 +0530
Message-Id: &lt;1431704550-19937-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1431704550-19937-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
References: &lt;1431704550-19937-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15051515-0017-0000-0000-00000525A8B1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 15, 2015, 3:42 p.m.</div>
<pre class="content">
Architectures like ppc64 [1] need to do special things while clearing
pmd before a collapse. For them this operation is largely different
from a normal hugepage pte clear. Hence add a separate function
to clear pmd before collapse. After this patch pmdp_* functions
operate only on hugepage pte, and not on regular pmd_t values
pointing to page table.

[1] ppc64 needs to invalidate all the normal page pte mappings we
already have inserted in the hardware hash page table. But before
doing that we need to make sure there are no parallel hash page
table insert going on. So we need to do a kick_all_cpus_sync()
before flushing the older hash table entries. By moving this to
a separate function we capture these details and mention how it
is different from a hugepage pte clear.

This patch is a cleanup and only does code movement for clarity.
There should not be any change in functionality.
<span class="acked-by">
Acked-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++
 arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------
 include/asm-generic/pgtable.h            | 22 +++++++++
 mm/huge_memory.c                         |  2 +-
 4 files changed, 68 insertions(+), 36 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">index 43e6ad424c7f..129c67ebc81a 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_chunk">@@ -576,6 +576,10 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
 extern void pmdp_splitting_flush(struct vm_area_struct *vma,
 				 unsigned long address, pmd_t *pmdp);
 
<span class="p_add">+extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long address, pmd_t *pmdp);</span>
<span class="p_add">+#define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">index 59daa5eeec25..9171c1a37290 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_chunk">@@ -560,41 +560,47 @@</span> <span class="p_context"> pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	pmd_t pmd;
 
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	if (pmd_trans_huge(*pmdp)) {</span>
<span class="p_del">-		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * khugepaged calls this for normal pmd</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = *pmdp;</span>
<span class="p_del">-		pmd_clear(pmdp);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_del">-		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_del">-		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_del">-		 * the PTE entries. The assumption here is that any low level</span>
<span class="p_del">-		 * page fault will see a none pmd and take the slow path that</span>
<span class="p_del">-		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_del">-		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_del">-		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_del">-		 * That means we could be modifying the page content as we</span>
<span class="p_del">-		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_del">-		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_del">-		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_del">-		 * function there.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		kick_all_cpus_sync();</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Now invalidate the hpte entries in the range</span>
<span class="p_del">-		 * covered by pmd. This make sure we take a</span>
<span class="p_del">-		 * fault and will find the pmd as none, which will</span>
<span class="p_del">-		 * result in a major fault which takes mmap_sem and</span>
<span class="p_del">-		 * hence wait for collapse to complete. Without this</span>
<span class="p_del">-		 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_del">-		 * the old content.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="p_add">+	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_add">+	return pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pmd_t pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+			  pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = *pmdp;</span>
<span class="p_add">+	pmd_clear(pmdp);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_add">+	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_add">+	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_add">+	 * the PTE entries. The assumption here is that any low level</span>
<span class="p_add">+	 * page fault will see a none pmd and take the slow path that</span>
<span class="p_add">+	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_add">+	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_add">+	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_add">+	 * That means we could be modifying the page content as we</span>
<span class="p_add">+	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_add">+	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_add">+	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_add">+	 * function there.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kick_all_cpus_sync();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now invalidate the hpte entries in the range</span>
<span class="p_add">+	 * covered by pmd. This make sure we take a</span>
<span class="p_add">+	 * fault and will find the pmd as none, which will</span>
<span class="p_add">+	 * result in a major fault which takes mmap_sem and</span>
<span class="p_add">+	 * hence wait for collapse to complete. Without this</span>
<span class="p_add">+	 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_add">+	 * the old content.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
 	return pmd;
 }
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 39f1d6a2b04d..acdcaac77d93 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -189,6 +189,28 @@</span> <span class="p_context"> extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
 				 unsigned long address, pmd_t *pmdp);
 #endif
 
<span class="p_add">+#ifndef pmdp_collapse_flush</span>
<span class="p_add">+#define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address,</span>
<span class="p_add">+					pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address,</span>
<span class="p_add">+					pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return *pmdp;</span>
<span class="p_add">+}</span>
<span class="p_add">+#define pmdp_collapse_flush pmdp_collapse_flush</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 078832cf3636..88f695a4e38b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2499,7 +2499,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * huge and small TLB entries for the same virtual address
 	 * to avoid the risk of CPU bugs in that area.
 	 */
<span class="p_del">-	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="p_add">+	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>
 	spin_unlock(pmd_ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



