
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 3.2.90 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 3.2.90</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 3, 2017, 3:37 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1499096269.2707.14.camel@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9823643/mbox/"
   >mbox</a>
|
   <a href="/patch/9823643/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9823643/">/patch/9823643/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9AA3360246 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 814C7285EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7525C285F2; Mon,  3 Jul 2017 15:38:09 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-5.4 required=2.0 tests=BAYES_00, BODY_ENHANCEMENT2,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 70EC0285EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753459AbdGCPh5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 3 Jul 2017 11:37:57 -0400
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:50771 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752222AbdGCPhz (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 3 Jul 2017 11:37:55 -0400
Received: from 82-70-136-246.dsl.in-addr.zen.co.uk ([82.70.136.246]
	helo=deadeye) by shadbolt.decadent.org.uk with esmtps
	(TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256) (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dS3Q6-0006AK-DI; Mon, 03 Jul 2017 16:37:51 +0100
Received: from ben by deadeye with local (Exim 4.89)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dS3Q5-0004sv-LO; Mon, 03 Jul 2017 16:37:49 +0100
Message-ID: &lt;1499096269.2707.14.camel@decadent.org.uk&gt;
Subject: Linux 3.2.90
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, Jiri Slaby &lt;jslaby@suse.cz&gt;,
	stable@vger.kernel.org
Cc: lwn@lwn.net
Date: Mon, 03 Jul 2017 16:37:49 +0100
Content-Type: multipart/signed; micalg=&quot;pgp-sha512&quot;;
	protocol=&quot;application/pgp-signature&quot;;
	boundary=&quot;=-bJWgf+X2Eun5VLyatYM8&quot;
X-Mailer: Evolution 3.22.6-1 
Mime-Version: 1.0
X-SA-Exim-Connect-IP: 82.70.136.246
X-SA-Exim-Mail-From: ben@decadent.org.uk
X-SA-Exim-Scanned: No (on shadbolt.decadent.org.uk);
	SAEximRunCond expanded to false
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - July 3, 2017, 3:37 p.m.</div>
<pre class="content">
I&#39;m announcing the release of the 3.2.90 kernel.

All users of the 3.2 kernel series should upgrade.

The updated 3.2.y git tree can be found at:
        https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git linux-3.2.y
and can be browsed at the normal kernel.org git web browser:
        https://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git

The diff from 3.2.89 is attached to this message.

Ben.

------------

 Documentation/kernel-parameters.txt |   7 ++
 Makefile                            |   2 +-
 arch/alpha/kernel/osf_sys.c         |   2 +-
 arch/arm/mm/mmap.c                  |  12 ++-
 arch/frv/mm/elf-fdpic.c             |   6 +-
 arch/ia64/kernel/sys_ia64.c         |  18 +++-
 arch/ia64/mm/hugetlbpage.c          |   4 +-
 arch/mips/mm/mmap.c                 |  19 ++--
 arch/parisc/kernel/sys_parisc.c     |  40 ++++++--
 arch/powerpc/mm/slice.c             |  24 +++--
 arch/sh/mm/mmap.c                   |  29 +++---
 arch/sparc/kernel/sys_sparc_32.c    |   2 +-
 arch/sparc/kernel/sys_sparc_64.c    |  30 +++---
 arch/sparc/mm/hugetlbpage.c         |  27 +++---
 arch/tile/mm/hugetlbpage.c          |  26 ++---
 arch/x86/kernel/sys_x86_64.c        |  29 +++---
 arch/x86/mm/hugetlbpage.c           |  24 +++--
 fs/hugetlbfs/inode.c                |   4 +-
 fs/proc/task_mmu.c                  |   4 -
 include/linux/mm.h                  |  53 +++++-----
 mm/memory.c                         |  49 ----------
 mm/mmap.c                           | 189 ++++++++++++++++++++++--------------
 net/rxrpc/ar-key.c                  |  64 ++++++------
 23 files changed, 363 insertions(+), 301 deletions(-)

Ben Hutchings (1):
      Linux 3.2.90

David Howells (1):
      rxrpc: Fix several cases where a padded len isn&#39;t checked in ticket decode

Helge Deller (1):
      Allow stack to grow up to address space limit

Hugh Dickins (1):
      mm: larger stack guard gap, between vmas

KOSAKI Motohiro (1):
      mm: simplify find_vma_prev()

Linus Torvalds (1):
      mm: do not grow the stack vma just because of an overrun on preceding vma

Mikulas Patocka (1):
      mm: fix find_vma_prev
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index ac601c4b9f57..356bf4b4273b 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2457,6 +2457,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index df4aefae8171..efe5c9704afa 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 3
 PATCHLEVEL = 2
<span class="p_del">-SUBLEVEL = 89</span>
<span class="p_add">+SUBLEVEL = 90</span>
 EXTRAVERSION =
 NAME = Saber-toothed Squirrel
 
<span class="p_header">diff --git a/arch/alpha/kernel/osf_sys.c b/arch/alpha/kernel/osf_sys.c</span>
<span class="p_header">index 01e8715e26d9..b9abe5bb437b 100644</span>
<span class="p_header">--- a/arch/alpha/kernel/osf_sys.c</span>
<span class="p_header">+++ b/arch/alpha/kernel/osf_sys.c</span>
<span class="p_chunk">@@ -1147,7 +1147,7 @@</span> <span class="p_context"> arch_get_unmapped_area_1(unsigned long addr, unsigned long len,</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (limit - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
 		addr = vma-&gt;vm_end;
 		vma = vma-&gt;vm_next;
<span class="p_header">diff --git a/arch/arm/mm/mmap.c b/arch/arm/mm/mmap.c</span>
<span class="p_header">index 44b628e4d6ea..4497b5ef688e 100644</span>
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_align = 0;
 	int aliasing = cache_is_vipt_aliasing();
 
<span class="p_chunk">@@ -62,7 +62,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -96,15 +96,17 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 		if (do_align)
 			addr = COLOUR_ALIGN(addr, pgoff);
<span class="p_header">diff --git a/arch/frv/mm/elf-fdpic.c b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">index 385fd30b142f..96eca58b9679 100644</span>
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 			for (; vma; vma = vma-&gt;vm_next) {
 				if (addr &gt; limit)
 					break;
<span class="p_del">-				if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+				if (addr + len &lt;= vm_start_gap(vma))</span>
 					goto success;
 				addr = vma-&gt;vm_end;
 			}
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 		for (; vma; vma = vma-&gt;vm_next) {
 			if (addr &gt; limit)
 				break;
<span class="p_del">-			if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (addr + len &lt;= vm_start_gap(vma))</span>
 				goto success;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_header">diff --git a/arch/ia64/kernel/sys_ia64.c b/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_header">index 609d50056a6c..77c0aff5aaf0 100644</span>
<span class="p_header">--- a/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_chunk">@@ -27,7 +27,8 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *filp, unsigned long addr, unsigned long len</span>
 	long map_shared = (flags &amp; MAP_SHARED);
 	unsigned long start_addr, align_mask = PAGE_SIZE - 1;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	if (len &gt; RGN_MAP_LIMIT)
 		return -ENOMEM;
<span class="p_chunk">@@ -58,7 +59,17 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *filp, unsigned long addr, unsigned long len</span>
   full_search:
 	start_addr = addr = (addr + align_mask) &amp; ~align_mask;
 
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = (prev_end + align_mask) &amp; ~align_mask;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr || RGN_MAP_LIMIT - len &lt; REGION_OFFSET(addr)) {
 			if (start_addr != TASK_UNMAPPED_BASE) {
<span class="p_chunk">@@ -68,12 +79,11 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *filp, unsigned long addr, unsigned long len</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/* Remember the address where we stopped this search:  */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		addr = (vma-&gt;vm_end + align_mask) &amp; ~align_mask;</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/ia64/mm/hugetlbpage.c b/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_header">index 5ca674b74737..66a1ec0a2467 100644</span>
<span class="p_header">--- a/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -171,9 +171,9 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr, u</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (REGION_OFFSET(addr) + len &gt; RGN_MAP_LIMIT)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || (addr + len) &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || (addr + len) &lt;= vm_start_gap(vmm))</span>
 			return addr;
<span class="p_del">-		addr = ALIGN(vmm-&gt;vm_end, HPAGE_SIZE);</span>
<span class="p_add">+		addr = ALIGN(vm_end_gap(vmm), HPAGE_SIZE);</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/mips/mm/mmap.c b/arch/mips/mm/mmap.c</span>
<span class="p_header">index 302d779d5b0d..a79ddcfec1c7 100644</span>
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_color_align;
 
 	if (unlikely(len &gt; TASK_SIZE))
<span class="p_chunk">@@ -103,7 +104,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -118,7 +119,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 			/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 			if (TASK_SIZE - len &lt; addr)
 				return -ENOMEM;
<span class="p_del">-			if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 				return addr;
 			addr = vma-&gt;vm_end;
 			if (do_color_align)
<span class="p_chunk">@@ -145,7 +146,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 		/* make sure it can fit in the remaining address space */
 		if (likely(addr &gt; len)) {
 			vma = find_vma(mm, addr - len);
<span class="p_del">-			if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+			if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr - len;
 			}
<span class="p_chunk">@@ -165,20 +166,22 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 			 * return with success:
 			 */
 			vma = find_vma(mm, addr);
<span class="p_del">-			if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+			if (vma)</span>
<span class="p_add">+				vm_start = vm_start_gap(vma);</span>
<span class="p_add">+			if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr;
 			}
 
 			/* remember the largest hole we saw so far */
<span class="p_del">-			if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-				mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+			if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+				mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 			/* try just below the current vma-&gt;vm_start */
<span class="p_del">-			addr = vma-&gt;vm_start - len;</span>
<span class="p_add">+			addr = vm_start - len;</span>
 			if (do_color_align)
 				addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-		} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 		/*
<span class="p_header">diff --git a/arch/parisc/kernel/sys_parisc.c b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">index 7ea75d14aa65..1d4ac8d7cdd4 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_chunk">@@ -35,17 +35,27 @@</span> <span class="p_context"></span>
 
 static unsigned long get_unshared_area(unsigned long addr, unsigned long len)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	addr = PAGE_ALIGN(addr);
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
 	}
 }
 
<span class="p_chunk">@@ -70,22 +80,32 @@</span> <span class="p_context"> static int get_offset(struct address_space *mapping)</span>
 static unsigned long get_shared_area(struct address_space *mapping,
 		unsigned long addr, unsigned long len, unsigned long pgoff)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 	int offset = mapping ? get_offset(mapping) : 0;
 
 	offset = (offset + (pgoff &lt;&lt; PAGE_SHIFT)) &amp; 0x3FF000;
 
 	addr = DCACHE_ALIGN(addr - offset) + offset;
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = DCACHE_ALIGN(prev_end - offset) + offset;</span>
<span class="p_add">+				if (addr &lt; prev_end)	/* handle wraparound */</span>
<span class="p_add">+					return -ENOMEM;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = DCACHE_ALIGN(vma-&gt;vm_end - offset) + offset;</span>
<span class="p_del">-		if (addr &lt; vma-&gt;vm_end) /* handle wraparound */</span>
<span class="p_del">-			return -ENOMEM;</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/powerpc/mm/slice.c b/arch/powerpc/mm/slice.c</span>
<span class="p_header">index 73709f7ce92c..57654c9b70f8 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_struct *mm, unsigned long addr,</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_chunk">@@ -227,7 +227,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_bottomup(struct mm_struct *mm,</span>
 					      int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr, addr;</span>
<span class="p_add">+	unsigned long start_addr, addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -256,7 +256,9 @@</span> <span class="p_context"> static unsigned long slice_find_area_bottomup(struct mm_struct *mm,</span>
 				addr = _ALIGN_UP(addr + 1,  1ul &lt;&lt; SLICE_HIGH_SHIFT);
 			continue;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
<span class="p_chunk">@@ -264,8 +266,8 @@</span> <span class="p_context"> static unsigned long slice_find_area_bottomup(struct mm_struct *mm,</span>
 				mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 	}
 
<span class="p_chunk">@@ -284,7 +286,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_topdown(struct mm_struct *mm,</span>
 					     int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long addr;</span>
<span class="p_add">+	unsigned long addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -336,7 +338,9 @@</span> <span class="p_context"> static unsigned long slice_find_area_topdown(struct mm_struct *mm,</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || (addr + len) &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || (addr + len) &lt;= vm_start) {</span>
 			/* remember the address as a hint for next time */
 			if (use_cache)
 				mm-&gt;free_area_cache = addr;
<span class="p_chunk">@@ -344,11 +348,11 @@</span> <span class="p_context"> static unsigned long slice_find_area_topdown(struct mm_struct *mm,</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start;</span>
<span class="p_add">+		addr = vm_start;</span>
 	}
 
 	/*
<span class="p_header">diff --git a/arch/sh/mm/mmap.c b/arch/sh/mm/mmap.c</span>
<span class="p_header">index afeb710ec5c3..22eff46d8ef5 100644</span>
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -106,15 +106,17 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_colour_align)
<span class="p_chunk">@@ -130,6 +132,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -158,7 +161,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -179,7 +182,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -199,20 +202,22 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start-len;</span>
 		if (do_colour_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_header">diff --git a/arch/sparc/kernel/sys_sparc_32.c b/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_header">index 42b282fa6112..eeae89bada55 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 		}
 		if (TASK_SIZE - PAGE_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">diff --git a/arch/sparc/kernel/sys_sparc_64.c b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">index a062fe9a4e49..39f49991575e 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_color_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,15 +181,17 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_color_align)
<span class="p_chunk">@@ -205,7 +207,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long task_size = STACK_TOP32;
<span class="p_del">-	unsigned long addr = addr0;</span>
<span class="p_add">+	unsigned long addr = addr0, vm_start;</span>
 	int do_color_align;
 
 	/* This should only ever run for 32-bit processes.  */
<span class="p_chunk">@@ -237,7 +239,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -258,7 +260,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -278,20 +280,22 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+ 		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+ 		        mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start - len;</span>
 		if (do_color_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index 07e14535375c..e13e85dbfcd9 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *filp,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (test_thread_flag(TIF_32BIT))
 		task_size = STACK_TOP32;
<span class="p_chunk">@@ -67,15 +67,17 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *filp,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = ALIGN(vma-&gt;vm_end, HPAGE_SIZE);
 	}
<span class="p_chunk">@@ -90,6 +92,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* This should only ever run for 32-bit processes.  */
 	BUG_ON(!test_thread_flag(TIF_32BIT));
<span class="p_chunk">@@ -106,7 +109,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -124,18 +127,20 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start-len) &amp; HPAGE_MASK;</span>
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		addr = (vm_start - len) &amp; HPAGE_MASK;</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -182,7 +187,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/tile/mm/hugetlbpage.c b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">index 42cfcba4e1ef..184e0339c056 100644</span>
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -159,7 +159,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 		start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -185,12 +185,14 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -204,6 +206,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -234,9 +237,10 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 
 		/*
 		 * new region fits between prev_vma-&gt;vm_end and
<span class="p_del">-		 * vma-&gt;vm_start, use it:</span>
<span class="p_add">+		 * vm_start, use it:</span>
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 			    (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 			mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -251,13 +255,13 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
 
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -312,7 +316,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index cdb2fc9235b7..0dbfff8e1a58 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -126,7 +126,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	unsigned long begin, end;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (((flags &amp; MAP_32BIT) || test_thread_flag(TIF_IA32))
<span class="p_chunk">@@ -172,15 +172,17 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		addr = align_addr(addr, filp, 0);
<span class="p_chunk">@@ -196,6 +198,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
<span class="p_chunk">@@ -213,7 +216,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -232,7 +235,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 						    ALIGN_TOPDOWN);
 
 		vma = find_vma(mm, tmp_addr);
<span class="p_del">-		if (!vma || tmp_addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || tmp_addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = tmp_addr;
 	}
<span class="p_chunk">@@ -251,17 +254,19 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start)</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = addr;
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (len &lt; vma-&gt;vm_start);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (len &lt; vm_start);</span>
 
 bottomup:
 	/*
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index df7d12c9af24..67b87605e223 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -277,7 +277,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 	        start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -303,12 +303,14 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -322,6 +324,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -351,7 +354,8 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		 * new region fits between prev_vma-&gt;vm_end and
 		 * vma-&gt;vm_start, use it:
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 		            (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 		        mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -365,12 +369,12 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -426,7 +430,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 55573322d1bb..99c51d6cb925 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -176,7 +176,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 			return -ENOMEM;
 		}
 
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index de404f204b6a..6037a132fc7f 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -230,11 +230,7 @@</span> <span class="p_context"> static void show_map_vma(struct seq_file *m, struct vm_area_struct *vma)</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n&quot;,
 			start,
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 16394da01d2d..19f9043e3692 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1015,34 +1015,6 @@</span> <span class="p_context"> int set_page_dirty(struct page *page);</span>
 int set_page_dirty_lock(struct page *page);
 int clear_page_dirty_for_io(struct page *page);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len);
<span class="p_chunk">@@ -1462,6 +1434,7 @@</span> <span class="p_context"> unsigned long ra_submit(struct file_ra_state *ra,</span>
 			struct address_space *mapping,
 			struct file *filp);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1490,6 +1463,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * m</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 2917e9b2e4d4..6325103db6f3 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1605,12 +1605,6 @@</span> <span class="p_context"> struct page *follow_page(struct vm_area_struct *vma, unsigned long address,</span>
 	return page;
 }
 
<span class="p_del">-static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return stack_guard_page_start(vma, addr) ||</span>
<span class="p_del">-	       stack_guard_page_end(vma, addr+PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * __get_user_pages() - pin user pages in memory
  * @tsk:	task_struct of target task
<span class="p_chunk">@@ -1761,11 +1755,6 @@</span> <span class="p_context"> int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 				int ret;
 				unsigned int fault_flags = 0;
 
<span class="p_del">-				/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-				if (foll_flags &amp; FOLL_MLOCK) {</span>
<span class="p_del">-					if (stack_guard_page(vma, start))</span>
<span class="p_del">-						goto next_page;</span>
<span class="p_del">-				}</span>
 				if (foll_flags &amp; FOLL_WRITE)
 					fault_flags |= FAULT_FLAG_WRITE;
 				if (nonblocking)
<span class="p_chunk">@@ -3122,40 +3111,6 @@</span> <span class="p_context"> static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -3174,10 +3129,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 94f4e3444ae5..c7cbb405d5b3 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -245,6 +245,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -289,7 +290,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -1368,8 +1370,8 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long start_addr, vm_start, prev_end;</span>
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
 		return -ENOMEM;
<span class="p_chunk">@@ -1379,9 +1381,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -1392,7 +1395,17 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	}
 
 full_search:
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr) {
 			/*
<span class="p_chunk">@@ -1407,16 +1420,16 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : TASK_SIZE;</span>
<span class="p_add">+		if (addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 	}
 }
 #endif	
<span class="p_chunk">@@ -1442,9 +1455,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start, prev_end;</span>
 	unsigned long low_limit = max(PAGE_SIZE, mmap_min_addr);
 
 	/* requested length too big for entire address space */
<span class="p_chunk">@@ -1457,9 +1471,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1474,8 +1489,9 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 	/* make sure it can fit in the remaining address space */
 	if (addr &gt;= low_limit + len) {
<span class="p_del">-		vma = find_vma(mm, addr-len);</span>
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr-len, &amp;prev);</span>
<span class="p_add">+		if ((!vma || addr &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr-len &gt;= vm_end_gap(prev)))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 	}
<span class="p_chunk">@@ -1491,18 +1507,21 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		 * else if new region fits below vma-&gt;vm_start,
 		 * return with success:
 		 */
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : mm-&gt;mmap_base;</span>
<span class="p_add">+		prev_end = prev ? vm_end_gap(prev) : low_limit;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp; addr &gt;= prev_end)</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (vma-&gt;vm_start &gt;= low_limit + len);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (vm_start &gt;= low_limit + len);</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -1607,39 +1626,27 @@</span> <span class="p_context"> struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)</span>
 
 EXPORT_SYMBOL(find_vma);
 
<span class="p_del">-/* Same as find_vma, but also return a pointer to the previous VMA in *pprev. */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Same as find_vma, but also return a pointer to the previous VMA in *pprev.</span>
<span class="p_add">+ */</span>
 struct vm_area_struct *
 find_vma_prev(struct mm_struct *mm, unsigned long addr,
 			struct vm_area_struct **pprev)
 {
<span class="p_del">-	struct vm_area_struct *vma = NULL, *prev = NULL;</span>
<span class="p_del">-	struct rb_node *rb_node;</span>
<span class="p_del">-	if (!mm)</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Guard against addr being lower than the first VMA */</span>
<span class="p_del">-	vma = mm-&gt;mmap;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Go through the RB tree quickly. */</span>
<span class="p_del">-	rb_node = mm-&gt;mm_rb.rb_node;</span>
<span class="p_del">-</span>
<span class="p_del">-	while (rb_node) {</span>
<span class="p_del">-		struct vm_area_struct *vma_tmp;</span>
<span class="p_del">-		vma_tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
 
<span class="p_del">-		if (addr &lt; vma_tmp-&gt;vm_end) {</span>
<span class="p_del">-			rb_node = rb_node-&gt;rb_left;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			prev = vma_tmp;</span>
<span class="p_del">-			if (!prev-&gt;vm_next || (addr &lt; prev-&gt;vm_next-&gt;vm_end))</span>
<span class="p_del">-				break;</span>
<span class="p_add">+	vma = find_vma(mm, addr);</span>
<span class="p_add">+	if (vma) {</span>
<span class="p_add">+		*pprev = vma-&gt;vm_prev;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		struct rb_node *rb_node = mm-&gt;mm_rb.rb_node;</span>
<span class="p_add">+		*pprev = NULL;</span>
<span class="p_add">+		while (rb_node) {</span>
<span class="p_add">+			*pprev = rb_entry(rb_node, struct vm_area_struct, vm_rb);</span>
 			rb_node = rb_node-&gt;rb_right;
 		}
 	}
<span class="p_del">-</span>
<span class="p_del">-out:</span>
<span class="p_del">-	*pprev = prev;</span>
<span class="p_del">-	return prev ? prev-&gt;vm_next : vma;</span>
<span class="p_add">+	return vma;</span>
 }
 
 /*
<span class="p_chunk">@@ -1647,21 +1654,19 @@</span> <span class="p_context"> find_vma_prev(struct mm_struct *mm, unsigned long addr,</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -1703,32 +1708,43 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, uns</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	int error;</span>
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
<span class="p_add">+	int error = 0;</span>
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	/* Guard against exceeding limits of the address space. */</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (address &gt;= TASK_SIZE)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Guard against overflow */</span>
<span class="p_add">+	if (gap_addr &lt; address || gap_addr &gt; TASK_SIZE)</span>
<span class="p_add">+		gap_addr = TASK_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
<span class="p_del">-	 * Also guard against wrapping around to address 0.</span>
 	 */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else {</span>
<span class="p_del">-		vma_unlock_anon_vma(vma);</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	error = 0;</span>
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &gt; vma-&gt;vm_end) {
<span class="p_chunk">@@ -1758,27 +1774,36 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	address &amp;= PAGE_MASK;
 	error = security_file_mmap(NULL, 0, 0, 0, address, 1);
 	if (error)
 		return error;
 
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &lt; vma-&gt;vm_start) {
<span class="p_chunk">@@ -1802,6 +1827,22 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 	return error;
 }
 
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_header">diff --git a/net/rxrpc/ar-key.c b/net/rxrpc/ar-key.c</span>
<span class="p_header">index 43ea7de2fc8e..9259f966b2a0 100644</span>
<span class="p_header">--- a/net/rxrpc/ar-key.c</span>
<span class="p_header">+++ b/net/rxrpc/ar-key.c</span>
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 				       unsigned *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned toklen = *_toklen, n_parts, loop, tmp;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, n_parts, loop, tmp, paddedlen;</span>
 
 	/* there must be at least one name, and at least #names+1 length
 	 * words */
<span class="p_chunk">@@ -243,16 +243,16 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 		toklen -= 4;
 		if (tmp &lt;= 0 || tmp &gt; AFSTOKEN_STRING_MAX)
 			return -EINVAL;
<span class="p_del">-		if (tmp &gt; toklen)</span>
<span class="p_add">+		paddedlen = (tmp + 3) &amp; ~3;</span>
<span class="p_add">+		if (paddedlen &gt; toklen)</span>
 			return -EINVAL;
 		princ-&gt;name_parts[loop] = kmalloc(tmp + 1, GFP_KERNEL);
 		if (!princ-&gt;name_parts[loop])
 			return -ENOMEM;
 		memcpy(princ-&gt;name_parts[loop], xdr, tmp);
 		princ-&gt;name_parts[loop][tmp] = 0;
<span class="p_del">-		tmp = (tmp + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= tmp;</span>
<span class="p_del">-		xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	if (toklen &lt; 4)
<span class="p_chunk">@@ -261,16 +261,16 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 	toklen -= 4;
 	if (tmp &lt;= 0 || tmp &gt; AFSTOKEN_K5_REALM_MAX)
 		return -EINVAL;
<span class="p_del">-	if (tmp &gt; toklen)</span>
<span class="p_add">+	paddedlen = (tmp + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
 		return -EINVAL;
 	princ-&gt;realm = kmalloc(tmp + 1, GFP_KERNEL);
 	if (!princ-&gt;realm)
 		return -ENOMEM;
 	memcpy(princ-&gt;realm, xdr, tmp);
 	princ-&gt;realm[tmp] = 0;
<span class="p_del">-	tmp = (tmp + 3) &amp; ~3;</span>
<span class="p_del">-	toklen -= tmp;</span>
<span class="p_del">-	xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+	toklen -= paddedlen;</span>
<span class="p_add">+	xdr += paddedlen &gt;&gt; 2;</span>
 
 	_debug(&quot;%s/...@%s&quot;, princ-&gt;name_parts[0], princ-&gt;realm);
 
<span class="p_chunk">@@ -289,7 +289,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_tagged_data(struct krb5_tagged_data *td,</span>
 					 unsigned *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned toklen = *_toklen, len;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, len, paddedlen;</span>
 
 	/* there must be at least one tag and one length word */
 	if (toklen &lt;= 8)
<span class="p_chunk">@@ -303,6 +303,9 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_tagged_data(struct krb5_tagged_data *td,</span>
 	toklen -= 8;
 	if (len &gt; max_data_size)
 		return -EINVAL;
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
<span class="p_add">+		return -EINVAL;</span>
 	td-&gt;data_len = len;
 
 	if (len &gt; 0) {
<span class="p_chunk">@@ -310,9 +313,8 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_tagged_data(struct krb5_tagged_data *td,</span>
 		if (!td-&gt;data)
 			return -ENOMEM;
 		memcpy(td-&gt;data, xdr, len);
<span class="p_del">-		len = (len + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= len;</span>
<span class="p_del">-		xdr += len &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	_debug(&quot;tag %x len %x&quot;, td-&gt;tag, td-&gt;data_len);
<span class="p_chunk">@@ -384,7 +386,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 				    const __be32 **_xdr, unsigned *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned toklen = *_toklen, len;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, len, paddedlen;</span>
 
 	/* there must be at least one length word */
 	if (toklen &lt;= 4)
<span class="p_chunk">@@ -396,6 +398,9 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 	toklen -= 4;
 	if (len &gt; AFSTOKEN_K5_TIX_MAX)
 		return -EINVAL;
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
<span class="p_add">+		return -EINVAL;</span>
 	*_tktlen = len;
 
 	_debug(&quot;ticket len %u&quot;, len);
<span class="p_chunk">@@ -405,9 +410,8 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 		if (!*_ticket)
 			return -ENOMEM;
 		memcpy(*_ticket, xdr, len);
<span class="p_del">-		len = (len + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= len;</span>
<span class="p_del">-		xdr += len &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	*_xdr = xdr;
<span class="p_chunk">@@ -551,7 +555,7 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 {
 	const __be32 *xdr = data, *token;
 	const char *cp;
<span class="p_del">-	unsigned len, tmp, loop, ntoken, toklen, sec_ix;</span>
<span class="p_add">+	unsigned int len, paddedlen, loop, ntoken, toklen, sec_ix;</span>
 	int ret;
 
 	_enter(&quot;,{%x,%x,%x,%x},%zu&quot;,
<span class="p_chunk">@@ -576,22 +580,21 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 	if (len &lt; 1 || len &gt; AFSTOKEN_CELL_MAX)
 		goto not_xdr;
 	datalen -= 4;
<span class="p_del">-	tmp = (len + 3) &amp; ~3;</span>
<span class="p_del">-	if (tmp &gt; datalen)</span>
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; datalen)</span>
 		goto not_xdr;
 
 	cp = (const char *) xdr;
 	for (loop = 0; loop &lt; len; loop++)
 		if (!isprint(cp[loop]))
 			goto not_xdr;
<span class="p_del">-	if (len &lt; tmp)</span>
<span class="p_del">-		for (; loop &lt; tmp; loop++)</span>
<span class="p_del">-			if (cp[loop])</span>
<span class="p_del">-				goto not_xdr;</span>
<span class="p_add">+	for (; loop &lt; paddedlen; loop++)</span>
<span class="p_add">+		if (cp[loop])</span>
<span class="p_add">+			goto not_xdr;</span>
 	_debug(&quot;cellname: [%u/%u] &#39;%*.*s&#39;&quot;,
<span class="p_del">-	       len, tmp, len, len, (const char *) xdr);</span>
<span class="p_del">-	datalen -= tmp;</span>
<span class="p_del">-	xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+	       len, paddedlen, len, len, (const char *) xdr);</span>
<span class="p_add">+	datalen -= paddedlen;</span>
<span class="p_add">+	xdr += paddedlen &gt;&gt; 2;</span>
 
 	/* get the token count */
 	if (datalen &lt; 12)
<span class="p_chunk">@@ -612,10 +615,11 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 		sec_ix = ntohl(*xdr);
 		datalen -= 4;
 		_debug(&quot;token: [%x/%zx] %x&quot;, toklen, datalen, sec_ix);
<span class="p_del">-		if (toklen &lt; 20 || toklen &gt; datalen)</span>
<span class="p_add">+		paddedlen = (toklen + 3) &amp; ~3;</span>
<span class="p_add">+		if (toklen &lt; 20 || toklen &gt; datalen || paddedlen &gt; datalen)</span>
 			goto not_xdr;
<span class="p_del">-		datalen -= (toklen + 3) &amp; ~3;</span>
<span class="p_del">-		xdr += (toklen + 3) &gt;&gt; 2;</span>
<span class="p_add">+		datalen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 
 	} while (--loop &gt; 0);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



