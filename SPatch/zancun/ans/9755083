
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/6] mm: vmstat: move slab statistics from zone to node counters - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/6] mm: vmstat: move slab statistics from zone to node counters</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 30, 2017, 6:17 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170530181724.27197-3-hannes@cmpxchg.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9755083/mbox/"
   >mbox</a>
|
   <a href="/patch/9755083/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9755083/">/patch/9755083/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	62A8A601D2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 30 May 2017 18:17:54 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 52421267EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 30 May 2017 18:17:54 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 46B4A2785D; Tue, 30 May 2017 18:17:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AFAE1267EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 30 May 2017 18:17:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751333AbdE3SRr (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 30 May 2017 14:17:47 -0400
Received: from gum.cmpxchg.org ([85.214.110.215]:42392 &quot;EHLO gum.cmpxchg.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751268AbdE3SRq (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 30 May 2017 14:17:46 -0400
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=cmpxchg.org ; s=x;
	h=References:In-Reply-To:Message-Id:Date:Subject:Cc:To:From:Sender:
	Reply-To:MIME-Version:Content-Type:Content-Transfer-Encoding:Content-ID:
	Content-Description:Resent-Date:Resent-From:Resent-Sender:Resent-To:Resent-Cc
	:Resent-Message-ID:List-Id:List-Help:List-Unsubscribe:List-Subscribe:
	List-Post:List-Owner:List-Archive;
	bh=DQsEq+jj+pxI5Wch3dMWnebcUCctTbWXYrPoDOtiOik=;
	b=hk4dOxJdaRodzNFBiGZ0Z8RDh+
	lkOYt1iQ64juw3sLjaYEspdrnRPaZf15gPMRYQfuNJIEbtmUzF8UBO4TUc6/WrYj192vt4faMaqB3
	URguKb69mCbiZfpJsJEMgtNNnsnwJ8Padxf0qLoxSrY5/2VacfgvxGYgCn6XHfkgduko=;
From: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
To: Josef Bacik &lt;josef@toxicpanda.com&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;,
	Vladimir Davydov &lt;vdavydov.dev@gmail.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, linux-mm@kvack.org,
	cgroups@vger.kernel.org, linux-kernel@vger.kernel.org, kernel-team@fb.com
Subject: [PATCH 2/6] mm: vmstat: move slab statistics from zone to node
	counters
Date: Tue, 30 May 2017 14:17:20 -0400
Message-Id: &lt;20170530181724.27197-3-hannes@cmpxchg.org&gt;
X-Mailer: git-send-email 2.12.2
In-Reply-To: &lt;20170530181724.27197-1-hannes@cmpxchg.org&gt;
References: &lt;20170530181724.27197-1-hannes@cmpxchg.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - May 30, 2017, 6:17 p.m.</div>
<pre class="content">
To re-implement slab cache vs. page cache balancing, we&#39;ll need the
slab counters at the lruvec level, which, ever since lru reclaim was
moved from the zone to the node, is the intersection of the node, not
the zone, and the memcg.

We could retain the per-zone counters for when the page allocator
dumps its memory information on failures, and have counters on both
levels - which on all but NUMA node 0 is usually redundant. But let&#39;s
keep it simple for now and just move them. If anybody complains we can
restore the per-zone counters.
<span class="signed-off-by">
Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
---
 drivers/base/node.c    | 10 +++++-----
 include/linux/mmzone.h |  4 ++--
 mm/page_alloc.c        |  4 ----
 mm/slab.c              |  8 ++++----
 mm/slub.c              |  4 ++--
 mm/vmscan.c            |  2 +-
 mm/vmstat.c            |  4 ++--
 7 files changed, 16 insertions(+), 20 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=236">Heiko Carstens</a> - May 31, 2017, 9:12 a.m.</div>
<pre class="content">
On Tue, May 30, 2017 at 02:17:20PM -0400, Johannes Weiner wrote:
<span class="quote">&gt; To re-implement slab cache vs. page cache balancing, we&#39;ll need the</span>
<span class="quote">&gt; slab counters at the lruvec level, which, ever since lru reclaim was</span>
<span class="quote">&gt; moved from the zone to the node, is the intersection of the node, not</span>
<span class="quote">&gt; the zone, and the memcg.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We could retain the per-zone counters for when the page allocator</span>
<span class="quote">&gt; dumps its memory information on failures, and have counters on both</span>
<span class="quote">&gt; levels - which on all but NUMA node 0 is usually redundant. But let&#39;s</span>
<span class="quote">&gt; keep it simple for now and just move them. If anybody complains we can</span>
<span class="quote">&gt; restore the per-zone counters.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>

This patch causes an early boot crash on s390 (linux-next as of today).
CONFIG_NUMA on/off doesn&#39;t make any difference. I haven&#39;t looked any
further into this yet, maybe you have an idea?

Kernel BUG at 00000000002b0362 [verbose debug info unavailable]
addressing exception: 0005 ilc:3 [#1] SMP
Modules linked in:
CPU: 0 PID: 0 Comm: swapper Not tainted 4.12.0-rc3-00153-gb6bc6724488a #16
Hardware name: IBM 2964 N96 702 (z/VM 6.4.0)
task: 0000000000d75d00 task.stack: 0000000000d60000
Krnl PSW : 0404200180000000 00000000002b0362 (mod_node_page_state+0x62/0x158)
           R:0 T:1 IO:0 EX:0 Key:0 M:1 W:0 P:0 AS:0 CC:2 PM:0 RI:0 EA:3
Krnl GPRS: 0000000000000001 000000003d81f000 0000000000000000 0000000000000006
           0000000000000001 0000000000f29b52 0000000000000041 0000000000000000
           0000000000000007 0000000000000040 000000003fe81000 000003d100ffa000
           0000000000ee1cd0 0000000000979040 0000000000300abc 0000000000d63c90
Krnl Code: 00000000002b0350: e31003900004 lg %r1,912
           00000000002b0356: e320f0a80004 lg %r2,168(%r15)
          #00000000002b035c: e31120000090 llgc %r1,0(%r1,%r2)
<span class="quote">          &gt;00000000002b0362: b9060011  lgbr %r1,%r1</span>
           00000000002b0366: e32003900004 lg %r2,912
           00000000002b036c: e3c280000090 llgc %r12,0(%r2,%r8)
           00000000002b0372: b90600ac  lgbr %r10,%r12
           00000000002b0376: b904002a  lgr %r2,%r10
Call Trace:
([&lt;0000000000000000&gt;]           (null))
 [&lt;0000000000300abc&gt;] new_slab+0x35c/0x628
 [&lt;000000000030740c&gt;] __kmem_cache_create+0x33c/0x638
 [&lt;0000000000e99c0e&gt;] create_boot_cache+0xae/0xe0
 [&lt;0000000000e9e12c&gt;] kmem_cache_init+0x5c/0x138
 [&lt;0000000000e7999c&gt;] start_kernel+0x24c/0x440
 [&lt;0000000000100020&gt;] _stext+0x20/0x80
Last Breaking-Event-Address:
 [&lt;0000000000300ab6&gt;] new_slab+0x356/0x628

Kernel panic - not syncing: Fatal exception: panic_on_oops
<span class="quote">
&gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt; index 5548f9686016..e57e06e6df4c 100644</span>
<span class="quote">&gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt; @@ -129,11 +129,11 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt;  		       nid, K(node_page_state(pgdat, NR_UNSTABLE_NFS)),</span>
<span class="quote">&gt;  		       nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(pgdat, NR_WRITEBACK_TEMP)),</span>
<span class="quote">&gt; -		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="quote">&gt; -				sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="quote">&gt; -		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
<span class="quote">&gt; +		       nid, K(node_page_state(pgdat, NR_SLAB_RECLAIMABLE) +</span>
<span class="quote">&gt; +			      node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="quote">&gt; +		       nid, K(node_page_state(pgdat, NR_SLAB_RECLAIMABLE)),</span>
<span class="quote">&gt;  #ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; -		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="quote">&gt; +		       nid, K(node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(pgdat, NR_ANON_THPS) *</span>
<span class="quote">&gt;  				       HPAGE_PMD_NR),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(pgdat, NR_SHMEM_THPS) *</span>
<span class="quote">&gt; @@ -141,7 +141,7 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt;  		       nid, K(node_page_state(pgdat, NR_SHMEM_PMDMAPPED) *</span>
<span class="quote">&gt;  				       HPAGE_PMD_NR));</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; -		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
<span class="quote">&gt; +		       nid, K(node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)));</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	n += hugetlb_report_node_meminfo(nid, buf + n);</span>
<span class="quote">&gt;  	return n;</span>
<span class="quote">&gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="quote">&gt; index ebaccd4e7d8c..eacadee83964 100644</span>
<span class="quote">&gt; --- a/include/linux/mmzone.h</span>
<span class="quote">&gt; +++ b/include/linux/mmzone.h</span>
<span class="quote">&gt; @@ -125,8 +125,6 @@ enum zone_stat_item {</span>
<span class="quote">&gt;  	NR_ZONE_UNEVICTABLE,</span>
<span class="quote">&gt;  	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */</span>
<span class="quote">&gt;  	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */</span>
<span class="quote">&gt; -	NR_SLAB_RECLAIMABLE,</span>
<span class="quote">&gt; -	NR_SLAB_UNRECLAIMABLE,</span>
<span class="quote">&gt;  	NR_PAGETABLE,		/* used for pagetables */</span>
<span class="quote">&gt;  	NR_KERNEL_STACK_KB,	/* measured in KiB */</span>
<span class="quote">&gt;  	/* Second 128 byte cacheline */</span>
<span class="quote">&gt; @@ -152,6 +150,8 @@ enum node_stat_item {</span>
<span class="quote">&gt;  	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; +	NR_SLAB_RECLAIMABLE,</span>
<span class="quote">&gt; +	NR_SLAB_UNRECLAIMABLE,</span>
<span class="quote">&gt;  	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */</span>
<span class="quote">&gt;  	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */</span>
<span class="quote">&gt;  	WORKINGSET_REFAULT,</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index f9e450c6b6e4..5f89cfaddc4b 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -4601,8 +4601,6 @@ void show_free_areas(unsigned int filter, nodemask_t *nodemask)</span>
<span class="quote">&gt;  			&quot; present:%lukB&quot;</span>
<span class="quote">&gt;  			&quot; managed:%lukB&quot;</span>
<span class="quote">&gt;  			&quot; mlocked:%lukB&quot;</span>
<span class="quote">&gt; -			&quot; slab_reclaimable:%lukB&quot;</span>
<span class="quote">&gt; -			&quot; slab_unreclaimable:%lukB&quot;</span>
<span class="quote">&gt;  			&quot; kernel_stack:%lukB&quot;</span>
<span class="quote">&gt;  			&quot; pagetables:%lukB&quot;</span>
<span class="quote">&gt;  			&quot; bounce:%lukB&quot;</span>
<span class="quote">&gt; @@ -4624,8 +4622,6 @@ void show_free_areas(unsigned int filter, nodemask_t *nodemask)</span>
<span class="quote">&gt;  			K(zone-&gt;present_pages),</span>
<span class="quote">&gt;  			K(zone-&gt;managed_pages),</span>
<span class="quote">&gt;  			K(zone_page_state(zone, NR_MLOCK)),</span>
<span class="quote">&gt; -			K(zone_page_state(zone, NR_SLAB_RECLAIMABLE)),</span>
<span class="quote">&gt; -			K(zone_page_state(zone, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="quote">&gt;  			zone_page_state(zone, NR_KERNEL_STACK_KB),</span>
<span class="quote">&gt;  			K(zone_page_state(zone, NR_PAGETABLE)),</span>
<span class="quote">&gt;  			K(zone_page_state(zone, NR_BOUNCE)),</span>
<span class="quote">&gt; diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="quote">&gt; index 2a31ee3c5814..b55853399559 100644</span>
<span class="quote">&gt; --- a/mm/slab.c</span>
<span class="quote">&gt; +++ b/mm/slab.c</span>
<span class="quote">&gt; @@ -1425,10 +1425,10 @@ static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	nr_pages = (1 &lt;&lt; cachep-&gt;gfporder);</span>
<span class="quote">&gt;  	if (cachep-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT)</span>
<span class="quote">&gt; -		add_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +		add_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  			NR_SLAB_RECLAIMABLE, nr_pages);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		add_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +		add_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  			NR_SLAB_UNRECLAIMABLE, nr_pages);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	__SetPageSlab(page);</span>
<span class="quote">&gt; @@ -1459,10 +1459,10 @@ static void kmem_freepages(struct kmem_cache *cachep, struct page *page)</span>
<span class="quote">&gt;  	kmemcheck_free_shadow(page, order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (cachep-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT)</span>
<span class="quote">&gt; -		sub_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +		sub_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  				NR_SLAB_RECLAIMABLE, nr_freed);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		sub_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +		sub_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  				NR_SLAB_UNRECLAIMABLE, nr_freed);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	BUG_ON(!PageSlab(page));</span>
<span class="quote">&gt; diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="quote">&gt; index 57e5156f02be..673e72698d9b 100644</span>
<span class="quote">&gt; --- a/mm/slub.c</span>
<span class="quote">&gt; +++ b/mm/slub.c</span>
<span class="quote">&gt; @@ -1615,7 +1615,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mod_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +	mod_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  		(s-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT) ?</span>
<span class="quote">&gt;  		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,</span>
<span class="quote">&gt;  		1 &lt;&lt; oo_order(oo));</span>
<span class="quote">&gt; @@ -1655,7 +1655,7 @@ static void __free_slab(struct kmem_cache *s, struct page *page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	kmemcheck_free_shadow(page, compound_order(page));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mod_zone_page_state(page_zone(page),</span>
<span class="quote">&gt; +	mod_node_page_state(page_pgdat(page),</span>
<span class="quote">&gt;  		(s-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT) ?</span>
<span class="quote">&gt;  		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,</span>
<span class="quote">&gt;  		-pages);</span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index c5f9d1673392..5d187ee618c0 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -3815,7 +3815,7 @@ int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  	 * unmapped file backed pages.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (node_pagecache_reclaimable(pgdat) &lt;= pgdat-&gt;min_unmapped_pages &amp;&amp;</span>
<span class="quote">&gt; -	    sum_zone_node_page_state(pgdat-&gt;node_id, NR_SLAB_RECLAIMABLE) &lt;= pgdat-&gt;min_slab_pages)</span>
<span class="quote">&gt; +	    node_page_state(pgdat, NR_SLAB_RECLAIMABLE) &lt;= pgdat-&gt;min_slab_pages)</span>
<span class="quote">&gt;  		return NODE_RECLAIM_FULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 76f73670200a..a64f1c764f17 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -928,8 +928,6 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt;  	&quot;nr_zone_unevictable&quot;,</span>
<span class="quote">&gt;  	&quot;nr_zone_write_pending&quot;,</span>
<span class="quote">&gt;  	&quot;nr_mlock&quot;,</span>
<span class="quote">&gt; -	&quot;nr_slab_reclaimable&quot;,</span>
<span class="quote">&gt; -	&quot;nr_slab_unreclaimable&quot;,</span>
<span class="quote">&gt;  	&quot;nr_page_table_pages&quot;,</span>
<span class="quote">&gt;  	&quot;nr_kernel_stack&quot;,</span>
<span class="quote">&gt;  	&quot;nr_bounce&quot;,</span>
<span class="quote">&gt; @@ -952,6 +950,8 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt;  	&quot;nr_inactive_file&quot;,</span>
<span class="quote">&gt;  	&quot;nr_active_file&quot;,</span>
<span class="quote">&gt;  	&quot;nr_unevictable&quot;,</span>
<span class="quote">&gt; +	&quot;nr_slab_reclaimable&quot;,</span>
<span class="quote">&gt; +	&quot;nr_slab_unreclaimable&quot;,</span>
<span class="quote">&gt;  	&quot;nr_isolated_anon&quot;,</span>
<span class="quote">&gt;  	&quot;nr_isolated_file&quot;,</span>
<span class="quote">&gt;  	&quot;workingset_refault&quot;,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.12.2</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=236">Heiko Carstens</a> - May 31, 2017, 11:39 a.m.</div>
<pre class="content">
On Wed, May 31, 2017 at 11:12:56AM +0200, Heiko Carstens wrote:
<span class="quote">&gt; On Tue, May 30, 2017 at 02:17:20PM -0400, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; To re-implement slab cache vs. page cache balancing, we&#39;ll need the</span>
<span class="quote">&gt; &gt; slab counters at the lruvec level, which, ever since lru reclaim was</span>
<span class="quote">&gt; &gt; moved from the zone to the node, is the intersection of the node, not</span>
<span class="quote">&gt; &gt; the zone, and the memcg.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We could retain the per-zone counters for when the page allocator</span>
<span class="quote">&gt; &gt; dumps its memory information on failures, and have counters on both</span>
<span class="quote">&gt; &gt; levels - which on all but NUMA node 0 is usually redundant. But let&#39;s</span>
<span class="quote">&gt; &gt; keep it simple for now and just move them. If anybody complains we can</span>
<span class="quote">&gt; &gt; restore the per-zone counters.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch causes an early boot crash on s390 (linux-next as of today).</span>
<span class="quote">&gt; CONFIG_NUMA on/off doesn&#39;t make any difference. I haven&#39;t looked any</span>
<span class="quote">&gt; further into this yet, maybe you have an idea?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Kernel BUG at 00000000002b0362 [verbose debug info unavailable]</span>
<span class="quote">&gt; addressing exception: 0005 ilc:3 [#1] SMP</span>
<span class="quote">&gt; Modules linked in:</span>
<span class="quote">&gt; CPU: 0 PID: 0 Comm: swapper Not tainted 4.12.0-rc3-00153-gb6bc6724488a #16</span>
<span class="quote">&gt; Hardware name: IBM 2964 N96 702 (z/VM 6.4.0)</span>
<span class="quote">&gt; task: 0000000000d75d00 task.stack: 0000000000d60000</span>
<span class="quote">&gt; Krnl PSW : 0404200180000000 00000000002b0362 (mod_node_page_state+0x62/0x158)</span>
<span class="quote">&gt;            R:0 T:1 IO:0 EX:0 Key:0 M:1 W:0 P:0 AS:0 CC:2 PM:0 RI:0 EA:3</span>
<span class="quote">&gt; Krnl GPRS: 0000000000000001 000000003d81f000 0000000000000000 0000000000000006</span>
<span class="quote">&gt;            0000000000000001 0000000000f29b52 0000000000000041 0000000000000000</span>
<span class="quote">&gt;            0000000000000007 0000000000000040 000000003fe81000 000003d100ffa000</span>
<span class="quote">&gt;            0000000000ee1cd0 0000000000979040 0000000000300abc 0000000000d63c90</span>
<span class="quote">&gt; Krnl Code: 00000000002b0350: e31003900004 lg %r1,912</span>
<span class="quote">&gt;            00000000002b0356: e320f0a80004 lg %r2,168(%r15)</span>
<span class="quote">&gt;           #00000000002b035c: e31120000090 llgc %r1,0(%r1,%r2)</span>
<span class="quote">&gt;           &gt;00000000002b0362: b9060011  lgbr %r1,%r1</span>
<span class="quote">&gt;            00000000002b0366: e32003900004 lg %r2,912</span>
<span class="quote">&gt;            00000000002b036c: e3c280000090 llgc %r12,0(%r2,%r8)</span>
<span class="quote">&gt;            00000000002b0372: b90600ac  lgbr %r10,%r12</span>
<span class="quote">&gt;            00000000002b0376: b904002a  lgr %r2,%r10</span>
<span class="quote">&gt; Call Trace:</span>
<span class="quote">&gt; ([&lt;0000000000000000&gt;]           (null))</span>
<span class="quote">&gt;  [&lt;0000000000300abc&gt;] new_slab+0x35c/0x628</span>
<span class="quote">&gt;  [&lt;000000000030740c&gt;] __kmem_cache_create+0x33c/0x638</span>
<span class="quote">&gt;  [&lt;0000000000e99c0e&gt;] create_boot_cache+0xae/0xe0</span>
<span class="quote">&gt;  [&lt;0000000000e9e12c&gt;] kmem_cache_init+0x5c/0x138</span>
<span class="quote">&gt;  [&lt;0000000000e7999c&gt;] start_kernel+0x24c/0x440</span>
<span class="quote">&gt;  [&lt;0000000000100020&gt;] _stext+0x20/0x80</span>
<span class="quote">&gt; Last Breaking-Event-Address:</span>
<span class="quote">&gt;  [&lt;0000000000300ab6&gt;] new_slab+0x356/0x628</span>

FWIW, it looks like your patch only triggers a bug that was introduced with
a different change that somehow messes around with the pages used to setup
the kernel page tables. I&#39;ll look into this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=141271">Yury Norov</a> - May 31, 2017, 5:11 p.m.</div>
<pre class="content">
On Wed, May 31, 2017 at 01:39:00PM +0200, Heiko Carstens wrote:
<span class="quote">&gt; On Wed, May 31, 2017 at 11:12:56AM +0200, Heiko Carstens wrote:</span>
<span class="quote">&gt; &gt; On Tue, May 30, 2017 at 02:17:20PM -0400, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; &gt; To re-implement slab cache vs. page cache balancing, we&#39;ll need the</span>
<span class="quote">&gt; &gt; &gt; slab counters at the lruvec level, which, ever since lru reclaim was</span>
<span class="quote">&gt; &gt; &gt; moved from the zone to the node, is the intersection of the node, not</span>
<span class="quote">&gt; &gt; &gt; the zone, and the memcg.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; We could retain the per-zone counters for when the page allocator</span>
<span class="quote">&gt; &gt; &gt; dumps its memory information on failures, and have counters on both</span>
<span class="quote">&gt; &gt; &gt; levels - which on all but NUMA node 0 is usually redundant. But let&#39;s</span>
<span class="quote">&gt; &gt; &gt; keep it simple for now and just move them. If anybody complains we can</span>
<span class="quote">&gt; &gt; &gt; restore the per-zone counters.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch causes an early boot crash on s390 (linux-next as of today).</span>
<span class="quote">&gt; &gt; CONFIG_NUMA on/off doesn&#39;t make any difference. I haven&#39;t looked any</span>
<span class="quote">&gt; &gt; further into this yet, maybe you have an idea?</span>

The same on arm64.

Yury
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=81661">Michael Ellerman</a> - June 1, 2017, 10:07 a.m.</div>
<pre class="content">
Yury Norov &lt;ynorov@caviumnetworks.com&gt; writes:
<span class="quote">
&gt; On Wed, May 31, 2017 at 01:39:00PM +0200, Heiko Carstens wrote:</span>
<span class="quote">&gt;&gt; On Wed, May 31, 2017 at 11:12:56AM +0200, Heiko Carstens wrote:</span>
<span class="quote">&gt;&gt; &gt; On Tue, May 30, 2017 at 02:17:20PM -0400, Johannes Weiner wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt; To re-implement slab cache vs. page cache balancing, we&#39;ll need the</span>
<span class="quote">&gt;&gt; &gt; &gt; slab counters at the lruvec level, which, ever since lru reclaim was</span>
<span class="quote">&gt;&gt; &gt; &gt; moved from the zone to the node, is the intersection of the node, not</span>
<span class="quote">&gt;&gt; &gt; &gt; the zone, and the memcg.</span>
<span class="quote">&gt;&gt; &gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; &gt; We could retain the per-zone counters for when the page allocator</span>
<span class="quote">&gt;&gt; &gt; &gt; dumps its memory information on failures, and have counters on both</span>
<span class="quote">&gt;&gt; &gt; &gt; levels - which on all but NUMA node 0 is usually redundant. But let&#39;s</span>
<span class="quote">&gt;&gt; &gt; &gt; keep it simple for now and just move them. If anybody complains we can</span>
<span class="quote">&gt;&gt; &gt; &gt; restore the per-zone counters.</span>
<span class="quote">&gt;&gt; &gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; &gt; Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; This patch causes an early boot crash on s390 (linux-next as of today).</span>
<span class="quote">&gt;&gt; &gt; CONFIG_NUMA on/off doesn&#39;t make any difference. I haven&#39;t looked any</span>
<span class="quote">&gt;&gt; &gt; further into this yet, maybe you have an idea?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The same on arm64.</span>

And powerpc.

cheers
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index 5548f9686016..e57e06e6df4c 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -129,11 +129,11 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(node_page_state(pgdat, NR_UNSTABLE_NFS)),
 		       nid, K(sum_zone_node_page_state(nid, NR_BOUNCE)),
 		       nid, K(node_page_state(pgdat, NR_WRITEBACK_TEMP)),
<span class="p_del">-		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_del">-				sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_del">-		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_RECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(node_page_state(pgdat, NR_SLAB_RECLAIMABLE) +</span>
<span class="p_add">+			      node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(node_page_state(pgdat, NR_SLAB_RECLAIMABLE)),</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)),</span>
<span class="p_add">+		       nid, K(node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)),</span>
 		       nid, K(node_page_state(pgdat, NR_ANON_THPS) *
 				       HPAGE_PMD_NR),
 		       nid, K(node_page_state(pgdat, NR_SHMEM_THPS) *
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(node_page_state(pgdat, NR_SHMEM_PMDMAPPED) *
 				       HPAGE_PMD_NR));
 #else
<span class="p_del">-		       nid, K(sum_zone_node_page_state(nid, NR_SLAB_UNRECLAIMABLE)));</span>
<span class="p_add">+		       nid, K(node_page_state(pgdat, NR_SLAB_UNRECLAIMABLE)));</span>
 #endif
 	n += hugetlb_report_node_meminfo(nid, buf + n);
 	return n;
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index ebaccd4e7d8c..eacadee83964 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -125,8 +125,6 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_ZONE_UNEVICTABLE,
 	NR_ZONE_WRITE_PENDING,	/* Count of dirty, writeback and unstable pages */
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
<span class="p_del">-	NR_SLAB_RECLAIMABLE,</span>
<span class="p_del">-	NR_SLAB_UNRECLAIMABLE,</span>
 	NR_PAGETABLE,		/* used for pagetables */
 	NR_KERNEL_STACK_KB,	/* measured in KiB */
 	/* Second 128 byte cacheline */
<span class="p_chunk">@@ -152,6 +150,8 @@</span> <span class="p_context"> enum node_stat_item {</span>
 	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
<span class="p_add">+	NR_SLAB_RECLAIMABLE,</span>
<span class="p_add">+	NR_SLAB_UNRECLAIMABLE,</span>
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
 	WORKINGSET_REFAULT,
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index f9e450c6b6e4..5f89cfaddc4b 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -4601,8 +4601,6 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter, nodemask_t *nodemask)</span>
 			&quot; present:%lukB&quot;
 			&quot; managed:%lukB&quot;
 			&quot; mlocked:%lukB&quot;
<span class="p_del">-			&quot; slab_reclaimable:%lukB&quot;</span>
<span class="p_del">-			&quot; slab_unreclaimable:%lukB&quot;</span>
 			&quot; kernel_stack:%lukB&quot;
 			&quot; pagetables:%lukB&quot;
 			&quot; bounce:%lukB&quot;
<span class="p_chunk">@@ -4624,8 +4622,6 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter, nodemask_t *nodemask)</span>
 			K(zone-&gt;present_pages),
 			K(zone-&gt;managed_pages),
 			K(zone_page_state(zone, NR_MLOCK)),
<span class="p_del">-			K(zone_page_state(zone, NR_SLAB_RECLAIMABLE)),</span>
<span class="p_del">-			K(zone_page_state(zone, NR_SLAB_UNRECLAIMABLE)),</span>
 			zone_page_state(zone, NR_KERNEL_STACK_KB),
 			K(zone_page_state(zone, NR_PAGETABLE)),
 			K(zone_page_state(zone, NR_BOUNCE)),
<span class="p_header">diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="p_header">index 2a31ee3c5814..b55853399559 100644</span>
<span class="p_header">--- a/mm/slab.c</span>
<span class="p_header">+++ b/mm/slab.c</span>
<span class="p_chunk">@@ -1425,10 +1425,10 @@</span> <span class="p_context"> static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,</span>
 
 	nr_pages = (1 &lt;&lt; cachep-&gt;gfporder);
 	if (cachep-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT)
<span class="p_del">-		add_zone_page_state(page_zone(page),</span>
<span class="p_add">+		add_node_page_state(page_pgdat(page),</span>
 			NR_SLAB_RECLAIMABLE, nr_pages);
 	else
<span class="p_del">-		add_zone_page_state(page_zone(page),</span>
<span class="p_add">+		add_node_page_state(page_pgdat(page),</span>
 			NR_SLAB_UNRECLAIMABLE, nr_pages);
 
 	__SetPageSlab(page);
<span class="p_chunk">@@ -1459,10 +1459,10 @@</span> <span class="p_context"> static void kmem_freepages(struct kmem_cache *cachep, struct page *page)</span>
 	kmemcheck_free_shadow(page, order);
 
 	if (cachep-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT)
<span class="p_del">-		sub_zone_page_state(page_zone(page),</span>
<span class="p_add">+		sub_node_page_state(page_pgdat(page),</span>
 				NR_SLAB_RECLAIMABLE, nr_freed);
 	else
<span class="p_del">-		sub_zone_page_state(page_zone(page),</span>
<span class="p_add">+		sub_node_page_state(page_pgdat(page),</span>
 				NR_SLAB_UNRECLAIMABLE, nr_freed);
 
 	BUG_ON(!PageSlab(page));
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index 57e5156f02be..673e72698d9b 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -1615,7 +1615,7 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 	if (!page)
 		return NULL;
 
<span class="p_del">-	mod_zone_page_state(page_zone(page),</span>
<span class="p_add">+	mod_node_page_state(page_pgdat(page),</span>
 		(s-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT) ?
 		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
 		1 &lt;&lt; oo_order(oo));
<span class="p_chunk">@@ -1655,7 +1655,7 @@</span> <span class="p_context"> static void __free_slab(struct kmem_cache *s, struct page *page)</span>
 
 	kmemcheck_free_shadow(page, compound_order(page));
 
<span class="p_del">-	mod_zone_page_state(page_zone(page),</span>
<span class="p_add">+	mod_node_page_state(page_pgdat(page),</span>
 		(s-&gt;flags &amp; SLAB_RECLAIM_ACCOUNT) ?
 		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
 		-pages);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index c5f9d1673392..5d187ee618c0 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -3815,7 +3815,7 @@</span> <span class="p_context"> int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)</span>
 	 * unmapped file backed pages.
 	 */
 	if (node_pagecache_reclaimable(pgdat) &lt;= pgdat-&gt;min_unmapped_pages &amp;&amp;
<span class="p_del">-	    sum_zone_node_page_state(pgdat-&gt;node_id, NR_SLAB_RECLAIMABLE) &lt;= pgdat-&gt;min_slab_pages)</span>
<span class="p_add">+	    node_page_state(pgdat, NR_SLAB_RECLAIMABLE) &lt;= pgdat-&gt;min_slab_pages)</span>
 		return NODE_RECLAIM_FULL;
 
 	/*
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 76f73670200a..a64f1c764f17 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -928,8 +928,6 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_zone_unevictable&quot;,
 	&quot;nr_zone_write_pending&quot;,
 	&quot;nr_mlock&quot;,
<span class="p_del">-	&quot;nr_slab_reclaimable&quot;,</span>
<span class="p_del">-	&quot;nr_slab_unreclaimable&quot;,</span>
 	&quot;nr_page_table_pages&quot;,
 	&quot;nr_kernel_stack&quot;,
 	&quot;nr_bounce&quot;,
<span class="p_chunk">@@ -952,6 +950,8 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_inactive_file&quot;,
 	&quot;nr_active_file&quot;,
 	&quot;nr_unevictable&quot;,
<span class="p_add">+	&quot;nr_slab_reclaimable&quot;,</span>
<span class="p_add">+	&quot;nr_slab_unreclaimable&quot;,</span>
 	&quot;nr_isolated_anon&quot;,
 	&quot;nr_isolated_file&quot;,
 	&quot;workingset_refault&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



