
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,2/2] mm, hugetlb: do not rely on overcommit limit during migration - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,2/2] mm, hugetlb: do not rely on overcommit limit during migration</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 29, 2017, 11:33 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171129113330.znjgwtviw6tr6npo@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10081959/mbox/"
   >mbox</a>
|
   <a href="/patch/10081959/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10081959/">/patch/10081959/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1FA6360234 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 29 Nov 2017 11:33:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0EF8F2963A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 29 Nov 2017 11:33:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 03D912965D; Wed, 29 Nov 2017 11:33:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2ECCF2963A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 29 Nov 2017 11:33:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753480AbdK2Ldf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 29 Nov 2017 06:33:35 -0500
Received: from mx2.suse.de ([195.135.220.15]:51552 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751496AbdK2Lde (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 29 Nov 2017 06:33:34 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 5D40CAD14;
	Wed, 29 Nov 2017 11:33:32 +0000 (UTC)
Date: Wed, 29 Nov 2017 12:33:30 +0100
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: linux-mm@kvack.org
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: [PATCH RFC v2 2/2] mm, hugetlb: do not rely on overcommit limit
	during migration
Message-ID: &lt;20171129113330.znjgwtviw6tr6npo@dhcp22.suse.cz&gt;
References: &lt;20171128101907.jtjthykeuefxu7gl@dhcp22.suse.cz&gt;
	&lt;20171128141211.11117-1-mhocko@kernel.org&gt;
	&lt;20171128141211.11117-3-mhocko@kernel.org&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20171128141211.11117-3-mhocko@kernel.org&gt;
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 29, 2017, 11:33 a.m.</div>
<pre class="content">
OK, so this is the v2 with all the fixups folded in. It doesn&#39;t blow up
immediately and even seem to work.
---
From 5de969daf882122690f0dfc028d09f88f312b7fa Mon Sep 17 00:00:00 2001
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Date: Wed, 29 Nov 2017 12:31:49 +0100
Subject: [PATCH] mm, hugetlb: do not rely on overcommit limit during migration

hugepage migration relies on __alloc_buddy_huge_page to get a new page.
This has 2 main disadvantages.
1) it doesn&#39;t allow to migrate any huge page if the pool is used
completely which is not an exceptional case as the pool is static and
unused memory is just wasted.
2) it leads to a weird semantic when migration between two numa nodes
might increase the pool size of the destination NUMA node while the page
is in use. The issue is caused by per NUMA node surplus pages tracking
(see free_huge_page).

Address both issues by changing the way how we allocate and account
pages allocated for migration. Those should temporal by definition.
So we mark them that way (we will abuse page flags in the 3rd page)
and update free_huge_page to free such pages to the page allocator.
Page migration path then just transfers the temporal status from the
new page to the old one which will be freed on the last reference.
The global surplus count will never change during this path but we still
have to be careful when migrating a per-node suprlus page. This is now
handled in move_hugetlb_state which is called from the migration path
and it copies the hugetlb specific page state and fixes up the
accounting when needed

Rename __alloc_buddy_huge_page to __alloc_surplus_huge_page to better
reflect its purpose. The new allocation routine for the migration path
is __alloc_migrate_huge_page.

The user visible effect of this patch is that migrated pages are really
temporal and they travel between NUMA nodes as per the migration
request:
Before migration
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:1
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0

After

/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:1
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0

with the previous implementation, both nodes would have nr_hugepages:1
until the page is freed.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |   3 ++
 mm/hugetlb.c            | 109 +++++++++++++++++++++++++++++++++++++++++-------
 mm/migrate.c            |   3 +-
 3 files changed, 97 insertions(+), 18 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 6e3696c7b35a..1a9c89850e4a 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -119,6 +119,7 @@</span> <span class="p_context"> long hugetlb_unreserve_pages(struct inode *inode, long start, long end,</span>
 						long freed);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
<span class="p_add">+void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason);</span>
 void free_huge_page(struct page *page);
 void hugetlb_fix_reserve_counts(struct inode *inode);
 extern struct mutex *hugetlb_fault_mutex_table;
<span class="p_chunk">@@ -157,6 +158,7 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
 bool is_hugetlb_entry_migration(pte_t pte);
<span class="p_add">+</span>
 #else /* !CONFIG_HUGETLB_PAGE */
 
 static inline void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
<span class="p_chunk">@@ -197,6 +199,7 @@</span> <span class="p_context"> static inline bool isolate_huge_page(struct page *page, struct list_head *list)</span>
 	return false;
 }
 #define putback_active_hugepage(p)	do {} while (0)
<span class="p_add">+#define move_hugetlb_state(old, new, reason)	do {} while (0)</span>
 
 static inline unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot)
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 8189c92fac82..8d83e9802f11 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/hugetlb_cgroup.h&gt;
 #include &lt;linux/node.h&gt;
 #include &lt;linux/userfaultfd_k.h&gt;
<span class="p_add">+#include &lt;linux/page_owner.h&gt;</span>
 #include &quot;internal.h&quot;
 
 int hugetlb_max_hstate __read_mostly;
<span class="p_chunk">@@ -1249,6 +1250,28 @@</span> <span class="p_context"> static void clear_page_huge_active(struct page *page)</span>
 	ClearPagePrivate(&amp;page[1]);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Internal hugetlb specific page flag. Do not use outside of the hugetlb</span>
<span class="p_add">+ * code</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool PageHugeTemporary(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!PageHuge(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return page[2].mapping == -1U;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void SetPageHugeTemporary(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	page[2].mapping = -1U;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void ClearPageHugeTemporary(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	page[2].mapping = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void free_huge_page(struct page *page)
 {
 	/*
<span class="p_chunk">@@ -1283,7 +1306,11 @@</span> <span class="p_context"> void free_huge_page(struct page *page)</span>
 	if (restore_reserve)
 		h-&gt;resv_huge_pages++;
 
<span class="p_del">-	if (h-&gt;surplus_huge_pages_node[nid]) {</span>
<span class="p_add">+	if (PageHugeTemporary(page)) {</span>
<span class="p_add">+		list_del(&amp;page-&gt;lru);</span>
<span class="p_add">+		ClearPageHugeTemporary(page);</span>
<span class="p_add">+		update_and_free_page(h, page);</span>
<span class="p_add">+	} else if (h-&gt;surplus_huge_pages_node[nid]) {</span>
 		/* remove the page from active list */
 		list_del(&amp;page-&gt;lru);
 		update_and_free_page(h, page);
<span class="p_chunk">@@ -1531,7 +1558,10 @@</span> <span class="p_context"> int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 	return rc;
 }
 
<span class="p_del">-static struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocates a fresh surplus page from the page allocator.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		int nid, nodemask_t *nmask)
 {
 	struct page *page;
<span class="p_chunk">@@ -1595,6 +1625,28 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	return page;
 }
 
<span class="p_add">+static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+		int nid, nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We do not account these pages as surplus because they are only</span>
<span class="p_add">+	 * temporary and will be released properly on the last reference</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="p_add">+	SetPageHugeTemporary(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.
  */
<span class="p_chunk">@@ -1609,17 +1661,13 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 	nodemask_t *nodemask;
 
 	nid = huge_node(vma, addr, gfp_mask, &amp;mpol, &amp;nodemask);
<span class="p_del">-	page = __alloc_buddy_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="p_add">+	page = __alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
 	mpol_cond_put(mpol);
 
 	return page;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * This allocation function is useful in the context where vma is irrelevant.</span>
<span class="p_del">- * E.g. soft-offlining uses this function because it only cares physical</span>
<span class="p_del">- * address of error page.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* page migration callback function */</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid)
 {
 	gfp_t gfp_mask = htlb_alloc_mask(h);
<span class="p_chunk">@@ -1634,12 +1682,12 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_del">-		page = __alloc_buddy_huge_page(h, gfp_mask, nid, NULL);</span>
<span class="p_add">+		page = __alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
 
 	return page;
 }
 
<span class="p_del">-</span>
<span class="p_add">+/* page migration callback function */</span>
 struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 		nodemask_t *nmask)
 {
<span class="p_chunk">@@ -1657,9 +1705,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	/* No reservations, try to overcommit */</span>
<span class="p_del">-</span>
<span class="p_del">-	return __alloc_buddy_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
<span class="p_add">+	return __alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
 }
 
 /*
<span class="p_chunk">@@ -1687,7 +1733,7 @@</span> <span class="p_context"> static int gather_surplus_pages(struct hstate *h, int delta)</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = __alloc_buddy_huge_page(h, htlb_alloc_mask(h),</span>
<span class="p_add">+		page = __alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
 				NUMA_NO_NODE, NULL);
 		if (!page) {
 			alloc_ok = false;
<span class="p_chunk">@@ -2284,7 +2330,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * First take pages out of surplus state.  Then make up the
 	 * remaining difference by allocating fresh huge pages.
 	 *
<span class="p_del">-	 * We might race with __alloc_buddy_huge_page() here and be unable</span>
<span class="p_add">+	 * We might race with __alloc_surplus_huge_page() here and be unable</span>
 	 * to convert a surplus huge page to a normal huge page. That is
 	 * not critical, though, it just means the overall size of the
 	 * pool might be one hugepage larger than it needs to be, but
<span class="p_chunk">@@ -2330,7 +2376,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * By placing pages into the surplus state independent of the
 	 * overcommit value, we are allowing the surplus pool size to
 	 * exceed overcommit. There are few sane options here. Since
<span class="p_del">-	 * __alloc_buddy_huge_page() is checking the global counter,</span>
<span class="p_add">+	 * __alloc_surplus_huge_page() is checking the global counter,</span>
 	 * though, we&#39;ll note that we&#39;re not allowed to exceed surplus
 	 * and won&#39;t grow the pool anywhere else. Not until one of the
 	 * sysctls are changed, or the surplus pages go out of use.
<span class="p_chunk">@@ -4804,3 +4850,34 @@</span> <span class="p_context"> void putback_active_hugepage(struct page *page)</span>
 	spin_unlock(&amp;hugetlb_lock);
 	put_page(page);
 }
<span class="p_add">+</span>
<span class="p_add">+void move_hugetlb_state(struct page *oldpage, struct page *newpage, int reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hstate *h = page_hstate(oldpage);</span>
<span class="p_add">+</span>
<span class="p_add">+	hugetlb_cgroup_migrate(oldpage, newpage);</span>
<span class="p_add">+	set_page_owner_migrate_reason(newpage, reason);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * transfer temporary state of the new huge page. This is</span>
<span class="p_add">+	 * reverse to other transitions because the newpage is going to</span>
<span class="p_add">+	 * be final while the old one will be freed so it takes over</span>
<span class="p_add">+	 * the temporary status.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Also note that we have to transfer the per-node surplus state</span>
<span class="p_add">+	 * here as well otherwise the global surplus count will not match</span>
<span class="p_add">+	 * the per-node&#39;s.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PageHugeTemporary(newpage)) {</span>
<span class="p_add">+		int old_nid = page_to_nid(oldpage);</span>
<span class="p_add">+		int new_nid = page_to_nid(newpage);</span>
<span class="p_add">+</span>
<span class="p_add">+		SetPageHugeTemporary(oldpage);</span>
<span class="p_add">+		ClearPageHugeTemporary(newpage);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (h-&gt;surplus_huge_pages_node[old_nid]) {</span>
<span class="p_add">+			h-&gt;surplus_huge_pages_node[old_nid]--;</span>
<span class="p_add">+			h-&gt;surplus_huge_pages_node[new_nid]++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 4d0be47a322a..1e5525a25691 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1323,9 +1323,8 @@</span> <span class="p_context"> static int unmap_and_move_huge_page(new_page_t get_new_page,</span>
 		put_anon_vma(anon_vma);
 
 	if (rc == MIGRATEPAGE_SUCCESS) {
<span class="p_del">-		hugetlb_cgroup_migrate(hpage, new_hpage);</span>
<span class="p_add">+		move_hugetlb_state(hpage, new_hpage, reason);</span>
 		put_new_page = NULL;
<span class="p_del">-		set_page_owner_migrate_reason(new_hpage, reason);</span>
 	}
 
 	unlock_page(hpage);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



