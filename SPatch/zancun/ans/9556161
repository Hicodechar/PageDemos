
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,09/14] mm: thp: check pmd migration entry in common path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,09/14] mm: thp: check pmd migration entry in common path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 5, 2017, 4:12 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170205161252.85004-10-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9556161/mbox/"
   >mbox</a>
|
   <a href="/patch/9556161/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9556161/">/patch/9556161/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	905EA60236 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:16:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7FA0A26419
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:16:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 741B326785; Sun,  5 Feb 2017 16:16:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D02AD26419
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:16:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752457AbdBEQPb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 5 Feb 2017 11:15:31 -0500
Received: from out1-smtp.messagingengine.com ([66.111.4.25]:33487 &quot;EHLO
	out1-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752176AbdBEQOf (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 5 Feb 2017 11:14:35 -0500
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by mailout.nyi.internal (Postfix) with ESMTP id 7EEDE207F7;
	Sun,  5 Feb 2017 11:14:34 -0500 (EST)
Received: from frontend2 ([10.202.2.161])
	by compute3.internal (MEProxy); Sun, 05 Feb 2017 11:14:34 -0500
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-me-sender:x-me-sender:x-sasl-enc:x-sasl-enc; s=mesmtp; bh=doc
	F3/ChvRRYxxzRuwq0UibEteU=; b=C9xbqNTTAPNL/r/4MosE/bEmxATAGpaOR4q
	lIg64hQYl+2WtNikylN0GqQ7WH3twMUuo9Dx/Z9DwupVRpuEDE8pijLr7dNtln/q
	wjZDCyp1FfUgnYEhJfV2+OYg88IyvGIrKhpeAzxdn1dc0kFhWNdM/soKLlSy+Cl+
	DWzrNEbY=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-me-sender:x-me-sender:x-sasl-enc
	:x-sasl-enc; s=smtpout; bh=docF3/ChvRRYxxzRuwq0UibEteU=; b=GEbv5
	mGBWEAqYp77+VYTNPDpGNkBjaDHaQeVizCSfFdcHSh0evkuc6Rk0nZ5wHNRs2wln
	0ekrUyBkkGW1FXa5l86sxvEsmGGtKrnsT0ai4DLi1bsYigZvtSzO6z6cpeUgVTN6
	BTm6leFqTRS2/GqMKC2rE/dP+kneRk80v6SmF0=
X-ME-Sender: &lt;xms:ak-XWE7QuVGxOaBoyI3mFgOTNQxzLYMCQd0lSi1rWhaDh5dA9Zxvag&gt;
X-Sasl-enc: 6vhv3oS7pD5j73t4qjT061sklJz499scAOpmpuOEokWw 1486311274
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id 11DE124573;
	Sun,  5 Feb 2017 11:14:34 -0500 (EST)
From: Zi Yan &lt;zi.yan@sent.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	kirill.shutemov@linux.intel.com
Cc: akpm@linux-foundation.org, minchan@kernel.org, vbabka@suse.cz,
	mgorman@techsingularity.net, n-horiguchi@ah.jp.nec.com,
	khandual@linux.vnet.ibm.com, zi.yan@cs.rutgers.edu
Subject: [PATCH v3 09/14] mm: thp: check pmd migration entry in common path
Date: Sun,  5 Feb 2017 11:12:47 -0500
Message-Id: &lt;20170205161252.85004-10-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170205161252.85004-1-zi.yan@sent.com&gt;
References: &lt;20170205161252.85004-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - Feb. 5, 2017, 4:12 p.m.</div>
<pre class="content">
<span class="from">From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

If one of callers of page migration starts to handle thp,
memory management code start to see pmd migration entry, so we need
to prepare for it before enabling. This patch changes various code
point which checks the status of given pmds in order to prevent race
between thp migration and the pmd-related works.

ChangeLog v1 -&gt; v2:
- introduce pmd_related() (I know the naming is not good, but can&#39;t
  think up no better name. Any suggesntion is welcomed.)
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

ChangeLog v2 -&gt; v3:
- add is_swap_pmd()
- a pmd entry should be is_swap_pmd(), pmd_trans_huge(), pmd_devmap(),
  or pmd_none()
- use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear()
- flush_cache_range() while set_pmd_migration_entry()
- pmd_none_or_trans_huge_or_clear_bad() and pmd_trans_unstable() return
  true on pmd_migration_entry, so that migration entries are not
  treated as pmd page table entries.
<span class="signed-off-by">
Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
---
 arch/x86/mm/gup.c             |  4 +--
 fs/proc/task_mmu.c            | 22 ++++++++-----
 include/asm-generic/pgtable.h | 71 ----------------------------------------
 include/linux/huge_mm.h       | 21 ++++++++++--
 include/linux/swapops.h       | 74 +++++++++++++++++++++++++++++++++++++++++
 mm/gup.c                      | 20 ++++++++++--
 mm/huge_memory.c              | 76 ++++++++++++++++++++++++++++++++++++-------
 mm/madvise.c                  |  2 ++
 mm/memcontrol.c               |  2 ++
 mm/memory.c                   |  9 +++--
 mm/memory_hotplug.c           | 13 +++++++-
 mm/mempolicy.c                |  1 +
 mm/mprotect.c                 |  6 ++--
 mm/mremap.c                   |  2 +-
 mm/pagewalk.c                 |  2 ++
 15 files changed, 221 insertions(+), 104 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Feb. 9, 2017, 9:16 a.m.</div>
<pre class="content">
On Sun, Feb 05, 2017 at 11:12:47AM -0500, Zi Yan wrote:
<span class="quote">&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If one of callers of page migration starts to handle thp,</span>
<span class="quote">&gt; memory management code start to see pmd migration entry, so we need</span>
<span class="quote">&gt; to prepare for it before enabling. This patch changes various code</span>
<span class="quote">&gt; point which checks the status of given pmds in order to prevent race</span>
<span class="quote">&gt; between thp migration and the pmd-related works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - introduce pmd_related() (I know the naming is not good, but can&#39;t</span>
<span class="quote">&gt;   think up no better name. Any suggesntion is welcomed.)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; - add is_swap_pmd()</span>
<span class="quote">&gt; - a pmd entry should be is_swap_pmd(), pmd_trans_huge(), pmd_devmap(),</span>
<span class="quote">&gt;   or pmd_none()</span>

(nitpick) ... or normal pmd pointing to pte pages?
<span class="quote">
&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear()</span>
<span class="quote">&gt; - flush_cache_range() while set_pmd_migration_entry()</span>
<span class="quote">&gt; - pmd_none_or_trans_huge_or_clear_bad() and pmd_trans_unstable() return</span>
<span class="quote">&gt;   true on pmd_migration_entry, so that migration entries are not</span>
<span class="quote">&gt;   treated as pmd page table entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/gup.c             |  4 +--</span>
<span class="quote">&gt;  fs/proc/task_mmu.c            | 22 ++++++++-----</span>
<span class="quote">&gt;  include/asm-generic/pgtable.h | 71 ----------------------------------------</span>
<span class="quote">&gt;  include/linux/huge_mm.h       | 21 ++++++++++--</span>
<span class="quote">&gt;  include/linux/swapops.h       | 74 +++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/gup.c                      | 20 ++++++++++--</span>
<span class="quote">&gt;  mm/huge_memory.c              | 76 ++++++++++++++++++++++++++++++++++++-------</span>
<span class="quote">&gt;  mm/madvise.c                  |  2 ++</span>
<span class="quote">&gt;  mm/memcontrol.c               |  2 ++</span>
<span class="quote">&gt;  mm/memory.c                   |  9 +++--</span>
<span class="quote">&gt;  mm/memory_hotplug.c           | 13 +++++++-</span>
<span class="quote">&gt;  mm/mempolicy.c                |  1 +</span>
<span class="quote">&gt;  mm/mprotect.c                 |  6 ++--</span>
<span class="quote">&gt;  mm/mremap.c                   |  2 +-</span>
<span class="quote">&gt;  mm/pagewalk.c                 |  2 ++</span>
<span class="quote">&gt;  15 files changed, 221 insertions(+), 104 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
<span class="quote">&gt; index 0d4fb3ebbbac..78a153d90064 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/gup.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/gup.c</span>
<span class="quote">&gt; @@ -222,9 +222,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt; -		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; +		if (unlikely(pmd_large(pmd))) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="quote">&gt;  			 * slowpath for accounting purposes and so that they</span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 6c07c7813b26..1e64d6898c68 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -596,7 +596,8 @@ static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt; -		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt; +		if (pmd_present(*pmd))</span>
<span class="quote">&gt; +			smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -929,6 +930,9 @@ static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Clear accessed and referenced bits. */</span>
<span class="quote">&gt; @@ -1208,19 +1212,19 @@ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt;  		u64 flags = 0, frame = 0;</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))</span>
<span class="quote">&gt;  			flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Currently pmd for thp is always present because thp</span>
<span class="quote">&gt; -		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="quote">&gt; -		 * (split in such cases instead.)</span>
<span class="quote">&gt; -		 * This if-check is just to prepare for future implementation.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (pmd_present(pmd)) {</span>
<span class="quote">&gt; -			struct page *page = pmd_page(pmd);</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(pmd)) {</span>
<span class="quote">&gt; +			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +			frame = swp_type(entry) |</span>
<span class="quote">&gt; +				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +		} else if (pmd_present(pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(pmd);</span>
<span class="quote">&gt;  			if (page_mapcount(page) == 1)</span>
<span class="quote">&gt;  				flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; index b71a431ed649..6cf9e9b5a7be 100644</span>
<span class="quote">&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; @@ -726,77 +726,6 @@ static inline pmd_t pmd_read_atomic(pmd_t *pmdp)</span>
<span class="quote">&gt;  #ifndef arch_needs_pgtable_deposit</span>
<span class="quote">&gt;  #define arch_needs_pgtable_deposit() (false)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * This function is meant to be used by sites walking pagetables with</span>
<span class="quote">&gt; - * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="quote">&gt; - * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="quote">&gt; - * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="quote">&gt; - * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="quote">&gt; - * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="quote">&gt; - * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="quote">&gt; - * transhuge pmd. When those races occurs and this function makes a</span>
<span class="quote">&gt; - * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="quote">&gt; - * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="quote">&gt; - * can return none anyway). The compiler level barrier() is critically</span>
<span class="quote">&gt; - * important to compute the two checks atomically on the same pmdval.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="quote">&gt; - * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="quote">&gt; - * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="quote">&gt; - * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="quote">&gt; - * version above, is also needed when THP is disabled because the page</span>
<span class="quote">&gt; - * fault can populate the pmd from under us).</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="quote">&gt; -	 * the stack so that it will stop changing under the code.</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="quote">&gt; -	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="quote">&gt; -	 * (for example pointing to an hugepage that has never been</span>
<span class="quote">&gt; -	 * mapped in the pmd). The below checks will only care about</span>
<span class="quote">&gt; -	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="quote">&gt; -	 * exception of pmd_none(). So the important thing is that if</span>
<span class="quote">&gt; -	 * the low part of the pmd is found null, the high part will</span>
<span class="quote">&gt; -	 * be also null or the pmd_none() check below would be</span>
<span class="quote">&gt; -	 * confused.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; -	barrier();</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -	if (pmd_none(pmdval) || pmd_trans_huge(pmdval))</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; -	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="quote">&gt; -		pmd_clear_bad(pmd);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="quote">&gt; - * the kernel. Otherwise it is equivalent to</span>
<span class="quote">&gt; - * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="quote">&gt; - * places that already verified the pmd is not none and they want to</span>
<span class="quote">&gt; - * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="quote">&gt; - * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="quote">&gt; - * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="quote">&gt; - * run a pmd_trans_unstable before walking the ptes after</span>
<span class="quote">&gt; - * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="quote">&gt; - * become null, but then a page fault can map in a THP and not a</span>
<span class="quote">&gt; - * regular page).</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; -	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef CONFIG_NUMA_BALANCING</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="quote">&gt; index 83a8d42f9d55..c2e5a4eab84a 100644</span>
<span class="quote">&gt; --- a/include/linux/huge_mm.h</span>
<span class="quote">&gt; +++ b/include/linux/huge_mm.h</span>
<span class="quote">&gt; @@ -131,7 +131,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  #define split_huge_pmd(__vma, __pmd, __address)				\</span>
<span class="quote">&gt;  	do {								\</span>
<span class="quote">&gt;  		pmd_t *____pmd = (__pmd);				\</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*____pmd)				\</span>
<span class="quote">&gt; +		if (is_swap_pmd(*____pmd) || pmd_trans_huge(*____pmd)	\</span>
<span class="quote">&gt;  					|| pmd_devmap(*____pmd))	\</span>
<span class="quote">&gt;  			__split_huge_pmd(__vma, __pmd, __address,	\</span>
<span class="quote">&gt;  						false, NULL);		\</span>
<span class="quote">&gt; @@ -162,12 +162,18 @@ extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt;  extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_none(pmd) &amp;&amp; !pmd_present(pmd);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* mmap_sem must be held on entry */</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt; +	if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt;  		return __pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; @@ -192,6 +198,12 @@ struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;  		pmd_t *pmd, int flags);</span>
<span class="quote">&gt;  struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;  		pud_t *pud, int flags);</span>
<span class="quote">&gt; +static inline int hpage_order(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (unlikely(PageTransHuge(page)))</span>
<span class="quote">&gt; +		return HPAGE_PMD_ORDER;</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -232,6 +244,7 @@ static inline bool thp_migration_supported(void)</span>
<span class="quote">&gt;  #define HPAGE_PUD_SIZE ({ BUILD_BUG(); 0; })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define hpage_nr_pages(x) 1</span>
<span class="quote">&gt; +#define hpage_order(x) 0</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define transparent_hugepage_enabled(__vma) 0</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -274,6 +287,10 @@ static inline void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  					 long adjust_next)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 6625bea13869..50e4aa7e7ff9 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -229,6 +229,80 @@ static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * This function is meant to be used by sites walking pagetables with</span>
<span class="quote">&gt; + * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="quote">&gt; + * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="quote">&gt; + * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="quote">&gt; + * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="quote">&gt; + * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="quote">&gt; + * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="quote">&gt; + * transhuge pmd. When those races occurs and this function makes a</span>
<span class="quote">&gt; + * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="quote">&gt; + * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="quote">&gt; + * can return none anyway). The compiler level barrier() is critically</span>
<span class="quote">&gt; + * important to compute the two checks atomically on the same pmdval.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="quote">&gt; + * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="quote">&gt; + * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="quote">&gt; + * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="quote">&gt; + * version above, is also needed when THP is disabled because the page</span>
<span class="quote">&gt; + * fault can populate the pmd from under us).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="quote">&gt; +	 * the stack so that it will stop changing under the code.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="quote">&gt; +	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="quote">&gt; +	 * (for example pointing to an hugepage that has never been</span>
<span class="quote">&gt; +	 * mapped in the pmd). The below checks will only care about</span>
<span class="quote">&gt; +	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="quote">&gt; +	 * exception of pmd_none(). So the important thing is that if</span>
<span class="quote">&gt; +	 * the low part of the pmd is found null, the high part will</span>
<span class="quote">&gt; +	 * be also null or the pmd_none() check below would be</span>
<span class="quote">&gt; +	 * confused.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; +	barrier();</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +	if (pmd_none(pmdval) || pmd_trans_huge(pmdval)</span>
<span class="quote">&gt; +			|| is_pmd_migration_entry(pmdval))</span>
<span class="quote">&gt; +		return 1;</span>
<span class="quote">&gt; +	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="quote">&gt; +		pmd_clear_bad(pmd);</span>
<span class="quote">&gt; +		return 1;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="quote">&gt; + * the kernel. Otherwise it is equivalent to</span>
<span class="quote">&gt; + * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="quote">&gt; + * places that already verified the pmd is not none and they want to</span>
<span class="quote">&gt; + * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="quote">&gt; + * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="quote">&gt; + * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="quote">&gt; + * run a pmd_trans_unstable before walking the ptes after</span>
<span class="quote">&gt; + * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="quote">&gt; + * become null, but then a page fault can map in a THP and not a</span>
<span class="quote">&gt; + * regular page).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; +	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>

These functions are page table or thp matter, so putting them in swapops.h
looks weird to me. Maybe you can avoid this code transfer by using !pmd_present
instead of is_pmd_migration_entry?
And we have to consider renaming pmd_none_or_trans_huge_or_clear_bad(),
I like a simple name like __pmd_trans_unstable(), but if you have an idea,
that&#39;s great.
<span class="quote">
&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt; diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="quote">&gt; index 1e67461b2733..82e0304e5d29 100644</span>
<span class="quote">&gt; --- a/mm/gup.c</span>
<span class="quote">&gt; +++ b/mm/gup.c</span>
<span class="quote">&gt; @@ -274,6 +274,13 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		return no_page_table(vma, flags);</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd)) {</span>
<span class="quote">&gt; +retry:</span>
<span class="quote">&gt; +		if (likely(!(flags &amp; FOLL_MIGRATION)))</span>
<span class="quote">&gt; +			return no_page_table(vma, flags);</span>
<span class="quote">&gt; +		pmd_migration_entry_wait(mm, pmd);</span>
<span class="quote">&gt; +		goto retry;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (pmd_devmap(*pmd)) {</span>
<span class="quote">&gt;  		ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  		page = follow_devmap_pmd(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -285,6 +292,15 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="quote">&gt; +retry_locked:</span>
<span class="quote">&gt; +		if (likely(!(flags &amp; FOLL_MIGRATION))) {</span>
<span class="quote">&gt; +			spin_unlock(ptl);</span>
<span class="quote">&gt; +			return no_page_table(vma, flags);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pmd_migration_entry_wait(mm, pmd);</span>
<span class="quote">&gt; +		goto retry_locked;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -340,7 +356,7 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  	pud = pud_offset(pgd, address);</span>
<span class="quote">&gt;  	BUG_ON(pud_none(*pud));</span>
<span class="quote">&gt;  	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; -	if (pmd_none(*pmd))</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="quote">&gt;  	pte = pte_offset_map(pmd, address);</span>
<span class="quote">&gt; @@ -1368,7 +1384,7 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = READ_ONCE(*pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index fd54bbdc16cf..4ac923539372 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -897,6 +897,21 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret = -EAGAIN;</span>
<span class="quote">&gt;  	pmd = *src_pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="quote">&gt; +		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +			make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +			pmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="quote">&gt; +		ret = 0;</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	WARN_ONCE(!pmd_present(pmd), &quot;Uknown non-present format on pmd.\n&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(pmd))) {</span>
<span class="quote">&gt;  		pte_free(dst_mm, pgtable);</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; @@ -1203,6 +1218,9 @@ int do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*vmf-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -1337,7 +1355,15 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; +	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +		page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +		if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +	} else</span>
<span class="quote">&gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);</span>
<span class="quote">&gt;  	if (flags &amp; FOLL_TOUCH)</span>
<span class="quote">&gt;  		touch_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; @@ -1533,6 +1559,9 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (is_huge_zero_pmd(orig_pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If other processes are mapping this page, we couldn&#39;t discard</span>
<span class="quote">&gt; @@ -1659,7 +1688,8 @@ int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt;  			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		if (!migration)</span>
<span class="quote">&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt; @@ -1775,10 +1805,22 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		 * data is likely to be read-cached on the local CPU and</span>
<span class="quote">&gt;  		 * local/remote hits to the zero page are not interesting.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt; -			spin_unlock(ptl);</span>
<span class="quote">&gt; -			return ret;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; +			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +				pmd_t newpmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +		} else if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			WARN_ONCE(1, &quot;Uknown non-present format on pmd.\n&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!prot_numa || !pmd_protnone(*pmd)) {</span>
<span class="quote">&gt;  			entry = pmdp_huge_get_and_clear_notify(mm, addr, pmd);</span>
<span class="quote">&gt; @@ -1790,6 +1832,7 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;</span>
<span class="quote">&gt;  					pmd_write(entry));</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1806,7 +1849,8 @@ spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt;  	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="quote">&gt; -	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="quote">&gt; +	if (likely(is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) ||</span>
<span class="quote">&gt; +			pmd_devmap(*pmd)))</span>
<span class="quote">&gt;  		return ptl;</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt; @@ -1924,7 +1968,7 @@ void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	pgtable_t pgtable;</span>
<span class="quote">&gt;  	pmd_t _pmd;</span>
<span class="quote">&gt; -	bool young, write, dirty, soft_dirty;</span>
<span class="quote">&gt; +	bool young, write, dirty, soft_dirty, pmd_migration;</span>
<span class="quote">&gt;  	unsigned long addr;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; @@ -1932,7 +1976,8 @@ void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  	VM_BUG_ON(haddr &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;  	VM_BUG_ON_VMA(vma-&gt;vm_start &gt; haddr, vma);</span>
<span class="quote">&gt;  	VM_BUG_ON_VMA(vma-&gt;vm_end &lt; haddr + HPAGE_PMD_SIZE, vma);</span>
<span class="quote">&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd));</span>
<span class="quote">&gt; +	VM_BUG_ON(!is_pmd_migration_entry(*pmd) &amp;&amp; !pmd_trans_huge(*pmd)</span>
<span class="quote">&gt; +				&amp;&amp; !pmd_devmap(*pmd));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	count_vm_event(THP_SPLIT_PMD);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1960,7 +2005,14 @@ void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; +	pmd_migration = is_pmd_migration_entry(*pmd);</span>
<span class="quote">&gt; +	if (pmd_migration) {</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +		page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +	} else</span>
<span class="quote">&gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="quote">&gt;  	page_ref_add(page, HPAGE_PMD_NR - 1);</span>
<span class="quote">&gt;  	write = pmd_write(*pmd);</span>
<span class="quote">&gt; @@ -1979,7 +2031,7 @@ void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		 * transferred to avoid any possibility of altering</span>
<span class="quote">&gt;  		 * permissions across VMAs.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (freeze) {</span>
<span class="quote">&gt; +		if (freeze || pmd_migration) {</span>
<span class="quote">&gt;  			swp_entry_t swp_entry;</span>
<span class="quote">&gt;  			swp_entry = make_migration_entry(page + i, write);</span>
<span class="quote">&gt;  			entry = swp_entry_to_pte(swp_entry);</span>
<span class="quote">&gt; @@ -2077,7 +2129,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  		if (PageMlocked(page))</span>
<span class="quote">&gt;  			clear_page_mlock(page);</span>
<span class="quote">&gt; -	} else if (!pmd_devmap(*pmd))</span>
<span class="quote">&gt; +	} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	__split_huge_pmd_locked(vma, pmd, address, freeze);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index e424a06e9f2b..0497a502351f 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -310,6 +310,8 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt;  	if (pmd_trans_huge(*pmd))</span>
<span class="quote">&gt;  		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))</span>
<span class="quote">&gt;  			goto next;</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index 44fb1e80701a..09bce3f0d622 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -4633,6 +4633,8 @@ static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	enum mc_target_type ret = MC_TARGET_NONE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(pmd)))</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt;  	page = pmd_page(pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!page || !PageHead(page), page);</span>
<span class="quote">&gt;  	if (!(mc.flags &amp; MOVE_ANON))</span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index 7cfdd5208ef5..bf10b19e02d3 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -999,7 +999,8 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
<span class="quote">&gt;  	src_pmd = pmd_offset(src_pud, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="quote">&gt; +		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)</span>
<span class="quote">&gt; +			|| pmd_devmap(*src_pmd)) {</span>
<span class="quote">&gt;  			int err;</span>
<span class="quote">&gt;  			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);</span>
<span class="quote">&gt;  			err = copy_huge_pmd(dst_mm, src_mm,</span>
<span class="quote">&gt; @@ -1240,7 +1241,7 @@ static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="quote">&gt; +		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="quote">&gt;  			if (next - addr != HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt;  				VM_BUG_ON_VMA(vma_is_anonymous(vma) &amp;&amp;</span>
<span class="quote">&gt;  				    !rwsem_is_locked(&amp;tlb-&gt;mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt; @@ -3697,6 +3698,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		pmd_t orig_pmd = *vmf.pmd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt; +		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="quote">&gt; +			pmd_migration_entry_wait(mm, vmf.pmd);</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt;  			vmf.flags |= FAULT_FLAG_SIZE_PMD;</span>
<span class="quote">&gt;  			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt; diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="quote">&gt; index 19b460acb5e1..9cb4c83151a8 100644</span>
<span class="quote">&gt; --- a/mm/memory_hotplug.c</span>
<span class="quote">&gt; +++ b/mm/memory_hotplug.c</span>

Changes on mm/memory_hotplug.c should be with patch 14/14?
# If that&#39;s right, definition of hpage_order() also should go to 14/14.

Thanks,
Naoya Horiguchi
<span class="quote">
&gt; @@ -1570,6 +1570,7 @@ static struct page *new_node_page(struct page *page, unsigned long private,</span>
<span class="quote">&gt;  	int nid = page_to_nid(page);</span>
<span class="quote">&gt;  	nodemask_t nmask = node_states[N_MEMORY];</span>
<span class="quote">&gt;  	struct page *new_page = NULL;</span>
<span class="quote">&gt; +	unsigned int order = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * TODO: allocate a destination hugepage from a nearest neighbor node,</span>
<span class="quote">&gt; @@ -1580,6 +1581,11 @@ static struct page *new_node_page(struct page *page, unsigned long private,</span>
<span class="quote">&gt;  		return alloc_huge_page_node(page_hstate(compound_head(page)),</span>
<span class="quote">&gt;  					next_node_in(nid, nmask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (thp_migration_supported() &amp;&amp; PageTransHuge(page)) {</span>
<span class="quote">&gt; +		order = hpage_order(page);</span>
<span class="quote">&gt; +		gfp_mask |= GFP_TRANSHUGE;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	node_clear(nid, nmask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHighMem(page)</span>
<span class="quote">&gt; @@ -1593,6 +1599,9 @@ static struct page *new_node_page(struct page *page, unsigned long private,</span>
<span class="quote">&gt;  		new_page = __alloc_pages(gfp_mask, 0,</span>
<span class="quote">&gt;  					node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (new_page &amp;&amp; order == hpage_order(page))</span>
<span class="quote">&gt; +		prep_transhuge_page(new_page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return new_page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1622,7 +1631,9 @@ do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  			if (isolate_huge_page(page, &amp;source))</span>
<span class="quote">&gt;  				move_pages -= 1 &lt;&lt; compound_order(head);</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		} else if (thp_migration_supported() &amp;&amp; PageTransHuge(page))</span>
<span class="quote">&gt; +			pfn = page_to_pfn(compound_head(page))</span>
<span class="quote">&gt; +				+ hpage_nr_pages(page) - 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!get_page_unless_zero(page))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="quote">&gt; index 5cc6a99918ab..021ff13b9a7a 100644</span>
<span class="quote">&gt; --- a/mm/mempolicy.c</span>
<span class="quote">&gt; +++ b/mm/mempolicy.c</span>
<span class="quote">&gt; @@ -94,6 +94,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/mm_inline.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mmu_notifier.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/printk.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/uaccess.h&gt;</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index 98acf7d5cef2..bfbe66798a7a 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -150,7 +150,9 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		unsigned long this_pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		if (!is_swap_pmd(*pmd) &amp;&amp; !pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="quote">&gt;  				&amp;&amp; pmd_none_or_clear_bad(pmd))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -160,7 +162,7 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			mmu_notifier_invalidate_range_start(mm, mni_start, end);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="quote">&gt; +		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="quote">&gt;  			if (next - addr != HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt;  				__split_huge_pmd(vma, pmd, addr, false, NULL);</span>
<span class="quote">&gt;  				if (pmd_trans_unstable(pmd))</span>
<span class="quote">&gt; diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="quote">&gt; index 8233b0105c82..5d537ce12adc 100644</span>
<span class="quote">&gt; --- a/mm/mremap.c</span>
<span class="quote">&gt; +++ b/mm/mremap.c</span>
<span class="quote">&gt; @@ -213,7 +213,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);</span>
<span class="quote">&gt;  		if (!new_pmd)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="quote">&gt; +		if (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {</span>
<span class="quote">&gt;  			if (extent == HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt;  				bool moved;</span>
<span class="quote">&gt;  				/* See comment in move_ptes() */</span>
<span class="quote">&gt; diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="quote">&gt; index 03761577ae86..114fc2b5a370 100644</span>
<span class="quote">&gt; --- a/mm/pagewalk.c</span>
<span class="quote">&gt; +++ b/mm/pagewalk.c</span>
<span class="quote">&gt; @@ -2,6 +2,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/hugetlb.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  			  struct mm_walk *walk)</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123671">Zi Yan</a> - Feb. 9, 2017, 5:36 p.m.</div>
<pre class="content">
On 9 Feb 2017, at 3:16, Naoya Horiguchi wrote:
<span class="quote">
&gt; On Sun, Feb 05, 2017 at 11:12:47AM -0500, Zi Yan wrote:</span>
<span class="quote">&gt;&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If one of callers of page migration starts to handle thp,</span>
<span class="quote">&gt;&gt; memory management code start to see pmd migration entry, so we need</span>
<span class="quote">&gt;&gt; to prepare for it before enabling. This patch changes various code</span>
<span class="quote">&gt;&gt; point which checks the status of given pmds in order to prevent race</span>
<span class="quote">&gt;&gt; between thp migration and the pmd-related works.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt;&gt; - introduce pmd_related() (I know the naming is not good, but can&#39;t</span>
<span class="quote">&gt;&gt;   think up no better name. Any suggesntion is welcomed.)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt;&gt; - add is_swap_pmd()</span>
<span class="quote">&gt;&gt; - a pmd entry should be is_swap_pmd(), pmd_trans_huge(), pmd_devmap(),</span>
<span class="quote">&gt;&gt;   or pmd_none()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; (nitpick) ... or normal pmd pointing to pte pages?</span>

Sure, I will add it.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear()</span>
<span class="quote">&gt;&gt; - flush_cache_range() while set_pmd_migration_entry()</span>
<span class="quote">&gt;&gt; - pmd_none_or_trans_huge_or_clear_bad() and pmd_trans_unstable() return</span>
<span class="quote">&gt;&gt;   true on pmd_migration_entry, so that migration entries are not</span>
<span class="quote">&gt;&gt;   treated as pmd page table entries.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/mm/gup.c             |  4 +--</span>
<span class="quote">&gt;&gt;  fs/proc/task_mmu.c            | 22 ++++++++-----</span>
<span class="quote">&gt;&gt;  include/asm-generic/pgtable.h | 71 ----------------------------------------</span>
<span class="quote">&gt;&gt;  include/linux/huge_mm.h       | 21 ++++++++++--</span>
<span class="quote">&gt;&gt;  include/linux/swapops.h       | 74 +++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;  mm/gup.c                      | 20 ++++++++++--</span>
<span class="quote">&gt;&gt;  mm/huge_memory.c              | 76 ++++++++++++++++++++++++++++++++++++-------</span>
<span class="quote">&gt;&gt;  mm/madvise.c                  |  2 ++</span>
<span class="quote">&gt;&gt;  mm/memcontrol.c               |  2 ++</span>
<span class="quote">&gt;&gt;  mm/memory.c                   |  9 +++--</span>
<span class="quote">&gt;&gt;  mm/memory_hotplug.c           | 13 +++++++-</span>
<span class="quote">&gt;&gt;  mm/mempolicy.c                |  1 +</span>
<span class="quote">&gt;&gt;  mm/mprotect.c                 |  6 ++--</span>
<span class="quote">&gt;&gt;  mm/mremap.c                   |  2 +-</span>
<span class="quote">&gt;&gt;  mm/pagewalk.c                 |  2 ++</span>
<span class="quote">&gt;&gt;  15 files changed, 221 insertions(+), 104 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
<span class="quote">&gt;&gt; index 0d4fb3ebbbac..78a153d90064 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/gup.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/gup.c</span>
<span class="quote">&gt;&gt; @@ -222,9 +222,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt;&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;&gt;  			return 0;</span>
<span class="quote">&gt;&gt; -		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt;&gt; +		if (unlikely(pmd_large(pmd))) {</span>
<span class="quote">&gt;&gt;  			/*</span>
<span class="quote">&gt;&gt;  			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="quote">&gt;&gt;  			 * slowpath for accounting purposes and so that they</span>
<span class="quote">&gt;&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; index 6c07c7813b26..1e64d6898c68 100644</span>
<span class="quote">&gt;&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; @@ -596,7 +596,8 @@ static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  	ptl = pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;&gt;  	if (ptl) {</span>
<span class="quote">&gt;&gt; -		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt;&gt; +		if (pmd_present(*pmd))</span>
<span class="quote">&gt;&gt; +			smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt;  		return 0;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt; @@ -929,6 +930,9 @@ static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;&gt;  			goto out;</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt;&gt; +			goto out;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  		/* Clear accessed and referenced bits. */</span>
<span class="quote">&gt;&gt; @@ -1208,19 +1212,19 @@ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;  	if (ptl) {</span>
<span class="quote">&gt;&gt;  		u64 flags = 0, frame = 0;</span>
<span class="quote">&gt;&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt;&gt; +		struct page *page;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))</span>
<span class="quote">&gt;&gt;  			flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * Currently pmd for thp is always present because thp</span>
<span class="quote">&gt;&gt; -		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="quote">&gt;&gt; -		 * (split in such cases instead.)</span>
<span class="quote">&gt;&gt; -		 * This if-check is just to prepare for future implementation.</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		if (pmd_present(pmd)) {</span>
<span class="quote">&gt;&gt; -			struct page *page = pmd_page(pmd);</span>
<span class="quote">&gt;&gt; +		if (is_pmd_migration_entry(pmd)) {</span>
<span class="quote">&gt;&gt; +			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +			frame = swp_type(entry) |</span>
<span class="quote">&gt;&gt; +				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt;&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt;&gt; +		} else if (pmd_present(pmd)) {</span>
<span class="quote">&gt;&gt; +			page = pmd_page(pmd);</span>
<span class="quote">&gt;&gt;  			if (page_mapcount(page) == 1)</span>
<span class="quote">&gt;&gt;  				flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; index b71a431ed649..6cf9e9b5a7be 100644</span>
<span class="quote">&gt;&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -726,77 +726,6 @@ static inline pmd_t pmd_read_atomic(pmd_t *pmdp)</span>
<span class="quote">&gt;&gt;  #ifndef arch_needs_pgtable_deposit</span>
<span class="quote">&gt;&gt;  #define arch_needs_pgtable_deposit() (false)</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * This function is meant to be used by sites walking pagetables with</span>
<span class="quote">&gt;&gt; - * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="quote">&gt;&gt; - * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="quote">&gt;&gt; - * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="quote">&gt;&gt; - * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="quote">&gt;&gt; - * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="quote">&gt;&gt; - * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="quote">&gt;&gt; - * transhuge pmd. When those races occurs and this function makes a</span>
<span class="quote">&gt;&gt; - * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="quote">&gt;&gt; - * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="quote">&gt;&gt; - * can return none anyway). The compiler level barrier() is critically</span>
<span class="quote">&gt;&gt; - * important to compute the two checks atomically on the same pmdval.</span>
<span class="quote">&gt;&gt; - *</span>
<span class="quote">&gt;&gt; - * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="quote">&gt;&gt; - * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="quote">&gt;&gt; - * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="quote">&gt;&gt; - * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="quote">&gt;&gt; - * version above, is also needed when THP is disabled because the page</span>
<span class="quote">&gt;&gt; - * fault can populate the pmd from under us).</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="quote">&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt; -	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="quote">&gt;&gt; -	 * the stack so that it will stop changing under the code.</span>
<span class="quote">&gt;&gt; -	 *</span>
<span class="quote">&gt;&gt; -	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="quote">&gt;&gt; -	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="quote">&gt;&gt; -	 * (for example pointing to an hugepage that has never been</span>
<span class="quote">&gt;&gt; -	 * mapped in the pmd). The below checks will only care about</span>
<span class="quote">&gt;&gt; -	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="quote">&gt;&gt; -	 * exception of pmd_none(). So the important thing is that if</span>
<span class="quote">&gt;&gt; -	 * the low part of the pmd is found null, the high part will</span>
<span class="quote">&gt;&gt; -	 * be also null or the pmd_none() check below would be</span>
<span class="quote">&gt;&gt; -	 * confused.</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; -#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; -	barrier();</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -	if (pmd_none(pmdval) || pmd_trans_huge(pmdval))</span>
<span class="quote">&gt;&gt; -		return 1;</span>
<span class="quote">&gt;&gt; -	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="quote">&gt;&gt; -		pmd_clear_bad(pmd);</span>
<span class="quote">&gt;&gt; -		return 1;</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; -	return 0;</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="quote">&gt;&gt; - * the kernel. Otherwise it is equivalent to</span>
<span class="quote">&gt;&gt; - * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="quote">&gt;&gt; - * places that already verified the pmd is not none and they want to</span>
<span class="quote">&gt;&gt; - * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="quote">&gt;&gt; - * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="quote">&gt;&gt; - * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="quote">&gt;&gt; - * run a pmd_trans_unstable before walking the ptes after</span>
<span class="quote">&gt;&gt; - * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="quote">&gt;&gt; - * become null, but then a page fault can map in a THP and not a</span>
<span class="quote">&gt;&gt; - * regular page).</span>
<span class="quote">&gt;&gt; - */</span>
<span class="quote">&gt;&gt; -static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; -	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="quote">&gt;&gt; -#else</span>
<span class="quote">&gt;&gt; -	return 0;</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #ifndef CONFIG_NUMA_BALANCING</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="quote">&gt;&gt; index 83a8d42f9d55..c2e5a4eab84a 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/huge_mm.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/huge_mm.h</span>
<span class="quote">&gt;&gt; @@ -131,7 +131,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;&gt;  #define split_huge_pmd(__vma, __pmd, __address)				\</span>
<span class="quote">&gt;&gt;  	do {								\</span>
<span class="quote">&gt;&gt;  		pmd_t *____pmd = (__pmd);				\</span>
<span class="quote">&gt;&gt; -		if (pmd_trans_huge(*____pmd)				\</span>
<span class="quote">&gt;&gt; +		if (is_swap_pmd(*____pmd) || pmd_trans_huge(*____pmd)	\</span>
<span class="quote">&gt;&gt;  					|| pmd_devmap(*____pmd))	\</span>
<span class="quote">&gt;&gt;  			__split_huge_pmd(__vma, __pmd, __address,	\</span>
<span class="quote">&gt;&gt;  						false, NULL);		\</span>
<span class="quote">&gt;&gt; @@ -162,12 +162,18 @@ extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt;&gt;  extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,</span>
<span class="quote">&gt;&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return !pmd_none(pmd) &amp;&amp; !pmd_present(pmd);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /* mmap_sem must be held on entry */</span>
<span class="quote">&gt;&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt;&gt; -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt;&gt; +	if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt;&gt;  		return __pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;&gt;  	else</span>
<span class="quote">&gt;&gt;  		return NULL;</span>
<span class="quote">&gt;&gt; @@ -192,6 +198,12 @@ struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;&gt;  		pmd_t *pmd, int flags);</span>
<span class="quote">&gt;&gt;  struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;&gt;  		pud_t *pud, int flags);</span>
<span class="quote">&gt;&gt; +static inline int hpage_order(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	if (unlikely(PageTransHuge(page)))</span>
<span class="quote">&gt;&gt; +		return HPAGE_PMD_ORDER;</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -232,6 +244,7 @@ static inline bool thp_migration_supported(void)</span>
<span class="quote">&gt;&gt;  #define HPAGE_PUD_SIZE ({ BUILD_BUG(); 0; })</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define hpage_nr_pages(x) 1</span>
<span class="quote">&gt;&gt; +#define hpage_order(x) 0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define transparent_hugepage_enabled(__vma) 0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -274,6 +287,10 @@ static inline void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  					 long adjust_next)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; +static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; index 6625bea13869..50e4aa7e7ff9 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; @@ -229,6 +229,80 @@ static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * This function is meant to be used by sites walking pagetables with</span>
<span class="quote">&gt;&gt; + * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="quote">&gt;&gt; + * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="quote">&gt;&gt; + * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="quote">&gt;&gt; + * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="quote">&gt;&gt; + * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="quote">&gt;&gt; + * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="quote">&gt;&gt; + * transhuge pmd. When those races occurs and this function makes a</span>
<span class="quote">&gt;&gt; + * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="quote">&gt;&gt; + * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="quote">&gt;&gt; + * can return none anyway). The compiler level barrier() is critically</span>
<span class="quote">&gt;&gt; + * important to compute the two checks atomically on the same pmdval.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="quote">&gt;&gt; + * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="quote">&gt;&gt; + * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="quote">&gt;&gt; + * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="quote">&gt;&gt; + * version above, is also needed when THP is disabled because the page</span>
<span class="quote">&gt;&gt; + * fault can populate the pmd from under us).</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="quote">&gt;&gt; +	 * the stack so that it will stop changing under the code.</span>
<span class="quote">&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt; +	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="quote">&gt;&gt; +	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="quote">&gt;&gt; +	 * (for example pointing to an hugepage that has never been</span>
<span class="quote">&gt;&gt; +	 * mapped in the pmd). The below checks will only care about</span>
<span class="quote">&gt;&gt; +	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="quote">&gt;&gt; +	 * exception of pmd_none(). So the important thing is that if</span>
<span class="quote">&gt;&gt; +	 * the low part of the pmd is found null, the high part will</span>
<span class="quote">&gt;&gt; +	 * be also null or the pmd_none() check below would be</span>
<span class="quote">&gt;&gt; +	 * confused.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; +	barrier();</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +	if (pmd_none(pmdval) || pmd_trans_huge(pmdval)</span>
<span class="quote">&gt;&gt; +			|| is_pmd_migration_entry(pmdval))</span>
<span class="quote">&gt;&gt; +		return 1;</span>
<span class="quote">&gt;&gt; +	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="quote">&gt;&gt; +		pmd_clear_bad(pmd);</span>
<span class="quote">&gt;&gt; +		return 1;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="quote">&gt;&gt; + * the kernel. Otherwise it is equivalent to</span>
<span class="quote">&gt;&gt; + * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="quote">&gt;&gt; + * places that already verified the pmd is not none and they want to</span>
<span class="quote">&gt;&gt; + * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="quote">&gt;&gt; + * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="quote">&gt;&gt; + * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="quote">&gt;&gt; + * run a pmd_trans_unstable before walking the ptes after</span>
<span class="quote">&gt;&gt; + * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="quote">&gt;&gt; + * become null, but then a page fault can map in a THP and not a</span>
<span class="quote">&gt;&gt; + * regular page).</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; +	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; These functions are page table or thp matter, so putting them in swapops.h</span>
<span class="quote">&gt; looks weird to me. Maybe you can avoid this code transfer by using !pmd_present</span>
<span class="quote">&gt; instead of is_pmd_migration_entry?</span>
<span class="quote">&gt; And we have to consider renaming pmd_none_or_trans_huge_or_clear_bad(),</span>
<span class="quote">&gt; I like a simple name like __pmd_trans_unstable(), but if you have an idea,</span>
<span class="quote">&gt; that&#39;s great.</span>

Yes. I will move it back.

I am not sure if it is OK to only use !pmd_present. We may miss some pmd_bad.

Kirill and Andrea, can you give some insight on this?
<span class="quote">

&gt;&gt; diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="quote">&gt;&gt; index 19b460acb5e1..9cb4c83151a8 100644</span>
<span class="quote">&gt;&gt; --- a/mm/memory_hotplug.c</span>
<span class="quote">&gt;&gt; +++ b/mm/memory_hotplug.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Changes on mm/memory_hotplug.c should be with patch 14/14?</span>
<span class="quote">&gt; # If that&#39;s right, definition of hpage_order() also should go to 14/14.</span>

Got it. I will move it.


--
Best Regards
Yan Zi
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
<span class="p_header">index 0d4fb3ebbbac..78a153d90064 100644</span>
<span class="p_header">--- a/arch/x86/mm/gup.c</span>
<span class="p_header">+++ b/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -222,9 +222,9 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		pmd_t pmd = *pmdp;
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_none(pmd))</span>
<span class="p_add">+		if (!pmd_present(pmd))</span>
 			return 0;
<span class="p_del">-		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="p_add">+		if (unlikely(pmd_large(pmd))) {</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 6c07c7813b26..1e64d6898c68 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -596,7 +596,8 @@</span> <span class="p_context"> static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 
 	ptl = pmd_trans_huge_lock(pmd, vma);
 	if (ptl) {
<span class="p_del">-		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="p_add">+		if (pmd_present(*pmd))</span>
<span class="p_add">+			smaps_pmd_entry(pmd, addr, walk);</span>
 		spin_unlock(ptl);
 		return 0;
 	}
<span class="p_chunk">@@ -929,6 +930,9 @@</span> <span class="p_context"> static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			goto out;
 		}
 
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+</span>
 		page = pmd_page(*pmd);
 
 		/* Clear accessed and referenced bits. */
<span class="p_chunk">@@ -1208,19 +1212,19 @@</span> <span class="p_context"> static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
 	if (ptl) {
 		u64 flags = 0, frame = 0;
 		pmd_t pmd = *pmdp;
<span class="p_add">+		struct page *page;</span>
 
 		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))
 			flags |= PM_SOFT_DIRTY;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Currently pmd for thp is always present because thp</span>
<span class="p_del">-		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="p_del">-		 * (split in such cases instead.)</span>
<span class="p_del">-		 * This if-check is just to prepare for future implementation.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (pmd_present(pmd)) {</span>
<span class="p_del">-			struct page *page = pmd_page(pmd);</span>
<span class="p_add">+		if (is_pmd_migration_entry(pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
 
<span class="p_add">+			frame = swp_type(entry) |</span>
<span class="p_add">+				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+		} else if (pmd_present(pmd)) {</span>
<span class="p_add">+			page = pmd_page(pmd);</span>
 			if (page_mapcount(page) == 1)
 				flags |= PM_MMAP_EXCLUSIVE;
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index b71a431ed649..6cf9e9b5a7be 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -726,77 +726,6 @@</span> <span class="p_context"> static inline pmd_t pmd_read_atomic(pmd_t *pmdp)</span>
 #ifndef arch_needs_pgtable_deposit
 #define arch_needs_pgtable_deposit() (false)
 #endif
<span class="p_del">-/*</span>
<span class="p_del">- * This function is meant to be used by sites walking pagetables with</span>
<span class="p_del">- * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="p_del">- * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="p_del">- * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="p_del">- * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="p_del">- * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="p_del">- * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="p_del">- * transhuge pmd. When those races occurs and this function makes a</span>
<span class="p_del">- * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="p_del">- * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="p_del">- * can return none anyway). The compiler level barrier() is critically</span>
<span class="p_del">- * important to compute the two checks atomically on the same pmdval.</span>
<span class="p_del">- *</span>
<span class="p_del">- * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="p_del">- * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="p_del">- * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="p_del">- * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="p_del">- * version above, is also needed when THP is disabled because the page</span>
<span class="p_del">- * fault can populate the pmd from under us).</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="p_del">-	 * the stack so that it will stop changing under the code.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="p_del">-	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="p_del">-	 * (for example pointing to an hugepage that has never been</span>
<span class="p_del">-	 * mapped in the pmd). The below checks will only care about</span>
<span class="p_del">-	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="p_del">-	 * exception of pmd_none(). So the important thing is that if</span>
<span class="p_del">-	 * the low part of the pmd is found null, the high part will</span>
<span class="p_del">-	 * be also null or the pmd_none() check below would be</span>
<span class="p_del">-	 * confused.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	if (pmd_none(pmdval) || pmd_trans_huge(pmdval))</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="p_del">-		pmd_clear_bad(pmd);</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="p_del">- * the kernel. Otherwise it is equivalent to</span>
<span class="p_del">- * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="p_del">- * places that already verified the pmd is not none and they want to</span>
<span class="p_del">- * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="p_del">- * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="p_del">- * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="p_del">- * run a pmd_trans_unstable before walking the ptes after</span>
<span class="p_del">- * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="p_del">- * become null, but then a page fault can map in a THP and not a</span>
<span class="p_del">- * regular page).</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
 
 #ifndef CONFIG_NUMA_BALANCING
 /*
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index 83a8d42f9d55..c2e5a4eab84a 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -131,7 +131,7 @@</span> <span class="p_context"> void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 #define split_huge_pmd(__vma, __pmd, __address)				\
 	do {								\
 		pmd_t *____pmd = (__pmd);				\
<span class="p_del">-		if (pmd_trans_huge(*____pmd)				\</span>
<span class="p_add">+		if (is_swap_pmd(*____pmd) || pmd_trans_huge(*____pmd)	\</span>
 					|| pmd_devmap(*____pmd))	\
 			__split_huge_pmd(__vma, __pmd, __address,	\
 						false, NULL);		\
<span class="p_chunk">@@ -162,12 +162,18 @@</span> <span class="p_context"> extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
 		struct vm_area_struct *vma);
 extern spinlock_t *__pud_trans_huge_lock(pud_t *pud,
 		struct vm_area_struct *vma);
<span class="p_add">+</span>
<span class="p_add">+static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_none(pmd) &amp;&amp; !pmd_present(pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* mmap_sem must be held on entry */
 static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma)
 {
 	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);
<span class="p_del">-	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="p_add">+	if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
 		return __pmd_trans_huge_lock(pmd, vma);
 	else
 		return NULL;
<span class="p_chunk">@@ -192,6 +198,12 @@</span> <span class="p_context"> struct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
 		pmd_t *pmd, int flags);
 struct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,
 		pud_t *pud, int flags);
<span class="p_add">+static inline int hpage_order(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (unlikely(PageTransHuge(page)))</span>
<span class="p_add">+		return HPAGE_PMD_ORDER;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
 extern int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t orig_pmd);
 
<span class="p_chunk">@@ -232,6 +244,7 @@</span> <span class="p_context"> static inline bool thp_migration_supported(void)</span>
 #define HPAGE_PUD_SIZE ({ BUILD_BUG(); 0; })
 
 #define hpage_nr_pages(x) 1
<span class="p_add">+#define hpage_order(x) 0</span>
 
 #define transparent_hugepage_enabled(__vma) 0
 
<span class="p_chunk">@@ -274,6 +287,10 @@</span> <span class="p_context"> static inline void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 					 long adjust_next)
 {
 }
<span class="p_add">+static inline int is_swap_pmd(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma)
 {
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 6625bea13869..50e4aa7e7ff9 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -229,6 +229,80 @@</span> <span class="p_context"> static inline int is_pmd_migration_entry(pmd_t pmd)</span>
 }
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * This function is meant to be used by sites walking pagetables with</span>
<span class="p_add">+ * the mmap_sem hold in read mode to protect against MADV_DONTNEED and</span>
<span class="p_add">+ * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd</span>
<span class="p_add">+ * into a null pmd and the transhuge page fault can convert a null pmd</span>
<span class="p_add">+ * into an hugepmd or into a regular pmd (if the hugepage allocation</span>
<span class="p_add">+ * fails). While holding the mmap_sem in read mode the pmd becomes</span>
<span class="p_add">+ * stable and stops changing under us only if it&#39;s not null and not a</span>
<span class="p_add">+ * transhuge pmd. When those races occurs and this function makes a</span>
<span class="p_add">+ * difference vs the standard pmd_none_or_clear_bad, the result is</span>
<span class="p_add">+ * undefined so behaving like if the pmd was none is safe (because it</span>
<span class="p_add">+ * can return none anyway). The compiler level barrier() is critically</span>
<span class="p_add">+ * important to compute the two checks atomically on the same pmdval.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For 32bit kernels with a 64bit large pmd_t this automatically takes</span>
<span class="p_add">+ * care of reading the pmd atomically to avoid SMP race conditions</span>
<span class="p_add">+ * against pmd_populate() when the mmap_sem is hold for reading by the</span>
<span class="p_add">+ * caller (a special atomic read not done by &quot;gcc&quot; as in the generic</span>
<span class="p_add">+ * version above, is also needed when THP is disabled because the page</span>
<span class="p_add">+ * fault can populate the pmd from under us).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t pmdval = pmd_read_atomic(pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The barrier will stabilize the pmdval in a register or on</span>
<span class="p_add">+	 * the stack so that it will stop changing under the code.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,</span>
<span class="p_add">+	 * pmd_read_atomic is allowed to return a not atomic pmdval</span>
<span class="p_add">+	 * (for example pointing to an hugepage that has never been</span>
<span class="p_add">+	 * mapped in the pmd). The below checks will only care about</span>
<span class="p_add">+	 * the low part of the pmd with 32bit PAE x86 anyway, with the</span>
<span class="p_add">+	 * exception of pmd_none(). So the important thing is that if</span>
<span class="p_add">+	 * the low part of the pmd is found null, the high part will</span>
<span class="p_add">+	 * be also null or the pmd_none() check below would be</span>
<span class="p_add">+	 * confused.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	if (pmd_none(pmdval) || pmd_trans_huge(pmdval)</span>
<span class="p_add">+			|| is_pmd_migration_entry(pmdval))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	if (unlikely(pmd_bad(pmdval))) {</span>
<span class="p_add">+		pmd_clear_bad(pmd);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a noop if Transparent Hugepage Support is not built into</span>
<span class="p_add">+ * the kernel. Otherwise it is equivalent to</span>
<span class="p_add">+ * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in</span>
<span class="p_add">+ * places that already verified the pmd is not none and they want to</span>
<span class="p_add">+ * walk ptes while holding the mmap sem in read mode (write mode don&#39;t</span>
<span class="p_add">+ * need this). If THP is not enabled, the pmd can&#39;t go away under the</span>
<span class="p_add">+ * code even if MADV_DONTNEED runs, but if THP is enabled we need to</span>
<span class="p_add">+ * run a pmd_trans_unstable before walking the ptes after</span>
<span class="p_add">+ * split_huge_page_pmd returns (because it may have run when the pmd</span>
<span class="p_add">+ * become null, but then a page fault can map in a THP and not a</span>
<span class="p_add">+ * regular page).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int pmd_trans_unstable(pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+	return pmd_none_or_trans_huge_or_clear_bad(pmd);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MEMORY_FAILURE
 
 extern atomic_long_t num_poisoned_pages __read_mostly;
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 1e67461b2733..82e0304e5d29 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -274,6 +274,13 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 	}
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		return no_page_table(vma, flags);
<span class="p_add">+	if (!pmd_present(*pmd)) {</span>
<span class="p_add">+retry:</span>
<span class="p_add">+		if (likely(!(flags &amp; FOLL_MIGRATION)))</span>
<span class="p_add">+			return no_page_table(vma, flags);</span>
<span class="p_add">+		pmd_migration_entry_wait(mm, pmd);</span>
<span class="p_add">+		goto retry;</span>
<span class="p_add">+	}</span>
 	if (pmd_devmap(*pmd)) {
 		ptl = pmd_lock(mm, pmd);
 		page = follow_devmap_pmd(vma, address, pmd, flags);
<span class="p_chunk">@@ -285,6 +292,15 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 		return follow_page_pte(vma, address, pmd, flags);
 
 	ptl = pmd_lock(mm, pmd);
<span class="p_add">+	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="p_add">+retry_locked:</span>
<span class="p_add">+		if (likely(!(flags &amp; FOLL_MIGRATION))) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			return no_page_table(vma, flags);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pmd_migration_entry_wait(mm, pmd);</span>
<span class="p_add">+		goto retry_locked;</span>
<span class="p_add">+	}</span>
 	if (unlikely(!pmd_trans_huge(*pmd))) {
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
<span class="p_chunk">@@ -340,7 +356,7 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 	pud = pud_offset(pgd, address);
 	BUG_ON(pud_none(*pud));
 	pmd = pmd_offset(pud, address);
<span class="p_del">-	if (pmd_none(*pmd))</span>
<span class="p_add">+	if (!pmd_present(*pmd))</span>
 		return -EFAULT;
 	VM_BUG_ON(pmd_trans_huge(*pmd));
 	pte = pte_offset_map(pmd, address);
<span class="p_chunk">@@ -1368,7 +1384,7 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		pmd_t pmd = READ_ONCE(*pmdp);
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_none(pmd))</span>
<span class="p_add">+		if (!pmd_present(pmd))</span>
 			return 0;
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index fd54bbdc16cf..4ac923539372 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -897,6 +897,21 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 
 	ret = -EAGAIN;
 	pmd = *src_pmd;
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="p_add">+		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+			make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+			pmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	WARN_ONCE(!pmd_present(pmd), &quot;Uknown non-present format on pmd.\n&quot;);</span>
<span class="p_add">+</span>
 	if (unlikely(!pmd_trans_huge(pmd))) {
 		pte_free(dst_mm, pgtable);
 		goto out_unlock;
<span class="p_chunk">@@ -1203,6 +1218,9 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)</span>
 	if (unlikely(!pmd_same(*vmf-&gt;pmd, orig_pmd)))
 		goto out_unlock;
 
<span class="p_add">+	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);
 	/*
<span class="p_chunk">@@ -1337,7 +1355,15 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		goto out;
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+		if (!is_migration_entry(entry))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
 	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);
 	if (flags &amp; FOLL_TOUCH)
 		touch_pmd(vma, addr, pmd);
<span class="p_chunk">@@ -1533,6 +1559,9 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	if (is_huge_zero_pmd(orig_pmd))
 		goto out;
 
<span class="p_add">+	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	/*
 	 * If other processes are mapping this page, we couldn&#39;t discard
<span class="p_chunk">@@ -1659,7 +1688,8 @@</span> <span class="p_context"> int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 			free_swap_and_cache(entry); /* waring in failure? */
 			migration = 1;
 		}
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 
 	return 1;
<span class="p_chunk">@@ -1775,10 +1805,22 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		 * data is likely to be read-cached on the local CPU and
 		 * local/remote hits to the zero page are not interesting.
 		 */
<span class="p_del">-		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="p_del">-			spin_unlock(ptl);</span>
<span class="p_del">-			return ret;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+				pmd_t newpmd;</span>
<span class="p_add">+</span>
<span class="p_add">+				make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+				newpmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+		} else if (!pmd_present(*pmd))</span>
<span class="p_add">+			WARN_ONCE(1, &quot;Uknown non-present format on pmd.\n&quot;);</span>
 
 		if (!prot_numa || !pmd_protnone(*pmd)) {
 			entry = pmdp_huge_get_and_clear_notify(mm, addr, pmd);
<span class="p_chunk">@@ -1790,6 +1832,7 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;
 					pmd_write(entry));
 		}
<span class="p_add">+unlock:</span>
 		spin_unlock(ptl);
 	}
 
<span class="p_chunk">@@ -1806,7 +1849,8 @@</span> <span class="p_context"> spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
 {
 	spinlock_t *ptl;
 	ptl = pmd_lock(vma-&gt;vm_mm, pmd);
<span class="p_del">-	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="p_add">+	if (likely(is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) ||</span>
<span class="p_add">+			pmd_devmap(*pmd)))</span>
 		return ptl;
 	spin_unlock(ptl);
 	return NULL;
<span class="p_chunk">@@ -1924,7 +1968,7 @@</span> <span class="p_context"> void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	struct page *page;
 	pgtable_t pgtable;
 	pmd_t _pmd;
<span class="p_del">-	bool young, write, dirty, soft_dirty;</span>
<span class="p_add">+	bool young, write, dirty, soft_dirty, pmd_migration;</span>
 	unsigned long addr;
 	int i;
 	unsigned long haddr = address &amp; HPAGE_PMD_MASK;
<span class="p_chunk">@@ -1932,7 +1976,8 @@</span> <span class="p_context"> void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	VM_BUG_ON(haddr &amp; ~HPAGE_PMD_MASK);
 	VM_BUG_ON_VMA(vma-&gt;vm_start &gt; haddr, vma);
 	VM_BUG_ON_VMA(vma-&gt;vm_end &lt; haddr + HPAGE_PMD_SIZE, vma);
<span class="p_del">-	VM_BUG_ON(!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd));</span>
<span class="p_add">+	VM_BUG_ON(!is_pmd_migration_entry(*pmd) &amp;&amp; !pmd_trans_huge(*pmd)</span>
<span class="p_add">+				&amp;&amp; !pmd_devmap(*pmd));</span>
 
 	count_vm_event(THP_SPLIT_PMD);
 
<span class="p_chunk">@@ -1960,7 +2005,14 @@</span> <span class="p_context"> void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		goto out;
 	}
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	pmd_migration = is_pmd_migration_entry(*pmd);</span>
<span class="p_add">+	if (pmd_migration) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
 	VM_BUG_ON_PAGE(!page_count(page), page);
 	page_ref_add(page, HPAGE_PMD_NR - 1);
 	write = pmd_write(*pmd);
<span class="p_chunk">@@ -1979,7 +2031,7 @@</span> <span class="p_context"> void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		 * transferred to avoid any possibility of altering
 		 * permissions across VMAs.
 		 */
<span class="p_del">-		if (freeze) {</span>
<span class="p_add">+		if (freeze || pmd_migration) {</span>
 			swp_entry_t swp_entry;
 			swp_entry = make_migration_entry(page + i, write);
 			entry = swp_entry_to_pte(swp_entry);
<span class="p_chunk">@@ -2077,7 +2129,7 @@</span> <span class="p_context"> void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		page = pmd_page(*pmd);
 		if (PageMlocked(page))
 			clear_page_mlock(page);
<span class="p_del">-	} else if (!pmd_devmap(*pmd))</span>
<span class="p_add">+	} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))</span>
 		goto out;
 	__split_huge_pmd_locked(vma, pmd, address, freeze);
 out:
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index e424a06e9f2b..0497a502351f 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -310,6 +310,8 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	unsigned long next;
 
 	next = pmd_addr_end(addr, end);
<span class="p_add">+	if (!pmd_present(*pmd))</span>
<span class="p_add">+		return 0;</span>
 	if (pmd_trans_huge(*pmd))
 		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))
 			goto next;
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 44fb1e80701a..09bce3f0d622 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4633,6 +4633,8 @@</span> <span class="p_context"> static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 	struct page *page = NULL;
 	enum mc_target_type ret = MC_TARGET_NONE;
 
<span class="p_add">+	if (unlikely(!pmd_present(pmd)))</span>
<span class="p_add">+		return ret;</span>
 	page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!page || !PageHead(page), page);
 	if (!(mc.flags &amp; MOVE_ANON))
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 7cfdd5208ef5..bf10b19e02d3 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -999,7 +999,8 @@</span> <span class="p_context"> static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 	src_pmd = pmd_offset(src_pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="p_add">+		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)</span>
<span class="p_add">+			|| pmd_devmap(*src_pmd)) {</span>
 			int err;
 			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
 			err = copy_huge_pmd(dst_mm, src_mm,
<span class="p_chunk">@@ -1240,7 +1241,7 @@</span> <span class="p_context"> static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,</span>
 	ptl = pmd_lock(vma-&gt;vm_mm, pmd);
 	do {
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="p_add">+		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
 			if (next - addr != HPAGE_PMD_SIZE) {
 				VM_BUG_ON_VMA(vma_is_anonymous(vma) &amp;&amp;
 				    !rwsem_is_locked(&amp;tlb-&gt;mm-&gt;mmap_sem), vma);
<span class="p_chunk">@@ -3697,6 +3698,10 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		pmd_t orig_pmd = *vmf.pmd;
 
 		barrier();
<span class="p_add">+		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="p_add">+			pmd_migration_entry_wait(mm, vmf.pmd);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 			vmf.flags |= FAULT_FLAG_SIZE_PMD;
 			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index 19b460acb5e1..9cb4c83151a8 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -1570,6 +1570,7 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long private,</span>
 	int nid = page_to_nid(page);
 	nodemask_t nmask = node_states[N_MEMORY];
 	struct page *new_page = NULL;
<span class="p_add">+	unsigned int order = 0;</span>
 
 	/*
 	 * TODO: allocate a destination hugepage from a nearest neighbor node,
<span class="p_chunk">@@ -1580,6 +1581,11 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long private,</span>
 		return alloc_huge_page_node(page_hstate(compound_head(page)),
 					next_node_in(nid, nmask));
 
<span class="p_add">+	if (thp_migration_supported() &amp;&amp; PageTransHuge(page)) {</span>
<span class="p_add">+		order = hpage_order(page);</span>
<span class="p_add">+		gfp_mask |= GFP_TRANSHUGE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	node_clear(nid, nmask);
 
 	if (PageHighMem(page)
<span class="p_chunk">@@ -1593,6 +1599,9 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long private,</span>
 		new_page = __alloc_pages(gfp_mask, 0,
 					node_zonelist(nid, gfp_mask));
 
<span class="p_add">+	if (new_page &amp;&amp; order == hpage_order(page))</span>
<span class="p_add">+		prep_transhuge_page(new_page);</span>
<span class="p_add">+</span>
 	return new_page;
 }
 
<span class="p_chunk">@@ -1622,7 +1631,9 @@</span> <span class="p_context"> do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)</span>
 			if (isolate_huge_page(page, &amp;source))
 				move_pages -= 1 &lt;&lt; compound_order(head);
 			continue;
<span class="p_del">-		}</span>
<span class="p_add">+		} else if (thp_migration_supported() &amp;&amp; PageTransHuge(page))</span>
<span class="p_add">+			pfn = page_to_pfn(compound_head(page))</span>
<span class="p_add">+				+ hpage_nr_pages(page) - 1;</span>
 
 		if (!get_page_unless_zero(page))
 			continue;
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 5cc6a99918ab..021ff13b9a7a 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -94,6 +94,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm_inline.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
 #include &lt;linux/printk.h&gt;
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/uaccess.h&gt;
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 98acf7d5cef2..bfbe66798a7a 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -150,7 +150,9 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		unsigned long this_pages;
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!is_swap_pmd(*pmd) &amp;&amp; !pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
 				&amp;&amp; pmd_none_or_clear_bad(pmd))
 			continue;
 
<span class="p_chunk">@@ -160,7 +162,7 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 			mmu_notifier_invalidate_range_start(mm, mni_start, end);
 		}
 
<span class="p_del">-		if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
<span class="p_add">+		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {</span>
 			if (next - addr != HPAGE_PMD_SIZE) {
 				__split_huge_pmd(vma, pmd, addr, false, NULL);
 				if (pmd_trans_unstable(pmd))
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 8233b0105c82..5d537ce12adc 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);
 		if (!new_pmd)
 			break;
<span class="p_del">-		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="p_add">+		if (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {</span>
 			if (extent == HPAGE_PMD_SIZE) {
 				bool moved;
 				/* See comment in move_ptes() */
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 03761577ae86..114fc2b5a370 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -2,6 +2,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/hugetlb.h&gt;
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
 			  struct mm_walk *walk)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



