
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/3] lockdep: Apply crossrelease to PG_locked locks - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/3] lockdep: Apply crossrelease to PG_locked locks</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60821">byungchul park</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 16, 2017, 3:14 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1510802067-18609-2-git-send-email-byungchul.park@lge.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10060619/mbox/"
   >mbox</a>
|
   <a href="/patch/10060619/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10060619/">/patch/10060619/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7F075604D4 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Nov 2017 03:15:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6E26E2A60F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Nov 2017 03:15:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6212E2A650; Thu, 16 Nov 2017 03:15:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 515E92A60F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Nov 2017 03:15:37 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1759210AbdKPDPe (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 15 Nov 2017 22:15:34 -0500
Received: from LGEAMRELO13.lge.com ([156.147.23.53]:36317 &quot;EHLO
	lgeamrelo13.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1759023AbdKPDOg (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 15 Nov 2017 22:14:36 -0500
Received: from unknown (HELO lgemrelse7q.lge.com) (156.147.1.151)
	by 156.147.23.53 with ESMTP; 16 Nov 2017 12:14:33 +0900
X-Original-SENDERIP: 156.147.1.151
X-Original-MAILFROM: byungchul.park@lge.com
Received: from unknown (HELO localhost.localdomain) (10.177.222.33)
	by 156.147.1.151 with ESMTP; 16 Nov 2017 12:14:33 +0900
X-Original-SENDERIP: 10.177.222.33
X-Original-MAILFROM: byungchul.park@lge.com
From: Byungchul Park &lt;byungchul.park@lge.com&gt;
To: peterz@infradead.org, mingo@kernel.org, akpm@linux-foundation.org
Cc: tglx@linutronix.de, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org, linux-block@vger.kernel.org,
	kernel-team@lge.com, jack@suse.cz, jlayton@redhat.com,
	viro@zeniv.linux.org.uk, hannes@cmpxchg.org, npiggin@gmail.com,
	rgoldwyn@suse.com, vbabka@suse.cz, mhocko@suse.com,
	pombredanne@nexb.com, vinmenon@codeaurora.org, gregkh@linuxfoundation.org
Subject: [PATCH 1/3] lockdep: Apply crossrelease to PG_locked locks
Date: Thu, 16 Nov 2017 12:14:25 +0900
Message-Id: &lt;1510802067-18609-2-git-send-email-byungchul.park@lge.com&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1510802067-18609-1-git-send-email-byungchul.park@lge.com&gt;
References: &lt;1510802067-18609-1-git-send-email-byungchul.park@lge.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60821">byungchul park</a> - Nov. 16, 2017, 3:14 a.m.</div>
<pre class="content">
Although lock_page() and its family can cause deadlock, lockdep have not
worked with them, because unlock_page() might be called in a different
context from the acquire context, which violated lockdep&#39;s assumption.

Now CONFIG_LOCKDEP_CROSSRELEASE has been introduced, lockdep can work
with page locks.
<span class="signed-off-by">
Signed-off-by: Byungchul Park &lt;byungchul.park@lge.com&gt;</span>
---
 include/linux/mm_types.h |   8 ++++
 include/linux/pagemap.h  | 101 ++++++++++++++++++++++++++++++++++++++++++++---
 lib/Kconfig.debug        |   7 ++++
 mm/filemap.c             |   4 +-
 mm/page_alloc.c          |   3 ++
 5 files changed, 115 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 16, 2017, 12:02 p.m.</div>
<pre class="content">
[I have only briefly looked at patches so I might have missed some
details.]

On Thu 16-11-17 12:14:25, Byungchul Park wrote:
<span class="quote">&gt; Although lock_page() and its family can cause deadlock, lockdep have not</span>
<span class="quote">&gt; worked with them, because unlock_page() might be called in a different</span>
<span class="quote">&gt; context from the acquire context, which violated lockdep&#39;s assumption.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Now CONFIG_LOCKDEP_CROSSRELEASE has been introduced, lockdep can work</span>
<span class="quote">&gt; with page locks.</span>

I definitely agree that debugging page_lock deadlocks is a major PITA
but your implementation seems prohibitively too expensive.

[...]
<span class="quote">&gt; @@ -218,6 +222,10 @@ struct page {</span>
<span class="quote">&gt;  #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS</span>
<span class="quote">&gt;  	int _last_cpupid;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="quote">&gt; +	struct lockdep_map_cross map;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  }</span>

now you are adding 
struct lockdep_map_cross {
        struct lockdep_map         map;                  /*     0    40 */
        struct cross_lock          xlock;                /*    40    56 */
        /* --- cacheline 1 boundary (64 bytes) was 32 bytes ago --- */

        /* size: 96, cachelines: 2, members: 2 */
        /* last cacheline: 32 bytes */
};

for each struct page. So you are doubling the size. Who is going to
enable this config option? You are moving this to page_ext in a later
patch which is a good step but it doesn&#39;t go far enough because this
still consumes those resources. Is there any problem to make this
kernel command line controllable? Something we do for page_owner for
example?

Also it would be really great if you could give us some measures about
the runtime overhead. I do not expect it to be very large but this is
something people are usually interested in when enabling debugging
features.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60821">byungchul park</a> - Nov. 16, 2017, 12:48 p.m.</div>
<pre class="content">
On 11/16/2017 9:02 PM, Michal Hocko wrote:
<span class="quote">&gt; for each struct page. So you are doubling the size. Who is going to</span>
<span class="quote">&gt; enable this config option? You are moving this to page_ext in a later</span>
<span class="quote">&gt; patch which is a good step but it doesn&#39;t go far enough because this</span>
<span class="quote">&gt; still consumes those resources. Is there any problem to make this</span>
<span class="quote">&gt; kernel command line controllable? Something we do for page_owner for</span>
<span class="quote">&gt; example?</span>

Sure. I will add it.
<span class="quote">
&gt; Also it would be really great if you could give us some measures about</span>
<span class="quote">&gt; the runtime overhead. I do not expect it to be very large but this is</span>

The major overhead would come from the amount of additional memory
consumption for &#39;lockdep_map&#39;s.

Do you want me to measure the overhead by the additional memory
consumption?

Or do you expect another overhead?

Could you tell me what kind of result you want to get?
<span class="quote">
&gt; something people are usually interested in when enabling debugging</span>
<span class="quote">&gt; features.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 16, 2017, 1:07 p.m.</div>
<pre class="content">
On Thu 16-11-17 21:48:05, Byungchul Park wrote:
<span class="quote">&gt; On 11/16/2017 9:02 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; for each struct page. So you are doubling the size. Who is going to</span>
<span class="quote">&gt; &gt; enable this config option? You are moving this to page_ext in a later</span>
<span class="quote">&gt; &gt; patch which is a good step but it doesn&#39;t go far enough because this</span>
<span class="quote">&gt; &gt; still consumes those resources. Is there any problem to make this</span>
<span class="quote">&gt; &gt; kernel command line controllable? Something we do for page_owner for</span>
<span class="quote">&gt; &gt; example?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure. I will add it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Also it would be really great if you could give us some measures about</span>
<span class="quote">&gt; &gt; the runtime overhead. I do not expect it to be very large but this is</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The major overhead would come from the amount of additional memory</span>
<span class="quote">&gt; consumption for &#39;lockdep_map&#39;s.</span>

yes
<span class="quote">
&gt; Do you want me to measure the overhead by the additional memory</span>
<span class="quote">&gt; consumption?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or do you expect another overhead?</span>

I would be also interested how much impact this has on performance. I do
not expect it would be too large but having some numbers for cache cold
parallel kbuild or other heavy page lock workloads.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60821">byungchul park</a> - Nov. 24, 2017, 3:02 a.m.</div>
<pre class="content">
On Thu, Nov 16, 2017 at 02:07:46PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Thu 16-11-17 21:48:05, Byungchul Park wrote:</span>
<span class="quote">&gt; &gt; On 11/16/2017 9:02 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; for each struct page. So you are doubling the size. Who is going to</span>
<span class="quote">&gt; &gt; &gt; enable this config option? You are moving this to page_ext in a later</span>
<span class="quote">&gt; &gt; &gt; patch which is a good step but it doesn&#39;t go far enough because this</span>
<span class="quote">&gt; &gt; &gt; still consumes those resources. Is there any problem to make this</span>
<span class="quote">&gt; &gt; &gt; kernel command line controllable? Something we do for page_owner for</span>
<span class="quote">&gt; &gt; &gt; example?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sure. I will add it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Also it would be really great if you could give us some measures about</span>
<span class="quote">&gt; &gt; &gt; the runtime overhead. I do not expect it to be very large but this is</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The major overhead would come from the amount of additional memory</span>
<span class="quote">&gt; &gt; consumption for &#39;lockdep_map&#39;s.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; yes</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Do you want me to measure the overhead by the additional memory</span>
<span class="quote">&gt; &gt; consumption?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Or do you expect another overhead?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would be also interested how much impact this has on performance. I do</span>
<span class="quote">&gt; not expect it would be too large but having some numbers for cache cold</span>
<span class="quote">&gt; parallel kbuild or other heavy page lock workloads.</span>

Hello Michal,

I measured &#39;cache cold parallel kbuild&#39; on my qemu machine. The result
varies much so I cannot confirm, but I think there&#39;s no meaningful
difference between before and after applying crossrelease to page locks.

Actually, I expect little overhead in lock_page() and unlock_page() even
after applying crossreleas to page locks, but only expect a bit overhead
by additional memory consumption for &#39;lockdep_map&#39;s per page.

I run the following instructions within &quot;QEMU x86_64 4GB memory 4 cpus&quot;:

   make clean
   echo 3 &gt; drop_caches
   time make -j4

The results are:

   # w/o page lock tracking

   At the 1st try,
   real     5m28.105s
   user     17m52.716s
   sys      3m8.871s

   At the 2nd try,
   real     5m27.023s
   user     17m50.134s
   sys      3m9.289s

   At the 3rd try,
   real     5m22.837s
   user     17m34.514s
   sys      3m8.097s

   # w/ page lock tracking

   At the 1st try,
   real     5m18.158s
   user     17m18.200s
   sys      3m8.639s

   At the 2nd try,
   real     5m19.329s
   user     17m19.982s
   sys      3m8.345s

   At the 3rd try,
   real     5m19.626s
   user     17m21.363s
   sys      3m9.869s

I think thers&#39;s no meaningful difference on my small machine.

--
Thanks,
Byungchul
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 24, 2017, 8:11 a.m.</div>
<pre class="content">
On Fri 24-11-17 12:02:36, Byungchul Park wrote:
<span class="quote">&gt; On Thu, Nov 16, 2017 at 02:07:46PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Thu 16-11-17 21:48:05, Byungchul Park wrote:</span>
<span class="quote">&gt; &gt; &gt; On 11/16/2017 9:02 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; for each struct page. So you are doubling the size. Who is going to</span>
<span class="quote">&gt; &gt; &gt; &gt; enable this config option? You are moving this to page_ext in a later</span>
<span class="quote">&gt; &gt; &gt; &gt; patch which is a good step but it doesn&#39;t go far enough because this</span>
<span class="quote">&gt; &gt; &gt; &gt; still consumes those resources. Is there any problem to make this</span>
<span class="quote">&gt; &gt; &gt; &gt; kernel command line controllable? Something we do for page_owner for</span>
<span class="quote">&gt; &gt; &gt; &gt; example?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Sure. I will add it.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Also it would be really great if you could give us some measures about</span>
<span class="quote">&gt; &gt; &gt; &gt; the runtime overhead. I do not expect it to be very large but this is</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The major overhead would come from the amount of additional memory</span>
<span class="quote">&gt; &gt; &gt; consumption for &#39;lockdep_map&#39;s.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; yes</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Do you want me to measure the overhead by the additional memory</span>
<span class="quote">&gt; &gt; &gt; consumption?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Or do you expect another overhead?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I would be also interested how much impact this has on performance. I do</span>
<span class="quote">&gt; &gt; not expect it would be too large but having some numbers for cache cold</span>
<span class="quote">&gt; &gt; parallel kbuild or other heavy page lock workloads.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hello Michal,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I measured &#39;cache cold parallel kbuild&#39; on my qemu machine. The result</span>
<span class="quote">&gt; varies much so I cannot confirm, but I think there&#39;s no meaningful</span>
<span class="quote">&gt; difference between before and after applying crossrelease to page locks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Actually, I expect little overhead in lock_page() and unlock_page() even</span>
<span class="quote">&gt; after applying crossreleas to page locks, but only expect a bit overhead</span>
<span class="quote">&gt; by additional memory consumption for &#39;lockdep_map&#39;s per page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I run the following instructions within &quot;QEMU x86_64 4GB memory 4 cpus&quot;:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    make clean</span>
<span class="quote">&gt;    echo 3 &gt; drop_caches</span>
<span class="quote">&gt;    time make -j4</span>

Maybe FS people will help you find a more representative workload. E.g.
linear cache cold file read should be good as well. Maybe there are some
tests in fstests (or how they call xfstests these days).
<span class="quote">
&gt; The results are:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    # w/o page lock tracking</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 1st try,</span>
<span class="quote">&gt;    real     5m28.105s</span>
<span class="quote">&gt;    user     17m52.716s</span>
<span class="quote">&gt;    sys      3m8.871s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 2nd try,</span>
<span class="quote">&gt;    real     5m27.023s</span>
<span class="quote">&gt;    user     17m50.134s</span>
<span class="quote">&gt;    sys      3m9.289s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 3rd try,</span>
<span class="quote">&gt;    real     5m22.837s</span>
<span class="quote">&gt;    user     17m34.514s</span>
<span class="quote">&gt;    sys      3m8.097s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    # w/ page lock tracking</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 1st try,</span>
<span class="quote">&gt;    real     5m18.158s</span>
<span class="quote">&gt;    user     17m18.200s</span>
<span class="quote">&gt;    sys      3m8.639s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 2nd try,</span>
<span class="quote">&gt;    real     5m19.329s</span>
<span class="quote">&gt;    user     17m19.982s</span>
<span class="quote">&gt;    sys      3m8.345s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    At the 3rd try,</span>
<span class="quote">&gt;    real     5m19.626s</span>
<span class="quote">&gt;    user     17m21.363s</span>
<span class="quote">&gt;    sys      3m9.869s</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think thers&#39;s no meaningful difference on my small machine.</span>

Yeah, this doesn&#39;t seem to indicate anything. Maybe moving the build to
shmem to rule out IO cost would tell more. But as I&#39;ve said previously
page I do not really expect this would be very visible. It was more a
matter of my curiosity than an acceptance requirement. I think it is
much more important to make this runtime configurable because almost
nobody is going to enable the feature if it is only build time. The cost
is jut too high.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=305">Jan Kara</a> - Nov. 24, 2017, 9:38 a.m.</div>
<pre class="content">
On Fri 24-11-17 09:11:49, Michal Hocko wrote:
<span class="quote">&gt; On Fri 24-11-17 12:02:36, Byungchul Park wrote:</span>
<span class="quote">&gt; &gt; On Thu, Nov 16, 2017 at 02:07:46PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu 16-11-17 21:48:05, Byungchul Park wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On 11/16/2017 9:02 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; for each struct page. So you are doubling the size. Who is going to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; enable this config option? You are moving this to page_ext in a later</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; patch which is a good step but it doesn&#39;t go far enough because this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; still consumes those resources. Is there any problem to make this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; kernel command line controllable? Something we do for page_owner for</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; example?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Sure. I will add it.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Also it would be really great if you could give us some measures about</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; the runtime overhead. I do not expect it to be very large but this is</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; The major overhead would come from the amount of additional memory</span>
<span class="quote">&gt; &gt; &gt; &gt; consumption for &#39;lockdep_map&#39;s.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; yes</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Do you want me to measure the overhead by the additional memory</span>
<span class="quote">&gt; &gt; &gt; &gt; consumption?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Or do you expect another overhead?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I would be also interested how much impact this has on performance. I do</span>
<span class="quote">&gt; &gt; &gt; not expect it would be too large but having some numbers for cache cold</span>
<span class="quote">&gt; &gt; &gt; parallel kbuild or other heavy page lock workloads.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hello Michal,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I measured &#39;cache cold parallel kbuild&#39; on my qemu machine. The result</span>
<span class="quote">&gt; &gt; varies much so I cannot confirm, but I think there&#39;s no meaningful</span>
<span class="quote">&gt; &gt; difference between before and after applying crossrelease to page locks.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Actually, I expect little overhead in lock_page() and unlock_page() even</span>
<span class="quote">&gt; &gt; after applying crossreleas to page locks, but only expect a bit overhead</span>
<span class="quote">&gt; &gt; by additional memory consumption for &#39;lockdep_map&#39;s per page.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I run the following instructions within &quot;QEMU x86_64 4GB memory 4 cpus&quot;:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;    make clean</span>
<span class="quote">&gt; &gt;    echo 3 &gt; drop_caches</span>
<span class="quote">&gt; &gt;    time make -j4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe FS people will help you find a more representative workload. E.g.</span>
<span class="quote">&gt; linear cache cold file read should be good as well. Maybe there are some</span>
<span class="quote">&gt; tests in fstests (or how they call xfstests these days).</span>

So a relatively good test of page handling costs is to mmap cache hot file
and measure time to fault in all the pages in the mapping. That way IO and
filesystem stays out of the way and you measure only page table lookup,
page handling (taking page ref and locking the page), and instantiation of
the new PTE. Out of this page handling is actually the significant part.

								Honza
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index c85f11d..263b861 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -17,6 +17,10 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/mmu.h&gt;
 
<span class="p_add">+#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="p_add">+#include &lt;linux/lockdep.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
 #endif
<span class="p_chunk">@@ -218,6 +222,10 @@</span> <span class="p_context"> struct page {</span>
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="p_add">+	struct lockdep_map_cross map;</span>
<span class="p_add">+#endif</span>
 }
 /*
  * The struct page can be forced to be double word aligned so that atomic ops
<span class="p_header">diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h</span>
<span class="p_header">index e08b533..35b4f67 100644</span>
<span class="p_header">--- a/include/linux/pagemap.h</span>
<span class="p_header">+++ b/include/linux/pagemap.h</span>
<span class="p_chunk">@@ -15,6 +15,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/bitops.h&gt;
 #include &lt;linux/hardirq.h&gt; /* for in_interrupt() */
 #include &lt;linux/hugetlb_inline.h&gt;
<span class="p_add">+#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="p_add">+#include &lt;linux/lockdep.h&gt;</span>
<span class="p_add">+#endif</span>
 
 /*
  * Bits in mapping-&gt;flags.
<span class="p_chunk">@@ -457,26 +460,91 @@</span> <span class="p_context"> static inline pgoff_t linear_page_index(struct vm_area_struct *vma,</span>
 	return pgoff;
 }
 
<span class="p_add">+#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="p_add">+#define lock_page_init(p)						\</span>
<span class="p_add">+do {									\</span>
<span class="p_add">+	static struct lock_class_key __key;				\</span>
<span class="p_add">+	lockdep_init_map_crosslock((struct lockdep_map *)&amp;(p)-&gt;map,	\</span>
<span class="p_add">+			&quot;(PG_locked)&quot; #p, &amp;__key, 0);			\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void lock_page_acquire(struct page *page, int try)</span>
<span class="p_add">+{</span>
<span class="p_add">+	page = compound_head(page);</span>
<span class="p_add">+	lock_acquire_exclusive((struct lockdep_map *)&amp;page-&gt;map, 0,</span>
<span class="p_add">+			       try, NULL, _RET_IP_);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void lock_page_release(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	page = compound_head(page);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * lock_commit_crosslock() is necessary for crosslocks.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	lock_commit_crosslock((struct lockdep_map *)&amp;page-&gt;map);</span>
<span class="p_add">+	lock_release((struct lockdep_map *)&amp;page-&gt;map, 0, _RET_IP_);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void lock_page_init(struct page *page) {}</span>
<span class="p_add">+static inline void lock_page_free(struct page *page) {}</span>
<span class="p_add">+static inline void lock_page_acquire(struct page *page, int try) {}</span>
<span class="p_add">+static inline void lock_page_release(struct page *page) {}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 extern void __lock_page(struct page *page);
 extern int __lock_page_killable(struct page *page);
 extern int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
 				unsigned int flags);
<span class="p_del">-extern void unlock_page(struct page *page);</span>
<span class="p_add">+extern void do_raw_unlock_page(struct page *page);</span>
 
<span class="p_del">-static inline int trylock_page(struct page *page)</span>
<span class="p_add">+static inline void unlock_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	lock_page_release(page);</span>
<span class="p_add">+	do_raw_unlock_page(page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int do_raw_trylock_page(struct page *page)</span>
 {
 	page = compound_head(page);
 	return (likely(!test_and_set_bit_lock(PG_locked, &amp;page-&gt;flags)));
 }
 
<span class="p_add">+static inline int trylock_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (do_raw_trylock_page(page)) {</span>
<span class="p_add">+		lock_page_acquire(page, 1);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * lock_page may only be called if we have the page&#39;s inode pinned.
  */
 static inline void lock_page(struct page *page)
 {
 	might_sleep();
<span class="p_del">-	if (!trylock_page(page))</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!do_raw_trylock_page(page))</span>
 		__lock_page(page);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * acquire() must be after actual lock operation for crosslocks.</span>
<span class="p_add">+	 * This way a crosslock and current lock can be ordered like:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *	CONTEXT 1		CONTEXT 2</span>
<span class="p_add">+	 *	---------		---------</span>
<span class="p_add">+	 *	lock A (cross)</span>
<span class="p_add">+	 *	acquire A</span>
<span class="p_add">+	 *	  X = atomic_inc_return(&amp;cross_gen_id)</span>
<span class="p_add">+	 *	~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="p_add">+	 *				acquire B</span>
<span class="p_add">+	 *				  Y = atomic_read_acquire(&amp;cross_gen_id)</span>
<span class="p_add">+	 *				lock B</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * so that &#39;lock A and then lock B&#39; can be seen globally,</span>
<span class="p_add">+	 * if X &lt;= Y.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	lock_page_acquire(page, 0);</span>
 }
 
 /*
<span class="p_chunk">@@ -486,9 +554,20 @@</span> <span class="p_context"> static inline void lock_page(struct page *page)</span>
  */
 static inline int lock_page_killable(struct page *page)
 {
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	if (!trylock_page(page))</span>
<span class="p_del">-		return __lock_page_killable(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!do_raw_trylock_page(page)) {</span>
<span class="p_add">+		ret = __lock_page_killable(page);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * acquire() must be after actual lock operation for crosslocks.</span>
<span class="p_add">+	 * This way a crosslock and other locks can be ordered.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	lock_page_acquire(page, 0);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -503,7 +582,17 @@</span> <span class="p_context"> static inline int lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 				     unsigned int flags)
 {
 	might_sleep();
<span class="p_del">-	return trylock_page(page) || __lock_page_or_retry(page, mm, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (do_raw_trylock_page(page) || __lock_page_or_retry(page, mm, flags)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * acquire() must be after actual lock operation for crosslocks.</span>
<span class="p_add">+		 * This way a crosslock and other locks can be ordered.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		lock_page_acquire(page, 0);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
 }
 
 /*
<span class="p_header">diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug</span>
<span class="p_header">index 2b439a5..2e8c679 100644</span>
<span class="p_header">--- a/lib/Kconfig.debug</span>
<span class="p_header">+++ b/lib/Kconfig.debug</span>
<span class="p_chunk">@@ -1094,6 +1094,7 @@</span> <span class="p_context"> config PROVE_LOCKING</span>
 	select DEBUG_LOCK_ALLOC
 	select LOCKDEP_CROSSRELEASE
 	select LOCKDEP_COMPLETIONS
<span class="p_add">+	select LOCKDEP_PAGELOCK</span>
 	select TRACE_IRQFLAGS
 	default n
 	help
<span class="p_chunk">@@ -1179,6 +1180,12 @@</span> <span class="p_context"> config LOCKDEP_COMPLETIONS</span>
 	 A deadlock caused by wait_for_completion() and complete() can be
 	 detected by lockdep using crossrelease feature.
 
<span class="p_add">+config LOCKDEP_PAGELOCK</span>
<span class="p_add">+	bool</span>
<span class="p_add">+	help</span>
<span class="p_add">+	 PG_locked lock is a kind of crosslock. Using crossrelease feature,</span>
<span class="p_add">+	 PG_locked lock can work with lockdep.</span>
<span class="p_add">+</span>
 config BOOTPARAM_LOCKDEP_CROSSRELEASE_FULLSTACK
 	bool &quot;Enable the boot parameter, crossrelease_fullstack&quot;
 	depends on LOCKDEP_CROSSRELEASE
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 594d73f..870d442 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -1099,7 +1099,7 @@</span> <span class="p_context"> static inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem</span>
  * portably (architectures that do LL/SC can test any bit, while x86 can
  * test the sign bit).
  */
<span class="p_del">-void unlock_page(struct page *page)</span>
<span class="p_add">+void do_raw_unlock_page(struct page *page)</span>
 {
 	BUILD_BUG_ON(PG_waiters != 7);
 	page = compound_head(page);
<span class="p_chunk">@@ -1107,7 +1107,7 @@</span> <span class="p_context"> void unlock_page(struct page *page)</span>
 	if (clear_bit_unlock_is_negative_byte(PG_locked, &amp;page-&gt;flags))
 		wake_up_page_bit(page, PG_locked);
 }
<span class="p_del">-EXPORT_SYMBOL(unlock_page);</span>
<span class="p_add">+EXPORT_SYMBOL(do_raw_unlock_page);</span>
 
 /**
  * end_page_writeback - end writeback against a page
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 77e4d3c..8436b28 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -5371,6 +5371,9 @@</span> <span class="p_context"> void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,</span>
 		} else {
 			__init_single_pfn(pfn, zone, nid);
 		}
<span class="p_add">+#ifdef CONFIG_LOCKDEP_PAGELOCK</span>
<span class="p_add">+		lock_page_init(pfn_to_page(pfn));</span>
<span class="p_add">+#endif</span>
 	}
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



