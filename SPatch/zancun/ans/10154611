
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.14.13 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.14.13</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 10, 2018, 9:11 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180110091115.GB18792@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10154611/mbox/"
   >mbox</a>
|
   <a href="/patch/10154611/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10154611/">/patch/10154611/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	977E2602D8 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 09:11:42 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7CAF7283D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 09:11:42 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6FEAB28419; Wed, 10 Jan 2018 09:11:42 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CBC9C283D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 10 Jan 2018 09:11:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S965151AbeAJJLh (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 10 Jan 2018 04:11:37 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:53438 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756069AbeAJJLR (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 10 Jan 2018 04:11:17 -0500
Received: from localhost (unknown [37.170.134.120])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 32FFDF4A;
	Wed, 10 Jan 2018 09:11:14 +0000 (UTC)
Date: Wed, 10 Jan 2018 10:11:15 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.14.13
Message-ID: &lt;20180110091115.GB18792@kroah.com&gt;
References: &lt;20180110091107.GA18792@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20180110091107.GA18792@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 10, 2018, 9:11 a.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index ad41b3813f0a..ea91cb61a602 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,8 +12,9 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_del">-fffffe0000000000 - fffffe7fffffffff (=39 bits) LDT remap for PTI</span>
<span class="p_del">-fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+				    vaddr_end for KASLR</span>
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT remap for PTI</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_chunk">@@ -37,13 +38,15 @@</span> <span class="p_context"> ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)</span>
 ... unused hole ...
 ffdf000000000000 - fffffc0000000000 (=53 bits) kasan shadow memory (8PB)
 ... unused hole ...
<span class="p_del">-fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+				    vaddr_end for KASLR</span>
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+... unused hole ...</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+ffffffffa0000000 - fffffffffeffffff (1520 MB) module mapping space</span>
 [fixmap start]   - ffffffffff5fffff kernel-internal fixmap range
 ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
<span class="p_chunk">@@ -67,9 +70,10 @@</span> <span class="p_context"> memory window (this size is arbitrary, it can be raised later if needed).</span>
 The mappings are not part of any other kernel PGD and are only available
 during EFI runtime calls.
 
<span class="p_del">-The module mapping space size changes based on the CONFIG requirements for the</span>
<span class="p_del">-following fixmap section.</span>
<span class="p_del">-</span>
 Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all
 physical memory, vmalloc/ioremap space and virtual memory map are randomized.
 Their order is preserved but their base will be offset early at boot time.
<span class="p_add">+</span>
<span class="p_add">+Be very careful vs. KASLR when changing anything here. The KASLR address</span>
<span class="p_add">+range must not overlap with anything except the KASAN shadow area, which is</span>
<span class="p_add">+correct as KASAN disables KASLR.</span>
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 20f7d4de0f1c..a67c5179052a 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 4
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 12</span>
<span class="p_add">+SUBLEVEL = 13</span>
 EXTRAVERSION =
 NAME = Petit Gorille
 
<span class="p_header">diff --git a/arch/arc/include/asm/uaccess.h b/arch/arc/include/asm/uaccess.h</span>
<span class="p_header">index f35974ee7264..c9173c02081c 100644</span>
<span class="p_header">--- a/arch/arc/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/arc/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -668,6 +668,7 @@</span> <span class="p_context"> __arc_strncpy_from_user(char *dst, const char __user *src, long count)</span>
 		return 0;
 
 	__asm__ __volatile__(
<span class="p_add">+	&quot;	mov	lp_count, %5		\n&quot;</span>
 	&quot;	lp	3f			\n&quot;
 	&quot;1:	ldb.ab  %3, [%2, 1]		\n&quot;
 	&quot;	breq.d	%3, 0, 3f               \n&quot;
<span class="p_chunk">@@ -684,8 +685,8 @@</span> <span class="p_context"> __arc_strncpy_from_user(char *dst, const char __user *src, long count)</span>
 	&quot;	.word   1b, 4b			\n&quot;
 	&quot;	.previous			\n&quot;
 	: &quot;+r&quot;(res), &quot;+r&quot;(dst), &quot;+r&quot;(src), &quot;=r&quot;(val)
<span class="p_del">-	: &quot;g&quot;(-EFAULT), &quot;l&quot;(count)</span>
<span class="p_del">-	: &quot;memory&quot;);</span>
<span class="p_add">+	: &quot;g&quot;(-EFAULT), &quot;r&quot;(count)</span>
<span class="p_add">+	: &quot;lp_count&quot;, &quot;lp_start&quot;, &quot;lp_end&quot;, &quot;memory&quot;);</span>
 
 	return res;
 }
<span class="p_header">diff --git a/arch/parisc/include/asm/ldcw.h b/arch/parisc/include/asm/ldcw.h</span>
<span class="p_header">index dd5a08aaa4da..3eb4bfc1fb36 100644</span>
<span class="p_header">--- a/arch/parisc/include/asm/ldcw.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/ldcw.h</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
    for the semaphore.  */
 
 #define __PA_LDCW_ALIGNMENT	16
<span class="p_add">+#define __PA_LDCW_ALIGN_ORDER	4</span>
 #define __ldcw_align(a) ({					\
 	unsigned long __ret = (unsigned long) &amp;(a)-&gt;lock[0];	\
 	__ret = (__ret + __PA_LDCW_ALIGNMENT - 1)		\
<span class="p_chunk">@@ -29,6 +30,7 @@</span> <span class="p_context"></span>
    ldcd). */
 
 #define __PA_LDCW_ALIGNMENT	4
<span class="p_add">+#define __PA_LDCW_ALIGN_ORDER	2</span>
 #define __ldcw_align(a) (&amp;(a)-&gt;slock)
 #define __LDCW	&quot;ldcw,co&quot;
 
<span class="p_header">diff --git a/arch/parisc/kernel/entry.S b/arch/parisc/kernel/entry.S</span>
<span class="p_header">index f3cecf5117cf..e95207c0565e 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/entry.S</span>
<span class="p_header">+++ b/arch/parisc/kernel/entry.S</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/signal.h&gt;
 #include &lt;asm/unistd.h&gt;
<span class="p_add">+#include &lt;asm/ldcw.h&gt;</span>
 #include &lt;asm/thread_info.h&gt;
 
 #include &lt;linux/linkage.h&gt;
<span class="p_chunk">@@ -46,6 +47,14 @@</span> <span class="p_context"></span>
 #endif
 
 	.import		pa_tlb_lock,data
<span class="p_add">+	.macro  load_pa_tlb_lock reg</span>
<span class="p_add">+#if __PA_LDCW_ALIGNMENT &gt; 4</span>
<span class="p_add">+	load32	PA(pa_tlb_lock) + __PA_LDCW_ALIGNMENT-1, \reg</span>
<span class="p_add">+	depi	0,31,__PA_LDCW_ALIGN_ORDER, \reg</span>
<span class="p_add">+#else</span>
<span class="p_add">+	load32	PA(pa_tlb_lock), \reg</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	.endm</span>
 
 	/* space_to_prot macro creates a prot id from a space id */
 
<span class="p_chunk">@@ -457,7 +466,7 @@</span> <span class="p_context"></span>
 	.macro		tlb_lock	spc,ptp,pte,tmp,tmp1,fault
 #ifdef CONFIG_SMP
 	cmpib,COND(=),n	0,\spc,2f
<span class="p_del">-	load32		PA(pa_tlb_lock),\tmp</span>
<span class="p_add">+	load_pa_tlb_lock \tmp</span>
 1:	LDCW		0(\tmp),\tmp1
 	cmpib,COND(=)	0,\tmp1,1b
 	nop
<span class="p_chunk">@@ -480,7 +489,7 @@</span> <span class="p_context"></span>
 	/* Release pa_tlb_lock lock. */
 	.macro		tlb_unlock1	spc,tmp
 #ifdef CONFIG_SMP
<span class="p_del">-	load32		PA(pa_tlb_lock),\tmp</span>
<span class="p_add">+	load_pa_tlb_lock \tmp</span>
 	tlb_unlock0	\spc,\tmp
 #endif
 	.endm
<span class="p_header">diff --git a/arch/parisc/kernel/pacache.S b/arch/parisc/kernel/pacache.S</span>
<span class="p_header">index adf7187f8951..2d40c4ff3f69 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/pacache.S</span>
<span class="p_header">+++ b/arch/parisc/kernel/pacache.S</span>
<span class="p_chunk">@@ -36,6 +36,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/assembly.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/cache.h&gt;
<span class="p_add">+#include &lt;asm/ldcw.h&gt;</span>
 #include &lt;linux/linkage.h&gt;
 
 	.text
<span class="p_chunk">@@ -333,8 +334,12 @@</span> <span class="p_context"> ENDPROC_CFI(flush_data_cache_local)</span>
 
 	.macro	tlb_lock	la,flags,tmp
 #ifdef CONFIG_SMP
<span class="p_del">-	ldil		L%pa_tlb_lock,%r1</span>
<span class="p_del">-	ldo		R%pa_tlb_lock(%r1),\la</span>
<span class="p_add">+#if __PA_LDCW_ALIGNMENT &gt; 4</span>
<span class="p_add">+	load32		pa_tlb_lock + __PA_LDCW_ALIGNMENT-1, \la</span>
<span class="p_add">+	depi		0,31,__PA_LDCW_ALIGN_ORDER, \la</span>
<span class="p_add">+#else</span>
<span class="p_add">+	load32		pa_tlb_lock, \la</span>
<span class="p_add">+#endif</span>
 	rsm		PSW_SM_I,\flags
 1:	LDCW		0(\la),\tmp
 	cmpib,&lt;&gt;,n	0,\tmp,3f
<span class="p_header">diff --git a/arch/parisc/kernel/process.c b/arch/parisc/kernel/process.c</span>
<span class="p_header">index 30f92391a93e..cad3e8661cd6 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/process.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/process.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kernel.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/fs.h&gt;
<span class="p_add">+#include &lt;linux/cpu.h&gt;</span>
 #include &lt;linux/module.h&gt;
 #include &lt;linux/personality.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_chunk">@@ -183,6 +184,44 @@</span> <span class="p_context"> int dump_task_fpu (struct task_struct *tsk, elf_fpregset_t *r)</span>
 	return 1;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Idle thread support</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Detect when running on QEMU with SeaBIOS PDC Firmware and let</span>
<span class="p_add">+ * QEMU idle the host too.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+int running_on_qemu __read_mostly;</span>
<span class="p_add">+</span>
<span class="p_add">+void __cpuidle arch_cpu_idle_dead(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* nop on real hardware, qemu will offline CPU. */</span>
<span class="p_add">+	asm volatile(&quot;or %%r31,%%r31,%%r31\n&quot;:::);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __cpuidle arch_cpu_idle(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* nop on real hardware, qemu will idle sleep. */</span>
<span class="p_add">+	asm volatile(&quot;or %%r10,%%r10,%%r10\n&quot;:::);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init parisc_idle_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const char *marker;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* check QEMU/SeaBIOS marker in PAGE0 */</span>
<span class="p_add">+	marker = (char *) &amp;PAGE0-&gt;pad0;</span>
<span class="p_add">+	running_on_qemu = (memcmp(marker, &quot;SeaBIOS&quot;, 8) == 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!running_on_qemu)</span>
<span class="p_add">+		cpu_idle_poll_ctrl(1);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+arch_initcall(parisc_idle_init);</span>
<span class="p_add">+</span>
 /*
  * Copy architecture-specific thread state
  */
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index 4797d08581ce..6e1e39035380 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -145,6 +145,11 @@</span> <span class="p_context"> static noinline int bad_area(struct pt_regs *regs, unsigned long address)</span>
 	return __bad_area(regs, address, SEGV_MAPERR);
 }
 
<span class="p_add">+static noinline int bad_access(struct pt_regs *regs, unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __bad_area(regs, address, SEGV_ACCERR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int do_sigbus(struct pt_regs *regs, unsigned long address,
 		     unsigned int fault)
 {
<span class="p_chunk">@@ -490,7 +495,7 @@</span> <span class="p_context"> static int __do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 
 good_area:
 	if (unlikely(access_error(is_write, is_exec, vma)))
<span class="p_del">-		return bad_area(regs, address);</span>
<span class="p_add">+		return bad_access(regs, address);</span>
 
 	/*
 	 * If for any reason at all we couldn&#39;t handle the fault,
<span class="p_header">diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c</span>
<span class="p_header">index 40d0a1a97889..b87a930c2201 100644</span>
<span class="p_header">--- a/arch/s390/kvm/kvm-s390.c</span>
<span class="p_header">+++ b/arch/s390/kvm/kvm-s390.c</span>
<span class="p_chunk">@@ -794,11 +794,12 @@</span> <span class="p_context"> static int kvm_s390_vm_start_migration(struct kvm *kvm)</span>
 
 	if (kvm-&gt;arch.use_cmma) {
 		/*
<span class="p_del">-		 * Get the last slot. They should be sorted by base_gfn, so the</span>
<span class="p_del">-		 * last slot is also the one at the end of the address space.</span>
<span class="p_del">-		 * We have verified above that at least one slot is present.</span>
<span class="p_add">+		 * Get the first slot. They are reverse sorted by base_gfn, so</span>
<span class="p_add">+		 * the first slot is also the one at the end of the address</span>
<span class="p_add">+		 * space. We have verified above that at least one slot is</span>
<span class="p_add">+		 * present.</span>
 		 */
<span class="p_del">-		ms = slots-&gt;memslots + slots-&gt;used_slots - 1;</span>
<span class="p_add">+		ms = slots-&gt;memslots;</span>
 		/* round up so we only use full longs */
 		ram_pages = roundup(ms-&gt;base_gfn + ms-&gt;npages, BITS_PER_LONG);
 		/* allocate enough bytes to store all the bits */
<span class="p_header">diff --git a/arch/s390/kvm/priv.c b/arch/s390/kvm/priv.c</span>
<span class="p_header">index 5b25287f449b..7bd3a59232f0 100644</span>
<span class="p_header">--- a/arch/s390/kvm/priv.c</span>
<span class="p_header">+++ b/arch/s390/kvm/priv.c</span>
<span class="p_chunk">@@ -1009,7 +1009,7 @@</span> <span class="p_context"> static inline int do_essa(struct kvm_vcpu *vcpu, const int orc)</span>
 		cbrlo[entries] = gfn &lt;&lt; PAGE_SHIFT;
 	}
 
<span class="p_del">-	if (orc) {</span>
<span class="p_add">+	if (orc &amp;&amp; gfn &lt; ms-&gt;bitmap_size) {</span>
 		/* increment only if we are really flipping the bit to 1 */
 		if (!test_and_set_bit(gfn, ms-&gt;pgste_bitmap))
 			atomic64_inc(&amp;ms-&gt;dirty_pages);
<span class="p_header">diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c</span>
<span class="p_header">index 8f0aace08b87..8156e47da7ba 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/ds.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/ds.c</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/cpu_entry_area.h&gt;
 #include &lt;asm/perf_event.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/insn.h&gt;
 
 #include &quot;../perf_event.h&quot;
<span class="p_chunk">@@ -283,20 +284,35 @@</span> <span class="p_context"> static DEFINE_PER_CPU(void *, insn_buffer);</span>
 
 static void ds_update_cea(void *cea, void *addr, size_t size, pgprot_t prot)
 {
<span class="p_add">+	unsigned long start = (unsigned long)cea;</span>
 	phys_addr_t pa;
 	size_t msz = 0;
 
 	pa = virt_to_phys(addr);
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
 	for (; msz &lt; size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)
 		cea_set_pte(cea, pa, prot);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is a cross-CPU update of the cpu_entry_area, we must shoot down</span>
<span class="p_add">+	 * all TLB entries for it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_kernel_range(start, start + size);</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
 static void ds_clear_cea(void *cea, size_t size)
 {
<span class="p_add">+	unsigned long start = (unsigned long)cea;</span>
 	size_t msz = 0;
 
<span class="p_add">+	preempt_disable();</span>
 	for (; msz &lt; size; msz += PAGE_SIZE, cea += PAGE_SIZE)
 		cea_set_pte(cea, 0, PAGE_NONE);
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(start, start + size);</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
 static void *dsalloc_pages(size_t size, gfp_t flags, int cpu)
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index dbfd0854651f..cf5961ca8677 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 	&quot;.popsection\n&quot;							\
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr, feature, 1)			\
<span class="p_del">-	&quot;.popsection&quot;</span>
<span class="p_add">+	&quot;.popsection\n&quot;</span>
 
 #define ALTERNATIVE_2(oldinstr, newinstr1, feature1, newinstr2, feature2)\
 	OLDINSTR_2(oldinstr, 1, 2)					\
<span class="p_chunk">@@ -151,7 +151,7 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr1, feature1, 1)			\
 	ALTINSTR_REPLACEMENT(newinstr2, feature2, 2)			\
<span class="p_del">-	&quot;.popsection&quot;</span>
<span class="p_add">+	&quot;.popsection\n&quot;</span>
 
 /*
  * Alternative instructions for different CPU types or capabilities.
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 07cdd1715705..21ac898df2d8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -341,6 +341,6 @@</span> <span class="p_context"></span>
 #define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
<span class="p_del">-#define X86_BUG_CPU_INSECURE		X86_BUG(14) /* CPU is insecure and needs kernel page table isolation */</span>
<span class="p_add">+#define X86_BUG_CPU_MELTDOWN		X86_BUG(14) /* CPU is affected by meltdown attack and needs kernel page table isolation */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index b97a539bcdee..6b8f73dcbc2c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -75,7 +75,13 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define PGDIR_SIZE	(_AC(1, UL) &lt;&lt; PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 
<span class="p_del">-/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * See Documentation/x86/x86_64/mm.txt for a description of the memory map.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Be very careful vs. KASLR when changing anything here. The KASLR address</span>
<span class="p_add">+ * range must not overlap with anything except the KASAN shadow area, which</span>
<span class="p_add">+ * is correct as KASAN disables KASLR.</span>
<span class="p_add">+ */</span>
 #define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_chunk">@@ -88,7 +94,7 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 # define VMALLOC_SIZE_TB	_AC(32, UL)
 # define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)
 # define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)
<span class="p_del">-# define LDT_PGD_ENTRY		_AC(-4, UL)</span>
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-3, UL)</span>
 # define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)
 #endif
 
<span class="p_chunk">@@ -104,13 +110,13 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 
 #define MODULES_VADDR		(__START_KERNEL_map + KERNEL_IMAGE_SIZE)
 /* The module sections ends with the start of the fixmap */
<span class="p_del">-#define MODULES_END		__fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_add">+#define MODULES_END		_AC(0xffffffffff000000, UL)</span>
 #define MODULES_LEN		(MODULES_END - MODULES_VADDR)
 
 #define ESPFIX_PGD_ENTRY	_AC(-2, UL)
 #define ESPFIX_BASE_ADDR	(ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)
 
<span class="p_del">-#define CPU_ENTRY_AREA_PGD	_AC(-3, UL)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PGD	_AC(-4, UL)</span>
 #define CPU_ENTRY_AREA_BASE	(CPU_ENTRY_AREA_PGD &lt;&lt; P4D_SHIFT)
 
 #define EFI_VA_START		( -4 * (_AC(1, UL) &lt;&lt; 30))
<span class="p_header">diff --git a/arch/x86/kernel/cpu/Makefile b/arch/x86/kernel/cpu/Makefile</span>
<span class="p_header">index 90cb82dbba57..570e8bb1f386 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/Makefile</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/Makefile</span>
<span class="p_chunk">@@ -22,7 +22,7 @@</span> <span class="p_context"> obj-y			+= common.o</span>
 obj-y			+= rdrand.o
 obj-y			+= match.o
 obj-y			+= bugs.o
<span class="p_del">-obj-$(CONFIG_CPU_FREQ)	+= aperfmperf.o</span>
<span class="p_add">+obj-y			+= aperfmperf.o</span>
 obj-y			+= cpuid-deps.o
 
 obj-$(CONFIG_PROC_FS)	+= proc.o
<span class="p_header">diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c</span>
<span class="p_header">index 0ee83321a313..7eba34df54c3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/aperfmperf.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/aperfmperf.c</span>
<span class="p_chunk">@@ -14,6 +14,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/percpu.h&gt;
 #include &lt;linux/smp.h&gt;
 
<span class="p_add">+#include &quot;cpu.h&quot;</span>
<span class="p_add">+</span>
 struct aperfmperf_sample {
 	unsigned int	khz;
 	ktime_t	time;
<span class="p_chunk">@@ -24,7 +26,7 @@</span> <span class="p_context"> struct aperfmperf_sample {</span>
 static DEFINE_PER_CPU(struct aperfmperf_sample, samples);
 
 #define APERFMPERF_CACHE_THRESHOLD_MS	10
<span class="p_del">-#define APERFMPERF_REFRESH_DELAY_MS	20</span>
<span class="p_add">+#define APERFMPERF_REFRESH_DELAY_MS	10</span>
 #define APERFMPERF_STALE_THRESHOLD_MS	1000
 
 /*
<span class="p_chunk">@@ -38,14 +40,8 @@</span> <span class="p_context"> static void aperfmperf_snapshot_khz(void *dummy)</span>
 	u64 aperf, aperf_delta;
 	u64 mperf, mperf_delta;
 	struct aperfmperf_sample *s = this_cpu_ptr(&amp;samples);
<span class="p_del">-	ktime_t now = ktime_get();</span>
<span class="p_del">-	s64 time_delta = ktime_ms_delta(now, s-&gt;time);</span>
 	unsigned long flags;
 
<span class="p_del">-	/* Don&#39;t bother re-computing within the cache threshold time. */</span>
<span class="p_del">-	if (time_delta &lt; APERFMPERF_CACHE_THRESHOLD_MS)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
 	local_irq_save(flags);
 	rdmsrl(MSR_IA32_APERF, aperf);
 	rdmsrl(MSR_IA32_MPERF, mperf);
<span class="p_chunk">@@ -61,31 +57,68 @@</span> <span class="p_context"> static void aperfmperf_snapshot_khz(void *dummy)</span>
 	if (mperf_delta == 0)
 		return;
 
<span class="p_del">-	s-&gt;time = now;</span>
<span class="p_add">+	s-&gt;time = ktime_get();</span>
 	s-&gt;aperf = aperf;
 	s-&gt;mperf = mperf;
<span class="p_add">+	s-&gt;khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	/* If the previous iteration was too long ago, discard it. */</span>
<span class="p_del">-	if (time_delta &gt; APERFMPERF_STALE_THRESHOLD_MS)</span>
<span class="p_del">-		s-&gt;khz = 0;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		s-&gt;khz = div64_u64((cpu_khz * aperf_delta), mperf_delta);</span>
<span class="p_add">+static bool aperfmperf_snapshot_cpu(int cpu, ktime_t now, bool wait)</span>
<span class="p_add">+{</span>
<span class="p_add">+	s64 time_delta = ktime_ms_delta(now, per_cpu(samples.time, cpu));</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Don&#39;t bother re-computing within the cache threshold time. */</span>
<span class="p_add">+	if (time_delta &lt; APERFMPERF_CACHE_THRESHOLD_MS)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, wait);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Return false if the previous iteration was too long ago. */</span>
<span class="p_add">+	return time_delta &lt;= APERFMPERF_STALE_THRESHOLD_MS;</span>
 }
 
<span class="p_del">-unsigned int arch_freq_get_on_cpu(int cpu)</span>
<span class="p_add">+unsigned int aperfmperf_get_khz(int cpu)</span>
 {
<span class="p_del">-	unsigned int khz;</span>
<span class="p_add">+	if (!cpu_khz)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_APERFMPERF))</span>
<span class="p_add">+		return 0;</span>
 
<span class="p_add">+	aperfmperf_snapshot_cpu(cpu, ktime_get(), true);</span>
<span class="p_add">+	return per_cpu(samples.khz, cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void arch_freq_prepare_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ktime_t now = ktime_get();</span>
<span class="p_add">+	bool wait = false;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_khz)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_APERFMPERF))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_online_cpu(cpu)</span>
<span class="p_add">+		if (!aperfmperf_snapshot_cpu(cpu, now, false))</span>
<span class="p_add">+			wait = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (wait)</span>
<span class="p_add">+		msleep(APERFMPERF_REFRESH_DELAY_MS);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned int arch_freq_get_on_cpu(int cpu)</span>
<span class="p_add">+{</span>
 	if (!cpu_khz)
 		return 0;
 
 	if (!static_cpu_has(X86_FEATURE_APERFMPERF))
 		return 0;
 
<span class="p_del">-	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);</span>
<span class="p_del">-	khz = per_cpu(samples.khz, cpu);</span>
<span class="p_del">-	if (khz)</span>
<span class="p_del">-		return khz;</span>
<span class="p_add">+	if (aperfmperf_snapshot_cpu(cpu, ktime_get(), true))</span>
<span class="p_add">+		return per_cpu(samples.khz, cpu);</span>
 
 	msleep(APERFMPERF_REFRESH_DELAY_MS);
 	smp_call_function_single(cpu, aperfmperf_snapshot_khz, NULL, 1);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index b1be494ab4e8..2d3bd2215e5b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -900,7 +900,7 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
 	if (c-&gt;x86_vendor != X86_VENDOR_AMD)
<span class="p_del">-		setup_force_cpu_bug(X86_BUG_CPU_INSECURE);</span>
<span class="p_add">+		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);</span>
 
 	fpu__init_system(c);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/cpu.h b/arch/x86/kernel/cpu/cpu.h</span>
<span class="p_header">index f52a370b6c00..e806b11a99af 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/cpu.h</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/cpu.h</span>
<span class="p_chunk">@@ -47,4 +47,7 @@</span> <span class="p_context"> extern const struct cpu_dev *const __x86_cpu_dev_start[],</span>
 
 extern void get_cpu_cap(struct cpuinfo_x86 *c);
 extern void cpu_detect_cache_sizes(struct cpuinfo_x86 *c);
<span class="p_add">+</span>
<span class="p_add">+unsigned int aperfmperf_get_khz(int cpu);</span>
<span class="p_add">+</span>
 #endif /* ARCH_X86_CPU_H */
<span class="p_header">diff --git a/arch/x86/kernel/cpu/microcode/amd.c b/arch/x86/kernel/cpu/microcode/amd.c</span>
<span class="p_header">index c6daec4bdba5..330b8462d426 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/microcode/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/microcode/amd.c</span>
<span class="p_chunk">@@ -470,6 +470,7 @@</span> <span class="p_context"> static unsigned int verify_patch_size(u8 family, u32 patch_size,</span>
 #define F14H_MPB_MAX_SIZE 1824
 #define F15H_MPB_MAX_SIZE 4096
 #define F16H_MPB_MAX_SIZE 3458
<span class="p_add">+#define F17H_MPB_MAX_SIZE 3200</span>
 
 	switch (family) {
 	case 0x14:
<span class="p_chunk">@@ -481,6 +482,9 @@</span> <span class="p_context"> static unsigned int verify_patch_size(u8 family, u32 patch_size,</span>
 	case 0x16:
 		max_size = F16H_MPB_MAX_SIZE;
 		break;
<span class="p_add">+	case 0x17:</span>
<span class="p_add">+		max_size = F17H_MPB_MAX_SIZE;</span>
<span class="p_add">+		break;</span>
 	default:
 		max_size = F1XH_MPB_MAX_SIZE;
 		break;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">index 6b7e17bf0b71..e7ecedafa1c8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_chunk">@@ -5,6 +5,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/seq_file.h&gt;
 #include &lt;linux/cpufreq.h&gt;
 
<span class="p_add">+#include &quot;cpu.h&quot;</span>
<span class="p_add">+</span>
 /*
  *	Get CPU information for use by the procfs.
  */
<span class="p_chunk">@@ -78,8 +80,10 @@</span> <span class="p_context"> static int show_cpuinfo(struct seq_file *m, void *v)</span>
 		seq_printf(m, &quot;microcode\t: 0x%x\n&quot;, c-&gt;microcode);
 
 	if (cpu_has(c, X86_FEATURE_TSC)) {
<span class="p_del">-		unsigned int freq = cpufreq_quick_get(cpu);</span>
<span class="p_add">+		unsigned int freq = aperfmperf_get_khz(cpu);</span>
 
<span class="p_add">+		if (!freq)</span>
<span class="p_add">+			freq = cpufreq_quick_get(cpu);</span>
 		if (!freq)
 			freq = cpu_khz;
 		seq_printf(m, &quot;cpu MHz\t\t: %u.%03u\n&quot;,
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index f56902c1f04b..2a4849e92831 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -61,10 +61,10 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
 #if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)
 	LDT_NR,
 #endif
<span class="p_del">-	CPU_ENTRY_AREA_NR,</span>
 #ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
 #endif
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 80259ad8c386..6b462a472a7b 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -870,7 +870,7 @@</span> <span class="p_context"> __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
 	.next_asid = 1,
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
 };
<span class="p_del">-EXPORT_SYMBOL_GPL(cpu_tlbstate);</span>
<span class="p_add">+EXPORT_PER_CPU_SYMBOL(cpu_tlbstate);</span>
 
 void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)
 {
<span class="p_header">diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c</span>
<span class="p_header">index 879ef930e2c2..aedebd2ebf1e 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaslr.c</span>
<span class="p_chunk">@@ -34,25 +34,14 @@</span> <span class="p_context"></span>
 #define TB_SHIFT 40
 
 /*
<span class="p_del">- * Virtual address start and end range for randomization. The end changes base</span>
<span class="p_del">- * on configuration to have the highest amount of space for randomization.</span>
<span class="p_del">- * It increases the possible random position for each randomized region.</span>
<span class="p_add">+ * Virtual address start and end range for randomization.</span>
  *
<span class="p_del">- * You need to add an if/def entry if you introduce a new memory region</span>
<span class="p_del">- * compatible with KASLR. Your entry must be in logical order with memory</span>
<span class="p_del">- * layout. For example, ESPFIX is before EFI because its virtual address is</span>
<span class="p_del">- * before. You also need to add a BUILD_BUG_ON() in kernel_randomize_memory() to</span>
<span class="p_del">- * ensure that this order is correct and won&#39;t be changed.</span>
<span class="p_add">+ * The end address could depend on more configuration options to make the</span>
<span class="p_add">+ * highest amount of space for randomization available, but that&#39;s too hard</span>
<span class="p_add">+ * to keep straight and caused issues already.</span>
  */
 static const unsigned long vaddr_start = __PAGE_OFFSET_BASE;
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_X86_ESPFIX64)</span>
<span class="p_del">-static const unsigned long vaddr_end = ESPFIX_BASE_ADDR;</span>
<span class="p_del">-#elif defined(CONFIG_EFI)</span>
<span class="p_del">-static const unsigned long vaddr_end = EFI_VA_END;</span>
<span class="p_del">-#else</span>
<span class="p_del">-static const unsigned long vaddr_end = __START_KERNEL_map;</span>
<span class="p_del">-#endif</span>
<span class="p_add">+static const unsigned long vaddr_end = CPU_ENTRY_AREA_BASE;</span>
 
 /* Default values */
 unsigned long page_offset_base = __PAGE_OFFSET_BASE;
<span class="p_chunk">@@ -101,15 +90,12 @@</span> <span class="p_context"> void __init kernel_randomize_memory(void)</span>
 	unsigned long remain_entropy;
 
 	/*
<span class="p_del">-	 * All these BUILD_BUG_ON checks ensures the memory layout is</span>
<span class="p_del">-	 * consistent with the vaddr_start/vaddr_end variables.</span>
<span class="p_add">+	 * These BUILD_BUG_ON checks ensure the memory layout is consistent</span>
<span class="p_add">+	 * with the vaddr_start/vaddr_end variables. These checks are very</span>
<span class="p_add">+	 * limited....</span>
 	 */
 	BUILD_BUG_ON(vaddr_start &gt;= vaddr_end);
<span class="p_del">-	BUILD_BUG_ON(IS_ENABLED(CONFIG_X86_ESPFIX64) &amp;&amp;</span>
<span class="p_del">-		     vaddr_end &gt;= EFI_VA_END);</span>
<span class="p_del">-	BUILD_BUG_ON((IS_ENABLED(CONFIG_X86_ESPFIX64) ||</span>
<span class="p_del">-		      IS_ENABLED(CONFIG_EFI)) &amp;&amp;</span>
<span class="p_del">-		     vaddr_end &gt;= __START_KERNEL_map);</span>
<span class="p_add">+	BUILD_BUG_ON(vaddr_end != CPU_ENTRY_AREA_BASE);</span>
 	BUILD_BUG_ON(vaddr_end &gt; __START_KERNEL_map);
 
 	if (!kaslr_memory_enabled())
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index 2da28ba97508..43d4a4a29037 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -56,13 +56,13 @@</span> <span class="p_context"></span>
 
 static void __init pti_print_if_insecure(const char *reason)
 {
<span class="p_del">-	if (boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		pr_info(&quot;%s\n&quot;, reason);
 }
 
 static void __init pti_print_if_secure(const char *reason)
 {
<span class="p_del">-	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		pr_info(&quot;%s\n&quot;, reason);
 }
 
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> void __init pti_check_boottime_disable(void)</span>
 	}
 
 autosel:
<span class="p_del">-	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		return;
 enable:
 	setup_force_cpu_cap(X86_FEATURE_PTI);
<span class="p_header">diff --git a/arch/x86/platform/efi/quirks.c b/arch/x86/platform/efi/quirks.c</span>
<span class="p_header">index 8a99a2e96537..5b513ccffde4 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/quirks.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/quirks.c</span>
<span class="p_chunk">@@ -592,7 +592,18 @@</span> <span class="p_context"> static int qrk_capsule_setup_info(struct capsule_info *cap_info, void **pkbuff,</span>
 	/*
 	 * Update the first page pointer to skip over the CSH header.
 	 */
<span class="p_del">-	cap_info-&gt;pages[0] += csh-&gt;headersize;</span>
<span class="p_add">+	cap_info-&gt;phys[0] += csh-&gt;headersize;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * cap_info-&gt;capsule should point at a virtual mapping of the entire</span>
<span class="p_add">+	 * capsule, starting at the capsule header. Our image has the Quark</span>
<span class="p_add">+	 * security header prepended, so we cannot rely on the default vmap()</span>
<span class="p_add">+	 * mapping created by the generic capsule code.</span>
<span class="p_add">+	 * Given that the Quark firmware does not appear to care about the</span>
<span class="p_add">+	 * virtual mapping, let&#39;s just point cap_info-&gt;capsule at our copy</span>
<span class="p_add">+	 * of the capsule header.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cap_info-&gt;capsule = &amp;cap_info-&gt;header;</span>
 
 	return 1;
 }
<span class="p_header">diff --git a/crypto/chacha20poly1305.c b/crypto/chacha20poly1305.c</span>
<span class="p_header">index db1bc3147bc4..600afa99941f 100644</span>
<span class="p_header">--- a/crypto/chacha20poly1305.c</span>
<span class="p_header">+++ b/crypto/chacha20poly1305.c</span>
<span class="p_chunk">@@ -610,6 +610,11 @@</span> <span class="p_context"> static int chachapoly_create(struct crypto_template *tmpl, struct rtattr **tb,</span>
 						    algt-&gt;mask));
 	if (IS_ERR(poly))
 		return PTR_ERR(poly);
<span class="p_add">+	poly_hash = __crypto_hash_alg_common(poly);</span>
<span class="p_add">+</span>
<span class="p_add">+	err = -EINVAL;</span>
<span class="p_add">+	if (poly_hash-&gt;digestsize != POLY1305_DIGEST_SIZE)</span>
<span class="p_add">+		goto out_put_poly;</span>
 
 	err = -ENOMEM;
 	inst = kzalloc(sizeof(*inst) + sizeof(*ctx), GFP_KERNEL);
<span class="p_chunk">@@ -618,7 +623,6 @@</span> <span class="p_context"> static int chachapoly_create(struct crypto_template *tmpl, struct rtattr **tb,</span>
 
 	ctx = aead_instance_ctx(inst);
 	ctx-&gt;saltlen = CHACHAPOLY_IV_SIZE - ivsize;
<span class="p_del">-	poly_hash = __crypto_hash_alg_common(poly);</span>
 	err = crypto_init_ahash_spawn(&amp;ctx-&gt;poly, poly_hash,
 				      aead_crypto_instance(inst));
 	if (err)
<span class="p_header">diff --git a/crypto/pcrypt.c b/crypto/pcrypt.c</span>
<span class="p_header">index ee9cfb99fe25..f8ec3d4ba4a8 100644</span>
<span class="p_header">--- a/crypto/pcrypt.c</span>
<span class="p_header">+++ b/crypto/pcrypt.c</span>
<span class="p_chunk">@@ -254,6 +254,14 @@</span> <span class="p_context"> static void pcrypt_aead_exit_tfm(struct crypto_aead *tfm)</span>
 	crypto_free_aead(ctx-&gt;child);
 }
 
<span class="p_add">+static void pcrypt_free(struct aead_instance *inst)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pcrypt_instance_ctx *ctx = aead_instance_ctx(inst);</span>
<span class="p_add">+</span>
<span class="p_add">+	crypto_drop_aead(&amp;ctx-&gt;spawn);</span>
<span class="p_add">+	kfree(inst);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int pcrypt_init_instance(struct crypto_instance *inst,
 				struct crypto_alg *alg)
 {
<span class="p_chunk">@@ -319,6 +327,8 @@</span> <span class="p_context"> static int pcrypt_create_aead(struct crypto_template *tmpl, struct rtattr **tb,</span>
 	inst-&gt;alg.encrypt = pcrypt_aead_encrypt;
 	inst-&gt;alg.decrypt = pcrypt_aead_decrypt;
 
<span class="p_add">+	inst-&gt;free = pcrypt_free;</span>
<span class="p_add">+</span>
 	err = aead_register_instance(tmpl, inst);
 	if (err)
 		goto out_drop_aead;
<span class="p_chunk">@@ -349,14 +359,6 @@</span> <span class="p_context"> static int pcrypt_create(struct crypto_template *tmpl, struct rtattr **tb)</span>
 	return -EINVAL;
 }
 
<span class="p_del">-static void pcrypt_free(struct crypto_instance *inst)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct pcrypt_instance_ctx *ctx = crypto_instance_ctx(inst);</span>
<span class="p_del">-</span>
<span class="p_del">-	crypto_drop_aead(&amp;ctx-&gt;spawn);</span>
<span class="p_del">-	kfree(inst);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static int pcrypt_cpumask_change_notify(struct notifier_block *self,
 					unsigned long val, void *data)
 {
<span class="p_chunk">@@ -469,7 +471,6 @@</span> <span class="p_context"> static void pcrypt_fini_padata(struct padata_pcrypt *pcrypt)</span>
 static struct crypto_template pcrypt_tmpl = {
 	.name = &quot;pcrypt&quot;,
 	.create = pcrypt_create,
<span class="p_del">-	.free = pcrypt_free,</span>
 	.module = THIS_MODULE,
 };
 
<span class="p_header">diff --git a/drivers/bus/sunxi-rsb.c b/drivers/bus/sunxi-rsb.c</span>
<span class="p_header">index 328ca93781cf..1b76d9585902 100644</span>
<span class="p_header">--- a/drivers/bus/sunxi-rsb.c</span>
<span class="p_header">+++ b/drivers/bus/sunxi-rsb.c</span>
<span class="p_chunk">@@ -178,6 +178,7 @@</span> <span class="p_context"> static struct bus_type sunxi_rsb_bus = {</span>
 	.match		= sunxi_rsb_device_match,
 	.probe		= sunxi_rsb_device_probe,
 	.remove		= sunxi_rsb_device_remove,
<span class="p_add">+	.uevent		= of_device_uevent_modalias,</span>
 };
 
 static void sunxi_rsb_dev_release(struct device *dev)
<span class="p_header">diff --git a/drivers/crypto/chelsio/Kconfig b/drivers/crypto/chelsio/Kconfig</span>
<span class="p_header">index 3e104f5aa0c2..b56b3f711d94 100644</span>
<span class="p_header">--- a/drivers/crypto/chelsio/Kconfig</span>
<span class="p_header">+++ b/drivers/crypto/chelsio/Kconfig</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"> config CRYPTO_DEV_CHELSIO</span>
 	select CRYPTO_SHA256
 	select CRYPTO_SHA512
 	select CRYPTO_AUTHENC
<span class="p_add">+	select CRYPTO_GF128MUL</span>
 	---help---
 	  The Chelsio Crypto Co-processor driver for T6 adapters.
 
<span class="p_header">diff --git a/drivers/crypto/n2_core.c b/drivers/crypto/n2_core.c</span>
<span class="p_header">index a9fd8b9e86cd..699ee5a9a8f9 100644</span>
<span class="p_header">--- a/drivers/crypto/n2_core.c</span>
<span class="p_header">+++ b/drivers/crypto/n2_core.c</span>
<span class="p_chunk">@@ -1625,6 +1625,7 @@</span> <span class="p_context"> static int queue_cache_init(void)</span>
 					  CWQ_ENTRY_SIZE, 0, NULL);
 	if (!queue_cache[HV_NCS_QTYPE_CWQ - 1]) {
 		kmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);
<span class="p_add">+		queue_cache[HV_NCS_QTYPE_MAU - 1] = NULL;</span>
 		return -ENOMEM;
 	}
 	return 0;
<span class="p_chunk">@@ -1634,6 +1635,8 @@</span> <span class="p_context"> static void queue_cache_destroy(void)</span>
 {
 	kmem_cache_destroy(queue_cache[HV_NCS_QTYPE_MAU - 1]);
 	kmem_cache_destroy(queue_cache[HV_NCS_QTYPE_CWQ - 1]);
<span class="p_add">+	queue_cache[HV_NCS_QTYPE_MAU - 1] = NULL;</span>
<span class="p_add">+	queue_cache[HV_NCS_QTYPE_CWQ - 1] = NULL;</span>
 }
 
 static long spu_queue_register_workfn(void *arg)
<span class="p_header">diff --git a/drivers/firmware/efi/capsule-loader.c b/drivers/firmware/efi/capsule-loader.c</span>
<span class="p_header">index ec8ac5c4dd84..055e2e8f985a 100644</span>
<span class="p_header">--- a/drivers/firmware/efi/capsule-loader.c</span>
<span class="p_header">+++ b/drivers/firmware/efi/capsule-loader.c</span>
<span class="p_chunk">@@ -20,10 +20,6 @@</span> <span class="p_context"></span>
 
 #define NO_FURTHER_WRITE_ACTION -1
 
<span class="p_del">-#ifndef phys_to_page</span>
<span class="p_del">-#define phys_to_page(x)		pfn_to_page((x) &gt;&gt; PAGE_SHIFT)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /**
  * efi_free_all_buff_pages - free all previous allocated buffer pages
  * @cap_info: pointer to current instance of capsule_info structure
<span class="p_chunk">@@ -35,7 +31,7 @@</span> <span class="p_context"></span>
 static void efi_free_all_buff_pages(struct capsule_info *cap_info)
 {
 	while (cap_info-&gt;index &gt; 0)
<span class="p_del">-		__free_page(phys_to_page(cap_info-&gt;pages[--cap_info-&gt;index]));</span>
<span class="p_add">+		__free_page(cap_info-&gt;pages[--cap_info-&gt;index]);</span>
 
 	cap_info-&gt;index = NO_FURTHER_WRITE_ACTION;
 }
<span class="p_chunk">@@ -71,6 +67,14 @@</span> <span class="p_context"> int __efi_capsule_setup_info(struct capsule_info *cap_info)</span>
 
 	cap_info-&gt;pages = temp_page;
 
<span class="p_add">+	temp_page = krealloc(cap_info-&gt;phys,</span>
<span class="p_add">+			     pages_needed * sizeof(phys_addr_t *),</span>
<span class="p_add">+			     GFP_KERNEL | __GFP_ZERO);</span>
<span class="p_add">+	if (!temp_page)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	cap_info-&gt;phys = temp_page;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -105,9 +109,24 @@</span> <span class="p_context"> int __weak efi_capsule_setup_info(struct capsule_info *cap_info, void *kbuff,</span>
  **/
 static ssize_t efi_capsule_submit_update(struct capsule_info *cap_info)
 {
<span class="p_add">+	bool do_vunmap = false;</span>
 	int ret;
 
<span class="p_del">-	ret = efi_capsule_update(&amp;cap_info-&gt;header, cap_info-&gt;pages);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * cap_info-&gt;capsule may have been assigned already by a quirk</span>
<span class="p_add">+	 * handler, so only overwrite it if it is NULL</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!cap_info-&gt;capsule) {</span>
<span class="p_add">+		cap_info-&gt;capsule = vmap(cap_info-&gt;pages, cap_info-&gt;index,</span>
<span class="p_add">+					 VM_MAP, PAGE_KERNEL);</span>
<span class="p_add">+		if (!cap_info-&gt;capsule)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		do_vunmap = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = efi_capsule_update(cap_info-&gt;capsule, cap_info-&gt;phys);</span>
<span class="p_add">+	if (do_vunmap)</span>
<span class="p_add">+		vunmap(cap_info-&gt;capsule);</span>
 	if (ret) {
 		pr_err(&quot;capsule update failed\n&quot;);
 		return ret;
<span class="p_chunk">@@ -165,10 +184,12 @@</span> <span class="p_context"> static ssize_t efi_capsule_write(struct file *file, const char __user *buff,</span>
 			goto failed;
 		}
 
<span class="p_del">-		cap_info-&gt;pages[cap_info-&gt;index++] = page_to_phys(page);</span>
<span class="p_add">+		cap_info-&gt;pages[cap_info-&gt;index] = page;</span>
<span class="p_add">+		cap_info-&gt;phys[cap_info-&gt;index] = page_to_phys(page);</span>
 		cap_info-&gt;page_bytes_remain = PAGE_SIZE;
<span class="p_add">+		cap_info-&gt;index++;</span>
 	} else {
<span class="p_del">-		page = phys_to_page(cap_info-&gt;pages[cap_info-&gt;index - 1]);</span>
<span class="p_add">+		page = cap_info-&gt;pages[cap_info-&gt;index - 1];</span>
 	}
 
 	kbuff = kmap(page);
<span class="p_chunk">@@ -252,6 +273,7 @@</span> <span class="p_context"> static int efi_capsule_release(struct inode *inode, struct file *file)</span>
 	struct capsule_info *cap_info = file-&gt;private_data;
 
 	kfree(cap_info-&gt;pages);
<span class="p_add">+	kfree(cap_info-&gt;phys);</span>
 	kfree(file-&gt;private_data);
 	file-&gt;private_data = NULL;
 	return 0;
<span class="p_chunk">@@ -281,6 +303,13 @@</span> <span class="p_context"> static int efi_capsule_open(struct inode *inode, struct file *file)</span>
 		return -ENOMEM;
 	}
 
<span class="p_add">+	cap_info-&gt;phys = kzalloc(sizeof(void *), GFP_KERNEL);</span>
<span class="p_add">+	if (!cap_info-&gt;phys) {</span>
<span class="p_add">+		kfree(cap_info-&gt;pages);</span>
<span class="p_add">+		kfree(cap_info);</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	file-&gt;private_data = cap_info;
 
 	return 0;
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_header">index c9bcc6c45012..ce2ed16f2a30 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_chunk">@@ -6944,6 +6944,7 @@</span> <span class="p_context"> enum {</span>
 #define  RESET_PCH_HANDSHAKE_ENABLE	(1&lt;&lt;4)
 
 #define GEN8_CHICKEN_DCPR_1		_MMIO(0x46430)
<span class="p_add">+#define   SKL_SELECT_ALTERNATE_DC_EXIT	(1&lt;&lt;30)</span>
 #define   MASK_WAKEMEM			(1&lt;&lt;13)
 
 #define SKL_DFSM			_MMIO(0x51000)
<span class="p_chunk">@@ -8475,6 +8476,7 @@</span> <span class="p_context"> enum skl_power_gate {</span>
 #define  BXT_CDCLK_CD2X_DIV_SEL_2	(2&lt;&lt;22)
 #define  BXT_CDCLK_CD2X_DIV_SEL_4	(3&lt;&lt;22)
 #define  BXT_CDCLK_CD2X_PIPE(pipe)	((pipe)&lt;&lt;20)
<span class="p_add">+#define  CDCLK_DIVMUX_CD_OVERRIDE	(1&lt;&lt;19)</span>
 #define  BXT_CDCLK_CD2X_PIPE_NONE	BXT_CDCLK_CD2X_PIPE(3)
 #define  BXT_CDCLK_SSA_PRECHARGE_ENABLE	(1&lt;&lt;16)
 #define  CDCLK_FREQ_DECIMAL_MASK	(0x7ff)
<span class="p_header">diff --git a/drivers/gpu/drm/i915/intel_cdclk.c b/drivers/gpu/drm/i915/intel_cdclk.c</span>
<span class="p_header">index 1241e5891b29..26a8dcd2c549 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/intel_cdclk.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/intel_cdclk.c</span>
<span class="p_chunk">@@ -859,16 +859,10 @@</span> <span class="p_context"> static void skl_set_preferred_cdclk_vco(struct drm_i915_private *dev_priv,</span>
 
 static void skl_dpll0_enable(struct drm_i915_private *dev_priv, int vco)
 {
<span class="p_del">-	int min_cdclk = skl_calc_cdclk(0, vco);</span>
 	u32 val;
 
 	WARN_ON(vco != 8100000 &amp;&amp; vco != 8640000);
 
<span class="p_del">-	/* select the minimum CDCLK before enabling DPLL 0 */</span>
<span class="p_del">-	val = CDCLK_FREQ_337_308 | skl_cdclk_decimal(min_cdclk);</span>
<span class="p_del">-	I915_WRITE(CDCLK_CTL, val);</span>
<span class="p_del">-	POSTING_READ(CDCLK_CTL);</span>
<span class="p_del">-</span>
 	/*
 	 * We always enable DPLL0 with the lowest link rate possible, but still
 	 * taking into account the VCO required to operate the eDP panel at the
<span class="p_chunk">@@ -922,7 +916,7 @@</span> <span class="p_context"> static void skl_set_cdclk(struct drm_i915_private *dev_priv,</span>
 {
 	int cdclk = cdclk_state-&gt;cdclk;
 	int vco = cdclk_state-&gt;vco;
<span class="p_del">-	u32 freq_select, pcu_ack;</span>
<span class="p_add">+	u32 freq_select, pcu_ack, cdclk_ctl;</span>
 	int ret;
 
 	WARN_ON((cdclk == 24000) != (vco == 0));
<span class="p_chunk">@@ -939,7 +933,7 @@</span> <span class="p_context"> static void skl_set_cdclk(struct drm_i915_private *dev_priv,</span>
 		return;
 	}
 
<span class="p_del">-	/* set CDCLK_CTL */</span>
<span class="p_add">+	/* Choose frequency for this cdclk */</span>
 	switch (cdclk) {
 	case 450000:
 	case 432000:
<span class="p_chunk">@@ -967,10 +961,33 @@</span> <span class="p_context"> static void skl_set_cdclk(struct drm_i915_private *dev_priv,</span>
 	    dev_priv-&gt;cdclk.hw.vco != vco)
 		skl_dpll0_disable(dev_priv);
 
<span class="p_add">+	cdclk_ctl = I915_READ(CDCLK_CTL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dev_priv-&gt;cdclk.hw.vco != vco) {</span>
<span class="p_add">+		/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+		cdclk_ctl &amp;= ~(CDCLK_FREQ_SEL_MASK | CDCLK_FREQ_DECIMAL_MASK);</span>
<span class="p_add">+		cdclk_ctl |= freq_select | skl_cdclk_decimal(cdclk);</span>
<span class="p_add">+		I915_WRITE(CDCLK_CTL, cdclk_ctl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+	cdclk_ctl |= CDCLK_DIVMUX_CD_OVERRIDE;</span>
<span class="p_add">+	I915_WRITE(CDCLK_CTL, cdclk_ctl);</span>
<span class="p_add">+	POSTING_READ(CDCLK_CTL);</span>
<span class="p_add">+</span>
 	if (dev_priv-&gt;cdclk.hw.vco != vco)
 		skl_dpll0_enable(dev_priv, vco);
 
<span class="p_del">-	I915_WRITE(CDCLK_CTL, freq_select | skl_cdclk_decimal(cdclk));</span>
<span class="p_add">+	/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+	cdclk_ctl &amp;= ~(CDCLK_FREQ_SEL_MASK | CDCLK_FREQ_DECIMAL_MASK);</span>
<span class="p_add">+	I915_WRITE(CDCLK_CTL, cdclk_ctl);</span>
<span class="p_add">+</span>
<span class="p_add">+	cdclk_ctl |= freq_select | skl_cdclk_decimal(cdclk);</span>
<span class="p_add">+	I915_WRITE(CDCLK_CTL, cdclk_ctl);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+	cdclk_ctl &amp;= ~CDCLK_DIVMUX_CD_OVERRIDE;</span>
<span class="p_add">+	I915_WRITE(CDCLK_CTL, cdclk_ctl);</span>
 	POSTING_READ(CDCLK_CTL);
 
 	/* inform PCU of the change */
<span class="p_header">diff --git a/drivers/gpu/drm/i915/intel_runtime_pm.c b/drivers/gpu/drm/i915/intel_runtime_pm.c</span>
<span class="p_header">index 49577eba8e7e..51cb5293bf43 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/intel_runtime_pm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/intel_runtime_pm.c</span>
<span class="p_chunk">@@ -598,6 +598,11 @@</span> <span class="p_context"> void gen9_enable_dc5(struct drm_i915_private *dev_priv)</span>
 
 	DRM_DEBUG_KMS(&quot;Enabling DC5\n&quot;);
 
<span class="p_add">+	/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+	if (IS_GEN9_BC(dev_priv))</span>
<span class="p_add">+		I915_WRITE(GEN8_CHICKEN_DCPR_1, I915_READ(GEN8_CHICKEN_DCPR_1) |</span>
<span class="p_add">+			   SKL_SELECT_ALTERNATE_DC_EXIT);</span>
<span class="p_add">+</span>
 	gen9_set_dc_state(dev_priv, DC_STATE_EN_UPTO_DC5);
 }
 
<span class="p_chunk">@@ -625,6 +630,11 @@</span> <span class="p_context"> void skl_disable_dc6(struct drm_i915_private *dev_priv)</span>
 {
 	DRM_DEBUG_KMS(&quot;Disabling DC6\n&quot;);
 
<span class="p_add">+	/* Wa Display #1183: skl,kbl,cfl */</span>
<span class="p_add">+	if (IS_GEN9_BC(dev_priv))</span>
<span class="p_add">+		I915_WRITE(GEN8_CHICKEN_DCPR_1, I915_READ(GEN8_CHICKEN_DCPR_1) |</span>
<span class="p_add">+			   SKL_SELECT_ALTERNATE_DC_EXIT);</span>
<span class="p_add">+</span>
 	gen9_set_dc_state(dev_priv, DC_STATE_DISABLE);
 }
 
<span class="p_chunk">@@ -1786,6 +1796,7 @@</span> <span class="p_context"> void intel_display_power_put(struct drm_i915_private *dev_priv,</span>
 	GLK_DISPLAY_POWERWELL_2_POWER_DOMAINS |		\
 	BIT_ULL(POWER_DOMAIN_MODESET) |			\
 	BIT_ULL(POWER_DOMAIN_AUX_A) |			\
<span class="p_add">+	BIT_ULL(POWER_DOMAIN_GMBUS) |			\</span>
 	BIT_ULL(POWER_DOMAIN_INIT))
 
 #define CNL_DISPLAY_POWERWELL_2_POWER_DOMAINS (		\
<span class="p_header">diff --git a/drivers/input/mouse/elantech.c b/drivers/input/mouse/elantech.c</span>
<span class="p_header">index b84cd978fce2..a4aaa748e987 100644</span>
<span class="p_header">--- a/drivers/input/mouse/elantech.c</span>
<span class="p_header">+++ b/drivers/input/mouse/elantech.c</span>
<span class="p_chunk">@@ -1613,7 +1613,7 @@</span> <span class="p_context"> static int elantech_set_properties(struct elantech_data *etd)</span>
 		case 5:
 			etd-&gt;hw_version = 3;
 			break;
<span class="p_del">-		case 6 ... 14:</span>
<span class="p_add">+		case 6 ... 15:</span>
 			etd-&gt;hw_version = 4;
 			break;
 		default:
<span class="p_header">diff --git a/drivers/iommu/arm-smmu-v3.c b/drivers/iommu/arm-smmu-v3.c</span>
<span class="p_header">index e67ba6c40faf..8f7a3c00b6cf 100644</span>
<span class="p_header">--- a/drivers/iommu/arm-smmu-v3.c</span>
<span class="p_header">+++ b/drivers/iommu/arm-smmu-v3.c</span>
<span class="p_chunk">@@ -1611,13 +1611,15 @@</span> <span class="p_context"> static int arm_smmu_domain_finalise(struct iommu_domain *domain)</span>
 	domain-&gt;pgsize_bitmap = pgtbl_cfg.pgsize_bitmap;
 	domain-&gt;geometry.aperture_end = (1UL &lt;&lt; ias) - 1;
 	domain-&gt;geometry.force_aperture = true;
<span class="p_del">-	smmu_domain-&gt;pgtbl_ops = pgtbl_ops;</span>
 
 	ret = finalise_stage_fn(smmu_domain, &amp;pgtbl_cfg);
<span class="p_del">-	if (ret &lt; 0)</span>
<span class="p_add">+	if (ret &lt; 0) {</span>
 		free_io_pgtable_ops(pgtbl_ops);
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	return ret;</span>
<span class="p_add">+	smmu_domain-&gt;pgtbl_ops = pgtbl_ops;</span>
<span class="p_add">+	return 0;</span>
 }
 
 static __le64 *arm_smmu_get_step_for_sid(struct arm_smmu_device *smmu, u32 sid)
<span class="p_chunk">@@ -1644,7 +1646,7 @@</span> <span class="p_context"> static __le64 *arm_smmu_get_step_for_sid(struct arm_smmu_device *smmu, u32 sid)</span>
 
 static void arm_smmu_install_ste_for_dev(struct iommu_fwspec *fwspec)
 {
<span class="p_del">-	int i;</span>
<span class="p_add">+	int i, j;</span>
 	struct arm_smmu_master_data *master = fwspec-&gt;iommu_priv;
 	struct arm_smmu_device *smmu = master-&gt;smmu;
 
<span class="p_chunk">@@ -1652,6 +1654,13 @@</span> <span class="p_context"> static void arm_smmu_install_ste_for_dev(struct iommu_fwspec *fwspec)</span>
 		u32 sid = fwspec-&gt;ids[i];
 		__le64 *step = arm_smmu_get_step_for_sid(smmu, sid);
 
<span class="p_add">+		/* Bridged PCI devices may end up with duplicated IDs */</span>
<span class="p_add">+		for (j = 0; j &lt; i; j++)</span>
<span class="p_add">+			if (fwspec-&gt;ids[j] == sid)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		if (j &lt; i)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		arm_smmu_write_strtab_ent(smmu, sid, step, &amp;master-&gt;ste);
 	}
 }
<span class="p_header">diff --git a/drivers/mtd/nand/pxa3xx_nand.c b/drivers/mtd/nand/pxa3xx_nand.c</span>
<span class="p_header">index 85cff68643e0..125b744c9c28 100644</span>
<span class="p_header">--- a/drivers/mtd/nand/pxa3xx_nand.c</span>
<span class="p_header">+++ b/drivers/mtd/nand/pxa3xx_nand.c</span>
<span class="p_chunk">@@ -950,6 +950,7 @@</span> <span class="p_context"> static void prepare_start_command(struct pxa3xx_nand_info *info, int command)</span>
 
 	switch (command) {
 	case NAND_CMD_READ0:
<span class="p_add">+	case NAND_CMD_READOOB:</span>
 	case NAND_CMD_PAGEPROG:
 		info-&gt;use_ecc = 1;
 		break;
<span class="p_header">diff --git a/fs/btrfs/delayed-inode.c b/fs/btrfs/delayed-inode.c</span>
<span class="p_header">index 19e4ad2f3f2e..0c4b690cf761 100644</span>
<span class="p_header">--- a/fs/btrfs/delayed-inode.c</span>
<span class="p_header">+++ b/fs/btrfs/delayed-inode.c</span>
<span class="p_chunk">@@ -87,6 +87,7 @@</span> <span class="p_context"> static struct btrfs_delayed_node *btrfs_get_delayed_node(</span>
 
 	spin_lock(&amp;root-&gt;inode_lock);
 	node = radix_tree_lookup(&amp;root-&gt;delayed_nodes_tree, ino);
<span class="p_add">+</span>
 	if (node) {
 		if (btrfs_inode-&gt;delayed_node) {
 			refcount_inc(&amp;node-&gt;refs);	/* can be accessed */
<span class="p_chunk">@@ -94,9 +95,30 @@</span> <span class="p_context"> static struct btrfs_delayed_node *btrfs_get_delayed_node(</span>
 			spin_unlock(&amp;root-&gt;inode_lock);
 			return node;
 		}
<span class="p_del">-		btrfs_inode-&gt;delayed_node = node;</span>
<span class="p_del">-		/* can be accessed and cached in the inode */</span>
<span class="p_del">-		refcount_add(2, &amp;node-&gt;refs);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * It&#39;s possible that we&#39;re racing into the middle of removing</span>
<span class="p_add">+		 * this node from the radix tree.  In this case, the refcount</span>
<span class="p_add">+		 * was zero and it should never go back to one.  Just return</span>
<span class="p_add">+		 * NULL like it was never in the radix at all; our release</span>
<span class="p_add">+		 * function is in the process of removing it.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Some implementations of refcount_inc refuse to bump the</span>
<span class="p_add">+		 * refcount once it has hit zero.  If we don&#39;t do this dance</span>
<span class="p_add">+		 * here, refcount_inc() may decide to just WARN_ONCE() instead</span>
<span class="p_add">+		 * of actually bumping the refcount.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If this node is properly in the radix, we want to bump the</span>
<span class="p_add">+		 * refcount twice, once for the inode and once for this get</span>
<span class="p_add">+		 * operation.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (refcount_inc_not_zero(&amp;node-&gt;refs)) {</span>
<span class="p_add">+			refcount_inc(&amp;node-&gt;refs);</span>
<span class="p_add">+			btrfs_inode-&gt;delayed_node = node;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			node = NULL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		spin_unlock(&amp;root-&gt;inode_lock);
 		return node;
 	}
<span class="p_chunk">@@ -254,17 +276,18 @@</span> <span class="p_context"> static void __btrfs_release_delayed_node(</span>
 	mutex_unlock(&amp;delayed_node-&gt;mutex);
 
 	if (refcount_dec_and_test(&amp;delayed_node-&gt;refs)) {
<span class="p_del">-		bool free = false;</span>
 		struct btrfs_root *root = delayed_node-&gt;root;
<span class="p_add">+</span>
 		spin_lock(&amp;root-&gt;inode_lock);
<span class="p_del">-		if (refcount_read(&amp;delayed_node-&gt;refs) == 0) {</span>
<span class="p_del">-			radix_tree_delete(&amp;root-&gt;delayed_nodes_tree,</span>
<span class="p_del">-					  delayed_node-&gt;inode_id);</span>
<span class="p_del">-			free = true;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Once our refcount goes to zero, nobody is allowed to bump it</span>
<span class="p_add">+		 * back up.  We can delete it now.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ASSERT(refcount_read(&amp;delayed_node-&gt;refs) == 0);</span>
<span class="p_add">+		radix_tree_delete(&amp;root-&gt;delayed_nodes_tree,</span>
<span class="p_add">+				  delayed_node-&gt;inode_id);</span>
 		spin_unlock(&amp;root-&gt;inode_lock);
<span class="p_del">-		if (free)</span>
<span class="p_del">-			kmem_cache_free(delayed_node_cache, delayed_node);</span>
<span class="p_add">+		kmem_cache_free(delayed_node_cache, delayed_node);</span>
 	}
 }
 
<span class="p_header">diff --git a/fs/proc/cpuinfo.c b/fs/proc/cpuinfo.c</span>
<span class="p_header">index e0f867cd8553..96f1087e372c 100644</span>
<span class="p_header">--- a/fs/proc/cpuinfo.c</span>
<span class="p_header">+++ b/fs/proc/cpuinfo.c</span>
<span class="p_chunk">@@ -1,12 +1,18 @@</span> <span class="p_context"></span>
 // SPDX-License-Identifier: GPL-2.0
<span class="p_add">+#include &lt;linux/cpufreq.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/proc_fs.h&gt;
 #include &lt;linux/seq_file.h&gt;
 
<span class="p_add">+__weak void arch_freq_prepare_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 extern const struct seq_operations cpuinfo_op;
 static int cpuinfo_open(struct inode *inode, struct file *file)
 {
<span class="p_add">+	arch_freq_prepare_all();</span>
 	return seq_open(file, &amp;cpuinfo_op);
 }
 
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 1c713fd5b3e6..5aa392eae1c3 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -570,11 +570,14 @@</span> <span class="p_context"> int handle_userfault(struct vm_fault *vmf, unsigned long reason)</span>
 static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,
 					      struct userfaultfd_wait_queue *ewq)
 {
<span class="p_add">+	struct userfaultfd_ctx *release_new_ctx;</span>
<span class="p_add">+</span>
 	if (WARN_ON_ONCE(current-&gt;flags &amp; PF_EXITING))
 		goto out;
 
 	ewq-&gt;ctx = ctx;
 	init_waitqueue_entry(&amp;ewq-&gt;wq, current);
<span class="p_add">+	release_new_ctx = NULL;</span>
 
 	spin_lock(&amp;ctx-&gt;event_wqh.lock);
 	/*
<span class="p_chunk">@@ -601,8 +604,7 @@</span> <span class="p_context"> static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,</span>
 				new = (struct userfaultfd_ctx *)
 					(unsigned long)
 					ewq-&gt;msg.arg.reserved.reserved1;
<span class="p_del">-</span>
<span class="p_del">-				userfaultfd_ctx_put(new);</span>
<span class="p_add">+				release_new_ctx = new;</span>
 			}
 			break;
 		}
<span class="p_chunk">@@ -617,6 +619,20 @@</span> <span class="p_context"> static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,</span>
 	__set_current_state(TASK_RUNNING);
 	spin_unlock(&amp;ctx-&gt;event_wqh.lock);
 
<span class="p_add">+	if (release_new_ctx) {</span>
<span class="p_add">+		struct vm_area_struct *vma;</span>
<span class="p_add">+		struct mm_struct *mm = release_new_ctx-&gt;mm;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* the various vma-&gt;vm_userfaultfd_ctx still points to it */</span>
<span class="p_add">+		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)</span>
<span class="p_add">+			if (vma-&gt;vm_userfaultfd_ctx.ctx == release_new_ctx)</span>
<span class="p_add">+				vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;</span>
<span class="p_add">+		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+		userfaultfd_ctx_put(release_new_ctx);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * ctx may go away after this if the userfault pseudo fd is
 	 * already released.
<span class="p_header">diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h</span>
<span class="p_header">index 537ff842ff73..cbf85c4c745f 100644</span>
<span class="p_header">--- a/include/linux/cpufreq.h</span>
<span class="p_header">+++ b/include/linux/cpufreq.h</span>
<span class="p_chunk">@@ -917,6 +917,7 @@</span> <span class="p_context"> static inline bool policy_has_boost_freq(struct cpufreq_policy *policy)</span>
 }
 #endif
 
<span class="p_add">+extern void arch_freq_prepare_all(void);</span>
 extern unsigned int arch_freq_get_on_cpu(int cpu);
 
 /* the following are really really optional */
<span class="p_header">diff --git a/include/linux/efi.h b/include/linux/efi.h</span>
<span class="p_header">index d813f7b04da7..29fdf8029cf6 100644</span>
<span class="p_header">--- a/include/linux/efi.h</span>
<span class="p_header">+++ b/include/linux/efi.h</span>
<span class="p_chunk">@@ -140,11 +140,13 @@</span> <span class="p_context"> struct efi_boot_memmap {</span>
 
 struct capsule_info {
 	efi_capsule_header_t	header;
<span class="p_add">+	efi_capsule_header_t	*capsule;</span>
 	int			reset_type;
 	long			index;
 	size_t			count;
 	size_t			total_size;
<span class="p_del">-	phys_addr_t		*pages;</span>
<span class="p_add">+	struct page		**pages;</span>
<span class="p_add">+	phys_addr_t		*phys;</span>
 	size_t			page_bytes_remain;
 };
 
<span class="p_header">diff --git a/include/linux/fscache.h b/include/linux/fscache.h</span>
<span class="p_header">index f4ff47d4a893..fe0c349684fa 100644</span>
<span class="p_header">--- a/include/linux/fscache.h</span>
<span class="p_header">+++ b/include/linux/fscache.h</span>
<span class="p_chunk">@@ -755,7 +755,7 @@</span> <span class="p_context"> bool fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
 {
 	if (fscache_cookie_valid(cookie) &amp;&amp; PageFsCache(page))
 		return __fscache_maybe_release_page(cookie, page, gfp);
<span class="p_del">-	return false;</span>
<span class="p_add">+	return true;</span>
 }
 
 /**
<span class="p_header">diff --git a/kernel/acct.c b/kernel/acct.c</span>
<span class="p_header">index 6670fbd3e466..354578d253d5 100644</span>
<span class="p_header">--- a/kernel/acct.c</span>
<span class="p_header">+++ b/kernel/acct.c</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> static int check_free_space(struct bsd_acct_struct *acct)</span>
 {
 	struct kstatfs sbuf;
 
<span class="p_del">-	if (time_is_before_jiffies(acct-&gt;needcheck))</span>
<span class="p_add">+	if (time_is_after_jiffies(acct-&gt;needcheck))</span>
 		goto out;
 
 	/* May block */
<span class="p_header">diff --git a/kernel/signal.c b/kernel/signal.c</span>
<span class="p_header">index 8dcd8825b2de..1facff1dbbae 100644</span>
<span class="p_header">--- a/kernel/signal.c</span>
<span class="p_header">+++ b/kernel/signal.c</span>
<span class="p_chunk">@@ -78,7 +78,7 @@</span> <span class="p_context"> static int sig_task_ignored(struct task_struct *t, int sig, bool force)</span>
 	handler = sig_handler(t, sig);
 
 	if (unlikely(t-&gt;signal-&gt;flags &amp; SIGNAL_UNKILLABLE) &amp;&amp;
<span class="p_del">-			handler == SIG_DFL &amp;&amp; !force)</span>
<span class="p_add">+	    handler == SIG_DFL &amp;&amp; !(force &amp;&amp; sig_kernel_only(sig)))</span>
 		return 1;
 
 	return sig_handler_ignored(handler, sig);
<span class="p_chunk">@@ -94,13 +94,15 @@</span> <span class="p_context"> static int sig_ignored(struct task_struct *t, int sig, bool force)</span>
 	if (sigismember(&amp;t-&gt;blocked, sig) || sigismember(&amp;t-&gt;real_blocked, sig))
 		return 0;
 
<span class="p_del">-	if (!sig_task_ignored(t, sig, force))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
 	/*
<span class="p_del">-	 * Tracers may want to know about even ignored signals.</span>
<span class="p_add">+	 * Tracers may want to know about even ignored signal unless it</span>
<span class="p_add">+	 * is SIGKILL which can&#39;t be reported anyway but can be ignored</span>
<span class="p_add">+	 * by SIGNAL_UNKILLABLE task.</span>
 	 */
<span class="p_del">-	return !t-&gt;ptrace;</span>
<span class="p_add">+	if (t-&gt;ptrace &amp;&amp; sig != SIGKILL)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return sig_task_ignored(t, sig, force);</span>
 }
 
 /*
<span class="p_chunk">@@ -929,9 +931,9 @@</span> <span class="p_context"> static void complete_signal(int sig, struct task_struct *p, int group)</span>
 	 * then start taking the whole group down immediately.
 	 */
 	if (sig_fatal(p, sig) &amp;&amp;
<span class="p_del">-	    !(signal-&gt;flags &amp; (SIGNAL_UNKILLABLE | SIGNAL_GROUP_EXIT)) &amp;&amp;</span>
<span class="p_add">+	    !(signal-&gt;flags &amp; SIGNAL_GROUP_EXIT) &amp;&amp;</span>
 	    !sigismember(&amp;t-&gt;real_blocked, sig) &amp;&amp;
<span class="p_del">-	    (sig == SIGKILL || !t-&gt;ptrace)) {</span>
<span class="p_add">+	    (sig == SIGKILL || !p-&gt;ptrace)) {</span>
 		/*
 		 * This signal will be fatal to the whole group.
 		 */
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index ec39f730a0bf..58b629bb70de 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		next = pmd_addr_end(addr, end);
 		if (!is_swap_pmd(*pmd) &amp;&amp; !pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)
 				&amp;&amp; pmd_none_or_clear_bad(pmd))
<span class="p_del">-			continue;</span>
<span class="p_add">+			goto next;</span>
 
 		/* invoke the mmu notifier if the pmd is populated */
 		if (!mni_start) {
<span class="p_chunk">@@ -188,7 +188,7 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 					}
 
 					/* huge pmd was handled */
<span class="p_del">-					continue;</span>
<span class="p_add">+					goto next;</span>
 				}
 			}
 			/* fall through, the trans huge pmd just split */
<span class="p_chunk">@@ -196,6 +196,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
 				 dirty_accountable, prot_numa);
 		pages += this_pages;
<span class="p_add">+next:</span>
<span class="p_add">+		cond_resched();</span>
 	} while (pmd++, addr = next, addr != end);
 
 	if (mni_start)
<span class="p_header">diff --git a/mm/sparse.c b/mm/sparse.c</span>
<span class="p_header">index 60805abf98af..30e56a100ee8 100644</span>
<span class="p_header">--- a/mm/sparse.c</span>
<span class="p_header">+++ b/mm/sparse.c</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> void __init memory_present(int nid, unsigned long start, unsigned long end)</span>
 	if (unlikely(!mem_section)) {
 		unsigned long size, align;
 
<span class="p_del">-		size = sizeof(struct mem_section) * NR_SECTION_ROOTS;</span>
<span class="p_add">+		size = sizeof(struct mem_section*) * NR_SECTION_ROOTS;</span>
 		align = 1 &lt;&lt; (INTERNODE_CACHE_SHIFT);
 		mem_section = memblock_virt_alloc(size, align);
 	}
<span class="p_header">diff --git a/security/apparmor/mount.c b/security/apparmor/mount.c</span>
<span class="p_header">index 82a64b58041d..e395137ecff1 100644</span>
<span class="p_header">--- a/security/apparmor/mount.c</span>
<span class="p_header">+++ b/security/apparmor/mount.c</span>
<span class="p_chunk">@@ -330,6 +330,9 @@</span> <span class="p_context"> static int match_mnt_path_str(struct aa_profile *profile,</span>
 	AA_BUG(!mntpath);
 	AA_BUG(!buffer);
 
<span class="p_add">+	if (!PROFILE_MEDIATES(profile, AA_CLASS_MOUNT))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	error = aa_path_name(mntpath, path_flags(profile, mntpath), buffer,
 			     &amp;mntpnt, &amp;info, profile-&gt;disconnected);
 	if (error)
<span class="p_chunk">@@ -381,6 +384,9 @@</span> <span class="p_context"> static int match_mnt(struct aa_profile *profile, const struct path *path,</span>
 	AA_BUG(!profile);
 	AA_BUG(devpath &amp;&amp; !devbuffer);
 
<span class="p_add">+	if (!PROFILE_MEDIATES(profile, AA_CLASS_MOUNT))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	if (devpath) {
 		error = aa_path_name(devpath, path_flags(profile, devpath),
 				     devbuffer, &amp;devname, &amp;info,
<span class="p_chunk">@@ -559,6 +565,9 @@</span> <span class="p_context"> static int profile_umount(struct aa_profile *profile, struct path *path,</span>
 	AA_BUG(!profile);
 	AA_BUG(!path);
 
<span class="p_add">+	if (!PROFILE_MEDIATES(profile, AA_CLASS_MOUNT))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	error = aa_path_name(path, path_flags(profile, path), buffer, &amp;name,
 			     &amp;info, profile-&gt;disconnected);
 	if (error)
<span class="p_chunk">@@ -614,7 +623,8 @@</span> <span class="p_context"> static struct aa_label *build_pivotroot(struct aa_profile *profile,</span>
 	AA_BUG(!new_path);
 	AA_BUG(!old_path);
 
<span class="p_del">-	if (profile_unconfined(profile))</span>
<span class="p_add">+	if (profile_unconfined(profile) ||</span>
<span class="p_add">+	    !PROFILE_MEDIATES(profile, AA_CLASS_MOUNT))</span>
 		return aa_get_newest_label(&amp;profile-&gt;label);
 
 	error = aa_path_name(old_path, path_flags(profile, old_path),

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



