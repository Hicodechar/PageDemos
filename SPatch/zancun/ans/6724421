
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/4] mm: Defer flush of writable TLB entries - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/4] mm: Defer flush of writable TLB entries</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 6, 2015, 1:39 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436189996-7220-4-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6724421/mbox/"
   >mbox</a>
|
   <a href="/patch/6724421/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6724421/">/patch/6724421/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 58029C05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:41:00 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id C01BA20434
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:40:54 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A169D206DC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jul 2015 13:40:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756598AbbGFNkv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Jul 2015 09:40:51 -0400
Received: from cantor2.suse.de ([195.135.220.15]:49219 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755913AbbGFNkF (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Jul 2015 09:40:05 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 0FA41ADB9;
	Mon,  6 Jul 2015 13:40:04 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 3/4] mm: Defer flush of writable TLB entries
Date: Mon,  6 Jul 2015 14:39:55 +0100
Message-Id: &lt;1436189996-7220-4-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1436189996-7220-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1436189996-7220-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - July 6, 2015, 1:39 p.m.</div>
<pre class="content">
If a PTE is unmapped and it&#39;s dirty then it was writable recently. Due
to deferred TLB flushing, it&#39;s best to assume a writable TLB cache entry
exists. With that assumption, the TLB must be flushed before any IO can
start or the page is freed to avoid lost writes or data corruption. This
patch defers flushing of potentially writable TLBs as long as possible.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="reviewed-by">Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
---
 include/linux/sched.h |  7 +++++++
 mm/internal.h         |  4 ++++
 mm/rmap.c             | 28 +++++++++++++++++++++-------
 mm/vmscan.c           |  7 ++++++-
 4 files changed, 38 insertions(+), 8 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 1a83fb44ab34..e769d5b4975c 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1351,6 +1351,13 @@</span> <span class="p_context"> struct tlbflush_unmap_batch {</span>
 
 	/* True if any bit in cpumask is set */
 	bool flush_required;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If true then the PTE was dirty when unmapped. The entry must be</span>
<span class="p_add">+	 * flushed before IO is initiated or a stale TLB entry potentially</span>
<span class="p_add">+	 * allows an update without redirtying the page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool writable;</span>
 };
 
 struct task_struct {
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index bd6372ac5f7f..1195dd2d6a2b 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -431,10 +431,14 @@</span> <span class="p_context"> struct tlbflush_unmap_batch;</span>
 
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 void try_to_unmap_flush(void);
<span class="p_add">+void try_to_unmap_flush_dirty(void);</span>
 #else
 static inline void try_to_unmap_flush(void)
 {
 }
<span class="p_add">+static inline void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 
 #endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index d54f47666af5..85a8aea2d593 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -625,16 +625,34 @@</span> <span class="p_context"> void try_to_unmap_flush(void)</span>
 	}
 	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);
 	tlb_ubc-&gt;flush_required = false;
<span class="p_add">+	tlb_ubc-&gt;writable = false;</span>
 	put_cpu();
 }
 
<span class="p_add">+/* Flush iff there are potentially writable TLB entries that can race with IO */</span>
<span class="p_add">+void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = &amp;current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_ubc-&gt;writable)</span>
<span class="p_add">+		try_to_unmap_flush();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 	struct tlbflush_unmap_batch *tlb_ubc = &amp;current-&gt;tlb_ubc;
 
 	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));
 	tlb_ubc-&gt;flush_required = true;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The</span>
<span class="p_add">+	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()</span>
<span class="p_add">+	 * before the page is queued for IO.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (writable)</span>
<span class="p_add">+		tlb_ubc-&gt;writable = true;</span>
 }
 
 /*
<span class="p_chunk">@@ -657,7 +675,7 @@</span> <span class="p_context"> static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
 }
 #else
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 }
 
<span class="p_chunk">@@ -1314,11 +1332,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		 */
 		pteval = ptep_get_and_clear(mm, address, pte);
 
<span class="p_del">-		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_del">-		if (pte_dirty(pteval))</span>
<span class="p_del">-			flush_tlb_page(vma, address);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+		set_tlb_ubc_flush_pending(mm, page, pte_dirty(pteval));</span>
 	} else {
 		pteval = ptep_clear_flush(vma, address, pte);
 	}
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index e4f1df1052a2..b5c5dc0997a1 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1102,7 +1102,12 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			if (!sc-&gt;may_writepage)
 				goto keep_locked;
 
<span class="p_del">-			/* Page is dirty, try to write it out here */</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Page is dirty. Flush the TLB if a writable entry</span>
<span class="p_add">+			 * potentially exists to avoid CPU writes after IO</span>
<span class="p_add">+			 * starts and then write it out here.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			try_to_unmap_flush_dirty();</span>
 			switch (pageout(page, mapping, sc)) {
 			case PAGE_KEEP:
 				goto keep_locked;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



