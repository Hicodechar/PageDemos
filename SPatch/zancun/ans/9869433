
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/2] iommu/omap: Use DMA-API for performing cache flushes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/2] iommu/omap: Use DMA-API for performing cache flushes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=175619">Josue Albarran</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 28, 2017, 8:49 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1501274954-20973-3-git-send-email-j-albarran@ti.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9869433/mbox/"
   >mbox</a>
|
   <a href="/patch/9869433/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9869433/">/patch/9869433/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	4AD456038F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 20:49:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 32E72288E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 20:49:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 266E528901; Fri, 28 Jul 2017 20:49:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 578CE288E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 28 Jul 2017 20:49:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752896AbdG1Utr (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 28 Jul 2017 16:49:47 -0400
Received: from lelnx193.ext.ti.com ([198.47.27.77]:41363 &quot;EHLO
	lelnx193.ext.ti.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752612AbdG1Uto (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 28 Jul 2017 16:49:44 -0400
Received: from dflxv15.itg.ti.com ([128.247.5.124])
	by lelnx193.ext.ti.com (8.15.1/8.15.1) with ESMTP id v6SKnA85008238; 
	Fri, 28 Jul 2017 15:49:10 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=ti.com;
	s=ti-com-17Q1; t=1501274950;
	bh=lYuKmcnTRLRTKZfBMkps4JtFz4ahj2joSBM4M9t9U28=;
	h=From:To:CC:Subject:Date:In-Reply-To:References;
	b=ir6QTbPwWW2MzvbW8a173utnIEe1gnTgzZlig85ZU9IoZavkUm5R0wdXZcEZ+e9JQ
	ESWXzKCn19/A+CCfSOku9hn8ZVSm/KMVvE2uw8ngURPTIw4GaFplt2JZFcvDQo3K8K
	04DVWd+n7yfO+3IgKncZ8pYVNU3k1vhoEYwT8bcY=
Received: from DLEE70.ent.ti.com (dlee70.ent.ti.com [157.170.170.113])
	by dflxv15.itg.ti.com (8.14.3/8.13.8) with ESMTP id v6SKnAtp009231;
	Fri, 28 Jul 2017 15:49:10 -0500
Received: from dflp33.itg.ti.com (10.64.6.16) by DLEE70.ent.ti.com
	(157.170.170.113) with Microsoft SMTP Server id 14.3.294.0;
	Fri, 28 Jul 2017 15:49:10 -0500
Received: from legion.dal.design.ti.com (legion.dal.design.ti.com
	[128.247.22.53]) by dflp33.itg.ti.com (8.14.3/8.13.8) with ESMTP id
	v6SKnA4k029926;        Fri, 28 Jul 2017 15:49:10 -0500
Received: from localhost (ula0226779.dhcp.ti.com [128.247.58.135])      by
	legion.dal.design.ti.com (8.11.7p1+Sun/8.11.7) with ESMTP id
	v6SKnA302767; Fri, 28 Jul 2017 15:49:10 -0500 (CDT)
From: Josue Albarran &lt;j-albarran@ti.com&gt;
To: Joerg Roedel &lt;joro@8bytes.org&gt;
CC: &lt;iommu@lists.linux-foundation.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	Suman Anna &lt;s-anna@ti.com&gt;,
	Laurent Pinchart &lt;laurent.pinchart@ideasonboard.com&gt;,
	&lt;linux-omap@vger.kernel.org&gt;, &lt;linux-arm-kernel@lists.infradead.org&gt;
Subject: [PATCH 2/2] iommu/omap: Use DMA-API for performing cache flushes
Date: Fri, 28 Jul 2017 15:49:14 -0500
Message-ID: &lt;1501274954-20973-3-git-send-email-j-albarran@ti.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1501274954-20973-1-git-send-email-j-albarran@ti.com&gt;
References: &lt;1501274954-20973-1-git-send-email-j-albarran@ti.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175619">Josue Albarran</a> - July 28, 2017, 8:49 p.m.</div>
<pre class="content">
The OMAP IOMMU driver was using ARM assembly code directly for
flushing the MMU page table entries from the caches. This caused
MMU faults on OMAP4 (Cortex-A9 based SoCs) as L2 caches were not
handled due to the presence of a PL310 L2 Cache Controller. These
faults were however not seen on OMAP5/DRA7 SoCs (Cortex-A15 based
SoCs).

The OMAP IOMMU driver is adapted to use the DMA Streaming API
instead now to flush the page table/directory table entries from
the CPU caches. This ensures that the devices always see the
updated page table entries. The outer caches are now addressed
automatically with the usage of the DMA API.
<span class="signed-off-by">
Signed-off-by: Josue Albarran &lt;j-albarran@ti.com&gt;</span>
---
 drivers/iommu/omap-iommu.c | 123 +++++++++++++++++++++++++++++----------------
 drivers/iommu/omap-iommu.h |   1 +
 2 files changed, 80 insertions(+), 44 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6924">Suman Anna</a> - Aug. 1, 2017, 5:38 p.m.</div>
<pre class="content">
On 07/28/2017 03:49 PM, Josue Albarran wrote:
<span class="quote">&gt; The OMAP IOMMU driver was using ARM assembly code directly for</span>
<span class="quote">&gt; flushing the MMU page table entries from the caches. This caused</span>
<span class="quote">&gt; MMU faults on OMAP4 (Cortex-A9 based SoCs) as L2 caches were not</span>
<span class="quote">&gt; handled due to the presence of a PL310 L2 Cache Controller. These</span>
<span class="quote">&gt; faults were however not seen on OMAP5/DRA7 SoCs (Cortex-A15 based</span>
<span class="quote">&gt; SoCs).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The OMAP IOMMU driver is adapted to use the DMA Streaming API</span>
<span class="quote">&gt; instead now to flush the page table/directory table entries from</span>
<span class="quote">&gt; the CPU caches. This ensures that the devices always see the</span>
<span class="quote">&gt; updated page table entries. The outer caches are now addressed</span>
<span class="quote">&gt; automatically with the usage of the DMA API.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Josue Albarran &lt;j-albarran@ti.com&gt;</span>

Thanks for fixing this,
<span class="acked-by">Acked-by: Suman Anna &lt;s-anna@ti.com&gt;</span>

[snip]
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/omap-iommu.c b/drivers/iommu/omap-iommu.c</span>
<span class="p_header">index 10c9de8de45d..bd67e1b2c64e 100644</span>
<span class="p_header">--- a/drivers/iommu/omap-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/omap-iommu.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
  * published by the Free Software Foundation.
  */
 
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
 #include &lt;linux/err.h&gt;
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/interrupt.h&gt;
<span class="p_chunk">@@ -29,8 +30,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/regmap.h&gt;
 #include &lt;linux/mfd/syscon.h&gt;
 
<span class="p_del">-#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_del">-</span>
 #include &lt;linux/platform_data/iommu-omap.h&gt;
 
 #include &quot;omap-iopgtable.h&quot;
<span class="p_chunk">@@ -454,36 +453,35 @@</span> <span class="p_context"> static void flush_iotlb_all(struct omap_iommu *obj)</span>
 /*
  *	H/W pagetable operations
  */
<span class="p_del">-static void flush_iopgd_range(u32 *first, u32 *last)</span>
<span class="p_add">+static void flush_iopte_range(struct device *dev, dma_addr_t dma,</span>
<span class="p_add">+			      unsigned long offset, int num_entries)</span>
 {
<span class="p_del">-	/* FIXME: L2 cache should be taken care of if it exists */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		asm(&quot;mcr	p15, 0, %0, c7, c10, 1 @ flush_pgd&quot;</span>
<span class="p_del">-		    : : &quot;r&quot; (first));</span>
<span class="p_del">-		first += L1_CACHE_BYTES / sizeof(*first);</span>
<span class="p_del">-	} while (first &lt;= last);</span>
<span class="p_del">-}</span>
<span class="p_add">+	size_t size = num_entries * sizeof(u32);</span>
 
<span class="p_del">-static void flush_iopte_range(u32 *first, u32 *last)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* FIXME: L2 cache should be taken care of if it exists */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		asm(&quot;mcr	p15, 0, %0, c7, c10, 1 @ flush_pte&quot;</span>
<span class="p_del">-		    : : &quot;r&quot; (first));</span>
<span class="p_del">-		first += L1_CACHE_BYTES / sizeof(*first);</span>
<span class="p_del">-	} while (first &lt;= last);</span>
<span class="p_add">+	dma_sync_single_range_for_device(dev, dma, offset, size, DMA_TO_DEVICE);</span>
 }
 
<span class="p_del">-static void iopte_free(u32 *iopte)</span>
<span class="p_add">+static void iopte_free(struct omap_iommu *obj, u32 *iopte, bool dma_valid)</span>
 {
<span class="p_add">+	dma_addr_t pt_dma;</span>
<span class="p_add">+</span>
 	/* Note: freed iopte&#39;s must be clean ready for re-use */
<span class="p_del">-	if (iopte)</span>
<span class="p_add">+	if (iopte) {</span>
<span class="p_add">+		if (dma_valid) {</span>
<span class="p_add">+			pt_dma = virt_to_phys(iopte);</span>
<span class="p_add">+			dma_unmap_single(obj-&gt;dev, pt_dma, IOPTE_TABLE_SIZE,</span>
<span class="p_add">+					 DMA_TO_DEVICE);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		kmem_cache_free(iopte_cachep, iopte);
<span class="p_add">+	}</span>
 }
 
<span class="p_del">-static u32 *iopte_alloc(struct omap_iommu *obj, u32 *iopgd, u32 da)</span>
<span class="p_add">+static u32 *iopte_alloc(struct omap_iommu *obj, u32 *iopgd,</span>
<span class="p_add">+			dma_addr_t *pt_dma, u32 da)</span>
 {
 	u32 *iopte;
<span class="p_add">+	unsigned long offset = iopgd_index(da) * sizeof(da);</span>
 
 	/* a table has already existed */
 	if (*iopgd)
<span class="p_chunk">@@ -500,18 +498,38 @@</span> <span class="p_context"> static u32 *iopte_alloc(struct omap_iommu *obj, u32 *iopgd, u32 da)</span>
 		if (!iopte)
 			return ERR_PTR(-ENOMEM);
 
<span class="p_add">+		*pt_dma = dma_map_single(obj-&gt;dev, iopte, IOPTE_TABLE_SIZE,</span>
<span class="p_add">+					 DMA_TO_DEVICE);</span>
<span class="p_add">+		if (dma_mapping_error(obj-&gt;dev, *pt_dma)) {</span>
<span class="p_add">+			dev_err(obj-&gt;dev, &quot;DMA map error for L2 table\n&quot;);</span>
<span class="p_add">+			iopte_free(obj, iopte, false);</span>
<span class="p_add">+			return ERR_PTR(-ENOMEM);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * we rely on dma address and the physical address to be</span>
<span class="p_add">+		 * the same for mapping the L2 table</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (WARN_ON(*pt_dma != virt_to_phys(iopte))) {</span>
<span class="p_add">+			dev_err(obj-&gt;dev, &quot;DMA translation error for L2 table\n&quot;);</span>
<span class="p_add">+			dma_unmap_single(obj-&gt;dev, *pt_dma, IOPTE_TABLE_SIZE,</span>
<span class="p_add">+					 DMA_TO_DEVICE);</span>
<span class="p_add">+			iopte_free(obj, iopte, false);</span>
<span class="p_add">+			return ERR_PTR(-ENOMEM);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		*iopgd = virt_to_phys(iopte) | IOPGD_TABLE;
<span class="p_del">-		flush_iopgd_range(iopgd, iopgd);</span>
 
<span class="p_add">+		flush_iopte_range(obj-&gt;dev, obj-&gt;pd_dma, offset, 1);</span>
 		dev_vdbg(obj-&gt;dev, &quot;%s: a new pte:%p\n&quot;, __func__, iopte);
 	} else {
 		/* We raced, free the reduniovant table */
<span class="p_del">-		iopte_free(iopte);</span>
<span class="p_add">+		iopte_free(obj, iopte, false);</span>
 	}
 
 pte_ready:
 	iopte = iopte_offset(iopgd, da);
<span class="p_del">-</span>
<span class="p_add">+	*pt_dma = virt_to_phys(iopte);</span>
 	dev_vdbg(obj-&gt;dev,
 		 &quot;%s: da:%08x pgd:%p *pgd:%08x pte:%p *pte:%08x\n&quot;,
 		 __func__, da, iopgd, *iopgd, iopte, *iopte);
<span class="p_chunk">@@ -522,6 +540,7 @@</span> <span class="p_context"> static u32 *iopte_alloc(struct omap_iommu *obj, u32 *iopgd, u32 da)</span>
 static int iopgd_alloc_section(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)
 {
 	u32 *iopgd = iopgd_offset(obj, da);
<span class="p_add">+	unsigned long offset = iopgd_index(da) * sizeof(da);</span>
 
 	if ((da | pa) &amp; ~IOSECTION_MASK) {
 		dev_err(obj-&gt;dev, &quot;%s: %08x:%08x should aligned on %08lx\n&quot;,
<span class="p_chunk">@@ -530,13 +549,14 @@</span> <span class="p_context"> static int iopgd_alloc_section(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)</span>
 	}
 
 	*iopgd = (pa &amp; IOSECTION_MASK) | prot | IOPGD_SECTION;
<span class="p_del">-	flush_iopgd_range(iopgd, iopgd);</span>
<span class="p_add">+	flush_iopte_range(obj-&gt;dev, obj-&gt;pd_dma, offset, 1);</span>
 	return 0;
 }
 
 static int iopgd_alloc_super(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)
 {
 	u32 *iopgd = iopgd_offset(obj, da);
<span class="p_add">+	unsigned long offset = iopgd_index(da) * sizeof(da);</span>
 	int i;
 
 	if ((da | pa) &amp; ~IOSUPER_MASK) {
<span class="p_chunk">@@ -547,20 +567,22 @@</span> <span class="p_context"> static int iopgd_alloc_super(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)</span>
 
 	for (i = 0; i &lt; 16; i++)
 		*(iopgd + i) = (pa &amp; IOSUPER_MASK) | prot | IOPGD_SUPER;
<span class="p_del">-	flush_iopgd_range(iopgd, iopgd + 15);</span>
<span class="p_add">+	flush_iopte_range(obj-&gt;dev, obj-&gt;pd_dma, offset, 16);</span>
 	return 0;
 }
 
 static int iopte_alloc_page(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)
 {
 	u32 *iopgd = iopgd_offset(obj, da);
<span class="p_del">-	u32 *iopte = iopte_alloc(obj, iopgd, da);</span>
<span class="p_add">+	dma_addr_t pt_dma;</span>
<span class="p_add">+	u32 *iopte = iopte_alloc(obj, iopgd, &amp;pt_dma, da);</span>
<span class="p_add">+	unsigned long offset = iopte_index(da) * sizeof(da);</span>
 
 	if (IS_ERR(iopte))
 		return PTR_ERR(iopte);
 
 	*iopte = (pa &amp; IOPAGE_MASK) | prot | IOPTE_SMALL;
<span class="p_del">-	flush_iopte_range(iopte, iopte);</span>
<span class="p_add">+	flush_iopte_range(obj-&gt;dev, pt_dma, offset, 1);</span>
 
 	dev_vdbg(obj-&gt;dev, &quot;%s: da:%08x pa:%08x pte:%p *pte:%08x\n&quot;,
 		 __func__, da, pa, iopte, *iopte);
<span class="p_chunk">@@ -571,7 +593,9 @@</span> <span class="p_context"> static int iopte_alloc_page(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)</span>
 static int iopte_alloc_large(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)
 {
 	u32 *iopgd = iopgd_offset(obj, da);
<span class="p_del">-	u32 *iopte = iopte_alloc(obj, iopgd, da);</span>
<span class="p_add">+	dma_addr_t pt_dma;</span>
<span class="p_add">+	u32 *iopte = iopte_alloc(obj, iopgd, &amp;pt_dma, da);</span>
<span class="p_add">+	unsigned long offset = iopte_index(da) * sizeof(da);</span>
 	int i;
 
 	if ((da | pa) &amp; ~IOLARGE_MASK) {
<span class="p_chunk">@@ -585,7 +609,7 @@</span> <span class="p_context"> static int iopte_alloc_large(struct omap_iommu *obj, u32 da, u32 pa, u32 prot)</span>
 
 	for (i = 0; i &lt; 16; i++)
 		*(iopte + i) = (pa &amp; IOLARGE_MASK) | prot | IOPTE_LARGE;
<span class="p_del">-	flush_iopte_range(iopte, iopte + 15);</span>
<span class="p_add">+	flush_iopte_range(obj-&gt;dev, pt_dma, offset, 16);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -674,6 +698,9 @@</span> <span class="p_context"> static size_t iopgtable_clear_entry_core(struct omap_iommu *obj, u32 da)</span>
 	size_t bytes;
 	u32 *iopgd = iopgd_offset(obj, da);
 	int nent = 1;
<span class="p_add">+	dma_addr_t pt_dma;</span>
<span class="p_add">+	unsigned long pd_offset = iopgd_index(da) * sizeof(da);</span>
<span class="p_add">+	unsigned long pt_offset = iopte_index(da) * sizeof(da);</span>
 
 	if (!*iopgd)
 		return 0;
<span class="p_chunk">@@ -690,7 +717,8 @@</span> <span class="p_context"> static size_t iopgtable_clear_entry_core(struct omap_iommu *obj, u32 da)</span>
 		}
 		bytes *= nent;
 		memset(iopte, 0, nent * sizeof(*iopte));
<span class="p_del">-		flush_iopte_range(iopte, iopte + (nent - 1) * sizeof(*iopte));</span>
<span class="p_add">+		pt_dma = virt_to_phys(iopte);</span>
<span class="p_add">+		flush_iopte_range(obj-&gt;dev, pt_dma, pt_offset, nent);</span>
 
 		/*
 		 * do table walk to check if this table is necessary or not
<span class="p_chunk">@@ -700,7 +728,7 @@</span> <span class="p_context"> static size_t iopgtable_clear_entry_core(struct omap_iommu *obj, u32 da)</span>
 			if (iopte[i])
 				goto out;
 
<span class="p_del">-		iopte_free(iopte);</span>
<span class="p_add">+		iopte_free(obj, iopte, true);</span>
 		nent = 1; /* for the next L1 entry */
 	} else {
 		bytes = IOPGD_SIZE;
<span class="p_chunk">@@ -712,7 +740,7 @@</span> <span class="p_context"> static size_t iopgtable_clear_entry_core(struct omap_iommu *obj, u32 da)</span>
 		bytes *= nent;
 	}
 	memset(iopgd, 0, nent * sizeof(*iopgd));
<span class="p_del">-	flush_iopgd_range(iopgd, iopgd + (nent - 1) * sizeof(*iopgd));</span>
<span class="p_add">+	flush_iopte_range(obj-&gt;dev, obj-&gt;pd_dma, pd_offset, nent);</span>
 out:
 	return bytes;
 }
<span class="p_chunk">@@ -738,6 +766,7 @@</span> <span class="p_context"> static size_t iopgtable_clear_entry(struct omap_iommu *obj, u32 da)</span>
 
 static void iopgtable_clear_entry_all(struct omap_iommu *obj)
 {
<span class="p_add">+	unsigned long offset;</span>
 	int i;
 
 	spin_lock(&amp;obj-&gt;page_table_lock);
<span class="p_chunk">@@ -748,15 +777,16 @@</span> <span class="p_context"> static void iopgtable_clear_entry_all(struct omap_iommu *obj)</span>
 
 		da = i &lt;&lt; IOPGD_SHIFT;
 		iopgd = iopgd_offset(obj, da);
<span class="p_add">+		offset = iopgd_index(da) * sizeof(da);</span>
 
 		if (!*iopgd)
 			continue;
 
 		if (iopgd_is_table(*iopgd))
<span class="p_del">-			iopte_free(iopte_offset(iopgd, 0));</span>
<span class="p_add">+			iopte_free(obj, iopte_offset(iopgd, 0), true);</span>
 
 		*iopgd = 0;
<span class="p_del">-		flush_iopgd_range(iopgd, iopgd);</span>
<span class="p_add">+		flush_iopte_range(obj-&gt;dev, obj-&gt;pd_dma, offset, 1);</span>
 	}
 
 	flush_iotlb_all(obj);
<span class="p_chunk">@@ -815,10 +845,18 @@</span> <span class="p_context"> static int omap_iommu_attach(struct omap_iommu *obj, u32 *iopgd)</span>
 
 	spin_lock(&amp;obj-&gt;iommu_lock);
 
<span class="p_add">+	obj-&gt;pd_dma = dma_map_single(obj-&gt;dev, iopgd, IOPGD_TABLE_SIZE,</span>
<span class="p_add">+				     DMA_TO_DEVICE);</span>
<span class="p_add">+	if (dma_mapping_error(obj-&gt;dev, obj-&gt;pd_dma)) {</span>
<span class="p_add">+		dev_err(obj-&gt;dev, &quot;DMA map error for L1 table\n&quot;);</span>
<span class="p_add">+		err = -ENOMEM;</span>
<span class="p_add">+		goto out_err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	obj-&gt;iopgd = iopgd;
 	err = iommu_enable(obj);
 	if (err)
<span class="p_del">-		goto err_enable;</span>
<span class="p_add">+		goto out_err;</span>
 	flush_iotlb_all(obj);
 
 	spin_unlock(&amp;obj-&gt;iommu_lock);
<span class="p_chunk">@@ -827,7 +865,7 @@</span> <span class="p_context"> static int omap_iommu_attach(struct omap_iommu *obj, u32 *iopgd)</span>
 
 	return 0;
 
<span class="p_del">-err_enable:</span>
<span class="p_add">+out_err:</span>
 	spin_unlock(&amp;obj-&gt;iommu_lock);
 
 	return err;
<span class="p_chunk">@@ -844,7 +882,10 @@</span> <span class="p_context"> static void omap_iommu_detach(struct omap_iommu *obj)</span>
 
 	spin_lock(&amp;obj-&gt;iommu_lock);
 
<span class="p_add">+	dma_unmap_single(obj-&gt;dev, obj-&gt;pd_dma, IOPGD_TABLE_SIZE,</span>
<span class="p_add">+			 DMA_TO_DEVICE);</span>
 	iommu_disable(obj);
<span class="p_add">+	obj-&gt;pd_dma = 0;</span>
 	obj-&gt;iopgd = NULL;
 
 	spin_unlock(&amp;obj-&gt;iommu_lock);
<span class="p_chunk">@@ -1008,11 +1049,6 @@</span> <span class="p_context"> static struct platform_driver omap_iommu_driver = {</span>
 	},
 };
 
<span class="p_del">-static void iopte_cachep_ctor(void *iopte)</span>
<span class="p_del">-{</span>
<span class="p_del">-	clean_dcache_area(iopte, IOPTE_TABLE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static u32 iotlb_init_entry(struct iotlb_entry *e, u32 da, u32 pa, int pgsz)
 {
 	memset(e, 0, sizeof(*e));
<span class="p_chunk">@@ -1159,7 +1195,6 @@</span> <span class="p_context"> static struct iommu_domain *omap_iommu_domain_alloc(unsigned type)</span>
 	if (WARN_ON(!IS_ALIGNED((long)omap_domain-&gt;pgtable, IOPGD_TABLE_SIZE)))
 		goto fail_align;
 
<span class="p_del">-	clean_dcache_area(omap_domain-&gt;pgtable, IOPGD_TABLE_SIZE);</span>
 	spin_lock_init(&amp;omap_domain-&gt;lock);
 
 	omap_domain-&gt;domain.geometry.aperture_start = 0;
<span class="p_chunk">@@ -1347,7 +1382,7 @@</span> <span class="p_context"> static int __init omap_iommu_init(void)</span>
 	of_node_put(np);
 
 	p = kmem_cache_create(&quot;iopte_cache&quot;, IOPTE_TABLE_SIZE, align, flags,
<span class="p_del">-			      iopte_cachep_ctor);</span>
<span class="p_add">+			      NULL);</span>
 	if (!p)
 		return -ENOMEM;
 	iopte_cachep = p;
<span class="p_header">diff --git a/drivers/iommu/omap-iommu.h b/drivers/iommu/omap-iommu.h</span>
<span class="p_header">index 6e70515e6038..a675af29a6ec 100644</span>
<span class="p_header">--- a/drivers/iommu/omap-iommu.h</span>
<span class="p_header">+++ b/drivers/iommu/omap-iommu.h</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"> struct omap_iommu {</span>
 	 */
 	u32		*iopgd;
 	spinlock_t	page_table_lock; /* protect iopgd */
<span class="p_add">+	dma_addr_t	pd_dma;</span>
 
 	int		nr_tlb_entries;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



