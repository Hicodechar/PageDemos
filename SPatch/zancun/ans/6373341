
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V3] mm/thp: Split out pmd collpase flush into a separate functions - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V3] mm/thp: Split out pmd collpase flush into a separate functions</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 11, 2015, 6:39 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1431326370-24247-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6373341/mbox/"
   >mbox</a>
|
   <a href="/patch/6373341/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6373341/">/patch/6373341/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 8C0639F399
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 May 2015 06:39:53 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 715BD20384
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 May 2015 06:39:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 2E214203C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 May 2015 06:39:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752678AbbEKGjo (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 11 May 2015 02:39:44 -0400
Received: from e28smtp04.in.ibm.com ([122.248.162.4]:46231 &quot;EHLO
	e28smtp04.in.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752455AbbEKGjn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 11 May 2015 02:39:43 -0400
Received: from /spool/local
	by e28smtp04.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Mon, 11 May 2015 12:09:41 +0530
Received: from d28dlp03.in.ibm.com (9.184.220.128)
	by e28smtp04.in.ibm.com (192.168.1.134) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Mon, 11 May 2015 12:09:39 +0530
Received: from d28relay02.in.ibm.com (d28relay02.in.ibm.com [9.184.220.59])
	by d28dlp03.in.ibm.com (Postfix) with ESMTP id C15D81258063
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 11 May 2015 12:11:48 +0530 (IST)
Received: from d28av01.in.ibm.com (d28av01.in.ibm.com [9.184.220.63])
	by d28relay02.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	t4B6dX2p18284674
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 11 May 2015 12:09:34 +0530
Received: from d28av01.in.ibm.com (localhost [127.0.0.1])
	by d28av01.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	t4B6dVHa030813
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 11 May 2015 12:09:33 +0530
Received: from skywalker.in.ibm.com (skywalker.in.ibm.com [9.124.35.29])
	by d28av01.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	t4B6dVd5030802; Mon, 11 May 2015 12:09:31 +0530
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: benh@kernel.crashing.org, paulus@samba.org, mpe@ellerman.id.au,
	kirill.shutemov@linux.intel.com, aarcange@redhat.com,
	akpm@linux-foundation.org
Cc: linuxppc-dev@lists.ozlabs.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH V3] mm/thp: Split out pmd collpase flush into a separate
	functions
Date: Mon, 11 May 2015 12:09:30 +0530
Message-Id: &lt;1431326370-24247-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.1.4
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15051106-0013-0000-0000-0000051C0A71
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 11, 2015, 6:39 a.m.</div>
<pre class="content">
Architectures like ppc64 [1] need to do special things while clearing
pmd before a collapse. For them this operation is largely different
from a normal hugepage pte clear. Hence add a separate function
to clear pmd before collapse. After this patch pmdp_* functions
operate only on hugepage pte, and not on regular pmd_t values
pointing to page table.

[1] ppc64 needs to invalidate all the normal page pte mappings we
already have inserted in the hardware hash page table. But before
doing that we need to make sure there are no parallel hash page
table insert going on. So we need to do a kick_all_cpus_sync()
before flushing the older hash table entries. By moving this to
a separate function we capture these details and mention how it
is different from a hugepage pte clear.

This patch is a cleanup and only does code movement for clarity.
There should not be any change in functionality.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
Changes from V2:
* Update commit message
* Address review feedback

 arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++
 arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------
 include/asm-generic/pgtable.h            | 19 ++++++++
 mm/huge_memory.c                         |  2 +-
 4 files changed, 65 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - May 11, 2015, 8:05 a.m.</div>
<pre class="content">
On Mon, May 11, 2015 at 12:09:30PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Architectures like ppc64 [1] need to do special things while clearing</span>
<span class="quote">&gt; pmd before a collapse. For them this operation is largely different</span>
<span class="quote">&gt; from a normal hugepage pte clear. Hence add a separate function</span>
<span class="quote">&gt; to clear pmd before collapse. After this patch pmdp_* functions</span>
<span class="quote">&gt; operate only on hugepage pte, and not on regular pmd_t values</span>
<span class="quote">&gt; pointing to page table.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] ppc64 needs to invalidate all the normal page pte mappings we</span>
<span class="quote">&gt; already have inserted in the hardware hash page table. But before</span>
<span class="quote">&gt; doing that we need to make sure there are no parallel hash page</span>
<span class="quote">&gt; table insert going on. So we need to do a kick_all_cpus_sync()</span>
<span class="quote">&gt; before flushing the older hash table entries. By moving this to</span>
<span class="quote">&gt; a separate function we capture these details and mention how it</span>
<span class="quote">&gt; is different from a hugepage pte clear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch is a cleanup and only does code movement for clarity.</span>
<span class="quote">&gt; There should not be any change in functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Changes from V2:</span>
<span class="quote">&gt; * Update commit message</span>
<span class="quote">&gt; * Address review feedback</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++</span>
<span class="quote">&gt;  arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------</span>
<span class="quote">&gt;  include/asm-generic/pgtable.h            | 19 ++++++++</span>
<span class="quote">&gt;  mm/huge_memory.c                         |  2 +-</span>
<span class="quote">&gt;  4 files changed, 65 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; index 43e6ad424c7f..f5b98b2a45f0 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define pmd_collapse_flush pmd_collapse_flush</span>
<span class="quote">&gt; +extern pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; index 59daa5eeec25..b651179ac4da 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt; @@ -560,41 +560,47 @@ pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; -		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt; -	} else {</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * khugepaged calls this for normal pmd</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		pmd = *pmdp;</span>
<span class="quote">&gt; -		pmd_clear(pmdp);</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt; -		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt; -		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt; -		 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt; -		 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt; -		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt; -		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt; -		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt; -		 * That means we could be modifying the page content as we</span>
<span class="quote">&gt; -		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt; -		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt; -		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt; -		 * function there.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		kick_all_cpus_sync();</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt; -		 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt; -		 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt; -		 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt; -		 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt; -		 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt; -		 * the old content.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt; +	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt; +	return pmd;</span>

Looks like with this cange you don&#39;t need Power-specific
pmdp_clear_flush() -- generic one would work for you.

It seems you want change semantics of pmdp_clear_flush(): it should be
called only for huge pmds. I&#39;m fine with that. But we need at least
document that. And probably rename the helper to reflect semantics:
pmdp_huge_clear_flush()?

And we need  VM_BUG_ON(!pmd_trans_huge(*pmdp)) in generic helper too.

What about pmdp_clear_flush_young()? Should we change it the same way?
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +pmd_t pmd_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; +			 pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pmd_t pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; +	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = *pmdp;</span>
<span class="quote">&gt; +	pmd_clear(pmdp);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt; +	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt; +	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt; +	 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt; +	 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt; +	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt; +	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt; +	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt; +	 * That means we could be modifying the page content as we</span>
<span class="quote">&gt; +	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt; +	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt; +	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt; +	 * function there.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	kick_all_cpus_sync();</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt; +	 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt; +	 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt; +	 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt; +	 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt; +	 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt; +	 * the old content.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; index 39f1d6a2b04d..edc90a2261f7 100644</span>
<span class="quote">&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt; @@ -189,6 +189,25 @@ extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifndef pmd_collapse_flush</span>
<span class="quote">&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return __pmd(0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 078832cf3636..009a5de619fd 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -2499,7 +2499,7 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	 * huge and small TLB entries for the same virtual address</span>
<span class="quote">&gt;  	 * to avoid the risk of CPU bugs in that area.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="quote">&gt; +	_pmd = pmd_collapse_flush(vma, address, pmd);</span>

Let&#39;s name it pmdp_collapse_flush().
We are not hugely consistent on pmd vs. pmdp, but we have
pmdp_splittitng_flush() counterpart.
<span class="quote">
&gt;  	spin_unlock(pmd_ptl);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
<span class="quote">&gt; Please read the FAQ at  http://www.tux.org/lkml/</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 11, 2015, 8:56 a.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:
<span class="quote">
&gt; On Mon, May 11, 2015 at 12:09:30PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; Architectures like ppc64 [1] need to do special things while clearing</span>
<span class="quote">&gt;&gt; pmd before a collapse. For them this operation is largely different</span>
<span class="quote">&gt;&gt; from a normal hugepage pte clear. Hence add a separate function</span>
<span class="quote">&gt;&gt; to clear pmd before collapse. After this patch pmdp_* functions</span>
<span class="quote">&gt;&gt; operate only on hugepage pte, and not on regular pmd_t values</span>
<span class="quote">&gt;&gt; pointing to page table.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; [1] ppc64 needs to invalidate all the normal page pte mappings we</span>
<span class="quote">&gt;&gt; already have inserted in the hardware hash page table. But before</span>
<span class="quote">&gt;&gt; doing that we need to make sure there are no parallel hash page</span>
<span class="quote">&gt;&gt; table insert going on. So we need to do a kick_all_cpus_sync()</span>
<span class="quote">&gt;&gt; before flushing the older hash table entries. By moving this to</span>
<span class="quote">&gt;&gt; a separate function we capture these details and mention how it</span>
<span class="quote">&gt;&gt; is different from a hugepage pte clear.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; This patch is a cleanup and only does code movement for clarity.</span>
<span class="quote">&gt;&gt; There should not be any change in functionality.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; Changes from V2:</span>
<span class="quote">&gt;&gt; * Update commit message</span>
<span class="quote">&gt;&gt; * Address review feedback</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;  arch/powerpc/include/asm/pgtable-ppc64.h |  4 ++</span>
<span class="quote">&gt;&gt;  arch/powerpc/mm/pgtable_64.c             | 76 +++++++++++++++++---------------</span>
<span class="quote">&gt;&gt;  include/asm-generic/pgtable.h            | 19 ++++++++</span>
<span class="quote">&gt;&gt;  mm/huge_memory.c                         |  2 +-</span>
<span class="quote">&gt;&gt;  4 files changed, 65 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; index 43e6ad424c7f..f5b98b2a45f0 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="quote">&gt;&gt; @@ -576,6 +576,10 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;&gt;  extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#define pmd_collapse_flush pmd_collapse_flush</span>
<span class="quote">&gt;&gt; +extern pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt;&gt; diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; index 59daa5eeec25..b651179ac4da 100644</span>
<span class="quote">&gt;&gt; --- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; +++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="quote">&gt;&gt; @@ -560,41 +560,47 @@ pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;&gt; -	if (pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt;&gt; -		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;&gt; -	} else {</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * khugepaged calls this for normal pmd</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		pmd = *pmdp;</span>
<span class="quote">&gt;&gt; -		pmd_clear(pmdp);</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt;&gt; -		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt;&gt; -		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt;&gt; -		 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt;&gt; -		 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt;&gt; -		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt;&gt; -		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt;&gt; -		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt;&gt; -		 * That means we could be modifying the page content as we</span>
<span class="quote">&gt;&gt; -		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt;&gt; -		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt;&gt; -		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt;&gt; -		 * function there.</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		kick_all_cpus_sync();</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt;&gt; -		 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt;&gt; -		 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt;&gt; -		 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt;&gt; -		 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt;&gt; -		 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt;&gt; -		 * the old content.</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt;&gt; +	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;&gt; +	return pmd;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Looks like with this cange you don&#39;t need Power-specific</span>
<span class="quote">&gt; pmdp_clear_flush() -- generic one would work for you.</span>

That is correct. Will update the series and do that as a follow up patch ?
<span class="quote">
&gt;</span>
<span class="quote">&gt; It seems you want change semantics of pmdp_clear_flush(): it should be</span>
<span class="quote">&gt; called only for huge pmds. I&#39;m fine with that. But we need at least</span>
<span class="quote">&gt; document that. And probably rename the helper to reflect semantics:</span>
<span class="quote">&gt; pmdp_huge_clear_flush()?</span>
<span class="quote">&gt;</span>

So you are suggesting to callout _huge_ in the APIs. Will fix. Also will
do a cleanup to update other functions ?
<span class="quote">
&gt; And we need  VM_BUG_ON(!pmd_trans_huge(*pmdp)) in generic helper too.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What about pmdp_clear_flush_young()? Should we change it the same way?</span>
<span class="quote">&gt;</span>

ok
<span class="quote">
&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +pmd_t pmd_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt; +			 pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pmd_t pmd;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pmd = *pmdp;</span>
<span class="quote">&gt;&gt; +	pmd_clear(pmdp);</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="quote">&gt;&gt; +	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="quote">&gt;&gt; +	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="quote">&gt;&gt; +	 * the PTE entries. The assumption here is that any low level</span>
<span class="quote">&gt;&gt; +	 * page fault will see a none pmd and take the slow path that</span>
<span class="quote">&gt;&gt; +	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="quote">&gt;&gt; +	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="quote">&gt;&gt; +	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="quote">&gt;&gt; +	 * That means we could be modifying the page content as we</span>
<span class="quote">&gt;&gt; +	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="quote">&gt;&gt; +	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="quote">&gt;&gt; +	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="quote">&gt;&gt; +	 * function there.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	kick_all_cpus_sync();</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Now invalidate the hpte entries in the range</span>
<span class="quote">&gt;&gt; +	 * covered by pmd. This make sure we take a</span>
<span class="quote">&gt;&gt; +	 * fault and will find the pmd as none, which will</span>
<span class="quote">&gt;&gt; +	 * result in a major fault which takes mmap_sem and</span>
<span class="quote">&gt;&gt; +	 * hence wait for collapse to complete. Without this</span>
<span class="quote">&gt;&gt; +	 * the __collapse_huge_page_copy can result in copying</span>
<span class="quote">&gt;&gt; +	 * the old content.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="quote">&gt;&gt;  	return pmd;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; index 39f1d6a2b04d..edc90a2261f7 100644</span>
<span class="quote">&gt;&gt; --- a/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/include/asm-generic/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -189,6 +189,25 @@ extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long address, pmd_t *pmdp);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#ifndef pmd_collapse_flush</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt;&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				       unsigned long address,</span>
<span class="quote">&gt;&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				       unsigned long address,</span>
<span class="quote">&gt;&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +	return __pmd(0);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT</span>
<span class="quote">&gt;&gt;  extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,</span>
<span class="quote">&gt;&gt;  				       pgtable_t pgtable);</span>
<span class="quote">&gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; index 078832cf3636..009a5de619fd 100644</span>
<span class="quote">&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; @@ -2499,7 +2499,7 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  	 * huge and small TLB entries for the same virtual address</span>
<span class="quote">&gt;&gt;  	 * to avoid the risk of CPU bugs in that area.</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="quote">&gt;&gt; +	_pmd = pmd_collapse_flush(vma, address, pmd);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Let&#39;s name it pmdp_collapse_flush().</span>
<span class="quote">&gt; We are not hugely consistent on pmd vs. pmdp, but we have</span>
<span class="quote">&gt; pmdp_splittitng_flush() counterpart.</span>
<span class="quote">&gt;</span>

ok 
<span class="quote">

&gt;&gt;  	spin_unlock(pmd_ptl);</span>
<span class="quote">&gt;&gt;  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt;  </span>

-aneesh

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - May 11, 2015, 10:40 p.m.</div>
<pre class="content">
On Mon, 11 May 2015 12:09:30 +0530 &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; wrote:
<span class="quote">
&gt; Architectures like ppc64 [1] need to do special things while clearing</span>
<span class="quote">&gt; pmd before a collapse. For them this operation is largely different</span>
<span class="quote">&gt; from a normal hugepage pte clear. Hence add a separate function</span>
<span class="quote">&gt; to clear pmd before collapse. After this patch pmdp_* functions</span>
<span class="quote">&gt; operate only on hugepage pte, and not on regular pmd_t values</span>
<span class="quote">&gt; pointing to page table.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] ppc64 needs to invalidate all the normal page pte mappings we</span>
<span class="quote">&gt; already have inserted in the hardware hash page table. But before</span>
<span class="quote">&gt; doing that we need to make sure there are no parallel hash page</span>
<span class="quote">&gt; table insert going on. So we need to do a kick_all_cpus_sync()</span>
<span class="quote">&gt; before flushing the older hash table entries. By moving this to</span>
<span class="quote">&gt; a separate function we capture these details and mention how it</span>
<span class="quote">&gt; is different from a hugepage pte clear.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch is a cleanup and only does code movement for clarity.</span>
<span class="quote">&gt; There should not be any change in functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +#ifndef pmd_collapse_flush</span>
<span class="quote">&gt; +#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				       unsigned long address,</span>
<span class="quote">&gt; +				       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return __pmd(0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>

You want

#define pmd_collapse_flush pmd_collapse_flush

here, just in case a later header file performs the same test.
<span class="quote">
&gt; +#endif</span>
<span class="quote">&gt; +</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgtable-ppc64.h b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">index 43e6ad424c7f..f5b98b2a45f0 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgtable-ppc64.h</span>
<span class="p_chunk">@@ -576,6 +576,10 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr,</span>
 extern void pmdp_splitting_flush(struct vm_area_struct *vma,
 				 unsigned long address, pmd_t *pmdp);
 
<span class="p_add">+#define pmd_collapse_flush pmd_collapse_flush</span>
<span class="p_add">+extern pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long address, pmd_t *pmdp);</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">index 59daa5eeec25..b651179ac4da 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_chunk">@@ -560,41 +560,47 @@</span> <span class="p_context"> pmd_t pmdp_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 	pmd_t pmd;
 
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	if (pmd_trans_huge(*pmdp)) {</span>
<span class="p_del">-		pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * khugepaged calls this for normal pmd</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = *pmdp;</span>
<span class="p_del">-		pmd_clear(pmdp);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_del">-		 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_del">-		 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_del">-		 * the PTE entries. The assumption here is that any low level</span>
<span class="p_del">-		 * page fault will see a none pmd and take the slow path that</span>
<span class="p_del">-		 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_del">-		 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_del">-		 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_del">-		 * That means we could be modifying the page content as we</span>
<span class="p_del">-		 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_del">-		 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_del">-		 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_del">-		 * function there.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		kick_all_cpus_sync();</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Now invalidate the hpte entries in the range</span>
<span class="p_del">-		 * covered by pmd. This make sure we take a</span>
<span class="p_del">-		 * fault and will find the pmd as none, which will</span>
<span class="p_del">-		 * result in a major fault which takes mmap_sem and</span>
<span class="p_del">-		 * hence wait for collapse to complete. Without this</span>
<span class="p_del">-		 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_del">-		 * the old content.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	VM_BUG_ON(!pmd_trans_huge(*pmdp));</span>
<span class="p_add">+	pmd = pmdp_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="p_add">+	return pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pmd_t pmd_collapse_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+			 pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = *pmdp;</span>
<span class="p_add">+	pmd_clear(pmdp);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Wait for all pending hash_page to finish. This is needed</span>
<span class="p_add">+	 * in case of subpage collapse. When we collapse normal pages</span>
<span class="p_add">+	 * to hugepage, we first clear the pmd, then invalidate all</span>
<span class="p_add">+	 * the PTE entries. The assumption here is that any low level</span>
<span class="p_add">+	 * page fault will see a none pmd and take the slow path that</span>
<span class="p_add">+	 * will wait on mmap_sem. But we could very well be in a</span>
<span class="p_add">+	 * hash_page with local ptep pointer value. Such a hash page</span>
<span class="p_add">+	 * can result in adding new HPTE entries for normal subpages.</span>
<span class="p_add">+	 * That means we could be modifying the page content as we</span>
<span class="p_add">+	 * copy them to a huge page. So wait for parallel hash_page</span>
<span class="p_add">+	 * to finish before invalidating HPTE entries. We can do this</span>
<span class="p_add">+	 * by sending an IPI to all the cpus and executing a dummy</span>
<span class="p_add">+	 * function there.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kick_all_cpus_sync();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now invalidate the hpte entries in the range</span>
<span class="p_add">+	 * covered by pmd. This make sure we take a</span>
<span class="p_add">+	 * fault and will find the pmd as none, which will</span>
<span class="p_add">+	 * result in a major fault which takes mmap_sem and</span>
<span class="p_add">+	 * hence wait for collapse to complete. Without this</span>
<span class="p_add">+	 * the __collapse_huge_page_copy can result in copying</span>
<span class="p_add">+	 * the old content.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_pmd_range(vma-&gt;vm_mm, &amp;pmd, address);</span>
 	return pmd;
 }
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 39f1d6a2b04d..edc90a2261f7 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -189,6 +189,25 @@</span> <span class="p_context"> extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
 				 unsigned long address, pmd_t *pmdp);
 #endif
 
<span class="p_add">+#ifndef pmd_collapse_flush</span>
<span class="p_add">+#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				       unsigned long address,</span>
<span class="p_add">+				       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pmdp_clear_flush(vma, address, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pmd_t pmd_collapse_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+				       unsigned long address,</span>
<span class="p_add">+				       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return __pmd(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
 extern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 				       pgtable_t pgtable);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 078832cf3636..009a5de619fd 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2499,7 +2499,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * huge and small TLB entries for the same virtual address
 	 * to avoid the risk of CPU bugs in that area.
 	 */
<span class="p_del">-	_pmd = pmdp_clear_flush(vma, address, pmd);</span>
<span class="p_add">+	_pmd = pmd_collapse_flush(vma, address, pmd);</span>
 	spin_unlock(pmd_ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



