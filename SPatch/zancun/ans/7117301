
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv10,25/36] mm, thp: remove infrastructure for handling splitting PMDs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv10,25/36] mm, thp: remove infrastructure for handling splitting PMDs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 3, 2015, 3:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1441293202-137314-26-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7117301/mbox/"
   >mbox</a>
|
   <a href="/patch/7117301/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7117301/">/patch/7117301/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 2539ABEEC1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 15:16:24 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 7EDA320713
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 15:16:22 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C72362070C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 15:16:19 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932546AbbICPQP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 3 Sep 2015 11:16:15 -0400
Received: from mga02.intel.com ([134.134.136.20]:3146 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1756891AbbICPOI (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 3 Sep 2015 11:14:08 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by orsmga101.jf.intel.com with ESMTP; 03 Sep 2015 08:13:43 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,462,1437462000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;797109133&quot;
Received: from black.fi.intel.com ([10.237.72.213])
	by orsmga002.jf.intel.com with ESMTP; 03 Sep 2015 08:13:37 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id E48067FA; Thu,  3 Sep 2015 18:13:25 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Christoph Lameter &lt;cl@gentwo.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Steve Capper &lt;steve.capper@linaro.org&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Jerome Marchand &lt;jmarchan@redhat.com&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv10 25/36] mm,
	thp: remove infrastructure for handling splitting PMDs
Date: Thu,  3 Sep 2015 18:13:11 +0300
Message-Id: &lt;1441293202-137314-26-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.5.0
In-Reply-To: &lt;1441293202-137314-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1441293202-137314-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Sept. 3, 2015, 3:13 p.m.</div>
<pre class="content">
With new refcounting we don&#39;t need to mark PMDs splitting. Let&#39;s drop code
to handle this.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="tested-by">Tested-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
<span class="tested-by">Tested-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="acked-by">Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="acked-by">Acked-by: Jerome Marchand &lt;jmarchan@redhat.com&gt;</span>
---
 fs/proc/task_mmu.c            |  8 +++---
 include/asm-generic/pgtable.h |  9 ------
 include/linux/huge_mm.h       | 21 ++++----------
 mm/gup.c                      | 12 +-------
 mm/huge_memory.c              | 67 +++++++++++--------------------------------
 mm/memcontrol.c               | 13 ++-------
 mm/memory.c                   | 18 ++----------
 mm/mincore.c                  |  2 +-
 mm/mremap.c                   | 15 +++++-----
 mm/pgtable-generic.c          | 14 ---------
 mm/rmap.c                     |  4 +--
 11 files changed, 40 insertions(+), 143 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 6b73be84271c..9c0401711acb 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -563,7 +563,7 @@</span> <span class="p_context"> static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 	pte_t *pte;
 	spinlock_t *ptl;
 
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		smaps_pmd_entry(pmd, addr, walk);
 		spin_unlock(ptl);
 		return 0;
<span class="p_chunk">@@ -844,7 +844,7 @@</span> <span class="p_context"> static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	spinlock_t *ptl;
 	struct page *page;
 
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		if (cp-&gt;type == CLEAR_REFS_SOFT_DIRTY) {
 			clear_soft_dirty_pmd(vma, addr, pmd);
 			goto out;
<span class="p_chunk">@@ -1118,7 +1118,7 @@</span> <span class="p_context"> static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
 	int err = 0;
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-	if (pmd_trans_huge_lock(pmdp, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmdp, vma, &amp;ptl)) {</span>
 		u64 flags = 0, frame = 0;
 		pmd_t pmd = *pmdp;
 
<span class="p_chunk">@@ -1450,7 +1450,7 @@</span> <span class="p_context"> static int gather_pte_stats(pmd_t *pmd, unsigned long addr,</span>
 	pte_t *orig_pte;
 	pte_t *pte;
 
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		pte_t huge_pte = *(pte_t *)pmd;
 		struct page *page;
 
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 29c57b2cb344..010a7e3f6ad1 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -184,11 +184,6 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_SPLITTING_FLUSH</span>
<span class="p_del">-extern void pmdp_splitting_flush(struct vm_area_struct *vma,</span>
<span class="p_del">-				 unsigned long address, pmd_t *pmdp);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #ifndef pmdp_collapse_flush
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 extern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,
<span class="p_chunk">@@ -586,10 +581,6 @@</span> <span class="p_context"> static inline int pmd_trans_huge(pmd_t pmd)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int pmd_trans_splitting(pmd_t pmd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
 #ifndef __HAVE_ARCH_PMD_WRITE
 static inline int pmd_write(pmd_t pmd)
 {
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index 87ea3723ff04..b82c0c2a96f6 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -28,7 +28,7 @@</span> <span class="p_context"> extern int zap_huge_pmd(struct mmu_gather *tlb,</span>
 extern int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 			unsigned long addr, unsigned long end,
 			unsigned char *vec);
<span class="p_del">-extern int move_huge_pmd(struct vm_area_struct *vma,</span>
<span class="p_add">+extern bool move_huge_pmd(struct vm_area_struct *vma,</span>
 			 struct vm_area_struct *new_vma,
 			 unsigned long old_addr,
 			 unsigned long new_addr, unsigned long old_end,
<span class="p_chunk">@@ -51,15 +51,9 @@</span> <span class="p_context"> enum transparent_hugepage_flag {</span>
 #endif
 };
 
<span class="p_del">-enum page_check_address_pmd_flag {</span>
<span class="p_del">-	PAGE_CHECK_ADDRESS_PMD_FLAG,</span>
<span class="p_del">-	PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG,</span>
<span class="p_del">-	PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG,</span>
<span class="p_del">-};</span>
 extern pmd_t *page_check_address_pmd(struct page *page,
 				     struct mm_struct *mm,
 				     unsigned long address,
<span class="p_del">-				     enum page_check_address_pmd_flag flag,</span>
 				     spinlock_t **ptl);
 extern int pmd_freeable(pmd_t pmd);
 
<span class="p_chunk">@@ -104,7 +98,6 @@</span> <span class="p_context"> extern unsigned long transparent_hugepage_flags;</span>
 #define split_huge_page(page) BUILD_BUG()
 #define split_huge_pmd(__vma, __pmd, __address) BUILD_BUG()
 
<span class="p_del">-#define wait_split_huge_page(__anon_vma, __pmd) BUILD_BUG()</span>
 #if HPAGE_PMD_ORDER &gt;= MAX_ORDER
 #error &quot;hugepages can&#39;t be allocated by the buddy allocator&quot;
 #endif
<span class="p_chunk">@@ -114,17 +107,17 @@</span> <span class="p_context"> extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 				    unsigned long start,
 				    unsigned long end,
 				    long adjust_next);
<span class="p_del">-extern int __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
<span class="p_add">+extern bool __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
 		spinlock_t **ptl);
 /* mmap_sem must be held on entry */
<span class="p_del">-static inline int pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
<span class="p_add">+static inline bool pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
 		spinlock_t **ptl)
 {
 	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);
 	if (pmd_trans_huge(*pmd))
 		return __pmd_trans_huge_lock(pmd, vma, ptl);
 	else
<span class="p_del">-		return 0;</span>
<span class="p_add">+		return false;</span>
 }
 static inline int hpage_nr_pages(struct page *page)
 {
<span class="p_chunk">@@ -169,8 +162,6 @@</span> <span class="p_context"> static inline int split_huge_page(struct page *page)</span>
 {
 	return 0;
 }
<span class="p_del">-#define wait_split_huge_page(__anon_vma, __pmd)	\</span>
<span class="p_del">-	do { } while (0)</span>
 #define split_huge_pmd(__vma, __pmd, __address)	\
 	do { } while (0)
 static inline int hugepage_madvise(struct vm_area_struct *vma,
<span class="p_chunk">@@ -185,10 +176,10 @@</span> <span class="p_context"> static inline void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 					 long adjust_next)
 {
 }
<span class="p_del">-static inline int pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
<span class="p_add">+static inline bool pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
 		spinlock_t **ptl)
 {
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return false;</span>
 }
 
 static inline int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 7017abea9fd6..70d65e4015a4 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -241,13 +241,6 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
 	}
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(pmd_trans_splitting(*pmd))) {</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_del">-		wait_split_huge_page(vma-&gt;anon_vma, pmd);</span>
<span class="p_del">-		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	if (flags &amp; FOLL_SPLIT) {
 		int ret;
 		page = pmd_page(*pmd);
<span class="p_chunk">@@ -1068,9 +1061,6 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
  *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free
  *      pages containing page tables.
  *
<span class="p_del">- *  *) THP splits will broadcast an IPI, this can be achieved by overriding</span>
<span class="p_del">- *      pmdp_splitting_flush.</span>
<span class="p_del">- *</span>
  *  *) ptes can be read atomically by the architecture.
  *
  *  *) access_ok is sufficient to validate userspace address ranges.
<span class="p_chunk">@@ -1267,7 +1257,7 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		pmd_t pmd = READ_ONCE(*pmdp);
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_none(pmd) || pmd_trans_splitting(pmd))</span>
<span class="p_add">+		if (pmd_none(pmd))</span>
 			return 0;
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 5dc2b822b692..d24e4c131733 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -958,15 +958,6 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		goto out_unlock;
 	}
 
<span class="p_del">-	if (unlikely(pmd_trans_splitting(pmd))) {</span>
<span class="p_del">-		/* split huge page running from under us */</span>
<span class="p_del">-		spin_unlock(src_ptl);</span>
<span class="p_del">-		spin_unlock(dst_ptl);</span>
<span class="p_del">-		pte_free(dst_mm, pgtable);</span>
<span class="p_del">-</span>
<span class="p_del">-		wait_split_huge_page(vma-&gt;anon_vma, src_pmd); /* src_vma */</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
 	src_page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!PageHead(src_page), src_page);
 	get_page(src_page);
<span class="p_chunk">@@ -1472,7 +1463,7 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	pmd_t orig_pmd;
 	spinlock_t *ptl;
 
<span class="p_del">-	if (__pmd_trans_huge_lock(pmd, vma, &amp;ptl) != 1)</span>
<span class="p_add">+	if (!__pmd_trans_huge_lock(pmd, vma, &amp;ptl))</span>
 		return 0;
 	/*
 	 * For architectures like ppc64 we look at deposited pgtable
<span class="p_chunk">@@ -1506,13 +1497,12 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	return 1;
 }
 
<span class="p_del">-int move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,</span>
<span class="p_add">+bool move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,</span>
 		  unsigned long old_addr,
 		  unsigned long new_addr, unsigned long old_end,
 		  pmd_t *old_pmd, pmd_t *new_pmd)
 {
 	spinlock_t *old_ptl, *new_ptl;
<span class="p_del">-	int ret = 0;</span>
 	pmd_t pmd;
 
 	struct mm_struct *mm = vma-&gt;vm_mm;
<span class="p_chunk">@@ -1521,7 +1511,7 @@</span> <span class="p_context"> int move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,</span>
 	    (new_addr &amp; ~HPAGE_PMD_MASK) ||
 	    old_end - old_addr &lt; HPAGE_PMD_SIZE ||
 	    (new_vma-&gt;vm_flags &amp; VM_NOHUGEPAGE))
<span class="p_del">-		goto out;</span>
<span class="p_add">+		return false;</span>
 
 	/*
 	 * The destination pmd shouldn&#39;t be established, free_pgtables()
<span class="p_chunk">@@ -1529,15 +1519,14 @@</span> <span class="p_context"> int move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,</span>
 	 */
 	if (WARN_ON(!pmd_none(*new_pmd))) {
 		VM_BUG_ON(pmd_trans_huge(*new_pmd));
<span class="p_del">-		goto out;</span>
<span class="p_add">+		return false;</span>
 	}
 
 	/*
 	 * We don&#39;t have to worry about the ordering of src and dst
 	 * ptlocks because exclusive mmap_sem prevents deadlock.
 	 */
<span class="p_del">-	ret = __pmd_trans_huge_lock(old_pmd, vma, &amp;old_ptl);</span>
<span class="p_del">-	if (ret == 1) {</span>
<span class="p_add">+	if (__pmd_trans_huge_lock(old_pmd, vma, &amp;old_ptl)) {</span>
 		new_ptl = pmd_lockptr(mm, new_pmd);
 		if (new_ptl != old_ptl)
 			spin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);
<span class="p_chunk">@@ -1553,9 +1542,9 @@</span> <span class="p_context"> int move_huge_pmd(struct vm_area_struct *vma, struct vm_area_struct *new_vma,</span>
 		if (new_ptl != old_ptl)
 			spin_unlock(new_ptl);
 		spin_unlock(old_ptl);
<span class="p_add">+		return true;</span>
 	}
<span class="p_del">-out:</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return false;</span>
 }
 
 /*
<span class="p_chunk">@@ -1571,7 +1560,7 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	spinlock_t *ptl;
 	int ret = 0;
 
<span class="p_del">-	if (__pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (__pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		pmd_t entry;
 		bool preserve_write = prot_numa &amp;&amp; pmd_write(*pmd);
 		ret = 1;
<span class="p_chunk">@@ -1602,29 +1591,19 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 }
 
 /*
<span class="p_del">- * Returns 1 if a given pmd maps a stable (not under splitting) thp.</span>
<span class="p_del">- * Returns -1 if it maps a thp under splitting. Returns 0 otherwise.</span>
<span class="p_add">+ * Returns true if a given pmd maps a thp, false otherwise.</span>
  *
<span class="p_del">- * Note that if it returns 1, this routine returns without unlocking page</span>
<span class="p_del">- * table locks. So callers must unlock them.</span>
<span class="p_add">+ * Note that if it returns true, this routine returns without unlocking page</span>
<span class="p_add">+ * table lock. So callers must unlock it.</span>
  */
<span class="p_del">-int __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
<span class="p_add">+bool __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
 		spinlock_t **ptl)
 {
 	*ptl = pmd_lock(vma-&gt;vm_mm, pmd);
<span class="p_del">-	if (likely(pmd_trans_huge(*pmd))) {</span>
<span class="p_del">-		if (unlikely(pmd_trans_splitting(*pmd))) {</span>
<span class="p_del">-			spin_unlock(*ptl);</span>
<span class="p_del">-			wait_split_huge_page(vma-&gt;anon_vma, pmd);</span>
<span class="p_del">-			return -1;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			/* Thp mapped by &#39;pmd&#39; is stable, so we can</span>
<span class="p_del">-			 * handle it as it is. */</span>
<span class="p_del">-			return 1;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (likely(pmd_trans_huge(*pmd)))</span>
<span class="p_add">+		return true;</span>
 	spin_unlock(*ptl);
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return false;</span>
 }
 
 /*
<span class="p_chunk">@@ -1638,7 +1617,6 @@</span> <span class="p_context"> int __pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma,</span>
 pmd_t *page_check_address_pmd(struct page *page,
 			      struct mm_struct *mm,
 			      unsigned long address,
<span class="p_del">-			      enum page_check_address_pmd_flag flag,</span>
 			      spinlock_t **ptl)
 {
 	pgd_t *pgd;
<span class="p_chunk">@@ -1661,21 +1639,8 @@</span> <span class="p_context"> pmd_t *page_check_address_pmd(struct page *page,</span>
 		goto unlock;
 	if (pmd_page(*pmd) != page)
 		goto unlock;
<span class="p_del">-	/*</span>
<span class="p_del">-	 * split_vma() may create temporary aliased mappings. There is</span>
<span class="p_del">-	 * no risk as long as all huge pmd are found and have their</span>
<span class="p_del">-	 * splitting bit set before __split_huge_page_refcount</span>
<span class="p_del">-	 * runs. Finding the same huge pmd more than once during the</span>
<span class="p_del">-	 * same rmap walk is not a problem.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG &amp;&amp;</span>
<span class="p_del">-	    pmd_trans_splitting(*pmd))</span>
<span class="p_del">-		goto unlock;</span>
<span class="p_del">-	if (pmd_trans_huge(*pmd)) {</span>
<span class="p_del">-		VM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG &amp;&amp;</span>
<span class="p_del">-			  !pmd_trans_splitting(*pmd));</span>
<span class="p_add">+	if (pmd_trans_huge(*pmd))</span>
 		return pmd;
<span class="p_del">-	}</span>
 unlock:
 	spin_unlock(*ptl);
 	return NULL;
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index f0efdf89b495..57d27be48bb9 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4717,7 +4717,7 @@</span> <span class="p_context"> static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
 	pte_t *pte;
 	spinlock_t *ptl;
 
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		if (get_mctgt_type_thp(vma, addr, *pmd, NULL) == MC_TARGET_PAGE)
 			mc.precharge += HPAGE_PMD_NR;
 		spin_unlock(ptl);
<span class="p_chunk">@@ -4888,16 +4888,7 @@</span> <span class="p_context"> static int mem_cgroup_move_charge_pte_range(pmd_t *pmd,</span>
 	union mc_target target;
 	struct page *page;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * No race with splitting thp happens because:</span>
<span class="p_del">-	 *  - if pmd_trans_huge_lock() returns 1, the relevant thp is not</span>
<span class="p_del">-	 *    under splitting, which means there&#39;s no concurrent thp split,</span>
<span class="p_del">-	 *  - if another thread runs into split_huge_page() just after we</span>
<span class="p_del">-	 *    entered this if-block, the thread must wait for page table lock</span>
<span class="p_del">-	 *    to be unlocked in __split_huge_page_splitting(), where the main</span>
<span class="p_del">-	 *    part of thp split is not executed yet.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		if (mc.precharge &lt; HPAGE_PMD_NR) {
 			spin_unlock(ptl);
 			return 0;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index c246f0801e3c..24db58765b42 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -566,7 +566,6 @@</span> <span class="p_context"> int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 {
 	spinlock_t *ptl;
 	pgtable_t new = pte_alloc_one(mm, address);
<span class="p_del">-	int wait_split_huge_page;</span>
 	if (!new)
 		return -ENOMEM;
 
<span class="p_chunk">@@ -586,18 +585,14 @@</span> <span class="p_context"> int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
 
 	ptl = pmd_lock(mm, pmd);
<span class="p_del">-	wait_split_huge_page = 0;</span>
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
 		atomic_long_inc(&amp;mm-&gt;nr_ptes);
 		pmd_populate(mm, pmd, new);
 		new = NULL;
<span class="p_del">-	} else if (unlikely(pmd_trans_splitting(*pmd)))</span>
<span class="p_del">-		wait_split_huge_page = 1;</span>
<span class="p_add">+	}</span>
 	spin_unlock(ptl);
 	if (new)
 		pte_free(mm, new);
<span class="p_del">-	if (wait_split_huge_page)</span>
<span class="p_del">-		wait_split_huge_page(vma-&gt;anon_vma, pmd);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -613,8 +608,7 @@</span> <span class="p_context"> int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)</span>
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
 		pmd_populate_kernel(&amp;init_mm, pmd, new);
 		new = NULL;
<span class="p_del">-	} else</span>
<span class="p_del">-		VM_BUG_ON(pmd_trans_splitting(*pmd));</span>
<span class="p_add">+	}</span>
 	spin_unlock(&amp;init_mm.page_table_lock);
 	if (new)
 		pte_free_kernel(&amp;init_mm, new);
<span class="p_chunk">@@ -3365,14 +3359,6 @@</span> <span class="p_context"> static int __handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		if (pmd_trans_huge(orig_pmd)) {
 			unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;
 
<span class="p_del">-			/*</span>
<span class="p_del">-			 * If the pmd is splitting, return and retry the</span>
<span class="p_del">-			 * the fault.  Alternative: wait until the split</span>
<span class="p_del">-			 * is done, and goto retry.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			if (pmd_trans_splitting(orig_pmd))</span>
<span class="p_del">-				return 0;</span>
<span class="p_del">-</span>
 			if (pmd_protnone(orig_pmd))
 				return do_huge_pmd_numa_page(mm, vma, address,
 							     orig_pmd, pmd);
<span class="p_header">diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="p_header">index be25efde64a4..feb867f5fdf4 100644</span>
<span class="p_header">--- a/mm/mincore.c</span>
<span class="p_header">+++ b/mm/mincore.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 	unsigned char *vec = walk-&gt;private;
 	int nr = (end - addr) &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="p_add">+	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl)) {</span>
 		memset(vec, 1, nr);
 		spin_unlock(ptl);
 		goto out;
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 9cf393ac6e43..0dbae2e91e19 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -192,25 +192,24 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 		if (!new_pmd)
 			break;
 		if (pmd_trans_huge(*old_pmd)) {
<span class="p_del">-			int err = 0;</span>
 			if (extent == HPAGE_PMD_SIZE) {
<span class="p_add">+				bool moved;</span>
 				VM_BUG_ON_VMA(vma-&gt;vm_file || !vma-&gt;anon_vma,
 					      vma);
 				/* See comment in move_ptes() */
 				if (need_rmap_locks)
 					anon_vma_lock_write(vma-&gt;anon_vma);
<span class="p_del">-				err = move_huge_pmd(vma, new_vma, old_addr,</span>
<span class="p_add">+				moved = move_huge_pmd(vma, new_vma, old_addr,</span>
 						    new_addr, old_end,
 						    old_pmd, new_pmd);
 				if (need_rmap_locks)
 					anon_vma_unlock_write(vma-&gt;anon_vma);
<span class="p_add">+				if (moved) {</span>
<span class="p_add">+					need_flush = true;</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+				}</span>
 			}
<span class="p_del">-			if (err &gt; 0) {</span>
<span class="p_del">-				need_flush = true;</span>
<span class="p_del">-				continue;</span>
<span class="p_del">-			} else if (!err) {</span>
<span class="p_del">-				split_huge_pmd(vma, old_pmd, old_addr);</span>
<span class="p_del">-			}</span>
<span class="p_add">+			split_huge_pmd(vma, old_pmd, old_addr);</span>
 			VM_BUG_ON(pmd_trans_huge(*old_pmd));
 		}
 		if (pmd_none(*new_pmd) &amp;&amp; __pte_alloc(new_vma-&gt;vm_mm, new_vma,
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 6b674e00153c..89b150f8c920 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -134,20 +134,6 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 #endif
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_SPLITTING_FLUSH</span>
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-			  pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t pmd = pmd_mksplitting(*pmdp);</span>
<span class="p_del">-	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_del">-	set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="p_del">-	/* tlb flush only to serialize against gup-fast */</span>
<span class="p_del">-	flush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_TRANSPARENT_HUGEPAGE */</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #ifndef __HAVE_ARCH_PGTABLE_DEPOSIT
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 740a1851056a..f631177bda98 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -843,8 +843,7 @@</span> <span class="p_context"> static int page_referenced_one(struct page *page, struct vm_area_struct *vma,</span>
 		 * rmap might return false positives; we must filter
 		 * these out using page_check_address_pmd().
 		 */
<span class="p_del">-		pmd = page_check_address_pmd(page, mm, address,</span>
<span class="p_del">-					     PAGE_CHECK_ADDRESS_PMD_FLAG, &amp;ptl);</span>
<span class="p_add">+		pmd = page_check_address_pmd(page, mm, address, &amp;ptl);</span>
 		if (!pmd)
 			return SWAP_AGAIN;
 
<span class="p_chunk">@@ -855,7 +854,6 @@</span> <span class="p_context"> static int page_referenced_one(struct page *page, struct vm_area_struct *vma,</span>
 			return SWAP_FAIL; /* To break the loop */
 		}
 
<span class="p_del">-		/* go ahead even if the pmd is pmd_trans_splitting() */</span>
 		if (pmdp_clear_flush_young_notify(vma, address, pmd))
 			referenced++;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



