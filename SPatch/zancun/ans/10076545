
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[04/24] x86/mm/kaiser: Unmap kernel mappings from userspace page tables, core patch - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [04/24] x86/mm/kaiser: Unmap kernel mappings from userspace page tables, core patch</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 27, 2017, 10:49 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171127104923.14378-5-mingo@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10076545/mbox/"
   >mbox</a>
|
   <a href="/patch/10076545/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10076545/">/patch/10076545/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	23FA1602BD for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 27 Nov 2017 10:57:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F04D628CF1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 27 Nov 2017 10:57:08 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E4C0028DB5; Mon, 27 Nov 2017 10:57:08 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7D9ED28CF1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 27 Nov 2017 10:57:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752618AbdK0K5F (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 27 Nov 2017 05:57:05 -0500
Received: from mail-wm0-f68.google.com ([74.125.82.68]:35169 &quot;EHLO
	mail-wm0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751793AbdK0Kvq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 27 Nov 2017 05:51:46 -0500
Received: by mail-wm0-f68.google.com with SMTP id w73so5130386wmw.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 27 Nov 2017 02:51:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=IuXYpQUW6G5idNLbTdGohzp61V9OA/xC7Tkj/zMOK9E=;
	b=NDNvbnY0aYMTuK+/87vqG9oE5jhXbtb+PcQw4xWCsrmMAil/zmZxhqcSJA9zy0/OcL
	pG+tJEhJMij8yOlXXcYl2op7RKL4tE7LovbOoZOJDyj3ZKOMMs0HsRODbC8Ums30d24c
	364YWKlVZsIi1KnP+ptQVChb+/WDcdHddoNDHNNcJYjwhbh15/dL4qs6u6kFzcYT3Ocw
	C4+2vJFCu+0WmMzA6LHHG5mW/Q/kELs14uCPjQAx2iGWTL/MmdwRWiJEPzyuwGA6VeZX
	gxLPBMXJoJNwNdltw0vOxlsFkBgCJ1imzFz7gpye7sQDyLPbLwBjUeRXVKXFuHidYCsQ
	F7DA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:from:to:cc:subject:date:message-id
	:in-reply-to:references;
	bh=IuXYpQUW6G5idNLbTdGohzp61V9OA/xC7Tkj/zMOK9E=;
	b=MaI2rmjs9wctzn6tIV5td0SMdsFgVWS5Idljo9+d6w9PYFGK3RApG2jMcrskaidWrr
	0ZqYTrArEQ2b2HVebv1RNa2s/ngOp80fTrBNQe617cD2cb0Egz25o9cUZlBUtIJAdfeh
	SRoYbY2p9/oR3TkDwrBp2wRvGp5UD3onTeytk+qLnwOyx2IIE0clH/CFCa7nYg3X6I9g
	+QR99ReOnrvRw6wj1Tteb0g5sW3ENfXgsG6ddImsW+5l+zhtg3tofwCa/A6IxZ7G196G
	1aeaHTusvg1E7QZU1F2FH1OYlNE623iXUVkPbgyNdHQJMP6VKgxtb11m+2oHN/9GRAO6
	yAeg==
X-Gm-Message-State: AJaThX77UMl/kFvNVHP5c1cYkQmDA+T3xfIbsnRxUBRstMciYZxVlINv
	E8o5fXepynased2UEAcWYG1QSw==
X-Google-Smtp-Source: AGs4zMaXq6yKG+EBz057yMQozmaTUHzsYkB3vxccZS1XYGiViCKTSLsCBiATzYaybEdwTWf9IGJA4Q==
X-Received: by 10.28.47.68 with SMTP id v65mr18057347wmv.11.1511779904195;
	Mon, 27 Nov 2017 02:51:44 -0800 (PST)
Received: from localhost.localdomain (2E8B0CD5.catv.pool.telekom.hu.
	[46.139.12.213]) by smtp.gmail.com with ESMTPSA id
	80sm13455774wmk.14.2017.11.27.02.51.42
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 27 Nov 2017 02:51:43 -0800 (PST)
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, &quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Subject: [PATCH 04/24] x86/mm/kaiser: Unmap kernel mappings from userspace
	page tables, core patch
Date: Mon, 27 Nov 2017 11:49:03 +0100
Message-Id: &lt;20171127104923.14378-5-mingo@kernel.org&gt;
X-Mailer: git-send-email 2.14.1
In-Reply-To: &lt;20171127104923.14378-1-mingo@kernel.org&gt;
References: &lt;20171127104923.14378-1-mingo@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Nov. 27, 2017, 10:49 a.m.</div>
<pre class="content">
<span class="from">From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

These patches are based on work from a team at the Graz University of
Technology:

   https://github.com/IAIK/KAISER

Signed off by the following gentlemen:

  *   Signed-off-by: Richard Fellner &lt;richard.fellner@student.tugraz.at&gt;
  *   Signed-off-by: Moritz Lipp     &lt;moritz.lipp@iaik.tugraz.at&gt;
  *   Signed-off-by: Daniel Gruss    &lt;daniel.gruss@iaik.tugraz.at&gt;
  *   Signed-off-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;

This work would not have been possible without their work as a starting point.

To credit the original work we kept the &#39;KAISER&#39; name, and added the
Signed-off-by tags to the KAISER source code file itself. This patch
also carries their &#39;Originally-by&#39; tags to show credit. (We could not
keep the original Signed-off-by tags for the patch itself, as it was
changed significantly.)

KAISER is a countermeasure against side channel attacks against kernel
virtual memory.  It leaves the existing page tables largely alone and
refers to them as the &quot;kernel page tables.  It adds a &quot;shadow&quot; PGD for
every process which is intended for use when running userspace.  The
shadow PGD maps all the same user memory as the &quot;kernel&quot; copy, but
only maps a minimal set of kernel memory.

Whenever entering the kernel (syscalls, interrupts, exceptions), the
PGD is switched to the &quot;kernel&quot; copy.  When switching back to user
mode, the shadow PGD is used.

The minimalistic kernel page tables try to map only what is needed to
enter/exit the kernel such as the entry/exit functions themselves and
the interrupt descriptors (IDT).
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
=== Page Table Poisoning ===

KAISER has two copies of the page tables: one for the kernel and
one for when running in userspace.  There is also a kernel
portion of each of the page tables: the part that *maps* the
kernel.

The kernel portion is relatively static and uses pre-populated
PGDs.  Nobody ever calls set_pgd() on the kernel portion during
normal operation.

The userspace portion of the page tables is updated frequently as
userspace pages are mapped and page table pages are allocated.
These updates of the userspace *portion* of the tables need to be
reflected into both the kernel and user/shadow copies.

The original KAISER patches did this by effectively looking at the
address that is being updated.  If it is &lt;PAGE_OFFSET, it is
considered to be doing an update for the userspace portion of the page
tables and must make an entry in the shadow.

However, this has a wrinkle: there are a few places where low
addresses are used in supervisor (kernel) mode.  When EFI calls
are made, they use what are traditionally user addresses in
supervisor mode and trip over these checks.  The trampoline code
that used for booting secondary CPUs has a similar issue.

Remember, there are two things that KAISER needs performed on a
userspace PGD:

 1. Populate the shadow itself
 2. Poison the kernel PGD so it can not be used by userspace.

Only perform these actions when dealing with a user address *and* the
PGD has _PAGE_USER set.  That way, in-kernel users of low addresses
typically used by userspace are not accidentally poisoned.

Here an (incomplete) list of changes from original KAISER patch:

 * Gobs of coding style cleanups

 * The original patch tried to allocate an order-2 page, then
   8k-align the result.  That&#39;s silly since order-2 is already
   guaranteed to be 16k-aligned.  Removed that gunk and just
   allocate an order-1 page.

 * Handle (or at least detect and warn on) allocation failures

 * Use _KERNPG_TABLE, not _PAGE_TABLE when creating mappings for
   the kernel in the shadow (user) page tables.

 * BUG_ON() for !pte_none() case was totally insane: it checked
   the physical address of the &#39;struct page&#39; against the physical
   address of the page being mapped.

 * Added 5-level page table support

 * Never free KAISER page tables.  We don&#39;t have the locking to
   keep them from getting referenced during the freeing process.

 * Use a totally different scheme in the entry code.  The
   original code just fell apart in horrific ways in debug faults,
   NMIs, or when iret faults.  Big thanks to Andy Lutomirski for
   reducing the number of places that needed to be patched.  He
   made the code a ton simpler.

 * Use new entry trampoline instead of mapping process stacks.

Originally-by: Richard Fellner &lt;richard.fellner@student.tugraz.at&gt;
Originally-by: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;
Originally-by: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;
Originally-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;
Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
Signed-off-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Brian Gerst &lt;brgerst@gmail.com&gt;
Cc: Denys Vlasenko &lt;dvlasenk@redhat.com&gt;
Cc: H. Peter Anvin &lt;hpa@zytor.com&gt;
Cc: Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: daniel.gruss@iaik.tugraz.at
Cc: hughd@google.com
Cc: keescook@google.com
Cc: linux-mm@kvack.org
Cc: michael.schwarz@iaik.tugraz.at
Cc: moritz.lipp@iaik.tugraz.at
Cc: richard.fellner@student.tugraz.at
Link: https://lkml.kernel.org/r/20171123003447.1DB395E3@viggo.jf.intel.com
Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
<span class="p_del">---</span>
 Documentation/x86/kaiser.txt      | 159 ++++++++++++++
 arch/x86/entry/calling.h          |   1 +
 arch/x86/include/asm/kaiser.h     |  57 +++++
 arch/x86/include/asm/pgtable.h    |   5 +
 arch/x86/include/asm/pgtable_64.h | 133 ++++++++++++
 arch/x86/kernel/espfix_64.c       |  18 ++
 arch/x86/kernel/head_64.S         |  14 +-
 arch/x86/mm/Makefile              |   1 +
 arch/x86/mm/kaiser.c              | 444 ++++++++++++++++++++++++++++++++++++++
 arch/x86/mm/pageattr.c            |   2 +-
 arch/x86/mm/pgtable.c             |  16 +-
 include/linux/kaiser.h            |  29 +++
 init/main.c                       |   3 +
 kernel/fork.c                     |   1 +
 14 files changed, 877 insertions(+), 6 deletions(-)

<span class="p_header">diff --git a/Documentation/x86/kaiser.txt b/Documentation/x86/kaiser.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..f2df0441f6ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/x86/kaiser.txt</span>
<span class="p_chunk">@@ -0,0 +1,159 @@</span> <span class="p_context"></span>
<span class="p_add">+Overview</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+KAISER is a countermeasure against attacks on kernel address</span>
<span class="p_add">+information.  There are at least three existing, published,</span>
<span class="p_add">+approaches using the shared user/kernel mapping and hardware features</span>
<span class="p_add">+to defeat KASLR.  One approach referenced in the paper</span>
<span class="p_add">+(https://gruss.cc/files/kaiser.pdf) locates the kernel by</span>
<span class="p_add">+observing differences in page fault timing between</span>
<span class="p_add">+present-but-inaccessible kernel pages and non-present pages.</span>
<span class="p_add">+</span>
<span class="p_add">+When the kernel is entered via syscalls, interrupts or exceptions,</span>
<span class="p_add">+page tables are switched to the full &quot;kernel&quot; copy.  When the</span>
<span class="p_add">+system switches back to user mode, the user/shadow copy is used.</span>
<span class="p_add">+</span>
<span class="p_add">+The minimalistic kernel portion of the user page tables try to</span>
<span class="p_add">+map only what is needed to enter/exit the kernel such as the</span>
<span class="p_add">+entry/exit functions themselves and the interrupt descriptor</span>
<span class="p_add">+table (IDT).</span>
<span class="p_add">+</span>
<span class="p_add">+This helps to ensure that side-channel attacks that leverage the</span>
<span class="p_add">+paging structures do not function when KAISER is enabled, by setting</span>
<span class="p_add">+CONFIG_KAISER=y.</span>
<span class="p_add">+</span>
<span class="p_add">+Page Table Management</span>
<span class="p_add">+=====================</span>
<span class="p_add">+</span>
<span class="p_add">+When KAISER is enabled, the kernel manages two sets of page</span>
<span class="p_add">+tables.  The first copy is very similar to what would be present</span>
<span class="p_add">+for a kernel without KAISER.  It includes a complete mapping of</span>
<span class="p_add">+userspace that the kernel needs for things like copy_*_user().</span>
<span class="p_add">+</span>
<span class="p_add">+The second (shadow) is used when running userspace and mirrors the</span>
<span class="p_add">+mapping of userspace present in the kernel copy.  It maps only</span>
<span class="p_add">+the kernel data needed to enter and exit the kernel.</span>
<span class="p_add">+</span>
<span class="p_add">+The shadow is populated by the kaiser_add_*() functions.  Only</span>
<span class="p_add">+kernel data which has been explicitly mapped will appear in the</span>
<span class="p_add">+shadow copy. These calls are rare at runtime.</span>
<span class="p_add">+</span>
<span class="p_add">+For a new userspace mapping, the kernel makes the entries in its</span>
<span class="p_add">+page tables like normal.  The only difference is when the kernel</span>
<span class="p_add">+makes entries in the top (PGD) level.  In addition to setting the</span>
<span class="p_add">+entry in the main kernel PGD, a copy of the entry is made in the</span>
<span class="p_add">+shadow PGD.</span>
<span class="p_add">+</span>
<span class="p_add">+For user space mappings the kernel creates an entry in the kernel</span>
<span class="p_add">+PGD and the same entry in the shadow PGD, so the underlying page</span>
<span class="p_add">+table to which the PGD entry points to, is shared down to the PTE</span>
<span class="p_add">+level.  This leaves a single, shared set of userspace page tables</span>
<span class="p_add">+to manage.  One PTE to lock, one set of accessed, dirty bits, etc...</span>
<span class="p_add">+</span>
<span class="p_add">+Overhead</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+Protection against side-channel attacks is important.  But,</span>
<span class="p_add">+this protection comes at a cost:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Increased Memory Use</span>
<span class="p_add">+  a. Each process now needs an order-1 PGD instead of order-0.</span>
<span class="p_add">+     (Consumes 4k per process).</span>
<span class="p_add">+  b. The pre-allocated second-level (p4d or pud) kernel page</span>
<span class="p_add">+     table pages cost ~1MB of additional memory at boot.  This</span>
<span class="p_add">+     is not totally wasted because some of these pages would</span>
<span class="p_add">+     have been needed eventually for normal kernel page tables</span>
<span class="p_add">+     and things in the vmalloc() area like vmemmap[].</span>
<span class="p_add">+  c. Statically-allocated structures and entry/exit text must</span>
<span class="p_add">+     be padded out to 4k (or 8k for PGDs) so they can be mapped</span>
<span class="p_add">+     into the user page tables.  This bloats the kernel image</span>
<span class="p_add">+     by ~20-30k.</span>
<span class="p_add">+  d. The shadow page tables eventually grow to map all of used</span>
<span class="p_add">+     vmalloc() space.  They can have roughly the same memory</span>
<span class="p_add">+     consumption as the vmalloc() page tables.</span>
<span class="p_add">+</span>
<span class="p_add">+2. Runtime Cost</span>
<span class="p_add">+  a. CR3 manipulation to switch between the page table copies</span>
<span class="p_add">+     must be done at interrupt, syscall, and exception entry</span>
<span class="p_add">+     and exit (it can be skipped when the kernel is interrupted,</span>
<span class="p_add">+     though.)  CR3 modifications are in the order of a hundred</span>
<span class="p_add">+     cycles, and are required at every entry and exit.</span>
<span class="p_add">+  b. Task stacks must be mapped/unmapped.  We need to walk</span>
<span class="p_add">+     and modify the shadow page tables at fork() and exit().</span>
<span class="p_add">+  c. Global pages are disabled.  This feature of the MMU</span>
<span class="p_add">+     allows different processes to share TLB entries mapping</span>
<span class="p_add">+     the kernel.  Losing the feature means potentially more</span>
<span class="p_add">+     TLB misses after a context switch.</span>
<span class="p_add">+  d. Process Context IDentifiers (PCID) is a CPU feature that</span>
<span class="p_add">+     allows us to skip flushing the entire TLB when switching</span>
<span class="p_add">+     page tables.  This makes switching the page tables (at</span>
<span class="p_add">+     context switch, or kernel entry/exit) cheaper.  But, on</span>
<span class="p_add">+     systems with PCID support, the context switch code must flush</span>
<span class="p_add">+     both the user and kernel entries out of the TLB, with an</span>
<span class="p_add">+     INVPCID in addition to the CR3 write.  This INVPCID is</span>
<span class="p_add">+     generally slower than a CR3 write, but still in the order of</span>
<span class="p_add">+     a hundred cycles.</span>
<span class="p_add">+  e. The shadow page tables must be populated for each new</span>
<span class="p_add">+     process.  Even without KAISER, the shared kernel mappings</span>
<span class="p_add">+     are created by copying top-level (PGD) entries into each</span>
<span class="p_add">+     new process.  But, with KAISER, there are now *two* kernel</span>
<span class="p_add">+     mappings: one in the kernel page tables that maps everything</span>
<span class="p_add">+     and one in the user/shadow page tables mapping the &quot;minimal&quot;</span>
<span class="p_add">+     kernel.  At fork(), a copy of the portion of the shadow PGD</span>
<span class="p_add">+     that maps the minimal kernel structures is needed in</span>
<span class="p_add">+     addition to the normal kernel PGD.</span>
<span class="p_add">+  f. In addition to the fork()-time copying, there must also</span>
<span class="p_add">+     be an update to the shadow PGD any time a set_pgd() is done</span>
<span class="p_add">+     on a PGD used to map userspace.  This ensures that the kernel</span>
<span class="p_add">+     and user/shadow copies always map the same userspace</span>
<span class="p_add">+     memory.</span>
<span class="p_add">+  g. On systems without PCID support, each CR3 write flushes</span>
<span class="p_add">+     the entire TLB.  That means that each syscall, interrupt</span>
<span class="p_add">+     or exception flushes the TLB.</span>
<span class="p_add">+</span>
<span class="p_add">+Possible Future Work:</span>
<span class="p_add">+1. We can be more careful about not actually writing to CR3</span>
<span class="p_add">+   unless its value is actually changed.</span>
<span class="p_add">+2. Compress the user/shadow-mapped data to be mapped together</span>
<span class="p_add">+   underneath a single PGD entry.</span>
<span class="p_add">+3. Re-enable global pages, but use them for mappings in the</span>
<span class="p_add">+   user/shadow page tables.  This would allow the kernel to</span>
<span class="p_add">+   take advantage of TLB entries that were established from</span>
<span class="p_add">+   the user page tables.  This might speed up the entry/exit</span>
<span class="p_add">+   code or userspace since it will not have to reload all of</span>
<span class="p_add">+   its TLB entries.  However, its upside is limited by PCID</span>
<span class="p_add">+   being used.</span>
<span class="p_add">+4. Allow KAISER to be enabled/disabled at runtime so folks can</span>
<span class="p_add">+   run a single kernel image.</span>
<span class="p_add">+</span>
<span class="p_add">+Debugging:</span>
<span class="p_add">+</span>
<span class="p_add">+Bugs in KAISER cause a few different signatures of crashes</span>
<span class="p_add">+that are worth noting here.</span>
<span class="p_add">+</span>
<span class="p_add">+ * Crashes in early boot, especially around CPU bringup.  Bugs</span>
<span class="p_add">+   in the trampoline code or mappings cause these.</span>
<span class="p_add">+ * Crashes at the first interrupt.  Caused by bugs in entry_64.S,</span>
<span class="p_add">+   like screwing up a page table switch.  Also caused by</span>
<span class="p_add">+   incorrectly mapping the IRQ handler entry code.</span>
<span class="p_add">+ * Crashes at the first NMI.  The NMI code is separate from main</span>
<span class="p_add">+   interrupt handlers and can have bugs that do not affect</span>
<span class="p_add">+   normal interrupts.  Also caused by incorrectly mapping NMI</span>
<span class="p_add">+   code.  NMIs that interrupt the entry code must be very</span>
<span class="p_add">+   careful and can be the cause of crashes that show up when</span>
<span class="p_add">+   running perf.</span>
<span class="p_add">+ * Kernel crashes at the first exit to userspace.  entry_64.S</span>
<span class="p_add">+   bugs, or failing to map some of the exit code.</span>
<span class="p_add">+ * Crashes at the first interrupt that interrupts userspace. The paths</span>
<span class="p_add">+   in entry_64.S that return to userspace are sometimes separate</span>
<span class="p_add">+   from the ones that return to the kernel.</span>
<span class="p_add">+ * Double faults: overflowing the kernel stack because of page</span>
<span class="p_add">+   faults upon page faults.  Caused by touching non-kaiser-mapped</span>
<span class="p_add">+   data in the entry code, or forgetting to switch to kernel</span>
<span class="p_add">+   CR3 before calling into C functions which are not kaiser-mapped.</span>
<span class="p_add">+ * Failures of the selftests/x86 code.  Usually a bug in one of the</span>
<span class="p_add">+   more obscure corners of entry_64.S</span>
<span class="p_add">+ * Userspace segfaults early in boot, sometimes manifesting</span>
<span class="p_add">+   as mount(8) failing to mount the rootfs.  These have</span>
<span class="p_add">+   tended to be TLB invalidation issues.  Usually invalidating</span>
<span class="p_add">+   the wrong PCID, or otherwise missing an invalidation.</span>
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index e1650da01323..d087c3aa0514 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -2,6 +2,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/jump_label.h&gt;
 #include &lt;asm/unwind_hints.h&gt;
 #include &lt;asm/cpufeatures.h&gt;
<span class="p_add">+#include &lt;asm/page_types.h&gt;</span>
 
 /*
 
<span class="p_header">diff --git a/arch/x86/include/asm/kaiser.h b/arch/x86/include/asm/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..3c2cc71b4058</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,57 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_KAISER_H</span>
<span class="p_add">+#define _ASM_X86_KAISER_H</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Based on work published here: https://github.com/IAIK/KAISER</span>
<span class="p_add">+ * Modified by Dave Hansen &lt;dave.hansen@intel.com to actually work.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_add_mapping - map a kernel range into the user page tables</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ *  @flags: The mapping flags of the pages</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Use this on all data and code that need to be mapped into both</span>
<span class="p_add">+ *  copies of the page tables.  This includes the code that switches</span>
<span class="p_add">+ *  to/from userspace and all of the hardware structures that are</span>
<span class="p_add">+ *  virtually-addressed and needed in userspace like the interrupt</span>
<span class="p_add">+ *  table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+			      unsigned long flags);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_remove_mapping - remove a kernel mapping from the userpage tables</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_remove_mapping(unsigned long start, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_init - Initialize the shadow mapping</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Most parts of the shadow mapping can be mapped upon boot</span>
<span class="p_add">+ *  time.  Only per-process things like the thread stacks</span>
<span class="p_add">+ *  or a new LDT have to be mapped at runtime.  These boot-</span>
<span class="p_add">+ *  time mappings are permanent and never unmapped.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_KAISER_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index f735c3016325..d3901124143f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -1106,6 +1106,11 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
        memcpy(dst, src, count * sizeof(pgd_t));
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	/* Clone the shadow pgd part as well */</span>
<span class="p_add">+	memcpy(kernel_to_shadow_pgdp(dst), kernel_to_shadow_pgdp(src),</span>
<span class="p_add">+	       count * sizeof(pgd_t));</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index e9f05331e732..0c6e14f1e274 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -131,9 +131,138 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_clear(pud_t *xp)</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * All top-level KAISER page tables are order-1 pages (8k-aligned</span>
<span class="p_add">+ * and 8k in size).  The kernel one is at the beginning 4k and</span>
<span class="p_add">+ * the user (shadow) one is in the last 4k.  To switch between</span>
<span class="p_add">+ * them, you just need to flip the 12th bit in their addresses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KAISER_PGTABLE_SWITCH_BIT	PAGE_SHIFT</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This generates better code than the inline assembly in</span>
<span class="p_add">+ * __set_bit().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void *ptr_set_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr |= BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void *ptr_clear_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr &amp;= ~BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *kernel_to_shadow_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(pgdp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline pgd_t *shadow_to_kernel_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(pgdp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline p4d_t *kernel_to_shadow_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(p4dp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline p4d_t *shadow_to_kernel_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(p4dp, KAISER_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_KAISER */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true for parts of the PGD that map userspace and</span>
<span class="p_add">+ * false for the parts that map the kernel.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool pgdp_maps_userspace(void *__ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ptr = (unsigned long)__ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (ptr &amp; ~PAGE_MASK) &lt; (PAGE_SIZE / 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Does this PGD allow access from userspace?</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool pgd_userspace_access(pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pgd.pgd &amp; _PAGE_USER;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Take a PGD location (pgdp) and a pgd value that needs</span>
<span class="p_add">+ * to be set there.  Populates the shadow and returns</span>
<span class="p_add">+ * the resulting PGD that must be set in the kernel copy</span>
<span class="p_add">+ * of the page tables.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	if (pgd_userspace_access(pgd)) {</span>
<span class="p_add">+		if (pgdp_maps_userspace(pgdp)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * The user/shadow page tables get the full</span>
<span class="p_add">+			 * PGD, accessible from userspace:</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			kernel_to_shadow_pgdp(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For the copy of the pgd that the kernel</span>
<span class="p_add">+			 * uses, make it unusable to userspace.  This</span>
<span class="p_add">+			 * ensures if we get out to userspace with the</span>
<span class="p_add">+			 * wrong CR3 value, userspace will crash</span>
<span class="p_add">+			 * instead of running.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (pgd_userspace_access(*pgdp)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We are clearing a _PAGE_USER PGD for which we</span>
<span class="p_add">+		 * presumably populated the shadow.  We must now</span>
<span class="p_add">+		 * clear the shadow PGD entry.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (pgdp_maps_userspace(pgdp)) {</span>
<span class="p_add">+			kernel_to_shadow_pgdp(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Attempted to clear a _PAGE_USER PGD which</span>
<span class="p_add">+			 * is in the kernel porttion of the address</span>
<span class="p_add">+			 * space.  PGDs are pre-populated and we</span>
<span class="p_add">+			 * never clear them.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			WARN_ON_ONCE(1);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * _PAGE_USER was not set in either the PGD being set</span>
<span class="p_add">+		 * or cleared.  All kernel PGDs should be</span>
<span class="p_add">+		 * pre-populated so this should never happen after</span>
<span class="p_add">+		 * boot.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON_ONCE(system_state == SYSTEM_RUNNING);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	/* return the copy of the PGD we want the kernel to use: */</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 static inline void native_set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
<span class="p_add">+#if defined(CONFIG_KAISER) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	p4dp-&gt;pgd = kaiser_set_shadow_pgd(&amp;p4dp-&gt;pgd, p4d.pgd);</span>
<span class="p_add">+#else</span>
 	*p4dp = p4d;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_p4d_clear(p4d_t *p4d)
<span class="p_chunk">@@ -147,7 +276,11 @@</span> <span class="p_context"> static inline void native_p4d_clear(p4d_t *p4d)</span>
 
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	*pgdp = kaiser_set_shadow_pgd(pgdp, pgd);</span>
<span class="p_add">+#else</span>
 	*pgdp = pgd;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff --git a/arch/x86/kernel/espfix_64.c b/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">index 7d7715dde901..7b95cb47a3cf 100644</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/espfix.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * Note: we only need 6*8 = 48 bytes for the espfix stack, but round
<span class="p_chunk">@@ -129,6 +130,23 @@</span> <span class="p_context"> void __init init_espfix_bsp(void)</span>
 	p4d = p4d_alloc(&amp;init_mm, pgd, ESPFIX_BASE_ADDR);
 	p4d_populate(&amp;init_mm, p4d, espfix_pud_page);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Just copy the top-level PGD that is mapping the espfix</span>
<span class="p_add">+	 * area to ensure it is mapped into the shadow user page</span>
<span class="p_add">+	 * tables.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For 5-level paging, the espfix pgd was populated when</span>
<span class="p_add">+	 * kaiser_init() pre-populated all the pgd entries.  The above</span>
<span class="p_add">+	 * p4d_alloc() would never do anything and the p4d_populate()</span>
<span class="p_add">+	 * would be done to a p4d already mapped in the userspace pgd.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+	if (CONFIG_PGTABLE_LEVELS &lt;= 4) {</span>
<span class="p_add">+		set_pgd(kernel_to_shadow_pgdp(pgd),</span>
<span class="p_add">+			__pgd(_KERNPG_TABLE | (p4d_pfn(*p4d) &lt;&lt; PAGE_SHIFT)));</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* Randomize the locations */
 	init_espfix_random();
 
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 7dca675fe78d..43d1cffd1fcf 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -341,6 +341,14 @@</span> <span class="p_context"> GLOBAL(early_recursion_flag)</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -350,7 +358,7 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_top_pgt)</span>
 	.fill	511,8,0
 #ifdef CONFIG_X86_5LEVEL
 	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
<span class="p_chunk">@@ -364,7 +372,7 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #if defined(CONFIG_XEN_PV) || defined(CONFIG_XEN_PVH)
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
<span class="p_chunk">@@ -381,7 +389,7 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #else
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
 #endif
 
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 7ba7f3d7f477..1684e8891165 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -46,6 +46,7 @@</span> <span class="p_context"> obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o</span>
 obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o
 obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o
 obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o
<span class="p_add">+obj-$(CONFIG_KAISER)		+= kaiser.o</span>
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff --git a/arch/x86/mm/kaiser.c b/arch/x86/mm/kaiser.c</span>
new file mode 100644
<span class="p_header">index 000000000000..72dc15364390</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c</span>
<span class="p_chunk">@@ -0,0 +1,444 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This code is based in part on work published here:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	https://github.com/IAIK/KAISER</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The original work was written by and and signed off by for the Linux</span>
<span class="p_add">+ * kernel by:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   Signed-off-by: Richard Fellner &lt;richard.fellner@student.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Major changes to the original code by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define KAISER_WALK_ATOMIC  0x1</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * At runtime, the only things we map are some things for CPU</span>
<span class="p_add">+ * hotplug, and stacks for new processes.  No two CPUs will ever</span>
<span class="p_add">+ * be populating the same addresses, so we only need to ensure</span>
<span class="p_add">+ * that we protect between two CPUs trying to allocate and</span>
<span class="p_add">+ * populate the same page table page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only take this lock when doing a set_p[4um]d(), but it is not</span>
<span class="p_add">+ * needed for doing a set_pte().  We assume that only the *owner*</span>
<span class="p_add">+ * of a given allocation will be doing this for _their_</span>
<span class="p_add">+ * allocation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures that once a system has been running for a while</span>
<span class="p_add">+ * and there have been stacks all over and these page tables</span>
<span class="p_add">+ * are fully populated, there will be no further acquisitions of</span>
<span class="p_add">+ * this lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static DEFINE_SPINLOCK(shadow_table_allocation_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is only for walking kernel addresses.  We use it to help</span>
<span class="p_add">+ * recreate the &quot;shadow&quot; page tables which are used while we are in</span>
<span class="p_add">+ * userspace.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This can be called on any kernel memory addresses and will work</span>
<span class="p_add">+ * with any page sizes and any types: normal linear map memory,</span>
<span class="p_add">+ * vmalloc(), even kmap().</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: this is only used when mapping new *kernel* entries into</span>
<span class="p_add">+ * the user/shadow page tables.  It is never used for userspace</span>
<span class="p_add">+ * addresses.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns -1 on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long get_pa_from_kernel_map(unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We should only be asked to walk kernel addresses */</span>
<span class="p_add">+	if (vaddr &lt; PAGE_OFFSET) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(vaddr);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We made all the kernel PGDs present in kaiser_init().</span>
<span class="p_add">+	 * We expect them to stay that way.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PGDs are either 512GB or 128TB on all x86_64</span>
<span class="p_add">+	 * configurations.  We don&#39;t handle these.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, vaddr);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, vaddr);</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_large(*pud))</span>
<span class="p_add">+		return (pud_pfn(*pud) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PUD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, vaddr);</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_large(*pmd))</span>
<span class="p_add">+		return (pmd_pfn(*pmd) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PMD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, vaddr);</span>
<span class="p_add">+	if (pte_none(*pte)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pte_pfn(*pte) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PAGE_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the shadow copy of the page tables (optionally) trying to</span>
<span class="p_add">+ * allocate page table pages on the way down.  Does not support</span>
<span class="p_add">+ * large pages since the data we are mapping is (generally) not</span>
<span class="p_add">+ * large enough or aligned to 2MB.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: this is only used when mapping *new* kernel data into the</span>
<span class="p_add">+ * user/shadow page tables.  It is never used for userspace data.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *kaiser_shadow_pagetable_walk(unsigned long address,</span>
<span class="p_add">+					   unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pgd_t *pgd = kernel_to_shadow_pgdp(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; KAISER_WALK_ATOMIC) {</span>
<span class="p_add">+		gfp &amp;= ~GFP_KERNEL;</span>
<span class="p_add">+		gfp |= __GFP_HIGH | __GFP_ATOMIC;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (address &lt; PAGE_OFFSET) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk user address\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;All shadow pgds should have been populated\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_large(*p4d) != 0);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		unsigned long new_pud_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pud_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (p4d_none(*p4d))</span>
<span class="p_add">+			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pud_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pud_none(*pud))</span>
<span class="p_add">+			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pmd_none(*pmd))</span>
<span class="p_add">+			set_pmd(pmd, __pmd(_KERNPG_TABLE  | __pa(new_pte_page)));</span>
<span class="p_add">+		else</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, address);</span>
<span class="p_add">+	if (pte_flags(*pte) &amp; _PAGE_USER) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk to user pte\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Given a kernel address, @__start_addr, copy that mapping into</span>
<span class="p_add">+ * the user (shadow) page tables.  This may need to allocate page</span>
<span class="p_add">+ * table pages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int kaiser_add_user_map(const void *__start_addr, unsigned long size,</span>
<span class="p_add">+			unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	unsigned long start_addr = (unsigned long)__start_addr;</span>
<span class="p_add">+	unsigned long address = start_addr &amp; PAGE_MASK;</span>
<span class="p_add">+	unsigned long end_addr = PAGE_ALIGN(start_addr + size);</span>
<span class="p_add">+	unsigned long target_address;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; address &lt; end_addr; address += PAGE_SIZE) {</span>
<span class="p_add">+		target_address = get_pa_from_kernel_map(address);</span>
<span class="p_add">+		if (target_address == -1)</span>
<span class="p_add">+			return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = kaiser_shadow_pagetable_walk(address, false);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Errors come from either -ENOMEM for a page</span>
<span class="p_add">+		 * table page, or something screwy that did a</span>
<span class="p_add">+		 * WARN_ON().  Just return -ENOMEM.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pte)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		if (pte_none(*pte)) {</span>
<span class="p_add">+			set_pte(pte, __pte(flags | target_address));</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pte_t tmp;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Make a fake, temporary PTE that mimics the</span>
<span class="p_add">+			 * one we would have created.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_pte(&amp;tmp, __pte(flags | target_address));</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Warn if the pte that would have been</span>
<span class="p_add">+			 * created is different from the one that</span>
<span class="p_add">+			 * was there previously.  In other words,</span>
<span class="p_add">+			 * we allow the same PTE value to be set,</span>
<span class="p_add">+			 * but not changed.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			WARN_ON_ONCE(!pte_same(*pte, tmp));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_add_user_map_ptrs(const void *__start_addr,</span>
<span class="p_add">+			     const void *__end_addr,</span>
<span class="p_add">+			     unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kaiser_add_user_map(__start_addr,</span>
<span class="p_add">+				   __end_addr - __start_addr,</span>
<span class="p_add">+				   flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Ensure that the top level of the (shadow) page tables are</span>
<span class="p_add">+ * entirely populated.  This ensures that all processes that get</span>
<span class="p_add">+ * forked have the same entries.  This way, we do not have to</span>
<span class="p_add">+ * ever go set up new entries in older processes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: we never free these, so there are no updates to them</span>
<span class="p_add">+ * after this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init kaiser_init_all_pgds(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = kernel_to_shadow_pgdp(pgd_offset_k(0UL));</span>
<span class="p_add">+	for (i = PTRS_PER_PGD / 2; i &lt; PTRS_PER_PGD; i++) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Each PGD entry moves up PGDIR_SIZE bytes through</span>
<span class="p_add">+		 * the address space, so get the first virtual</span>
<span class="p_add">+		 * address mapped by PGD #i:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		unsigned long addr = i * PGDIR_SIZE;</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 4</span>
<span class="p_add">+		p4d_t *p4d = p4d_alloc_one(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!p4d) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, __pgd(_KERNPG_TABLE | __pa(p4d)));</span>
<span class="p_add">+#else /* CONFIG_PGTABLE_LEVELS &lt;= 4 */</span>
<span class="p_add">+		pud_t *pud = pud_alloc_one(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!pud) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, __pgd(_KERNPG_TABLE | __pa(pud)));</span>
<span class="p_add">+#endif /* CONFIG_PGTABLE_LEVELS */</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table allocations called by kaiser_add_user_map() can</span>
<span class="p_add">+ * theoretically fail, but are very unlikely to fail in early boot.</span>
<span class="p_add">+ * This would at least output a warning before crashing.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Do the checking and warning in a macro to make it more readable and</span>
<span class="p_add">+ * preserve line numbers in the warning message that you would not get</span>
<span class="p_add">+ * with an inline.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define kaiser_add_user_map_early(start, size, flags) do {	\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map(start, size, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_ptrs_early(start, end, flags) do {		\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map_ptrs(start, end, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * If anything in here fails, we will likely die on one of the</span>
<span class="p_add">+ * first kernel-&gt;user transitions and init will die.  But, we</span>
<span class="p_add">+ * will have most of the kernel up by then and should be able to</span>
<span class="p_add">+ * get a clean warning out of it.  If we BUG_ON() here, we run</span>
<span class="p_add">+ * the risk of being before we have good console output.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * When KAISER is enabled, we remove _PAGE_GLOBAL from all of the</span>
<span class="p_add">+ * kernel PTE permissions.  This ensures that the TLB entries for</span>
<span class="p_add">+ * the kernel are not available when in userspace.  However, for</span>
<span class="p_add">+ * the pages that are available to userspace *anyway*, we might as</span>
<span class="p_add">+ * well continue to map them _PAGE_GLOBAL and enjoy the potential</span>
<span class="p_add">+ * performance advantages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_init_all_pgds();</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		void *percpu_vaddr = __per_cpu_user_mapped_start +</span>
<span class="p_add">+				     per_cpu_offset(cpu);</span>
<span class="p_add">+		unsigned long percpu_sz = __per_cpu_user_mapped_end -</span>
<span class="p_add">+					  __per_cpu_user_mapped_start;</span>
<span class="p_add">+		kaiser_add_user_map_early(percpu_vaddr, percpu_sz,</span>
<span class="p_add">+					  __PAGE_KERNEL | _PAGE_GLOBAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__entry_text_start, __entry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX | _PAGE_GLOBAL);</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__irqentry_text_start, __irqentry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX | _PAGE_GLOBAL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* the fixed map address of the idt_table */</span>
<span class="p_add">+	kaiser_add_user_map_early((void *)idt_descr.address,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL_RO | _PAGE_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+		       unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return kaiser_add_user_map((const void *)addr, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* The shadow page tables always use small pages: */</span>
<span class="p_add">+	for (addr = start; addr &lt; start + size; addr += PAGE_SIZE) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Do an &quot;atomic&quot; walk in case this got called from an atomic</span>
<span class="p_add">+		 * context.  This should not do any allocations because we</span>
<span class="p_add">+		 * should only be walking things that are known to be mapped.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pte_t *pte = kaiser_shadow_pagetable_walk(addr, KAISER_WALK_ATOMIC);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We are removing a mapping that should</span>
<span class="p_add">+		 * exist.  WARN if it was not there:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			WARN_ON_ONCE(1);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_clear(&amp;init_mm, addr, pte);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This ensures that the TLB entries used to map this data are</span>
<span class="p_add">+	 * no longer usable on *this* CPU.  We theoretically want to</span>
<span class="p_add">+	 * flush the entries on all CPUs here, but that&#39;s too</span>
<span class="p_add">+	 * expensive right now: this is called to unmap process</span>
<span class="p_add">+	 * stacks in the exit() path.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This can change if we get to the point where this is not</span>
<span class="p_add">+	 * in a remotely hot path, like only called via write_ldt().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: we could probably also just invalidate the individual</span>
<span class="p_add">+	 * addresses to take care of *this* PCID and then do a</span>
<span class="p_add">+	 * tlb_flush_shared_nonglobals() to ensure that all other</span>
<span class="p_add">+	 * PCIDs get flushed before being used again.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__native_flush_tlb_global();</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index ffe584fa1f5e..1b3dbf3b3846 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -859,7 +859,7 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pud_range(p4d_t *p4d, unsigned long start, unsigned long end)</span>
<span class="p_add">+void unmap_pud_range(p4d_t *p4d, unsigned long start, unsigned long end)</span>
 {
 	pud_t *pud = pud_offset(p4d, start);
 
<span class="p_header">diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c</span>
<span class="p_header">index 17ebc5a978cc..1e47ce734404 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c</span>
<span class="p_chunk">@@ -355,14 +355,26 @@</span> <span class="p_context"> static inline void _pgd_free(pgd_t *pgd)</span>
 		kmem_cache_free(pgd_cache, pgd);
 }
 #else
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one pgd, we aquire two pgds.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline pgd_t *_pgd_alloc(void)
 {
<span class="p_del">-	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP, PGD_ALLOCATION_ORDER);</span>
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
 }
 #endif /* CONFIG_X86_PAE */
 
<span class="p_header">diff --git a/include/linux/kaiser.h b/include/linux/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0fd800efa95c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,29 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _INCLUDE_KAISER_H</span>
<span class="p_add">+#define _INCLUDE_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These stubs are used whenever CONFIG_KAISER is off, which</span>
<span class="p_add">+ * includes architectures that support KAISER, but have it</span>
<span class="p_add">+ * disabled.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int kaiser_add_mapping(unsigned long addr, unsigned long size,</span>
<span class="p_add">+				     unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !CONFIG_KAISER */</span>
<span class="p_add">+#endif /* _INCLUDE_KAISER_H */</span>
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 3bdd8da90f69..559bc0a6e9ad 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -76,6 +76,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/blkdev.h&gt;
 #include &lt;linux/elevator.h&gt;
 #include &lt;linux/sched_clock.h&gt;
<span class="p_chunk">@@ -505,6 +506,8 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	pgtable_init();
 	vmalloc_init();
 	ioremap_huge_init();
<span class="p_add">+	/* This just needs to be done before we first run userspace: */</span>
<span class="p_add">+	kaiser_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 07cc743698d3..685202058d65 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/tsacct_kern.h&gt;
 #include &lt;linux/cn_proc.h&gt;
 #include &lt;linux/freezer.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/delayacct.h&gt;
 #include &lt;linux/taskstats_kern.h&gt;
 #include &lt;linux/random.h&gt;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



