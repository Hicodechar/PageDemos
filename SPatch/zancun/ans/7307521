
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/1] mm: vmstat: Add OOM kill count in vmstat counter - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/1] mm: vmstat: Add OOM kill count in vmstat counter</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 1, 2015, 10:48 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1443696523-27262-1-git-send-email-pintu.k@samsung.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7307521/mbox/"
   >mbox</a>
|
   <a href="/patch/7307521/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7307521/">/patch/7307521/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 249CABEEA4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Oct 2015 11:03:30 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 28B79207C5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Oct 2015 11:03:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 17873207EF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Oct 2015 11:03:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756182AbbJALDZ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 1 Oct 2015 07:03:25 -0400
Received: from mailout2.samsung.com ([203.254.224.25]:35559 &quot;EHLO
	mailout2.samsung.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755242AbbJALDV (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 1 Oct 2015 07:03:21 -0400
Received: from epcpsbgr4.samsung.com
	(u144.gpu120.samsung.co.kr [203.254.230.144])
	by mailout2.samsung.com (Oracle Communications Messaging Server
	7.0.5.31.0 64bit (built May  5 2014))
	with ESMTP id &lt;0NVJ01NMZG1JV410@mailout2.samsung.com&gt; for
	linux-kernel@vger.kernel.org; Thu, 01 Oct 2015 20:03:19 +0900 (KST)
Received: from epcpsbgm1new.samsung.com ( [172.20.52.126])
	by epcpsbgr4.samsung.com (EPCPMTA) with SMTP id 7D.6D.05342.7F21D065;
	Thu, 1 Oct 2015 20:03:19 +0900 (KST)
X-AuditID: cbfee690-f794e6d0000014de-05-560d12f71fd8
Received: from epmmp1.local.host ( [203.254.227.16])
	by epcpsbgm1new.samsung.com (EPCPMTA) with SMTP id
	BF.12.23663.6F21D065; Thu, 1 Oct 2015 20:03:19 +0900 (KST)
Received: from pintu-ubuntu.sisodomain.com ([107.108.86.218])
	by mmp1.samsung.com
	(Oracle Communications Messaging Server 7.0.5.31.0 64bit (built May 5
	2014)) with ESMTPA id &lt;0NVJ005ZTFXLHN00@mmp1.samsung.com&gt;; Thu,
	01 Oct 2015 20:03:18 +0900 (KST)
From: Pintu Kumar &lt;pintu.k@samsung.com&gt;
To: akpm@linux-foundation.org, minchan@kernel.org, dave@stgolabs.net,
	pintu.k@samsung.com, mhocko@suse.cz, koct9i@gmail.com,
	rientjes@google.com, hannes@cmpxchg.org,
	penguin-kernel@i-love.sakura.ne.jp, bywxiaobai@163.com,
	mgorman@suse.de, vbabka@suse.cz, js1304@gmail.com,
	kirill.shutemov@linux.intel.com, alexander.h.duyck@redhat.com,
	sasha.levin@oracle.com, cl@linux.com, fengguang.wu@intel.com,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: cpgs@samsung.com, pintu_agarwal@yahoo.com, pintu.ping@gmail.com,
	vishnu.ps@samsung.com, rohit.kr@samsung.com,
	c.rajkumar@samsung.com, sreenathd@samsung.com
Subject: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter
Date: Thu, 01 Oct 2015 16:18:43 +0530
Message-id: &lt;1443696523-27262-1-git-send-email-pintu.k@samsung.com&gt;
X-Mailer: git-send-email 1.7.9.5
X-Brightmail-Tracker: H4sIAAAAAAAAAy2Sa0hTcRjG+59zdlFcnqbWX6OSgYmK5mXqPzJJQzh9SAzBD/XBlp6mqFM2
	lSTymuUFlul01VZIs1hz3qaFVopuBqYSkZZ5t7xM87JMdJmiba1vz8vze3kfeF42zh0l3NjJ
	okxaLBKk8pj2hNaZn+tr5nLi/Of/cJGySctEu3tyAkmVnqhcVoWhka0VgJb0Xqh+VAuQydiE
	o3rdRfSw8zEDjRqVBHpROsNAQ6+VTDSl3WegqrUFgH4UmRno+aaJhRS31xhIajaw0MhSDYG2
	VsdZ6E5dC4aMRcUEUvWO46h8tohAigIpQDLpJDh3lFKpZQzKsGLCqY5HkyyqVpdFdTRoMEr1
	dgmjdJpSJqX7Vcmi+h7sENTcsByjnry/RK3PjxGUqeszk5K2aQBVrcijBmt7WTFOl+3DEunU
	5GxafCr8qn3SzmoPM2PV9canyUE8HxS6lAE2G5J8+HLapwzYWeRh+HGqiVkG7NlcUg3g0EwF
	sBl82LU4htmMZwDe62/7TxVgsO9bK8tKMUlP+HtlB1gNZ7Ifh3JtF24dcFIF4N3tmn+UExkF
	35n6GVZNkB6wcWwXWHNwyPNwpSjSFskdKivDrKuQnGPD1le7uA0n4VaVnrAxx6CuG7elc4U9
	6q9EBThUCw5ogAudkZAhuSYU8/0kgjRJlkjol5CepgOWxgf2FirawVT3GT0g2YDnwIFChzgu
	Q5AtyUnTg2DLhfu4m0tCuuVJRJnxAUEhgSiYHxwUGHo6hHeEs+y2HcslhYJMOoWmM2hxvDgr
	lZboAca2c8sHFQGOoQvOMnPzcMHTidQGg+Py8dHoSN/qfPJ7invoxk3YdiWpLoJjFL4JKj9x
	K1+MtWu8fMMjNqILXU/+7As3xH7JW5p0vz7vv+fpHRecW9JsZxjcL143pXhc8HHcFAW2zObI
	pwNdEwccJ7rlCtEHdXO9dvFgpyKq8exUSUxWNI+QJAkCvHGxRPAXtSYY5+wCAAA=
X-Brightmail-Tracker: H4sIAAAAAAAAA+NgFlrOKsWRmVeSWpSXmKPExsVy+t9jAd3vQrxhBqc3s1jMWb+GzeLPv+ks
	Fn1z1C26p0xmsrj+7Q2jxctDmharb65htHj/fD2zxepNvhYz985ltbj5fA6LxcrOB6wWl3fN
	YbO4t+Y/q8Xkd88YLV41f2e1WPb1PbvF7JZ3rBZ93w+zW1x/OY3F4tvb2+wWbUs2Mlk8b25l
	sVh85DazRffjZhaL2Y19jBZT+u4yOkh7LF4xhdXj8Jv3zB47Z91l91iwqdRj59pVTB6L97xk
	8ti0qpPNY9OnSeweJ2b8ZvF4cmU6k8e8k4EeH5/eYvF4v+8qm0ffllWMHlNn13ucWXCEPUA4
	qoHRJiM1MSW1SCE1Lzk/JTMv3VbJOzjeOd7UzMBQ19DSwlxJIS8xN9VWycUnQNctMwcYOkoK
	ZYk5pUChgMTiYiV9O0wTQkPcdC1gGiN0fUOC4HqMDNBAwhrGjN9vD7IVvJWsuHT3DHMDY5No
	FyMnh4SAicS+F7eYIGwxiQv31rN1MXJxCAksZZToP7UFymlkkjjxcDM7SBWbgLrEjze/GUES
	IgKnmCWmr9nHDOIwCyxmlGj/OQ2sSljAVeLo+1OsIDaLgKrEult/gDo4OHgFnCXeNDuBmBIC
	ChJzJtlMYORewMiwilEitSC5oDgpPdcwL7Vcrzgxt7g0L10vOT93EyM4kTyT2sF4cJf7IUYB
	DkYlHt4DKTxhQqyJZcWVuYcYJTiYlUR4Zd4ChXhTEiurUovy44tKc1KLDzGaAq2fyCwlmpwP
	THJ5JfGGxibmpsamliYWJmaWSuK8Nw4xhAkJpCeWpGanphakFsH0MXFwSjUwbuqe+y1z1s3k
	uH+7GAPZ3f5eSD3AMzv5ss0FwYPeR9WDX1lkPHnEsPOjTI+b4ZxN2289q5k6yb5ZP5axYtbq
	qtTMcp2FR3fO+CqsofK3vsHgeqVFR/624jy7r2vvX1o6ke8KO9vMo/mKhfFWy2YfX/zybqvk
	XOk1Jt950u+6LhZ80y+oIaUppMRSnJFoqMVcVJwIAGbShwY6AwAA
DLP-Filter: Pass
X-MTR: 20000000000000000@CPGS
X-CFilter-Loop: Reflected
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 1, 2015, 10:48 a.m.</div>
<pre class="content">
This patch maintains number of oom calls and number of oom kill
count in /proc/vmstat.
It is helpful during sluggish, aging or long duration tests.
Currently if the OOM happens, it can be only seen in kernel ring buffer.
But during long duration tests, all the dmesg and /var/log/messages* could
be overwritten.
So, just like other counters, the oom can also be maintained in
/proc/vmstat.
It can be also seen if all logs are disabled in kernel.

A snapshot of the result of over night test is shown below:
$ cat /proc/vmstat
oom_stall 610
oom_kill_count 1763

Here, oom_stall indicates that there are 610 times, kernel entered into OOM
cases. However, there were around 1763 oom killing happens.
The OOM is bad for the any system. So, this counter can help the developer
in tuning the memory requirement at least during initial bringup.
<span class="signed-off-by">
Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
---
 include/linux/vm_event_item.h |    2 ++
 mm/oom_kill.c                 |    2 ++
 mm/page_alloc.c               |    2 +-
 mm/vmstat.c                   |    2 ++
 4 files changed, 7 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Oct. 1, 2015, 1:29 p.m.</div>
<pre class="content">
On 10/01/2015 04:18 PM, Pintu Kumar wrote:
<span class="quote">&gt; This patch maintains number of oom calls and number of oom kill</span>
<span class="quote">&gt; count in /proc/vmstat.</span>
<span class="quote">&gt; It is helpful during sluggish, aging or long duration tests.</span>
<span class="quote">&gt; Currently if the OOM happens, it can be only seen in kernel ring buffer.</span>
<span class="quote">&gt; But during long duration tests, all the dmesg and /var/log/messages* could</span>
<span class="quote">&gt; be overwritten.</span>
<span class="quote">&gt; So, just like other counters, the oom can also be maintained in</span>
<span class="quote">&gt; /proc/vmstat.</span>
<span class="quote">&gt; It can be also seen if all logs are disabled in kernel.</span>

Makes sense.
<span class="quote">
&gt; </span>
<span class="quote">&gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; oom_stall 610</span>
<span class="quote">&gt; oom_kill_count 1763</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here, oom_stall indicates that there are 610 times, kernel entered into OOM</span>
<span class="quote">&gt; cases. However, there were around 1763 oom killing happens.</span>
<span class="quote">&gt; The OOM is bad for the any system. So, this counter can help the developer</span>
<span class="quote">&gt; in tuning the memory requirement at least during initial bringup.</span>

Can you please fix the formatting of the commit message above ?
<span class="quote">
&gt; </span>
<span class="quote">&gt; Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/vm_event_item.h |    2 ++</span>
<span class="quote">&gt;  mm/oom_kill.c                 |    2 ++</span>
<span class="quote">&gt;  mm/page_alloc.c               |    2 +-</span>
<span class="quote">&gt;  mm/vmstat.c                   |    2 ++</span>
<span class="quote">&gt;  4 files changed, 7 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; index 2b1cef8..ade0851 100644</span>
<span class="quote">&gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; @@ -57,6 +57,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt;  		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +		OOM_STALL,</span>
<span class="quote">&gt; +		OOM_KILL_COUNT,</span>

Removing the COUNT will be better and in sync with others.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 1, 2015, 1:38 p.m.</div>
<pre class="content">
On Thu 01-10-15 16:18:43, Pintu Kumar wrote:
<span class="quote">&gt; This patch maintains number of oom calls and number of oom kill</span>
<span class="quote">&gt; count in /proc/vmstat.</span>
<span class="quote">&gt; It is helpful during sluggish, aging or long duration tests.</span>
<span class="quote">&gt; Currently if the OOM happens, it can be only seen in kernel ring buffer.</span>
<span class="quote">&gt; But during long duration tests, all the dmesg and /var/log/messages* could</span>
<span class="quote">&gt; be overwritten.</span>
<span class="quote">&gt; So, just like other counters, the oom can also be maintained in</span>
<span class="quote">&gt; /proc/vmstat.</span>
<span class="quote">&gt; It can be also seen if all logs are disabled in kernel.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; oom_stall 610</span>
<span class="quote">&gt; oom_kill_count 1763</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here, oom_stall indicates that there are 610 times, kernel entered into OOM</span>
<span class="quote">&gt; cases. However, there were around 1763 oom killing happens.</span>

This alone looks quite suspicious. Unless you have tasks which share the
address space without being in the same thread group this shouldn&#39;t
happen in such a large scale.
&lt;/me looks into the patch&gt;
And indeed the patch is incorrect. You are only counting OOMs from the
page allocator slow path. You are missing all the OOM invocations from
the page fault path.
The placement inside __alloc_pages_may_oom looks quite arbitrary as
well. You are not counting events where we are OOM but somebody is
holding the oom_mutex but you do count last attempt before going really
OOM. Then we have cases which do not invoke OOM killer which are counted
into oom_stall as well. I am not sure whether they should because I am
not quite sure about the semantic of the counter in the first place.
What is it supposed to tell us? How many times the system had to go into
emergency OOM steps? How many times the direct reclaim didn&#39;t make any
progress so we can consider the system OOM?

oom_kill_count has a slightly misleading names because it suggests how
many times oom_kill was called but in fact it counts the oom victims.
Not sure whether this information is so much useful but the semantic is
clear at least.
<span class="quote">
&gt; The OOM is bad for the any system. So, this counter can help the developer</span>
<span class="quote">&gt; in tuning the memory requirement at least during initial bringup.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/vm_event_item.h |    2 ++</span>
<span class="quote">&gt;  mm/oom_kill.c                 |    2 ++</span>
<span class="quote">&gt;  mm/page_alloc.c               |    2 +-</span>
<span class="quote">&gt;  mm/vmstat.c                   |    2 ++</span>
<span class="quote">&gt;  4 files changed, 7 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; index 2b1cef8..ade0851 100644</span>
<span class="quote">&gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; @@ -57,6 +57,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt;  		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +		OOM_STALL,</span>
<span class="quote">&gt; +		OOM_KILL_COUNT,</span>
<span class="quote">&gt;  		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */</span>
<span class="quote">&gt;  		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */</span>
<span class="quote">&gt;  		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */</span>
<span class="quote">&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; index 03b612b..e79caed 100644</span>
<span class="quote">&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; @@ -570,6 +570,7 @@ void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
<span class="quote">&gt;  	 * space under its control.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);</span>
<span class="quote">&gt; +	count_vm_event(OOM_KILL_COUNT);</span>
<span class="quote">&gt;  	mark_oom_victim(victim);</span>
<span class="quote">&gt;  	pr_err(&quot;Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\n&quot;,</span>
<span class="quote">&gt;  		task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),</span>
<span class="quote">&gt; @@ -600,6 +601,7 @@ void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
<span class="quote">&gt;  				task_pid_nr(p), p-&gt;comm);</span>
<span class="quote">&gt;  			task_unlock(p);</span>
<span class="quote">&gt;  			do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);</span>
<span class="quote">&gt; +			count_vm_event(OOM_KILL_COUNT);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	rcu_read_unlock();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 9bcfd70..1d82210 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -2761,7 +2761,7 @@ __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  		schedule_timeout_uninterruptible(1);</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	count_vm_event(OOM_STALL);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Go through the zonelist yet one more time, keep very high watermark</span>
<span class="quote">&gt;  	 * here, this is only to catch a parallel oom killing, we must fail if</span>
<span class="quote">&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 1fd0886..f054265 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -808,6 +808,8 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt;  	&quot;htlb_buddy_alloc_success&quot;,</span>
<span class="quote">&gt;  	&quot;htlb_buddy_alloc_fail&quot;,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +	&quot;oom_stall&quot;,</span>
<span class="quote">&gt; +	&quot;oom_kill_count&quot;,</span>
<span class="quote">&gt;  	&quot;unevictable_pgs_culled&quot;,</span>
<span class="quote">&gt;  	&quot;unevictable_pgs_scanned&quot;,</span>
<span class="quote">&gt;  	&quot;unevictable_pgs_rescued&quot;,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 1.7.9.5</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 5, 2015, 6:12 a.m.</div>
<pre class="content">
Hi,
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Michal Hocko [mailto:mhocko@kernel.org]</span>
<span class="quote">&gt; Sent: Thursday, October 01, 2015 7:09 PM</span>
<span class="quote">&gt; To: Pintu Kumar</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; koct9i@gmail.com; rientjes@google.com; hannes@cmpxchg.org; penguin-</span>
<span class="quote">&gt; kernel@i-love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de;</span>
<span class="quote">&gt; vbabka@suse.cz; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu 01-10-15 16:18:43, Pintu Kumar wrote:</span>
<span class="quote">&gt; &gt; This patch maintains number of oom calls and number of oom kill count</span>
<span class="quote">&gt; &gt; in /proc/vmstat.</span>
<span class="quote">&gt; &gt; It is helpful during sluggish, aging or long duration tests.</span>
<span class="quote">&gt; &gt; Currently if the OOM happens, it can be only seen in kernel ring buffer.</span>
<span class="quote">&gt; &gt; But during long duration tests, all the dmesg and /var/log/messages*</span>
<span class="quote">&gt; &gt; could be overwritten.</span>
<span class="quote">&gt; &gt; So, just like other counters, the oom can also be maintained in</span>
<span class="quote">&gt; &gt; /proc/vmstat.</span>
<span class="quote">&gt; &gt; It can be also seen if all logs are disabled in kernel.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; &gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; &gt; oom_stall 610</span>
<span class="quote">&gt; &gt; oom_kill_count 1763</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Here, oom_stall indicates that there are 610 times, kernel entered</span>
<span class="quote">&gt; &gt; into OOM cases. However, there were around 1763 oom killing happens.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This alone looks quite suspicious. Unless you have tasks which share the</span>
address
<span class="quote">&gt; space without being in the same thread group this shouldn&#39;t happen in such a</span>
<span class="quote">&gt; large scale.</span>

Yes, this accounts for out_of_memory even from memory cgroups.
Please check few snapshots of dmesg outputs captured during over-night tests.
........
[49479.078033]  [2:      xxxxxxxx:20874] Memory cgroup out of memory: Kill
process 20880 (xxxxxxx) score 112 or sacrifice child
[49480.910430]  [2:      xxxxxxxx:20882] Memory cgroup out of memory: Kill
process 20888 (xxxxxxxx) score 112 or sacrifice child
[49567.046203]  [0:        yyyyyyy:  548] Out of memory: Kill process 20458
(zzzzzzzzzz) score 102 or sacrifice child
[49567.346588]  [0:        yyyyyyy:  548] Out of memory: Kill process 21102
(zzzzzzzzzz) score 104 or sacrifice child
.........
The _out of memory_ count in dmesg dump output exactly matches the number in
/proc/vmstat -&gt; oom_kill_count
<span class="quote">
&gt; &lt;/me looks into the patch&gt;</span>
<span class="quote">&gt; And indeed the patch is incorrect. You are only counting OOMs from the page</span>
<span class="quote">&gt; allocator slow path. You are missing all the OOM invocations from the page</span>
fault
<span class="quote">&gt; path.</span>

Sorry, I am not sure what exactly you mean. Please point me out if I am missing
some places.
Actually, I tried to add it at generic place that is; oom_kill_process, which is
called by out_of_memory(...).
Are you talking about: pagefault_out_of_memory(...) ?
But, this is already calling: out_of_memory. No?
<span class="quote">
&gt; The placement inside __alloc_pages_may_oom looks quite arbitrary as well. You</span>
<span class="quote">&gt; are not counting events where we are OOM but somebody is holding the</span>
<span class="quote">&gt; oom_mutex but you do count last attempt before going really OOM. Then we</span>
<span class="quote">&gt; have cases which do not invoke OOM killer which are counted into oom_stall as</span>
<span class="quote">&gt; well. I am not sure whether they should because I am not quite sure about the</span>
<span class="quote">&gt; semantic of the counter in the first place.</span>

Ok. Yes, it can be added right after it enters into __alloc_pages_may_oom.
I will make the changes.
Actually, I knowingly skipped the oom_lock case, because in our 3.10 kernel, we
had note_oom_kill(..) 
Added right after this check.
So, I also added it exactly at the same place.
Ok, I can make the necessary changes, if the oom_lock case also matters. 
<span class="quote">
&gt; What is it supposed to tell us? How many times the system had to go into</span>
<span class="quote">&gt; emergency OOM steps? How many times the direct reclaim didn&#39;t make any</span>
<span class="quote">&gt; progress so we can consider the system OOM?</span>
<span class="quote">&gt; </span>
Yes, exactly, oom_stall can tell, how many times OOM is invoked in the system.
Yes, it can also tell how many times direct_reclaim fails completely.
Currently, we don&#39;t have any counter for direct_reclaim success/fail.
Also, oom_kill_process will not be invoked for higher orders
(PAGE_ALLOC_COSTLY_ORDER).
But, it will enter OOM and results into straight page allocation failure.
<span class="quote">
&gt; oom_kill_count has a slightly misleading names because it suggests how many</span>
<span class="quote">&gt; times oom_kill was called but in fact it counts the oom victims.</span>
<span class="quote">&gt; Not sure whether this information is so much useful but the semantic is clear</span>
at
<span class="quote">&gt; least.</span>
<span class="quote">&gt; </span>
Ok, agree about the semantic of the name: oom_kill_count.
If possible please suggest a better name.
How about the following names?
oom_victim_count ?
oom_nr_killed ?
oom_nr_victim ?
<span class="quote">
&gt; &gt; The OOM is bad for the any system. So, this counter can help the</span>
<span class="quote">&gt; &gt; developer in tuning the memory requirement at least during initial bringup.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/vm_event_item.h |    2 ++</span>
<span class="quote">&gt; &gt;  mm/oom_kill.c                 |    2 ++</span>
<span class="quote">&gt; &gt;  mm/page_alloc.c               |    2 +-</span>
<span class="quote">&gt; &gt;  mm/vmstat.c                   |    2 ++</span>
<span class="quote">&gt; &gt;  4 files changed, 7 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; b/include/linux/vm_event_item.h index 2b1cef8..ade0851 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; @@ -57,6 +57,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN,</span>
<span class="quote">&gt; &gt; PSWPOUT,  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt; &gt;  		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,  #endif</span>
<span class="quote">&gt; &gt; +		OOM_STALL,</span>
<span class="quote">&gt; &gt; +		OOM_KILL_COUNT,</span>
<span class="quote">&gt; &gt;  		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */</span>
<span class="quote">&gt; &gt;  		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */</span>
<span class="quote">&gt; &gt;  		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */</span>
<span class="quote">&gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c index 03b612b..e79caed</span>
<span class="quote">&gt; &gt; 100644</span>
<span class="quote">&gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; @@ -570,6 +570,7 @@ void oom_kill_process(struct oom_control *oc, struct</span>
<span class="quote">&gt; task_struct *p,</span>
<span class="quote">&gt; &gt;  	 * space under its control.</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt;  	do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);</span>
<span class="quote">&gt; &gt; +	count_vm_event(OOM_KILL_COUNT);</span>
<span class="quote">&gt; &gt;  	mark_oom_victim(victim);</span>
<span class="quote">&gt; &gt;  	pr_err(&quot;Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-</span>
<span class="quote">&gt; rss:%lukB\n&quot;,</span>
<span class="quote">&gt; &gt;  		task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),</span>
<span class="quote">&gt; @@</span>
<span class="quote">&gt; &gt; -600,6 +601,7 @@ void oom_kill_process(struct oom_control *oc, struct</span>
<span class="quote">&gt; task_struct *p,</span>
<span class="quote">&gt; &gt;  				task_pid_nr(p), p-&gt;comm);</span>
<span class="quote">&gt; &gt;  			task_unlock(p);</span>
<span class="quote">&gt; &gt;  			do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);</span>
<span class="quote">&gt; &gt; +			count_vm_event(OOM_KILL_COUNT);</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  	rcu_read_unlock();</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c index 9bcfd70..1d82210</span>
<span class="quote">&gt; &gt; 100644</span>
<span class="quote">&gt; &gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; @@ -2761,7 +2761,7 @@ __alloc_pages_may_oom(gfp_t gfp_mask, unsigned</span>
<span class="quote">&gt; int order,</span>
<span class="quote">&gt; &gt;  		schedule_timeout_uninterruptible(1);</span>
<span class="quote">&gt; &gt;  		return NULL;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; +	count_vm_event(OOM_STALL);</span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * Go through the zonelist yet one more time, keep very high watermark</span>
<span class="quote">&gt; &gt;  	 * here, this is only to catch a parallel oom killing, we must fail</span>
<span class="quote">&gt; &gt; if diff --git a/mm/vmstat.c b/mm/vmstat.c index 1fd0886..f054265</span>
<span class="quote">&gt; &gt; 100644</span>
<span class="quote">&gt; &gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; &gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; &gt; @@ -808,6 +808,8 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt; &gt;  	&quot;htlb_buddy_alloc_success&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;htlb_buddy_alloc_fail&quot;,</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; +	&quot;oom_stall&quot;,</span>
<span class="quote">&gt; &gt; +	&quot;oom_kill_count&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;unevictable_pgs_culled&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;unevictable_pgs_scanned&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;unevictable_pgs_rescued&quot;,</span>
<span class="quote">&gt; &gt; --</span>
<span class="quote">&gt; &gt; 1.7.9.5</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 5, 2015, 6:19 a.m.</div>
<pre class="content">
Hi,
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Anshuman Khandual [mailto:khandual@linux.vnet.ibm.com]</span>
<span class="quote">&gt; Sent: Thursday, October 01, 2015 7:00 PM</span>
<span class="quote">&gt; To: Pintu Kumar; akpm@linux-foundation.org; minchan@kernel.org;</span>
<span class="quote">&gt; dave@stgolabs.net; mhocko@suse.cz; koct9i@gmail.com; rientjes@google.com;</span>
<span class="quote">&gt; hannes@cmpxchg.org; penguin-kernel@i-love.sakura.ne.jp;</span>
<span class="quote">&gt; bywxiaobai@163.com; mgorman@suse.de; vbabka@suse.cz; js1304@gmail.com;</span>
<span class="quote">&gt; kirill.shutemov@linux.intel.com; alexander.h.duyck@redhat.com;</span>
<span class="quote">&gt; sasha.levin@oracle.com; cl@linux.com; fengguang.wu@intel.com; linux-</span>
<span class="quote">&gt; kernel@vger.kernel.org; linux-mm@kvack.org</span>
<span class="quote">&gt; Cc: cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 10/01/2015 04:18 PM, Pintu Kumar wrote:</span>
<span class="quote">&gt; &gt; This patch maintains number of oom calls and number of oom kill count</span>
<span class="quote">&gt; &gt; in /proc/vmstat.</span>
<span class="quote">&gt; &gt; It is helpful during sluggish, aging or long duration tests.</span>
<span class="quote">&gt; &gt; Currently if the OOM happens, it can be only seen in kernel ring buffer.</span>
<span class="quote">&gt; &gt; But during long duration tests, all the dmesg and /var/log/messages*</span>
<span class="quote">&gt; &gt; could be overwritten.</span>
<span class="quote">&gt; &gt; So, just like other counters, the oom can also be maintained in</span>
<span class="quote">&gt; &gt; /proc/vmstat.</span>
<span class="quote">&gt; &gt; It can be also seen if all logs are disabled in kernel.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Makes sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; &gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; &gt; oom_stall 610</span>
<span class="quote">&gt; &gt; oom_kill_count 1763</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Here, oom_stall indicates that there are 610 times, kernel entered</span>
<span class="quote">&gt; &gt; into OOM cases. However, there were around 1763 oom killing happens.</span>
<span class="quote">&gt; &gt; The OOM is bad for the any system. So, this counter can help the</span>
<span class="quote">&gt; &gt; developer in tuning the memory requirement at least during initial bringup.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you please fix the formatting of the commit message above ?</span>
<span class="quote">&gt; </span>
Not sure if there is any formatting issue here. I cannot see it.
The checkpatch returns no error/warnings.
Please point me out exactly, if there is any issue.
<span class="quote">
&gt; &gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/vm_event_item.h |    2 ++</span>
<span class="quote">&gt; &gt;  mm/oom_kill.c                 |    2 ++</span>
<span class="quote">&gt; &gt;  mm/page_alloc.c               |    2 +-</span>
<span class="quote">&gt; &gt;  mm/vmstat.c                   |    2 ++</span>
<span class="quote">&gt; &gt;  4 files changed, 7 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; b/include/linux/vm_event_item.h index 2b1cef8..ade0851 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; @@ -57,6 +57,8 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN,</span>
<span class="quote">&gt; &gt; PSWPOUT,  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt; &gt;  		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,  #endif</span>
<span class="quote">&gt; &gt; +		OOM_STALL,</span>
<span class="quote">&gt; &gt; +		OOM_KILL_COUNT,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Removing the COUNT will be better and in sync with others.</span>

Ok, even suggested by Michal Hocko and being discussed in another thread.


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 5, 2015, 12:22 p.m.</div>
<pre class="content">
On Mon 05-10-15 11:42:49, PINTU KUMAR wrote:
[...]
<span class="quote">&gt; &gt; &gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; &gt; &gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; &gt; &gt; oom_stall 610</span>
<span class="quote">&gt; &gt; &gt; oom_kill_count 1763</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Here, oom_stall indicates that there are 610 times, kernel entered</span>
<span class="quote">&gt; &gt; &gt; into OOM cases. However, there were around 1763 oom killing happens.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This alone looks quite suspicious. Unless you have tasks which share the</span>
<span class="quote">&gt; &gt; address</span>
<span class="quote">&gt; &gt; space without being in the same thread group this shouldn&#39;t happen in such a</span>
<span class="quote">&gt; &gt; large scale.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, this accounts for out_of_memory even from memory cgroups.</span>
<span class="quote">&gt; Please check few snapshots of dmesg outputs captured during over-night tests.</span>

OK, that would explain why the second counter is so much larger than
oom_stall. And that alone should have been a red flag IMO. Why should be
memcg OOM killer events accounted together with the global? How do you
distinguish the two?
<span class="quote">
&gt; ........</span>
<span class="quote">&gt; [49479.078033]  [2:      xxxxxxxx:20874] Memory cgroup out of memory: Kill</span>
<span class="quote">&gt; process 20880 (xxxxxxx) score 112 or sacrifice child</span>
<span class="quote">&gt; [49480.910430]  [2:      xxxxxxxx:20882] Memory cgroup out of memory: Kill</span>
<span class="quote">&gt; process 20888 (xxxxxxxx) score 112 or sacrifice child</span>
<span class="quote">&gt; [49567.046203]  [0:        yyyyyyy:  548] Out of memory: Kill process 20458</span>
<span class="quote">&gt; (zzzzzzzzzz) score 102 or sacrifice child</span>
<span class="quote">&gt; [49567.346588]  [0:        yyyyyyy:  548] Out of memory: Kill process 21102</span>
<span class="quote">&gt; (zzzzzzzzzz) score 104 or sacrifice child</span>
<span class="quote">&gt; .........</span>
<span class="quote">&gt; The _out of memory_ count in dmesg dump output exactly matches the number in</span>
<span class="quote">&gt; /proc/vmstat -&gt; oom_kill_count</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &lt;/me looks into the patch&gt;</span>
<span class="quote">&gt; &gt; And indeed the patch is incorrect. You are only counting OOMs from the page</span>
<span class="quote">&gt; &gt; allocator slow path. You are missing all the OOM invocations from the page</span>
<span class="quote">&gt; &gt; fault</span>
<span class="quote">&gt; &gt; path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, I am not sure what exactly you mean. Please point me out if I am missing</span>
<span class="quote">&gt; some places.</span>
<span class="quote">&gt; Actually, I tried to add it at generic place that is; oom_kill_process, which is</span>
<span class="quote">&gt; called by out_of_memory(...).</span>
<span class="quote">&gt; Are you talking about: pagefault_out_of_memory(...) ?</span>
<span class="quote">&gt; But, this is already calling: out_of_memory. No?</span>

Sorry, I wasn&#39;t clear enough here. I was talking about oom_stall counter
here not oom_kill_count one.

[...]
<span class="quote">&gt; &gt; What is it supposed to tell us? How many times the system had to go into</span>
<span class="quote">&gt; &gt; emergency OOM steps? How many times the direct reclaim didn&#39;t make any</span>
<span class="quote">&gt; &gt; progress so we can consider the system OOM?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Yes, exactly, oom_stall can tell, how many times OOM is invoked in the system.</span>
<span class="quote">&gt; Yes, it can also tell how many times direct_reclaim fails completely.</span>
<span class="quote">&gt; Currently, we don&#39;t have any counter for direct_reclaim success/fail.</span>

So why don&#39;t we add one? Direct reclaim failure is a clearly defined
event and it also can be evaluated reasonably against allocstall.
<span class="quote">
&gt; Also, oom_kill_process will not be invoked for higher orders</span>
<span class="quote">&gt; (PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="quote">&gt; But, it will enter OOM and results into straight page allocation failure.</span>

Yes there are other reasons to not invoke OOM killer or to prevent
actual killing if chances are high we can go without it. This is the
reason I am asking about the exact semantic.
<span class="quote">
&gt; &gt; oom_kill_count has a slightly misleading names because it suggests how many</span>
<span class="quote">&gt; &gt; times oom_kill was called but in fact it counts the oom victims.</span>
<span class="quote">&gt; &gt; Not sure whether this information is so much useful but the semantic is clear</span>
<span class="quote">&gt; &gt; at least.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Ok, agree about the semantic of the name: oom_kill_count.</span>
<span class="quote">&gt; If possible please suggest a better name.</span>
<span class="quote">&gt; How about the following names?</span>
<span class="quote">&gt; oom_victim_count ?</span>
<span class="quote">&gt; oom_nr_killed ?</span>
<span class="quote">&gt; oom_nr_victim ?</span>

nr_oom_victims?

I am still not sure how useful this counter would be, though. Sure the
log ringbuffer might overflow (the risk can be reduced by reducing the
loglevel) but how much it would help to know that we had additional N
OOM victims? From my experience checking the OOM reports which are still
in the logbuffer are sufficient to see whether there is a memory leak,
pinned memory or a continuous memory pressure. Your experience might be
different so it would be nice to mention that in the changelog.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 6, 2015, 6:59 a.m.</div>
<pre class="content">
Hi,
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Michal Hocko [mailto:mhocko@kernel.org]</span>
<span class="quote">&gt; Sent: Monday, October 05, 2015 5:53 PM</span>
<span class="quote">&gt; To: PINTU KUMAR</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; koct9i@gmail.com; rientjes@google.com; hannes@cmpxchg.org; penguin-</span>
<span class="quote">&gt; kernel@i-love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de;</span>
<span class="quote">&gt; vbabka@suse.cz; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon 05-10-15 11:42:49, PINTU KUMAR wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; A snapshot of the result of over night test is shown below:</span>
<span class="quote">&gt; &gt; &gt; &gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; &gt; &gt; &gt; oom_stall 610</span>
<span class="quote">&gt; &gt; &gt; &gt; oom_kill_count 1763</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Here, oom_stall indicates that there are 610 times, kernel entered</span>
<span class="quote">&gt; &gt; &gt; &gt; into OOM cases. However, there were around 1763 oom killing happens.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This alone looks quite suspicious. Unless you have tasks which share</span>
<span class="quote">&gt; &gt; &gt; the address space without being in the same thread group this</span>
<span class="quote">&gt; &gt; &gt; shouldn&#39;t happen in such a large scale.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yes, this accounts for out_of_memory even from memory cgroups.</span>
<span class="quote">&gt; &gt; Please check few snapshots of dmesg outputs captured during over-night</span>
tests.
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, that would explain why the second counter is so much larger than</span>
oom_stall.
<span class="quote">&gt; And that alone should have been a red flag IMO. Why should be memcg OOM</span>
<span class="quote">&gt; killer events accounted together with the global? How do you distinguish the</span>
<span class="quote">&gt; two?</span>
<span class="quote">&gt; </span>
Actually, here, we are just interested in knowing oom_kill. Let it be either
global, memcg or others.
Once we know there are oom kill happening, we can easily find it by enabling
logs.
Normally in production system, all system logs will be disabled.
<span class="quote">
&gt; &gt; ........</span>
<span class="quote">&gt; &gt; [49479.078033]  [2:      xxxxxxxx:20874] Memory cgroup out of memory: Kill</span>
<span class="quote">&gt; &gt; process 20880 (xxxxxxx) score 112 or sacrifice child</span>
<span class="quote">&gt; &gt; [49480.910430]  [2:      xxxxxxxx:20882] Memory cgroup out of memory: Kill</span>
<span class="quote">&gt; &gt; process 20888 (xxxxxxxx) score 112 or sacrifice child</span>
<span class="quote">&gt; &gt; [49567.046203]  [0:        yyyyyyy:  548] Out of memory: Kill process 20458</span>
<span class="quote">&gt; &gt; (zzzzzzzzzz) score 102 or sacrifice child</span>
<span class="quote">&gt; &gt; [49567.346588]  [0:        yyyyyyy:  548] Out of memory: Kill process 21102</span>
<span class="quote">&gt; &gt; (zzzzzzzzzz) score 104 or sacrifice child .........</span>
<span class="quote">&gt; &gt; The _out of memory_ count in dmesg dump output exactly matches the</span>
<span class="quote">&gt; &gt; number in /proc/vmstat -&gt; oom_kill_count</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &lt;/me looks into the patch&gt;</span>
<span class="quote">&gt; &gt; &gt; And indeed the patch is incorrect. You are only counting OOMs from</span>
<span class="quote">&gt; &gt; &gt; the page allocator slow path. You are missing all the OOM</span>
<span class="quote">&gt; &gt; &gt; invocations from the page fault path.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Sorry, I am not sure what exactly you mean. Please point me out if I</span>
<span class="quote">&gt; &gt; am missing some places.</span>
<span class="quote">&gt; &gt; Actually, I tried to add it at generic place that is;</span>
<span class="quote">&gt; &gt; oom_kill_process, which is called by out_of_memory(...).</span>
<span class="quote">&gt; &gt; Are you talking about: pagefault_out_of_memory(...) ?</span>
<span class="quote">&gt; &gt; But, this is already calling: out_of_memory. No?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, I wasn&#39;t clear enough here. I was talking about oom_stall counter here</span>
not
<span class="quote">&gt; oom_kill_count one.</span>
<span class="quote">&gt; </span>
Ok, I got your point.
Oom_kill_process, is called from 2 places:
1) out_of_memory
2) mem_cgroup_out_of_memory

And, out_of_memory is actually called from 3 places:
1) alloc_pages_may_oom
2) pagefault_out_of_memory
3) moom_callback (sysirq.c)

Thus, in this case, the oom_stall counter can be added in 4 places (in the
beginning).
1) alloc_pages_may_oom
2) mem_cgroup_out_of_memory
3) pagefault_out_of_memory
4) moom_callback (sysirq.c)

For, case {2,3,4}, we could have actually called at one place in out_of_memory,
But this result into calling it 2 times because alloc_pages_may_oom also call
out_of_memory.
If there is any better idea, please let me know.
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; What is it supposed to tell us? How many times the system had to go</span>
<span class="quote">&gt; &gt; &gt; into emergency OOM steps? How many times the direct reclaim didn&#39;t</span>
<span class="quote">&gt; &gt; &gt; make any progress so we can consider the system OOM?</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Yes, exactly, oom_stall can tell, how many times OOM is invoked in the</span>
system.
<span class="quote">&gt; &gt; Yes, it can also tell how many times direct_reclaim fails completely.</span>
<span class="quote">&gt; &gt; Currently, we don&#39;t have any counter for direct_reclaim success/fail.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So why don&#39;t we add one? Direct reclaim failure is a clearly defined event and</span>
it
<span class="quote">&gt; also can be evaluated reasonably against allocstall.</span>
<span class="quote">&gt; </span>
Yes, direct_reclaim success/fail is also planned ahead.
May be something like:
direct_reclaim_alloc_success
direct_reclaim_alloc_fail

But, then I thought oom_kill is more important than this. So I pushed this one
first.
<span class="quote">
&gt; &gt; Also, oom_kill_process will not be invoked for higher orders</span>
<span class="quote">&gt; &gt; (PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="quote">&gt; &gt; But, it will enter OOM and results into straight page allocation failure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes there are other reasons to not invoke OOM killer or to prevent actual</span>
killing
<span class="quote">&gt; if chances are high we can go without it. This is the reason I am asking about</span>
the
<span class="quote">&gt; exact semantic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; oom_kill_count has a slightly misleading names because it suggests</span>
<span class="quote">&gt; &gt; &gt; how many times oom_kill was called but in fact it counts the oom victims.</span>
<span class="quote">&gt; &gt; &gt; Not sure whether this information is so much useful but the semantic</span>
<span class="quote">&gt; &gt; &gt; is clear at least.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Ok, agree about the semantic of the name: oom_kill_count.</span>
<span class="quote">&gt; &gt; If possible please suggest a better name.</span>
<span class="quote">&gt; &gt; How about the following names?</span>
<span class="quote">&gt; &gt; oom_victim_count ?</span>
<span class="quote">&gt; &gt; oom_nr_killed ?</span>
<span class="quote">&gt; &gt; oom_nr_victim ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; nr_oom_victims?</span>
<span class="quote">&gt; </span>
Ok, nr_oom_victims is also nice name. If all agree I can change this name.
Please confirm.
<span class="quote">
&gt; I am still not sure how useful this counter would be, though. Sure the log</span>
<span class="quote">&gt; ringbuffer might overflow (the risk can be reduced by reducing the</span>
<span class="quote">&gt; loglevel) but how much it would help to know that we had additional N OOM</span>
<span class="quote">&gt; victims? From my experience checking the OOM reports which are still in the</span>
<span class="quote">&gt; logbuffer are sufficient to see whether there is a memory leak, pinned memory</span>
<span class="quote">&gt; or a continuous memory pressure. Your experience might be different so it</span>
<span class="quote">&gt; would be nice to mention that in the changelog.</span>

Ok. 
As I said earlier, normally all logs will be disabled in production system.
But, we can access /proc/vmstat. The oom would have happened in the system
Earlier, but the logs would have over-written.
The /proc/vmstat is the only counter which can tell, if ever system entered into
oom cases.
Once we know for sure that oom happened in the system, then we can enable all
logs in the system to reproduce the oom scenarios to analyze further.
Also it can help in initial tuning of the system for the memory needs of the
system.
In embedded world, we normally try to avoid the system to enter into kernel OOM
as far as possible.
For example, in Android, we have LMK (low memory killer) driver that controls
the OOM behavior. But most of the time these LMK threshold are statically
controlled.
Now with this oom counter we can dynamically control the LMK behavior.
For example, in LMK we can check, if ever oom_stall becomes 1, that means system
is hitting OOM state. At this stage we can immediately trigger the OOM killing
from user space or LMK driver.
Similar user case and requirement is there for Tizen that controls OOM from user
space (without LMK).
It can also trigger the thought for sluggish behavior in the system during long
run.
These are just few use cases. More can be thought of.
<span class="quote">
&gt; --</span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 6, 2015, 3:41 p.m.</div>
<pre class="content">
On Tue 06-10-15 12:29:52, PINTU KUMAR wrote:
[...]
<span class="quote">&gt; &gt; OK, that would explain why the second counter is so much larger than</span>
<span class="quote">&gt; &gt; oom_stall.</span>
<span class="quote">&gt; &gt; And that alone should have been a red flag IMO. Why should be memcg OOM</span>
<span class="quote">&gt; &gt; killer events accounted together with the global? How do you distinguish the</span>
<span class="quote">&gt; &gt; two?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Actually, here, we are just interested in knowing oom_kill. Let it be either</span>
<span class="quote">&gt; global, memcg or others.</span>
<span class="quote">&gt; Once we know there are oom kill happening, we can easily find it by enabling</span>
<span class="quote">&gt; logs.</span>
<span class="quote">&gt; Normally in production system, all system logs will be disabled.</span>

This doesn&#39;t make much sense to me. So you find out that _an oom killer_
was invoked but you have logs disabled. What now? You can hardly find
out what has happened and why it has happened. What is the point then?
Wait for another one to come? This might be never.

What is even more confusing is the mixing of memcg and global oom
conditions. They are really different things. Memcg API will even give
you notification about the OOM event.

[...]
<span class="quote">&gt; &gt; Sorry, I wasn&#39;t clear enough here. I was talking about oom_stall counter here</span>
<span class="quote">&gt; &gt; not</span>
<span class="quote">&gt; &gt; oom_kill_count one.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Ok, I got your point.</span>
<span class="quote">&gt; Oom_kill_process, is called from 2 places:</span>
<span class="quote">&gt; 1) out_of_memory</span>
<span class="quote">&gt; 2) mem_cgroup_out_of_memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And, out_of_memory is actually called from 3 places:</span>
<span class="quote">&gt; 1) alloc_pages_may_oom</span>
<span class="quote">&gt; 2) pagefault_out_of_memory</span>
<span class="quote">&gt; 3) moom_callback (sysirq.c)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thus, in this case, the oom_stall counter can be added in 4 places (in the</span>
<span class="quote">&gt; beginning).</span>
<span class="quote">&gt; 1) alloc_pages_may_oom</span>
<span class="quote">&gt; 2) mem_cgroup_out_of_memory</span>
<span class="quote">&gt; 3) pagefault_out_of_memory</span>
<span class="quote">&gt; 4) moom_callback (sysirq.c)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For, case {2,3,4}, we could have actually called at one place in out_of_memory,</span>

Why would you even consider 4 for oom_stall? This is an administrator
order to kill a memory hog. The system might be in a good shape just the
memory hog is misbehaving. I realize this is not a usual usecase but if
oom_stall is supposed to measure a memory pressure of some sort then
binding it to a user action is wrong thing to do.
<span class="quote">
&gt; But this result into calling it 2 times because alloc_pages_may_oom also call</span>
<span class="quote">&gt; out_of_memory.</span>
<span class="quote">&gt; If there is any better idea, please let me know.</span>

I think you are focusing too much on the implementation before you are
clear in what should be the desired semantic.
<span class="quote">
&gt; &gt; &gt; &gt; What is it supposed to tell us? How many times the system had to go</span>
<span class="quote">&gt; &gt; &gt; &gt; into emergency OOM steps? How many times the direct reclaim didn&#39;t</span>
<span class="quote">&gt; &gt; &gt; &gt; make any progress so we can consider the system OOM?</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Yes, exactly, oom_stall can tell, how many times OOM is invoked in the</span>
<span class="quote">&gt; &gt; &gt; system.</span>
<span class="quote">&gt; &gt; &gt; Yes, it can also tell how many times direct_reclaim fails completely.</span>
<span class="quote">&gt; &gt; &gt; Currently, we don&#39;t have any counter for direct_reclaim success/fail.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So why don&#39;t we add one? Direct reclaim failure is a clearly defined event and</span>
<span class="quote">&gt; &gt; it</span>
<span class="quote">&gt; &gt; also can be evaluated reasonably against allocstall.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Yes, direct_reclaim success/fail is also planned ahead.</span>
<span class="quote">&gt; May be something like:</span>
<span class="quote">&gt; direct_reclaim_alloc_success</span>
<span class="quote">&gt; direct_reclaim_alloc_fail</span>

We already have alloc_stall so all_stall_noprogress or whatever better
name should be sufficient.

[...]
<span class="quote">
&gt; &gt; I am still not sure how useful this counter would be, though. Sure the log</span>
<span class="quote">&gt; &gt; ringbuffer might overflow (the risk can be reduced by reducing the</span>
<span class="quote">&gt; &gt; loglevel) but how much it would help to know that we had additional N OOM</span>
<span class="quote">&gt; &gt; victims? From my experience checking the OOM reports which are still in the</span>
<span class="quote">&gt; &gt; logbuffer are sufficient to see whether there is a memory leak, pinned memory</span>
<span class="quote">&gt; &gt; or a continuous memory pressure. Your experience might be different so it</span>
<span class="quote">&gt; &gt; would be nice to mention that in the changelog.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok. </span>
<span class="quote">&gt; As I said earlier, normally all logs will be disabled in production system.</span>
<span class="quote">&gt; But, we can access /proc/vmstat. The oom would have happened in the system</span>
<span class="quote">&gt; Earlier, but the logs would have over-written.</span>
<span class="quote">&gt; The /proc/vmstat is the only counter which can tell, if ever system entered into</span>
<span class="quote">&gt; oom cases.</span>
<span class="quote">&gt; Once we know for sure that oom happened in the system, then we can enable all</span>
<span class="quote">&gt; logs in the system to reproduce the oom scenarios to analyze further.</span>

Why reducing the loglevel is not sufficient here? The output should be
considerably reduced and chances to overflow the ringbuffer reduced as well.
<span class="quote">
&gt; Also it can help in initial tuning of the system for the memory needs of the</span>
<span class="quote">&gt; system.</span>
<span class="quote">&gt; In embedded world, we normally try to avoid the system to enter into kernel OOM</span>
<span class="quote">&gt; as far as possible.</span>

Which means that you should follow a completely different metric IMO.
oom_stall is way too late. It is at the time when no reclaim progress could
be done and we are OOM already.
<span class="quote">
&gt; For example, in Android, we have LMK (low memory killer) driver that controls</span>
<span class="quote">&gt; the OOM behavior. But most of the time these LMK threshold are statically</span>
<span class="quote">&gt; controlled.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Now with this oom counter we can dynamically control the LMK behavior.</span>
<span class="quote">&gt; For example, in LMK we can check, if ever oom_stall becomes 1, that means system</span>
<span class="quote">&gt; is hitting OOM state. At this stage we can immediately trigger the OOM killing</span>
<span class="quote">&gt; from user space or LMK driver.</span>

If you see oom_stall then you are basically OOM and the global OOM
killer will fire. Intervening with other party just sounds like a
terrible idea to me.
<span class="quote">
&gt; Similar user case and requirement is there for Tizen that controls OOM from user</span>
<span class="quote">&gt; space (without LMK).</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 7, 2015, 2:48 p.m.</div>
<pre class="content">
Hi,
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Michal Hocko [mailto:mhocko@kernel.org]</span>
<span class="quote">&gt; Sent: Tuesday, October 06, 2015 9:12 PM</span>
<span class="quote">&gt; To: PINTU KUMAR</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; koct9i@gmail.com; rientjes@google.com; hannes@cmpxchg.org; penguin-</span>
<span class="quote">&gt; kernel@i-love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de;</span>
<span class="quote">&gt; vbabka@suse.cz; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue 06-10-15 12:29:52, PINTU KUMAR wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; OK, that would explain why the second counter is so much larger than</span>
<span class="quote">&gt; &gt; &gt; oom_stall.</span>
<span class="quote">&gt; &gt; &gt; And that alone should have been a red flag IMO. Why should be memcg</span>
<span class="quote">&gt; &gt; &gt; OOM killer events accounted together with the global? How do you</span>
<span class="quote">&gt; &gt; &gt; distinguish the two?</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Actually, here, we are just interested in knowing oom_kill. Let it be</span>
<span class="quote">&gt; &gt; either global, memcg or others.</span>
<span class="quote">&gt; &gt; Once we know there are oom kill happening, we can easily find it by</span>
<span class="quote">&gt; &gt; enabling logs.</span>
<span class="quote">&gt; &gt; Normally in production system, all system logs will be disabled.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This doesn&#39;t make much sense to me. So you find out that _an oom killer_ was</span>
<span class="quote">&gt; invoked but you have logs disabled. What now? You can hardly find out what</span>
<span class="quote">&gt; has happened and why it has happened. What is the point then?</span>
<span class="quote">&gt; Wait for another one to come? This might be never.</span>
<span class="quote">&gt; </span>
Ok, let me explain the real case that we have experienced.
In our case, we have low memory killer in user space itself that invoked based
on some memory threshold.
Something like, below 100MB threshold starting killing until it comes back to
150MB.
During our long duration ageing test (more than 72 hours) we observed that many
applications are killed.
Now, we were not sure if killing happens in user space or kernel space.
When we saw the kernel logs, it generated many logs such as;
/var/log/{messages, messages.0, messages.1, messages.2, messages.3, etc.}
But, none of the logs contains kernel OOM messages. Although there were some LMK
kill in user space.
Then in another round of test we keep dumping _dmesg_ output to a file after
each iteration.
After 3 days of tests this time we observed that dmesg output dump contains many
kernel oom messages.
Now, every time this dumping is not feasible. And instead of counting manually
in log file, we wanted to know number of oom kills happened during this tests.
So we decided to add a counter in /proc/vmstat to track the kernel oom_kill, and
monitor it during our ageing test.
Basically, we wanted to tune our user space LMK killer for different threshold
values, so that we can completely avoid the kernel oom kill.
So, just by looking into this counter, we could able to tune the LMK threshold
values without depending on the kernel log messages.

Also, in most of the system /var/log/messages are not present and we just
depends on kernel dmesg output, which is petty small for longer run.
Even if we reduce the loglevel to 4, it may not be suitable to capture all logs.
<span class="quote">
&gt; What is even more confusing is the mixing of memcg and global oom conditions.</span>
<span class="quote">&gt; They are really different things. Memcg API will even give you notification</span>
about
<span class="quote">&gt; the OOM event.</span>
<span class="quote">&gt; </span>
Ok, you are suggesting to divide the oom_kill counter into 2 parts (global &amp;
memcg) ?
May be something like:
nr_oom_victims
nr_memcg_oom_victims
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; Sorry, I wasn&#39;t clear enough here. I was talking about oom_stall</span>
<span class="quote">&gt; &gt; &gt; counter here not oom_kill_count one.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Ok, I got your point.</span>
<span class="quote">&gt; &gt; Oom_kill_process, is called from 2 places:</span>
<span class="quote">&gt; &gt; 1) out_of_memory</span>
<span class="quote">&gt; &gt; 2) mem_cgroup_out_of_memory</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; And, out_of_memory is actually called from 3 places:</span>
<span class="quote">&gt; &gt; 1) alloc_pages_may_oom</span>
<span class="quote">&gt; &gt; 2) pagefault_out_of_memory</span>
<span class="quote">&gt; &gt; 3) moom_callback (sysirq.c)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Thus, in this case, the oom_stall counter can be added in 4 places (in</span>
<span class="quote">&gt; &gt; the beginning).</span>
<span class="quote">&gt; &gt; 1) alloc_pages_may_oom</span>
<span class="quote">&gt; &gt; 2) mem_cgroup_out_of_memory</span>
<span class="quote">&gt; &gt; 3) pagefault_out_of_memory</span>
<span class="quote">&gt; &gt; 4) moom_callback (sysirq.c)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; For, case {2,3,4}, we could have actually called at one place in</span>
<span class="quote">&gt; &gt; out_of_memory,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why would you even consider 4 for oom_stall? This is an administrator order to</span>
<span class="quote">&gt; kill a memory hog. The system might be in a good shape just the memory hog is</span>
<span class="quote">&gt; misbehaving. I realize this is not a usual usecase but if oom_stall is</span>
supposed to
<span class="quote">&gt; measure a memory pressure of some sort then binding it to a user action is</span>
<span class="quote">&gt; wrong thing to do.</span>
<span class="quote">&gt; </span>
I think, oom_stall is not so important. So I think we can drop it. It also
creates confusion with memcg and others and makes it more complicated. So I am
thinking to remove it.
The more important thing is : nr_oom_victims.
I think this should be sufficient.
<span class="quote">
&gt; &gt; But this result into calling it 2 times because alloc_pages_may_oom</span>
<span class="quote">&gt; &gt; also call out_of_memory.</span>
<span class="quote">&gt; &gt; If there is any better idea, please let me know.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you are focusing too much on the implementation before you are clear</span>
in
<span class="quote">&gt; what should be the desired semantic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; What is it supposed to tell us? How many times the system had to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; go into emergency OOM steps? How many times the direct reclaim</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; didn&#39;t make any progress so we can consider the system OOM?</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Yes, exactly, oom_stall can tell, how many times OOM is invoked in</span>
<span class="quote">&gt; &gt; &gt; &gt; the system.</span>
<span class="quote">&gt; &gt; &gt; &gt; Yes, it can also tell how many times direct_reclaim fails completely.</span>
<span class="quote">&gt; &gt; &gt; &gt; Currently, we don&#39;t have any counter for direct_reclaim success/fail.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; So why don&#39;t we add one? Direct reclaim failure is a clearly defined</span>
<span class="quote">&gt; &gt; &gt; event and it also can be evaluated reasonably against allocstall.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Yes, direct_reclaim success/fail is also planned ahead.</span>
<span class="quote">&gt; &gt; May be something like:</span>
<span class="quote">&gt; &gt; direct_reclaim_alloc_success</span>
<span class="quote">&gt; &gt; direct_reclaim_alloc_fail</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We already have alloc_stall so all_stall_noprogress or whatever better name</span>
<span class="quote">&gt; should be sufficient.</span>
<span class="quote">&gt; </span>
Ok, this we can discuss later and finalize on the name.
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; I am still not sure how useful this counter would be, though. Sure</span>
<span class="quote">&gt; &gt; &gt; the log ringbuffer might overflow (the risk can be reduced by</span>
<span class="quote">&gt; &gt; &gt; reducing the</span>
<span class="quote">&gt; &gt; &gt; loglevel) but how much it would help to know that we had additional</span>
<span class="quote">&gt; &gt; &gt; N OOM victims? From my experience checking the OOM reports which are</span>
<span class="quote">&gt; &gt; &gt; still in the logbuffer are sufficient to see whether there is a</span>
<span class="quote">&gt; &gt; &gt; memory leak, pinned memory or a continuous memory pressure. Your</span>
<span class="quote">&gt; &gt; &gt; experience might be different so it would be nice to mention that in the</span>
<span class="quote">&gt; changelog.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Ok.</span>
<span class="quote">&gt; &gt; As I said earlier, normally all logs will be disabled in production system.</span>
<span class="quote">&gt; &gt; But, we can access /proc/vmstat. The oom would have happened in the</span>
<span class="quote">&gt; &gt; system Earlier, but the logs would have over-written.</span>
<span class="quote">&gt; &gt; The /proc/vmstat is the only counter which can tell, if ever system</span>
<span class="quote">&gt; &gt; entered into oom cases.</span>
<span class="quote">&gt; &gt; Once we know for sure that oom happened in the system, then we can</span>
<span class="quote">&gt; &gt; enable all logs in the system to reproduce the oom scenarios to analyze</span>
<span class="quote">&gt; further.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why reducing the loglevel is not sufficient here? The output should be</span>
<span class="quote">&gt; considerably reduced and chances to overflow the ringbuffer reduced as well.</span>
<span class="quote">&gt; </span>
I think, I explained it above.
In most of the system /var/log/messages are not present and we just depends on
kernel dmesg output, which is petty small for longer run.
<span class="quote">
&gt; &gt; Also it can help in initial tuning of the system for the memory needs</span>
<span class="quote">&gt; &gt; of the system.</span>
<span class="quote">&gt; &gt; In embedded world, we normally try to avoid the system to enter into</span>
<span class="quote">&gt; &gt; kernel OOM as far as possible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which means that you should follow a completely different metric IMO.</span>
<span class="quote">&gt; oom_stall is way too late. It is at the time when no reclaim progress could be</span>
<span class="quote">&gt; done and we are OOM already.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; For example, in Android, we have LMK (low memory killer) driver that</span>
<span class="quote">&gt; &gt; controls the OOM behavior. But most of the time these LMK threshold</span>
<span class="quote">&gt; &gt; are statically controlled.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Now with this oom counter we can dynamically control the LMK behavior.</span>
<span class="quote">&gt; &gt; For example, in LMK we can check, if ever oom_stall becomes 1, that</span>
<span class="quote">&gt; &gt; means system is hitting OOM state. At this stage we can immediately</span>
<span class="quote">&gt; &gt; trigger the OOM killing from user space or LMK driver.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you see oom_stall then you are basically OOM and the global OOM killer will</span>
<span class="quote">&gt; fire. Intervening with other party just sounds like a terrible idea to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Similar user case and requirement is there for Tizen that controls OOM</span>
<span class="quote">&gt; &gt; from user space (without LMK).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 8, 2015, 2:18 p.m.</div>
<pre class="content">
On Wed 07-10-15 20:18:16, PINTU KUMAR wrote:
[...]
<span class="quote">&gt; Ok, let me explain the real case that we have experienced.</span>
<span class="quote">&gt; In our case, we have low memory killer in user space itself that invoked based</span>
<span class="quote">&gt; on some memory threshold.</span>
<span class="quote">&gt; Something like, below 100MB threshold starting killing until it comes back to</span>
<span class="quote">&gt; 150MB.</span>
<span class="quote">&gt; During our long duration ageing test (more than 72 hours) we observed that many</span>
<span class="quote">&gt; applications are killed.</span>
<span class="quote">&gt; Now, we were not sure if killing happens in user space or kernel space.</span>
<span class="quote">&gt; When we saw the kernel logs, it generated many logs such as;</span>
<span class="quote">&gt; /var/log/{messages, messages.0, messages.1, messages.2, messages.3, etc.}</span>
<span class="quote">&gt; But, none of the logs contains kernel OOM messages. Although there were some LMK</span>
<span class="quote">&gt; kill in user space.</span>
<span class="quote">&gt; Then in another round of test we keep dumping _dmesg_ output to a file after</span>
<span class="quote">&gt; each iteration.</span>
<span class="quote">&gt; After 3 days of tests this time we observed that dmesg output dump contains many</span>
<span class="quote">&gt; kernel oom messages.</span>

I am confused. So you suspect that the OOM report didn&#39;t get to
/var/log/messages while it was in dmesg?
<span class="quote">
&gt; Now, every time this dumping is not feasible. And instead of counting manually</span>
<span class="quote">&gt; in log file, we wanted to know number of oom kills happened during this tests.</span>
<span class="quote">&gt; So we decided to add a counter in /proc/vmstat to track the kernel oom_kill, and</span>
<span class="quote">&gt; monitor it during our ageing test.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Basically, we wanted to tune our user space LMK killer for different threshold</span>
<span class="quote">&gt; values, so that we can completely avoid the kernel oom kill.</span>
<span class="quote">&gt; So, just by looking into this counter, we could able to tune the LMK threshold</span>
<span class="quote">&gt; values without depending on the kernel log messages.</span>

Wouldn&#39;t a trace point suit you better for this particular use case
considering this is a testing environment?
<span class="quote"> 
&gt; Also, in most of the system /var/log/messages are not present and we just</span>
<span class="quote">&gt; depends on kernel dmesg output, which is petty small for longer run.</span>
<span class="quote">&gt; Even if we reduce the loglevel to 4, it may not be suitable to capture all logs.</span>

Hmm, I would consider a logless system considerably crippled but I see
your point and I can imagine that especially small devices might try
to save every single B of the storage. Such a system is basically
undebugable IMO but it still might be interesting to see OOM killer
traces.
<span class="quote"> 
&gt; &gt; What is even more confusing is the mixing of memcg and global oom</span>
<span class="quote">&gt; &gt; conditions.  They are really different things. Memcg API will even</span>
<span class="quote">&gt; &gt; give you notification about the OOM event.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Ok, you are suggesting to divide the oom_kill counter into 2 parts (global &amp;</span>
<span class="quote">&gt; memcg) ?</span>
<span class="quote">&gt; May be something like:</span>
<span class="quote">&gt; nr_oom_victims</span>
<span class="quote">&gt; nr_memcg_oom_victims</span>

You do not need the later. Memcg interface already provides you with a
notification API and if a counter is _really_ needed then it should be
per-memcg not a global cumulative number.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 8, 2015, 4:06 p.m.</div>
<pre class="content">
Hi,

Thank you very much for your reply and comments.
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Michal Hocko [mailto:mhocko@kernel.org]</span>
<span class="quote">&gt; Sent: Thursday, October 08, 2015 7:49 PM</span>
<span class="quote">&gt; To: PINTU KUMAR</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; koct9i@gmail.com; rientjes@google.com; hannes@cmpxchg.org; penguin-</span>
<span class="quote">&gt; kernel@i-love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de;</span>
<span class="quote">&gt; vbabka@suse.cz; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Wed 07-10-15 20:18:16, PINTU KUMAR wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Ok, let me explain the real case that we have experienced.</span>
<span class="quote">&gt; &gt; In our case, we have low memory killer in user space itself that</span>
<span class="quote">&gt; &gt; invoked based on some memory threshold.</span>
<span class="quote">&gt; &gt; Something like, below 100MB threshold starting killing until it comes</span>
<span class="quote">&gt; &gt; back to 150MB.</span>
<span class="quote">&gt; &gt; During our long duration ageing test (more than 72 hours) we observed</span>
<span class="quote">&gt; &gt; that many applications are killed.</span>
<span class="quote">&gt; &gt; Now, we were not sure if killing happens in user space or kernel space.</span>
<span class="quote">&gt; &gt; When we saw the kernel logs, it generated many logs such as;</span>
<span class="quote">&gt; &gt; /var/log/{messages, messages.0, messages.1, messages.2, messages.3,</span>
<span class="quote">&gt; &gt; etc.} But, none of the logs contains kernel OOM messages. Although</span>
<span class="quote">&gt; &gt; there were some LMK kill in user space.</span>
<span class="quote">&gt; &gt; Then in another round of test we keep dumping _dmesg_ output to a file</span>
<span class="quote">&gt; &gt; after each iteration.</span>
<span class="quote">&gt; &gt; After 3 days of tests this time we observed that dmesg output dump</span>
<span class="quote">&gt; &gt; contains many kernel oom messages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am confused. So you suspect that the OOM report didn&#39;t get to</span>
<span class="quote">&gt; /var/log/messages while it was in dmesg?</span>

No, I mean to say that all the /var/log/messages were over-written (after 3
days).
Or, it was cleared due to storage space constraints. So, oom kill logs were not
visible.
So, in our ageing test scripts, we keep dumping the dmesg output, during our
tests.
For_each_application:
Do
	Launch an application from cmdline
	Sleep 10 seconds
	dmesg -c &gt;&gt; /var/log/dmesg.log
Done
Continue this loop for more than 300 times.
After 3 days, when we analyzed the dump, we found that dmesg.log contains some
OOM messages.
Whereas, these OOM logs were not found in /var/log/messages.
May be we do heavy logging because in ageing test we enable maximum
functionality (Wifi, BT, GPS, fully loaded system).

Hope, it is clear now. If not, please ask me for more information.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; Now, every time this dumping is not feasible. And instead of counting</span>
<span class="quote">&gt; &gt; manually in log file, we wanted to know number of oom kills happened during</span>
<span class="quote">&gt; this tests.</span>
<span class="quote">&gt; &gt; So we decided to add a counter in /proc/vmstat to track the kernel</span>
<span class="quote">&gt; &gt; oom_kill, and monitor it during our ageing test.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Basically, we wanted to tune our user space LMK killer for different</span>
<span class="quote">&gt; &gt; threshold values, so that we can completely avoid the kernel oom kill.</span>
<span class="quote">&gt; &gt; So, just by looking into this counter, we could able to tune the LMK</span>
<span class="quote">&gt; &gt; threshold values without depending on the kernel log messages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Wouldn&#39;t a trace point suit you better for this particular use case</span>
considering this
<span class="quote">&gt; is a testing environment?</span>
<span class="quote">&gt; </span>
Tracing for oom_kill count?
Actually, tracing related configs will be normally disabled in release binary.
And it is not always feasible to perform tracing for such long duration tests.
Then it should be valid for other counters as well.
<span class="quote">
&gt; &gt; Also, in most of the system /var/log/messages are not present and we</span>
<span class="quote">&gt; &gt; just depends on kernel dmesg output, which is petty small for longer run.</span>
<span class="quote">&gt; &gt; Even if we reduce the loglevel to 4, it may not be suitable to capture all</span>
logs.
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, I would consider a logless system considerably crippled but I see your</span>
<span class="quote">&gt; point and I can imagine that especially small devices might try to save every</span>
<span class="quote">&gt; single B of the storage. Such a system is basically undebugable IMO but it</span>
still
<span class="quote">&gt; might be interesting to see OOM killer traces.</span>
<span class="quote">&gt; </span>
Exactly, some of the small embedded systems might be having 512MB, 256MB, 128MB,
or even lesser.
Also, the storage space will be 8GB or below.
In such a system we cannot afford heavy log files and exact tuning and stability
is most important.
Even all tracing / profiling configs will be disabled to lowest level for
reducing kernel code size as well.
<span class="quote">
&gt; &gt; &gt; What is even more confusing is the mixing of memcg and global oom</span>
<span class="quote">&gt; &gt; &gt; conditions.  They are really different things. Memcg API will even</span>
<span class="quote">&gt; &gt; &gt; give you notification about the OOM event.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Ok, you are suggesting to divide the oom_kill counter into 2 parts</span>
<span class="quote">&gt; &gt; (global &amp;</span>
<span class="quote">&gt; &gt; memcg) ?</span>
<span class="quote">&gt; &gt; May be something like:</span>
<span class="quote">&gt; &gt; nr_oom_victims</span>
<span class="quote">&gt; &gt; nr_memcg_oom_victims</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You do not need the later. Memcg interface already provides you with a</span>
<span class="quote">&gt; notification API and if a counter is _really_ needed then it should be</span>
per-memcg
<span class="quote">&gt; not a global cumulative number.</span>

Ok, for memory cgroups, you mean to say this one?
sh-3.2# cat /sys/fs/cgroup/memory/memory.oom_control
oom_kill_disable 0
under_oom 0

I am actually confused here what to do next?
Shall I push a new patch set with just:
nr_oom_victims counter ?

Or, please let me know, if more information is missing.
If you have any more suggestions, please let me know.
I will really feel glad about it.
Thank you very much for all your suggestions and review so far.
<span class="quote">

&gt; --</span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Oct. 8, 2015, 4:30 p.m.</div>
<pre class="content">
On Thu 08-10-15 21:36:24, PINTU KUMAR wrote:
[...]
<span class="quote">&gt; Whereas, these OOM logs were not found in /var/log/messages.</span>
<span class="quote">&gt; May be we do heavy logging because in ageing test we enable maximum</span>
<span class="quote">&gt; functionality (Wifi, BT, GPS, fully loaded system).</span>

If you swamp your logs so heavily that even critical messages won&#39;t make
it into the log files then your logging is basically useless for
anything serious. But that is not really that important.
<span class="quote">
&gt; Hope, it is clear now. If not, please ask me for more information.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Now, every time this dumping is not feasible. And instead of counting</span>
<span class="quote">&gt; &gt; &gt; manually in log file, we wanted to know number of oom kills happened during</span>
<span class="quote">&gt; &gt; this tests.</span>
<span class="quote">&gt; &gt; &gt; So we decided to add a counter in /proc/vmstat to track the kernel</span>
<span class="quote">&gt; &gt; &gt; oom_kill, and monitor it during our ageing test.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Basically, we wanted to tune our user space LMK killer for different</span>
<span class="quote">&gt; &gt; &gt; threshold values, so that we can completely avoid the kernel oom kill.</span>
<span class="quote">&gt; &gt; &gt; So, just by looking into this counter, we could able to tune the LMK</span>
<span class="quote">&gt; &gt; &gt; threshold values without depending on the kernel log messages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Wouldn&#39;t a trace point suit you better for this particular use case</span>
<span class="quote">&gt; &gt; considering this</span>
<span class="quote">&gt; &gt; is a testing environment?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Tracing for oom_kill count?</span>
<span class="quote">&gt; Actually, tracing related configs will be normally disabled in release binary.</span>

Yes but your use case described a testing environment.
<span class="quote">
&gt; And it is not always feasible to perform tracing for such long duration tests.</span>

I do not see why long duration would be a problem. Each tracepoint can
be enabled separatelly.
<span class="quote">
&gt; Then it should be valid for other counters as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; Also, in most of the system /var/log/messages are not present and we</span>
<span class="quote">&gt; &gt; &gt; just depends on kernel dmesg output, which is petty small for longer run.</span>
<span class="quote">&gt; &gt; &gt; Even if we reduce the loglevel to 4, it may not be suitable to capture all</span>
<span class="quote">&gt; logs.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hmm, I would consider a logless system considerably crippled but I see your</span>
<span class="quote">&gt; &gt; point and I can imagine that especially small devices might try to save every</span>
<span class="quote">&gt; &gt; single B of the storage. Such a system is basically undebugable IMO but it</span>
<span class="quote">&gt; still</span>
<span class="quote">&gt; &gt; might be interesting to see OOM killer traces.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Exactly, some of the small embedded systems might be having 512MB, 256MB, 128MB,</span>
<span class="quote">&gt; or even lesser.</span>
<span class="quote">&gt; Also, the storage space will be 8GB or below.</span>
<span class="quote">&gt; In such a system we cannot afford heavy log files and exact tuning and stability</span>
<span class="quote">&gt; is most important.</span>

And that is what log level is for. If your logs are heavy with error
levels then you are far from being production ready... ;)
<span class="quote">
&gt; Even all tracing / profiling configs will be disabled to lowest level for</span>
<span class="quote">&gt; reducing kernel code size as well.</span>

What level is that? crit? Is err really that noisy?
 
[...]
<span class="quote">&gt; &gt; &gt; Ok, you are suggesting to divide the oom_kill counter into 2 parts</span>
<span class="quote">&gt; &gt; &gt; (global &amp;</span>
<span class="quote">&gt; &gt; &gt; memcg) ?</span>
<span class="quote">&gt; &gt; &gt; May be something like:</span>
<span class="quote">&gt; &gt; &gt; nr_oom_victims</span>
<span class="quote">&gt; &gt; &gt; nr_memcg_oom_victims</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; You do not need the later. Memcg interface already provides you with a</span>
<span class="quote">&gt; &gt; notification API and if a counter is _really_ needed then it should be</span>
<span class="quote">&gt; &gt; per-memcg</span>
<span class="quote">&gt; &gt; not a global cumulative number.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, for memory cgroups, you mean to say this one?</span>
<span class="quote">&gt; sh-3.2# cat /sys/fs/cgroup/memory/memory.oom_control</span>
<span class="quote">&gt; oom_kill_disable 0</span>
<span class="quote">&gt; under_oom 0</span>

Yes this is the notification API.
<span class="quote">
&gt; I am actually confused here what to do next?</span>
<span class="quote">&gt; Shall I push a new patch set with just:</span>
<span class="quote">&gt; nr_oom_victims counter ?</span>

Yes you can repost with a better description about a typical usage
scenarios. I cannot say I would be completely sold to this because
the only relevant usecase I&#39;ve heard so far is the logless system
which is pretty much a corner case. This is not a reason to nack it
though. It is definitely better than the original oom_stall suggestion
because it has a clear semantic at least.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 9, 2015, 12:59 p.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Michal Hocko [mailto:mhocko@kernel.org]</span>
<span class="quote">&gt; Sent: Thursday, October 08, 2015 10:01 PM</span>
<span class="quote">&gt; To: PINTU KUMAR</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; koct9i@gmail.com; rientjes@google.com; hannes@cmpxchg.org; penguin-</span>
<span class="quote">&gt; kernel@i-love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de;</span>
<span class="quote">&gt; vbabka@suse.cz; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [PATCH 1/1] mm: vmstat: Add OOM kill count in vmstat counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu 08-10-15 21:36:24, PINTU KUMAR wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Whereas, these OOM logs were not found in /var/log/messages.</span>
<span class="quote">&gt; &gt; May be we do heavy logging because in ageing test we enable maximum</span>
<span class="quote">&gt; &gt; functionality (Wifi, BT, GPS, fully loaded system).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you swamp your logs so heavily that even critical messages won&#39;t make it</span>
into
<span class="quote">&gt; the log files then your logging is basically useless for anything serious. But</span>
that is
<span class="quote">&gt; not really that important.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Hope, it is clear now. If not, please ask me for more information.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Now, every time this dumping is not feasible. And instead of</span>
<span class="quote">&gt; &gt; &gt; &gt; counting manually in log file, we wanted to know number of oom</span>
<span class="quote">&gt; &gt; &gt; &gt; kills happened during</span>
<span class="quote">&gt; &gt; &gt; this tests.</span>
<span class="quote">&gt; &gt; &gt; &gt; So we decided to add a counter in /proc/vmstat to track the kernel</span>
<span class="quote">&gt; &gt; &gt; &gt; oom_kill, and monitor it during our ageing test.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Basically, we wanted to tune our user space LMK killer for</span>
<span class="quote">&gt; &gt; &gt; &gt; different threshold values, so that we can completely avoid the kernel</span>
oom
<span class="quote">&gt; kill.</span>
<span class="quote">&gt; &gt; &gt; &gt; So, just by looking into this counter, we could able to tune the</span>
<span class="quote">&gt; &gt; &gt; &gt; LMK threshold values without depending on the kernel log messages.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Wouldn&#39;t a trace point suit you better for this particular use case</span>
<span class="quote">&gt; &gt; &gt; considering this is a testing environment?</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Tracing for oom_kill count?</span>
<span class="quote">&gt; &gt; Actually, tracing related configs will be normally disabled in release</span>
binary.
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes but your use case described a testing environment.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; And it is not always feasible to perform tracing for such long duration</span>
tests.
<span class="quote">&gt; </span>
<span class="quote">&gt; I do not see why long duration would be a problem. Each tracepoint can be</span>
<span class="quote">&gt; enabled separatelly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Then it should be valid for other counters as well.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Also, in most of the system /var/log/messages are not present and</span>
<span class="quote">&gt; &gt; &gt; &gt; we just depends on kernel dmesg output, which is petty small for longer</span>
<span class="quote">&gt; run.</span>
<span class="quote">&gt; &gt; &gt; &gt; Even if we reduce the loglevel to 4, it may not be suitable to</span>
<span class="quote">&gt; &gt; &gt; &gt; capture all</span>
<span class="quote">&gt; &gt; logs.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Hmm, I would consider a logless system considerably crippled but I</span>
<span class="quote">&gt; &gt; &gt; see your point and I can imagine that especially small devices might</span>
<span class="quote">&gt; &gt; &gt; try to save every single B of the storage. Such a system is</span>
<span class="quote">&gt; &gt; &gt; basically undebugable IMO but it</span>
<span class="quote">&gt; &gt; still</span>
<span class="quote">&gt; &gt; &gt; might be interesting to see OOM killer traces.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Exactly, some of the small embedded systems might be having 512MB,</span>
<span class="quote">&gt; &gt; 256MB, 128MB, or even lesser.</span>
<span class="quote">&gt; &gt; Also, the storage space will be 8GB or below.</span>
<span class="quote">&gt; &gt; In such a system we cannot afford heavy log files and exact tuning and</span>
<span class="quote">&gt; &gt; stability is most important.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And that is what log level is for. If your logs are heavy with error levels</span>
then you
<span class="quote">&gt; are far from being production ready... ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Even all tracing / profiling configs will be disabled to lowest level</span>
<span class="quote">&gt; &gt; for reducing kernel code size as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What level is that? crit? Is err really that noisy?</span>
<span class="quote">&gt; </span>
No. I was talking about kernel configs. Normally we keep some profiling/tracing
related configs disabled for low memory system, to save some kernel code size.
The point is that it&#39;s always not easy for all systems to heavily depends on
logging and tracing.
Else, the other counters would also not be required.
We thought that the /proc/vmstat output (which is ideally available in all
systems, small or big, embedded or none embedded), it can quickly tell us what
has happened really.
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; Ok, you are suggesting to divide the oom_kill counter into 2 parts</span>
<span class="quote">&gt; &gt; &gt; &gt; (global &amp;</span>
<span class="quote">&gt; &gt; &gt; &gt; memcg) ?</span>
<span class="quote">&gt; &gt; &gt; &gt; May be something like:</span>
<span class="quote">&gt; &gt; &gt; &gt; nr_oom_victims</span>
<span class="quote">&gt; &gt; &gt; &gt; nr_memcg_oom_victims</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; You do not need the later. Memcg interface already provides you with</span>
<span class="quote">&gt; &gt; &gt; a notification API and if a counter is _really_ needed then it</span>
<span class="quote">&gt; &gt; &gt; should be per-memcg not a global cumulative number.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Ok, for memory cgroups, you mean to say this one?</span>
<span class="quote">&gt; &gt; sh-3.2# cat /sys/fs/cgroup/memory/memory.oom_control</span>
<span class="quote">&gt; &gt; oom_kill_disable 0</span>
<span class="quote">&gt; &gt; under_oom 0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes this is the notification API.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I am actually confused here what to do next?</span>
<span class="quote">&gt; &gt; Shall I push a new patch set with just:</span>
<span class="quote">&gt; &gt; nr_oom_victims counter ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes you can repost with a better description about a typical usage scenarios.</span>
I
<span class="quote">&gt; cannot say I would be completely sold to this because the only relevant</span>
usecase
<span class="quote">&gt; I&#39;ve heard so far is the logless system which is pretty much a corner case.</span>
This is
<span class="quote">&gt; not a reason to nack it though. It is definitely better than the original</span>
oom_stall
<span class="quote">&gt; suggestion because it has a clear semantic at least.</span>

Ok, thank you very much for your suggestions.
I agree, oom_stall is not so important.
I will try to submit a new patch set with only _nr_oom_victims_ with the
descriptions about the usefulness that I came across.
If anybody else can point out other use cases, please let me know. 
I will be happy to try that and share the results.
<span class="quote">
&gt; --</span>
<span class="quote">&gt; Michal Hocko</span>
<span class="quote">&gt; SUSE Labs</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 2b1cef8..ade0851 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -57,6 +57,8 @@</span> <span class="p_context"> enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
 #endif
<span class="p_add">+		OOM_STALL,</span>
<span class="p_add">+		OOM_KILL_COUNT,</span>
 		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
 		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
 		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 03b612b..e79caed 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -570,6 +570,7 @@</span> <span class="p_context"> void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
 	 * space under its control.
 	 */
 	do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);
<span class="p_add">+	count_vm_event(OOM_KILL_COUNT);</span>
 	mark_oom_victim(victim);
 	pr_err(&quot;Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\n&quot;,
 		task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),
<span class="p_chunk">@@ -600,6 +601,7 @@</span> <span class="p_context"> void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
 				task_pid_nr(p), p-&gt;comm);
 			task_unlock(p);
 			do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);
<span class="p_add">+			count_vm_event(OOM_KILL_COUNT);</span>
 		}
 	rcu_read_unlock();
 
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 9bcfd70..1d82210 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -2761,7 +2761,7 @@</span> <span class="p_context"> __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,</span>
 		schedule_timeout_uninterruptible(1);
 		return NULL;
 	}
<span class="p_del">-</span>
<span class="p_add">+	count_vm_event(OOM_STALL);</span>
 	/*
 	 * Go through the zonelist yet one more time, keep very high watermark
 	 * here, this is only to catch a parallel oom killing, we must fail if
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 1fd0886..f054265 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -808,6 +808,8 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;htlb_buddy_alloc_success&quot;,
 	&quot;htlb_buddy_alloc_fail&quot;,
 #endif
<span class="p_add">+	&quot;oom_stall&quot;,</span>
<span class="p_add">+	&quot;oom_kill_count&quot;,</span>
 	&quot;unevictable_pgs_culled&quot;,
 	&quot;unevictable_pgs_scanned&quot;,
 	&quot;unevictable_pgs_rescued&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



