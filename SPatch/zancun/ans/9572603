
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V3,3/7] mm: reclaim MADV_FREE pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V3,3/7] mm: reclaim MADV_FREE pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 14, 2017, 7:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;cd6a477063c40ad899ad8f4e964c347525ea23a3.1487100204.git.shli@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9572603/mbox/"
   >mbox</a>
|
   <a href="/patch/9572603/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9572603/">/patch/9572603/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	218D360573 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Feb 2017 19:36:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 199812793B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Feb 2017 19:36:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0DB2627D16; Tue, 14 Feb 2017 19:36:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 242EA26E77
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Feb 2017 19:36:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754045AbdBNTgd (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Feb 2017 14:36:33 -0500
Received: from mx0a-00082601.pphosted.com ([67.231.145.42]:33732 &quot;EHLO
	mx0a-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752318AbdBNTgR (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Feb 2017 14:36:17 -0500
Received: from pps.filterd (m0044010.ppops.net [127.0.0.1])
	by mx0a-00082601.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v1EJYqMJ013683
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 14 Feb 2017 11:36:17 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : in-reply-to : references : mime-version :
	content-type; s=facebook;
	bh=iyfLGAe2zO9Rv184Ly6GLCd8UUf1NFFzP1hmhWsQuSk=; 
	b=Wgo8ZPxP9jjhmSAXeTfh8LXNPMAtxg46RgK/wdD6gvCw7w3yi2T29Vs0OPAm7Y9SMsDV
	ULT5CvLu/2MjZtHj35EKHc57NG+BKticgIG6o2n6YOQ7z5g9x0GcpjTchNmYxSIS7ZEd
	LznXWq548wXKJmLE+6MQydUUzOOXMF1n5No= 
Received: from mail.thefacebook.com ([199.201.64.23])
	by mx0a-00082601.pphosted.com with ESMTP id 28m6rbgens-6
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 14 Feb 2017 11:36:17 -0800
Received: from mx-out.facebook.com (192.168.52.123) by
	PRN-CHUB12.TheFacebook.com (192.168.16.22) with Microsoft SMTP Server
	(TLS) id 14.3.294.0; Tue, 14 Feb 2017 11:36:16 -0800
Received: from facebook.com (2401:db00:21:603d:face:0:19:0)     by
	mx-out.facebook.com (10.222.219.45) with ESMTP id
	d8b4a1aef2ec11e69c8624be05904660-72bf9a00 for
	&lt;linux-kernel@vger.kernel.org&gt;; Tue, 14 Feb 2017 11:36:14 -0800
Received: by devbig638.prn2.facebook.com (Postfix, from userid 11222)   id
	57F2C46A1658; Tue, 14 Feb 2017 11:36:13 -0800 (PST)
Smtp-Origin-Hostprefix: devbig
From: Shaohua Li &lt;shli@fb.com&gt;
Smtp-Origin-Hostname: devbig638.prn2.facebook.com
To: &lt;linux-mm@kvack.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;
CC: &lt;Kernel-team@fb.com&gt;, &lt;mhocko@suse.com&gt;, &lt;minchan@kernel.org&gt;,
	&lt;hughd@google.com&gt;, &lt;hannes@cmpxchg.org&gt;, &lt;riel@redhat.com&gt;,
	&lt;mgorman@techsingularity.net&gt;, &lt;akpm@linux-foundation.org&gt;
Smtp-Origin-Cluster: prn2c22
Subject: [PATCH V3 3/7] mm: reclaim MADV_FREE pages
Date: Tue, 14 Feb 2017 11:36:09 -0800
Message-ID: &lt;cd6a477063c40ad899ad8f4e964c347525ea23a3.1487100204.git.shli@fb.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;cover.1487100204.git.shli@fb.com&gt;
References: &lt;cover.1487100204.git.shli@fb.com&gt;
X-FB-Internal: Safe
MIME-Version: 1.0
Content-Type: text/plain
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-02-14_12:, , signatures=0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 14, 2017, 7:36 p.m.</div>
<pre class="content">
When memory pressure is high, we free MADV_FREE pages. If the pages are
not dirty in pte, the pages could be freed immediately. Otherwise we
can&#39;t reclaim them. We put the pages back to anonumous LRU list (by
setting SwapBacked flag) and the pages will be reclaimed in normal
swapout way.

We use normal page reclaim policy. Since MADV_FREE pages are put into
inactive file list, such pages and inactive file pages are reclaimed
according to their age. This is expected, because we don&#39;t want to
reclaim too many MADV_FREE pages before used once pages.

Based on Minchan&#39;s original patch

Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
<span class="signed-off-by">Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
---
 mm/huge_memory.c |  2 ++
 mm/madvise.c     |  1 +
 mm/rmap.c        | 17 ++++++++++++-----
 mm/vmscan.c      | 30 +++++++++++++++++++++---------
 4 files changed, 36 insertions(+), 14 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 16, 2017, 6:40 p.m.</div>
<pre class="content">
On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:
<span class="quote">&gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt;  				page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; -				goto discard;</span>
<span class="quote">&gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; +					goto discard;</span>
<span class="quote">&gt; +				} else {</span>
<span class="quote">&gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; +					break;</span>
<span class="quote">&gt; +				}</span>

I don&#39;t understand why we need the TTU_LZFREE bit in general. More on
that below at the callsite.
<span class="quote">
&gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>

Do we need this? MADV_FREE clears the dirty bit off the page; we could
just let them go through with the function without any special-casing.
<span class="quote">
&gt; @@ -986,6 +986,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		sc-&gt;nr_scanned++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		lazyfree = page_is_lazyfree(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		if (unlikely(!page_evictable(page)))</span>
<span class="quote">&gt;  			goto cull_mlocked;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -993,7 +995,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			goto keep_locked;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp; !lazyfree)</span>
<span class="quote">&gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; @@ -1119,13 +1121,13 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp; !lazyfree) {</span>

lazyfree duplicates the anon check. As per the previous email, IMO it
would be much preferable to get rid of that &quot;lazyfree&quot; obscuring here.

This would simply be:

		if (PageAnon(page) &amp;&amp; PageSwapBacked &amp;&amp; !PageSwapCache)
<span class="quote">
&gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>

Do we actually need to filter for mapping || lazyfree? If we fail to
allocate swap, we don&#39;t reach here. If the page is a truncated file
page, ttu returns pretty much instantly with SWAP_AGAIN. We should be
able to just check for page_mapped() alone, no?
<span class="quote">
&gt;  			switch (ret = try_to_unmap(page, lazyfree ?</span>
<span class="quote">&gt;  				(ttu_flags | TTU_BATCH_FLUSH | TTU_LZFREE) :</span>
<span class="quote">&gt;  				(ttu_flags | TTU_BATCH_FLUSH))) {</span>

That bit I don&#39;t understand. Why do we need to pass TTU_LZFREE? What
information does that carry that cannot be gathered from inside ttu?

I.e. when ttu runs into PageAnon, can it simply check !PageSwapBacked?
And if it&#39;s still clean, it can lazyfreed++; goto discard.

Am I overlooking something?
<span class="quote">
&gt; @@ -1154,7 +1156,14 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt;  				goto cull_mlocked;</span>
<span class="quote">&gt;  			case SWAP_LZFREE:</span>
<span class="quote">&gt; -				goto lazyfree;</span>
<span class="quote">&gt; +				/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; +				if (page_ref_freeze(page, 1)) {</span>
<span class="quote">&gt; +					if (!PageDirty(page))</span>
<span class="quote">&gt; +						goto lazyfree;</span>
<span class="quote">&gt; +					else</span>
<span class="quote">&gt; +						page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; +				}</span>
<span class="quote">&gt; +				goto keep_locked;</span>
<span class="quote">&gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt;  				; /* try to free the page below */</span>

This is a similar situation.

Can we let the page go through the regular __remove_mapping() process
and simply have that function check for PageAnon &amp;&amp; !PageSwapBacked?
<span class="quote">
&gt; @@ -1266,10 +1275,9 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -lazyfree:</span>
<span class="quote">&gt;  		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt;  			goto keep_locked;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +lazyfree:</span>

... eliminating this special casing.
<span class="quote">
&gt; @@ -1294,6 +1302,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  cull_mlocked:</span>
<span class="quote">&gt;  		if (PageSwapCache(page))</span>
<span class="quote">&gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; +		if (lazyfree)</span>
<span class="quote">&gt; +			clear_page_lazyfree(page);</span>

Why cancel the MADV_FREE state? The combination seems non-sensical,
but we can simply retain the invalidated state while the page goes to
the unevictable list; munlock should move it back to inactive_file.
<span class="quote">
&gt;  		unlock_page(page);</span>
<span class="quote">&gt;  		list_add(&amp;page-&gt;lru, &amp;ret_pages);</span>
<span class="quote">&gt;  		continue;</span>
<span class="quote">&gt; @@ -1303,6 +1313,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))</span>
<span class="quote">&gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt;  		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="quote">&gt; +		if (lazyfree)</span>
<span class="quote">&gt; +			clear_page_lazyfree(page);</span>

This is similar too.

Can we leave simply leave the page alone here? The only way we get to
this point is if somebody is reading the invalidated page. It&#39;s weird
for a lazyfreed page to become active, but it doesn&#39;t seem to warrant
active intervention here.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 17, 2017, 12:27 a.m.</div>
<pre class="content">
On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:
<span class="quote">&gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt; &gt;  				page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; &gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; -				goto discard;</span>
<span class="quote">&gt; &gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; &gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; +					goto discard;</span>
<span class="quote">&gt; &gt; +				} else {</span>
<span class="quote">&gt; &gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; &gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; +					break;</span>
<span class="quote">&gt; &gt; +				}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t understand why we need the TTU_LZFREE bit in general. More on</span>
<span class="quote">&gt; that below at the callsite.</span>

Sounds useless flag, don&#39;t see any reason we shouldn&#39;t free the MADV_FREE page
in places other than reclaim. Looks TTU_UNMAP is useless too..
<span class="quote">
&gt; &gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt; &gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt; &gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; &gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we need this? MADV_FREE clears the dirty bit off the page; we could</span>
<span class="quote">&gt; just let them go through with the function without any special-casing.</span>

this is just to zero dirty and writeback
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -986,6 +986,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		sc-&gt;nr_scanned++;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +		lazyfree = page_is_lazyfree(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  		if (unlikely(!page_evictable(page)))</span>
<span class="quote">&gt; &gt;  			goto cull_mlocked;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -993,7 +995,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  			goto keep_locked;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; &gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; &gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp; !lazyfree)</span>
<span class="quote">&gt; &gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; &gt; @@ -1119,13 +1121,13 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  		/*</span>
<span class="quote">&gt; &gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt; &gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; &gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; &gt; +		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp; !lazyfree) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; lazyfree duplicates the anon check. As per the previous email, IMO it</span>
<span class="quote">&gt; would be much preferable to get rid of that &quot;lazyfree&quot; obscuring here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This would simply be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (PageAnon(page) &amp;&amp; PageSwapBacked &amp;&amp; !PageSwapCache)</span>

I&#39;d agree if we only don&#39;t need the lazyfree variable, but I think we still
need to check it in other places. More in below.
<span class="quote">
&gt; &gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt; &gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; &gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we actually need to filter for mapping || lazyfree? If we fail to</span>
<span class="quote">&gt; allocate swap, we don&#39;t reach here. If the page is a truncated file</span>
<span class="quote">&gt; page, ttu returns pretty much instantly with SWAP_AGAIN. We should be</span>
<span class="quote">&gt; able to just check for page_mapped() alone, no?</span>

checking the mapping is faster than running into try_to_unamp, right?
<span class="quote">
&gt; &gt;  			switch (ret = try_to_unmap(page, lazyfree ?</span>
<span class="quote">&gt; &gt;  				(ttu_flags | TTU_BATCH_FLUSH | TTU_LZFREE) :</span>
<span class="quote">&gt; &gt;  				(ttu_flags | TTU_BATCH_FLUSH))) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That bit I don&#39;t understand. Why do we need to pass TTU_LZFREE? What</span>
<span class="quote">&gt; information does that carry that cannot be gathered from inside ttu?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I.e. when ttu runs into PageAnon, can it simply check !PageSwapBacked?</span>
<span class="quote">&gt; And if it&#39;s still clean, it can lazyfreed++; goto discard.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Am I overlooking something?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -1154,7 +1156,14 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt; &gt;  				goto cull_mlocked;</span>
<span class="quote">&gt; &gt;  			case SWAP_LZFREE:</span>
<span class="quote">&gt; &gt; -				goto lazyfree;</span>
<span class="quote">&gt; &gt; +				/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; &gt; +				if (page_ref_freeze(page, 1)) {</span>
<span class="quote">&gt; &gt; +					if (!PageDirty(page))</span>
<span class="quote">&gt; &gt; +						goto lazyfree;</span>
<span class="quote">&gt; &gt; +					else</span>
<span class="quote">&gt; &gt; +						page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; &gt; +				}</span>
<span class="quote">&gt; &gt; +				goto keep_locked;</span>
<span class="quote">&gt; &gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt; &gt;  				; /* try to free the page below */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is a similar situation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we let the page go through the regular __remove_mapping() process</span>
<span class="quote">&gt; and simply have that function check for PageAnon &amp;&amp; !PageSwapBacked?</span>

That will make the code more complicated. We don&#39;t call __remove_mapping if
!mapping. And we need to do bypass in __remove_mapping, for example, avoid
taking mapping-&gt;lock.
<span class="quote"> 
&gt; &gt; @@ -1266,10 +1275,9 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  			}</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -lazyfree:</span>
<span class="quote">&gt; &gt;  		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt; &gt;  			goto keep_locked;</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; +lazyfree:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... eliminating this special casing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -1294,6 +1302,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  cull_mlocked:</span>
<span class="quote">&gt; &gt;  		if (PageSwapCache(page))</span>
<span class="quote">&gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why cancel the MADV_FREE state? The combination seems non-sensical,</span>
<span class="quote">&gt; but we can simply retain the invalidated state while the page goes to</span>
<span class="quote">&gt; the unevictable list; munlock should move it back to inactive_file.</span>

This depends on the policy. If user locks the page, I think it&#39;s reasonable to
assume the page is hot, so it doesn&#39;t make sense to treat the page lazyfree.
<span class="quote"> 
&gt; &gt;  		unlock_page(page);</span>
<span class="quote">&gt; &gt;  		list_add(&amp;page-&gt;lru, &amp;ret_pages);</span>
<span class="quote">&gt; &gt;  		continue;</span>
<span class="quote">&gt; &gt; @@ -1303,6 +1313,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))</span>
<span class="quote">&gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt;  		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="quote">&gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is similar too.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we leave simply leave the page alone here? The only way we get to</span>
<span class="quote">&gt; this point is if somebody is reading the invalidated page. It&#39;s weird</span>
<span class="quote">&gt; for a lazyfreed page to become active, but it doesn&#39;t seem to warrant</span>
<span class="quote">&gt; active intervention here.</span>

So the unmap fails here probably because the page is dirty, which means the
page is written recently. It makes sense to assume the page is hot.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 17, 2017, 5:41 a.m.</div>
<pre class="content">
Hi Johannes,

On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:
<span class="quote">&gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt; &gt;  				page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; &gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; -				goto discard;</span>
<span class="quote">&gt; &gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; &gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; +					goto discard;</span>
<span class="quote">&gt; &gt; +				} else {</span>
<span class="quote">&gt; &gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; &gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; +					break;</span>
<span class="quote">&gt; &gt; +				}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t understand why we need the TTU_LZFREE bit in general. More on</span>
<span class="quote">&gt; that below at the callsite.</span>

The reason I introduced it was ttu is used for migration/THP split path
as well as reclaim. It&#39;s clear to discard them in reclaim path because
it means surely memory pressure now but not sure with other path.

If you guys think it&#39;s always win to discard them in try_to_unmap
unconditionally, I think it would be better to be separate patch.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt; &gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt; &gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; &gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we need this? MADV_FREE clears the dirty bit off the page; we could</span>
<span class="quote">&gt; just let them go through with the function without any special-casing.</span>

I thought some driver potentially can do GUP with FOLL_TOUCH so that the
lazyfree page can have PG_dirty with !PG_swapbacked. In this case,
throttling logic of shrink_page_list can be confused?
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; @@ -986,6 +986,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		sc-&gt;nr_scanned++;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +		lazyfree = page_is_lazyfree(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  		if (unlikely(!page_evictable(page)))</span>
<span class="quote">&gt; &gt;  			goto cull_mlocked;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -993,7 +995,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  			goto keep_locked;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; &gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; &gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp; !lazyfree)</span>
<span class="quote">&gt; &gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; &gt; @@ -1119,13 +1121,13 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  		/*</span>
<span class="quote">&gt; &gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt; &gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; &gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; &gt; +		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp; !lazyfree) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; lazyfree duplicates the anon check. As per the previous email, IMO it</span>
<span class="quote">&gt; would be much preferable to get rid of that &quot;lazyfree&quot; obscuring here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This would simply be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (PageAnon(page) &amp;&amp; PageSwapBacked &amp;&amp; !PageSwapCache)</span>

Agree.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt; &gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; &gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we actually need to filter for mapping || lazyfree? If we fail to</span>
<span class="quote">&gt; allocate swap, we don&#39;t reach here. If the page is a truncated file</span>
<span class="quote">&gt; page, ttu returns pretty much instantly with SWAP_AGAIN. We should be</span>
<span class="quote">&gt; able to just check for page_mapped() alone, no?</span>

try_to_unmap_one assumes every anonymous pages reached will have swp_entry
so it should be changed to check PageSwapCache if we go to the way.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  			switch (ret = try_to_unmap(page, lazyfree ?</span>
<span class="quote">&gt; &gt;  				(ttu_flags | TTU_BATCH_FLUSH | TTU_LZFREE) :</span>
<span class="quote">&gt; &gt;  				(ttu_flags | TTU_BATCH_FLUSH))) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That bit I don&#39;t understand. Why do we need to pass TTU_LZFREE? What</span>
<span class="quote">&gt; information does that carry that cannot be gathered from inside ttu?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I.e. when ttu runs into PageAnon, can it simply check !PageSwapBacked?</span>
<span class="quote">&gt; And if it&#39;s still clean, it can lazyfreed++; goto discard.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Am I overlooking something?</span>

As I said above, TTU_LZFREE signals when we should discard the page and
in my implementation, I thought it was only shrink_page_list which is
event for memory pressure.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 17, 2017, 5:45 a.m.</div>
<pre class="content">
Hi Shaohua,

On Thu, Feb 16, 2017 at 04:27:18PM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt; &gt; &gt;  				page);</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; &gt; &gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; -				goto discard;</span>
<span class="quote">&gt; &gt; &gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; &gt; &gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; &gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; +					goto discard;</span>
<span class="quote">&gt; &gt; &gt; +				} else {</span>
<span class="quote">&gt; &gt; &gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; &gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; &gt; &gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; &gt; +					break;</span>
<span class="quote">&gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t understand why we need the TTU_LZFREE bit in general. More on</span>
<span class="quote">&gt; &gt; that below at the callsite.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sounds useless flag, don&#39;t see any reason we shouldn&#39;t free the MADV_FREE page</span>
<span class="quote">&gt; in places other than reclaim. Looks TTU_UNMAP is useless too..</span>

Agree on TTU_UNMAP but for example, THP split doesn&#39;t mean free lazyfree pages,
I think.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 17, 2017, 9:27 a.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 02:41:08PM +0900, Minchan Kim wrote:
<span class="quote">&gt; Hi Johannes,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt; &gt; &gt;  				page);</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; &gt; &gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; -				goto discard;</span>
<span class="quote">&gt; &gt; &gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; &gt; &gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; &gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; +					goto discard;</span>
<span class="quote">&gt; &gt; &gt; +				} else {</span>
<span class="quote">&gt; &gt; &gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; &gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; &gt; &gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; &gt; +					break;</span>
<span class="quote">&gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t understand why we need the TTU_LZFREE bit in general. More on</span>
<span class="quote">&gt; &gt; that below at the callsite.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The reason I introduced it was ttu is used for migration/THP split path</span>
<span class="quote">&gt; as well as reclaim. It&#39;s clear to discard them in reclaim path because</span>
<span class="quote">&gt; it means surely memory pressure now but not sure with other path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you guys think it&#39;s always win to discard them in try_to_unmap</span>
<span class="quote">&gt; unconditionally, I think it would be better to be separate patch.</span>

I was totally wrong.

Anon page with THP split/migration/HWPoison will not reach to discard path
in try_to_unmap_one so Johannes is right. We don&#39;t need TTU_LZFREE.

Sorry for the noise.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 17, 2017, 4:01 p.m.</div>
<pre class="content">
On Thu, Feb 16, 2017 at 04:27:18PM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt; &gt; &gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt; &gt; &gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; &gt; &gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we need this? MADV_FREE clears the dirty bit off the page; we could</span>
<span class="quote">&gt; &gt; just let them go through with the function without any special-casing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; this is just to zero dirty and writeback</span>

Okay, I assumed that the page would always be !dirty &amp;&amp; !writeback
here anyway, so we might as well fall through and check those bits.

But a previously failed TTU might have moved a pte dirty bit to the
page, so yes, we do need to filter for anon &amp;&amp; !swapbacked here.
<span class="quote">
&gt; &gt; &gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt; &gt; &gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt; &gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; &gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; &gt; &gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we actually need to filter for mapping || lazyfree? If we fail to</span>
<span class="quote">&gt; &gt; allocate swap, we don&#39;t reach here. If the page is a truncated file</span>
<span class="quote">&gt; &gt; page, ttu returns pretty much instantly with SWAP_AGAIN. We should be</span>
<span class="quote">&gt; &gt; able to just check for page_mapped() alone, no?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; checking the mapping is faster than running into try_to_unamp, right?</span>

!mapping should be a rare case. In reclaim code, I think it&#39;s better
to keep it simple than to optimize away the rare function call.
<span class="quote">
&gt; &gt; &gt; @@ -1154,7 +1156,14 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt; &gt; &gt;  				goto cull_mlocked;</span>
<span class="quote">&gt; &gt; &gt;  			case SWAP_LZFREE:</span>
<span class="quote">&gt; &gt; &gt; -				goto lazyfree;</span>
<span class="quote">&gt; &gt; &gt; +				/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; &gt; &gt; +				if (page_ref_freeze(page, 1)) {</span>
<span class="quote">&gt; &gt; &gt; +					if (!PageDirty(page))</span>
<span class="quote">&gt; &gt; &gt; +						goto lazyfree;</span>
<span class="quote">&gt; &gt; &gt; +					else</span>
<span class="quote">&gt; &gt; &gt; +						page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; +				goto keep_locked;</span>
<span class="quote">&gt; &gt; &gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt; &gt; &gt;  				; /* try to free the page below */</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is a similar situation.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we let the page go through the regular __remove_mapping() process</span>
<span class="quote">&gt; &gt; and simply have that function check for PageAnon &amp;&amp; !PageSwapBacked?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That will make the code more complicated. We don&#39;t call __remove_mapping if</span>
<span class="quote">&gt; !mapping. And we need to do bypass in __remove_mapping, for example, avoid</span>
<span class="quote">&gt; taking mapping-&gt;lock.</span>

True, we won&#39;t get around a separate freeing path as long as the
refcount handling is intertwined with the mapping removal like that :/

What we should be able to do, however, is remove at least SWAP_LZFREE
and stick with SWAP_SUCCESS. On success, we can fall through up until
we do the __remove_mapping call. The page isn&#39;t dirty, so we skip that
PageDirty block; the page doesn&#39;t have private data, so we skip that
block too. And then we can branch on PageAnon &amp;&amp; !PageSwapBacked that
does our alternate freeing path or __remove_mapping for others.
<span class="quote">
&gt; &gt; &gt; @@ -1294,6 +1302,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt;  cull_mlocked:</span>
<span class="quote">&gt; &gt; &gt;  		if (PageSwapCache(page))</span>
<span class="quote">&gt; &gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Why cancel the MADV_FREE state? The combination seems non-sensical,</span>
<span class="quote">&gt; &gt; but we can simply retain the invalidated state while the page goes to</span>
<span class="quote">&gt; &gt; the unevictable list; munlock should move it back to inactive_file.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This depends on the policy. If user locks the page, I think it&#39;s reasonable to</span>
<span class="quote">&gt; assume the page is hot, so it doesn&#39;t make sense to treat the page lazyfree.</span>

I think the key issue is whether the page contains valid data, not
whether it is hot. When we clear the dirty bits along with
PageSwapBacked, we&#39;re declaring the data in the page invalid. There is
no practical usecase to mlock a page with invalid data, sure, but the
act of mlocking a page doesn&#39;t make its contents suddenly valid again.

I.e. I&#39;d stick with the pure data integrity perspective here. That&#39;s
clearer and less error prone than intermingling it with eviction
policy, to avoid accidents where we lose valid data.
<span class="quote">
&gt; &gt; &gt;  		unlock_page(page);</span>
<span class="quote">&gt; &gt; &gt;  		list_add(&amp;page-&gt;lru, &amp;ret_pages);</span>
<span class="quote">&gt; &gt; &gt;  		continue;</span>
<span class="quote">&gt; &gt; &gt; @@ -1303,6 +1313,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt;  		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))</span>
<span class="quote">&gt; &gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; &gt;  		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="quote">&gt; &gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is similar too.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we leave simply leave the page alone here? The only way we get to</span>
<span class="quote">&gt; &gt; this point is if somebody is reading the invalidated page. It&#39;s weird</span>
<span class="quote">&gt; &gt; for a lazyfreed page to become active, but it doesn&#39;t seem to warrant</span>
<span class="quote">&gt; &gt; active intervention here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the unmap fails here probably because the page is dirty, which means the</span>
<span class="quote">&gt; page is written recently. It makes sense to assume the page is hot.</span>

Ah, good point.

But can we handle that explicitly please? Like above, I don&#39;t want to
undo the data invalidation just because somebody read the invalid data
a bunch of times and it has the access bits set. We should only re-set
the PageSwapBacked based on whether the page is actually dirty.

Maybe along the lines of SWAP_MLOCK we could add SWAP_DIRTY when TTU
fails because the page is dirty, and then have a cull_dirty: label in
shrink_page_list handle the lazy rescue of a reused MADV_FREE page?

This should work well with removing the mapping || lazyfree check when
calling TTU. Then TTU can fail on dirty &amp;&amp; !mapping, which is a much
more obvious way of expressing it IMO - &quot;This page contains valid data
but there is no mapping that backs it once we unmap it. Abort.&quot;

That&#39;s mostly why I&#39;m in favor of removing the idea of a &quot;lazyfree&quot;
page as much as possible. IMO this whole thing becomes much more
understandable - and less bolted on to the side of the VM - when we
express it in existing concepts the VM uses for data integrity.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 17, 2017, 4:11 p.m.</div>
<pre class="content">
Hi Minchan,

On Fri, Feb 17, 2017 at 02:45:55PM +0900, Minchan Kim wrote:
<span class="quote">&gt; On Thu, Feb 16, 2017 at 04:27:18PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1419,11 +1419,18 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt; &gt; &gt; &gt;  				page);</span>
<span class="quote">&gt; &gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; &gt; -			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; -				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; &gt; -				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; &gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; &gt; -				goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; +			if (flags &amp; TTU_LZFREE) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt; &gt; &gt; &gt; +					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; &gt; +					rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt; &gt; &gt; +					goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; +				} else {</span>
<span class="quote">&gt; &gt; &gt; &gt; +					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; &gt; &gt; +					ret = SWAP_FAIL;</span>
<span class="quote">&gt; &gt; &gt; &gt; +					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; &gt; &gt; +					break;</span>
<span class="quote">&gt; &gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I don&#39;t understand why we need the TTU_LZFREE bit in general. More on</span>
<span class="quote">&gt; &gt; &gt; that below at the callsite.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sounds useless flag, don&#39;t see any reason we shouldn&#39;t free the MADV_FREE page</span>
<span class="quote">&gt; &gt; in places other than reclaim. Looks TTU_UNMAP is useless too..</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agree on TTU_UNMAP but for example, THP split doesn&#39;t mean free lazyfree pages,</span>
<span class="quote">&gt; I think.</span>

Anon THP splitting uses the migration branch, so we should be fine.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 17, 2017, 4:15 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 02:41:08PM +0900, Minchan Kim wrote:
<span class="quote">&gt; Hi Johannes,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt; &gt; &gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt; &gt; &gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; &gt; &gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we need this? MADV_FREE clears the dirty bit off the page; we could</span>
<span class="quote">&gt; &gt; just let them go through with the function without any special-casing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought some driver potentially can do GUP with FOLL_TOUCH so that the</span>
<span class="quote">&gt; lazyfree page can have PG_dirty with !PG_swapbacked. In this case,</span>
<span class="quote">&gt; throttling logic of shrink_page_list can be confused?</span>

Yep, agreed. We should filter these pages here.
<span class="quote">
&gt; &gt; &gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt; &gt; &gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt; &gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; &gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; &gt; &gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we actually need to filter for mapping || lazyfree? If we fail to</span>
<span class="quote">&gt; &gt; allocate swap, we don&#39;t reach here. If the page is a truncated file</span>
<span class="quote">&gt; &gt; page, ttu returns pretty much instantly with SWAP_AGAIN. We should be</span>
<span class="quote">&gt; &gt; able to just check for page_mapped() alone, no?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; try_to_unmap_one assumes every anonymous pages reached will have swp_entry</span>
<span class="quote">&gt; so it should be changed to check PageSwapCache if we go to the way.</span>

Yep, I think it should check page_mapping(). To me that would make the
most sense, see other email: &quot;Don&#39;t unmap a ram page with valid data
when there is no secondary storage mapping to maintain integrity.&quot;
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 17, 2017, 6:43 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 11:01:54AM -0500, Johannes Weiner wrote:
<span class="quote">&gt; On Thu, Feb 16, 2017 at 04:27:18PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -911,7 +911,7 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt; &gt; &gt; &gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt; &gt; &gt; &gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt; &gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Do we need this? MADV_FREE clears the dirty bit off the page; we could</span>
<span class="quote">&gt; &gt; &gt; just let them go through with the function without any special-casing.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; this is just to zero dirty and writeback</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Okay, I assumed that the page would always be !dirty &amp;&amp; !writeback</span>
<span class="quote">&gt; here anyway, so we might as well fall through and check those bits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But a previously failed TTU might have moved a pte dirty bit to the</span>
<span class="quote">&gt; page, so yes, we do need to filter for anon &amp;&amp; !swapbacked here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1142,7 +1144,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt; &gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt; &gt; &gt; &gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt; &gt; &gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; &gt; &gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Do we actually need to filter for mapping || lazyfree? If we fail to</span>
<span class="quote">&gt; &gt; &gt; allocate swap, we don&#39;t reach here. If the page is a truncated file</span>
<span class="quote">&gt; &gt; &gt; page, ttu returns pretty much instantly with SWAP_AGAIN. We should be</span>
<span class="quote">&gt; &gt; &gt; able to just check for page_mapped() alone, no?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; checking the mapping is faster than running into try_to_unamp, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; !mapping should be a rare case. In reclaim code, I think it&#39;s better</span>
<span class="quote">&gt; to keep it simple than to optimize away the rare function call.</span>

ok 
<span class="quote">&gt; &gt; &gt; &gt; @@ -1154,7 +1156,14 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt; &gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt; &gt; &gt; &gt;  				goto cull_mlocked;</span>
<span class="quote">&gt; &gt; &gt; &gt;  			case SWAP_LZFREE:</span>
<span class="quote">&gt; &gt; &gt; &gt; -				goto lazyfree;</span>
<span class="quote">&gt; &gt; &gt; &gt; +				/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; &gt; &gt; &gt; +				if (page_ref_freeze(page, 1)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +					if (!PageDirty(page))</span>
<span class="quote">&gt; &gt; &gt; &gt; +						goto lazyfree;</span>
<span class="quote">&gt; &gt; &gt; &gt; +					else</span>
<span class="quote">&gt; &gt; &gt; &gt; +						page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; &gt; &gt; &gt; +				}</span>
<span class="quote">&gt; &gt; &gt; &gt; +				goto keep_locked;</span>
<span class="quote">&gt; &gt; &gt; &gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt; &gt; &gt; &gt;  				; /* try to free the page below */</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This is a similar situation.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Can we let the page go through the regular __remove_mapping() process</span>
<span class="quote">&gt; &gt; &gt; and simply have that function check for PageAnon &amp;&amp; !PageSwapBacked?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That will make the code more complicated. We don&#39;t call __remove_mapping if</span>
<span class="quote">&gt; &gt; !mapping. And we need to do bypass in __remove_mapping, for example, avoid</span>
<span class="quote">&gt; &gt; taking mapping-&gt;lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; True, we won&#39;t get around a separate freeing path as long as the</span>
<span class="quote">&gt; refcount handling is intertwined with the mapping removal like that :/</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What we should be able to do, however, is remove at least SWAP_LZFREE</span>
<span class="quote">&gt; and stick with SWAP_SUCCESS. On success, we can fall through up until</span>
<span class="quote">&gt; we do the __remove_mapping call. The page isn&#39;t dirty, so we skip that</span>
<span class="quote">&gt; PageDirty block; the page doesn&#39;t have private data, so we skip that</span>
<span class="quote">&gt; block too. And then we can branch on PageAnon &amp;&amp; !PageSwapBacked that</span>
<span class="quote">&gt; does our alternate freeing path or __remove_mapping for others.</span>

Sounds good 
<span class="quote">&gt; &gt; &gt; &gt; @@ -1294,6 +1302,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt; &gt;  cull_mlocked:</span>
<span class="quote">&gt; &gt; &gt; &gt;  		if (PageSwapCache(page))</span>
<span class="quote">&gt; &gt; &gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; &gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Why cancel the MADV_FREE state? The combination seems non-sensical,</span>
<span class="quote">&gt; &gt; &gt; but we can simply retain the invalidated state while the page goes to</span>
<span class="quote">&gt; &gt; &gt; the unevictable list; munlock should move it back to inactive_file.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This depends on the policy. If user locks the page, I think it&#39;s reasonable to</span>
<span class="quote">&gt; &gt; assume the page is hot, so it doesn&#39;t make sense to treat the page lazyfree.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the key issue is whether the page contains valid data, not</span>
<span class="quote">&gt; whether it is hot. When we clear the dirty bits along with</span>
<span class="quote">&gt; PageSwapBacked, we&#39;re declaring the data in the page invalid. There is</span>
<span class="quote">&gt; no practical usecase to mlock a page with invalid data, sure, but the</span>
<span class="quote">&gt; act of mlocking a page doesn&#39;t make its contents suddenly valid again.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I.e. I&#39;d stick with the pure data integrity perspective here. That&#39;s</span>
<span class="quote">&gt; clearer and less error prone than intermingling it with eviction</span>
<span class="quote">&gt; policy, to avoid accidents where we lose valid data.</span>

The problem is if user mlock the page, it&#39;s very likely the user will dirty the
page soon. After the page is munlocked, putting it back to layyfree means page
reclaim will waste time to reclaim the page because it&#39;s dirty. But that said,
I don&#39;t argue about this. It&#39;s a rare case, so either way is ok to me.
<span class="quote">
&gt; &gt; &gt; &gt;  		unlock_page(page);</span>
<span class="quote">&gt; &gt; &gt; &gt;  		list_add(&amp;page-&gt;lru, &amp;ret_pages);</span>
<span class="quote">&gt; &gt; &gt; &gt;  		continue;</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1303,6 +1313,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt; &gt;  		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))</span>
<span class="quote">&gt; &gt; &gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; &gt; &gt;  		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="quote">&gt; &gt; &gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; &gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This is similar too.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Can we leave simply leave the page alone here? The only way we get to</span>
<span class="quote">&gt; &gt; &gt; this point is if somebody is reading the invalidated page. It&#39;s weird</span>
<span class="quote">&gt; &gt; &gt; for a lazyfreed page to become active, but it doesn&#39;t seem to warrant</span>
<span class="quote">&gt; &gt; &gt; active intervention here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So the unmap fails here probably because the page is dirty, which means the</span>
<span class="quote">&gt; &gt; page is written recently. It makes sense to assume the page is hot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah, good point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But can we handle that explicitly please? Like above, I don&#39;t want to</span>
<span class="quote">&gt; undo the data invalidation just because somebody read the invalid data</span>
<span class="quote">&gt; a bunch of times and it has the access bits set. We should only re-set</span>
<span class="quote">&gt; the PageSwapBacked based on whether the page is actually dirty.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe along the lines of SWAP_MLOCK we could add SWAP_DIRTY when TTU</span>
<span class="quote">&gt; fails because the page is dirty, and then have a cull_dirty: label in</span>
<span class="quote">&gt; shrink_page_list handle the lazy rescue of a reused MADV_FREE page?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should work well with removing the mapping || lazyfree check when</span>
<span class="quote">&gt; calling TTU. Then TTU can fail on dirty &amp;&amp; !mapping, which is a much</span>
<span class="quote">&gt; more obvious way of expressing it IMO - &quot;This page contains valid data</span>
<span class="quote">&gt; but there is no mapping that backs it once we unmap it. Abort.&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s mostly why I&#39;m in favor of removing the idea of a &quot;lazyfree&quot;</span>
<span class="quote">&gt; page as much as possible. IMO this whole thing becomes much more</span>
<span class="quote">&gt; understandable - and less bolted on to the side of the VM - when we</span>
<span class="quote">&gt; express it in existing concepts the VM uses for data integrity.</span>

Ok, it makes sense to only reset the PageSwapBacked bit for dirty page. In this
way, we jump to activate_locked for SWAP_DIRTY || (SWAP_FAIL &amp;&amp; pagelazyfree)
and jump to activate_locked for SWAP_FAIL &amp;&amp; !pagelazyfree. Is this what you
want to do? This will add extra checks for SWAP_FAIL. I&#39;m not sure if this is
really worthy because it&#39;s rare the MADV_FREE page is read.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 17, 2017, 8:03 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 10:43:41AM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 11:01:54AM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Thu, Feb 16, 2017 at 04:27:18PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Feb 16, 2017 at 01:40:18PM -0500, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Tue, Feb 14, 2017 at 11:36:09AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		unlock_page(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		list_add(&amp;page-&gt;lru, &amp;ret_pages);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		continue;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1303,6 +1313,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  			try_to_free_swap(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +		if (lazyfree)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +			clear_page_lazyfree(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Can we leave simply leave the page alone here? The only way we get to</span>
<span class="quote">&gt; &gt; &gt; &gt; this point is if somebody is reading the invalidated page. It&#39;s weird</span>
<span class="quote">&gt; &gt; &gt; &gt; for a lazyfreed page to become active, but it doesn&#39;t seem to warrant</span>
<span class="quote">&gt; &gt; &gt; &gt; active intervention here.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; So the unmap fails here probably because the page is dirty, which means the</span>
<span class="quote">&gt; &gt; &gt; page is written recently. It makes sense to assume the page is hot.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ah, good point.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But can we handle that explicitly please? Like above, I don&#39;t want to</span>
<span class="quote">&gt; &gt; undo the data invalidation just because somebody read the invalid data</span>
<span class="quote">&gt; &gt; a bunch of times and it has the access bits set. We should only re-set</span>
<span class="quote">&gt; &gt; the PageSwapBacked based on whether the page is actually dirty.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Maybe along the lines of SWAP_MLOCK we could add SWAP_DIRTY when TTU</span>
<span class="quote">&gt; &gt; fails because the page is dirty, and then have a cull_dirty: label in</span>
<span class="quote">&gt; &gt; shrink_page_list handle the lazy rescue of a reused MADV_FREE page?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This should work well with removing the mapping || lazyfree check when</span>
<span class="quote">&gt; &gt; calling TTU. Then TTU can fail on dirty &amp;&amp; !mapping, which is a much</span>
<span class="quote">&gt; &gt; more obvious way of expressing it IMO - &quot;This page contains valid data</span>
<span class="quote">&gt; &gt; but there is no mapping that backs it once we unmap it. Abort.&quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s mostly why I&#39;m in favor of removing the idea of a &quot;lazyfree&quot;</span>
<span class="quote">&gt; &gt; page as much as possible. IMO this whole thing becomes much more</span>
<span class="quote">&gt; &gt; understandable - and less bolted on to the side of the VM - when we</span>
<span class="quote">&gt; &gt; express it in existing concepts the VM uses for data integrity.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, it makes sense to only reset the PageSwapBacked bit for dirty page. In this</span>
<span class="quote">&gt; way, we jump to activate_locked for SWAP_DIRTY || (SWAP_FAIL &amp;&amp; pagelazyfree)</span>
<span class="quote">&gt; and jump to activate_locked for SWAP_FAIL &amp;&amp; !pagelazyfree. Is this what you</span>
<span class="quote">&gt; want to do? This will add extra checks for SWAP_FAIL. I&#39;m not sure if this is</span>
<span class="quote">&gt; really worthy because it&#39;s rare the MADV_FREE page is read.</span>

Yes, for SWAP_DIRTY jump to activate_locked or have its own label that
sets PG_swapbacked again and moves the page back to the proper LRU.

SWAP_FAIL of an anon &amp;&amp; !swapbacked &amp;&amp; !dirty &amp;&amp; referenced page can
be ignored IMO. This happens only when the user is reading invalid
data over and over, I see no reason to optimize for that. We activate
a MADV_FREE page, which is weird, but not a correctness issue, right?

Just to clarify, right now we have this:

---

SWAP_FAIL (failure on pte, swap, lazyfree):
  if pagelazyfree:
    clear pagelazyfree
  activate

SWAP_SUCCESS:
  regular reclaim

SWAP_LZFREE (success on lazyfree when page and ptes are all clean):
  free page

---

What I&#39;m proposing is to separate lazyfree failure out from SWAP_FAIL
into its own branch. Then merge lazyfree success into SWAP_SUCCESS:

---

SWAP_FAIL (failure on pte, swap):
  activate

SWAP_SUCCESS:
  if anon &amp;&amp; !swapbacked:
    free manually
  else:
    __remove_mapping()

SWAP_DIRTY (anon &amp;&amp; !swapbacked &amp;&amp; dirty):
  set swapbacked
  putback/activate

---

This way we have a mostly unified success path (we might later be able
to refactor __remove_mapping to split refcounting from mapping stuff
to remove the last trace of difference), and SWAP_DIRTY follows the
same type of delayed LRU fixup as we do for SWAP_MLOCK right now.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 4ddda58..3bb5ad5 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1571,6 +1571,8 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		set_pmd_at(mm, addr, pmd, orig_pmd);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 	}
<span class="p_add">+</span>
<span class="p_add">+	mark_page_lazyfree(page);</span>
 	ret = true;
 out:
 	spin_unlock(ptl);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 639c476..2faed38 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -412,6 +412,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			set_pte_at(mm, addr, pte, ptent);
 			tlb_remove_tlb_entry(tlb, pte, addr);
 		}
<span class="p_add">+		mark_page_lazyfree(page);</span>
 	}
 out:
 	if (nr_swap) {
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index af50eae..2cbdada 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1419,11 +1419,18 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),
 				page);
 
<span class="p_del">-			if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_LZFREE)) {</span>
<span class="p_del">-				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_del">-				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_del">-				rp-&gt;lazyfreed++;</span>
<span class="p_del">-				goto discard;</span>
<span class="p_add">+			if (flags &amp; TTU_LZFREE) {</span>
<span class="p_add">+				if (!PageDirty(page)) {</span>
<span class="p_add">+					/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_add">+					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_add">+					rp-&gt;lazyfreed++;</span>
<span class="p_add">+					goto discard;</span>
<span class="p_add">+				} else {</span>
<span class="p_add">+					set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="p_add">+					ret = SWAP_FAIL;</span>
<span class="p_add">+					page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="p_add">+					break;</span>
<span class="p_add">+				}</span>
 			}
 
 			if (swap_duplicate(entry) &lt; 0) {
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 26c3b40..435149c 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -911,7 +911,7 @@</span> <span class="p_context"> static void page_check_dirty_writeback(struct page *page,</span>
 	 * Anonymous pages are not handled by flushers and must be written
 	 * from reclaim context. Do not stall reclaim based on them
 	 */
<span class="p_del">-	if (!page_is_file_cache(page)) {</span>
<span class="p_add">+	if (!page_is_file_cache(page) || page_is_lazyfree(page)) {</span>
 		*dirty = false;
 		*writeback = false;
 		return;
<span class="p_chunk">@@ -971,7 +971,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		int may_enter_fs;
 		enum page_references references = PAGEREF_RECLAIM_CLEAN;
 		bool dirty, writeback;
<span class="p_del">-		bool lazyfree = false;</span>
<span class="p_add">+		bool lazyfree;</span>
 		int ret = SWAP_SUCCESS;
 
 		cond_resched();
<span class="p_chunk">@@ -986,6 +986,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 
 		sc-&gt;nr_scanned++;
 
<span class="p_add">+		lazyfree = page_is_lazyfree(page);</span>
<span class="p_add">+</span>
 		if (unlikely(!page_evictable(page)))
 			goto cull_mlocked;
 
<span class="p_chunk">@@ -993,7 +995,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			goto keep_locked;
 
 		/* Double the slab pressure for mapped and swapcache pages */
<span class="p_del">-		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="p_add">+		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp; !lazyfree)</span>
 			sc-&gt;nr_scanned++;
 
 		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||
<span class="p_chunk">@@ -1119,13 +1121,13 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		/*
 		 * Anonymous process memory has backing store?
 		 * Try to allocate it some swap space here.
<span class="p_add">+		 * Lazyfree page could be freed directly</span>
 		 */
<span class="p_del">-		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="p_add">+		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp; !lazyfree) {</span>
 			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
 				goto activate_locked;
<span class="p_del">-			lazyfree = true;</span>
 			may_enter_fs = 1;
 
 			/* Adding to swap updated mapping */
<span class="p_chunk">@@ -1142,7 +1144,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * The page is mapped into the page tables of one or more
 		 * processes. Try to unmap it here.
 		 */
<span class="p_del">-		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="p_add">+		if (page_mapped(page) &amp;&amp; (mapping || lazyfree)) {</span>
 			switch (ret = try_to_unmap(page, lazyfree ?
 				(ttu_flags | TTU_BATCH_FLUSH | TTU_LZFREE) :
 				(ttu_flags | TTU_BATCH_FLUSH))) {
<span class="p_chunk">@@ -1154,7 +1156,14 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			case SWAP_MLOCK:
 				goto cull_mlocked;
 			case SWAP_LZFREE:
<span class="p_del">-				goto lazyfree;</span>
<span class="p_add">+				/* follow __remove_mapping for reference */</span>
<span class="p_add">+				if (page_ref_freeze(page, 1)) {</span>
<span class="p_add">+					if (!PageDirty(page))</span>
<span class="p_add">+						goto lazyfree;</span>
<span class="p_add">+					else</span>
<span class="p_add">+						page_ref_unfreeze(page, 1);</span>
<span class="p_add">+				}</span>
<span class="p_add">+				goto keep_locked;</span>
 			case SWAP_SUCCESS:
 				; /* try to free the page below */
 			}
<span class="p_chunk">@@ -1266,10 +1275,9 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			}
 		}
 
<span class="p_del">-lazyfree:</span>
 		if (!mapping || !__remove_mapping(mapping, page, true))
 			goto keep_locked;
<span class="p_del">-</span>
<span class="p_add">+lazyfree:</span>
 		/*
 		 * At this point, we have no other references and there is
 		 * no way to pick any more up (removed from LRU, removed
<span class="p_chunk">@@ -1294,6 +1302,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 cull_mlocked:
 		if (PageSwapCache(page))
 			try_to_free_swap(page);
<span class="p_add">+		if (lazyfree)</span>
<span class="p_add">+			clear_page_lazyfree(page);</span>
 		unlock_page(page);
 		list_add(&amp;page-&gt;lru, &amp;ret_pages);
 		continue;
<span class="p_chunk">@@ -1303,6 +1313,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		if (PageSwapCache(page) &amp;&amp; mem_cgroup_swap_full(page))
 			try_to_free_swap(page);
 		VM_BUG_ON_PAGE(PageActive(page), page);
<span class="p_add">+		if (lazyfree)</span>
<span class="p_add">+			clear_page_lazyfree(page);</span>
 		SetPageActive(page);
 		pgactivate++;
 keep_locked:

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



