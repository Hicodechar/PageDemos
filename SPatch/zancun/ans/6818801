
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[01/15] mmu_notifier: add event information to address invalidation v8 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [01/15] mmu_notifier: add event information to address invalidation v8</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 17, 2015, 6:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437159145-6548-2-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6818801/mbox/"
   >mbox</a>
|
   <a href="/patch/6818801/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6818801/">/patch/6818801/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 09E789F380
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:58:32 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6E231207D8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:58:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 936BC207D4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:58:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755024AbbGQS6M (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Jul 2015 14:58:12 -0400
Received: from mx1.redhat.com ([209.132.183.28]:53661 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753583AbbGQSxQ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Jul 2015 14:53:16 -0400
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by mx1.redhat.com (Postfix) with ESMTPS id 9C09031B7A1;
	Fri, 17 Jul 2015 18:53:16 +0000 (UTC)
Received: from localhost.localdomain.com (vpn-56-84.rdu2.redhat.com
	[10.10.56.84])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id t6HIqscC031583; Fri, 17 Jul 2015 14:53:12 -0400
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;, &lt;joro@8bytes.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Johannes Weiner &lt;jweiner@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Airlie &lt;airlied@redhat.com&gt;, Brendan Conoboy &lt;blc@redhat.com&gt;,
	Joe Donohue &lt;jdonohue@redhat.com&gt;, Christophe Harle &lt;charle@nvidia.com&gt;,
	Duncan Poole &lt;dpoole@nvidia.com&gt;, Sherry Cheung &lt;SCheung@nvidia.com&gt;,
	Subhash Gutti &lt;sgutti@nvidia.com&gt;, John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Lucien Dunning &lt;ldunning@nvidia.com&gt;,
	Cameron Buschardt &lt;cabuschardt@nvidia.com&gt;,
	Arvind Gopalakrishnan &lt;arvindg@nvidia.com&gt;,
	Haggai Eran &lt;haggaie@mellanox.com&gt;,
	Shachar Raindel &lt;raindel@mellanox.com&gt;, Liran Liss &lt;liranl@mellanox.com&gt;,
	Roland Dreier &lt;roland@purestorage.com&gt;, Ben Sander &lt;ben.sander@amd.com&gt;,
	Greg Stoner &lt;Greg.Stoner@amd.com&gt;, John Bridgman &lt;John.Bridgman@amd.com&gt;,
	Michael Mantor &lt;Michael.Mantor@amd.com&gt;,
	Paul Blinzer &lt;Paul.Blinzer@amd.com&gt;,
	Leonid Shamis &lt;Leonid.Shamis@amd.com&gt;,
	Laurent Morichetti &lt;Laurent.Morichetti@amd.com&gt;,
	Alexander Deucher &lt;Alexander.Deucher@amd.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
Subject: [PATCH 01/15] mmu_notifier: add event information to address
	invalidation v8
Date: Fri, 17 Jul 2015 14:52:11 -0400
Message-Id: &lt;1437159145-6548-2-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1437159145-6548-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1437159145-6548-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.1 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - July 17, 2015, 6:52 p.m.</div>
<pre class="content">
The event information will be useful for new user of mmu_notifier API.
The event argument differentiate between a vma disappearing, a page
being write protected or simply a page being unmaped. This allow new
user to take different path for different event for instance on unmap
the resource used to track a vma are still valid and should stay around.
While if the event is saying that a vma is being destroy it means that any
resources used to track this vma can be free.

Changed since v1:
  - renamed action into event (updated commit message too).
  - simplified the event names and clarified their usage
    also documenting what exceptation the listener can have in
    respect to each event.

Changed since v2:
  - Avoid crazy name.
  - Do not move code that do not need to move.

Changed since v3:
  - Separate huge page split from mlock/munlock and softdirty.

Changed since v4:
  - Rebase (no other changes).

Changed since v5:
  - Typo fix.
  - Changed zap_page_range from MMU_MUNMAP to MMU_MIGRATE to reflect the
    fact that the address range is still valid just the page backing it
    are no longer.

Changed since v6:
  - try_to_unmap_one() only invalidate when doing migration.
  - Differentiate fork from other case.

Changed since v7:
  - Renamed MMU_HUGE_PAGE_SPLIT to MMU_HUGE_PAGE_SPLIT.
  - Renamed MMU_ISDIRTY to MMU_CLEAR_SOFT_DIRTY.
  - Renamed MMU_WRITE_PROTECT to MMU_KSM_WRITE_PROTECT.
  - English syntax fixes.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c  |   3 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c |   3 +-
 drivers/gpu/drm/radeon/radeon_mn.c      |   3 +-
 drivers/infiniband/core/umem_odp.c      |   9 ++-
 drivers/iommu/amd_iommu_v2.c            |   3 +-
 drivers/misc/sgi-gru/grutlbpurge.c      |   9 ++-
 drivers/xen/gntdev.c                    |   9 ++-
 fs/proc/task_mmu.c                      |   6 +-
 include/linux/mmu_notifier.h            | 132 ++++++++++++++++++++++++++------
 kernel/events/uprobes.c                 |  10 ++-
 mm/huge_memory.c                        |  39 ++++++----
 mm/hugetlb.c                            |  23 +++---
 mm/ksm.c                                |  18 +++--
 mm/memory.c                             |  27 ++++---
 mm/migrate.c                            |   9 ++-
 mm/mmu_notifier.c                       |  28 ++++---
 mm/mprotect.c                           |   6 +-
 mm/mremap.c                             |   6 +-
 mm/rmap.c                               |   4 +-
 virt/kvm/kvm_main.c                     |  12 ++-
 20 files changed, 258 insertions(+), 101 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">index b1969f2..7ca805c 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_chunk">@@ -121,7 +121,8 @@</span> <span class="p_context"> static void amdgpu_mn_release(struct mmu_notifier *mn,</span>
 static void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
 					     unsigned long start,
<span class="p_del">-					     unsigned long end)</span>
<span class="p_add">+					     unsigned long end,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);
 	struct interval_tree_node *it;
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 1f4e5a3..dee1e3d 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -132,7 +132,8 @@</span> <span class="p_context"> restart:</span>
 static void i915_gem_userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 						       struct mm_struct *mm,
 						       unsigned long start,
<span class="p_del">-						       unsigned long end)</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 	struct i915_mmu_notifier *mn = container_of(_mn, struct i915_mmu_notifier, mn);
 	struct interval_tree_node *it = NULL;
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">index eef006c..3a9615b 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_chunk">@@ -121,7 +121,8 @@</span> <span class="p_context"> static void radeon_mn_release(struct mmu_notifier *mn,</span>
 static void radeon_mn_invalidate_range_start(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
 					     unsigned long start,
<span class="p_del">-					     unsigned long end)</span>
<span class="p_add">+					     unsigned long end,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct radeon_mn *rmn = container_of(mn, struct radeon_mn, mn);
 	struct interval_tree_node *it;
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index 40becdb..6ed69fa 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -165,7 +165,8 @@</span> <span class="p_context"> static int invalidate_page_trampoline(struct ib_umem *item, u64 start,</span>
 
 static void ib_umem_notifier_invalidate_page(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
<span class="p_del">-					     unsigned long address)</span>
<span class="p_add">+					     unsigned long address,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_chunk">@@ -192,7 +193,8 @@</span> <span class="p_context"> static int invalidate_range_start_trampoline(struct ib_umem *item, u64 start,</span>
 static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
 						    struct mm_struct *mm,
 						    unsigned long start,
<span class="p_del">-						    unsigned long end)</span>
<span class="p_add">+						    unsigned long end,</span>
<span class="p_add">+						    enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_chunk">@@ -217,7 +219,8 @@</span> <span class="p_context"> static int invalidate_range_end_trampoline(struct ib_umem *item, u64 start,</span>
 static void ib_umem_notifier_invalidate_range_end(struct mmu_notifier *mn,
 						  struct mm_struct *mm,
 						  unsigned long start,
<span class="p_del">-						  unsigned long end)</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">index 3465faf..4aa4de6 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_chunk">@@ -384,7 +384,8 @@</span> <span class="p_context"> static int mn_clear_flush_young(struct mmu_notifier *mn,</span>
 
 static void mn_invalidate_page(struct mmu_notifier *mn,
 			       struct mm_struct *mm,
<span class="p_del">-			       unsigned long address)</span>
<span class="p_add">+			       unsigned long address,</span>
<span class="p_add">+			       enum mmu_event event)</span>
 {
 	__mn_flush_page(mn, address);
 }
<span class="p_header">diff --git a/drivers/misc/sgi-gru/grutlbpurge.c b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_header">index 2129274..e67fed1 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_chunk">@@ -221,7 +221,8 @@</span> <span class="p_context"> void gru_flush_all_tlb(struct gru_state *gru)</span>
  */
 static void gru_invalidate_range_start(struct mmu_notifier *mn,
 				       struct mm_struct *mm,
<span class="p_del">-				       unsigned long start, unsigned long end)</span>
<span class="p_add">+				       unsigned long start, unsigned long end,</span>
<span class="p_add">+				       enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_chunk">@@ -235,7 +236,8 @@</span> <span class="p_context"> static void gru_invalidate_range_start(struct mmu_notifier *mn,</span>
 
 static void gru_invalidate_range_end(struct mmu_notifier *mn,
 				     struct mm_struct *mm, unsigned long start,
<span class="p_del">-				     unsigned long end)</span>
<span class="p_add">+				     unsigned long end,</span>
<span class="p_add">+				     enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_chunk">@@ -248,7 +250,8 @@</span> <span class="p_context"> static void gru_invalidate_range_end(struct mmu_notifier *mn,</span>
 }
 
 static void gru_invalidate_page(struct mmu_notifier *mn, struct mm_struct *mm,
<span class="p_del">-				unsigned long address)</span>
<span class="p_add">+				unsigned long address,</span>
<span class="p_add">+				enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_header">diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c</span>
<span class="p_header">index 67b9163..1afef26 100644</span>
<span class="p_header">--- a/drivers/xen/gntdev.c</span>
<span class="p_header">+++ b/drivers/xen/gntdev.c</span>
<span class="p_chunk">@@ -467,7 +467,9 @@</span> <span class="p_context"> static void unmap_if_in_range(struct grant_map *map,</span>
 
 static void mn_invl_range_start(struct mmu_notifier *mn,
 				struct mm_struct *mm,
<span class="p_del">-				unsigned long start, unsigned long end)</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end,</span>
<span class="p_add">+				enum mmu_event event)</span>
 {
 	struct gntdev_priv *priv = container_of(mn, struct gntdev_priv, mn);
 	struct grant_map *map;
<span class="p_chunk">@@ -484,9 +486,10 @@</span> <span class="p_context"> static void mn_invl_range_start(struct mmu_notifier *mn,</span>
 
 static void mn_invl_page(struct mmu_notifier *mn,
 			 struct mm_struct *mm,
<span class="p_del">-			 unsigned long address)</span>
<span class="p_add">+			 unsigned long address,</span>
<span class="p_add">+			 enum mmu_event event)</span>
 {
<span class="p_del">-	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE);</span>
<span class="p_add">+	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE, event);</span>
 }
 
 static void mn_release(struct mmu_notifier *mn,
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index ca1e091..4c450fa 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -934,11 +934,13 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 				downgrade_write(&amp;mm-&gt;mmap_sem);
 				break;
 			}
<span class="p_del">-			mmu_notifier_invalidate_range_start(mm, 0, -1);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_start(mm, 0, -1,</span>
<span class="p_add">+							MMU_CLEAR_SOFT_DIRTY);</span>
 		}
 		walk_page_range(0, ~0UL, &amp;clear_refs_walk);
 		if (type == CLEAR_REFS_SOFT_DIRTY)
<span class="p_del">-			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_end(mm, 0, -1,</span>
<span class="p_add">+							MMU_CLEAR_SOFT_DIRTY);</span>
 		flush_tlb_mm(mm);
 		up_read(&amp;mm-&gt;mmap_sem);
 out_mm:
<span class="p_header">diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="p_header">index 61cd67f..f9b1e10 100644</span>
<span class="p_header">--- a/include/linux/mmu_notifier.h</span>
<span class="p_header">+++ b/include/linux/mmu_notifier.h</span>
<span class="p_chunk">@@ -9,6 +9,67 @@</span> <span class="p_context"></span>
 struct mmu_notifier;
 struct mmu_notifier_ops;
 
<span class="p_add">+/* MMU Events report fine-grained information to the callback routine, allowing</span>
<span class="p_add">+ * the event listener to make a more informed decision as to what action to</span>
<span class="p_add">+ * take. The event types are:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_FORK a process is forking. This will lead to vmas getting</span>
<span class="p_add">+ *     write-protected, in order to set up COW</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_HUGE_PAGE_SPLIT the pages don&#39;t move, nor does their content change,</span>
<span class="p_add">+ *     but the page table structure is updated (levels added or removed).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_CLEAR_SOFT_DIRTY need to write protect so write properly update the</span>
<span class="p_add">+ *     soft dirty bit of page table entry.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MIGRATE: memory is migrating from one page to another, thus all write</span>
<span class="p_add">+ *     access must stop after invalidate_range_start callback returns.</span>
<span class="p_add">+ *     Furthermore, no read access should be allowed either, as a new page can</span>
<span class="p_add">+ *     be remapped with write access before the invalidate_range_end callback</span>
<span class="p_add">+ *     happens and thus any read access to old page might read stale data. There</span>
<span class="p_add">+ *     are several sources for this event, including:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *         - A page moving to swap (various reasons, including page reclaim),</span>
<span class="p_add">+ *         - An mremap syscall,</span>
<span class="p_add">+ *         - migration for NUMA reasons,</span>
<span class="p_add">+ *         - balancing the memory pool,</span>
<span class="p_add">+ *         - write fault on COW page,</span>
<span class="p_add">+ *         - and more that are not listed here.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MPROT: memory access protection is changing. Refer to the vma to get</span>
<span class="p_add">+ *     the new access protection. All memory access are still valid until the</span>
<span class="p_add">+ *     invalidate_range_end callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MUNLOCK: unlock memory. Content of page table stays the same but</span>
<span class="p_add">+ *     page are unlocked.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MUNMAP: the range is being unmapped (outcome of a munmap syscall or</span>
<span class="p_add">+ *     process destruction). However, access is still allowed, up until the</span>
<span class="p_add">+ *     invalidate_range_free_pages callback. This also implies that secondary</span>
<span class="p_add">+ *     page table can be trimmed, because the address range is no longer valid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_WRITE_BACK: memory is being written back to disk, all write accesses</span>
<span class="p_add">+ *     must stop after invalidate_range_start callback returns. Read access are</span>
<span class="p_add">+ *     still allowed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_KSM_WRITE_PROTECT: memory is being write protected for KSM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If in doubt when adding a new notifier caller, please use MMU_MIGRATE,</span>
<span class="p_add">+ * because it will always lead to reasonable behavior, but will not allow the</span>
<span class="p_add">+ * listener a chance to optimize its events.</span>
<span class="p_add">+ */</span>
<span class="p_add">+enum mmu_event {</span>
<span class="p_add">+	MMU_FORK = 0,</span>
<span class="p_add">+	MMU_HUGE_PAGE_SPLIT,</span>
<span class="p_add">+	MMU_CLEAR_SOFT_DIRTY,</span>
<span class="p_add">+	MMU_MIGRATE,</span>
<span class="p_add">+	MMU_MPROT,</span>
<span class="p_add">+	MMU_MUNLOCK,</span>
<span class="p_add">+	MMU_MUNMAP,</span>
<span class="p_add">+	MMU_WRITE_BACK,</span>
<span class="p_add">+	MMU_KSM_WRITE_PROTECT,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MMU_NOTIFIER
 
 /*
<span class="p_chunk">@@ -82,7 +143,8 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	void (*change_pte)(struct mmu_notifier *mn,
 			   struct mm_struct *mm,
 			   unsigned long address,
<span class="p_del">-			   pte_t pte);</span>
<span class="p_add">+			   pte_t pte,</span>
<span class="p_add">+			   enum mmu_event event);</span>
 
 	/*
 	 * Before this is invoked any secondary MMU is still ok to
<span class="p_chunk">@@ -93,7 +155,8 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	 */
 	void (*invalidate_page)(struct mmu_notifier *mn,
 				struct mm_struct *mm,
<span class="p_del">-				unsigned long address);</span>
<span class="p_add">+				unsigned long address,</span>
<span class="p_add">+				enum mmu_event event);</span>
 
 	/*
 	 * invalidate_range_start() and invalidate_range_end() must be
<span class="p_chunk">@@ -140,10 +203,14 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	 */
 	void (*invalidate_range_start)(struct mmu_notifier *mn,
 				       struct mm_struct *mm,
<span class="p_del">-				       unsigned long start, unsigned long end);</span>
<span class="p_add">+				       unsigned long start,</span>
<span class="p_add">+				       unsigned long end,</span>
<span class="p_add">+				       enum mmu_event event);</span>
 	void (*invalidate_range_end)(struct mmu_notifier *mn,
 				     struct mm_struct *mm,
<span class="p_del">-				     unsigned long start, unsigned long end);</span>
<span class="p_add">+				     unsigned long start,</span>
<span class="p_add">+				     unsigned long end,</span>
<span class="p_add">+				     enum mmu_event event);</span>
 
 	/*
 	 * invalidate_range() is either called between
<span class="p_chunk">@@ -206,13 +273,20 @@</span> <span class="p_context"> extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,</span>
 extern int __mmu_notifier_test_young(struct mm_struct *mm,
 				     unsigned long address);
 extern void __mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-				      unsigned long address, pte_t pte);</span>
<span class="p_add">+				      unsigned long address,</span>
<span class="p_add">+				      pte_t pte,</span>
<span class="p_add">+				      enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address);</span>
<span class="p_add">+					  unsigned long address,</span>
<span class="p_add">+					  enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end);</span>
<span class="p_add">+						  unsigned long start,</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end);</span>
<span class="p_add">+						unsigned long start,</span>
<span class="p_add">+						unsigned long end,</span>
<span class="p_add">+						enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range(struct mm_struct *mm,
 				  unsigned long start, unsigned long end);
 
<span class="p_chunk">@@ -240,31 +314,38 @@</span> <span class="p_context"> static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
 }
 
 static inline void mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-					   unsigned long address, pte_t pte)</span>
<span class="p_add">+					   unsigned long address,</span>
<span class="p_add">+					   pte_t pte,</span>
<span class="p_add">+					   enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_change_pte(mm, address, pte);</span>
<span class="p_add">+		__mmu_notifier_change_pte(mm, address, pte, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+						unsigned long address,</span>
<span class="p_add">+						enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		__mmu_notifier_invalidate_page(mm, address, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						       unsigned long start,</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+		__mmu_notifier_invalidate_range_start(mm, start, end, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						     unsigned long start,</span>
<span class="p_add">+						     unsigned long end,</span>
<span class="p_add">+						     enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+		__mmu_notifier_invalidate_range_end(mm, start, end, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,
<span class="p_chunk">@@ -359,13 +440,13 @@</span> <span class="p_context"> static inline void mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
  * old page would remain mapped readonly in the secondary MMUs after the new
  * page is already writable by some CPU through the primary MMU.
  */
<span class="p_del">-#define set_pte_at_notify(__mm, __address, __ptep, __pte)		\</span>
<span class="p_add">+#define set_pte_at_notify(__mm, __address, __ptep, __pte, __event)	\</span>
 ({									\
 	struct mm_struct *___mm = __mm;					\
 	unsigned long ___address = __address;				\
 	pte_t ___pte = __pte;						\
 									\
<span class="p_del">-	mmu_notifier_change_pte(___mm, ___address, ___pte);		\</span>
<span class="p_add">+	mmu_notifier_change_pte(___mm, ___address, ___pte, __event);	\</span>
 	set_pte_at(___mm, ___address, __ptep, ___pte);			\
 })
 
<span class="p_chunk">@@ -393,22 +474,29 @@</span> <span class="p_context"> static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
 }
 
 static inline void mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-					   unsigned long address, pte_t pte)</span>
<span class="p_add">+					   unsigned long address,</span>
<span class="p_add">+					   pte_t pte,</span>
<span class="p_add">+					   enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+						unsigned long address,</span>
<span class="p_add">+						enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						       unsigned long start,</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						     unsigned long start,</span>
<span class="p_add">+						     unsigned long end,</span>
<span class="p_add">+						     enum mmu_event event)</span>
 {
 }
 
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index cb346f2..802828a 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -176,7 +176,8 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 	/* For try_to_free_swap() and munlock_vma_page() below */
 	lock_page(page);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	err = -EAGAIN;
 	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);
 	if (!ptep)
<span class="p_chunk">@@ -194,7 +195,9 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
 	ptep_clear_flush_notify(vma, addr, ptep);
<span class="p_del">-	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	set_pte_at_notify(mm, addr, ptep,</span>
<span class="p_add">+			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="p_add">+			  MMU_MIGRATE);</span>
 
 	page_remove_rmap(page);
 	if (!page_mapped(page))
<span class="p_chunk">@@ -208,7 +211,8 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 	err = 0;
  unlock:
 	mem_cgroup_cancel_charge(kpage, memcg);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	unlock_page(page);
 	return err;
 }
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index c107094..80131c0 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1024,7 +1024,8 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, orig_pmd)))
<span class="p_chunk">@@ -1058,7 +1059,8 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 	page_remove_rmap(page);
 	spin_unlock(ptl);
 
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	ret |= VM_FAULT_WRITE;
 	put_page(page);
<span class="p_chunk">@@ -1068,7 +1070,8 @@</span> <span class="p_context"> out:</span>
 
 out_free_pages:
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
<span class="p_chunk">@@ -1160,7 +1163,8 @@</span> <span class="p_context"> alloc:</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	spin_lock(ptl);
 	if (page)
<span class="p_chunk">@@ -1192,7 +1196,8 @@</span> <span class="p_context"> alloc:</span>
 	}
 	spin_unlock(ptl);
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 out:
 	return ret;
 out_unlock:
<span class="p_chunk">@@ -1611,7 +1616,8 @@</span> <span class="p_context"> static int __split_huge_page_splitting(struct page *page,</span>
 	const unsigned long mmun_start = address;
 	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_HUGE_PAGE_SPLIT);</span>
 	pmd = page_check_address_pmd(page, mm, address,
 			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &amp;ptl);
 	if (pmd) {
<span class="p_chunk">@@ -1627,7 +1633,8 @@</span> <span class="p_context"> static int __split_huge_page_splitting(struct page *page,</span>
 		ret = 1;
 		spin_unlock(ptl);
 	}
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_HUGE_PAGE_SPLIT);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -2491,7 +2498,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 
 	mmun_start = address;
 	mmun_end   = address + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	pmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */
 	/*
 	 * After this gup_fast can&#39;t run anymore. This also removes
<span class="p_chunk">@@ -2501,7 +2509,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 */
 	_pmd = pmdp_collapse_flush(vma, address, pmd);
 	spin_unlock(pmd_ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	spin_lock(pte_ptl);
 	isolated = __collapse_huge_page_isolate(vma, address, pte);
<span class="p_chunk">@@ -2898,24 +2907,28 @@</span> <span class="p_context"> void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
 again:
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_trans_huge(*pmd))) {
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 		return;
 	}
 	if (is_huge_zero_pmd(*pmd)) {
 		__split_huge_zero_page_pmd(vma, haddr, pmd);
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 		return;
 	}
 	page = pmd_page(*pmd);
 	VM_BUG_ON_PAGE(!page_count(page), page);
 	get_page(page);
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	split_huge_page(page);
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8c3087..2b513e2 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -2749,7 +2749,8 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 	mmun_start = vma-&gt;vm_start;
 	mmun_end = vma-&gt;vm_end;
 	if (cow)
<span class="p_del">-		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(src, mmun_start,</span>
<span class="p_add">+						    mmun_end, MMU_MIGRATE);</span>
 
 	for (addr = vma-&gt;vm_start; addr &lt; vma-&gt;vm_end; addr += sz) {
 		spinlock_t *src_ptl, *dst_ptl;
<span class="p_chunk">@@ -2803,7 +2804,8 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 	}
 
 	if (cow)
<span class="p_del">-		mmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(src, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -2829,7 +2831,8 @@</span> <span class="p_context"> void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	BUG_ON(end &amp; ~huge_page_mask(h));
 
 	tlb_start_vma(tlb, vma);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	address = start;
 again:
 	for (; address &lt; end; address += sz) {
<span class="p_chunk">@@ -2903,7 +2906,8 @@</span> <span class="p_context"> unlock:</span>
 		if (address &lt; end &amp;&amp; !ref_page)
 			goto again;
 	}
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	tlb_end_vma(tlb, vma);
 }
 
<span class="p_chunk">@@ -3082,8 +3086,8 @@</span> <span class="p_context"> retry_avoidcopy:</span>
 
 	mmun_start = address &amp; huge_page_mask(h);
 	mmun_end = mmun_start + huge_page_size(h);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_del">-</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 	/*
 	 * Retake the page table lock to check for racing updates
 	 * before the page tables are altered
<span class="p_chunk">@@ -3104,7 +3108,8 @@</span> <span class="p_context"> retry_avoidcopy:</span>
 		new_page = old_page;
 	}
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_MIGRATE);</span>
 out_release_all:
 	page_cache_release(new_page);
 out_release_old:
<span class="p_chunk">@@ -3572,7 +3577,7 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 	BUG_ON(address &gt;= end);
 	flush_cache_range(vma, address, end);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MPROT);</span>
 	i_mmap_lock_write(vma-&gt;vm_file-&gt;f_mapping);
 	for (; address &lt; end; address += huge_page_size(h)) {
 		spinlock_t *ptl;
<span class="p_chunk">@@ -3622,7 +3627,7 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 	flush_tlb_range(vma, start, end);
 	mmu_notifier_invalidate_range(mm, start, end);
 	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MPROT);</span>
 
 	return pages &lt;&lt; h-&gt;order;
 }
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 7ee101e..eb1b2b5 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -872,7 +872,8 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	mmun_start = addr;
 	mmun_end   = addr + PAGE_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_KSM_WRITE_PROTECT);</span>
 
 	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);
 	if (!ptep)
<span class="p_chunk">@@ -904,7 +905,7 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 		if (pte_dirty(entry))
 			set_page_dirty(page);
 		entry = pte_mkclean(pte_wrprotect(entry));
<span class="p_del">-		set_pte_at_notify(mm, addr, ptep, entry);</span>
<span class="p_add">+		set_pte_at_notify(mm, addr, ptep, entry, MMU_KSM_WRITE_PROTECT);</span>
 	}
 	*orig_pte = *ptep;
 	err = 0;
<span class="p_chunk">@@ -912,7 +913,8 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 out_unlock:
 	pte_unmap_unlock(ptep, ptl);
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_KSM_WRITE_PROTECT);</span>
 out:
 	return err;
 }
<span class="p_chunk">@@ -948,7 +950,8 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	mmun_start = addr;
 	mmun_end   = addr + PAGE_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	ptep = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);
 	if (!pte_same(*ptep, orig_pte)) {
<span class="p_chunk">@@ -961,7 +964,9 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
 	ptep_clear_flush_notify(vma, addr, ptep);
<span class="p_del">-	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	set_pte_at_notify(mm, addr, ptep,</span>
<span class="p_add">+			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="p_add">+			  MMU_MIGRATE);</span>
 
 	page_remove_rmap(page);
 	if (!page_mapped(page))
<span class="p_chunk">@@ -971,7 +976,8 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 	pte_unmap_unlock(ptep, ptl);
 	err = 0;
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_MIGRATE);</span>
 out:
 	return err;
 }
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 388dcf9..1be64ce 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1048,7 +1048,7 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 	mmun_end   = end;
 	if (is_cow)
 		mmu_notifier_invalidate_range_start(src_mm, mmun_start,
<span class="p_del">-						    mmun_end);</span>
<span class="p_add">+						    mmun_end, MMU_FORK);</span>
 
 	ret = 0;
 	dst_pgd = pgd_offset(dst_mm, addr);
<span class="p_chunk">@@ -1065,7 +1065,8 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
 
 	if (is_cow)
<span class="p_del">-		mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(src_mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_FORK);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1335,10 +1336,12 @@</span> <span class="p_context"> void unmap_vmas(struct mmu_gather *tlb,</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start_addr,</span>
<span class="p_add">+					    end_addr, MMU_MUNMAP);</span>
 	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end_addr; vma = vma-&gt;vm_next)
 		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start_addr,</span>
<span class="p_add">+					  end_addr, MMU_MUNMAP);</span>
 }
 
 /**
<span class="p_chunk">@@ -1360,10 +1363,10 @@</span> <span class="p_context"> void zap_page_range(struct vm_area_struct *vma, unsigned long start,</span>
 	lru_add_drain();
 	tlb_gather_mmu(&amp;tlb, mm, start, end);
 	update_hiwater_rss(mm);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MIGRATE);</span>
 	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next)
 		unmap_single_vma(&amp;tlb, vma, start, end, details);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MIGRATE);</span>
 	tlb_finish_mmu(&amp;tlb, start, end);
 }
 
<span class="p_chunk">@@ -1386,9 +1389,9 @@</span> <span class="p_context"> static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr</span>
 	lru_add_drain();
 	tlb_gather_mmu(&amp;tlb, mm, address, end);
 	update_hiwater_rss(mm);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, address, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, address, end, MMU_MUNMAP);</span>
 	unmap_single_vma(&amp;tlb, vma, address, end, details);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, address, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, address, end, MMU_MUNMAP);</span>
 	tlb_finish_mmu(&amp;tlb, address, end);
 }
 
<span class="p_chunk">@@ -2087,7 +2090,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 	__SetPageUptodate(new_page);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 
 	/*
 	 * Re-check the pte - we dropped the lock
<span class="p_chunk">@@ -2120,7 +2124,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * mmu page tables (such as kvm shadow page tables), we want the
 		 * new page to be mapped directly into the secondary page table.
 		 */
<span class="p_del">-		set_pte_at_notify(mm, address, page_table, entry);</span>
<span class="p_add">+		set_pte_at_notify(mm, address, page_table, entry, MMU_MIGRATE);</span>
 		update_mmu_cache(vma, address, page_table);
 		if (old_page) {
 			/*
<span class="p_chunk">@@ -2159,7 +2163,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		page_cache_release(new_page);
 
 	pte_unmap_unlock(page_table, ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	if (old_page) {
 		/*
 		 * Don&#39;t let another task, with possibly unlocked vma,
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index ee401e4..31995b5 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1759,12 +1759,14 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	WARN_ON(PageLRU(new_page));
 
 	/* Recheck the target PMD */
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {
 fail_putback:
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 
 		/* Reverse changes made by migrate_page_copy() */
 		if (TestClearPageActive(new_page))
<span class="p_chunk">@@ -1818,7 +1820,8 @@</span> <span class="p_context"> fail_putback:</span>
 	page_remove_rmap(page);
 
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	/* Take an &quot;isolate&quot; reference and put new page on the LRU. */
 	get_page(new_page);
<span class="p_header">diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="p_header">index 3b9b3d0..e51ea02 100644</span>
<span class="p_header">--- a/mm/mmu_notifier.c</span>
<span class="p_header">+++ b/mm/mmu_notifier.c</span>
<span class="p_chunk">@@ -142,8 +142,10 @@</span> <span class="p_context"> int __mmu_notifier_test_young(struct mm_struct *mm,</span>
 	return young;
 }
 
<span class="p_del">-void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
<span class="p_del">-			       pte_t pte)</span>
<span class="p_add">+void __mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="p_add">+			       unsigned long address,</span>
<span class="p_add">+			       pte_t pte,</span>
<span class="p_add">+			       enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -151,13 +153,14 @@</span> <span class="p_context"> void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;change_pte)
<span class="p_del">-			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte);</span>
<span class="p_add">+			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 
 void __mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+				    unsigned long address,</span>
<span class="p_add">+				    enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -165,13 +168,16 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;invalidate_page)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_page(mn, mm, address);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_page(mn, mm, address, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+					   unsigned long start,</span>
<span class="p_add">+					   unsigned long end,</span>
<span class="p_add">+					   enum mmu_event event)</span>
<span class="p_add">+</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -179,14 +185,17 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;invalidate_range_start)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start, end);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start,</span>
<span class="p_add">+							end, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range_start);
 
 void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+					 unsigned long start,</span>
<span class="p_add">+					 unsigned long end,</span>
<span class="p_add">+					 enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -204,7 +213,8 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
 		if (mn-&gt;ops-&gt;invalidate_range)
 			mn-&gt;ops-&gt;invalidate_range(mn, mm, start, end);
 		if (mn-&gt;ops-&gt;invalidate_range_end)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start, end);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start,</span>
<span class="p_add">+						      end, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index e7d6f11..a57e8af 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -155,7 +155,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		/* invoke the mmu notifier if the pmd is populated */
 		if (!mni_start) {
 			mni_start = addr;
<span class="p_del">-			mmu_notifier_invalidate_range_start(mm, mni_start, end);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_start(mm, mni_start,</span>
<span class="p_add">+							    end, MMU_MPROT);</span>
 		}
 
 		if (pmd_trans_huge(*pmd)) {
<span class="p_chunk">@@ -183,7 +184,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 	} while (pmd++, addr = next, addr != end);
 
 	if (mni_start)
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mni_start, end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mni_start, end,</span>
<span class="p_add">+						  MMU_MPROT);</span>
 
 	if (nr_huge_updates)
 		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index a7c93ec..72051cf 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -176,7 +176,8 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 
 	mmun_start = old_addr;
 	mmun_end   = old_end;
<span class="p_del">-	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 
 	for (; old_addr &lt; old_end; old_addr += extent, new_addr += extent) {
 		cond_resched();
<span class="p_chunk">@@ -228,7 +229,8 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 	if (likely(need_flush))
 		flush_tlb_range(vma, old_end-len, old_addr);
 
<span class="p_del">-	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	return len + old_addr - old_end;	/* how much done */
 }
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 171b687..b1e6eae 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -891,7 +891,7 @@</span> <span class="p_context"> static int page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
 	pte_unmap_unlock(pte, ptl);
 
 	if (ret) {
<span class="p_del">-		mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		mmu_notifier_invalidate_page(mm, address, MMU_WRITE_BACK);</span>
 		(*cleaned)++;
 	}
 out:
<span class="p_chunk">@@ -1298,7 +1298,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 out_unmap:
 	pte_unmap_unlock(pte, ptl);
 	if (ret != SWAP_FAIL &amp;&amp; !(flags &amp; TTU_MUNLOCK))
<span class="p_del">-		mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		mmu_notifier_invalidate_page(mm, address, MMU_MIGRATE);</span>
 out:
 	return ret;
 
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 8b8a444..4dfa91c 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -259,7 +259,8 @@</span> <span class="p_context"> static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)</span>
 
 static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
<span class="p_del">-					     unsigned long address)</span>
<span class="p_add">+					     unsigned long address,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int need_tlb_flush, idx;
<span class="p_chunk">@@ -301,7 +302,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
 					struct mm_struct *mm,
 					unsigned long address,
<span class="p_del">-					pte_t pte)</span>
<span class="p_add">+					pte_t pte,</span>
<span class="p_add">+					enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int idx;
<span class="p_chunk">@@ -317,7 +319,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 						    struct mm_struct *mm,
 						    unsigned long start,
<span class="p_del">-						    unsigned long end)</span>
<span class="p_add">+						    unsigned long end,</span>
<span class="p_add">+						    enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int need_tlb_flush = 0, idx;
<span class="p_chunk">@@ -343,7 +346,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 						  struct mm_struct *mm,
 						  unsigned long start,
<span class="p_del">-						  unsigned long end)</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



