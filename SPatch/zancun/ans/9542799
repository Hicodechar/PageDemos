
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v17,07/14] mm/migrate: migrate_vma() unmap page from vma while collecting pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v17,07/14] mm/migrate: migrate_vma() unmap page from vma while collecting pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 27, 2017, 10:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1485557541-7806-8-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9542799/mbox/"
   >mbox</a>
|
   <a href="/patch/9542799/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9542799/">/patch/9542799/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BDE9460415 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:56:34 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AF3A6204C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:56:34 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A3E33276D6; Fri, 27 Jan 2017 21:56:34 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E7680204C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:56:33 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752014AbdA0V4b (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 27 Jan 2017 16:56:31 -0500
Received: from mx1.redhat.com ([209.132.183.28]:54472 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751178AbdA0Vu4 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 27 Jan 2017 16:50:56 -0500
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 7C576C04B30F;
	Fri, 27 Jan 2017 21:50:56 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id v0RLoj1B020725; Fri, 27 Jan 2017 16:50:55 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM v17 07/14] mm/migrate: migrate_vma() unmap page from vma while
	collecting pages
Date: Fri, 27 Jan 2017 17:52:14 -0500
Message-Id: &lt;1485557541-7806-8-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1485557541-7806-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1485557541-7806-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.31]);
	Fri, 27 Jan 2017 21:50:56 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 27, 2017, 10:52 p.m.</div>
<pre class="content">
Common case for migration of virtual address range is page are map
only once inside the vma in which migration is taking place. Because
we already walk the CPU page table for that range we can directly do
the unmap there and setup special migration swap entry.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 mm/migrate.c | 108 ++++++++++++++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 93 insertions(+), 15 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 150fc4d..d78c0e7 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -2142,9 +2142,10 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 {
 	struct migrate_vma *migrate = walk-&gt;private;
 	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;
<span class="p_del">-	unsigned long addr = start;</span>
<span class="p_add">+	unsigned long addr = start, unmaped = 0;</span>
 	spinlock_t *ptl;
 	pte_t *ptep;
<span class="p_add">+	int ret = 0;</span>
 
 	if (pmd_none(*pmdp) || pmd_trans_unstable(pmdp)) {
 		/* FIXME support THP */
<span class="p_chunk">@@ -2152,9 +2153,12 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 	}
 
 	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);
<span class="p_add">+	arch_enter_lazy_mmu_mode();</span>
<span class="p_add">+</span>
 	for (; addr &lt; end; addr += PAGE_SIZE, ptep++) {
 		unsigned long flags, pfn;
 		struct page *page;
<span class="p_add">+		swp_entry_t entry;</span>
 		pte_t pte;
 		int ret;
 
<span class="p_chunk">@@ -2186,17 +2190,50 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		flags = MIGRATE_PFN_VALID | MIGRATE_PFN_MIGRATE;
 		flags |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Optimize for common case where page is only map once in one</span>
<span class="p_add">+		 * process. If we can lock the page then we can safely setup</span>
<span class="p_add">+		 * special migration page table entry now.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (trylock_page(page)) {</span>
<span class="p_add">+			pte_t swp_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+			flags |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+			ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Setup special migration page table entry */</span>
<span class="p_add">+			entry = make_migration_entry(page, pte_write(pte));</span>
<span class="p_add">+			swp_pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+			if (pte_soft_dirty(pte))</span>
<span class="p_add">+				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This is like regulat unmap we remove the rmap and</span>
<span class="p_add">+			 * drop page refcount. Page won&#39;t be free as we took</span>
<span class="p_add">+			 * a reference just above.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			unmaped++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 next:
 		migrate-&gt;src[migrate-&gt;npages++] = pfn | flags;
 		ret = migrate_vma_array_full(migrate);
 		if (ret) {
<span class="p_del">-			pte_unmap_unlock(ptep, ptl);</span>
<span class="p_del">-			return ret;</span>
<span class="p_add">+			ptep++;</span>
<span class="p_add">+			break;</span>
 		}
 	}
<span class="p_add">+	arch_leave_lazy_mmu_mode();</span>
 	pte_unmap_unlock(ptep - 1, ptl);
 
<span class="p_del">-	return 0;</span>
<span class="p_add">+	/* Only flush the TLB if we actually modified any entries */</span>
<span class="p_add">+	if (unmaped)</span>
<span class="p_add">+		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
 }
 
 /*
<span class="p_chunk">@@ -2220,7 +2257,13 @@</span> <span class="p_context"> static void migrate_vma_collect(struct migrate_vma *migrate)</span>
 	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;
 	mm_walk.private = migrate;
 
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="p_add">+					    migrate-&gt;start,</span>
<span class="p_add">+					    migrate-&gt;end);</span>
 	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="p_add">+					  migrate-&gt;start,</span>
<span class="p_add">+					  migrate-&gt;end);</span>
 }
 
 /*
<span class="p_chunk">@@ -2264,20 +2307,25 @@</span> <span class="p_context"> static bool migrate_vma_check_page(struct page *page)</span>
  */
 static void migrate_vma_prepare(struct migrate_vma *migrate)
 {
<span class="p_del">-	unsigned long addr = migrate-&gt;start, i = 0, size;</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, size, restore = 0;</span>
 	bool allow_drain = true;
 
 	lru_add_drain();
 
 	for (; i &lt; migrate-&gt;npages &amp;&amp; migrate-&gt;cpages; i++, addr += size) {
 		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);
<span class="p_add">+		bool remap = true;</span>
<span class="p_add">+</span>
 		size = migrate_pfn_size(migrate-&gt;src[i]);
 
 		if (!page)
 			continue;
 
<span class="p_del">-		lock_page(page);</span>
<span class="p_del">-		migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_LOCKED)) {</span>
<span class="p_add">+			remap = false;</span>
<span class="p_add">+			lock_page(page);</span>
<span class="p_add">+			migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+		}</span>
 
 		if (!PageLRU(page) &amp;&amp; allow_drain) {
 			/* Drain CPU&#39;s pagevec */
<span class="p_chunk">@@ -2286,10 +2334,16 @@</span> <span class="p_context"> static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
 		}
 
 		if (isolate_lru_page(page)) {
<span class="p_del">-			migrate-&gt;src[i] = 0;</span>
<span class="p_del">-			unlock_page(page);</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_del">-			put_page(page);</span>
<span class="p_add">+			if (remap) {</span>
<span class="p_add">+				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				restore++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				migrate-&gt;src[i] = 0;</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+			}</span>
 			continue;
 		}
 
<span class="p_chunk">@@ -2297,13 +2351,37 @@</span> <span class="p_context"> static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
 		put_page(page);
 
 		if (!migrate_vma_check_page(page)) {
<span class="p_del">-			migrate-&gt;src[i] = 0;</span>
<span class="p_del">-			unlock_page(page);</span>
<span class="p_del">-			migrate-&gt;cpages--;</span>
<span class="p_add">+			if (remap) {</span>
<span class="p_add">+				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+				restore++;</span>
 
<span class="p_del">-			putback_lru_page(page);</span>
<span class="p_add">+				get_page(page);</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				migrate-&gt;src[i] = 0;</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+			}</span>
 		}
 	}
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; migrate-&gt;npages &amp;&amp; restore; i++, addr += size) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_pte(page, migrate-&gt;vma, addr, page);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;src[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+	}</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



