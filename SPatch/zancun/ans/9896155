
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[8/8] iommu/vt-d: Make use of iova deferred flushing - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [8/8] iommu/vt-d: Make use of iova deferred flushing</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 11, 2017, 2:41 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1502462478-22045-9-git-send-email-joro@8bytes.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9896155/mbox/"
   >mbox</a>
|
   <a href="/patch/9896155/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9896155/">/patch/9896155/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D713E60236 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:41:59 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C7D5A28429
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:41:59 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BCAF228C3B; Fri, 11 Aug 2017 14:41:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E7EBC28C1E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 11 Aug 2017 14:41:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753283AbdHKOln (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 11 Aug 2017 10:41:43 -0400
Received: from 8bytes.org ([81.169.241.247]:40252 &quot;EHLO theia.8bytes.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753127AbdHKOlY (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 11 Aug 2017 10:41:24 -0400
Received: by theia.8bytes.org (Postfix, from userid 1000)
	id 540BC602; Fri, 11 Aug 2017 16:41:22 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple; d=8bytes.org; s=mail-1; 
	t=1502462482; bh=rzsn648qLqfNkeEiWie5HeeFmfYGstx6ui2Qd8wW46k=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=LLcFHv0TFK/A5iBTj++8J4u4Xjc0slOL6b0R58RpHp/GsbyMDND5Ai8OL6TQQcbdu
	ErGM2Eiz4nUZVQIGYjy7iWEXFfkBybHooanAnuGeEHHWfc9YWHe+i1Z4UiuLCBSm4a
	UhB8FSBldLJZU/jm6j+zlEPplTkxUJIURTKVqWd7OlMYiORF8nQqcndME8iSIhrdaX
	OmOJ9kMSYBhkmCn1GE3VC6XiMaSfoIQ/yZDELe/l6gJuN7CXQ2Fld/brbLBm/W5b8/
	CsPwS+7Bt1jUlo9AkuBEDIAy+hxvJ2ZeBlN7szB0QfY4RCh+h/OfJY+2swdvP73aeX
	SiKonuRWJ9TtQ==
From: Joerg Roedel &lt;joro@8bytes.org&gt;
To: iommu@lists.linux-foundation.org
Cc: Robin Murphy &lt;robin.murphy@arm.com&gt;, dwmw2@infradead.org,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	linux-kernel@vger.kernel.org, Joerg Roedel &lt;jroedel@suse.de&gt;
Subject: [PATCH 8/8] iommu/vt-d: Make use of iova deferred flushing
Date: Fri, 11 Aug 2017 16:41:18 +0200
Message-Id: &lt;1502462478-22045-9-git-send-email-joro@8bytes.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1502462478-22045-1-git-send-email-joro@8bytes.org&gt;
References: &lt;1502462478-22045-1-git-send-email-joro@8bytes.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a> - Aug. 11, 2017, 2:41 p.m.</div>
<pre class="content">
<span class="from">From: Joerg Roedel &lt;jroedel@suse.de&gt;</span>

Remove the deferred flushing implementation in the Intel
VT-d driver and use the one from the common iova code
instead.
<span class="signed-off-by">
Signed-off-by: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
---
 drivers/iommu/intel-iommu.c | 197 +++++++++-----------------------------------
 1 file changed, 38 insertions(+), 159 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 687f18f..d5e8b86 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -458,31 +458,6 @@</span> <span class="p_context"> static LIST_HEAD(dmar_rmrr_units);</span>
 #define for_each_rmrr_units(rmrr) \
 	list_for_each_entry(rmrr, &amp;dmar_rmrr_units, list)
 
<span class="p_del">-static void flush_unmaps_timeout(unsigned long data);</span>
<span class="p_del">-</span>
<span class="p_del">-struct deferred_flush_entry {</span>
<span class="p_del">-	unsigned long iova_pfn;</span>
<span class="p_del">-	unsigned long nrpages;</span>
<span class="p_del">-	struct dmar_domain *domain;</span>
<span class="p_del">-	struct page *freelist;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#define HIGH_WATER_MARK 250</span>
<span class="p_del">-struct deferred_flush_table {</span>
<span class="p_del">-	int next;</span>
<span class="p_del">-	struct deferred_flush_entry entries[HIGH_WATER_MARK];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-struct deferred_flush_data {</span>
<span class="p_del">-	spinlock_t lock;</span>
<span class="p_del">-	int timer_on;</span>
<span class="p_del">-	struct timer_list timer;</span>
<span class="p_del">-	long size;</span>
<span class="p_del">-	struct deferred_flush_table *tables;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU(struct deferred_flush_data, deferred_flush);</span>
<span class="p_del">-</span>
 /* bitmap for indexing intel_iommus */
 static int g_num_of_iommus;
 
<span class="p_chunk">@@ -1309,6 +1284,13 @@</span> <span class="p_context"> static void dma_free_pagelist(struct page *freelist)</span>
 	}
 }
 
<span class="p_add">+static void iova_entry_free(unsigned long data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *freelist = (struct page *)data;</span>
<span class="p_add">+</span>
<span class="p_add">+	dma_free_pagelist(freelist);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* iommu handling */
 static int iommu_alloc_root_entry(struct intel_iommu *iommu)
 {
<span class="p_chunk">@@ -1622,6 +1604,25 @@</span> <span class="p_context"> static void iommu_flush_iotlb_psi(struct intel_iommu *iommu,</span>
 				      addr, mask);
 }
 
<span class="p_add">+static void iommu_flush_iova(struct iova_domain *iovad)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct dmar_domain *domain;</span>
<span class="p_add">+	int idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	domain = container_of(iovad, struct dmar_domain, iovad);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_domain_iommu(idx, domain) {</span>
<span class="p_add">+		struct intel_iommu *iommu = g_iommus[idx];</span>
<span class="p_add">+		u16 did = domain-&gt;iommu_did[iommu-&gt;seq_id];</span>
<span class="p_add">+</span>
<span class="p_add">+		iommu-&gt;flush.flush_iotlb(iommu, did, 0, 0, DMA_TLB_DSI_FLUSH);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!cap_caching_mode(iommu-&gt;cap))</span>
<span class="p_add">+			iommu_flush_dev_iotlb(get_iommu_domain(iommu, did),</span>
<span class="p_add">+					      0, MAX_AGAW_PFN_WIDTH);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void iommu_disable_protect_mem_regions(struct intel_iommu *iommu)
 {
 	u32 pmen;
<span class="p_chunk">@@ -1932,9 +1933,16 @@</span> <span class="p_context"> static int domain_init(struct dmar_domain *domain, struct intel_iommu *iommu,</span>
 {
 	int adjust_width, agaw;
 	unsigned long sagaw;
<span class="p_add">+	int err;</span>
 
 	init_iova_domain(&amp;domain-&gt;iovad, VTD_PAGE_SIZE, IOVA_START_PFN,
 			DMA_32BIT_PFN);
<span class="p_add">+</span>
<span class="p_add">+	err = init_iova_flush_queue(&amp;domain-&gt;iovad,</span>
<span class="p_add">+				    iommu_flush_iova, iova_entry_free);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		return err;</span>
<span class="p_add">+</span>
 	domain_reserve_special_ranges(domain);
 
 	/* calculate AGAW */
<span class="p_chunk">@@ -1986,14 +1994,6 @@</span> <span class="p_context"> static void domain_exit(struct dmar_domain *domain)</span>
 	if (!domain)
 		return;
 
<span class="p_del">-	/* Flush any lazy unmaps that may reference this domain */</span>
<span class="p_del">-	if (!intel_iommu_strict) {</span>
<span class="p_del">-		int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-		for_each_possible_cpu(cpu)</span>
<span class="p_del">-			flush_unmaps_timeout(cpu);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	/* Remove associated devices and clear attached or cached domains */
 	rcu_read_lock();
 	domain_remove_dev_info(domain);
<span class="p_chunk">@@ -3206,7 +3206,7 @@</span> <span class="p_context"> static int __init init_dmars(void)</span>
 	bool copied_tables = false;
 	struct device *dev;
 	struct intel_iommu *iommu;
<span class="p_del">-	int i, ret, cpu;</span>
<span class="p_add">+	int i, ret;</span>
 
 	/*
 	 * for each drhd
<span class="p_chunk">@@ -3239,22 +3239,6 @@</span> <span class="p_context"> static int __init init_dmars(void)</span>
 		goto error;
 	}
 
<span class="p_del">-	for_each_possible_cpu(cpu) {</span>
<span class="p_del">-		struct deferred_flush_data *dfd = per_cpu_ptr(&amp;deferred_flush,</span>
<span class="p_del">-							      cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		dfd-&gt;tables = kzalloc(g_num_of_iommus *</span>
<span class="p_del">-				      sizeof(struct deferred_flush_table),</span>
<span class="p_del">-				      GFP_KERNEL);</span>
<span class="p_del">-		if (!dfd-&gt;tables) {</span>
<span class="p_del">-			ret = -ENOMEM;</span>
<span class="p_del">-			goto free_g_iommus;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		spin_lock_init(&amp;dfd-&gt;lock);</span>
<span class="p_del">-		setup_timer(&amp;dfd-&gt;timer, flush_unmaps_timeout, cpu);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	for_each_active_iommu(iommu, drhd) {
 		g_iommus[iommu-&gt;seq_id] = iommu;
 
<span class="p_chunk">@@ -3437,10 +3421,9 @@</span> <span class="p_context"> static int __init init_dmars(void)</span>
 		disable_dmar_iommu(iommu);
 		free_dmar_iommu(iommu);
 	}
<span class="p_del">-free_g_iommus:</span>
<span class="p_del">-	for_each_possible_cpu(cpu)</span>
<span class="p_del">-		kfree(per_cpu_ptr(&amp;deferred_flush, cpu)-&gt;tables);</span>
<span class="p_add">+</span>
 	kfree(g_iommus);
<span class="p_add">+</span>
 error:
 	return ret;
 }
<span class="p_chunk">@@ -3645,110 +3628,6 @@</span> <span class="p_context"> static dma_addr_t intel_map_page(struct device *dev, struct page *page,</span>
 				  dir, *dev-&gt;dma_mask);
 }
 
<span class="p_del">-static void flush_unmaps(struct deferred_flush_data *flush_data)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i, j;</span>
<span class="p_del">-</span>
<span class="p_del">-	flush_data-&gt;timer_on = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* just flush them all */</span>
<span class="p_del">-	for (i = 0; i &lt; g_num_of_iommus; i++) {</span>
<span class="p_del">-		struct intel_iommu *iommu = g_iommus[i];</span>
<span class="p_del">-		struct deferred_flush_table *flush_table =</span>
<span class="p_del">-				&amp;flush_data-&gt;tables[i];</span>
<span class="p_del">-		if (!iommu)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!flush_table-&gt;next)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* In caching mode, global flushes turn emulation expensive */</span>
<span class="p_del">-		if (!cap_caching_mode(iommu-&gt;cap))</span>
<span class="p_del">-			iommu-&gt;flush.flush_iotlb(iommu, 0, 0, 0,</span>
<span class="p_del">-					 DMA_TLB_GLOBAL_FLUSH);</span>
<span class="p_del">-		for (j = 0; j &lt; flush_table-&gt;next; j++) {</span>
<span class="p_del">-			unsigned long mask;</span>
<span class="p_del">-			struct deferred_flush_entry *entry =</span>
<span class="p_del">-						&amp;flush_table-&gt;entries[j];</span>
<span class="p_del">-			unsigned long iova_pfn = entry-&gt;iova_pfn;</span>
<span class="p_del">-			unsigned long nrpages = entry-&gt;nrpages;</span>
<span class="p_del">-			struct dmar_domain *domain = entry-&gt;domain;</span>
<span class="p_del">-			struct page *freelist = entry-&gt;freelist;</span>
<span class="p_del">-</span>
<span class="p_del">-			/* On real hardware multiple invalidations are expensive */</span>
<span class="p_del">-			if (cap_caching_mode(iommu-&gt;cap))</span>
<span class="p_del">-				iommu_flush_iotlb_psi(iommu, domain,</span>
<span class="p_del">-					mm_to_dma_pfn(iova_pfn),</span>
<span class="p_del">-					nrpages, !freelist, 0);</span>
<span class="p_del">-			else {</span>
<span class="p_del">-				mask = ilog2(nrpages);</span>
<span class="p_del">-				iommu_flush_dev_iotlb(domain,</span>
<span class="p_del">-						(uint64_t)iova_pfn &lt;&lt; PAGE_SHIFT, mask);</span>
<span class="p_del">-			}</span>
<span class="p_del">-			free_iova_fast(&amp;domain-&gt;iovad, iova_pfn, nrpages);</span>
<span class="p_del">-			if (freelist)</span>
<span class="p_del">-				dma_free_pagelist(freelist);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		flush_table-&gt;next = 0;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	flush_data-&gt;size = 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void flush_unmaps_timeout(unsigned long cpuid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct deferred_flush_data *flush_data = per_cpu_ptr(&amp;deferred_flush, cpuid);</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	spin_lock_irqsave(&amp;flush_data-&gt;lock, flags);</span>
<span class="p_del">-	flush_unmaps(flush_data);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;flush_data-&gt;lock, flags);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void add_unmap(struct dmar_domain *dom, unsigned long iova_pfn,</span>
<span class="p_del">-		      unsigned long nrpages, struct page *freelist)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	int entry_id, iommu_id;</span>
<span class="p_del">-	struct intel_iommu *iommu;</span>
<span class="p_del">-	struct deferred_flush_entry *entry;</span>
<span class="p_del">-	struct deferred_flush_data *flush_data;</span>
<span class="p_del">-</span>
<span class="p_del">-	flush_data = raw_cpu_ptr(&amp;deferred_flush);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Flush all CPUs&#39; entries to avoid deferring too much.  If</span>
<span class="p_del">-	 * this becomes a bottleneck, can just flush us, and rely on</span>
<span class="p_del">-	 * flush timer for the rest.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (flush_data-&gt;size == HIGH_WATER_MARK) {</span>
<span class="p_del">-		int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-		for_each_online_cpu(cpu)</span>
<span class="p_del">-			flush_unmaps_timeout(cpu);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	spin_lock_irqsave(&amp;flush_data-&gt;lock, flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	iommu = domain_get_iommu(dom);</span>
<span class="p_del">-	iommu_id = iommu-&gt;seq_id;</span>
<span class="p_del">-</span>
<span class="p_del">-	entry_id = flush_data-&gt;tables[iommu_id].next;</span>
<span class="p_del">-	++(flush_data-&gt;tables[iommu_id].next);</span>
<span class="p_del">-</span>
<span class="p_del">-	entry = &amp;flush_data-&gt;tables[iommu_id].entries[entry_id];</span>
<span class="p_del">-	entry-&gt;domain = dom;</span>
<span class="p_del">-	entry-&gt;iova_pfn = iova_pfn;</span>
<span class="p_del">-	entry-&gt;nrpages = nrpages;</span>
<span class="p_del">-	entry-&gt;freelist = freelist;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!flush_data-&gt;timer_on) {</span>
<span class="p_del">-		mod_timer(&amp;flush_data-&gt;timer, jiffies + msecs_to_jiffies(10));</span>
<span class="p_del">-		flush_data-&gt;timer_on = 1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	flush_data-&gt;size++;</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;flush_data-&gt;lock, flags);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void intel_unmap(struct device *dev, dma_addr_t dev_addr, size_t size)
 {
 	struct dmar_domain *domain;
<span class="p_chunk">@@ -3784,7 +3663,8 @@</span> <span class="p_context"> static void intel_unmap(struct device *dev, dma_addr_t dev_addr, size_t size)</span>
 		free_iova_fast(&amp;domain-&gt;iovad, iova_pfn, dma_to_mm_pfn(nrpages));
 		dma_free_pagelist(freelist);
 	} else {
<span class="p_del">-		add_unmap(domain, iova_pfn, nrpages, freelist);</span>
<span class="p_add">+		queue_iova(&amp;domain-&gt;iovad, iova_pfn, nrpages,</span>
<span class="p_add">+			   (unsigned long)freelist);</span>
 		/*
 		 * queue up the release of the unmap to save the 1/6th of the
 		 * cpu used up by the iotlb flush operation...
<span class="p_chunk">@@ -4721,7 +4601,6 @@</span> <span class="p_context"> static void free_all_cpu_cached_iovas(unsigned int cpu)</span>
 static int intel_iommu_cpu_dead(unsigned int cpu)
 {
 	free_all_cpu_cached_iovas(cpu);
<span class="p_del">-	flush_unmaps_timeout(cpu);</span>
 	return 0;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



