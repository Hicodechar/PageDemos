
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.4.110 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.4.110</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 5, 2018, 2:54 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180105145441.GB11903@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10146653/mbox/"
   >mbox</a>
|
   <a href="/patch/10146653/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10146653/">/patch/10146653/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	65B16601A1 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 14:55:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3AA422882F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 14:55:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2D09928843; Fri,  5 Jan 2018 14:55:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E2A462882F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 14:55:14 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752068AbeAEOzI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 5 Jan 2018 09:55:08 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:43366 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751576AbeAEOyi (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 5 Jan 2018 09:54:38 -0500
Received: from localhost (LFbn-1-12258-90.w90-92.abo.wanadoo.fr
	[90.92.71.90])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id D4C2ABA3;
	Fri,  5 Jan 2018 14:54:36 +0000 (UTC)
Date: Fri, 5 Jan 2018 15:54:41 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.4.110
Message-ID: &lt;20180105145441.GB11903@kroah.com&gt;
References: &lt;20180105145433.GA11903@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20180105145433.GA11903@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 5, 2018, 2:54 p.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index b4a83a490212..5977c4d71356 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2523,6 +2523,8 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 
 	nojitter	[IA-64] Disables jitter checking for ITC timers.
 
<span class="p_add">+	nopti		[X86-64] Disable KAISER isolation of kernel from user.</span>
<span class="p_add">+</span>
 	no-kvmclock	[X86,KVM] Disable paravirtualized KVM clock driver
 
 	no-kvmapf	[X86,KVM] Disable paravirtualized asynchronous page
<span class="p_chunk">@@ -3054,6 +3056,12 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_add">+	pti=		[X86_64]</span>
<span class="p_add">+			Control KAISER user/kernel address space isolation:</span>
<span class="p_add">+			on - enable</span>
<span class="p_add">+			off - disable</span>
<span class="p_add">+			auto - default setting</span>
<span class="p_add">+</span>
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
 			default number.
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 5d67056e24dd..b028c106535b 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 4
 PATCHLEVEL = 4
<span class="p_del">-SUBLEVEL = 109</span>
<span class="p_add">+SUBLEVEL = 110</span>
 EXTRAVERSION =
 NAME = Blurry Fish Butt
 
<span class="p_header">diff --git a/arch/x86/boot/compressed/misc.h b/arch/x86/boot/compressed/misc.h</span>
<span class="p_header">index 3783dc3e10b3..4abb284a5b9c 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/misc.h</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/misc.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
  */
 #undef CONFIG_PARAVIRT
 #undef CONFIG_PARAVIRT_SPINLOCKS
<span class="p_add">+#undef CONFIG_PAGE_TABLE_ISOLATION</span>
 #undef CONFIG_KASAN
 
 #include &lt;linux/linkage.h&gt;
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index cc0f2f5da19b..952b23b5d4e9 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
 #include &lt;asm/pgtable_types.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;linux/err.h&gt;
 
 /* Avoid __ASSEMBLER__&#39;ifying &lt;linux/audit.h&gt; just for this.  */
<span class="p_chunk">@@ -135,6 +136,7 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64)</span>
 	 * it is too small to ever cause noticeable irq latency.
 	 */
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	/*
 	 * A hypervisor implementation might want to use a label
 	 * after the swapgs, so that it can do the swapgs
<span class="p_chunk">@@ -207,9 +209,17 @@</span> <span class="p_context"> entry_SYSCALL_64_fastpath:</span>
 	testl	$_TIF_ALLWORK_MASK, ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS)
 	jnz	int_ret_from_sys_call_irqs_off	/* Go to the slow path */
 
<span class="p_del">-	RESTORE_C_REGS_EXCEPT_RCX_R11</span>
 	movq	RIP(%rsp), %rcx
 	movq	EFLAGS(%rsp), %r11
<span class="p_add">+	RESTORE_C_REGS_EXCEPT_RCX_R11</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This opens a window where we have a user CR3, but are</span>
<span class="p_add">+	 * running in the kernel.  This makes using the CS</span>
<span class="p_add">+	 * register useless for telling whether or not we need to</span>
<span class="p_add">+	 * switch CR3 in NMIs.  Normal interrupts are OK because</span>
<span class="p_add">+	 * they are off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
 	movq	RSP(%rsp), %rsp
 	/*
 	 * 64-bit SYSRET restores rip from rcx,
<span class="p_chunk">@@ -347,10 +357,26 @@</span> <span class="p_context"> GLOBAL(int_ret_from_sys_call)</span>
 syscall_return_via_sysret:
 	/* rcx and r11 are already restored (see code above) */
 	RESTORE_C_REGS_EXCEPT_RCX_R11
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This opens a window where we have a user CR3, but are</span>
<span class="p_add">+	 * running in the kernel.  This makes using the CS</span>
<span class="p_add">+	 * register useless for telling whether or not we need to</span>
<span class="p_add">+	 * switch CR3 in NMIs.  Normal interrupts are OK because</span>
<span class="p_add">+	 * they are off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
 	movq	RSP(%rsp), %rsp
 	USERGS_SYSRET64
 
 opportunistic_sysret_failed:
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This opens a window where we have a user CR3, but are</span>
<span class="p_add">+	 * running in the kernel.  This makes using the CS</span>
<span class="p_add">+	 * register useless for telling whether or not we need to</span>
<span class="p_add">+	 * switch CR3 in NMIs.  Normal interrupts are OK because</span>
<span class="p_add">+	 * they are off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	jmp	restore_c_regs_and_iret
 END(entry_SYSCALL_64)
<span class="p_chunk">@@ -509,6 +535,7 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 	 * tracking that we&#39;re in kernel mode.
 	 */
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 
 	/*
 	 * We need to tell lockdep that IRQs are off.  We can&#39;t do this until
<span class="p_chunk">@@ -568,6 +595,7 @@</span> <span class="p_context"> GLOBAL(retint_user)</span>
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
 	TRACE_IRQS_IRETQ
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	jmp	restore_regs_and_iret
 
<span class="p_chunk">@@ -625,6 +653,7 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	pushq	%rax
 	pushq	%rdi
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
 	movq	%rax, (0*8)(%rdi)		/* RAX */
 	movq	(2*8)(%rsp), %rax		/* RIP */
<span class="p_chunk">@@ -640,6 +669,7 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	andl	$0xffff0000, %eax
 	popq	%rdi
 	orq	PER_CPU_VAR(espfix_stack), %rax
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	movq	%rax, %rsp
 	popq	%rax
<span class="p_chunk">@@ -995,7 +1025,11 @@</span> <span class="p_context"> idtentry machine_check					has_error_code=0	paranoid=1 do_sym=*machine_check_vec</span>
 /*
  * Save all registers in pt_regs, and switch gs if needed.
  * Use slow, but surefire &quot;are we in kernel?&quot; check.
<span class="p_del">- * Return: ebx=0: need swapgs on exit, ebx=1: otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Return: ebx=0: needs swapgs but not SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=1: needs neither swapgs nor SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=2: needs both swapgs and SWITCH_USER_CR3 in paranoid_exit</span>
<span class="p_add">+ *         ebx=3: needs SWITCH_USER_CR3 but not swapgs in paranoid_exit</span>
  */
 ENTRY(paranoid_entry)
 	cld
<span class="p_chunk">@@ -1008,7 +1042,26 @@</span> <span class="p_context"> ENTRY(paranoid_entry)</span>
 	js	1f				/* negative -&gt; in kernel */
 	SWAPGS
 	xorl	%ebx, %ebx
<span class="p_del">-1:	ret</span>
<span class="p_add">+1:</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We might have come in between a swapgs and a SWITCH_KERNEL_CR3</span>
<span class="p_add">+	 * on entry, or between a SWITCH_USER_CR3 and a swapgs on exit.</span>
<span class="p_add">+	 * Do a conditional SWITCH_KERNEL_CR3: this could safely be done</span>
<span class="p_add">+	 * unconditionally, but we need to find out whether the reverse</span>
<span class="p_add">+	 * should be done on return (conveyed to paranoid_exit in %ebx).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	testl	$KAISER_SHADOW_PGD_OFFSET, %eax</span>
<span class="p_add">+	jz	2f</span>
<span class="p_add">+	orl	$2, %ebx</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	ret</span>
 END(paranoid_entry)
 
 /*
<span class="p_chunk">@@ -1021,19 +1074,26 @@</span> <span class="p_context"> END(paranoid_entry)</span>
  * be complicated.  Fortunately, we there&#39;s no good reason
  * to try to handle preemption here.
  *
<span class="p_del">- * On entry, ebx is &quot;no swapgs&quot; flag (1: don&#39;t need swapgs, 0: need it)</span>
<span class="p_add">+ * On entry: ebx=0: needs swapgs but not SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=1: needs neither swapgs nor SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=2: needs both swapgs and SWITCH_USER_CR3</span>
<span class="p_add">+ *           ebx=3: needs SWITCH_USER_CR3 but not swapgs</span>
  */
 ENTRY(paranoid_exit)
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF_DEBUG
<span class="p_del">-	testl	%ebx, %ebx			/* swapgs needed? */</span>
<span class="p_add">+	TRACE_IRQS_IRETQ_DEBUG</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* No ALTERNATIVE for X86_FEATURE_KAISER: paranoid_entry sets %ebx */</span>
<span class="p_add">+	testl	$2, %ebx			/* SWITCH_USER_CR3 needed? */</span>
<span class="p_add">+	jz	paranoid_exit_no_switch</span>
<span class="p_add">+	SWITCH_USER_CR3</span>
<span class="p_add">+paranoid_exit_no_switch:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	testl	$1, %ebx			/* swapgs needed? */</span>
 	jnz	paranoid_exit_no_swapgs
<span class="p_del">-	TRACE_IRQS_IRETQ</span>
 	SWAPGS_UNSAFE_STACK
<span class="p_del">-	jmp	paranoid_exit_restore</span>
 paranoid_exit_no_swapgs:
<span class="p_del">-	TRACE_IRQS_IRETQ_DEBUG</span>
<span class="p_del">-paranoid_exit_restore:</span>
 	RESTORE_EXTRA_REGS
 	RESTORE_C_REGS
 	REMOVE_PT_GPREGS_FROM_STACK 8
<span class="p_chunk">@@ -1048,6 +1108,13 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
<span class="p_add">+	/*</span>
<span class="p_add">+	 * error_entry() always returns with a kernel gsbase and</span>
<span class="p_add">+	 * CR3.  We must also have a kernel CR3/gsbase before</span>
<span class="p_add">+	 * calling TRACE_IRQS_*.  Just unconditionally switch to</span>
<span class="p_add">+	 * the kernel CR3 here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_KERNEL_CR3</span>
 	xorl	%ebx, %ebx
 	testb	$3, CS+8(%rsp)
 	jz	.Lerror_kernelspace
<span class="p_chunk">@@ -1210,6 +1277,10 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	 */
 
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	/*</span>
<span class="p_add">+	 * percpu variables are mapped with user CR3, so no need</span>
<span class="p_add">+	 * to switch CR3 here.</span>
<span class="p_add">+	 */</span>
 	cld
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
<span class="p_chunk">@@ -1243,12 +1314,34 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* Unconditionally use kernel CR3 for do_nmi() */</span>
<span class="p_add">+	/* %rax is saved above, so OK to clobber here */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
 	call	do_nmi
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Unconditionally restore CR3.  I know we return to</span>
<span class="p_add">+	 * kernel code that needs user CR3, but do we ever return</span>
<span class="p_add">+	 * to &quot;user mode&quot; where we need the kernel CR3?</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;popq %rax; movq %rax, %cr3&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/*
 	 * Return back to user mode.  We must *not* do the normal exit
<span class="p_del">-	 * work, because we don&#39;t want to enable interrupts.  Fortunately,</span>
<span class="p_del">-	 * do_nmi doesn&#39;t modify pt_regs.</span>
<span class="p_add">+	 * work, because we don&#39;t want to enable interrupts.  Do not</span>
<span class="p_add">+	 * switch to user CR3: we might be going back to kernel code</span>
<span class="p_add">+	 * that had a user CR3 set.</span>
 	 */
 	SWAPGS
 	jmp	restore_c_regs_and_iret
<span class="p_chunk">@@ -1445,22 +1538,55 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 	ALLOC_PT_GPREGS_ON_STACK
 
 	/*
<span class="p_del">-	 * Use paranoid_entry to handle SWAPGS, but no need to use paranoid_exit</span>
<span class="p_del">-	 * as we should not be calling schedule in NMI context.</span>
<span class="p_del">-	 * Even with normal interrupts enabled. An NMI should not be</span>
<span class="p_del">-	 * setting NEED_RESCHED or anything that normal interrupts and</span>
<span class="p_del">-	 * exceptions might do.</span>
<span class="p_add">+	 * Use the same approach as paranoid_entry to handle SWAPGS, but</span>
<span class="p_add">+	 * without CR3 handling since we do that differently in NMIs.  No</span>
<span class="p_add">+	 * need to use paranoid_exit as we should not be calling schedule</span>
<span class="p_add">+	 * in NMI context.  Even with normal interrupts enabled. An NMI</span>
<span class="p_add">+	 * should not be setting NEED_RESCHED or anything that normal</span>
<span class="p_add">+	 * interrupts and exceptions might do.</span>
 	 */
<span class="p_del">-	call	paranoid_entry</span>
<span class="p_del">-</span>
<span class="p_del">-	/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */</span>
<span class="p_add">+	cld</span>
<span class="p_add">+	SAVE_C_REGS</span>
<span class="p_add">+	SAVE_EXTRA_REGS</span>
<span class="p_add">+	movl	$1, %ebx</span>
<span class="p_add">+	movl	$MSR_GS_BASE, %ecx</span>
<span class="p_add">+	rdmsr</span>
<span class="p_add">+	testl	%edx, %edx</span>
<span class="p_add">+	js	1f				/* negative -&gt; in kernel */</span>
<span class="p_add">+	SWAPGS</span>
<span class="p_add">+	xorl	%ebx, %ebx</span>
<span class="p_add">+1:</span>
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/* Unconditionally use kernel CR3 for do_nmi() */</span>
<span class="p_add">+	/* %rax is saved above, so OK to clobber here */</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp 2f&quot;, &quot;movq %cr3, %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, %rax&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	movq	%rax, %cr3</span>
<span class="p_add">+2:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */</span>
 	call	do_nmi
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Unconditionally restore CR3.  We might be returning to</span>
<span class="p_add">+	 * kernel code that needs user CR3, like just just before</span>
<span class="p_add">+	 * a sysret.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;popq %rax; movq %rax, %cr3&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	nmi_restore
 nmi_swapgs:
<span class="p_add">+	/* We fixed up CR3 above, so no need to switch it here */</span>
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
 	RESTORE_EXTRA_REGS
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 15cfebaa7688..d03bf0e28b8b 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -13,6 +13,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/irqflags.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/pgtable_types.h&gt;</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/err.h&gt;
 
<span class="p_chunk">@@ -50,6 +52,7 @@</span> <span class="p_context"> ENDPROC(native_usergs_sysret32)</span>
 ENTRY(entry_SYSENTER_compat)
 	/* Interrupts are off on entry. */
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
 	/*
<span class="p_chunk">@@ -161,6 +164,7 @@</span> <span class="p_context"> ENDPROC(entry_SYSENTER_compat)</span>
 ENTRY(entry_SYSCALL_compat)
 	/* Interrupts are off on entry. */
 	SWAPGS_UNSAFE_STACK
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 
 	/* Stash user ESP and switch to the kernel stack. */
 	movl	%esp, %r8d
<span class="p_chunk">@@ -208,6 +212,7 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_compat)</span>
 	/* Opportunistic SYSRET */
 sysret32_from_system_call:
 	TRACE_IRQS_ON			/* User mode traces as IRQs on. */
<span class="p_add">+	SWITCH_USER_CR3</span>
 	movq	RBX(%rsp), %rbx		/* pt_regs-&gt;rbx */
 	movq	RBP(%rsp), %rbp		/* pt_regs-&gt;rbp */
 	movq	EFLAGS(%rsp), %r11	/* pt_regs-&gt;flags (in r11) */
<span class="p_chunk">@@ -269,6 +274,7 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 	PARAVIRT_ADJUST_EXCEPTION_FRAME
 	ASM_CLAC			/* Do this early to minimize exposure */
 	SWAPGS
<span class="p_add">+	SWITCH_KERNEL_CR3_NO_STACK</span>
 
 	/*
 	 * User tracing code (ptrace or signal handlers) might assume that
<span class="p_chunk">@@ -311,6 +317,7 @@</span> <span class="p_context"> ENTRY(entry_INT80_compat)</span>
 
 	/* Go back to user mode. */
 	TRACE_IRQS_ON
<span class="p_add">+	SWITCH_USER_CR3</span>
 	SWAPGS
 	jmp	restore_regs_and_iret
 END(entry_INT80_compat)
<span class="p_header">diff --git a/arch/x86/entry/vdso/vclock_gettime.c b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">index ca94fa649251..5dd363d54348 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_chunk">@@ -36,6 +36,11 @@</span> <span class="p_context"> static notrace cycle_t vread_hpet(void)</span>
 }
 #endif
 
<span class="p_add">+#ifdef CONFIG_PARAVIRT_CLOCK</span>
<span class="p_add">+extern u8 pvclock_page</span>
<span class="p_add">+	__attribute__((visibility(&quot;hidden&quot;)));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef BUILD_VDSO32
 
 #include &lt;linux/kernel.h&gt;
<span class="p_chunk">@@ -62,63 +67,65 @@</span> <span class="p_context"> notrace static long vdso_fallback_gtod(struct timeval *tv, struct timezone *tz)</span>
 
 #ifdef CONFIG_PARAVIRT_CLOCK
 
<span class="p_del">-static notrace const struct pvclock_vsyscall_time_info *get_pvti(int cpu)</span>
<span class="p_add">+static notrace const struct pvclock_vsyscall_time_info *get_pvti0(void)</span>
 {
<span class="p_del">-	const struct pvclock_vsyscall_time_info *pvti_base;</span>
<span class="p_del">-	int idx = cpu / (PAGE_SIZE/PVTI_SIZE);</span>
<span class="p_del">-	int offset = cpu % (PAGE_SIZE/PVTI_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(PVCLOCK_FIXMAP_BEGIN + idx &gt; PVCLOCK_FIXMAP_END);</span>
<span class="p_del">-</span>
<span class="p_del">-	pvti_base = (struct pvclock_vsyscall_time_info *)</span>
<span class="p_del">-		    __fix_to_virt(PVCLOCK_FIXMAP_BEGIN+idx);</span>
<span class="p_del">-</span>
<span class="p_del">-	return &amp;pvti_base[offset];</span>
<span class="p_add">+	return (const struct pvclock_vsyscall_time_info *)&amp;pvclock_page;</span>
 }
 
 static notrace cycle_t vread_pvclock(int *mode)
 {
<span class="p_del">-	const struct pvclock_vsyscall_time_info *pvti;</span>
<span class="p_add">+	const struct pvclock_vcpu_time_info *pvti = &amp;get_pvti0()-&gt;pvti;</span>
 	cycle_t ret;
<span class="p_del">-	u64 last;</span>
<span class="p_del">-	u32 version;</span>
<span class="p_del">-	u8 flags;</span>
<span class="p_del">-	unsigned cpu, cpu1;</span>
<span class="p_del">-</span>
<span class="p_add">+	u64 tsc, pvti_tsc;</span>
<span class="p_add">+	u64 last, delta, pvti_system_time;</span>
<span class="p_add">+	u32 version, pvti_tsc_to_system_mul, pvti_tsc_shift;</span>
 
 	/*
<span class="p_del">-	 * Note: hypervisor must guarantee that:</span>
<span class="p_del">-	 * 1. cpu ID number maps 1:1 to per-CPU pvclock time info.</span>
<span class="p_del">-	 * 2. that per-CPU pvclock time info is updated if the</span>
<span class="p_del">-	 *    underlying CPU changes.</span>
<span class="p_del">-	 * 3. that version is increased whenever underlying CPU</span>
<span class="p_del">-	 *    changes.</span>
<span class="p_add">+	 * Note: The kernel and hypervisor must guarantee that cpu ID</span>
<span class="p_add">+	 * number maps 1:1 to per-CPU pvclock time info.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Because the hypervisor is entirely unaware of guest userspace</span>
<span class="p_add">+	 * preemption, it cannot guarantee that per-CPU pvclock time</span>
<span class="p_add">+	 * info is updated if the underlying CPU changes or that that</span>
<span class="p_add">+	 * version is increased whenever underlying CPU changes.</span>
 	 *
<span class="p_add">+	 * On KVM, we are guaranteed that pvti updates for any vCPU are</span>
<span class="p_add">+	 * atomic as seen by *all* vCPUs.  This is an even stronger</span>
<span class="p_add">+	 * guarantee than we get with a normal seqlock.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * On Xen, we don&#39;t appear to have that guarantee, but Xen still</span>
<span class="p_add">+	 * supplies a valid seqlock using the version field.</span>
<span class="p_add">+</span>
<span class="p_add">+	 * We only do pvclock vdso timing at all if</span>
<span class="p_add">+	 * PVCLOCK_TSC_STABLE_BIT is set, and we interpret that bit to</span>
<span class="p_add">+	 * mean that all vCPUs have matching pvti and that the TSC is</span>
<span class="p_add">+	 * synced, so we can just look at vCPU 0&#39;s pvti.</span>
 	 */
<span class="p_del">-	do {</span>
<span class="p_del">-		cpu = __getcpu() &amp; VGETCPU_CPU_MASK;</span>
<span class="p_del">-		/* TODO: We can put vcpu id into higher bits of pvti.version.</span>
<span class="p_del">-		 * This will save a couple of cycles by getting rid of</span>
<span class="p_del">-		 * __getcpu() calls (Gleb).</span>
<span class="p_del">-		 */</span>
<span class="p_del">-</span>
<span class="p_del">-		pvti = get_pvti(cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		version = __pvclock_read_cycles(&amp;pvti-&gt;pvti, &amp;ret, &amp;flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Test we&#39;re still on the cpu as well as the version.</span>
<span class="p_del">-		 * We could have been migrated just after the first</span>
<span class="p_del">-		 * vgetcpu but before fetching the version, so we</span>
<span class="p_del">-		 * wouldn&#39;t notice a version change.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		cpu1 = __getcpu() &amp; VGETCPU_CPU_MASK;</span>
<span class="p_del">-	} while (unlikely(cpu != cpu1 ||</span>
<span class="p_del">-			  (pvti-&gt;pvti.version &amp; 1) ||</span>
<span class="p_del">-			  pvti-&gt;pvti.version != version));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(!(flags &amp; PVCLOCK_TSC_STABLE_BIT)))</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!(pvti-&gt;flags &amp; PVCLOCK_TSC_STABLE_BIT))) {</span>
 		*mode = VCLOCK_NONE;
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		version = pvti-&gt;version;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* This is also a read barrier, so we&#39;ll read version first. */</span>
<span class="p_add">+		tsc = rdtsc_ordered();</span>
<span class="p_add">+</span>
<span class="p_add">+		pvti_tsc_to_system_mul = pvti-&gt;tsc_to_system_mul;</span>
<span class="p_add">+		pvti_tsc_shift = pvti-&gt;tsc_shift;</span>
<span class="p_add">+		pvti_system_time = pvti-&gt;system_time;</span>
<span class="p_add">+		pvti_tsc = pvti-&gt;tsc_timestamp;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Make sure that the version double-check is last. */</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+	} while (unlikely((version &amp; 1) || version != pvti-&gt;version));</span>
<span class="p_add">+</span>
<span class="p_add">+	delta = tsc - pvti_tsc;</span>
<span class="p_add">+	ret = pvti_system_time +</span>
<span class="p_add">+		pvclock_scale_delta(delta, pvti_tsc_to_system_mul,</span>
<span class="p_add">+				    pvti_tsc_shift);</span>
 
 	/* refer to tsc.c read_tsc() comment for rationale */
 	last = gtod-&gt;cycle_last;
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso-layout.lds.S b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">index de2c921025f5..4158acc17df0 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> SECTIONS</span>
 	 * segment.
 	 */
 
<span class="p_del">-	vvar_start = . - 2 * PAGE_SIZE;</span>
<span class="p_add">+	vvar_start = . - 3 * PAGE_SIZE;</span>
 	vvar_page = vvar_start;
 
 	/* Place all vvars at the offsets in asm/vvar.h. */
<span class="p_chunk">@@ -36,6 +36,7 @@</span> <span class="p_context"> SECTIONS</span>
 #undef EMIT_VVAR
 
 	hpet_page = vvar_start + PAGE_SIZE;
<span class="p_add">+	pvclock_page = vvar_start + 2 * PAGE_SIZE;</span>
 
 	. = SIZEOF_HEADERS;
 
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso2c.c b/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_header">index 785d9922b106..491020b2826d 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_chunk">@@ -73,6 +73,7 @@</span> <span class="p_context"> enum {</span>
 	sym_vvar_start,
 	sym_vvar_page,
 	sym_hpet_page,
<span class="p_add">+	sym_pvclock_page,</span>
 	sym_VDSO_FAKE_SECTION_TABLE_START,
 	sym_VDSO_FAKE_SECTION_TABLE_END,
 };
<span class="p_chunk">@@ -80,6 +81,7 @@</span> <span class="p_context"> enum {</span>
 const int special_pages[] = {
 	sym_vvar_page,
 	sym_hpet_page,
<span class="p_add">+	sym_pvclock_page,</span>
 };
 
 struct vdso_sym {
<span class="p_chunk">@@ -91,6 +93,7 @@</span> <span class="p_context"> struct vdso_sym required_syms[] = {</span>
 	[sym_vvar_start] = {&quot;vvar_start&quot;, true},
 	[sym_vvar_page] = {&quot;vvar_page&quot;, true},
 	[sym_hpet_page] = {&quot;hpet_page&quot;, true},
<span class="p_add">+	[sym_pvclock_page] = {&quot;pvclock_page&quot;, true},</span>
 	[sym_VDSO_FAKE_SECTION_TABLE_START] = {
 		&quot;VDSO_FAKE_SECTION_TABLE_START&quot;, false
 	},
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 64df47148160..aa828191c654 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -100,6 +100,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 		.name = &quot;[vvar]&quot;,
 		.pages = no_pages,
 	};
<span class="p_add">+	struct pvclock_vsyscall_time_info *pvti;</span>
 
 	if (calculate_addr) {
 		addr = vdso_addr(current-&gt;mm-&gt;start_stack,
<span class="p_chunk">@@ -169,6 +170,18 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 	}
 #endif
 
<span class="p_add">+	pvti = pvclock_pvti_cpu0_va();</span>
<span class="p_add">+	if (pvti &amp;&amp; image-&gt;sym_pvclock_page) {</span>
<span class="p_add">+		ret = remap_pfn_range(vma,</span>
<span class="p_add">+				      text_start + image-&gt;sym_pvclock_page,</span>
<span class="p_add">+				      __pa(pvti) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+				      PAGE_SIZE,</span>
<span class="p_add">+				      PAGE_READONLY);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto up_fail;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 up_fail:
 	if (ret)
 		current-&gt;mm-&gt;context.vdso = NULL;
<span class="p_header">diff --git a/arch/x86/include/asm/cmdline.h b/arch/x86/include/asm/cmdline.h</span>
<span class="p_header">index e01f7f7ccb0c..84ae170bc3d0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cmdline.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cmdline.h</span>
<span class="p_chunk">@@ -2,5 +2,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_CMDLINE_H
 
 int cmdline_find_option_bool(const char *cmdline_ptr, const char *option);
<span class="p_add">+int cmdline_find_option(const char *cmdline_ptr, const char *option,</span>
<span class="p_add">+			char *buffer, int bufsize);</span>
 
 #endif /* _ASM_X86_CMDLINE_H */
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index f7ba9fbf12ee..f6605712ca90 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -187,6 +187,7 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_ARAT	( 7*32+ 1) /* Always Running APIC Timer */
 #define X86_FEATURE_CPB		( 7*32+ 2) /* AMD Core Performance Boost */
 #define X86_FEATURE_EPB		( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE ( 7*32+ 4) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
 #define X86_FEATURE_PLN		( 7*32+ 5) /* Intel Power Limit Notification */
 #define X86_FEATURE_PTS		( 7*32+ 6) /* Intel Package Thermal Status */
 #define X86_FEATURE_DTHERM	( 7*32+ 7) /* Digital Thermal Sensor */
<span class="p_chunk">@@ -199,6 +200,9 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_HWP_PKG_REQ ( 7*32+14) /* Intel HWP_PKG_REQ */
 #define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */
 
<span class="p_add">+/* Because the ALTERNATIVE scheme is for members of the X86_FEATURE club... */</span>
<span class="p_add">+#define X86_FEATURE_KAISER	( 7*32+31) /* CONFIG_PAGE_TABLE_ISOLATION w/o nokaiser */</span>
<span class="p_add">+</span>
 /* Virtualization flags: Linux defined, word 8 */
 #define X86_FEATURE_TPR_SHADOW  ( 8*32+ 0) /* Intel TPR Shadow */
 #define X86_FEATURE_VNMI        ( 8*32+ 1) /* Intel Virtual NMI */
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 4e10d73cf018..880db91d9457 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> struct gdt_page {</span>
 	struct desc_struct gdt[GDT_ENTRIES];
 } __attribute__((aligned(PAGE_SIZE)));
 
<span class="p_del">-DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);</span>
<span class="p_add">+DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page);</span>
 
 static inline struct desc_struct *get_cpu_gdt_table(unsigned int cpu)
 {
<span class="p_header">diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h</span>
<span class="p_header">index 59caa55fb9b5..ee52ff858699 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hw_irq.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hw_irq.h</span>
<span class="p_chunk">@@ -187,7 +187,7 @@</span> <span class="p_context"> extern char irq_entries_start[];</span>
 #define VECTOR_RETRIGGERED	((void *)~0UL)
 
 typedef struct irq_desc* vector_irq_t[NR_VECTORS];
<span class="p_del">-DECLARE_PER_CPU(vector_irq_t, vector_irq);</span>
<span class="p_add">+DECLARE_PER_CPU_USER_MAPPED(vector_irq_t, vector_irq);</span>
 
 #endif /* !ASSEMBLY_ */
 
<span class="p_header">diff --git a/arch/x86/include/asm/kaiser.h b/arch/x86/include/asm/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..802bbbdfe143</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,141 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_KAISER_H</span>
<span class="p_add">+#define _ASM_X86_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/processor-flags.h&gt; /* For PCID constants */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This file includes the definitions for the KAISER feature.</span>
<span class="p_add">+ * KAISER is a counter measure against x86_64 side channel attacks on</span>
<span class="p_add">+ * the kernel virtual memory.  It has a shadow pgd for every process: the</span>
<span class="p_add">+ * shadow pgd has a minimalistic kernel-set mapped, but includes the whole</span>
<span class="p_add">+ * user memory. Within a kernel context switch, or when an interrupt is handled,</span>
<span class="p_add">+ * the pgd is switched to the normal one. When the system switches to user mode,</span>
<span class="p_add">+ * the shadow pgd is enabled. By this, the virtual memory caches are freed,</span>
<span class="p_add">+ * and the user may not attack the whole kernel memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A minimalistic kernel mapping holds the parts needed to be mapped in user</span>
<span class="p_add">+ * mode, such as the entry/exit functions of the user space, or the stacks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define KAISER_SHADOW_PGD_OFFSET 0x1000</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
<span class="p_add">+.macro _SWITCH_TO_KERNEL_CR3 reg</span>
<span class="p_add">+movq %cr3, \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */</span>
<span class="p_add">+ALTERNATIVE &quot;&quot;, &quot;bts $63, \reg&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+movq \reg, %cr3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro _SWITCH_TO_USER_CR3 reg regb</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * regb must be the low byte portion of reg: because we have arranged</span>
<span class="p_add">+ * for the low byte of the user PCID to serve as the high byte of NOFLUSH</span>
<span class="p_add">+ * (0x80 for each when PCID is enabled, or 0x00 when PCID and NOFLUSH are</span>
<span class="p_add">+ * not enabled): so that the one register can update both memory and cr3.</span>
<span class="p_add">+ */</span>
<span class="p_add">+movq %cr3, \reg</span>
<span class="p_add">+orq  PER_CPU_VAR(x86_cr3_pcid_user), \reg</span>
<span class="p_add">+js   9f</span>
<span class="p_add">+/* If PCID enabled, FLUSH this time, reset to NOFLUSH for next time */</span>
<span class="p_add">+movb \regb, PER_CPU_VAR(x86_cr3_pcid_user+7)</span>
<span class="p_add">+9:</span>
<span class="p_add">+movq \reg, %cr3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, &quot;pushq %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_KERNEL_CR3 %rax</span>
<span class="p_add">+popq %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_USER_CR3</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, &quot;pushq %rax&quot;, X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_USER_CR3 %rax %al</span>
<span class="p_add">+popq %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3_NO_STACK</span>
<span class="p_add">+ALTERNATIVE &quot;jmp 8f&quot;, \</span>
<span class="p_add">+	__stringify(movq %rax, PER_CPU_VAR(unsafe_stack_register_backup)), \</span>
<span class="p_add">+	X86_FEATURE_KAISER</span>
<span class="p_add">+_SWITCH_TO_KERNEL_CR3 %rax</span>
<span class="p_add">+movq PER_CPU_VAR(unsafe_stack_register_backup), %rax</span>
<span class="p_add">+8:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_USER_CR3</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_KERNEL_CR3_NO_STACK</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Upon kernel/user mode switch, it may happen that the address</span>
<span class="p_add">+ * space has to be switched before the registers have been</span>
<span class="p_add">+ * stored.  To change the address space, another register is</span>
<span class="p_add">+ * needed.  A register therefore has to be stored/restored.</span>
<span class="p_add">+*/</span>
<span class="p_add">+DECLARE_PER_CPU_USER_MAPPED(unsigned long, unsafe_stack_register_backup);</span>
<span class="p_add">+</span>
<span class="p_add">+DECLARE_PER_CPU(unsigned long, x86_cr3_pcid_user);</span>
<span class="p_add">+</span>
<span class="p_add">+extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];</span>
<span class="p_add">+</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+extern void __init kaiser_check_boottime_disable(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled	0</span>
<span class="p_add">+static inline void __init kaiser_check_boottime_disable(void) {}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Kaiser function prototypes are needed even when CONFIG_PAGE_TABLE_ISOLATION is not set,</span>
<span class="p_add">+ * so as to build with tests on kaiser_enabled instead of #ifdefs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_add_mapping - map a virtual memory part to the shadow (user) mapping</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ *  @flags: The mapping flags of the pages</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  The mapping is done on a global scope, so no bigger</span>
<span class="p_add">+ *  synchronization has to be done.  the pages have to be</span>
<span class="p_add">+ *  manually unmapped again when they are not needed any longer.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern int kaiser_add_mapping(unsigned long addr, unsigned long size, unsigned long flags);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_remove_mapping - unmap a virtual memory part of the shadow mapping</span>
<span class="p_add">+ *  @addr: the start address of the range</span>
<span class="p_add">+ *  @size: the size of the range</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_remove_mapping(unsigned long start, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ *  kaiser_init - Initialize the shadow mapping</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  Most parts of the shadow mapping can be mapped upon boot</span>
<span class="p_add">+ *  time.  Only per-process things like the thread stacks</span>
<span class="p_add">+ *  or a new LDT have to be mapped at runtime.  These boot-</span>
<span class="p_add">+ *  time mappings are permanent and never unmapped.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void kaiser_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_KAISER_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 6ec0c8b2e9df..84c62d950023 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -18,6 +18,12 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 #include &lt;asm/x86_init.h&gt;
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd);
 void ptdump_walk_pgd_level_checkwx(void);
 
<span class="p_chunk">@@ -653,7 +659,17 @@</span> <span class="p_context"> static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)</span>
 
 static inline int pgd_bad(pgd_t pgd)
 {
<span class="p_del">-	return (pgd_flags(pgd) &amp; ~_PAGE_USER) != _KERNPG_TABLE;</span>
<span class="p_add">+	pgdval_t ignore_flags = _PAGE_USER;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We set NX on KAISER pgds that map userspace memory so</span>
<span class="p_add">+	 * that userspace can not meaningfully use the kernel</span>
<span class="p_add">+	 * page table by accident; it will fault on the first</span>
<span class="p_add">+	 * instruction it tries to run.  See native_set_pgd().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pgd_flags(pgd) &amp; ~ignore_flags) != _KERNPG_TABLE;</span>
 }
 
 static inline int pgd_none(pgd_t pgd)
<span class="p_chunk">@@ -855,7 +871,15 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
  */
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
<span class="p_del">-       memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+	memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		/* Clone the shadow pgd part as well */</span>
<span class="p_add">+		memcpy(native_get_shadow_pgd(dst),</span>
<span class="p_add">+			native_get_shadow_pgd(src),</span>
<span class="p_add">+			count * sizeof(pgd_t));</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 2ee781114d34..c810226e741a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -106,9 +106,32 @@</span> <span class="p_context"> static inline void native_pud_clear(pud_t *pud)</span>
 	native_set_pud(pud, native_make_pud(0));
 }
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *native_get_shadow_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_VM</span>
<span class="p_add">+	/* linux/mmdebug.h may not have been included at this point */</span>
<span class="p_add">+	BUG_ON(!kaiser_enabled);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return (pgd_t *)((unsigned long)pgdp | (unsigned long)PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline pgd_t *native_get_shadow_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG_ON(1);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_del">-	*pgdp = pgd;</span>
<span class="p_add">+	*pgdp = kaiser_set_shadow_pgd(pgdp, pgd);</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index 79c91853e50e..8dba273da25a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"></span>
 #define _PAGE_NX	(_AT(pteval_t, 0))
 #endif
 
<span class="p_del">-#define _PAGE_PROTNONE	(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PROTNONE)</span>
<span class="p_add">+#define _PAGE_PROTNONE  (_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PROTNONE)</span>
 
 #define _PAGE_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |	\
 			 _PAGE_ACCESSED | _PAGE_DIRTY)
<span class="p_chunk">@@ -102,6 +102,33 @@</span> <span class="p_context"></span>
 			 _PAGE_SOFT_DIRTY)
 #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
 
<span class="p_add">+/* The ASID is the lower 12 bits of CR3 */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_MASK  (_AC((1&lt;&lt;12)-1,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Mask for all the PCID-related bits in CR3: */</span>
<span class="p_add">+#define X86_CR3_PCID_MASK       (X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_MASK)</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x0,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_PAGE_TABLE_ISOLATION) &amp;&amp; defined(CONFIG_X86_64)</span>
<span class="p_add">+/* Let X86_CR3_PCID_ASID_USER be usable for the X86_CR3_PCID_NOFLUSH bit */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER	(_AC(0x80,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x0,UL))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PCIDs are unsupported on 32-bit and none of these bits can be</span>
<span class="p_add">+ * set in CR3:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * The cache modes defined here are used to translate between pure SW usage
  * and the HW defined cache mode bits and/or PAT entries.
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 2d5a50cb61a2..f3bdaed0188f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -305,7 +305,7 @@</span> <span class="p_context"> struct tss_struct {</span>
 
 } ____cacheline_aligned;
 
<span class="p_del">-DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss);</span>
<span class="p_add">+DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss);</span>
 
 #ifdef CONFIG_X86_32
 DECLARE_PER_CPU(unsigned long, cpu_current_top_of_stack);
<span class="p_header">diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h</span>
<span class="p_header">index baad72e4c100..6045cef376c2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pvclock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pvclock.h</span>
<span class="p_chunk">@@ -4,6 +4,15 @@</span> <span class="p_context"></span>
 #include &lt;linux/clocksource.h&gt;
 #include &lt;asm/pvclock-abi.h&gt;
 
<span class="p_add">+#ifdef CONFIG_PARAVIRT_CLOCK</span>
<span class="p_add">+extern struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* some helper functions for xen and kvm pv clock sources */
 cycle_t pvclock_clocksource_read(struct pvclock_vcpu_time_info *src);
 u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src);
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 9fc5968da820..a691b66cc40a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -131,6 +131,24 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 	cr4_set_bits(mask);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Declare a couple of kaiser interfaces here for convenience,</span>
<span class="p_add">+ * to avoid the need for asm/kaiser.h in unexpected places.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern int kaiser_enabled;</span>
<span class="p_add">+extern void kaiser_setup_pcid(void);</span>
<span class="p_add">+extern void kaiser_flush_tlb_on_return_to_user(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define kaiser_enabled 0</span>
<span class="p_add">+static inline void kaiser_setup_pcid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_flush_tlb_on_return_to_user(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline void __native_flush_tlb(void)
 {
 	/*
<span class="p_chunk">@@ -139,6 +157,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 	 * back:
 	 */
 	preempt_disable();
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		kaiser_flush_tlb_on_return_to_user();</span>
 	native_write_cr3(native_read_cr3());
 	preempt_enable();
 }
<span class="p_chunk">@@ -148,20 +168,27 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global_irq_disabled(void)</span>
 	unsigned long cr4;
 
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
<span class="p_del">-	/* clear PGE */</span>
<span class="p_del">-	native_write_cr4(cr4 &amp; ~X86_CR4_PGE);</span>
<span class="p_del">-	/* write old PGE again and flush TLBs */</span>
<span class="p_del">-	native_write_cr4(cr4);</span>
<span class="p_add">+	if (cr4 &amp; X86_CR4_PGE) {</span>
<span class="p_add">+		/* clear PGE and flush TLB of all entries */</span>
<span class="p_add">+		native_write_cr4(cr4 &amp; ~X86_CR4_PGE);</span>
<span class="p_add">+		/* restore PGE as it was before */</span>
<span class="p_add">+		native_write_cr4(cr4);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* do it with cr3, letting kaiser flush user PCID */</span>
<span class="p_add">+		__native_flush_tlb();</span>
<span class="p_add">+	}</span>
 }
 
 static inline void __native_flush_tlb_global(void)
 {
 	unsigned long flags;
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_INVPCID)) {</span>
 		/*
 		 * Using INVPCID is considerably faster than a pair of writes
 		 * to CR4 sandwiched inside an IRQ flag save/restore.
<span class="p_add">+		 *</span>
<span class="p_add">+	 	 * Note, this works with CR4.PCIDE=0 or 1.</span>
 		 */
 		invpcid_flush_all();
 		return;
<span class="p_chunk">@@ -173,24 +200,45 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 	 * be called from deep inside debugging code.)
 	 */
 	raw_local_irq_save(flags);
<span class="p_del">-</span>
 	__native_flush_tlb_global_irq_disabled();
<span class="p_del">-</span>
 	raw_local_irq_restore(flags);
 }
 
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_del">-	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SIMICS #GP&#39;s if you run INVPCID with type 2/3</span>
<span class="p_add">+	 * and X86_CR4_PCIDE clear.  Shame!</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The ASIDs used below are hard-coded.  But, we must not</span>
<span class="p_add">+	 * call invpcid(type=1/2) before CR4.PCIDE=1.  Just call</span>
<span class="p_add">+	 * invlpg in the case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE)) {</span>
<span class="p_add">+		if (kaiser_enabled)</span>
<span class="p_add">+			kaiser_flush_tlb_on_return_to_user();</span>
<span class="p_add">+		asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Flush the address out of both PCIDs. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * An optimization here might be to determine addresses</span>
<span class="p_add">+	 * that are only kernel-mapped and only flush the kernel</span>
<span class="p_add">+	 * ASID.  But, userspace flushes are probably much more</span>
<span class="p_add">+	 * important performance-wise.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Make sure to do only a single invpcid when KAISER is</span>
<span class="p_add">+	 * disabled and we have only a single ASID.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled)</span>
<span class="p_add">+		invpcid_flush_one(X86_CR3_PCID_ASID_USER, addr);</span>
<span class="p_add">+	invpcid_flush_one(X86_CR3_PCID_ASID_KERN, addr);</span>
 }
 
 static inline void __flush_tlb_all(void)
 {
<span class="p_del">-	if (cpu_has_pge)</span>
<span class="p_del">-		__flush_tlb_global();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		__flush_tlb();</span>
<span class="p_del">-</span>
<span class="p_add">+	__flush_tlb_global();</span>
 	/*
 	 * Note: if we somehow had PCID but not PGE, then this wouldn&#39;t work --
 	 * we&#39;d end up flushing kernel translations for the current ASID but
<span class="p_header">diff --git a/arch/x86/include/asm/vdso.h b/arch/x86/include/asm/vdso.h</span>
<span class="p_header">index 756de9190aec..deabaf9759b6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/vdso.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/vdso.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"> struct vdso_image {</span>
 
 	long sym_vvar_page;
 	long sym_hpet_page;
<span class="p_add">+	long sym_pvclock_page;</span>
 	long sym_VDSO32_NOTE_MASK;
 	long sym___kernel_sigreturn;
 	long sym___kernel_rt_sigreturn;
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 79887abcb5e1..1361779f44fe 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -77,7 +77,8 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index aa1e7246b06b..cc154ac64f00 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static const struct cpu_dev default_cpu = {</span>
 
 static const struct cpu_dev *this_cpu = &amp;default_cpu;
 
<span class="p_del">-DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct gdt_page, gdt_page) = { .gdt = {</span>
 #ifdef CONFIG_X86_64
 	/*
 	 * We need valid kernel segments for data and code in long mode too
<span class="p_chunk">@@ -324,8 +324,21 @@</span> <span class="p_context"> static __always_inline void setup_smap(struct cpuinfo_x86 *c)</span>
 static void setup_pcid(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_PCID)) {
<span class="p_del">-		if (cpu_has(c, X86_FEATURE_PGE)) {</span>
<span class="p_add">+		if (cpu_has(c, X86_FEATURE_PGE) || kaiser_enabled) {</span>
 			cr4_set_bits(X86_CR4_PCIDE);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * INVPCID has two &quot;groups&quot; of types:</span>
<span class="p_add">+			 * 1/2: Invalidate an individual address</span>
<span class="p_add">+			 * 3/4: Invalidate all contexts</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * 1/2 take a PCID, but 3/4 do not.  So, 3/4</span>
<span class="p_add">+			 * ignore the PCID argument in the descriptor.</span>
<span class="p_add">+			 * But, we have to be careful not to call 1/2</span>
<span class="p_add">+			 * with an actual non-zero PCID in them before</span>
<span class="p_add">+			 * we do the above cr4_set_bits().</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (cpu_has(c, X86_FEATURE_INVPCID))</span>
<span class="p_add">+				set_cpu_cap(c, X86_FEATURE_INVPCID_SINGLE);</span>
 		} else {
 			/*
 			 * flush_tlb_all(), as currently implemented, won&#39;t
<span class="p_chunk">@@ -338,6 +351,7 @@</span> <span class="p_context"> static void setup_pcid(struct cpuinfo_x86 *c)</span>
 			clear_cpu_cap(c, X86_FEATURE_PCID);
 		}
 	}
<span class="p_add">+	kaiser_setup_pcid();</span>
 }
 
 /*
<span class="p_chunk">@@ -1229,7 +1243,7 @@</span> <span class="p_context"> static const unsigned int exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
 	  [DEBUG_STACK - 1]			= DEBUG_STKSZ
 };
 
<span class="p_del">-static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(char, exception_stacks</span>
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 
 /* May not be marked __init: used by software suspend */
<span class="p_chunk">@@ -1392,6 +1406,14 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	 * try to read it.
 	 */
 	cr4_init_shadow();
<span class="p_add">+	if (!kaiser_enabled) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * secondary_startup_64() deferred setting PGE in cr4:</span>
<span class="p_add">+		 * probe_page_size_mask() sets it on the boot cpu,</span>
<span class="p_add">+		 * but it needs to be set on each secondary cpu.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PGE);</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * Load microcode on this cpu if a valid microcode is available.
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event_intel_ds.c b/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_header">index 1e7de3cefc9c..f01b3a12dce0 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event_intel_ds.c</span>
<span class="p_chunk">@@ -2,11 +2,15 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/slab.h&gt;
 
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 #include &lt;asm/perf_event.h&gt;
 #include &lt;asm/insn.h&gt;
 
 #include &quot;perf_event.h&quot;
 
<span class="p_add">+static</span>
<span class="p_add">+DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
 /* The size of a BTS record in bytes: */
 #define BTS_RECORD_SIZE		24
 
<span class="p_chunk">@@ -268,6 +272,39 @@</span> <span class="p_context"> void fini_debug_store_on_cpu(int cpu)</span>
 
 static DEFINE_PER_CPU(void *, insn_buffer);
 
<span class="p_add">+static void *dsalloc(size_t size, gfp_t flags, int node)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	unsigned int order = get_order(size);</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = __alloc_pages_node(node, flags | __GFP_ZERO, order);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr = (unsigned long)page_address(page);</span>
<span class="p_add">+	if (kaiser_add_mapping(addr, size, __PAGE_KERNEL) &lt; 0) {</span>
<span class="p_add">+		__free_pages(page, order);</span>
<span class="p_add">+		addr = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return (void *)addr;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return kmalloc_node(size, flags | __GFP_ZERO, node);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dsfree(const void *buffer, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (!buffer)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	kaiser_remove_mapping((unsigned long)buffer, size);</span>
<span class="p_add">+	free_pages((unsigned long)buffer, get_order(size));</span>
<span class="p_add">+#else</span>
<span class="p_add">+	kfree(buffer);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int alloc_pebs_buffer(int cpu)
 {
 	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;
<span class="p_chunk">@@ -278,7 +315,7 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (!x86_pmu.pebs)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
<span class="p_add">+	buffer = dsalloc(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
 	if (unlikely(!buffer))
 		return -ENOMEM;
 
<span class="p_chunk">@@ -289,7 +326,7 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (x86_pmu.intel_cap.pebs_format &lt; 2) {
 		ibuffer = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);
 		if (!ibuffer) {
<span class="p_del">-			kfree(buffer);</span>
<span class="p_add">+			dsfree(buffer, x86_pmu.pebs_buffer_size);</span>
 			return -ENOMEM;
 		}
 		per_cpu(insn_buffer, cpu) = ibuffer;
<span class="p_chunk">@@ -315,7 +352,8 @@</span> <span class="p_context"> static void release_pebs_buffer(int cpu)</span>
 	kfree(per_cpu(insn_buffer, cpu));
 	per_cpu(insn_buffer, cpu) = NULL;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;pebs_buffer_base);</span>
<span class="p_add">+	dsfree((void *)(unsigned long)ds-&gt;pebs_buffer_base,</span>
<span class="p_add">+			x86_pmu.pebs_buffer_size);</span>
 	ds-&gt;pebs_buffer_base = 0;
 }
 
<span class="p_chunk">@@ -329,7 +367,7 @@</span> <span class="p_context"> static int alloc_bts_buffer(int cpu)</span>
 	if (!x86_pmu.bts)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
<span class="p_add">+	buffer = dsalloc(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
 	if (unlikely(!buffer)) {
 		WARN_ONCE(1, &quot;%s: BTS buffer allocation failure\n&quot;, __func__);
 		return -ENOMEM;
<span class="p_chunk">@@ -355,19 +393,15 @@</span> <span class="p_context"> static void release_bts_buffer(int cpu)</span>
 	if (!ds || !x86_pmu.bts)
 		return;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;bts_buffer_base);</span>
<span class="p_add">+	dsfree((void *)(unsigned long)ds-&gt;bts_buffer_base, BTS_BUFFER_SIZE);</span>
 	ds-&gt;bts_buffer_base = 0;
 }
 
 static int alloc_ds_buffer(int cpu)
 {
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	struct debug_store *ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds = kzalloc_node(sizeof(*ds), GFP_KERNEL, node);</span>
<span class="p_del">-	if (unlikely(!ds))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	struct debug_store *ds = per_cpu_ptr(&amp;cpu_debug_store, cpu);</span>
 
<span class="p_add">+	memset(ds, 0, sizeof(*ds));</span>
 	per_cpu(cpu_hw_events, cpu).ds = ds;
 
 	return 0;
<span class="p_chunk">@@ -381,7 +415,6 @@</span> <span class="p_context"> static void release_ds_buffer(int cpu)</span>
 		return;
 
 	per_cpu(cpu_hw_events, cpu).ds = NULL;
<span class="p_del">-	kfree(ds);</span>
 }
 
 void release_ds_buffers(void)
<span class="p_header">diff --git a/arch/x86/kernel/espfix_64.c b/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">index 4d38416e2a7f..b02cb2ec6726 100644</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/espfix.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * Note: we only need 6*8 = 48 bytes for the espfix stack, but round
<span class="p_chunk">@@ -126,6 +127,15 @@</span> <span class="p_context"> void __init init_espfix_bsp(void)</span>
 	/* Install the espfix pud into the kernel page directory */
 	pgd_p = &amp;init_level4_pgt[pgd_index(ESPFIX_BASE_ADDR)];
 	pgd_populate(&amp;init_mm, pgd_p, (pud_t *)espfix_pud_page);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Just copy the top-level PGD that is mapping the espfix</span>
<span class="p_add">+	 * area to ensure it is mapped into the shadow user page</span>
<span class="p_add">+	 * tables.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		set_pgd(native_get_shadow_pgd(pgd_p),</span>
<span class="p_add">+			__pgd(_KERNPG_TABLE | __pa((pud_t *)espfix_pud_page)));</span>
<span class="p_add">+	}</span>
 
 	/* Randomize the locations */
 	init_espfix_random();
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index ffdc0e860390..4034e905741a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -183,8 +183,8 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	movq	$(init_level4_pgt - __START_KERNEL_map), %rax
 1:
 
<span class="p_del">-	/* Enable PAE mode and PGE */</span>
<span class="p_del">-	movl	$(X86_CR4_PAE | X86_CR4_PGE), %ecx</span>
<span class="p_add">+	/* Enable PAE and PSE, but defer PGE until kaiser_enabled is decided */</span>
<span class="p_add">+	movl	$(X86_CR4_PAE | X86_CR4_PSE), %ecx</span>
 	movq	%rcx, %cr4
 
 	/* Setup early boot stage 4 level pagetables. */
<span class="p_chunk">@@ -441,6 +441,27 @@</span> <span class="p_context"> early_idt_ripmsg:</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Each PGD needs to be 8k long and 8k aligned.  We do not</span>
<span class="p_add">+ * ever go out to userspace with these, so we do not</span>
<span class="p_add">+ * strictly *need* the second page, but this allows us to</span>
<span class="p_add">+ * have a single set_pgd() implementation that does not</span>
<span class="p_add">+ * need to worry about whether it has 4k or 8k to work</span>
<span class="p_add">+ * with.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures PGDs are 8k long:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KAISER_USER_PGD_FILL	512</span>
<span class="p_add">+/* This ensures they are 8k-aligned: */</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#define KAISER_USER_PGD_FILL	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -450,9 +471,10 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_level4_pgt)</span>
 	.fill	511,8,0
 	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(early_dynamic_pgts)
 	.fill	512*EARLY_DYNAMIC_PAGE_TABLES,8,0
<span class="p_chunk">@@ -460,16 +482,18 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #ifndef CONFIG_XEN
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_level4_pgt)</span>
 	.fill	512,8,0
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 #else
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_level4_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
 	.org    init_level4_pgt + L4_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
 	.org    init_level4_pgt + L4_START_KERNEL*8, 0
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
 	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_ident_pgt)
 	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
<span class="p_chunk">@@ -480,6 +504,7 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #endif
<span class="p_add">+	.fill	KAISER_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_kernel_pgt)
 	.fill	L3_START_KERNEL,8,0
<span class="p_header">diff --git a/arch/x86/kernel/irqinit.c b/arch/x86/kernel/irqinit.c</span>
<span class="p_header">index 1423ab1b0312..f480b38a03c3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irqinit.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irqinit.c</span>
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> static struct irqaction irq2 = {</span>
 	.flags = IRQF_NO_THREAD,
 };
 
<span class="p_del">-DEFINE_PER_CPU(vector_irq_t, vector_irq) = {</span>
<span class="p_add">+DEFINE_PER_CPU_USER_MAPPED(vector_irq_t, vector_irq) = {</span>
 	[0 ... NR_VECTORS - 1] = VECTOR_UNUSED,
 };
 
<span class="p_header">diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">index 2bd81e302427..ec1b06dc82d2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvmclock.c</span>
<span class="p_chunk">@@ -45,6 +45,11 @@</span> <span class="p_context"> early_param(&quot;no-kvmclock&quot;, parse_no_kvmclock);</span>
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
<span class="p_add">+struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return hv_clock;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index d6279593bcdd..bc429365b72a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/uaccess.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 
 #include &lt;asm/ldt.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_chunk">@@ -34,11 +35,21 @@</span> <span class="p_context"> static void flush_ldt(void *current_mm)</span>
 	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;size);
 }
 
<span class="p_add">+static void __free_ldt_struct(struct ldt_struct *ldt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ldt-&gt;size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_add">+		vfree(ldt-&gt;entries);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		free_page((unsigned long)ldt-&gt;entries);</span>
<span class="p_add">+	kfree(ldt);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* The caller must call finalize_ldt_struct on the result. LDT starts zeroed. */
 static struct ldt_struct *alloc_ldt_struct(int size)
 {
 	struct ldt_struct *new_ldt;
 	int alloc_size;
<span class="p_add">+	int ret;</span>
 
 	if (size &gt; LDT_ENTRIES)
 		return NULL;
<span class="p_chunk">@@ -66,7 +77,13 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(int size)</span>
 		return NULL;
 	}
 
<span class="p_add">+	ret = kaiser_add_mapping((unsigned long)new_ldt-&gt;entries, alloc_size,</span>
<span class="p_add">+				 __PAGE_KERNEL);</span>
 	new_ldt-&gt;size = size;
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		__free_ldt_struct(new_ldt);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
 	return new_ldt;
 }
 
<span class="p_chunk">@@ -92,12 +109,10 @@</span> <span class="p_context"> static void free_ldt_struct(struct ldt_struct *ldt)</span>
 	if (likely(!ldt))
 		return;
 
<span class="p_add">+	kaiser_remove_mapping((unsigned long)ldt-&gt;entries,</span>
<span class="p_add">+			      ldt-&gt;size * LDT_ENTRY_SIZE);</span>
 	paravirt_free_ldt(ldt-&gt;entries, ldt-&gt;size);
<span class="p_del">-	if (ldt-&gt;size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-		vfree(ldt-&gt;entries);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		free_page((unsigned long)ldt-&gt;entries);</span>
<span class="p_del">-	kfree(ldt);</span>
<span class="p_add">+	__free_ldt_struct(ldt);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index 8aa05583bc42..0677bf8d3a42 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -9,7 +9,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_irq_ops, save_fl, &quot;pushfq; popq %rax&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;movq %cr2, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;movq %cr3, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;movq %rdi, %cr3&quot;);
<span class="p_del">-DEF_NATIVE(pv_mmu_ops, flush_tlb_single, &quot;invlpg (%rdi)&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 DEF_NATIVE(pv_cpu_ops, wbinvd, &quot;wbinvd&quot;);
 
<span class="p_chunk">@@ -62,7 +61,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
<span class="p_del">-		PATCH_SITE(pv_mmu_ops, flush_tlb_single);</span>
 		PATCH_SITE(pv_cpu_ops, wbinvd);
 #if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index 9f7c21c22477..7c5c5dc90ffa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -39,7 +39,7 @@</span> <span class="p_context"></span>
  * section. Since TSS&#39;s are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
<span class="p_del">-__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, cpu_tss) = {</span>
<span class="p_add">+__visible DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, cpu_tss) = {</span>
 	.x86_tss = {
 		.sp0 = TOP_OF_INIT_STACK,
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index e67b834279b2..bbaae4cf9e8e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -112,6 +112,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/prom.h&gt;
 #include &lt;asm/microcode.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  * max_low_pfn_mapped: highest direct mapped pfn under 4GB
<span class="p_chunk">@@ -1016,6 +1017,12 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 	 */
 	init_hypervisor_platform();
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This needs to happen right after XENPV is set on xen and</span>
<span class="p_add">+	 * kaiser_enabled is checked below in cleanup_highmap().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_check_boottime_disable();</span>
<span class="p_add">+</span>
 	x86_init.resources.probe_roms();
 
 	/* after parse_early_param, so could debug it */
<span class="p_header">diff --git a/arch/x86/kernel/tracepoint.c b/arch/x86/kernel/tracepoint.c</span>
<span class="p_header">index 1c113db9ed57..2bb5ee464df3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tracepoint.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tracepoint.c</span>
<span class="p_chunk">@@ -9,10 +9,12 @@</span> <span class="p_context"></span>
 #include &lt;linux/atomic.h&gt;
 
 atomic_t trace_idt_ctr = ATOMIC_INIT(0);
<span class="p_add">+__aligned(PAGE_SIZE)</span>
 struct desc_ptr trace_idt_descr = { NR_VECTORS * 16 - 1,
 				(unsigned long) trace_idt_table };
 
 /* No need to be aligned, but done to keep all IDTs defined the same way. */
<span class="p_add">+__aligned(PAGE_SIZE)</span>
 gate_desc trace_idt_table[NR_VECTORS] __page_aligned_bss;
 
 static int trace_irq_vector_refcount;
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 796f1ec67469..ccf17dbfea09 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -759,7 +759,8 @@</span> <span class="p_context"> int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)</span>
 			return 1;
 
 		/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */
<span class="p_del">-		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_MASK) || !is_long_mode(vcpu))</span>
<span class="p_add">+		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_ASID_MASK) ||</span>
<span class="p_add">+		    !is_long_mode(vcpu))</span>
 			return 1;
 	}
 
<span class="p_header">diff --git a/arch/x86/lib/cmdline.c b/arch/x86/lib/cmdline.c</span>
<span class="p_header">index 422db000d727..a744506856b1 100644</span>
<span class="p_header">--- a/arch/x86/lib/cmdline.c</span>
<span class="p_header">+++ b/arch/x86/lib/cmdline.c</span>
<span class="p_chunk">@@ -82,3 +82,108 @@</span> <span class="p_context"> int cmdline_find_option_bool(const char *cmdline, const char *option)</span>
 
 	return 0;	/* Buffer overrun */
 }
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Find a non-boolean option (i.e. option=argument). In accordance with</span>
<span class="p_add">+ * standard Linux practice, if this option is repeated, this returns the</span>
<span class="p_add">+ * last instance on the command line.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @cmdline: the cmdline string</span>
<span class="p_add">+ * @max_cmdline_size: the maximum size of cmdline</span>
<span class="p_add">+ * @option: option string to look for</span>
<span class="p_add">+ * @buffer: memory buffer to return the option argument</span>
<span class="p_add">+ * @bufsize: size of the supplied memory buffer</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns the length of the argument (regardless of if it was</span>
<span class="p_add">+ * truncated to fit in the buffer), or -1 on not found.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int</span>
<span class="p_add">+__cmdline_find_option(const char *cmdline, int max_cmdline_size,</span>
<span class="p_add">+		      const char *option, char *buffer, int bufsize)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char c;</span>
<span class="p_add">+	int pos = 0, len = -1;</span>
<span class="p_add">+	const char *opptr = NULL;</span>
<span class="p_add">+	char *bufptr = buffer;</span>
<span class="p_add">+	enum {</span>
<span class="p_add">+		st_wordstart = 0,	/* Start of word/after whitespace */</span>
<span class="p_add">+		st_wordcmp,	/* Comparing this word */</span>
<span class="p_add">+		st_wordskip,	/* Miscompare, skip */</span>
<span class="p_add">+		st_bufcpy,	/* Copying this to buffer */</span>
<span class="p_add">+	} state = st_wordstart;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cmdline)</span>
<span class="p_add">+		return -1;      /* No command line */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This &#39;pos&#39; check ensures we do not overrun</span>
<span class="p_add">+	 * a non-NULL-terminated &#39;cmdline&#39;</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (pos++ &lt; max_cmdline_size) {</span>
<span class="p_add">+		c = *(char *)cmdline++;</span>
<span class="p_add">+		if (!c)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (state) {</span>
<span class="p_add">+		case st_wordstart:</span>
<span class="p_add">+			if (myisspace(c))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			state = st_wordcmp;</span>
<span class="p_add">+			opptr = option;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_wordcmp:</span>
<span class="p_add">+			if ((c == &#39;=&#39;) &amp;&amp; !*opptr) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We matched all the way to the end of the</span>
<span class="p_add">+				 * option we were looking for, prepare to</span>
<span class="p_add">+				 * copy the argument.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				len = 0;</span>
<span class="p_add">+				bufptr = buffer;</span>
<span class="p_add">+				state = st_bufcpy;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			} else if (c == *opptr++) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We are currently matching, so continue</span>
<span class="p_add">+				 * to the next character on the cmdline.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			state = st_wordskip;</span>
<span class="p_add">+			/* fall through */</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_wordskip:</span>
<span class="p_add">+			if (myisspace(c))</span>
<span class="p_add">+				state = st_wordstart;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		case st_bufcpy:</span>
<span class="p_add">+			if (myisspace(c)) {</span>
<span class="p_add">+				state = st_wordstart;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Increment len, but don&#39;t overrun the</span>
<span class="p_add">+				 * supplied buffer and leave room for the</span>
<span class="p_add">+				 * NULL terminator.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (++len &lt; bufsize)</span>
<span class="p_add">+					*bufptr++ = c;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (bufsize)</span>
<span class="p_add">+		*bufptr = &#39;\0&#39;;</span>
<span class="p_add">+</span>
<span class="p_add">+	return len;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int cmdline_find_option(const char *cmdline, const char *option, char *buffer,</span>
<span class="p_add">+			int bufsize)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cmdline_find_option(cmdline, COMMAND_LINE_SIZE, option,</span>
<span class="p_add">+				     buffer, bufsize);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 1ae7c141f778..61e6cead9c4a 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -32,3 +32,4 @@</span> <span class="p_context"> obj-$(CONFIG_ACPI_NUMA)		+= srat.o</span>
 obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o
 
 obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o
<span class="p_add">+obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= kaiser.o</span>
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index ed4b372860e4..2bd45ae91eb3 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -165,7 +165,7 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	/* Enable PGE if available */
<span class="p_del">-	if (cpu_has_pge) {</span>
<span class="p_add">+	if (cpu_has_pge &amp;&amp; !kaiser_enabled) {</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
 		__supported_pte_mask |= _PAGE_GLOBAL;
 	} else
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index ec081fe0ce2c..d76ec9348cff 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -395,6 +395,16 @@</span> <span class="p_context"> void __init cleanup_highmap(void)</span>
 			continue;
 		if (vaddr &lt; (unsigned long) _text || vaddr &gt; end)
 			set_pmd(pmd, __pmd(0));
<span class="p_add">+		else if (kaiser_enabled) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * level2_kernel_pgt is initialized with _PAGE_GLOBAL:</span>
<span class="p_add">+			 * clear that now.  This is not important, so long as</span>
<span class="p_add">+			 * CR4.PGE remains clear, but it removes an anomaly.</span>
<span class="p_add">+			 * Physical mapping setup below avoids _PAGE_GLOBAL</span>
<span class="p_add">+			 * by use of massage_pgprot() inside pfn_pte() etc.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_pmd(pmd, pmd_clear_flags(*pmd, _PAGE_GLOBAL));</span>
<span class="p_add">+		}</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/x86/mm/kaiser.c b/arch/x86/mm/kaiser.c</span>
new file mode 100644
<span class="p_header">index 000000000000..b0b3a69f1c7f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c</span>
<span class="p_chunk">@@ -0,0 +1,456 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/interrupt.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/ftrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Kernel/User page tables isolation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;	/* to verify its kaiser declarations */</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+int kaiser_enabled __read_mostly = 1;</span>
<span class="p_add">+EXPORT_SYMBOL(kaiser_enabled);	/* for inlined TLB flush functions */</span>
<span class="p_add">+</span>
<span class="p_add">+__visible</span>
<span class="p_add">+DEFINE_PER_CPU_USER_MAPPED(unsigned long, unsafe_stack_register_backup);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These can have bit 63 set, so we can not just use a plain &quot;or&quot;</span>
<span class="p_add">+ * instruction to get their value or&#39;d into CR3.  It would take</span>
<span class="p_add">+ * another register.  So, we use a memory reference to these instead.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is also handy because systems that do not support PCIDs</span>
<span class="p_add">+ * just end up or&#39;ing a 0 into their CR3, which does no harm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+DEFINE_PER_CPU(unsigned long, x86_cr3_pcid_user);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * At runtime, the only things we map are some things for CPU</span>
<span class="p_add">+ * hotplug, and stacks for new processes.  No two CPUs will ever</span>
<span class="p_add">+ * be populating the same addresses, so we only need to ensure</span>
<span class="p_add">+ * that we protect between two CPUs trying to allocate and</span>
<span class="p_add">+ * populate the same page table page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only take this lock when doing a set_p[4um]d(), but it is not</span>
<span class="p_add">+ * needed for doing a set_pte().  We assume that only the *owner*</span>
<span class="p_add">+ * of a given allocation will be doing this for _their_</span>
<span class="p_add">+ * allocation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures that once a system has been running for a while</span>
<span class="p_add">+ * and there have been stacks all over and these page tables</span>
<span class="p_add">+ * are fully populated, there will be no further acquisitions of</span>
<span class="p_add">+ * this lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static DEFINE_SPINLOCK(shadow_table_allocation_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns -1 on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long get_pa_from_mapping(unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(vaddr);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We made all the kernel PGDs present in kaiser_init().</span>
<span class="p_add">+	 * We expect them to stay that way.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(pgd_none(*pgd));</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PGDs are either 512GB or 128TB on all x86_64</span>
<span class="p_add">+	 * configurations.  We don&#39;t handle these.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(pgd_large(*pgd));</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, vaddr);</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_large(*pud))</span>
<span class="p_add">+		return (pud_pfn(*pud) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PUD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, vaddr);</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_large(*pmd))</span>
<span class="p_add">+		return (pmd_pfn(*pmd) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PMD_PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, vaddr);</span>
<span class="p_add">+	if (pte_none(*pte)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pte_pfn(*pte) &lt;&lt; PAGE_SHIFT) | (vaddr &amp; ~PAGE_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a relatively normal page table walk, except that it</span>
<span class="p_add">+ * also tries to allocate page tables pages along the way.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *kaiser_pagetable_walk(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pgd_t *pgd = native_get_shadow_pgd(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;All shadow pgds should have been populated&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pud_none(*pud)) {</span>
<span class="p_add">+			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_add">+			__inc_zone_page_state(virt_to_page((void *)</span>
<span class="p_add">+						new_pmd_page), NR_KAISERTABLE);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	/* The shadow page tables do not use large mappings: */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		spin_lock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+		if (pmd_none(*pmd)) {</span>
<span class="p_add">+			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
<span class="p_add">+			__inc_zone_page_state(virt_to_page((void *)</span>
<span class="p_add">+						new_pte_page), NR_KAISERTABLE);</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+		spin_unlock(&amp;shadow_table_allocation_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pte_offset_kernel(pmd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int kaiser_add_user_map(const void *__start_addr, unsigned long size,</span>
<span class="p_add">+			       unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	unsigned long start_addr = (unsigned long )__start_addr;</span>
<span class="p_add">+	unsigned long address = start_addr &amp; PAGE_MASK;</span>
<span class="p_add">+	unsigned long end_addr = PAGE_ALIGN(start_addr + size);</span>
<span class="p_add">+	unsigned long target_address;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * It is convenient for callers to pass in __PAGE_KERNEL etc,</span>
<span class="p_add">+	 * and there is no actual harm from setting _PAGE_GLOBAL, so</span>
<span class="p_add">+	 * long as CR4.PGE is not set.  But it is nonetheless troubling</span>
<span class="p_add">+	 * to see Kaiser itself setting _PAGE_GLOBAL (now that &quot;nokaiser&quot;</span>
<span class="p_add">+	 * requires that not to be #defined to 0): so mask it off here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flags &amp;= ~_PAGE_GLOBAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; address &lt; end_addr; address += PAGE_SIZE) {</span>
<span class="p_add">+		target_address = get_pa_from_mapping(address);</span>
<span class="p_add">+		if (target_address == -1) {</span>
<span class="p_add">+			ret = -EIO;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte = kaiser_pagetable_walk(address);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (pte_none(*pte)) {</span>
<span class="p_add">+			set_pte(pte, __pte(flags | target_address));</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pte_t tmp;</span>
<span class="p_add">+			set_pte(&amp;tmp, __pte(flags | target_address));</span>
<span class="p_add">+			WARN_ON_ONCE(!pte_same(*pte, tmp));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int kaiser_add_user_map_ptrs(const void *start, const void *end, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long size = end - start;</span>
<span class="p_add">+</span>
<span class="p_add">+	return kaiser_add_user_map(start, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Ensure that the top level of the (shadow) page tables are</span>
<span class="p_add">+ * entirely populated.  This ensures that all processes that get</span>
<span class="p_add">+ * forked have the same entries.  This way, we do not have to</span>
<span class="p_add">+ * ever go set up new entries in older processes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: we never free these, so there are no updates to them</span>
<span class="p_add">+ * after this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init kaiser_init_all_pgds(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = native_get_shadow_pgd(pgd_offset_k((unsigned long )0));</span>
<span class="p_add">+	for (i = PTRS_PER_PGD / 2; i &lt; PTRS_PER_PGD; i++) {</span>
<span class="p_add">+		pgd_t new_pgd;</span>
<span class="p_add">+		pud_t *pud = pud_alloc_one(&amp;init_mm,</span>
<span class="p_add">+					   PAGE_OFFSET + i * PGDIR_SIZE);</span>
<span class="p_add">+		if (!pud) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		inc_zone_page_state(virt_to_page(pud), NR_KAISERTABLE);</span>
<span class="p_add">+		new_pgd = __pgd(_KERNPG_TABLE |__pa(pud));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure not to stomp on some other pgd entry.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pgd_none(pgd[i])) {</span>
<span class="p_add">+			WARN_ON(1);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pgd(pgd + i, new_pgd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_early(start, size, flags) do {	\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map(start, size, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define kaiser_add_user_map_ptrs_early(start, end, flags) do {		\</span>
<span class="p_add">+	int __ret = kaiser_add_user_map_ptrs(start, end, flags);	\</span>
<span class="p_add">+	WARN_ON(__ret);							\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+void __init kaiser_check_boottime_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool enable = true;</span>
<span class="p_add">+	char arg[5];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_XENPV))</span>
<span class="p_add">+		goto silent_disable;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;pti&quot;, arg, sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0) {</span>
<span class="p_add">+		if (!strncmp(arg, &quot;on&quot;, 2))</span>
<span class="p_add">+			goto enable;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!strncmp(arg, &quot;off&quot;, 3))</span>
<span class="p_add">+			goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!strncmp(arg, &quot;auto&quot;, 4))</span>
<span class="p_add">+			goto skip;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nopti&quot;))</span>
<span class="p_add">+		goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+skip:</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)</span>
<span class="p_add">+		goto disable;</span>
<span class="p_add">+</span>
<span class="p_add">+enable:</span>
<span class="p_add">+	if (enable)</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_KAISER);</span>
<span class="p_add">+</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+disable:</span>
<span class="p_add">+	pr_info(&quot;disabled\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+silent_disable:</span>
<span class="p_add">+	kaiser_enabled = 0;</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_KAISER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * If anything in here fails, we will likely die on one of the</span>
<span class="p_add">+ * first kernel-&gt;user transitions and init will die.  But, we</span>
<span class="p_add">+ * will have most of the kernel up by then and should be able to</span>
<span class="p_add">+ * get a clean warning out of it.  If we BUG_ON() here, we run</span>
<span class="p_add">+ * the risk of being before we have good console output.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaiser_init_all_pgds();</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		void *percpu_vaddr = __per_cpu_user_mapped_start +</span>
<span class="p_add">+				     per_cpu_offset(cpu);</span>
<span class="p_add">+		unsigned long percpu_sz = __per_cpu_user_mapped_end -</span>
<span class="p_add">+					  __per_cpu_user_mapped_start;</span>
<span class="p_add">+		kaiser_add_user_map_early(percpu_vaddr, percpu_sz,</span>
<span class="p_add">+					  __PAGE_KERNEL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Map the entry/exit text section, which is needed at</span>
<span class="p_add">+	 * switches from user to and from kernel.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__entry_text_start, __entry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX);</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)</span>
<span class="p_add">+	kaiser_add_user_map_ptrs_early(__irqentry_text_start,</span>
<span class="p_add">+				       __irqentry_text_end,</span>
<span class="p_add">+				       __PAGE_KERNEL_RX);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	kaiser_add_user_map_early((void *)idt_descr.address,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL_RO);</span>
<span class="p_add">+#ifdef CONFIG_TRACING</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;trace_idt_descr,</span>
<span class="p_add">+				  sizeof(trace_idt_descr),</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;trace_idt_table,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;debug_idt_descr, sizeof(debug_idt_descr),</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;debug_idt_table,</span>
<span class="p_add">+				  sizeof(gate_desc) * NR_VECTORS,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;enabled\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Add a mapping to the shadow mapping, and synchronize the mappings */</span>
<span class="p_add">+int kaiser_add_mapping(unsigned long addr, unsigned long size, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return kaiser_add_user_map((const void *)addr, size, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_remove_mapping(unsigned long start, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	extern void unmap_pud_range_nofree(pgd_t *pgd,</span>
<span class="p_add">+				unsigned long start, unsigned long end);</span>
<span class="p_add">+	unsigned long end = start + size;</span>
<span class="p_add">+	unsigned long addr, next;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	pgd = native_get_shadow_pgd(pgd_offset_k(start));</span>
<span class="p_add">+	for (addr = start; addr &lt; end; pgd++, addr = next) {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		unmap_pud_range_nofree(pgd, addr, next);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ * This returns true for user pages that need to get copied into</span>
<span class="p_add">+ * both the user and kernel copies of the page tables, and false</span>
<span class="p_add">+ * for kernel pages that should only be in the kernel copy.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool is_userspace_pgd(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((unsigned long)pgdp % PAGE_SIZE) &lt; (PAGE_SIZE / 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pgd_t kaiser_set_shadow_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do we need to also populate the shadow pgd?  Check _PAGE_USER to</span>
<span class="p_add">+	 * skip cases like kexec and EFI which make temporary low mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgd.pgd &amp; _PAGE_USER) {</span>
<span class="p_add">+		if (is_userspace_pgd(pgdp)) {</span>
<span class="p_add">+			native_get_shadow_pgd(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Even if the entry is *mapping* userspace, ensure</span>
<span class="p_add">+			 * that userspace can not use it.  This way, if we</span>
<span class="p_add">+			 * get out to userspace running on the kernel CR3,</span>
<span class="p_add">+			 * userspace will crash instead of running.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (__supported_pte_mask &amp; _PAGE_NX)</span>
<span class="p_add">+				pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (!pgd.pgd) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * pgd_clear() cannot check _PAGE_USER, and is even used to</span>
<span class="p_add">+		 * clear corrupted pgd entries: so just rely on cases like</span>
<span class="p_add">+		 * kexec and EFI never to be using pgd_clear().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!WARN_ON_ONCE((unsigned long)pgdp &amp; PAGE_SIZE) &amp;&amp;</span>
<span class="p_add">+		    is_userspace_pgd(pgdp))</span>
<span class="p_add">+			native_get_shadow_pgd(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void kaiser_setup_pcid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long user_cr3 = KAISER_SHADOW_PGD_OFFSET;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		user_cr3 |= X86_CR3_PCID_USER_NOFLUSH;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * These variables are used by the entry/exit</span>
<span class="p_add">+	 * code to change PCID and pgd and TLB flushing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	this_cpu_write(x86_cr3_pcid_user, user_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make a note that this cpu will need to flush USER tlb on return to user.</span>
<span class="p_add">+ * If cpu does not have PCID, then the NOFLUSH bit will never have been set.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void kaiser_flush_tlb_on_return_to_user(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (this_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		this_cpu_write(x86_cr3_pcid_user,</span>
<span class="p_add">+			X86_CR3_PCID_USER_FLUSH | KAISER_SHADOW_PGD_OFFSET);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(kaiser_flush_tlb_on_return_to_user);</span>
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index 4e5ac46adc9d..81ec7c02f968 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -121,11 +121,16 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),
 			(void *)KASAN_SHADOW_END);
 
<span class="p_del">-	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="p_del">-</span>
 	load_cr3(init_level4_pgt);
 	__flush_tlb_all();
<span class="p_del">-	init_task.kasan_depth = 0;</span>
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * kasan_zero_page has been used as early shadow memory, thus it may</span>
<span class="p_add">+	 * contain some garbage. Now we can clear it, since after the TLB flush</span>
<span class="p_add">+	 * no one should write to it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	init_task.kasan_depth = 0;</span>
 	pr_info(&quot;KernelAddressSanitizer initialized\n&quot;);
 }
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index b599a780a5a9..79377e2a7bcd 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -52,6 +52,7 @@</span> <span class="p_context"> static DEFINE_SPINLOCK(cpa_lock);</span>
 #define CPA_FLUSHTLB 1
 #define CPA_ARRAY 2
 #define CPA_PAGES_ARRAY 4
<span class="p_add">+#define CPA_FREE_PAGETABLES 8</span>
 
 #ifdef CONFIG_PROC_FS
 static unsigned long direct_pages_count[PG_LEVEL_NUM];
<span class="p_chunk">@@ -723,10 +724,13 @@</span> <span class="p_context"> static int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
 	return 0;
 }
 
<span class="p_del">-static bool try_to_free_pte_page(pte_t *pte)</span>
<span class="p_add">+static bool try_to_free_pte_page(struct cpa_data *cpa, pte_t *pte)</span>
 {
 	int i;
 
<span class="p_add">+	if (!(cpa-&gt;flags &amp; CPA_FREE_PAGETABLES))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; PTRS_PER_PTE; i++)
 		if (!pte_none(pte[i]))
 			return false;
<span class="p_chunk">@@ -735,10 +739,13 @@</span> <span class="p_context"> static bool try_to_free_pte_page(pte_t *pte)</span>
 	return true;
 }
 
<span class="p_del">-static bool try_to_free_pmd_page(pmd_t *pmd)</span>
<span class="p_add">+static bool try_to_free_pmd_page(struct cpa_data *cpa, pmd_t *pmd)</span>
 {
 	int i;
 
<span class="p_add">+	if (!(cpa-&gt;flags &amp; CPA_FREE_PAGETABLES))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; PTRS_PER_PMD; i++)
 		if (!pmd_none(pmd[i]))
 			return false;
<span class="p_chunk">@@ -759,7 +766,9 @@</span> <span class="p_context"> static bool try_to_free_pud_page(pud_t *pud)</span>
 	return true;
 }
 
<span class="p_del">-static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)</span>
<span class="p_add">+static bool unmap_pte_range(struct cpa_data *cpa, pmd_t *pmd,</span>
<span class="p_add">+			    unsigned long start,</span>
<span class="p_add">+			    unsigned long end)</span>
 {
 	pte_t *pte = pte_offset_kernel(pmd, start);
 
<span class="p_chunk">@@ -770,22 +779,23 @@</span> <span class="p_context"> static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)</span>
 		pte++;
 	}
 
<span class="p_del">-	if (try_to_free_pte_page((pte_t *)pmd_page_vaddr(*pmd))) {</span>
<span class="p_add">+	if (try_to_free_pte_page(cpa, (pte_t *)pmd_page_vaddr(*pmd))) {</span>
 		pmd_clear(pmd);
 		return true;
 	}
 	return false;
 }
 
<span class="p_del">-static void __unmap_pmd_range(pud_t *pud, pmd_t *pmd,</span>
<span class="p_add">+static void __unmap_pmd_range(struct cpa_data *cpa, pud_t *pud, pmd_t *pmd,</span>
 			      unsigned long start, unsigned long end)
 {
<span class="p_del">-	if (unmap_pte_range(pmd, start, end))</span>
<span class="p_del">-		if (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))</span>
<span class="p_add">+	if (unmap_pte_range(cpa, pmd, start, end))</span>
<span class="p_add">+		if (try_to_free_pmd_page(cpa, (pmd_t *)pud_page_vaddr(*pud)))</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
<span class="p_add">+static void unmap_pmd_range(struct cpa_data *cpa, pud_t *pud,</span>
<span class="p_add">+			    unsigned long start, unsigned long end)</span>
 {
 	pmd_t *pmd = pmd_offset(pud, start);
 
<span class="p_chunk">@@ -796,7 +806,7 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 		unsigned long next_page = (start + PMD_SIZE) &amp; PMD_MASK;
 		unsigned long pre_end = min_t(unsigned long, end, next_page);
 
<span class="p_del">-		__unmap_pmd_range(pud, pmd, start, pre_end);</span>
<span class="p_add">+		__unmap_pmd_range(cpa, pud, pmd, start, pre_end);</span>
 
 		start = pre_end;
 		pmd++;
<span class="p_chunk">@@ -809,7 +819,8 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 		if (pmd_large(*pmd))
 			pmd_clear(pmd);
 		else
<span class="p_del">-			__unmap_pmd_range(pud, pmd, start, start + PMD_SIZE);</span>
<span class="p_add">+			__unmap_pmd_range(cpa, pud, pmd,</span>
<span class="p_add">+					  start, start + PMD_SIZE);</span>
 
 		start += PMD_SIZE;
 		pmd++;
<span class="p_chunk">@@ -819,17 +830,19 @@</span> <span class="p_context"> static void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)</span>
 	 * 4K leftovers?
 	 */
 	if (start &lt; end)
<span class="p_del">-		return __unmap_pmd_range(pud, pmd, start, end);</span>
<span class="p_add">+		return __unmap_pmd_range(cpa, pud, pmd, start, end);</span>
 
 	/*
 	 * Try again to free the PMD page if haven&#39;t succeeded above.
 	 */
 	if (!pud_none(*pud))
<span class="p_del">-		if (try_to_free_pmd_page((pmd_t *)pud_page_vaddr(*pud)))</span>
<span class="p_add">+		if (try_to_free_pmd_page(cpa, (pmd_t *)pud_page_vaddr(*pud)))</span>
 			pud_clear(pud);
 }
 
<span class="p_del">-static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+static void __unmap_pud_range(struct cpa_data *cpa, pgd_t *pgd,</span>
<span class="p_add">+			      unsigned long start,</span>
<span class="p_add">+			      unsigned long end)</span>
 {
 	pud_t *pud = pud_offset(pgd, start);
 
<span class="p_chunk">@@ -840,7 +853,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 		unsigned long next_page = (start + PUD_SIZE) &amp; PUD_MASK;
 		unsigned long pre_end	= min_t(unsigned long, end, next_page);
 
<span class="p_del">-		unmap_pmd_range(pud, start, pre_end);</span>
<span class="p_add">+		unmap_pmd_range(cpa, pud, start, pre_end);</span>
 
 		start = pre_end;
 		pud++;
<span class="p_chunk">@@ -854,7 +867,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 		if (pud_large(*pud))
 			pud_clear(pud);
 		else
<span class="p_del">-			unmap_pmd_range(pud, start, start + PUD_SIZE);</span>
<span class="p_add">+			unmap_pmd_range(cpa, pud, start, start + PUD_SIZE);</span>
 
 		start += PUD_SIZE;
 		pud++;
<span class="p_chunk">@@ -864,7 +877,7 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 	 * 2M leftovers?
 	 */
 	if (start &lt; end)
<span class="p_del">-		unmap_pmd_range(pud, start, end);</span>
<span class="p_add">+		unmap_pmd_range(cpa, pud, start, end);</span>
 
 	/*
 	 * No need to try to free the PUD page because we&#39;ll free it in
<span class="p_chunk">@@ -872,6 +885,24 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 	 */
 }
 
<span class="p_add">+static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpa_data cpa = {</span>
<span class="p_add">+		.flags = CPA_FREE_PAGETABLES,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	__unmap_pud_range(&amp;cpa, pgd, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void unmap_pud_range_nofree(pgd_t *pgd, unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpa_data cpa = {</span>
<span class="p_add">+		.flags = 0,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	__unmap_pud_range(&amp;cpa, pgd, start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void unmap_pgd_range(pgd_t *root, unsigned long addr, unsigned long end)
 {
 	pgd_t *pgd_entry = root + pgd_index(addr);
<span class="p_header">diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c</span>
<span class="p_header">index fb0a9dd1d6e4..dbc27a2b4ad5 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c</span>
<span class="p_chunk">@@ -6,7 +6,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/mtrr.h&gt;
 
<span class="p_del">-#define PGALLOC_GFP GFP_KERNEL | __GFP_NOTRACK | __GFP_REPEAT | __GFP_ZERO</span>
<span class="p_add">+#define PGALLOC_GFP (GFP_KERNEL | __GFP_NOTRACK | __GFP_REPEAT | __GFP_ZERO)</span>
 
 #ifdef CONFIG_HIGHPTE
 #define PGALLOC_USER_GFP __GFP_HIGHMEM
<span class="p_chunk">@@ -340,14 +340,24 @@</span> <span class="p_context"> static inline void _pgd_free(pgd_t *pgd)</span>
 		kmem_cache_free(pgd_cache, pgd);
 }
 #else
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one pgd, Kaiser acquires two pgds.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER	kaiser_enabled</span>
<span class="p_add">+</span>
 static inline pgd_t *_pgd_alloc(void)
 {
<span class="p_del">-	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	/* No __GFP_REPEAT: to avoid page allocation stalls in order-1 case */</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP &amp; ~__GFP_REPEAT,</span>
<span class="p_add">+					 PGD_ALLOCATION_ORDER);</span>
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
 }
 #endif /* CONFIG_X86_PAE */
 
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 7a4cdb632508..7cad01af6dcd 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -6,13 +6,14 @@</span> <span class="p_context"></span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/debugfs.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/cache.h&gt;
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/uv/uv.h&gt;
<span class="p_del">-#include &lt;linux/debugfs.h&gt;</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 /*
  *	TLB flushing, formerly SMP-only
<span class="p_chunk">@@ -34,6 +35,36 @@</span> <span class="p_context"> struct flush_tlb_info {</span>
 	unsigned long flush_end;
 };
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3 = __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kaiser_enabled) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We reuse the same PCID for different tasks, so we must</span>
<span class="p_add">+		 * flush all the entries for the PCID out when we change tasks.</span>
<span class="p_add">+		 * Flush KERN below, flush USER when returning to userspace in</span>
<span class="p_add">+		 * kaiser&#39;s SWITCH_USER_CR3 (_SWITCH_TO_USER_CR3) macro.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * invpcid_flush_single_context(X86_CR3_PCID_ASID_USER) could</span>
<span class="p_add">+		 * do it here, but can only be used if X86_FEATURE_INVPCID is</span>
<span class="p_add">+		 * available - and many machines support pcid without invpcid.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If X86_CR3_PCID_KERN_FLUSH actually added something, then it</span>
<span class="p_add">+		 * would be needed in the write_cr3() below - if PCIDs enabled.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		BUILD_BUG_ON(X86_CR3_PCID_KERN_FLUSH);</span>
<span class="p_add">+		kaiser_flush_tlb_on_return_to_user();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * We cannot call mmdrop() because we are in interrupt context,
  * instead update mm-&gt;cpu_vm_mask.
<span class="p_chunk">@@ -45,7 +76,7 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		BUG();
 	if (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {
 		cpumask_clear_cpu(cpu, mm_cpumask(active_mm));
<span class="p_del">-		load_cr3(swapper_pg_dir);</span>
<span class="p_add">+		load_new_mm_cr3(swapper_pg_dir);</span>
 		/*
 		 * This gets called in the idle path where RCU
 		 * functions differently.  Tracing normally
<span class="p_chunk">@@ -105,7 +136,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		 * ordering guarantee we need.
 		 *
 		 */
<span class="p_del">-		load_cr3(next-&gt;pgd);</span>
<span class="p_add">+		load_new_mm_cr3(next-&gt;pgd);</span>
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 
<span class="p_chunk">@@ -152,7 +183,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			 * As above, load_cr3() is serializing and orders TLB
 			 * fills with respect to the mm_cpumask write.
 			 */
<span class="p_del">-			load_cr3(next-&gt;pgd);</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd);</span>
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 			load_mm_cr4(next);
 			load_mm_ldt(next);
<span class="p_header">diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">index ef2e8c97e183..a461b6604fd9 100644</span>
<span class="p_header">--- a/include/asm-generic/vmlinux.lds.h</span>
<span class="p_header">+++ b/include/asm-generic/vmlinux.lds.h</span>
<span class="p_chunk">@@ -725,7 +725,14 @@</span> <span class="p_context"></span>
  */
 #define PERCPU_INPUT(cacheline)						\
 	VMLINUX_SYMBOL(__per_cpu_start) = .;				\
<span class="p_add">+	VMLINUX_SYMBOL(__per_cpu_user_mapped_start) = .;		\</span>
 	*(.data..percpu..first)						\
<span class="p_add">+	. = ALIGN(cacheline);						\</span>
<span class="p_add">+	*(.data..percpu..user_mapped)					\</span>
<span class="p_add">+	*(.data..percpu..user_mapped..shared_aligned)			\</span>
<span class="p_add">+	. = ALIGN(PAGE_SIZE);						\</span>
<span class="p_add">+	*(.data..percpu..user_mapped..page_aligned)			\</span>
<span class="p_add">+	VMLINUX_SYMBOL(__per_cpu_user_mapped_end) = .;			\</span>
 	. = ALIGN(PAGE_SIZE);						\
 	*(.data..percpu..page_aligned)					\
 	. = ALIGN(cacheline);						\
<span class="p_header">diff --git a/include/linux/kaiser.h b/include/linux/kaiser.h</span>
new file mode 100644
<span class="p_header">index 000000000000..58c55b1589d0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/kaiser.h</span>
<span class="p_chunk">@@ -0,0 +1,52 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _LINUX_KAISER_H</span>
<span class="p_add">+#define _LINUX_KAISER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int kaiser_map_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Map that page of kernel stack on which we enter from user context.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return kaiser_add_mapping((unsigned long)stack +</span>
<span class="p_add">+			THREAD_SIZE - PAGE_SIZE, PAGE_SIZE, __PAGE_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_unmap_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: may be called even when kaiser_map_thread_stack() failed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kaiser_remove_mapping((unsigned long)stack +</span>
<span class="p_add">+			THREAD_SIZE - PAGE_SIZE, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These stubs are used whenever CONFIG_PAGE_TABLE_ISOLATION is off, which</span>
<span class="p_add">+ * includes architectures that support KAISER, but have it disabled.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kaiser_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int kaiser_add_mapping(unsigned long addr,</span>
<span class="p_add">+				     unsigned long size, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_remove_mapping(unsigned long start,</span>
<span class="p_add">+					 unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int kaiser_map_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void kaiser_unmap_thread_stack(void *stack)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+#endif /* _LINUX_KAISER_H */</span>
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index ff88d6189411..b93b578cfa42 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -131,8 +131,9 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_SLAB_RECLAIMABLE,
 	NR_SLAB_UNRECLAIMABLE,
 	NR_PAGETABLE,		/* used for pagetables */
<span class="p_del">-	NR_KERNEL_STACK,</span>
 	/* Second 128 byte cacheline */
<span class="p_add">+	NR_KERNEL_STACK,</span>
<span class="p_add">+	NR_KAISERTABLE,</span>
 	NR_UNSTABLE_NFS,	/* NFS unstable pages */
 	NR_BOUNCE,
 	NR_VMSCAN_WRITE,
<span class="p_header">diff --git a/include/linux/percpu-defs.h b/include/linux/percpu-defs.h</span>
<span class="p_header">index 8f16299ca068..8902f23bb770 100644</span>
<span class="p_header">--- a/include/linux/percpu-defs.h</span>
<span class="p_header">+++ b/include/linux/percpu-defs.h</span>
<span class="p_chunk">@@ -35,6 +35,12 @@</span> <span class="p_context"></span>
 
 #endif
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#define USER_MAPPED_SECTION &quot;..user_mapped&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define USER_MAPPED_SECTION &quot;&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Base implementations of per-CPU variable declarations and definitions, where
  * the section in which the variable is to be placed is provided by the
<span class="p_chunk">@@ -115,6 +121,12 @@</span> <span class="p_context"></span>
 #define DEFINE_PER_CPU(type, name)					\
 	DEFINE_PER_CPU_SECTION(type, name, &quot;&quot;)
 
<span class="p_add">+#define DECLARE_PER_CPU_USER_MAPPED(type, name)				\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_USER_MAPPED(type, name)				\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION)</span>
<span class="p_add">+</span>
 /*
  * Declaration/definition used for per-CPU variables that must come first in
  * the set of variables.
<span class="p_chunk">@@ -144,6 +156,14 @@</span> <span class="p_context"></span>
 	DEFINE_PER_CPU_SECTION(type, name, PER_CPU_SHARED_ALIGNED_SECTION) \
 	____cacheline_aligned_in_smp
 
<span class="p_add">+#define DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION PER_CPU_SHARED_ALIGNED_SECTION) \</span>
<span class="p_add">+	____cacheline_aligned_in_smp</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION PER_CPU_SHARED_ALIGNED_SECTION) \</span>
<span class="p_add">+	____cacheline_aligned_in_smp</span>
<span class="p_add">+</span>
 #define DECLARE_PER_CPU_ALIGNED(type, name)				\
 	DECLARE_PER_CPU_SECTION(type, name, PER_CPU_ALIGNED_SECTION)	\
 	____cacheline_aligned
<span class="p_chunk">@@ -162,11 +182,21 @@</span> <span class="p_context"></span>
 #define DEFINE_PER_CPU_PAGE_ALIGNED(type, name)				\
 	DEFINE_PER_CPU_SECTION(type, name, &quot;..page_aligned&quot;)		\
 	__aligned(PAGE_SIZE)
<span class="p_add">+/*</span>
<span class="p_add">+ * Declaration/definition used for per-CPU variables that must be page aligned and need to be mapped in user mode.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DECLARE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION&quot;..page_aligned&quot;) \</span>
<span class="p_add">+	__aligned(PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFINE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(type, name)		\</span>
<span class="p_add">+	DEFINE_PER_CPU_SECTION(type, name, USER_MAPPED_SECTION&quot;..page_aligned&quot;) \</span>
<span class="p_add">+	__aligned(PAGE_SIZE)</span>
 
 /*
  * Declaration/definition used for per-CPU variables that must be read mostly.
  */
<span class="p_del">-#define DECLARE_PER_CPU_READ_MOSTLY(type, name)			\</span>
<span class="p_add">+#define DECLARE_PER_CPU_READ_MOSTLY(type, name)				\</span>
 	DECLARE_PER_CPU_SECTION(type, name, &quot;..read_mostly&quot;)
 
 #define DEFINE_PER_CPU_READ_MOSTLY(type, name)				\
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 9e64d7097f1a..49926d95442f 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -81,6 +81,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/integrity.h&gt;
 #include &lt;linux/proc_ns.h&gt;
 #include &lt;linux/io.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 
 #include &lt;asm/io.h&gt;
 #include &lt;asm/bugs.h&gt;
<span class="p_chunk">@@ -492,6 +493,7 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	pgtable_init();
 	vmalloc_init();
 	ioremap_huge_init();
<span class="p_add">+	kaiser_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 68cfda1c1800..ac00f14208b7 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -58,6 +58,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/tsacct_kern.h&gt;
 #include &lt;linux/cn_proc.h&gt;
 #include &lt;linux/freezer.h&gt;
<span class="p_add">+#include &lt;linux/kaiser.h&gt;</span>
 #include &lt;linux/delayacct.h&gt;
 #include &lt;linux/taskstats_kern.h&gt;
 #include &lt;linux/random.h&gt;
<span class="p_chunk">@@ -169,6 +170,7 @@</span> <span class="p_context"> static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,</span>
 
 static inline void free_thread_info(struct thread_info *ti)
 {
<span class="p_add">+	kaiser_unmap_thread_stack(ti);</span>
 	free_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 # else
<span class="p_chunk">@@ -352,6 +354,10 @@</span> <span class="p_context"> static struct task_struct *dup_task_struct(struct task_struct *orig, int node)</span>
 		goto free_ti;
 
 	tsk-&gt;stack = ti;
<span class="p_add">+</span>
<span class="p_add">+	err = kaiser_map_thread_stack(tsk-&gt;stack);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		goto free_ti;</span>
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we&#39;re under
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index c344e3609c53..324b7e90b4c5 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -736,6 +736,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_slab_unreclaimable&quot;,
 	&quot;nr_page_table_pages&quot;,
 	&quot;nr_kernel_stack&quot;,
<span class="p_add">+	&quot;nr_overhead&quot;,</span>
 	&quot;nr_unstable&quot;,
 	&quot;nr_bounce&quot;,
 	&quot;nr_vmscan_write&quot;,
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index e45237897b43..a3ebb6ee5bd5 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -31,6 +31,16 @@</span> <span class="p_context"> config SECURITY</span>
 
 	  If you are unsure how to answer this question, answer N.
 
<span class="p_add">+config PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool &quot;Remove the kernel mapping in user mode&quot;</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; SMP</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This enforces a strict kernel and user space isolation, in order</span>
<span class="p_add">+	  to close hardware side channels on kernel address information.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If you are unsure how to answer this question, answer Y.</span>
<span class="p_add">+</span>
 config SECURITYFS
 	bool &quot;Enable the securityfs filesystem&quot;
 	help

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



