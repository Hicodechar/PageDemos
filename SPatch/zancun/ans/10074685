
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,2/2] x86: disable IRQs before changing CR4 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,2/2] x86: disable IRQs before changing CR4</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 25, 2017, 3:29 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171125032907.2241-3-namit@vmware.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10074685/mbox/"
   >mbox</a>
|
   <a href="/patch/10074685/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10074685/">/patch/10074685/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7132E60567 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 25 Nov 2017 03:44:36 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5B51E2A18A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 25 Nov 2017 03:44:36 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 503792A209; Sat, 25 Nov 2017 03:44:36 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 179312A1CC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 25 Nov 2017 03:44:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752236AbdKYDo2 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 24 Nov 2017 22:44:28 -0500
Received: from ex13-edg-ou-001.vmware.com ([208.91.0.189]:50934 &quot;EHLO
	EX13-EDG-OU-001.vmware.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751913AbdKYDoY (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 24 Nov 2017 22:44:24 -0500
Received: from sc9-mailhost3.vmware.com (10.113.161.73) by
	EX13-EDG-OU-001.vmware.com (10.113.208.155) with Microsoft SMTP
	Server id 15.0.1156.6; Fri, 24 Nov 2017 19:29:10 -0800
Received: from ubuntu.localdomain (unknown [10.2.44.15])
	by sc9-mailhost3.vmware.com (Postfix) with ESMTP id AC4C44062A;
	Fri, 24 Nov 2017 19:29:18 -0800 (PST)
From: Nadav Amit &lt;namit@vmware.com&gt;
To: &lt;linux-kernel@vger.kernel.org&gt;, &lt;linux-edac@vger.kernel.org&gt;
CC: &lt;nadav.amit@gmail.com&gt;, Nadav Amit &lt;namit@vmware.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, &lt;x86@kernel.org&gt;,
	&quot;Tony Luck&quot; &lt;tony.luck@intel.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	=?UTF-8?q?Radim=20Kr=C4=8Dm=C3=A1=C5=99?= &lt;rkrcmar@redhat.com&gt;
Subject: [PATCH v2 2/2] x86: disable IRQs before changing CR4
Date: Fri, 24 Nov 2017 19:29:07 -0800
Message-ID: &lt;20171125032907.2241-3-namit@vmware.com&gt;
X-Mailer: git-send-email 2.14.1
In-Reply-To: &lt;20171125032907.2241-1-namit@vmware.com&gt;
References: &lt;20171125032907.2241-1-namit@vmware.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;UTF-8&quot;
Content-Transfer-Encoding: 8bit
Received-SPF: None (EX13-EDG-OU-001.vmware.com: namit@vmware.com does not
	designate permitted sender hosts)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Nov. 25, 2017, 3:29 a.m.</div>
<pre class="content">
CR4 changes need to be performed while IRQs are disabled in order to
update the CR4 shadow and the actual register atomically. Actually, they
are needed regardless of CR4 shadowing, since CR4 are performed in a
read-modify-write manner.

Currently, however, this is not the case, as can be experienced by
adding warnings when CR4 updates are performed and interrupts are
enabled. It also appears that CR4 changes with enabled interrupts can be
triggered by the user (PR_SET_TSC).

If CR4 updates are done while interrupts are enabled, an interrupt can
be delivered between the CR4 read and the corresponding write of the
modified value to CR4. If the interrupt handler changes CR4, the write
would ignore the modified value.

It is not clear there are currently interrupt handlers that modify CR4
and do not restore immediately the original value, but there is an
interrupt handler that changes CR4, when global PTEs are invalidated.
Moreover, a recent patch considered doing change CR4 inside the
interrupt handler, emphasizing that the current scheme is error-prone.

Prevent the issue by: adding a debug warning if CR4 is updated with
enabled interrupts; changing CR4 manipulation function names to reflect
the fact they need disabled IRQs and fix the callers.

Note that in some cases, e.g., kvm_cpu_vmxon(), it appears that saving
and restoring the IRQs could have been spared. Yet, since these calls do
not appear to be on the hot path, save and restore the IRQs to be on the
safe side.

Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Ingo Molnar &lt;mingo@redhat.com&gt;
Cc: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: x86@kernel.org
Cc: Tony Luck &lt;tony.luck@intel.com&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: &quot;Radim Krčmář&quot; &lt;rkrcmar@redhat.com&gt;
<span class="signed-off-by">
Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
---
 arch/x86/include/asm/mmu_context.h   |  4 ++--
 arch/x86/include/asm/tlbflush.h      | 16 +++++++++++----
 arch/x86/include/asm/virtext.h       |  2 +-
 arch/x86/kernel/cpu/common.c         | 38 ++++++++++++++++++++++++++----------
 arch/x86/kernel/cpu/mcheck/mce.c     |  5 ++++-
 arch/x86/kernel/cpu/mcheck/p5.c      |  6 +++++-
 arch/x86/kernel/cpu/mcheck/winchip.c |  5 ++++-
 arch/x86/kernel/fpu/init.c           |  2 +-
 arch/x86/kernel/fpu/xstate.c         |  4 ++--
 arch/x86/kernel/process.c            | 20 ++++++++++++++-----
 arch/x86/kernel/reboot.c             |  2 +-
 arch/x86/kvm/vmx.c                   | 13 ++++++++++--
 arch/x86/mm/init.c                   |  6 +++++-
 13 files changed, 91 insertions(+), 32 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 25, 2017, 10:36 a.m.</div>
<pre class="content">
On Fri, 24 Nov 2017, Nadav Amit wrote:
<span class="quote">&gt;  /* Set in this cpu&#39;s CR4. */</span>
<span class="quote">&gt; -static inline void cr4_set_bits(unsigned long mask)</span>
<span class="quote">&gt; +static inline void cr4_set_bits_irqs_off(unsigned long mask)</span>

This change is kinda weird. I&#39;d expect that there is a corresponding
function cr4_set_bits() which takes care of disabling interrupts. But there
is not. All it does is creating a lot of pointless churn.
<span class="quote">
&gt;  static __always_inline void setup_smep(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;  static __init int setup_disable_smap(char *arg)</span>
<span class="quote">&gt;  static __always_inline void setup_pku(struct cpuinfo_x86 *c)</span>

Why are you not doing this at the call site around all calls which fiddle
with cr4, i.e. in identify_cpu() ?

identify_cpu() is called from two places:

    identify_boot_cpu() and identify_secondary_cpu()

identify_secondary_cpu is called with interrupts disabled anyway and there
is no reason why we can&#39;t enforce interrupts being disabled around
identify_cpu() completely.

But if we actually do the right thing, i.e. having cr4_set_bit() and
cr4_set_bit_irqsoff() all of this churn goes away magically.

Then the only place which needs to be changed is the context switch because
here interrupts are already disabled and we really care about performance.
<span class="quote">
&gt; @@ -293,7 +303,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((tifp ^ tifn) &amp; _TIF_NOTSC)</span>
<span class="quote">&gt; -		cr4_toggle_bits(X86_CR4_TSD);</span>
<span class="quote">&gt; +		cr4_toggle_bits_irqs_off(X86_CR4_TSD);</span>
<span class="quote">&gt;  </span>

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Nov. 25, 2017, 5:20 p.m.</div>
<pre class="content">
Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">
&gt; On Fri, 24 Nov 2017, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; /* Set in this cpu&#39;s CR4. */</span>
<span class="quote">&gt;&gt; -static inline void cr4_set_bits(unsigned long mask)</span>
<span class="quote">&gt;&gt; +static inline void cr4_set_bits_irqs_off(unsigned long mask)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This change is kinda weird. I&#39;d expect that there is a corresponding</span>
<span class="quote">&gt; function cr4_set_bits() which takes care of disabling interrupts. But there</span>
<span class="quote">&gt; is not. All it does is creating a lot of pointless churn.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; static __always_inline void setup_smep(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;&gt; static __init int setup_disable_smap(char *arg)</span>
<span class="quote">&gt;&gt; static __always_inline void setup_pku(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why are you not doing this at the call site around all calls which fiddle</span>
<span class="quote">&gt; with cr4, i.e. in identify_cpu() ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; identify_cpu() is called from two places:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    identify_boot_cpu() and identify_secondary_cpu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; identify_secondary_cpu is called with interrupts disabled anyway and there</span>
<span class="quote">&gt; is no reason why we can&#39;t enforce interrupts being disabled around</span>
<span class="quote">&gt; identify_cpu() completely.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But if we actually do the right thing, i.e. having cr4_set_bit() and</span>
<span class="quote">&gt; cr4_set_bit_irqsoff() all of this churn goes away magically.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then the only place which needs to be changed is the context switch because</span>
<span class="quote">&gt; here interrupts are already disabled and we really care about performance.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; @@ -293,7 +303,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,</span>
<span class="quote">&gt;&gt; 	}</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; 	if ((tifp ^ tifn) &amp; _TIF_NOTSC)</span>
<span class="quote">&gt;&gt; -		cr4_toggle_bits(X86_CR4_TSD);</span>
<span class="quote">&gt;&gt; +		cr4_toggle_bits_irqs_off(X86_CR4_TSD);</span>

You make a good point. I will add cr4_set_bit(). I will leave identify_cpu()
as is, since it is rather hard to maintain code that enables/disables irqs
at one point and rely on these operations at a completely different place.
As you said, it is less of an issue once cr4_set_bit() and friends are
introduced.

Thanks,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Nov. 25, 2017, 5:25 p.m.</div>
<pre class="content">
On Sat, 25 Nov 2017, Nadav Amit wrote:
<span class="quote">&gt; Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Fri, 24 Nov 2017, Nadav Amit wrote:</span>
<span class="quote">&gt; &gt;&gt; /* Set in this cpu&#39;s CR4. */</span>
<span class="quote">&gt; &gt;&gt; -static inline void cr4_set_bits(unsigned long mask)</span>
<span class="quote">&gt; &gt;&gt; +static inline void cr4_set_bits_irqs_off(unsigned long mask)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This change is kinda weird. I&#39;d expect that there is a corresponding</span>
<span class="quote">&gt; &gt; function cr4_set_bits() which takes care of disabling interrupts. But there</span>
<span class="quote">&gt; &gt; is not. All it does is creating a lot of pointless churn.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; static __always_inline void setup_smep(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt; &gt;&gt; static __init int setup_disable_smap(char *arg)</span>
<span class="quote">&gt; &gt;&gt; static __always_inline void setup_pku(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Why are you not doing this at the call site around all calls which fiddle</span>
<span class="quote">&gt; &gt; with cr4, i.e. in identify_cpu() ?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; identify_cpu() is called from two places:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;    identify_boot_cpu() and identify_secondary_cpu()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; identify_secondary_cpu is called with interrupts disabled anyway and there</span>
<span class="quote">&gt; &gt; is no reason why we can&#39;t enforce interrupts being disabled around</span>
<span class="quote">&gt; &gt; identify_cpu() completely.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But if we actually do the right thing, i.e. having cr4_set_bit() and</span>
<span class="quote">&gt; &gt; cr4_set_bit_irqsoff() all of this churn goes away magically.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Then the only place which needs to be changed is the context switch because</span>
<span class="quote">&gt; &gt; here interrupts are already disabled and we really care about performance.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; @@ -293,7 +303,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,</span>
<span class="quote">&gt; &gt;&gt; 	}</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; 	if ((tifp ^ tifn) &amp; _TIF_NOTSC)</span>
<span class="quote">&gt; &gt;&gt; -		cr4_toggle_bits(X86_CR4_TSD);</span>
<span class="quote">&gt; &gt;&gt; +		cr4_toggle_bits_irqs_off(X86_CR4_TSD);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You make a good point. I will add cr4_set_bit(). I will leave identify_cpu()</span>
<span class="quote">&gt; as is, since it is rather hard to maintain code that enables/disables irqs</span>
<span class="quote">&gt; at one point and rely on these operations at a completely different place.</span>
<span class="quote">&gt; As you said, it is less of an issue once cr4_set_bit() and friends are</span>
<span class="quote">&gt; introduced.</span>

I fixed that up already as I wanted to have it done, see the tip-bot mail
in your inbox.

Thanks,

	tglx
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Nov. 25, 2017, 5:31 p.m.</div>
<pre class="content">
Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">
&gt; On Sat, 25 Nov 2017, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On Fri, 24 Nov 2017, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; /* Set in this cpu&#39;s CR4. */</span>
<span class="quote">&gt;&gt;&gt;&gt; -static inline void cr4_set_bits(unsigned long mask)</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void cr4_set_bits_irqs_off(unsigned long mask)</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; This change is kinda weird. I&#39;d expect that there is a corresponding</span>
<span class="quote">&gt;&gt;&gt; function cr4_set_bits() which takes care of disabling interrupts. But there</span>
<span class="quote">&gt;&gt;&gt; is not. All it does is creating a lot of pointless churn.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; static __always_inline void setup_smep(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;&gt;&gt;&gt; static __init int setup_disable_smap(char *arg)</span>
<span class="quote">&gt;&gt;&gt;&gt; static __always_inline void setup_pku(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Why are you not doing this at the call site around all calls which fiddle</span>
<span class="quote">&gt;&gt;&gt; with cr4, i.e. in identify_cpu() ?</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; identify_cpu() is called from two places:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;   identify_boot_cpu() and identify_secondary_cpu()</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; identify_secondary_cpu is called with interrupts disabled anyway and there</span>
<span class="quote">&gt;&gt;&gt; is no reason why we can&#39;t enforce interrupts being disabled around</span>
<span class="quote">&gt;&gt;&gt; identify_cpu() completely.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; But if we actually do the right thing, i.e. having cr4_set_bit() and</span>
<span class="quote">&gt;&gt;&gt; cr4_set_bit_irqsoff() all of this churn goes away magically.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Then the only place which needs to be changed is the context switch because</span>
<span class="quote">&gt;&gt;&gt; here interrupts are already disabled and we really care about performance.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -293,7 +303,7 @@ void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,</span>
<span class="quote">&gt;&gt;&gt;&gt; 	}</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; 	if ((tifp ^ tifn) &amp; _TIF_NOTSC)</span>
<span class="quote">&gt;&gt;&gt;&gt; -		cr4_toggle_bits(X86_CR4_TSD);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		cr4_toggle_bits_irqs_off(X86_CR4_TSD);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; You make a good point. I will add cr4_set_bit(). I will leave identify_cpu()</span>
<span class="quote">&gt;&gt; as is, since it is rather hard to maintain code that enables/disables irqs</span>
<span class="quote">&gt;&gt; at one point and rely on these operations at a completely different place.</span>
<span class="quote">&gt;&gt; As you said, it is less of an issue once cr4_set_bit() and friends are</span>
<span class="quote">&gt;&gt; introduced.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I fixed that up already as I wanted to have it done, see the tip-bot mail</span>
<span class="quote">&gt; in your inbox.</span>

Thanks, your changes made it much better. At some point it might be better
to make the MTRR code to use these interfaces too instead of meddling with
CR4 directly. Anyhow, that is a different story.

Regards,
Nadav
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 6699fc441644..637cbda77eda 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -30,9 +30,9 @@</span> <span class="p_context"> static inline void load_mm_cr4(struct mm_struct *mm)</span>
 {
 	if (static_key_false(&amp;rdpmc_always_available) ||
 	    atomic_read(&amp;mm-&gt;context.perf_rdpmc_allowed))
<span class="p_del">-		cr4_set_bits(X86_CR4_PCE);</span>
<span class="p_add">+		cr4_set_bits_irqs_off(X86_CR4_PCE);</span>
 	else
<span class="p_del">-		cr4_clear_bits(X86_CR4_PCE);</span>
<span class="p_add">+		cr4_clear_bits_irqs_off(X86_CR4_PCE);</span>
 }
 #else
 static inline void load_mm_cr4(struct mm_struct *mm) {}
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index e736f7f0ba92..6b94dcf8c500 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -175,12 +175,15 @@</span> <span class="p_context"> static inline void cr4_init_shadow(void)</span>
 
 static inline void __cr4_set(unsigned long cr4)
 {
<span class="p_add">+#ifdef CONFIG_LOCKDEP</span>
<span class="p_add">+	WARN_ON(!irqs_disabled());</span>
<span class="p_add">+#endif</span>
 	this_cpu_write(cpu_tlbstate.cr4, cr4);
 	__write_cr4(cr4);
 }
 
 /* Set in this cpu&#39;s CR4. */
<span class="p_del">-static inline void cr4_set_bits(unsigned long mask)</span>
<span class="p_add">+static inline void cr4_set_bits_irqs_off(unsigned long mask)</span>
 {
 	unsigned long cr4;
 
<span class="p_chunk">@@ -190,7 +193,7 @@</span> <span class="p_context"> static inline void cr4_set_bits(unsigned long mask)</span>
 }
 
 /* Clear in this cpu&#39;s CR4. */
<span class="p_del">-static inline void cr4_clear_bits(unsigned long mask)</span>
<span class="p_add">+static inline void cr4_clear_bits_irqs_off(unsigned long mask)</span>
 {
 	unsigned long cr4;
 
<span class="p_chunk">@@ -199,7 +202,7 @@</span> <span class="p_context"> static inline void cr4_clear_bits(unsigned long mask)</span>
 		__cr4_set(cr4 &amp; ~mask);
 }
 
<span class="p_del">-static inline void cr4_toggle_bits(unsigned long mask)</span>
<span class="p_add">+static inline void cr4_toggle_bits_irqs_off(unsigned long mask)</span>
 {
 	unsigned long cr4;
 
<span class="p_chunk">@@ -224,10 +227,15 @@</span> <span class="p_context"> extern u32 *trampoline_cr4_features;</span>
 
 static inline void cr4_set_bits_and_update_boot(unsigned long mask)
 {
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
 	mmu_cr4_features |= mask;
 	if (trampoline_cr4_features)
 		*trampoline_cr4_features = mmu_cr4_features;
<span class="p_del">-	cr4_set_bits(mask);</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(mask);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
 }
 
 extern void initialize_tlbstate_and_flush(void);
<span class="p_header">diff --git a/arch/x86/include/asm/virtext.h b/arch/x86/include/asm/virtext.h</span>
<span class="p_header">index 0116b2ee9e64..b403ca417b7d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/virtext.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/virtext.h</span>
<span class="p_chunk">@@ -41,7 +41,7 @@</span> <span class="p_context"> static inline int cpu_has_vmx(void)</span>
 static inline void cpu_vmxoff(void)
 {
 	asm volatile (ASM_VMX_VMXOFF : : : &quot;cc&quot;);
<span class="p_del">-	cr4_clear_bits(X86_CR4_VMXE);</span>
<span class="p_add">+	cr4_clear_bits_irqs_off(X86_CR4_VMXE);</span>
 }
 
 static inline int cpu_vmx_enabled(void)
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index c9176bae7fd8..f31890787d01 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -302,8 +302,14 @@</span> <span class="p_context"> __setup(&quot;nosmep&quot;, setup_disable_smep);</span>
 
 static __always_inline void setup_smep(struct cpuinfo_x86 *c)
 {
<span class="p_del">-	if (cpu_has(c, X86_FEATURE_SMEP))</span>
<span class="p_del">-		cr4_set_bits(X86_CR4_SMEP);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_has(c, X86_FEATURE_SMEP))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_SMEP);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
 }
 
 static __init int setup_disable_smap(char *arg)
<span class="p_chunk">@@ -320,13 +326,16 @@</span> <span class="p_context"> static __always_inline void setup_smap(struct cpuinfo_x86 *c)</span>
 	/* This should have been cleared long ago */
 	BUG_ON(eflags &amp; X86_EFLAGS_AC);
 
<span class="p_del">-	if (cpu_has(c, X86_FEATURE_SMAP)) {</span>
<span class="p_add">+	if (!cpu_has(c, X86_FEATURE_SMAP))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(eflags);</span>
 #ifdef CONFIG_X86_SMAP
<span class="p_del">-		cr4_set_bits(X86_CR4_SMAP);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_SMAP);</span>
 #else
<span class="p_del">-		cr4_clear_bits(X86_CR4_SMAP);</span>
<span class="p_add">+	cr4_clear_bits_irqs_off(X86_CR4_SMAP);</span>
 #endif
<span class="p_del">-	}</span>
<span class="p_add">+	local_irq_restore(eflags);</span>
 }
 
 /*
<span class="p_chunk">@@ -336,6 +345,8 @@</span> <span class="p_context"> static bool pku_disabled;</span>
 
 static __always_inline void setup_pku(struct cpuinfo_x86 *c)
 {
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
 	/* check the boot processor, plus compile options for PKU: */
 	if (!cpu_feature_enabled(X86_FEATURE_PKU))
 		return;
<span class="p_chunk">@@ -345,7 +356,10 @@</span> <span class="p_context"> static __always_inline void setup_pku(struct cpuinfo_x86 *c)</span>
 	if (pku_disabled)
 		return;
 
<span class="p_del">-	cr4_set_bits(X86_CR4_PKE);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_PKE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
 	/*
 	 * Seting X86_CR4_PKE will cause the X86_FEATURE_OSPKE
 	 * cpuid bit to be set.  We need to ensure that we
<span class="p_chunk">@@ -1147,7 +1161,10 @@</span> <span class="p_context"> static void identify_cpu(struct cpuinfo_x86 *c)</span>
 	/* Disable the PN if appropriate */
 	squash_the_stupid_serial_number(c);
 
<span class="p_del">-	/* Set up SMEP/SMAP */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Set up SMEP/SMAP. Disable interrupts to prevent triggering a warning</span>
<span class="p_add">+	 * as CR4 changes must be done with disabled interrupts.</span>
<span class="p_add">+	 */</span>
 	setup_smep(c);
 	setup_smap(c);
 
<span class="p_chunk">@@ -1520,7 +1537,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 
 	pr_debug(&quot;Initializing CPU#%d\n&quot;, cpu);
 
<span class="p_del">-	cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
<span class="p_add">+	cr4_clear_bits_irqs_off(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
<span class="p_chunk">@@ -1613,7 +1630,8 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	if (cpu_feature_enabled(X86_FEATURE_VME) ||
 	    boot_cpu_has(X86_FEATURE_TSC) ||
 	    boot_cpu_has(X86_FEATURE_DE))
<span class="p_del">-		cr4_clear_bits(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);</span>
<span class="p_add">+		cr4_clear_bits_irqs_off(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|</span>
<span class="p_add">+					X86_CR4_DE);</span>
 
 	load_current_idt();
 	switch_to_new_gdt(cpu);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/mce.c b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">index 3b413065c613..1aac196eb145 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/mce.c</span>
<span class="p_chunk">@@ -1508,6 +1508,7 @@</span> <span class="p_context"> static void __mcheck_cpu_init_generic(void)</span>
 {
 	enum mcp_flags m_fl = 0;
 	mce_banks_t all_banks;
<span class="p_add">+	unsigned long flags;</span>
 	u64 cap;
 
 	if (!mca_cfg.bootlog)
<span class="p_chunk">@@ -1519,7 +1520,9 @@</span> <span class="p_context"> static void __mcheck_cpu_init_generic(void)</span>
 	bitmap_fill(all_banks, MAX_NR_BANKS);
 	machine_check_poll(MCP_UC | m_fl, &amp;all_banks);
 
<span class="p_del">-	cr4_set_bits(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
 
 	rdmsrl(MSR_IA32_MCG_CAP, cap);
 	if (cap &amp; MCG_CTL_P)
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/p5.c b/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_header">index 5cddf831720f..e4bb9573cfe5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/p5.c</span>
<span class="p_chunk">@@ -43,6 +43,7 @@</span> <span class="p_context"> static void pentium_machine_check(struct pt_regs *regs, long error_code)</span>
 /* Set up machine check reporting for processors with Intel style MCE: */
 void intel_p5_mcheck_init(struct cpuinfo_x86 *c)
 {
<span class="p_add">+	unsigned long flags;</span>
 	u32 l, h;
 
 	/* Default P5 to off as its often misconnected: */
<span class="p_chunk">@@ -63,7 +64,10 @@</span> <span class="p_context"> void intel_p5_mcheck_init(struct cpuinfo_x86 *c)</span>
 	pr_info(&quot;Intel old style machine check architecture supported.\n&quot;);
 
 	/* Enable MCE: */
<span class="p_del">-	cr4_set_bits(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
 	pr_info(&quot;Intel old style machine check reporting enabled on CPU#%d.\n&quot;,
 		smp_processor_id());
 }
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mcheck/winchip.c b/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_header">index 3b45b270a865..72213d75c865 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mcheck/winchip.c</span>
<span class="p_chunk">@@ -27,6 +27,7 @@</span> <span class="p_context"> static void winchip_machine_check(struct pt_regs *regs, long error_code)</span>
 /* Set up machine check reporting on the Winchip C6 series */
 void winchip_mcheck_init(struct cpuinfo_x86 *c)
 {
<span class="p_add">+	unsigned long flags;</span>
 	u32 lo, hi;
 
 	machine_check_vector = winchip_machine_check;
<span class="p_chunk">@@ -38,7 +39,9 @@</span> <span class="p_context"> void winchip_mcheck_init(struct cpuinfo_x86 *c)</span>
 	lo &amp;= ~(1&lt;&lt;4);	/* Enable MCE */
 	wrmsr(MSR_IDT_FCR1, lo, hi);
 
<span class="p_del">-	cr4_set_bits(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_MCE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
 
 	pr_info(&quot;Winchip machine check reporting enabled on CPU#0.\n&quot;);
 }
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 7affb7e3d9a5..db57b217e123 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -23,7 +23,7 @@</span> <span class="p_context"> static void fpu__init_cpu_generic(void)</span>
 	if (boot_cpu_has(X86_FEATURE_XMM))
 		cr4_mask |= X86_CR4_OSXMMEXCPT;
 	if (cr4_mask)
<span class="p_del">-		cr4_set_bits(cr4_mask);</span>
<span class="p_add">+		cr4_set_bits_irqs_off(cr4_mask);</span>
 
 	cr0 = read_cr0();
 	cr0 &amp;= ~(X86_CR0_TS|X86_CR0_EM); /* clear TS and EM */
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index f1d5476c9022..9d3c7a1a4ce5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -237,7 +237,7 @@</span> <span class="p_context"> void fpu__init_cpu_xstate(void)</span>
 
 	xfeatures_mask &amp;= ~XFEATURE_MASK_SUPERVISOR;
 
<span class="p_del">-	cr4_set_bits(X86_CR4_OSXSAVE);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_OSXSAVE);</span>
 	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
 }
 
<span class="p_chunk">@@ -713,7 +713,7 @@</span> <span class="p_context"> static int init_xstate_size(void)</span>
 static void fpu__init_disable_system_xstate(void)
 {
 	xfeatures_mask = 0;
<span class="p_del">-	cr4_clear_bits(X86_CR4_OSXSAVE);</span>
<span class="p_add">+	cr4_clear_bits_irqs_off(X86_CR4_OSXSAVE);</span>
 	fpu__xstate_clear_all_cpu_caps();
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index c67685337c5a..412265fe14df 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -128,25 +128,35 @@</span> <span class="p_context"> void flush_thread(void)</span>
 
 void disable_TSC(void)
 {
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
 	preempt_disable();
<span class="p_del">-	if (!test_and_set_thread_flag(TIF_NOTSC))</span>
<span class="p_add">+	if (!test_and_set_thread_flag(TIF_NOTSC)) {</span>
 		/*
 		 * Must flip the CPU state synchronously with
 		 * TIF_NOTSC in the current running context.
 		 */
<span class="p_del">-		cr4_set_bits(X86_CR4_TSD);</span>
<span class="p_add">+		local_irq_save(flags);</span>
<span class="p_add">+		cr4_set_bits_irqs_off(X86_CR4_TSD);</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+	}</span>
 	preempt_enable();
 }
 
 static void enable_TSC(void)
 {
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
 	preempt_disable();
<span class="p_del">-	if (test_and_clear_thread_flag(TIF_NOTSC))</span>
<span class="p_add">+	if (test_and_clear_thread_flag(TIF_NOTSC)) {</span>
 		/*
 		 * Must flip the CPU state synchronously with
 		 * TIF_NOTSC in the current running context.
 		 */
<span class="p_del">-		cr4_clear_bits(X86_CR4_TSD);</span>
<span class="p_add">+		local_irq_save(flags);</span>
<span class="p_add">+		cr4_clear_bits_irqs_off(X86_CR4_TSD);</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+	}</span>
 	preempt_enable();
 }
 
<span class="p_chunk">@@ -293,7 +303,7 @@</span> <span class="p_context"> void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,</span>
 	}
 
 	if ((tifp ^ tifn) &amp; _TIF_NOTSC)
<span class="p_del">-		cr4_toggle_bits(X86_CR4_TSD);</span>
<span class="p_add">+		cr4_toggle_bits_irqs_off(X86_CR4_TSD);</span>
 
 	if ((tifp ^ tifn) &amp; _TIF_NOCPUID)
 		set_cpuid_faulting(!!(tifn &amp; _TIF_NOCPUID));
<span class="p_header">diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c</span>
<span class="p_header">index 2126b9d27c34..86ad70c02607 100644</span>
<span class="p_header">--- a/arch/x86/kernel/reboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/reboot.c</span>
<span class="p_chunk">@@ -109,7 +109,7 @@</span> <span class="p_context"> void __noreturn machine_real_restart(unsigned int type)</span>
 
 	/* Exiting long mode will fail if CR4.PCIDE is set. */
 	if (static_cpu_has(X86_FEATURE_PCID))
<span class="p_del">-		cr4_clear_bits(X86_CR4_PCIDE);</span>
<span class="p_add">+		cr4_clear_bits_irqs_off(X86_CR4_PCIDE);</span>
 #endif
 
 	/* Jump to the identity-mapped low memory code */
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index a6f4f095f8f4..a0b387dfbf5c 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -3500,7 +3500,12 @@</span> <span class="p_context"> static __init int vmx_disabled_by_bios(void)</span>
 
 static void kvm_cpu_vmxon(u64 addr)
 {
<span class="p_del">-	cr4_set_bits(X86_CR4_VMXE);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_set_bits_irqs_off(X86_CR4_VMXE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
 	intel_pt_handle_vmx(1);
 
 	asm volatile (ASM_VMX_VMXON_RAX
<span class="p_chunk">@@ -3565,10 +3570,14 @@</span> <span class="p_context"> static void vmclear_local_loaded_vmcss(void)</span>
  */
 static void kvm_cpu_vmxoff(void)
 {
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
 	asm volatile (__ex(ASM_VMX_VMXOFF) : : : &quot;cc&quot;);
 
 	intel_pt_handle_vmx(0);
<span class="p_del">-	cr4_clear_bits(X86_CR4_VMXE);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cr4_clear_bits_irqs_off(X86_CR4_VMXE);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
 }
 
 static void hardware_disable(void)
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index af5c1ed21d43..ddd248acab8e 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -199,6 +199,8 @@</span> <span class="p_context"> static void setup_pcid(void)</span>
 #ifdef CONFIG_X86_64
 	if (boot_cpu_has(X86_FEATURE_PCID)) {
 		if (boot_cpu_has(X86_FEATURE_PGE)) {
<span class="p_add">+			unsigned long flags;</span>
<span class="p_add">+</span>
 			/*
 			 * This can&#39;t be cr4_set_bits_and_update_boot() --
 			 * the trampoline code can&#39;t handle CR4.PCIDE and
<span class="p_chunk">@@ -210,7 +212,9 @@</span> <span class="p_context"> static void setup_pcid(void)</span>
 			 * Instead, we brute-force it and set CR4.PCIDE
 			 * manually in start_secondary().
 			 */
<span class="p_del">-			cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_add">+			local_irq_save(flags);</span>
<span class="p_add">+			cr4_set_bits_irqs_off(X86_CR4_PCIDE);</span>
<span class="p_add">+			local_irq_restore(flags);</span>
 		} else {
 			/*
 			 * flush_tlb_all(), as currently implemented, won&#39;t

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



