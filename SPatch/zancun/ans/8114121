
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,3/3] x86/mm: If INVPCID is available, use it to flush global mappings - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,3/3] x86/mm: If INVPCID is available, use it to flush global mappings</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 25, 2016, 6:37 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;e3e4f31df42ea5d5e190a6d1e300e01d55e09d79.1453746505.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8114121/mbox/"
   >mbox</a>
|
   <a href="/patch/8114121/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8114121/">/patch/8114121/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id A7298BEEE5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jan 2016 18:38:59 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 2516320274
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jan 2016 18:38:59 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 793E62028D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jan 2016 18:38:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934155AbcAYSiz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 25 Jan 2016 13:38:55 -0500
Received: from mail.kernel.org ([198.145.29.136]:33280 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S964921AbcAYShv (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 25 Jan 2016 13:37:51 -0500
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 2787920364;
	Mon, 25 Jan 2016 18:37:50 +0000 (UTC)
Received: from localhost (50-76-60-73-ip-static.hfc.comcastbusiness.net
	[50.76.60.73])
	(using TLSv1.2 with cipher AES128-GCM-SHA256 (128/128 bits))
	(No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 6641920376;
	Mon, 25 Jan 2016 18:37:49 +0000 (UTC)
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org, linux-kernel@vger.kernel.org
Cc: Borislav Petkov &lt;bp@alien8.de&gt;, Brian Gerst &lt;brgerst@gmail.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH v2 3/3] x86/mm: If INVPCID is available,
	use it to flush global mappings
Date: Mon, 25 Jan 2016 10:37:44 -0800
Message-Id: &lt;e3e4f31df42ea5d5e190a6d1e300e01d55e09d79.1453746505.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.5.0
In-Reply-To: &lt;cover.1453746505.git.luto@kernel.org&gt;
References: &lt;cover.1453746505.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1453746505.git.luto@kernel.org&gt;
References: &lt;cover.1453746505.git.luto@kernel.org&gt;
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Jan. 25, 2016, 6:37 p.m.</div>
<pre class="content">
On my Skylake laptop, INVPCID function 2 (flush absolutely
everything) takes about 376ns, whereas saving flags, twiddling
CR4.PGE to flush global mappings, and restoring flags takes about
539ns.
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/tlbflush.h | 9 +++++++++
 1 file changed, 9 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - Jan. 29, 2016, 2:26 p.m.</div>
<pre class="content">
On Mon, Jan 25, 2016 at 10:37:44AM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; On my Skylake laptop, INVPCID function 2 (flush absolutely</span>
<span class="quote">&gt; everything) takes about 376ns, whereas saving flags, twiddling</span>
<span class="quote">&gt; CR4.PGE to flush global mappings, and restoring flags takes about</span>
<span class="quote">&gt; 539ns.</span>

FWIW, I ran your microbenchmark on the IVB laptop I have here 3 times
and some of the numbers from each run are pretty unstable. Not that it
means a whole lot - the thing doesn&#39;t have INVPCID support.

I&#39;m just questioning the microbenchmark and whether we should be rather
doing those measurements with a real benchmark, whatever that means. My
limited experience says that measuring TLB performance is hard.

 ./context_switch_latency 0 thread same
 use_xstate = 0
 Using threads
1: 100000 iters at 2676.2 ns/switch
2: 100000 iters at 2700.2 ns/switch
3: 100000 iters at 2656.1 ns/switch

 ./context_switch_latency 0 thread different
 use_xstate = 0
 Using threads
1: 100000 iters at 5174.8 ns/switch
2: 100000 iters at 5140.5 ns/switch
3: 100000 iters at 5292.9 ns/switch

 ./context_switch_latency 0 process same
 use_xstate = 0
 Using a subprocess
1: 100000 iters at 2361.2 ns/switch
2: 100000 iters at 2332.2 ns/switch
3: 100000 iters at 3436.9 ns/switch

 ./context_switch_latency 0 process different
 use_xstate = 0
 Using a subprocess
1: 100000 iters at 4713.6 ns/switch
2: 100000 iters at 4957.5 ns/switch
3: 100000 iters at 5012.2 ns/switch

 ./context_switch_latency 1 thread same
 use_xstate = 1
 Using threads
1: 100000 iters at 2505.6 ns/switch
2: 100000 iters at 2483.1 ns/switch
3: 100000 iters at 2479.7 ns/switch

 ./context_switch_latency 1 thread different
 use_xstate = 1
 Using threads
1: 100000 iters at 5245.9 ns/switch
2: 100000 iters at 5241.1 ns/switch
3: 100000 iters at 5220.3 ns/switch

 ./context_switch_latency 1 process same
 use_xstate = 1
 Using a subprocess
1: 100000 iters at 2329.8 ns/switch
2: 100000 iters at 2350.2 ns/switch
3: 100000 iters at 2500.9 ns/switch

 ./context_switch_latency 1 process different
 use_xstate = 1
 Using a subprocess
1: 100000 iters at 4970.7 ns/switch
2: 100000 iters at 5034.0 ns/switch
3: 100000 iters at 4991.6 ns/switch
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Jan. 29, 2016, 5:35 p.m.</div>
<pre class="content">
On Fri, Jan 29, 2016 at 6:26 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:
<span class="quote">&gt; On Mon, Jan 25, 2016 at 10:37:44AM -0800, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On my Skylake laptop, INVPCID function 2 (flush absolutely</span>
<span class="quote">&gt;&gt; everything) takes about 376ns, whereas saving flags, twiddling</span>
<span class="quote">&gt;&gt; CR4.PGE to flush global mappings, and restoring flags takes about</span>
<span class="quote">&gt;&gt; 539ns.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; FWIW, I ran your microbenchmark on the IVB laptop I have here 3 times</span>
<span class="quote">&gt; and some of the numbers from each run are pretty unstable. Not that it</span>
<span class="quote">&gt; means a whole lot - the thing doesn&#39;t have INVPCID support.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m just questioning the microbenchmark and whether we should be rather</span>
<span class="quote">&gt; doing those measurements with a real benchmark, whatever that means. My</span>
<span class="quote">&gt; limited experience says that measuring TLB performance is hard.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 0 thread same</span>
<span class="quote">&gt;  use_xstate = 0</span>
<span class="quote">&gt;  Using threads</span>
<span class="quote">&gt; 1: 100000 iters at 2676.2 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 2700.2 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 2656.1 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 0 thread different</span>
<span class="quote">&gt;  use_xstate = 0</span>
<span class="quote">&gt;  Using threads</span>
<span class="quote">&gt; 1: 100000 iters at 5174.8 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 5140.5 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 5292.9 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 0 process same</span>
<span class="quote">&gt;  use_xstate = 0</span>
<span class="quote">&gt;  Using a subprocess</span>
<span class="quote">&gt; 1: 100000 iters at 2361.2 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 2332.2 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 3436.9 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 0 process different</span>
<span class="quote">&gt;  use_xstate = 0</span>
<span class="quote">&gt;  Using a subprocess</span>
<span class="quote">&gt; 1: 100000 iters at 4713.6 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 4957.5 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 5012.2 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 1 thread same</span>
<span class="quote">&gt;  use_xstate = 1</span>
<span class="quote">&gt;  Using threads</span>
<span class="quote">&gt; 1: 100000 iters at 2505.6 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 2483.1 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 2479.7 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 1 thread different</span>
<span class="quote">&gt;  use_xstate = 1</span>
<span class="quote">&gt;  Using threads</span>
<span class="quote">&gt; 1: 100000 iters at 5245.9 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 5241.1 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 5220.3 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 1 process same</span>
<span class="quote">&gt;  use_xstate = 1</span>
<span class="quote">&gt;  Using a subprocess</span>
<span class="quote">&gt; 1: 100000 iters at 2329.8 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 2350.2 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 2500.9 ns/switch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  ./context_switch_latency 1 process different</span>
<span class="quote">&gt;  use_xstate = 1</span>
<span class="quote">&gt;  Using a subprocess</span>
<span class="quote">&gt; 1: 100000 iters at 4970.7 ns/switch</span>
<span class="quote">&gt; 2: 100000 iters at 5034.0 ns/switch</span>
<span class="quote">&gt; 3: 100000 iters at 4991.6 ns/switch</span>
<span class="quote">&gt;</span>

I&#39;ll fiddle with that benchmark a little bit.  Maybe I can make it
suck less.  If anyone knows a good non-micro benchmark for this, let
me know.  I refuse to use dbus as my benchmark :)

FWIW, I benchmarked cr4 vs invpcid by adding a prctl and calling it in
a loop.  If Ingo&#39;s fpu benchmark thing ever lands, I&#39;ll gladly send a
patch to add TLB flushes to it.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - Jan. 29, 2016, 6:27 p.m.</div>
<pre class="content">
On Fri, Jan 29, 2016 at 09:35:22AM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; I&#39;ll fiddle with that benchmark a little bit.  Maybe I can make it</span>
<span class="quote">&gt; suck less.  If anyone knows a good non-micro benchmark for this, let</span>
<span class="quote">&gt; me know.</span>

Yeah, I don&#39;t know of a good one. The TLB and all those intermediary
walker caches modern x86 CPUs have are really good. So it is hard to
measure any improvements there. I guess in this particular case, if one
can&#39;t measure slowdowns and the code is simple enough, then we&#39;re good
enough. In theory, we are carefully killing less TLB entries and the
related cached page walker data so that should be a good thing...
<span class="quote">
&gt; I refuse to use dbus as my benchmark :)</span>

Ha!
<span class="quote">
&gt; FWIW, I benchmarked cr4 vs invpcid by adding a prctl and calling it in</span>
<span class="quote">&gt; a loop.</span>

Apparently INVPCID is faster than the two CR4 writes.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 20fc38d8478a..4eba5164430d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -145,6 +145,15 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 {
 	unsigned long flags;
 
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Using INVPCID is considerably faster than a pair of writes</span>
<span class="p_add">+		 * to CR4 sandwiched inside an IRQ flag save/restore.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		invpcid_flush_everything();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Read-modify-write to CR4 - protect it from preemption and
 	 * from interrupts. (Use the raw variant because this code can

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



