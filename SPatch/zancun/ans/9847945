
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:x86/mm] x86, swiotlb: Add memory encryption support - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:x86/mm] x86, swiotlb: Add memory encryption support</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 18, 2017, 10:56 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-c7753208a94c73d5beb1e4bd843081d6dc7d4678@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9847945/mbox/"
   >mbox</a>
|
   <a href="/patch/9847945/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9847945/">/patch/9847945/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8AA5560392 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 11:04:10 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8B0D228550
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 11:04:10 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7CE7B2854A; Tue, 18 Jul 2017 11:04:10 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2C5752854A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 18 Jul 2017 11:04:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752396AbdGRLEF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 18 Jul 2017 07:04:05 -0400
Received: from terminus.zytor.com ([65.50.211.136]:35211 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751921AbdGRLEA (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 18 Jul 2017 07:04:00 -0400
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id v6IAu013025906;
	Tue, 18 Jul 2017 03:56:00 -0700
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id v6IAu0xA025902;
	Tue, 18 Jul 2017 03:56:00 -0700
Date: Tue, 18 Jul 2017 03:56:00 -0700
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: tip-bot for Tom Lendacky &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-c7753208a94c73d5beb1e4bd843081d6dc7d4678@git.kernel.org&gt;
Cc: luto@kernel.org, mst@redhat.com, dvyukov@google.com,
	rkrcmar@redhat.com, linux-kernel@vger.kernel.org, mingo@kernel.org,
	bp@alien8.de, glider@google.com, hpa@zytor.com,
	pbonzini@redhat.com, konrad.wilk@oracle.com, peterz@infradead.org,
	toshi.kani@hpe.com, arnd@arndb.de, corbet@lwn.net,
	thomas.lendacky@amd.com, riel@redhat.com, lwoodman@redhat.com,
	aryabinin@virtuozzo.com, tglx@linutronix.de,
	matt@codeblueprint.co.uk, torvalds@linux-foundation.org,
	brijesh.singh@amd.com, dyoung@redhat.com
Reply-To: brijesh.singh@amd.com, dyoung@redhat.com,
	torvalds@linux-foundation.org, matt@codeblueprint.co.uk,
	tglx@linutronix.de, aryabinin@virtuozzo.com, lwoodman@redhat.com,
	thomas.lendacky@amd.com, riel@redhat.com, corbet@lwn.net,
	arnd@arndb.de, toshi.kani@hpe.com, peterz@infradead.org,
	konrad.wilk@oracle.com, pbonzini@redhat.com, hpa@zytor.com,
	bp@alien8.de, glider@google.com, mingo@kernel.org,
	linux-kernel@vger.kernel.org, dvyukov@google.com,
	rkrcmar@redhat.com, mst@redhat.com, luto@kernel.org
In-Reply-To: &lt;aa2d29b78ae7d508db8881e46a3215231b9327a7.1500319216.git.thomas.lendacky@amd.com&gt;
References: &lt;aa2d29b78ae7d508db8881e46a3215231b9327a7.1500319216.git.thomas.lendacky@amd.com&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:x86/mm] x86, swiotlb: Add memory encryption support
Git-Commit-ID: c7753208a94c73d5beb1e4bd843081d6dc7d4678
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - July 18, 2017, 10:56 a.m.</div>
<pre class="content">
Commit-ID:  c7753208a94c73d5beb1e4bd843081d6dc7d4678
Gitweb:     http://git.kernel.org/tip/c7753208a94c73d5beb1e4bd843081d6dc7d4678
Author:     Tom Lendacky &lt;thomas.lendacky@amd.com&gt;
AuthorDate: Mon, 17 Jul 2017 16:10:21 -0500
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Tue, 18 Jul 2017 11:38:03 +0200

x86, swiotlb: Add memory encryption support

Since DMA addresses will effectively look like 48-bit addresses when the
memory encryption mask is set, SWIOTLB is needed if the DMA mask of the
device performing the DMA does not support 48-bits. SWIOTLB will be
initialized to create decrypted bounce buffers for use by these devices.
<span class="signed-off-by">
Signed-off-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
Cc: Alexander Potapenko &lt;glider@google.com&gt;
Cc: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Brijesh Singh &lt;brijesh.singh@amd.com&gt;
Cc: Dave Young &lt;dyoung@redhat.com&gt;
Cc: Dmitry Vyukov &lt;dvyukov@google.com&gt;
Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;
Cc: Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;
Cc: Larry Woodman &lt;lwoodman@redhat.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Matt Fleming &lt;matt@codeblueprint.co.uk&gt;
Cc: Michael S. Tsirkin &lt;mst@redhat.com&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Radim Krčmář &lt;rkrcmar@redhat.com&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Toshimitsu Kani &lt;toshi.kani@hpe.com&gt;
Cc: kasan-dev@googlegroups.com
Cc: kvm@vger.kernel.org
Cc: linux-arch@vger.kernel.org
Cc: linux-doc@vger.kernel.org
Cc: linux-efi@vger.kernel.org
Cc: linux-mm@kvack.org
Link: http://lkml.kernel.org/r/aa2d29b78ae7d508db8881e46a3215231b9327a7.1500319216.git.thomas.lendacky@amd.com
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/include/asm/dma-mapping.h |  5 ++--
 arch/x86/include/asm/mem_encrypt.h |  5 ++++
 arch/x86/kernel/pci-dma.c          | 11 +++++---
 arch/x86/kernel/pci-nommu.c        |  2 +-
 arch/x86/kernel/pci-swiotlb.c      | 15 +++++++++--
 arch/x86/mm/mem_encrypt.c          | 22 ++++++++++++++++
 include/linux/swiotlb.h            |  1 +
 init/main.c                        | 10 +++++++
 lib/swiotlb.c                      | 54 ++++++++++++++++++++++++++++++++------
 9 files changed, 108 insertions(+), 17 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_header">index 398c798..1387daf 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/io.h&gt;
 #include &lt;asm/swiotlb.h&gt;
 #include &lt;linux/dma-contiguous.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 #ifdef CONFIG_ISA
 # define ISA_DMA_BIT_MASK DMA_BIT_MASK(24)
<span class="p_chunk">@@ -57,12 +58,12 @@</span> <span class="p_context"> static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
 
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
<span class="p_del">-	return paddr;</span>
<span class="p_add">+	return __sme_set(paddr);</span>
 }
 
 static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)
 {
<span class="p_del">-	return daddr;</span>
<span class="p_add">+	return __sme_clr(daddr);</span>
 }
 #endif /* CONFIG_X86_DMA_REMAP */
 
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index ab1fe77..70e55f6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -34,6 +34,11 @@</span> <span class="p_context"> void __init sme_early_init(void);</span>
 void __init sme_encrypt_kernel(void);
 void __init sme_enable(void);
 
<span class="p_add">+/* Architecture __weak replacement functions */</span>
<span class="p_add">+void __init mem_encrypt_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+void swiotlb_set_mem_attributes(void *vaddr, unsigned long size);</span>
<span class="p_add">+</span>
 #else	/* !CONFIG_AMD_MEM_ENCRYPT */
 
 #define sme_me_mask	0UL
<span class="p_header">diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">index 5e16d3f..0accc24 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-dma.c</span>
<span class="p_chunk">@@ -93,9 +93,12 @@</span> <span class="p_context"> again:</span>
 	if (gfpflags_allow_blocking(flag)) {
 		page = dma_alloc_from_contiguous(dev, count, get_order(size),
 						 flag);
<span class="p_del">-		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="p_del">-			dma_release_from_contiguous(dev, page, count);</span>
<span class="p_del">-			page = NULL;</span>
<span class="p_add">+		if (page) {</span>
<span class="p_add">+			addr = phys_to_dma(dev, page_to_phys(page));</span>
<span class="p_add">+			if (addr + size &gt; dma_mask) {</span>
<span class="p_add">+				dma_release_from_contiguous(dev, page, count);</span>
<span class="p_add">+				page = NULL;</span>
<span class="p_add">+			}</span>
 		}
 	}
 	/* fallback */
<span class="p_chunk">@@ -104,7 +107,7 @@</span> <span class="p_context"> again:</span>
 	if (!page)
 		return NULL;
 
<span class="p_del">-	addr = page_to_phys(page);</span>
<span class="p_add">+	addr = phys_to_dma(dev, page_to_phys(page));</span>
 	if (addr + size &gt; dma_mask) {
 		__free_pages(page, get_order(size));
 
<span class="p_header">diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">index a6d4040..4fc3cb6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"> static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
 				 enum dma_data_direction dir,
 				 unsigned long attrs)
 {
<span class="p_del">-	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="p_add">+	dma_addr_t bus = phys_to_dma(dev, page_to_phys(page)) + offset;</span>
 	WARN_ON(size == 0);
 	if (!check_addr(&quot;map_single&quot;, dev, bus, size))
 		return NOMMU_MAPPING_ERROR;
<span class="p_header">diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">index 1e23577..6770775 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_chunk">@@ -6,12 +6,14 @@</span> <span class="p_context"></span>
 #include &lt;linux/swiotlb.h&gt;
 #include &lt;linux/bootmem.h&gt;
 #include &lt;linux/dma-mapping.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 #include &lt;asm/iommu.h&gt;
 #include &lt;asm/swiotlb.h&gt;
 #include &lt;asm/dma.h&gt;
 #include &lt;asm/xen/swiotlb-xen.h&gt;
 #include &lt;asm/iommu_table.h&gt;
<span class="p_add">+</span>
 int swiotlb __read_mostly;
 
 void *x86_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
<span class="p_chunk">@@ -79,8 +81,8 @@</span> <span class="p_context"> IOMMU_INIT_FINISH(pci_swiotlb_detect_override,</span>
 		  pci_swiotlb_late_init);
 
 /*
<span class="p_del">- * if 4GB or more detected (and iommu=off not set) return 1</span>
<span class="p_del">- * and set swiotlb to 1.</span>
<span class="p_add">+ * If 4GB or more detected (and iommu=off not set) or if SME is active</span>
<span class="p_add">+ * then set swiotlb to 1 and return 1.</span>
  */
 int __init pci_swiotlb_detect_4gb(void)
 {
<span class="p_chunk">@@ -89,6 +91,15 @@</span> <span class="p_context"> int __init pci_swiotlb_detect_4gb(void)</span>
 	if (!no_iommu &amp;&amp; max_possible_pfn &gt; MAX_DMA32_PFN)
 		swiotlb = 1;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If SME is active then swiotlb will be set to 1 so that bounce</span>
<span class="p_add">+	 * buffers are allocated and used for devices that do not support</span>
<span class="p_add">+	 * the addressing range required for the encryption mask.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (sme_active())</span>
<span class="p_add">+		swiotlb = 1;</span>
<span class="p_add">+</span>
 	return swiotlb;
 }
 IOMMU_INIT(pci_swiotlb_detect_4gb,
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 0843d02..a7400ec 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -13,11 +13,14 @@</span> <span class="p_context"></span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/swiotlb.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/bootparam.h&gt;
<span class="p_add">+#include &lt;asm/set_memory.h&gt;</span>
 
 /*
  * Since SME related variables are set early in the boot process they must
<span class="p_chunk">@@ -177,6 +180,25 @@</span> <span class="p_context"> void __init sme_early_init(void)</span>
 		protection_map[i] = pgprot_encrypted(protection_map[i]);
 }
 
<span class="p_add">+/* Architecture __weak replacement functions */</span>
<span class="p_add">+void __init mem_encrypt_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!sme_me_mask)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Call into SWIOTLB to update the SWIOTLB DMA buffers */</span>
<span class="p_add">+	swiotlb_update_mem_attributes();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN(PAGE_ALIGN(size) != size,</span>
<span class="p_add">+	     &quot;size is not page-aligned (%#lx)\n&quot;, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make the SWIOTLB buffer area decrypted */</span>
<span class="p_add">+	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init sme_encrypt_kernel(void)
 {
 }
<span class="p_header">diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h</span>
<span class="p_header">index 4ee479f..15e7160 100644</span>
<span class="p_header">--- a/include/linux/swiotlb.h</span>
<span class="p_header">+++ b/include/linux/swiotlb.h</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"> int swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose);</span>
 extern unsigned long swiotlb_nr_tbl(void);
 unsigned long swiotlb_size_or_default(void);
 extern int swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs);
<span class="p_add">+extern void __init swiotlb_update_mem_attributes(void);</span>
 
 /*
  * Enumeration for sync targets
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 052481f..9789ab7 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -488,6 +488,8 @@</span> <span class="p_context"> void __init __weak thread_stack_cache_init(void)</span>
 }
 #endif
 
<span class="p_add">+void __init __weak mem_encrypt_init(void) { }</span>
<span class="p_add">+</span>
 /*
  * Set up kernel memory allocators
  */
<span class="p_chunk">@@ -641,6 +643,14 @@</span> <span class="p_context"> asmlinkage __visible void __init start_kernel(void)</span>
 	 */
 	locking_selftest();
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This needs to be called before any devices perform DMA</span>
<span class="p_add">+	 * operations that might use the SWIOTLB bounce buffers. It will</span>
<span class="p_add">+	 * mark the bounce buffers as decrypted so that their usage will</span>
<span class="p_add">+	 * not cause &quot;plain-text&quot; data to be decrypted when accessed.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mem_encrypt_init();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start &amp;&amp; !initrd_below_start_ok &amp;&amp;
 	    page_to_pfn(virt_to_page((void *)initrd_start)) &lt; min_low_pfn) {
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index a8d74a7..04ac91a 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -30,6 +30,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/gfp.h&gt;
 #include &lt;linux/scatterlist.h&gt;
<span class="p_add">+#include &lt;linux/mem_encrypt.h&gt;</span>
 
 #include &lt;asm/io.h&gt;
 #include &lt;asm/dma.h&gt;
<span class="p_chunk">@@ -155,6 +156,15 @@</span> <span class="p_context"> unsigned long swiotlb_size_or_default(void)</span>
 	return size ? size : (IO_TLB_DEFAULT_SIZE);
 }
 
<span class="p_add">+void __weak swiotlb_set_mem_attributes(void *vaddr, unsigned long size) { }</span>
<span class="p_add">+</span>
<span class="p_add">+/* For swiotlb, clear memory encryption mask from dma addresses */</span>
<span class="p_add">+static dma_addr_t swiotlb_phys_to_dma(struct device *hwdev,</span>
<span class="p_add">+				      phys_addr_t address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __sme_clr(phys_to_dma(hwdev, address));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Note that this doesn&#39;t work with highmem page */
 static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,
 				      volatile void *address)
<span class="p_chunk">@@ -183,6 +193,31 @@</span> <span class="p_context"> void swiotlb_print_info(void)</span>
 	       bytes &gt;&gt; 20, vstart, vend - 1);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Early SWIOTLB allocation may be too early to allow an architecture to</span>
<span class="p_add">+ * perform the desired operations.  This function allows the architecture to</span>
<span class="p_add">+ * call SWIOTLB when the operations are possible.  It needs to be called</span>
<span class="p_add">+ * before the SWIOTLB memory is used.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init swiotlb_update_mem_attributes(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *vaddr;</span>
<span class="p_add">+	unsigned long bytes;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (no_iotlb_memory || late_alloc)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	vaddr = phys_to_virt(io_tlb_start);</span>
<span class="p_add">+	bytes = PAGE_ALIGN(io_tlb_nslabs &lt;&lt; IO_TLB_SHIFT);</span>
<span class="p_add">+	swiotlb_set_mem_attributes(vaddr, bytes);</span>
<span class="p_add">+	memset(vaddr, 0, bytes);</span>
<span class="p_add">+</span>
<span class="p_add">+	vaddr = phys_to_virt(io_tlb_overflow_buffer);</span>
<span class="p_add">+	bytes = PAGE_ALIGN(io_tlb_overflow);</span>
<span class="p_add">+	swiotlb_set_mem_attributes(vaddr, bytes);</span>
<span class="p_add">+	memset(vaddr, 0, bytes);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	void *v_overflow_buffer;
<span class="p_chunk">@@ -320,6 +355,7 @@</span> <span class="p_context"> swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
 	io_tlb_start = virt_to_phys(tlb);
 	io_tlb_end = io_tlb_start + bytes;
 
<span class="p_add">+	swiotlb_set_mem_attributes(tlb, bytes);</span>
 	memset(tlb, 0, bytes);
 
 	/*
<span class="p_chunk">@@ -330,6 +366,8 @@</span> <span class="p_context"> swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)</span>
 	if (!v_overflow_buffer)
 		goto cleanup2;
 
<span class="p_add">+	swiotlb_set_mem_attributes(v_overflow_buffer, io_tlb_overflow);</span>
<span class="p_add">+	memset(v_overflow_buffer, 0, io_tlb_overflow);</span>
 	io_tlb_overflow_buffer = virt_to_phys(v_overflow_buffer);
 
 	/*
<span class="p_chunk">@@ -581,7 +619,7 @@</span> <span class="p_context"> map_single(struct device *hwdev, phys_addr_t phys, size_t size,</span>
 		return SWIOTLB_MAP_ERROR;
 	}
 
<span class="p_del">-	start_dma_addr = phys_to_dma(hwdev, io_tlb_start);</span>
<span class="p_add">+	start_dma_addr = swiotlb_phys_to_dma(hwdev, io_tlb_start);</span>
 	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size,
 				      dir, attrs);
 }
<span class="p_chunk">@@ -702,7 +740,7 @@</span> <span class="p_context"> swiotlb_alloc_coherent(struct device *hwdev, size_t size,</span>
 			goto err_warn;
 
 		ret = phys_to_virt(paddr);
<span class="p_del">-		dev_addr = phys_to_dma(hwdev, paddr);</span>
<span class="p_add">+		dev_addr = swiotlb_phys_to_dma(hwdev, paddr);</span>
 
 		/* Confirm address can be DMA&#39;d by device */
 		if (dev_addr + size - 1 &gt; dma_mask) {
<span class="p_chunk">@@ -812,10 +850,10 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 	map = map_single(dev, phys, size, dir, attrs);
 	if (map == SWIOTLB_MAP_ERROR) {
 		swiotlb_full(dev, size, dir, 1);
<span class="p_del">-		return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+		return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 	}
 
<span class="p_del">-	dev_addr = phys_to_dma(dev, map);</span>
<span class="p_add">+	dev_addr = swiotlb_phys_to_dma(dev, map);</span>
 
 	/* Ensure that the address returned is DMA&#39;ble */
 	if (dma_capable(dev, dev_addr, size))
<span class="p_chunk">@@ -824,7 +862,7 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 	attrs |= DMA_ATTR_SKIP_CPU_SYNC;
 	swiotlb_tbl_unmap_single(dev, map, size, dir, attrs);
 
<span class="p_del">-	return phys_to_dma(dev, io_tlb_overflow_buffer);</span>
<span class="p_add">+	return swiotlb_phys_to_dma(dev, io_tlb_overflow_buffer);</span>
 }
 EXPORT_SYMBOL_GPL(swiotlb_map_page);
 
<span class="p_chunk">@@ -958,7 +996,7 @@</span> <span class="p_context"> swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,</span>
 				sg_dma_len(sgl) = 0;
 				return 0;
 			}
<span class="p_del">-			sg-&gt;dma_address = phys_to_dma(hwdev, map);</span>
<span class="p_add">+			sg-&gt;dma_address = swiotlb_phys_to_dma(hwdev, map);</span>
 		} else
 			sg-&gt;dma_address = dev_addr;
 		sg_dma_len(sg) = sg-&gt;length;
<span class="p_chunk">@@ -1026,7 +1064,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(swiotlb_sync_sg_for_device);</span>
 int
 swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)
 {
<span class="p_del">-	return (dma_addr == phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
<span class="p_add">+	return (dma_addr == swiotlb_phys_to_dma(hwdev, io_tlb_overflow_buffer));</span>
 }
 EXPORT_SYMBOL(swiotlb_dma_mapping_error);
 
<span class="p_chunk">@@ -1039,6 +1077,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(swiotlb_dma_mapping_error);</span>
 int
 swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
<span class="p_del">-	return phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
<span class="p_add">+	return swiotlb_phys_to_dma(hwdev, io_tlb_end - 1) &lt;= mask;</span>
 }
 EXPORT_SYMBOL(swiotlb_dma_supported);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



