
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/4] hugetlb, memory_hotplug: prefer to use reserved pages for migration - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/4] hugetlb, memory_hotplug: prefer to use reserved pages for migration</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 8, 2017, 7:45 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170608074553.22152-3-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9774125/mbox/"
   >mbox</a>
|
   <a href="/patch/9774125/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9774125/">/patch/9774125/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	72FDC60393 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 607BE22B39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 555F525404; Thu,  8 Jun 2017 07:46:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 97BCC22B39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751923AbdFHHqz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 8 Jun 2017 03:46:55 -0400
Received: from mail-wm0-f68.google.com ([74.125.82.68]:36726 &quot;EHLO
	mail-wm0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751815AbdFHHqJ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 8 Jun 2017 03:46:09 -0400
Received: by mail-wm0-f68.google.com with SMTP id d17so1056026wme.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 08 Jun 2017 00:46:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=oPPYiHEZPLTztlmddzUaYfB6WkoYfeW1gxQqlr4ZkaQ=;
	b=NYH0tOHbhbz9wsaAWAvFSQQ9taDU+n5m+0MQxcdVTTvF8fkp0BgJf0Bjk5csw496jS
	OBTt3SFA65x3O7igBFLTNeS23/4sTNZmAMKcdtau/OjA3zgc+ZywIs4XZ3t6cmP0jfBN
	vV5vjeOacy4Oj43OztRmTLexVMwORlfERHVfffXaRHr+Kb5+E9fwPeoqZAX6FUOpP5fF
	OE2SQkXt2oJiiWE98vXjX0QYoVEoRWNPAb2+rCtxEAsBj3d91aKBEVGLAXzwfaxSRldz
	m8QEb9KYe+q4OlUeIxaNdHebLarQ4CeO0t4Br9nliOSWQwZD+NEb85P5wlWRBXTZWgdH
	WsJA==
X-Gm-Message-State: AODbwcClLt6f1EBmeEooCAM9EghPEgUh3WYdfuPX72bdVnQruSDUHYci
	LLimABMJ9SYYIg==
X-Received: by 10.28.71.147 with SMTP id m19mr2255628wmi.92.1496907968388;
	Thu, 08 Jun 2017 00:46:08 -0700 (PDT)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	i64sm5834505wmd.33.2017.06.08.00.46.07
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 08 Jun 2017 00:46:07 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Xishi Qiu &lt;qiuxishi@huawei.com&gt;, zhong jiang &lt;zhongjiang@huawei.com&gt;,
	Joonsoo Kim &lt;js1304@gmail.com&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 2/4] hugetlb,
	memory_hotplug: prefer to use reserved pages for migration
Date: Thu,  8 Jun 2017 09:45:51 +0200
Message-Id: &lt;20170608074553.22152-3-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170608074553.22152-1-mhocko@kernel.org&gt;
References: &lt;20170608074553.22152-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 8, 2017, 7:45 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

new_node_page will try to use the origin&#39;s next NUMA node as the
migration destination for hugetlb pages. If such a node doesn&#39;t have any
preallocated pool it falls back to __alloc_buddy_huge_page_no_mpol to
allocate a surplus page instead. This is quite subotpimal for any
configuration when hugetlb pages are no distributed to all NUMA nodes
evenly. Say we have a hotplugable node 4 and spare hugetlb pages are
node 0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:10000
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node2/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node3/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node4/hugepages/hugepages-2048kB/nr_hugepages:10000
/sys/devices/system/node/node5/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node6/hugepages/hugepages-2048kB/nr_hugepages:0
/sys/devices/system/node/node7/hugepages/hugepages-2048kB/nr_hugepages:0

Now we consume the whole pool on node 4 and try to offline this
node. All the allocated pages should be moved to node0 which has enough
preallocated pages to hold them. With the current implementation
offlining very likely fails because hugetlb allocations during runtime
are much less reliable.

Fix this by reusing the nodemask which excludes migration source and try
to find a first node which has a page in the preallocated pool first and
fall back to __alloc_buddy_huge_page_no_mpol only when the whole pool is
consumed.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |  2 ++
 mm/hugetlb.c            | 27 +++++++++++++++++++++++++++
 mm/memory_hotplug.c     |  9 ++-------
 3 files changed, 31 insertions(+), 7 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 8, 2017, 8:22 a.m.</div>
<pre class="content">
On 06/08/2017 09:45 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; new_node_page will try to use the origin&#39;s next NUMA node as the</span>
<span class="quote">&gt; migration destination for hugetlb pages. If such a node doesn&#39;t have any</span>
<span class="quote">&gt; preallocated pool it falls back to __alloc_buddy_huge_page_no_mpol to</span>
<span class="quote">&gt; allocate a surplus page instead. This is quite subotpimal for any</span>
<span class="quote">&gt; configuration when hugetlb pages are no distributed to all NUMA nodes</span>
<span class="quote">&gt; evenly. Say we have a hotplugable node 4 and spare hugetlb pages are</span>
<span class="quote">&gt; node 0</span>
<span class="quote">&gt; /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:10000</span>
<span class="quote">&gt; /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; /sys/devices/system/node/node2/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; /sys/devices/system/node/node3/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; /sys/devices/system/node/node4/hugepages/hugepages-2048kB/nr_hugepages:10000</span>
<span class="quote">&gt; /sys/devices/system/node/node5/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; /sys/devices/system/node/node6/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; /sys/devices/system/node/node7/hugepages/hugepages-2048kB/nr_hugepages:0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now we consume the whole pool on node 4 and try to offline this</span>
<span class="quote">&gt; node. All the allocated pages should be moved to node0 which has enough</span>
<span class="quote">&gt; preallocated pages to hold them. With the current implementation</span>
<span class="quote">&gt; offlining very likely fails because hugetlb allocations during runtime</span>
<span class="quote">&gt; are much less reliable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fix this by reusing the nodemask which excludes migration source and try</span>
<span class="quote">&gt; to find a first node which has a page in the preallocated pool first and</span>
<span class="quote">&gt; fall back to __alloc_buddy_huge_page_no_mpol only when the whole pool is</span>
<span class="quote">&gt; consumed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index dbb118c566cd..c469191bb13b 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,6 +349,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_chunk">@@ -524,6 +525,7 @@</span> <span class="p_context"> static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr</span>
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
<span class="p_add">+#define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL</span>
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 761a669d0b62..01c11ceb47d6 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1723,6 +1723,33 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	int node;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;hugetlb_lock);</span>
<span class="p_add">+	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="p_add">+		for_each_node_mask(node, *nmask) {</span>
<span class="p_add">+			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="p_add">+			if (page)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(&amp;hugetlb_lock);</span>
<span class="p_add">+	if (page)</span>
<span class="p_add">+		return page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* No reservations, try to overcommit */</span>
<span class="p_add">+	for_each_node_mask(node, *nmask) {</span>
<span class="p_add">+		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Increase the hugetlb pool such that it can accommodate a reservation
  * of size &#39;delta&#39;.
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index 1ca373bdffbf..6e0d964ac561 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -1442,14 +1442,9 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long private,</span>
 	if (nodes_empty(nmask))
 		node_set(nid, nmask);
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * TODO: allocate a destination hugepage from a nearest neighbor node,</span>
<span class="p_del">-	 * accordance with memory policy of the user process if possible. For</span>
<span class="p_del">-	 * now as a simple work-around, we use the next node for destination.</span>
<span class="p_del">-	 */</span>
 	if (PageHuge(page))
<span class="p_del">-		return alloc_huge_page_node(page_hstate(compound_head(page)),</span>
<span class="p_del">-					next_node_in(nid, nmask));</span>
<span class="p_add">+		return alloc_huge_page_nodemask(</span>
<span class="p_add">+				page_hstate(compound_head(page)), &amp;nmask);</span>
 
 	if (PageHighMem(page)
 	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



