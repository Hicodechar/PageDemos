
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/3] IPI: Avoid to use 2 cache lines for one call_single_data - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/3] IPI: Avoid to use 2 cache lines for one call_single_data</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=294">Huang Ying</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 8, 2017, 4:30 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9886503/mbox/"
   >mbox</a>
|
   <a href="/patch/9886503/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9886503/">/patch/9886503/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9429E60384 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 04:31:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7E0C127D0E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 04:31:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7091028783; Tue,  8 Aug 2017 04:31:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 31FD427F85
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 04:31:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751965AbdHHEa1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 8 Aug 2017 00:30:27 -0400
Received: from mga09.intel.com ([134.134.136.24]:40480 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750727AbdHHEa0 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 8 Aug 2017 00:30:26 -0400
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	07 Aug 2017 21:30:25 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.41,341,1498546800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1001164266&quot;
Received: from tlim17-mobl1.ccr.corp.intel.com (HELO yhuang-mobile)
	([10.254.209.221])
	by orsmga003.jf.intel.com with ESMTP; 07 Aug 2017 21:30:08 -0700
From: &quot;Huang\, Ying&quot; &lt;ying.huang@intel.com&gt;
To: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: &quot;Huang\, Ying&quot; &lt;ying.huang@intel.com&gt;,
	Eric Dumazet &lt;eric.dumazet@gmail.com&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	Michael Ellerman &lt;mpe@ellerman.id.au&gt;, Borislav Petkov &lt;bp@suse.de&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;Juergen Gross&quot; &lt;jgross@suse.com&gt;, Aaron Lu &lt;aaron.lu@intel.com&gt;
Subject: Re: [PATCH 3/3] IPI: Avoid to use 2 cache lines for one
	call_single_data
References: &lt;20170802085220.4315-1-ying.huang@intel.com&gt;
	&lt;20170802085220.4315-4-ying.huang@intel.com&gt;
	&lt;1501669138.25002.20.camel@edumazet-glaptop3.roam.corp.google.com&gt;
	&lt;87d18d122e.fsf@yhuang-dev.intel.com&gt;
	&lt;20170803085752.yrqox3kwrvkj544a@hirez.programming.kicks-ass.net&gt;
	&lt;87wp6kyvda.fsf@yhuang-dev.intel.com&gt;
	&lt;87mv7gytmk.fsf@yhuang-dev.intel.com&gt;
	&lt;20170804092754.hyhbhyr2r4gonpu4@hirez.programming.kicks-ass.net&gt;
	&lt;87d18alu2h.fsf@yhuang-mobile.sh.intel.com&gt;
	&lt;20170807082837.dakfoq5kbj52opha@hirez.programming.kicks-ass.net&gt;
Date: Tue, 08 Aug 2017 12:30:00 +0800
In-Reply-To: &lt;20170807082837.dakfoq5kbj52opha@hirez.programming.kicks-ass.net&gt;
	(Peter Zijlstra&#39;s message of &quot;Mon, 7 Aug 2017 10:28:37 +0200&quot;)
Message-ID: &lt;87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com&gt;
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/25.1 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=ascii
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 8, 2017, 4:30 a.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; writes:
<span class="quote">
&gt; On Sat, Aug 05, 2017 at 08:47:02AM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt; Yes.  That looks good.  So you will prepare the final patch?  Or you</span>
<span class="quote">&gt;&gt; hope me to do that?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I was hoping you&#39;d do it ;-)</span>

Thanks!  Here is the updated patch

Best Regards,
Huang, Ying

----------&gt;8----------
From 957735e9ff3922368286540dab852986fc7b23b5 Mon Sep 17 00:00:00 2001
<span class="from">From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
Date: Mon, 7 Aug 2017 16:55:33 +0800
Subject: [PATCH -v3] IPI: Avoid to use 2 cache lines for one
 call_single_data

struct call_single_data is used in IPI to transfer information between
CPUs.  Its size is bigger than sizeof(unsigned long) and less than
cache line size.  Now, it is allocated with no explicit alignment
requirement.  This makes it possible for allocated call_single_data to
cross 2 cache lines.  So that double the number of the cache lines
that need to be transferred among CPUs.

This is resolved by requiring call_single_data to be aligned with the
size of call_single_data.  Now the size of call_single_data is the
power of 2.  If we add new fields to call_single_data, we may need to
add pads to make sure the size of new definition is the power of 2.
Fortunately, this is enforced by gcc, which will report error for not
power of 2 alignment requirement.

To set alignment requirement of call_single_data to the size of
call_single_data, a struct definition and a typedef is used.

To test the effect of the patch, we use the vm-scalability multiple
thread swap test case (swap-w-seq-mt).  The test will create multiple
threads and each thread will eat memory until all RAM and part of swap
is used, so that huge number of IPI will be triggered when unmapping
memory.  In the test, the throughput of memory writing improves ~5%
compared with misaligned call_single_data because of faster IPI.

[Add call_single_data_t and align with size of call_single_data]
Suggested-by: Peter Zijlstra &lt;peterz@infradead.org&gt;
<span class="signed-off-by">Signed-off-by: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;</span>
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: Michael Ellerman &lt;mpe@ellerman.id.au&gt;
Cc: Borislav Petkov &lt;bp@suse.de&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Juergen Gross &lt;jgross@suse.com&gt;
Cc: Aaron Lu &lt;aaron.lu@intel.com&gt;
---
 arch/mips/kernel/smp.c                             |  6 ++--
 block/blk-softirq.c                                |  2 +-
 drivers/block/null_blk.c                           |  2 +-
 drivers/cpuidle/coupled.c                          | 10 +++----
 drivers/net/ethernet/cavium/liquidio/lio_main.c    |  2 +-
 drivers/net/ethernet/cavium/liquidio/octeon_droq.h |  2 +-
 include/linux/blkdev.h                             |  2 +-
 include/linux/netdevice.h                          |  2 +-
 include/linux/smp.h                                |  8 ++++--
 kernel/sched/sched.h                               |  2 +-
 kernel/smp.c                                       | 32 ++++++++++++----------
 kernel/up.c                                        |  2 +-
 12 files changed, 39 insertions(+), 33 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 14, 2017, 5:44 a.m.</div>
<pre class="content">
Hi, Peter,

&quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; writes:
<span class="quote">
&gt; Peter Zijlstra &lt;peterz@infradead.org&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Sat, Aug 05, 2017 at 08:47:02AM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt;&gt; Yes.  That looks good.  So you will prepare the final patch?  Or you</span>
<span class="quote">&gt;&gt;&gt; hope me to do that?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I was hoping you&#39;d do it ;-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thanks!  Here is the updated patch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Best Regards,</span>
<span class="quote">&gt; Huang, Ying</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ----------&gt;8----------</span>
<span class="quote">&gt; From 957735e9ff3922368286540dab852986fc7b23b5 Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; Date: Mon, 7 Aug 2017 16:55:33 +0800</span>
<span class="quote">&gt; Subject: [PATCH -v3] IPI: Avoid to use 2 cache lines for one</span>
<span class="quote">&gt;  call_single_data</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; struct call_single_data is used in IPI to transfer information between</span>
<span class="quote">&gt; CPUs.  Its size is bigger than sizeof(unsigned long) and less than</span>
<span class="quote">&gt; cache line size.  Now, it is allocated with no explicit alignment</span>
<span class="quote">&gt; requirement.  This makes it possible for allocated call_single_data to</span>
<span class="quote">&gt; cross 2 cache lines.  So that double the number of the cache lines</span>
<span class="quote">&gt; that need to be transferred among CPUs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is resolved by requiring call_single_data to be aligned with the</span>
<span class="quote">&gt; size of call_single_data.  Now the size of call_single_data is the</span>
<span class="quote">&gt; power of 2.  If we add new fields to call_single_data, we may need to</span>
<span class="quote">&gt; add pads to make sure the size of new definition is the power of 2.</span>
<span class="quote">&gt; Fortunately, this is enforced by gcc, which will report error for not</span>
<span class="quote">&gt; power of 2 alignment requirement.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To set alignment requirement of call_single_data to the size of</span>
<span class="quote">&gt; call_single_data, a struct definition and a typedef is used.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To test the effect of the patch, we use the vm-scalability multiple</span>
<span class="quote">&gt; thread swap test case (swap-w-seq-mt).  The test will create multiple</span>
<span class="quote">&gt; threads and each thread will eat memory until all RAM and part of swap</span>
<span class="quote">&gt; is used, so that huge number of IPI will be triggered when unmapping</span>
<span class="quote">&gt; memory.  In the test, the throughput of memory writing improves ~5%</span>
<span class="quote">&gt; compared with misaligned call_single_data because of faster IPI.</span>

What do you think about this version?

Best Regards,
Huang, Ying
<span class="quote">
&gt; [Add call_single_data_t and align with size of call_single_data]</span>
<span class="quote">&gt; Suggested-by: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; Signed-off-by: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; Cc: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Michael Ellerman &lt;mpe@ellerman.id.au&gt;</span>
<span class="quote">&gt; Cc: Borislav Petkov &lt;bp@suse.de&gt;</span>
<span class="quote">&gt; Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; Cc: Juergen Gross &lt;jgross@suse.com&gt;</span>
<span class="quote">&gt; Cc: Aaron Lu &lt;aaron.lu@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/mips/kernel/smp.c                             |  6 ++--</span>
<span class="quote">&gt;  block/blk-softirq.c                                |  2 +-</span>
<span class="quote">&gt;  drivers/block/null_blk.c                           |  2 +-</span>
<span class="quote">&gt;  drivers/cpuidle/coupled.c                          | 10 +++----</span>
<span class="quote">&gt;  drivers/net/ethernet/cavium/liquidio/lio_main.c    |  2 +-</span>
<span class="quote">&gt;  drivers/net/ethernet/cavium/liquidio/octeon_droq.h |  2 +-</span>
<span class="quote">&gt;  include/linux/blkdev.h                             |  2 +-</span>
<span class="quote">&gt;  include/linux/netdevice.h                          |  2 +-</span>
<span class="quote">&gt;  include/linux/smp.h                                |  8 ++++--</span>
<span class="quote">&gt;  kernel/sched/sched.h                               |  2 +-</span>
<span class="quote">&gt;  kernel/smp.c                                       | 32 ++++++++++++----------</span>
<span class="quote">&gt;  kernel/up.c                                        |  2 +-</span>
<span class="quote">&gt;  12 files changed, 39 insertions(+), 33 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c</span>
<span class="quote">&gt; index 770d4d1516cb..bd8ba5472bca 100644</span>
<span class="quote">&gt; --- a/arch/mips/kernel/smp.c</span>
<span class="quote">&gt; +++ b/arch/mips/kernel/smp.c</span>
<span class="quote">&gt; @@ -648,12 +648,12 @@ EXPORT_SYMBOL(flush_tlb_one);</span>
<span class="quote">&gt;  #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);</span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct call_single_data, tick_broadcast_csd);</span>
<span class="quote">&gt; +static DEFINE_PER_CPU(call_single_data_t, tick_broadcast_csd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void tick_broadcast(const struct cpumask *mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	atomic_t *count;</span>
<span class="quote">&gt; -	struct call_single_data *csd;</span>
<span class="quote">&gt; +	call_single_data_t *csd;</span>
<span class="quote">&gt;  	int cpu;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_cpu(cpu, mask) {</span>
<span class="quote">&gt; @@ -674,7 +674,7 @@ static void tick_broadcast_callee(void *info)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int __init tick_broadcast_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct call_single_data *csd;</span>
<span class="quote">&gt; +	call_single_data_t *csd;</span>
<span class="quote">&gt;  	int cpu;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (cpu = 0; cpu &lt; NR_CPUS; cpu++) {</span>
<span class="quote">&gt; diff --git a/block/blk-softirq.c b/block/blk-softirq.c</span>
<span class="quote">&gt; index 87b7df4851bf..07125e7941f4 100644</span>
<span class="quote">&gt; --- a/block/blk-softirq.c</span>
<span class="quote">&gt; +++ b/block/blk-softirq.c</span>
<span class="quote">&gt; @@ -60,7 +60,7 @@ static void trigger_softirq(void *data)</span>
<span class="quote">&gt;  static int raise_blk_irq(int cpu, struct request *rq)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (cpu_online(cpu)) {</span>
<span class="quote">&gt; -		struct call_single_data *data = &amp;rq-&gt;csd;</span>
<span class="quote">&gt; +		call_single_data_t *data = &amp;rq-&gt;csd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		data-&gt;func = trigger_softirq;</span>
<span class="quote">&gt;  		data-&gt;info = rq;</span>
<span class="quote">&gt; diff --git a/drivers/block/null_blk.c b/drivers/block/null_blk.c</span>
<span class="quote">&gt; index 85c24cace973..81142ce781da 100644</span>
<span class="quote">&gt; --- a/drivers/block/null_blk.c</span>
<span class="quote">&gt; +++ b/drivers/block/null_blk.c</span>
<span class="quote">&gt; @@ -13,7 +13,7 @@</span>
<span class="quote">&gt;  struct nullb_cmd {</span>
<span class="quote">&gt;  	struct list_head list;</span>
<span class="quote">&gt;  	struct llist_node ll_list;</span>
<span class="quote">&gt; -	struct call_single_data csd;</span>
<span class="quote">&gt; +	call_single_data_t csd;</span>
<span class="quote">&gt;  	struct request *rq;</span>
<span class="quote">&gt;  	struct bio *bio;</span>
<span class="quote">&gt;  	unsigned int tag;</span>
<span class="quote">&gt; diff --git a/drivers/cpuidle/coupled.c b/drivers/cpuidle/coupled.c</span>
<span class="quote">&gt; index 71e586d7df71..147f38ea0fcd 100644</span>
<span class="quote">&gt; --- a/drivers/cpuidle/coupled.c</span>
<span class="quote">&gt; +++ b/drivers/cpuidle/coupled.c</span>
<span class="quote">&gt; @@ -119,13 +119,13 @@ struct cpuidle_coupled {</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define CPUIDLE_COUPLED_NOT_IDLE	(-1)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct call_single_data, cpuidle_coupled_poke_cb);</span>
<span class="quote">&gt; +static DEFINE_PER_CPU(call_single_data_t, cpuidle_coupled_poke_cb);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * The cpuidle_coupled_poke_pending mask is used to avoid calling</span>
<span class="quote">&gt; - * __smp_call_function_single with the per cpu call_single_data struct already</span>
<span class="quote">&gt; + * __smp_call_function_single with the per cpu call_single_data_t struct already</span>
<span class="quote">&gt;   * in use.  This prevents a deadlock where two cpus are waiting for each others</span>
<span class="quote">&gt; - * call_single_data struct to be available</span>
<span class="quote">&gt; + * call_single_data_t struct to be available</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static cpumask_t cpuidle_coupled_poke_pending;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -339,7 +339,7 @@ static void cpuidle_coupled_handle_poke(void *info)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static void cpuidle_coupled_poke(int cpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct call_single_data *csd = &amp;per_cpu(cpuidle_coupled_poke_cb, cpu);</span>
<span class="quote">&gt; +	call_single_data_t *csd = &amp;per_cpu(cpuidle_coupled_poke_cb, cpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!cpumask_test_and_set_cpu(cpu, &amp;cpuidle_coupled_poke_pending))</span>
<span class="quote">&gt;  		smp_call_function_single_async(cpu, csd);</span>
<span class="quote">&gt; @@ -651,7 +651,7 @@ int cpuidle_coupled_register_device(struct cpuidle_device *dev)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int cpu;</span>
<span class="quote">&gt;  	struct cpuidle_device *other_dev;</span>
<span class="quote">&gt; -	struct call_single_data *csd;</span>
<span class="quote">&gt; +	call_single_data_t *csd;</span>
<span class="quote">&gt;  	struct cpuidle_coupled *coupled;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (cpumask_empty(&amp;dev-&gt;coupled_cpus))</span>
<span class="quote">&gt; diff --git a/drivers/net/ethernet/cavium/liquidio/lio_main.c b/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="quote">&gt; index 51583ae4b1eb..120b6e537b28 100644</span>
<span class="quote">&gt; --- a/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="quote">&gt; +++ b/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="quote">&gt; @@ -2468,7 +2468,7 @@ static void liquidio_napi_drv_callback(void *arg)</span>
<span class="quote">&gt;  	if (OCTEON_CN23XX_PF(oct) || droq-&gt;cpu_id == this_cpu) {</span>
<span class="quote">&gt;  		napi_schedule_irqoff(&amp;droq-&gt;napi);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		struct call_single_data *csd = &amp;droq-&gt;csd;</span>
<span class="quote">&gt; +		call_single_data_t *csd = &amp;droq-&gt;csd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		csd-&gt;func = napi_schedule_wrapper;</span>
<span class="quote">&gt;  		csd-&gt;info = &amp;droq-&gt;napi;</span>
<span class="quote">&gt; diff --git a/drivers/net/ethernet/cavium/liquidio/octeon_droq.h b/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="quote">&gt; index 6efd139b894d..f91bc84d1719 100644</span>
<span class="quote">&gt; --- a/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="quote">&gt; +++ b/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="quote">&gt; @@ -328,7 +328,7 @@ struct octeon_droq {</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	u32 cpu_id;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	struct call_single_data csd;</span>
<span class="quote">&gt; +	call_single_data_t csd;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define OCT_DROQ_SIZE   (sizeof(struct octeon_droq))</span>
<span class="quote">&gt; diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h</span>
<span class="quote">&gt; index 25f6a0cb27d3..006fa09a641e 100644</span>
<span class="quote">&gt; --- a/include/linux/blkdev.h</span>
<span class="quote">&gt; +++ b/include/linux/blkdev.h</span>
<span class="quote">&gt; @@ -134,7 +134,7 @@ typedef __u32 __bitwise req_flags_t;</span>
<span class="quote">&gt;  struct request {</span>
<span class="quote">&gt;  	struct list_head queuelist;</span>
<span class="quote">&gt;  	union {</span>
<span class="quote">&gt; -		struct call_single_data csd;</span>
<span class="quote">&gt; +		call_single_data_t csd;</span>
<span class="quote">&gt;  		u64 fifo_time;</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h</span>
<span class="quote">&gt; index 779b23595596..6557f320b66e 100644</span>
<span class="quote">&gt; --- a/include/linux/netdevice.h</span>
<span class="quote">&gt; +++ b/include/linux/netdevice.h</span>
<span class="quote">&gt; @@ -2774,7 +2774,7 @@ struct softnet_data {</span>
<span class="quote">&gt;  	unsigned int		input_queue_head ____cacheline_aligned_in_smp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Elements below can be accessed between CPUs for RPS/RFS */</span>
<span class="quote">&gt; -	struct call_single_data	csd ____cacheline_aligned_in_smp;</span>
<span class="quote">&gt; +	call_single_data_t	csd ____cacheline_aligned_in_smp;</span>
<span class="quote">&gt;  	struct softnet_data	*rps_ipi_next;</span>
<span class="quote">&gt;  	unsigned int		cpu;</span>
<span class="quote">&gt;  	unsigned int		input_queue_tail;</span>
<span class="quote">&gt; diff --git a/include/linux/smp.h b/include/linux/smp.h</span>
<span class="quote">&gt; index 68123c1fe549..98b1fe027fc9 100644</span>
<span class="quote">&gt; --- a/include/linux/smp.h</span>
<span class="quote">&gt; +++ b/include/linux/smp.h</span>
<span class="quote">&gt; @@ -14,13 +14,17 @@</span>
<span class="quote">&gt;  #include &lt;linux/llist.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  typedef void (*smp_call_func_t)(void *info);</span>
<span class="quote">&gt; -struct call_single_data {</span>
<span class="quote">&gt; +struct __call_single_data {</span>
<span class="quote">&gt;  	struct llist_node llist;</span>
<span class="quote">&gt;  	smp_call_func_t func;</span>
<span class="quote">&gt;  	void *info;</span>
<span class="quote">&gt;  	unsigned int flags;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* Use __aligned() to avoid to use 2 cache lines for 1 csd */</span>
<span class="quote">&gt; +typedef struct __call_single_data call_single_data_t</span>
<span class="quote">&gt; +	__aligned(sizeof(struct __call_single_data));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* total number of cpus in this system (may exceed NR_CPUS) */</span>
<span class="quote">&gt;  extern unsigned int total_cpus;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -48,7 +52,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
<span class="quote">&gt;  		smp_call_func_t func, void *info, bool wait,</span>
<span class="quote">&gt;  		gfp_t gfp_flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -int smp_call_function_single_async(int cpu, struct call_single_data *csd);</span>
<span class="quote">&gt; +int smp_call_function_single_async(int cpu, call_single_data_t *csd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="quote">&gt; index eeef1a3086d1..f29a7d2b57e1 100644</span>
<span class="quote">&gt; --- a/kernel/sched/sched.h</span>
<span class="quote">&gt; +++ b/kernel/sched/sched.h</span>
<span class="quote">&gt; @@ -769,7 +769,7 @@ struct rq {</span>
<span class="quote">&gt;  #ifdef CONFIG_SCHED_HRTICK</span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  	int hrtick_csd_pending;</span>
<span class="quote">&gt; -	struct call_single_data hrtick_csd;</span>
<span class="quote">&gt; +	call_single_data_t hrtick_csd;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	struct hrtimer hrtick_timer;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="quote">&gt; index 3061483cb3ad..81cfca9b4cc3 100644</span>
<span class="quote">&gt; --- a/kernel/smp.c</span>
<span class="quote">&gt; +++ b/kernel/smp.c</span>
<span class="quote">&gt; @@ -28,7 +28,7 @@ enum {</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct call_function_data {</span>
<span class="quote">&gt; -	struct call_single_data	__percpu *csd;</span>
<span class="quote">&gt; +	call_single_data_t	__percpu *csd;</span>
<span class="quote">&gt;  	cpumask_var_t		cpumask;</span>
<span class="quote">&gt;  	cpumask_var_t		cpumask_ipi;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt; @@ -51,7 +51,7 @@ int smpcfd_prepare_cpu(unsigned int cpu)</span>
<span class="quote">&gt;  		free_cpumask_var(cfd-&gt;cpumask);</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	cfd-&gt;csd = alloc_percpu(struct call_single_data);</span>
<span class="quote">&gt; +	cfd-&gt;csd = alloc_percpu(call_single_data_t);</span>
<span class="quote">&gt;  	if (!cfd-&gt;csd) {</span>
<span class="quote">&gt;  		free_cpumask_var(cfd-&gt;cpumask);</span>
<span class="quote">&gt;  		free_cpumask_var(cfd-&gt;cpumask_ipi);</span>
<span class="quote">&gt; @@ -103,12 +103,12 @@ void __init call_function_init(void)</span>
<span class="quote">&gt;   * previous function call. For multi-cpu calls its even more interesting</span>
<span class="quote">&gt;   * as we&#39;ll have to ensure no other cpu is observing our csd.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static __always_inline void csd_lock_wait(struct call_single_data *csd)</span>
<span class="quote">&gt; +static __always_inline void csd_lock_wait(call_single_data_t *csd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	smp_cond_load_acquire(&amp;csd-&gt;flags, !(VAL &amp; CSD_FLAG_LOCK));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static __always_inline void csd_lock(struct call_single_data *csd)</span>
<span class="quote">&gt; +static __always_inline void csd_lock(call_single_data_t *csd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	csd_lock_wait(csd);</span>
<span class="quote">&gt;  	csd-&gt;flags |= CSD_FLAG_LOCK;</span>
<span class="quote">&gt; @@ -116,12 +116,12 @@ static __always_inline void csd_lock(struct call_single_data *csd)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * prevent CPU from reordering the above assignment</span>
<span class="quote">&gt;  	 * to -&gt;flags with any subsequent assignments to other</span>
<span class="quote">&gt; -	 * fields of the specified call_single_data structure:</span>
<span class="quote">&gt; +	 * fields of the specified call_single_data_t structure:</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	smp_wmb();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static __always_inline void csd_unlock(struct call_single_data *csd)</span>
<span class="quote">&gt; +static __always_inline void csd_unlock(call_single_data_t *csd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	WARN_ON(!(csd-&gt;flags &amp; CSD_FLAG_LOCK));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -131,14 +131,14 @@ static __always_inline void csd_unlock(struct call_single_data *csd)</span>
<span class="quote">&gt;  	smp_store_release(&amp;csd-&gt;flags, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_single_data, csd_data);</span>
<span class="quote">&gt; +static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Insert a previously allocated call_single_data element</span>
<span class="quote">&gt; + * Insert a previously allocated call_single_data_t element</span>
<span class="quote">&gt;   * for execution on the given CPU. data must already have</span>
<span class="quote">&gt;   * -&gt;func, -&gt;info, and -&gt;flags set.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int generic_exec_single(int cpu, struct call_single_data *csd,</span>
<span class="quote">&gt; +static int generic_exec_single(int cpu, call_single_data_t *csd,</span>
<span class="quote">&gt;  			       smp_call_func_t func, void *info)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (cpu == smp_processor_id()) {</span>
<span class="quote">&gt; @@ -210,7 +210,7 @@ static void flush_smp_call_function_queue(bool warn_cpu_offline)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct llist_head *head;</span>
<span class="quote">&gt;  	struct llist_node *entry;</span>
<span class="quote">&gt; -	struct call_single_data *csd, *csd_next;</span>
<span class="quote">&gt; +	call_single_data_t *csd, *csd_next;</span>
<span class="quote">&gt;  	static bool warned;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	WARN_ON(!irqs_disabled());</span>
<span class="quote">&gt; @@ -268,8 +268,10 @@ static void flush_smp_call_function_queue(bool warn_cpu_offline)</span>
<span class="quote">&gt;  int smp_call_function_single(int cpu, smp_call_func_t func, void *info,</span>
<span class="quote">&gt;  			     int wait)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct call_single_data *csd;</span>
<span class="quote">&gt; -	struct call_single_data csd_stack = { .flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS };</span>
<span class="quote">&gt; +	call_single_data_t *csd;</span>
<span class="quote">&gt; +	call_single_data_t csd_stack = {</span>
<span class="quote">&gt; +		.flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS,</span>
<span class="quote">&gt; +	};</span>
<span class="quote">&gt;  	int this_cpu;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -321,7 +323,7 @@ EXPORT_SYMBOL(smp_call_function_single);</span>
<span class="quote">&gt;   * NOTE: Be careful, there is unfortunately no current debugging facility to</span>
<span class="quote">&gt;   * validate the correctness of this serialization.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int smp_call_function_single_async(int cpu, struct call_single_data *csd)</span>
<span class="quote">&gt; +int smp_call_function_single_async(int cpu, call_single_data_t *csd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -444,7 +446,7 @@ void smp_call_function_many(const struct cpumask *mask,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	cpumask_clear(cfd-&gt;cpumask_ipi);</span>
<span class="quote">&gt;  	for_each_cpu(cpu, cfd-&gt;cpumask) {</span>
<span class="quote">&gt; -		struct call_single_data *csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span>
<span class="quote">&gt; +		call_single_data_t *csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		csd_lock(csd);</span>
<span class="quote">&gt;  		if (wait)</span>
<span class="quote">&gt; @@ -460,7 +462,7 @@ void smp_call_function_many(const struct cpumask *mask,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (wait) {</span>
<span class="quote">&gt;  		for_each_cpu(cpu, cfd-&gt;cpumask) {</span>
<span class="quote">&gt; -			struct call_single_data *csd;</span>
<span class="quote">&gt; +			call_single_data_t *csd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span>
<span class="quote">&gt;  			csd_lock_wait(csd);</span>
<span class="quote">&gt; diff --git a/kernel/up.c b/kernel/up.c</span>
<span class="quote">&gt; index ee81ac9af4ca..42c46bf3e0a5 100644</span>
<span class="quote">&gt; --- a/kernel/up.c</span>
<span class="quote">&gt; +++ b/kernel/up.c</span>
<span class="quote">&gt; @@ -23,7 +23,7 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(smp_call_function_single);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -int smp_call_function_single_async(int cpu, struct call_single_data *csd)</span>
<span class="quote">&gt; +int smp_call_function_single_async(int cpu, call_single_data_t *csd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long flags;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - Aug. 28, 2017, 5:19 a.m.</div>
<pre class="content">
&quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; writes:
<span class="quote">
&gt; Hi, Peter,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Peter Zijlstra &lt;peterz@infradead.org&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Sat, Aug 05, 2017 at 08:47:02AM +0800, Huang, Ying wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Yes.  That looks good.  So you will prepare the final patch?  Or you</span>
<span class="quote">&gt;&gt;&gt;&gt; hope me to do that?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I was hoping you&#39;d do it ;-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Thanks!  Here is the updated patch</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Best Regards,</span>
<span class="quote">&gt;&gt; Huang, Ying</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ----------&gt;8----------</span>
<span class="quote">&gt;&gt; From 957735e9ff3922368286540dab852986fc7b23b5 Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; Date: Mon, 7 Aug 2017 16:55:33 +0800</span>
<span class="quote">&gt;&gt; Subject: [PATCH -v3] IPI: Avoid to use 2 cache lines for one</span>
<span class="quote">&gt;&gt;  call_single_data</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; struct call_single_data is used in IPI to transfer information between</span>
<span class="quote">&gt;&gt; CPUs.  Its size is bigger than sizeof(unsigned long) and less than</span>
<span class="quote">&gt;&gt; cache line size.  Now, it is allocated with no explicit alignment</span>
<span class="quote">&gt;&gt; requirement.  This makes it possible for allocated call_single_data to</span>
<span class="quote">&gt;&gt; cross 2 cache lines.  So that double the number of the cache lines</span>
<span class="quote">&gt;&gt; that need to be transferred among CPUs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is resolved by requiring call_single_data to be aligned with the</span>
<span class="quote">&gt;&gt; size of call_single_data.  Now the size of call_single_data is the</span>
<span class="quote">&gt;&gt; power of 2.  If we add new fields to call_single_data, we may need to</span>
<span class="quote">&gt;&gt; add pads to make sure the size of new definition is the power of 2.</span>
<span class="quote">&gt;&gt; Fortunately, this is enforced by gcc, which will report error for not</span>
<span class="quote">&gt;&gt; power of 2 alignment requirement.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To set alignment requirement of call_single_data to the size of</span>
<span class="quote">&gt;&gt; call_single_data, a struct definition and a typedef is used.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To test the effect of the patch, we use the vm-scalability multiple</span>
<span class="quote">&gt;&gt; thread swap test case (swap-w-seq-mt).  The test will create multiple</span>
<span class="quote">&gt;&gt; threads and each thread will eat memory until all RAM and part of swap</span>
<span class="quote">&gt;&gt; is used, so that huge number of IPI will be triggered when unmapping</span>
<span class="quote">&gt;&gt; memory.  In the test, the throughput of memory writing improves ~5%</span>
<span class="quote">&gt;&gt; compared with misaligned call_single_data because of faster IPI.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What do you think about this version?</span>
<span class="quote">&gt;</span>

Ping.

Best Regards,
Huang, Ying
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 28, 2017, 8:49 a.m.</div>
<pre class="content">
On Mon, Aug 28, 2017 at 01:19:21PM +0800, Huang, Ying wrote:
<span class="quote">&gt; &gt; What do you think about this version?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ping.</span>

Thanks, yes that got lost in the inbox :-(

I&#39;ll queue it, thanks !
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c</span>
<span class="p_header">index 770d4d1516cb..bd8ba5472bca 100644</span>
<span class="p_header">--- a/arch/mips/kernel/smp.c</span>
<span class="p_header">+++ b/arch/mips/kernel/smp.c</span>
<span class="p_chunk">@@ -648,12 +648,12 @@</span> <span class="p_context"> EXPORT_SYMBOL(flush_tlb_one);</span>
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 
 static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);
<span class="p_del">-static DEFINE_PER_CPU(struct call_single_data, tick_broadcast_csd);</span>
<span class="p_add">+static DEFINE_PER_CPU(call_single_data_t, tick_broadcast_csd);</span>
 
 void tick_broadcast(const struct cpumask *mask)
 {
 	atomic_t *count;
<span class="p_del">-	struct call_single_data *csd;</span>
<span class="p_add">+	call_single_data_t *csd;</span>
 	int cpu;
 
 	for_each_cpu(cpu, mask) {
<span class="p_chunk">@@ -674,7 +674,7 @@</span> <span class="p_context"> static void tick_broadcast_callee(void *info)</span>
 
 static int __init tick_broadcast_init(void)
 {
<span class="p_del">-	struct call_single_data *csd;</span>
<span class="p_add">+	call_single_data_t *csd;</span>
 	int cpu;
 
 	for (cpu = 0; cpu &lt; NR_CPUS; cpu++) {
<span class="p_header">diff --git a/block/blk-softirq.c b/block/blk-softirq.c</span>
<span class="p_header">index 87b7df4851bf..07125e7941f4 100644</span>
<span class="p_header">--- a/block/blk-softirq.c</span>
<span class="p_header">+++ b/block/blk-softirq.c</span>
<span class="p_chunk">@@ -60,7 +60,7 @@</span> <span class="p_context"> static void trigger_softirq(void *data)</span>
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
<span class="p_del">-		struct call_single_data *data = &amp;rq-&gt;csd;</span>
<span class="p_add">+		call_single_data_t *data = &amp;rq-&gt;csd;</span>
 
 		data-&gt;func = trigger_softirq;
 		data-&gt;info = rq;
<span class="p_header">diff --git a/drivers/block/null_blk.c b/drivers/block/null_blk.c</span>
<span class="p_header">index 85c24cace973..81142ce781da 100644</span>
<span class="p_header">--- a/drivers/block/null_blk.c</span>
<span class="p_header">+++ b/drivers/block/null_blk.c</span>
<span class="p_chunk">@@ -13,7 +13,7 @@</span> <span class="p_context"></span>
 struct nullb_cmd {
 	struct list_head list;
 	struct llist_node ll_list;
<span class="p_del">-	struct call_single_data csd;</span>
<span class="p_add">+	call_single_data_t csd;</span>
 	struct request *rq;
 	struct bio *bio;
 	unsigned int tag;
<span class="p_header">diff --git a/drivers/cpuidle/coupled.c b/drivers/cpuidle/coupled.c</span>
<span class="p_header">index 71e586d7df71..147f38ea0fcd 100644</span>
<span class="p_header">--- a/drivers/cpuidle/coupled.c</span>
<span class="p_header">+++ b/drivers/cpuidle/coupled.c</span>
<span class="p_chunk">@@ -119,13 +119,13 @@</span> <span class="p_context"> struct cpuidle_coupled {</span>
 
 #define CPUIDLE_COUPLED_NOT_IDLE	(-1)
 
<span class="p_del">-static DEFINE_PER_CPU(struct call_single_data, cpuidle_coupled_poke_cb);</span>
<span class="p_add">+static DEFINE_PER_CPU(call_single_data_t, cpuidle_coupled_poke_cb);</span>
 
 /*
  * The cpuidle_coupled_poke_pending mask is used to avoid calling
<span class="p_del">- * __smp_call_function_single with the per cpu call_single_data struct already</span>
<span class="p_add">+ * __smp_call_function_single with the per cpu call_single_data_t struct already</span>
  * in use.  This prevents a deadlock where two cpus are waiting for each others
<span class="p_del">- * call_single_data struct to be available</span>
<span class="p_add">+ * call_single_data_t struct to be available</span>
  */
 static cpumask_t cpuidle_coupled_poke_pending;
 
<span class="p_chunk">@@ -339,7 +339,7 @@</span> <span class="p_context"> static void cpuidle_coupled_handle_poke(void *info)</span>
  */
 static void cpuidle_coupled_poke(int cpu)
 {
<span class="p_del">-	struct call_single_data *csd = &amp;per_cpu(cpuidle_coupled_poke_cb, cpu);</span>
<span class="p_add">+	call_single_data_t *csd = &amp;per_cpu(cpuidle_coupled_poke_cb, cpu);</span>
 
 	if (!cpumask_test_and_set_cpu(cpu, &amp;cpuidle_coupled_poke_pending))
 		smp_call_function_single_async(cpu, csd);
<span class="p_chunk">@@ -651,7 +651,7 @@</span> <span class="p_context"> int cpuidle_coupled_register_device(struct cpuidle_device *dev)</span>
 {
 	int cpu;
 	struct cpuidle_device *other_dev;
<span class="p_del">-	struct call_single_data *csd;</span>
<span class="p_add">+	call_single_data_t *csd;</span>
 	struct cpuidle_coupled *coupled;
 
 	if (cpumask_empty(&amp;dev-&gt;coupled_cpus))
<span class="p_header">diff --git a/drivers/net/ethernet/cavium/liquidio/lio_main.c b/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="p_header">index 51583ae4b1eb..120b6e537b28 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/cavium/liquidio/lio_main.c</span>
<span class="p_chunk">@@ -2468,7 +2468,7 @@</span> <span class="p_context"> static void liquidio_napi_drv_callback(void *arg)</span>
 	if (OCTEON_CN23XX_PF(oct) || droq-&gt;cpu_id == this_cpu) {
 		napi_schedule_irqoff(&amp;droq-&gt;napi);
 	} else {
<span class="p_del">-		struct call_single_data *csd = &amp;droq-&gt;csd;</span>
<span class="p_add">+		call_single_data_t *csd = &amp;droq-&gt;csd;</span>
 
 		csd-&gt;func = napi_schedule_wrapper;
 		csd-&gt;info = &amp;droq-&gt;napi;
<span class="p_header">diff --git a/drivers/net/ethernet/cavium/liquidio/octeon_droq.h b/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="p_header">index 6efd139b894d..f91bc84d1719 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="p_header">+++ b/drivers/net/ethernet/cavium/liquidio/octeon_droq.h</span>
<span class="p_chunk">@@ -328,7 +328,7 @@</span> <span class="p_context"> struct octeon_droq {</span>
 
 	u32 cpu_id;
 
<span class="p_del">-	struct call_single_data csd;</span>
<span class="p_add">+	call_single_data_t csd;</span>
 };
 
 #define OCT_DROQ_SIZE   (sizeof(struct octeon_droq))
<span class="p_header">diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h</span>
<span class="p_header">index 25f6a0cb27d3..006fa09a641e 100644</span>
<span class="p_header">--- a/include/linux/blkdev.h</span>
<span class="p_header">+++ b/include/linux/blkdev.h</span>
<span class="p_chunk">@@ -134,7 +134,7 @@</span> <span class="p_context"> typedef __u32 __bitwise req_flags_t;</span>
 struct request {
 	struct list_head queuelist;
 	union {
<span class="p_del">-		struct call_single_data csd;</span>
<span class="p_add">+		call_single_data_t csd;</span>
 		u64 fifo_time;
 	};
 
<span class="p_header">diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h</span>
<span class="p_header">index 779b23595596..6557f320b66e 100644</span>
<span class="p_header">--- a/include/linux/netdevice.h</span>
<span class="p_header">+++ b/include/linux/netdevice.h</span>
<span class="p_chunk">@@ -2774,7 +2774,7 @@</span> <span class="p_context"> struct softnet_data {</span>
 	unsigned int		input_queue_head ____cacheline_aligned_in_smp;
 
 	/* Elements below can be accessed between CPUs for RPS/RFS */
<span class="p_del">-	struct call_single_data	csd ____cacheline_aligned_in_smp;</span>
<span class="p_add">+	call_single_data_t	csd ____cacheline_aligned_in_smp;</span>
 	struct softnet_data	*rps_ipi_next;
 	unsigned int		cpu;
 	unsigned int		input_queue_tail;
<span class="p_header">diff --git a/include/linux/smp.h b/include/linux/smp.h</span>
<span class="p_header">index 68123c1fe549..98b1fe027fc9 100644</span>
<span class="p_header">--- a/include/linux/smp.h</span>
<span class="p_header">+++ b/include/linux/smp.h</span>
<span class="p_chunk">@@ -14,13 +14,17 @@</span> <span class="p_context"></span>
 #include &lt;linux/llist.h&gt;
 
 typedef void (*smp_call_func_t)(void *info);
<span class="p_del">-struct call_single_data {</span>
<span class="p_add">+struct __call_single_data {</span>
 	struct llist_node llist;
 	smp_call_func_t func;
 	void *info;
 	unsigned int flags;
 };
 
<span class="p_add">+/* Use __aligned() to avoid to use 2 cache lines for 1 csd */</span>
<span class="p_add">+typedef struct __call_single_data call_single_data_t</span>
<span class="p_add">+	__aligned(sizeof(struct __call_single_data));</span>
<span class="p_add">+</span>
 /* total number of cpus in this system (may exceed NR_CPUS) */
 extern unsigned int total_cpus;
 
<span class="p_chunk">@@ -48,7 +52,7 @@</span> <span class="p_context"> void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
 		smp_call_func_t func, void *info, bool wait,
 		gfp_t gfp_flags);
 
<span class="p_del">-int smp_call_function_single_async(int cpu, struct call_single_data *csd);</span>
<span class="p_add">+int smp_call_function_single_async(int cpu, call_single_data_t *csd);</span>
 
 #ifdef CONFIG_SMP
 
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index eeef1a3086d1..f29a7d2b57e1 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -769,7 +769,7 @@</span> <span class="p_context"> struct rq {</span>
 #ifdef CONFIG_SCHED_HRTICK
 #ifdef CONFIG_SMP
 	int hrtick_csd_pending;
<span class="p_del">-	struct call_single_data hrtick_csd;</span>
<span class="p_add">+	call_single_data_t hrtick_csd;</span>
 #endif
 	struct hrtimer hrtick_timer;
 #endif
<span class="p_header">diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="p_header">index 3061483cb3ad..81cfca9b4cc3 100644</span>
<span class="p_header">--- a/kernel/smp.c</span>
<span class="p_header">+++ b/kernel/smp.c</span>
<span class="p_chunk">@@ -28,7 +28,7 @@</span> <span class="p_context"> enum {</span>
 };
 
 struct call_function_data {
<span class="p_del">-	struct call_single_data	__percpu *csd;</span>
<span class="p_add">+	call_single_data_t	__percpu *csd;</span>
 	cpumask_var_t		cpumask;
 	cpumask_var_t		cpumask_ipi;
 };
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> int smpcfd_prepare_cpu(unsigned int cpu)</span>
 		free_cpumask_var(cfd-&gt;cpumask);
 		return -ENOMEM;
 	}
<span class="p_del">-	cfd-&gt;csd = alloc_percpu(struct call_single_data);</span>
<span class="p_add">+	cfd-&gt;csd = alloc_percpu(call_single_data_t);</span>
 	if (!cfd-&gt;csd) {
 		free_cpumask_var(cfd-&gt;cpumask);
 		free_cpumask_var(cfd-&gt;cpumask_ipi);
<span class="p_chunk">@@ -103,12 +103,12 @@</span> <span class="p_context"> void __init call_function_init(void)</span>
  * previous function call. For multi-cpu calls its even more interesting
  * as we&#39;ll have to ensure no other cpu is observing our csd.
  */
<span class="p_del">-static __always_inline void csd_lock_wait(struct call_single_data *csd)</span>
<span class="p_add">+static __always_inline void csd_lock_wait(call_single_data_t *csd)</span>
 {
 	smp_cond_load_acquire(&amp;csd-&gt;flags, !(VAL &amp; CSD_FLAG_LOCK));
 }
 
<span class="p_del">-static __always_inline void csd_lock(struct call_single_data *csd)</span>
<span class="p_add">+static __always_inline void csd_lock(call_single_data_t *csd)</span>
 {
 	csd_lock_wait(csd);
 	csd-&gt;flags |= CSD_FLAG_LOCK;
<span class="p_chunk">@@ -116,12 +116,12 @@</span> <span class="p_context"> static __always_inline void csd_lock(struct call_single_data *csd)</span>
 	/*
 	 * prevent CPU from reordering the above assignment
 	 * to -&gt;flags with any subsequent assignments to other
<span class="p_del">-	 * fields of the specified call_single_data structure:</span>
<span class="p_add">+	 * fields of the specified call_single_data_t structure:</span>
 	 */
 	smp_wmb();
 }
 
<span class="p_del">-static __always_inline void csd_unlock(struct call_single_data *csd)</span>
<span class="p_add">+static __always_inline void csd_unlock(call_single_data_t *csd)</span>
 {
 	WARN_ON(!(csd-&gt;flags &amp; CSD_FLAG_LOCK));
 
<span class="p_chunk">@@ -131,14 +131,14 @@</span> <span class="p_context"> static __always_inline void csd_unlock(struct call_single_data *csd)</span>
 	smp_store_release(&amp;csd-&gt;flags, 0);
 }
 
<span class="p_del">-static DEFINE_PER_CPU_SHARED_ALIGNED(struct call_single_data, csd_data);</span>
<span class="p_add">+static DEFINE_PER_CPU_SHARED_ALIGNED(call_single_data_t, csd_data);</span>
 
 /*
<span class="p_del">- * Insert a previously allocated call_single_data element</span>
<span class="p_add">+ * Insert a previously allocated call_single_data_t element</span>
  * for execution on the given CPU. data must already have
  * -&gt;func, -&gt;info, and -&gt;flags set.
  */
<span class="p_del">-static int generic_exec_single(int cpu, struct call_single_data *csd,</span>
<span class="p_add">+static int generic_exec_single(int cpu, call_single_data_t *csd,</span>
 			       smp_call_func_t func, void *info)
 {
 	if (cpu == smp_processor_id()) {
<span class="p_chunk">@@ -210,7 +210,7 @@</span> <span class="p_context"> static void flush_smp_call_function_queue(bool warn_cpu_offline)</span>
 {
 	struct llist_head *head;
 	struct llist_node *entry;
<span class="p_del">-	struct call_single_data *csd, *csd_next;</span>
<span class="p_add">+	call_single_data_t *csd, *csd_next;</span>
 	static bool warned;
 
 	WARN_ON(!irqs_disabled());
<span class="p_chunk">@@ -268,8 +268,10 @@</span> <span class="p_context"> static void flush_smp_call_function_queue(bool warn_cpu_offline)</span>
 int smp_call_function_single(int cpu, smp_call_func_t func, void *info,
 			     int wait)
 {
<span class="p_del">-	struct call_single_data *csd;</span>
<span class="p_del">-	struct call_single_data csd_stack = { .flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS };</span>
<span class="p_add">+	call_single_data_t *csd;</span>
<span class="p_add">+	call_single_data_t csd_stack = {</span>
<span class="p_add">+		.flags = CSD_FLAG_LOCK | CSD_FLAG_SYNCHRONOUS,</span>
<span class="p_add">+	};</span>
 	int this_cpu;
 	int err;
 
<span class="p_chunk">@@ -321,7 +323,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(smp_call_function_single);</span>
  * NOTE: Be careful, there is unfortunately no current debugging facility to
  * validate the correctness of this serialization.
  */
<span class="p_del">-int smp_call_function_single_async(int cpu, struct call_single_data *csd)</span>
<span class="p_add">+int smp_call_function_single_async(int cpu, call_single_data_t *csd)</span>
 {
 	int err = 0;
 
<span class="p_chunk">@@ -444,7 +446,7 @@</span> <span class="p_context"> void smp_call_function_many(const struct cpumask *mask,</span>
 
 	cpumask_clear(cfd-&gt;cpumask_ipi);
 	for_each_cpu(cpu, cfd-&gt;cpumask) {
<span class="p_del">-		struct call_single_data *csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span>
<span class="p_add">+		call_single_data_t *csd = per_cpu_ptr(cfd-&gt;csd, cpu);</span>
 
 		csd_lock(csd);
 		if (wait)
<span class="p_chunk">@@ -460,7 +462,7 @@</span> <span class="p_context"> void smp_call_function_many(const struct cpumask *mask,</span>
 
 	if (wait) {
 		for_each_cpu(cpu, cfd-&gt;cpumask) {
<span class="p_del">-			struct call_single_data *csd;</span>
<span class="p_add">+			call_single_data_t *csd;</span>
 
 			csd = per_cpu_ptr(cfd-&gt;csd, cpu);
 			csd_lock_wait(csd);
<span class="p_header">diff --git a/kernel/up.c b/kernel/up.c</span>
<span class="p_header">index ee81ac9af4ca..42c46bf3e0a5 100644</span>
<span class="p_header">--- a/kernel/up.c</span>
<span class="p_header">+++ b/kernel/up.c</span>
<span class="p_chunk">@@ -23,7 +23,7 @@</span> <span class="p_context"> int smp_call_function_single(int cpu, void (*func) (void *info), void *info,</span>
 }
 EXPORT_SYMBOL(smp_call_function_single);
 
<span class="p_del">-int smp_call_function_single_async(int cpu, struct call_single_data *csd)</span>
<span class="p_add">+int smp_call_function_single_async(int cpu, call_single_data_t *csd)</span>
 {
 	unsigned long flags;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



