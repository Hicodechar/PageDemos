
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm: larger stack guard gap, between vmas - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm: larger stack guard gap, between vmas</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2017, 12:30 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170622123045.GA2694@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9804201/mbox/"
   >mbox</a>
|
   <a href="/patch/9804201/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9804201/">/patch/9804201/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D941B60234 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:31:34 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C320827DCD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:31:34 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B686327F17; Thu, 22 Jun 2017 12:31:34 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 323F228599
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 12:31:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752409AbdFVMbX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Jun 2017 08:31:23 -0400
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:56667 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1750852AbdFVMbU (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Jun 2017 08:31:20 -0400
Received: from ben by shadbolt.decadent.org.uk with local (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dO1G1-00041X-KH; Thu, 22 Jun 2017 13:30:47 +0100
Date: Thu, 22 Jun 2017 13:30:45 +0100
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: Hugh Dickins &lt;hughd@google.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Oleg Nesterov &lt;oleg@redhat.com&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	&quot;Jason A. Donenfeld&quot; &lt;Jason@zx2c4.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Tony Luck &lt;tony.luck@intel.com&gt;,
	&quot;James E.J. Bottomley&quot; &lt;jejb@parisc-linux.org&gt;,
	Helge Diller &lt;deller@gmx.de&gt;, James Hogan &lt;james.hogan@imgtec.com&gt;,
	Laura Abbott &lt;labbott@redhat.com&gt;, Willy Tarreau &lt;w@1wt.eu&gt;,
	Greg KH &lt;greg@kroah.com&gt;, security@kernel.org,
	linux-distros@vs.openwall.org, qsa@qualys.com,
	stable &lt;stable@vger.kernel.org&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;
Message-ID: &lt;20170622123045.GA2694@decadent.org.uk&gt;
References: &lt;alpine.LSU.2.11.1706190355140.2626@eggly.anvils&gt;
MIME-Version: 1.0
Content-Type: multipart/signed; micalg=pgp-sha512;
	protocol=&quot;application/pgp-signature&quot;; boundary=&quot;fdj2RfSjLxBAspz7&quot;
Content-Disposition: inline
In-Reply-To: &lt;alpine.LSU.2.11.1706190355140.2626@eggly.anvils&gt;
User-Agent: Mutt/1.5.23 (2014-03-12)
X-SA-Exim-Connect-IP: &lt;locally generated&gt;
X-SA-Exim-Mail-From: ben@decadent.org.uk
Subject: Re: [PATCH] mm: larger stack guard gap, between vmas
X-SA-Exim-Version: 4.2.1 (built Mon, 26 Dec 2011 16:24:06 +0000)
X-SA-Exim-Scanned: Yes (on shadbolt.decadent.org.uk)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 22, 2017, 12:30 p.m.</div>
<pre class="content">
Here&#39;s my attempt at a backport to 3.2.  This is only tested on
x86_64 and I think I should introduce local variables for
vma_start_gap() in a few places.  I had to cherry-pick commit
09884964335e &quot;mm: do not grow the stack vma just because of an overrun
on preceding vma&quot; before this one (which was a clean cherry-pick).

Ben.

---
<span class="from">From: Hugh Dickins &lt;hughd@google.com&gt;</span>
Date: Mon, 19 Jun 2017 20:32:47 +0200
Subject: mm: larger stack guard gap, between vmas

commit 1be7107fbe18eed3e319a6c3e83c78254b693acb upstream.

Stack guard page is a useful feature to reduce a risk of stack smashing
into a different mapping. We have been using a single page gap which
is sufficient to prevent having stack adjacent to a different mapping.
But this seems to be insufficient in the light of the stack usage in
userspace. E.g. glibc uses as large as 64kB alloca() in many commonly
used functions. Others use constructs liks gid_t buffer[NGROUPS_MAX]
which is 256kB or stack strings with MAX_ARG_STRLEN.

This will become especially dangerous for suid binaries and the default
no limit for the stack size limit because those applications can be
tricked to consume a large portion of the stack and a single glibc call
could jump over the guard page. These attacks are not theoretical,
unfortunatelly.

Make those attacks less probable by increasing the stack guard gap
to 1MB (on systems with 4k pages; but make it depend on the page size
because systems with larger base pages might cap stack allocations in
the PAGE_SIZE units) which should cover larger alloca() and VLA stack
allocations. It is obviously not a full fix because the problem is
somehow inherent, but it should reduce attack space a lot.

One could argue that the gap size should be configurable from userspace,
but that can be done later when somebody finds that the new 1MB is wrong
for some special case applications.  For now, add a kernel command line
option (stack_guard_gap) to specify the stack gap size (in page units).

Implementation wise, first delete all the old code for stack guard page:
because although we could get away with accounting one extra page in a
stack vma, accounting a larger gap can break userspace - case in point,
a program run with &quot;ulimit -S -v 20000&quot; failed when the 1MB gap was
counted for RLIMIT_AS; similar problems could come with RLIMIT_MLOCK
and strict non-overcommit mode.

Instead of keeping gap inside the stack vma, maintain the stack guard
gap as a gap between vmas: using vm_start_gap() in place of vm_start
(or vm_end_gap() in place of vm_end if VM_GROWSUP) in just those few
places which need to respect the gap - mainly arch_get_unmapped_area(),
and and the vma tree&#39;s subtree_gap support for that.

Original-patch-by: Oleg Nesterov &lt;oleg@redhat.com&gt;
Original-patch-by: Michal Hocko &lt;mhocko@suse.com&gt;
<span class="signed-off-by">Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
[wt: backport to 4.11: adjust context]
[wt: backport to 4.9: adjust context ; kernel doc was not in admin-guide]
[wt: backport to 4.4: adjust context ; drop ppc hugetlb_radix changes]
[wt: backport to 3.18: adjust context ; no FOLL_POPULATE ;
     s390 uses generic arch_get_unmapped_area()]
[wt: backport to 3.16: adjust context]
[wt: backport to 3.10: adjust context ; code logic in PARISC&#39;s
     arch_get_unmapped_area() wasn&#39;t found ; code inserted into
     expand_upwards() and expand_downwards() runs under anon_vma lock;
     changes for gup.c:faultin_page go to memory.c:__get_user_pages()]
<span class="signed-off-by">Signed-off-by: Willy Tarreau &lt;w@1wt.eu&gt;</span>
---
Some of these suggested adjustments below are just what comparing mine
and yours showed up, and I&#39;m being anal in passing them on e.g. I do
like your blank line in mm.h, but Michal chose to leave it out, and
I think that the closer we keep these sources to each other,
the less trouble we shall have patching on top in future.

Hugh.
---
[bwh: Backported to 3.2:
 - Drop changes for arc (doesn&#39;t exist here), xtensa (doesn&#39;t implement
   arch_get_unmapped_area() here)
 - There&#39;s no rb_subtree_gap, so patch the loop in each
   arch_get_unmapped_area{,_topdown}()
 - Adjust context]
<span class="signed-off-by">Signed-off-by: Ben Hutchings &lt;ben@decadent.org.uk&gt;</span>
---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 22, 2017, 12:46 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 01:30:45PM +0100, Ben Hutchings wrote:
<span class="quote">&gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>

Ben, I can&#39;t apply it on top of 3.2.89 + the patch above, do you have
any other patch in your local branch ? For example the patch tries to
modify a hunk starting at line 183 of arch/arm/mm/mmap.c while the one
I&#39;m having here ends at line 159.

Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 22, 2017, 12:58 p.m.</div>
<pre class="content">
On Thu, 2017-06-22 at 14:46 +0200, Willy Tarreau wrote:
<span class="quote">&gt; On Thu, Jun 22, 2017 at 01:30:45PM +0100, Ben Hutchings wrote:</span>
<span class="quote">&gt; &gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; &gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; &gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; &gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; &gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ben, I can&#39;t apply it on top of 3.2.89 + the patch above, do you have</span>
<span class="quote">&gt; any other patch in your local branch ? For example the patch tries to</span>
<span class="quote">&gt; modify a hunk starting at line 183 of arch/arm/mm/mmap.c while the one</span>
<span class="quote">&gt; I&#39;m having here ends at line 159.</span>

Sorry, yes, I did this on top of the Debian 3.2 branch and that *does*
have a patch to arch/arm/mm/mmap.c that I had forgotten about (commit
7dbaa466780a &quot;ARM: 7169/1: topdown mmap support&quot;).  I think you can
just drop the changes in ARM&#39;s arch_get_unmapped_area_topdown().

Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 22, 2017, 1:10 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 01:58:11PM +0100, Ben Hutchings wrote:
<span class="quote">&gt; On Thu, 2017-06-22 at 14:46 +0200, Willy Tarreau wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jun 22, 2017 at 01:30:45PM +0100, Ben Hutchings wrote:</span>
<span class="quote">&gt; &gt; &gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; &gt; &gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; &gt; &gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; &gt; &gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; &gt; &gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ben, I can&#39;t apply it on top of 3.2.89 + the patch above, do you have</span>
<span class="quote">&gt; &gt; any other patch in your local branch ? For example the patch tries to</span>
<span class="quote">&gt; &gt; modify a hunk starting at line 183 of arch/arm/mm/mmap.c while the one</span>
<span class="quote">&gt; &gt; I&#39;m having here ends at line 159.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, yes, I did this on top of the Debian 3.2 branch and that *does*</span>
<span class="quote">&gt; have a patch to arch/arm/mm/mmap.c that I had forgotten about (commit</span>
<span class="quote">&gt; 7dbaa466780a &quot;ARM: 7169/1: topdown mmap support&quot;).  I think you can</span>
<span class="quote">&gt; just drop the changes in ARM&#39;s arch_get_unmapped_area_topdown().</span>

Thanks, I&#39;ve just applied this one and it&#39;s building now. I&#39;ll run the
same checks I did for 3.10.

Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 22, 2017, 1:28 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 03:10:34PM +0200, Willy Tarreau wrote:
<span class="quote">&gt; On Thu, Jun 22, 2017 at 01:58:11PM +0100, Ben Hutchings wrote:</span>
<span class="quote">&gt; &gt; On Thu, 2017-06-22 at 14:46 +0200, Willy Tarreau wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Jun 22, 2017 at 01:30:45PM +0100, Ben Hutchings wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; &gt; &gt; &gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; &gt; &gt; &gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; &gt; &gt; &gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; &gt; &gt; &gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Ben, I can&#39;t apply it on top of 3.2.89 + the patch above, do you have</span>
<span class="quote">&gt; &gt; &gt; any other patch in your local branch ? For example the patch tries to</span>
<span class="quote">&gt; &gt; &gt; modify a hunk starting at line 183 of arch/arm/mm/mmap.c while the one</span>
<span class="quote">&gt; &gt; &gt; I&#39;m having here ends at line 159.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sorry, yes, I did this on top of the Debian 3.2 branch and that *does*</span>
<span class="quote">&gt; &gt; have a patch to arch/arm/mm/mmap.c that I had forgotten about (commit</span>
<span class="quote">&gt; &gt; 7dbaa466780a &quot;ARM: 7169/1: topdown mmap support&quot;).  I think you can</span>
<span class="quote">&gt; &gt; just drop the changes in ARM&#39;s arch_get_unmapped_area_topdown().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks, I&#39;ve just applied this one and it&#39;s building now. I&#39;ll run the</span>
<span class="quote">&gt; same checks I did for 3.10.</span>

So I tested this with gap.c on an i386 VM running 2G/2G split memory, all
went fine. It properly stopped the stack growth before colliding with anon
pages.

I noticed that you included Hugh&#39;s last fix in it (mm: fix new crash in
unmapped_area_topdown). You&#39;ll also need Helge&#39;s fix bd726c90b (&quot;Allow
stack to grow up to address space limit&quot;), which applies without issues
on top of your patch.

I would have happily tested on an ARM board but I don&#39;t seem to have
3.2-compatible ARM boards with 2G of RAM :-/

Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 22, 2017, 1:59 p.m.</div>
<pre class="content">
Hi,

On Thu, Jun 22, 2017 at 03:15:51PM +0200, Levente Polyak wrote:
<span class="quote">&gt; Just a side note, but i think its worth mentioning to also have look at</span>
<span class="quote">&gt; these fixup patches:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -- mm: fix new crash in unmapped_area_topdown()</span>
<span class="quote">&gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=f4cb767d76cf7ee72f97dd76f6cfa6c76a5edc89</span>

As I mentionned in another mail (this thread starts to be huge), this
one seems to have already been included in Ben&#39;s backport.
<span class="quote">
&gt; -- Allow stack to grow up to address space limit</span>
<span class="quote">&gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=bd726c90b6b8ce87602208701b208a208e6d5600</span>

This one cleanly applies after, but I think Ben is currently looking
for feedback on the validity of his backport which was a difficult task.

Regards,
Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 22, 2017, 2:14 p.m.</div>
<pre class="content">
On Thu, 2017-06-22 at 15:59 +0200, Willy Tarreau wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Jun 22, 2017 at 03:15:51PM +0200, Levente Polyak wrote:</span>
<span class="quote">&gt; &gt; Just a side note, but i think its worth mentioning to also have</span>
<span class="quote">&gt; &gt; look at</span>
<span class="quote">&gt; &gt; these fixup patches:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; -- mm: fix new crash in unmapped_area_topdown()</span>
<span class="quote">&gt; &gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/</span>
<span class="quote">&gt; &gt; commit/?id=f4cb767d76cf7ee72f97dd76f6cfa6c76a5edc89</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As I mentionned in another mail (this thread starts to be huge), this</span>
<span class="quote">&gt; one seems to have already been included in Ben&#39;s backport.</span>

That code was completely replaced due to the lack of an rbtree for gaps
in 3.2, so the fix was not needed.
<span class="quote">
&gt; &gt; -- Allow stack to grow up to address space limit</span>
<span class="quote">&gt; &gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/</span>
<span class="quote">&gt; &gt; commit/?id=bd726c90b6b8ce87602208701b208a208e6d5600</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This one cleanly applies after, but I think Ben is currently looking</span>
<span class="quote">&gt; for feedback on the validity of his backport which was a difficult</span>
<span class="quote">&gt; task.</span>

Right.

Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 22, 2017, 2:34 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 03:14:00PM +0100, Ben Hutchings wrote:
<span class="quote">&gt; On Thu, 2017-06-22 at 15:59 +0200, Willy Tarreau wrote:</span>
<span class="quote">&gt; &gt; but I think Ben is currently looking</span>
<span class="quote">&gt; &gt; for feedback on the validity of his backport which was a difficult</span>
<span class="quote">&gt; &gt; task.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right.</span>

Ben, barring more feedback, I think your should put your patch to your
stable queue so that Guenter can run his build+boot tests. They managed
to spot a few issues in my patches and that will make you more confident
regarding the whole architectures coverage.

Just my 2c,
Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=102">Helge Deller</a> - June 22, 2017, 9:23 p.m.</div>
<pre class="content">
On 22.06.2017 16:14, Ben Hutchings wrote:
<span class="quote">&gt; On Thu, 2017-06-22 at 15:59 +0200, Willy Tarreau wrote:</span>
<span class="quote">&gt;&gt;&gt; -- Allow stack to grow up to address space limit</span>
<span class="quote">&gt;&gt;&gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/</span>
<span class="quote">&gt;&gt;&gt; commit/?id=bd726c90b6b8ce87602208701b208a208e6d5600</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This one cleanly applies after, but I think Ben is currently looking</span>
<span class="quote">&gt;&gt; for feedback on the validity of his backport which was a difficult</span>
<span class="quote">&gt;&gt; task.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right.</span>

Ben,
I might be able to give it a try, but I&#39;m not sure if I&#39;m able to boot
that kernel on my parisc box. Kernel 3.9 was basically the first one
which was run basically stable on parisc, but since then we had
some ABI changes in userspace (reduced SIGRTMIN, made
EWOULDBLOCK == EAGAIN) which might prevent me to boot that
kernel with current glibc.
Anyway, can you point me to your patches ?

Helge
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 23, 2017, 3:10 a.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 7:34 AM, Willy Tarreau &lt;w@1wt.eu&gt; wrote:
<span class="quote">&gt; On Thu, Jun 22, 2017 at 03:14:00PM +0100, Ben Hutchings wrote:</span>
<span class="quote">&gt;&gt; On Thu, 2017-06-22 at 15:59 +0200, Willy Tarreau wrote:</span>
<span class="quote">&gt;&gt; &gt; but I think Ben is currently looking</span>
<span class="quote">&gt;&gt; &gt; for feedback on the validity of his backport which was a difficult</span>
<span class="quote">&gt;&gt; &gt; task.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Right.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ben, barring more feedback, I think your should put your patch to your</span>
<span class="quote">&gt; stable queue so that Guenter can run his build+boot tests. They managed</span>
<span class="quote">&gt; to spot a few issues in my patches and that will make you more confident</span>
<span class="quote">&gt; regarding the whole architectures coverage.</span>
<span class="quote">&gt;</span>

Has anyone checked how grsecurity deals with this?  I think they have
a large stack guard gap.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a> - June 23, 2017, 4:35 a.m.</div>
<pre class="content">
On Thu, 22 Jun 2017, Ben Hutchings wrote:
<span class="quote">
&gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>

Both your speed and your stamina are much better than mine; and your
patch belies your Sturgeon&#39;s law signature.  I haven&#39;t got beyond the
architectures yet in my parallel attempt, and you do appear to be
doing everything right (but a local variable often welcome, yes).

I&#39;m giving up for the night, will contine tomorrow.
The only discrepancy I notice so far is that I have
arch/alpha/kernel/osf_sys.c
arch/ia64/mm/hugetlbpage.c
arch/sparc/kernel/sys_sparc_32.c
in my list of changed files, but they&#39;re not in yours.

Hugh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - June 23, 2017, 4:42 a.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 8:10 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Has anyone checked how grsecurity deals with this?  I think they have</span>
<span class="quote">&gt; a large stack guard gap.</span>

Don&#39;t bother with grsecurity.

Their approach has always been &quot;we don&#39;t care if we break anything,
we&#39;ll just claim it&#39;s because we&#39;re extra secure&quot;.

The thing is a joke, and they are clowns. When they started talking
about people taking advantage of them, I stopped trying to be polite
about their bullshit.

Their patches are pure garbage.

                  Linus
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2463,6 +2463,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -101,7 +101,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -131,15 +131,15 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 		addr = vma-&gt;vm_end;
 		if (do_align)
 			addr = COLOUR_ALIGN(addr, pgoff);
<span class="p_chunk">@@ -183,7 +183,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -222,19 +222,19 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start - len;</span>
<span class="p_add">+		addr = vm_start_gap(vma) - len;</span>
 		if (do_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (len &lt; vma-&gt;vm_start);</span>
<span class="p_add">+	} while (len &lt; vm_start_gap(vma));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 			for (; vma; vma = vma-&gt;vm_next) {
 				if (addr &gt; limit)
 					break;
<span class="p_del">-				if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+				if (addr + len &lt;= vm_start_gap(vma))</span>
 					goto success;
 				addr = vma-&gt;vm_end;
 			}
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		for (; vma; vma = vma-&gt;vm_next) {
 			if (addr &gt; limit)
 				break;
<span class="p_del">-			if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (addr + len &lt;= vm_start_gap(vma))</span>
 				goto success;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_header">--- a/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_chunk">@@ -27,7 +27,7 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 	long map_shared = (flags &amp; MAP_SHARED);
 	unsigned long start_addr, align_mask = PAGE_SIZE - 1;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 
 	if (len &gt; RGN_MAP_LIMIT)
 		return -ENOMEM;
<span class="p_chunk">@@ -58,7 +58,7 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
   full_search:
 	start_addr = addr = (addr + align_mask) &amp; ~align_mask;
 
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; vma = vma-&gt;vm_next) {</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr || RGN_MAP_LIMIT - len &lt; REGION_OFFSET(addr)) {
 			if (start_addr != TASK_UNMAPPED_BASE) {
<span class="p_chunk">@@ -68,12 +68,13 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if ((!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev))) {</span>
 			/* Remember the address where we stopped this search:  */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		addr = (vma-&gt;vm_end + align_mask) &amp; ~align_mask;</span>
<span class="p_add">+		addr = (vm_end_gap(vma) + align_mask) &amp; ~align_mask;</span>
 	}
 }
 
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -103,7 +103,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -118,7 +118,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 			if (TASK_SIZE - len &lt; addr)
 				return -ENOMEM;
<span class="p_del">-			if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 				return addr;
 			addr = vma-&gt;vm_end;
 			if (do_color_align)
<span class="p_chunk">@@ -145,7 +145,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 		/* make sure it can fit in the remaining address space */
 		if (likely(addr &gt; len)) {
 			vma = find_vma(mm, addr - len);
<span class="p_del">-			if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+			if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr - len;
 			}
<span class="p_chunk">@@ -165,20 +165,20 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			 * return with success:
 			 */
 			vma = find_vma(mm, addr);
<span class="p_del">-			if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+			if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr;
 			}
 
 			/* remember the largest hole we saw so far */
<span class="p_del">-			if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-				mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+			if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+				mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 			/* try just below the current vma-&gt;vm_start */
 			addr = vma-&gt;vm_start - len;
 			if (do_color_align)
 				addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-		} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		} while (likely(len &lt; vm_start_gap(vma)));</span>
 
 bottomup:
 		/*
<span class="p_header">--- a/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_chunk">@@ -35,17 +35,18 @@</span> <span class="p_context"></span>
 
 static unsigned long get_unshared_area(unsigned long addr, unsigned long len)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 
 	addr = PAGE_ALIGN(addr);
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; vma = vma-&gt;vm_next) {</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if ((!vma || addr + len &lt;= vma-&gt;vm_start) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
<span class="p_add">+		addr = vm_end_gap(vma);</span>
 	}
 }
 
<span class="p_chunk">@@ -70,21 +71,22 @@</span> <span class="p_context"> static int get_offset(struct address_spa</span>
 static unsigned long get_shared_area(struct address_space *mapping,
 		unsigned long addr, unsigned long len, unsigned long pgoff)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	int offset = mapping ? get_offset(mapping) : 0;
 
 	offset = (offset + (pgoff &lt;&lt; PAGE_SHIFT)) &amp; 0x3FF000;
 
 	addr = DCACHE_ALIGN(addr - offset) + offset;
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; vma = vma-&gt;vm_next) {</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if ((!vma || addr + len &lt;= vma-&gt;vm_start) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
<span class="p_del">-		addr = DCACHE_ALIGN(vma-&gt;vm_end - offset) + offset;</span>
<span class="p_del">-		if (addr &lt; vma-&gt;vm_end) /* handle wraparound */</span>
<span class="p_add">+		addr = DCACHE_ALIGN(vm_end_gap(vma) - offset) + offset;</span>
<span class="p_add">+		if (addr &lt; vm_end_gap(vma)) /* handle wraparound */</span>
 			return -ENOMEM;
 	}
 }
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_chunk">@@ -256,7 +256,7 @@</span> <span class="p_context"> full_search:</span>
 				addr = _ALIGN_UP(addr + 1,  1ul &lt;&lt; SLICE_HIGH_SHIFT);
 			continue;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
<span class="p_chunk">@@ -264,8 +264,8 @@</span> <span class="p_context"> full_search:</span>
 				mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 		addr = vma-&gt;vm_end;
 	}
 
<span class="p_chunk">@@ -336,7 +336,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || (addr + len) &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || (addr + len) &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			if (use_cache)
 				mm-&gt;free_area_cache = addr;
<span class="p_chunk">@@ -344,11 +344,11 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start;</span>
<span class="p_add">+		addr = vm_start_gap(vma);</span>
 	}
 
 	/*
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -106,15 +106,15 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_colour_align)
<span class="p_chunk">@@ -158,7 +158,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -179,7 +179,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -199,20 +199,20 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start_gap(vma) - len;</span>
 		if (do_colour_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start_gap(vma)));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,15 +181,15 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_color_align)
<span class="p_chunk">@@ -237,7 +237,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -258,7 +258,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -278,20 +278,20 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+ 		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+ 		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start_gap(vma) - len;</span>
 		if (do_color_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start_gap(vma)));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -67,15 +67,15 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start_gap(vma))) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		addr = ALIGN(vma-&gt;vm_end, HPAGE_SIZE);
 	}
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -185,12 +185,12 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -236,7 +236,7 @@</span> <span class="p_context"> try_again:</span>
 		 * new region fits between prev_vma-&gt;vm_end and
 		 * vma-&gt;vm_start, use it:
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		if (addr + len &lt;= vm_start_gap(vma) &amp;&amp;</span>
 			    (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 			mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -251,13 +251,13 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start_gap(vma))</span>
<span class="p_add">+			largest_hole = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+		addr = (vm_start_gap(vma) - len) &amp; huge_page_mask(h);</span>
 
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	} while (len &lt;= vm_start_gap(vma));</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -312,7 +312,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (((flags &amp; MAP_32BIT) || test_thread_flag(TIF_IA32))
<span class="p_chunk">@@ -172,15 +172,15 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		addr = align_addr(addr, filp, 0);
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -232,7 +232,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 						    ALIGN_TOPDOWN);
 
 		vma = find_vma(mm, tmp_addr);
<span class="p_del">-		if (!vma || tmp_addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || tmp_addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = tmp_addr;
 	}
<span class="p_chunk">@@ -251,17 +251,17 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = addr;
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (len &lt; vma-&gt;vm_start);</span>
<span class="p_add">+		addr = vm_start_gap(vma) - len;</span>
<span class="p_add">+	} while (len &lt; vm_start_gap(vma));</span>
 
 bottomup:
 	/*
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -303,12 +303,12 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -351,7 +351,7 @@</span> <span class="p_context"> try_again:</span>
 		 * new region fits between prev_vma-&gt;vm_end and
 		 * vma-&gt;vm_start, use it:
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		if (addr + len &lt;= vm_start_gap(vma) &amp;&amp;</span>
 		            (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 		        mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -365,12 +365,12 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        largest_hole = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+		addr = (vm_start_gap(vma) - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+	} while (len &lt;= vm_start_gap(vma));</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -426,7 +426,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -176,7 +176,7 @@</span> <span class="p_context"> full_search:</span>
 			return -ENOMEM;
 		}
 
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -230,11 +230,7 @@</span> <span class="p_context"> static void show_map_vma(struct seq_file</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n&quot;,
 			start,
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1015,34 +1015,6 @@</span> <span class="p_context"> int set_page_dirty(struct page *page);</span>
 int set_page_dirty_lock(struct page *page);
 int clear_page_dirty_for_io(struct page *page);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len);
<span class="p_chunk">@@ -1462,6 +1434,7 @@</span> <span class="p_context"> unsigned long ra_submit(struct file_ra_s</span>
 			struct address_space *mapping,
 			struct file *filp);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1490,6 +1463,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * fi</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1605,12 +1605,6 @@</span> <span class="p_context"> no_page_table:</span>
 	return page;
 }
 
<span class="p_del">-static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return stack_guard_page_start(vma, addr) ||</span>
<span class="p_del">-	       stack_guard_page_end(vma, addr+PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * __get_user_pages() - pin user pages in memory
  * @tsk:	task_struct of target task
<span class="p_chunk">@@ -1761,11 +1755,6 @@</span> <span class="p_context"> int __get_user_pages(struct task_struct</span>
 				int ret;
 				unsigned int fault_flags = 0;
 
<span class="p_del">-				/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-				if (foll_flags &amp; FOLL_MLOCK) {</span>
<span class="p_del">-					if (stack_guard_page(vma, start))</span>
<span class="p_del">-						goto next_page;</span>
<span class="p_del">-				}</span>
 				if (foll_flags &amp; FOLL_WRITE)
 					fault_flags |= FAULT_FLAG_WRITE;
 				if (nonblocking)
<span class="p_chunk">@@ -3122,40 +3111,6 @@</span> <span class="p_context"> out_release:</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -3174,10 +3129,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_s</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -245,6 +245,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -289,7 +290,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -1368,7 +1370,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	unsigned long start_addr;
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
<span class="p_chunk">@@ -1379,9 +1381,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -1407,16 +1410,16 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
<span class="p_add">+		addr = vm_end_gap(vma);</span>
 	}
 }
 #endif	
<span class="p_chunk">@@ -1442,7 +1445,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	unsigned long low_limit = max(PAGE_SIZE, mmap_min_addr);
<span class="p_chunk">@@ -1457,9 +1460,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1475,7 +1479,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (addr &gt;= low_limit + len) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 	}
<span class="p_chunk">@@ -1492,17 +1496,17 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+ 		if (addr + mm-&gt;cached_hole_size &lt; vm_start_gap(vma))</span>
<span class="p_add">+ 		        mm-&gt;cached_hole_size = vm_start_gap(vma) - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (vma-&gt;vm_start &gt;= low_limit + len);</span>
<span class="p_add">+		addr = vm_start_gap(vma) - len;</span>
<span class="p_add">+	} while (vm_start_gap(vma) &gt;= low_limit + len);</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -1647,21 +1651,19 @@</span> <span class="p_context"> out:</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -1703,32 +1705,40 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_a</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	int error;</span>
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
<span class="p_add">+	int error = 0;</span>
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	/* Guard against wrapping around to address 0. */</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+	if (!address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &lt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
<span class="p_del">-	 * Also guard against wrapping around to address 0.</span>
 	 */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else {</span>
<span class="p_del">-		vma_unlock_anon_vma(vma);</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	error = 0;</span>
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &gt; vma-&gt;vm_end) {
<span class="p_chunk">@@ -1758,27 +1768,36 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	address &amp;= PAGE_MASK;
 	error = security_file_mmap(NULL, 0, 0, 0, address, 1);
 	if (error)
 		return error;
 
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &lt; vma-&gt;vm_start) {
<span class="p_chunk">@@ -1802,28 +1821,25 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_stru</span>
 	return error;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Note how expand_stack() refuses to expand the stack all the way to</span>
<span class="p_del">- * abut the next virtual mapping, *unless* that mapping itself is also</span>
<span class="p_del">- * a stack mapping. We want to leave room for a guard page, after all</span>
<span class="p_del">- * (the guard page itself is not added here, that is done by the</span>
<span class="p_del">- * actual page faulting logic)</span>
<span class="p_del">- *</span>
<span class="p_del">- * This matches the behavior of the guard page logic (see mm/memory.c:</span>
<span class="p_del">- * check_stack_guard_page()), which only allows the guard page to be</span>
<span class="p_del">- * removed under these circumstances.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *next;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	next = vma-&gt;vm_next;</span>
<span class="p_del">-	if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE) {</span>
<span class="p_del">-		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_upwards(vma, address);
 }
 
<span class="p_chunk">@@ -1846,14 +1862,6 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, un</span>
 #else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *prev;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	prev = vma-&gt;vm_prev;</span>
<span class="p_del">-	if (prev &amp;&amp; prev-&gt;vm_end == address) {</span>
<span class="p_del">-		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_downwards(vma, address);
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



