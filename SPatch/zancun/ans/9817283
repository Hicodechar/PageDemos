
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,04/10] x86/mm: Track the TLB&#39;s tlb_gen and update the flushing algorithm - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,04/10] x86/mm: Track the TLB&#39;s tlb_gen and update the flushing algorithm</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 29, 2017, 3:53 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1210fb244bc9cbe7677f7f0b72db4d359675f24b.1498751203.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9817283/mbox/"
   >mbox</a>
|
   <a href="/patch/9817283/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9817283/">/patch/9817283/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1373F60365 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 15:54:32 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 039D22860D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 15:54:32 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E98402869D; Thu, 29 Jun 2017 15:54:31 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EEAFB2860D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 15:54:30 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752833AbdF2PyW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 29 Jun 2017 11:54:22 -0400
Received: from mail.kernel.org ([198.145.29.99]:49120 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752339AbdF2Pxd (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 29 Jun 2017 11:53:33 -0400
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 6171422BDF;
	Thu, 29 Jun 2017 15:53:32 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 6171422BDF
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH v4 04/10] x86/mm: Track the TLB&#39;s tlb_gen and update the
	flushing algorithm
Date: Thu, 29 Jun 2017 08:53:16 -0700
Message-Id: &lt;1210fb244bc9cbe7677f7f0b72db4d359675f24b.1498751203.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.9.4
In-Reply-To: &lt;cover.1498751203.git.luto@kernel.org&gt;
References: &lt;cover.1498751203.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1498751203.git.luto@kernel.org&gt;
References: &lt;cover.1498751203.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 29, 2017, 3:53 p.m.</div>
<pre class="content">
There are two kernel features that would benefit from tracking
how up-to-date each CPU&#39;s TLB is in the case where IPIs aren&#39;t keeping
it up to date in real time:

 - Lazy mm switching currently works by switching to init_mm when
   it would otherwise flush.  This is wasteful: there isn&#39;t fundamentally
   any need to update CR3 at all when going lazy or when returning from
   lazy mode, nor is there any need to receive flush IPIs at all.  Instead,
   we should just stop trying to keep the TLB coherent when we go lazy and,
   when unlazying, check whether we missed any flushes.

 - PCID will let us keep recent user contexts alive in the TLB.  If we
   start doing this, we need a way to decide whether those contexts are
   up to date.

On some paravirt systems, remote TLBs can be flushed without IPIs.
This won&#39;t update the target CPUs&#39; tlb_gens, which may cause
unnecessary local flushes later on.  We can address this if it becomes
a problem by carefully updating the target CPU&#39;s tlb_gen directly.

By itself, this patch is a very minor optimization that avoids
unnecessary flushes when multiple TLB flushes targetting the same CPU
race.  The complexity in this patch would not be worth it on its own,
but it will enable improved lazy TLB tracking and PCID.
<span class="reviewed-by">
Reviewed-by: Nadav Amit &lt;nadav.amit@gmail.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/tlbflush.h |  43 +++++++++++++++--
 arch/x86/mm/tlb.c               | 102 +++++++++++++++++++++++++++++++++++++---
 2 files changed, 135 insertions(+), 10 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index ad2135385699..d7df54cc7e4d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -82,6 +82,11 @@</span> <span class="p_context"> static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
 #define __flush_tlb_single(addr) __native_flush_tlb_single(addr)
 #endif
 
<span class="p_add">+struct tlb_context {</span>
<span class="p_add">+	u64 ctx_id;</span>
<span class="p_add">+	u64 tlb_gen;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct tlb_state {
 	/*
 	 * cpu_tlbstate.loaded_mm should match CR3 whenever interrupts
<span class="p_chunk">@@ -97,6 +102,21 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 * disabling interrupts when modifying either one.
 	 */
 	unsigned long cr4;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is a list of all contexts that might exist in the TLB.</span>
<span class="p_add">+	 * Since we don&#39;t yet use PCID, there is only one context.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For each context, ctx_id indicates which mm the TLB&#39;s user</span>
<span class="p_add">+	 * entries came from.  As an invariant, the TLB will never</span>
<span class="p_add">+	 * contain entries that are out-of-date as when that mm reached</span>
<span class="p_add">+	 * the tlb_gen in the list.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * To be clear, this means that it&#39;s legal for the TLB code to</span>
<span class="p_add">+	 * flush the TLB without updating tlb_gen.  This can happen</span>
<span class="p_add">+	 * (for now, at least) due to paravirt remote flushes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct tlb_context ctxs[1];</span>
 };
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate);
 
<span class="p_chunk">@@ -248,9 +268,26 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
  * and page-granular flushes are available only on i486 and up.
  */
 struct flush_tlb_info {
<span class="p_del">-	struct mm_struct *mm;</span>
<span class="p_del">-	unsigned long start;</span>
<span class="p_del">-	unsigned long end;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We support several kinds of flushes.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * - Fully flush a single mm.  .mm will be set, .end will be</span>
<span class="p_add">+	 *   TLB_FLUSH_ALL, and .new_tlb_gen will be the tlb_gen to</span>
<span class="p_add">+	 *   which the IPI sender is trying to catch us up.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * - Partially flush a single mm.  .mm will be set, .start and</span>
<span class="p_add">+	 *   .end will indicate the range, and .new_tlb_gen will be set</span>
<span class="p_add">+	 *   such that the changes between generation .new_tlb_gen-1 and</span>
<span class="p_add">+	 *   .new_tlb_gen are entirely contained in the indicated range.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * - Fully flush all mms whose tlb_gens have been updated.  .mm</span>
<span class="p_add">+	 *   will be NULL, .end will be TLB_FLUSH_ALL, and .new_tlb_gen</span>
<span class="p_add">+	 *   will be zero.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+	unsigned long		start;</span>
<span class="p_add">+	unsigned long		end;</span>
<span class="p_add">+	u64			new_tlb_gen;</span>
 };
 
 #define local_flush_tlb() __flush_tlb()
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 14f4f8f66aa8..4e5a5ddb9e4d 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -105,6 +105,8 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	}
 
 	this_cpu_write(cpu_tlbstate.loaded_mm, next);
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
 
 	WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));
 	cpumask_set_cpu(cpu, mm_cpumask(next));
<span class="p_chunk">@@ -155,25 +157,102 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	switch_ldt(real_prev, next);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * flush_tlb_func_common()&#39;s memory ordering requirement is that any</span>
<span class="p_add">+ * TLB fills that happen after we flush the TLB are ordered after we</span>
<span class="p_add">+ * read active_mm&#39;s tlb_gen.  We don&#39;t need any explicit barriers</span>
<span class="p_add">+ * because all x86 flush operations are serializing and the</span>
<span class="p_add">+ * atomic64_read operation won&#39;t be reordered by the compiler.</span>
<span class="p_add">+ */</span>
 static void flush_tlb_func_common(const struct flush_tlb_info *f,
 				  bool local, enum tlb_flush_reason reason)
 {
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We have three different tlb_gen values in here.  They are:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * - mm_tlb_gen:     the latest generation.</span>
<span class="p_add">+	 * - local_tlb_gen:  the generation that this CPU has already caught</span>
<span class="p_add">+	 *                   up to.</span>
<span class="p_add">+	 * - f-&gt;new_tlb_gen: the generation that the requester of the flush</span>
<span class="p_add">+	 *                   wants us to catch up to.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);</span>
<span class="p_add">+	u64 mm_tlb_gen = atomic64_read(&amp;loaded_mm-&gt;context.tlb_gen);</span>
<span class="p_add">+	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen);</span>
<span class="p_add">+</span>
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
 
<span class="p_add">+	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="p_add">+		   loaded_mm-&gt;context.ctx_id);</span>
<span class="p_add">+</span>
 	if (this_cpu_read(cpu_tlbstate.state) != TLBSTATE_OK) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * leave_mm() is adequate to handle any type of flush, and</span>
<span class="p_add">+		 * we would prefer not to receive further IPIs.  leave_mm()</span>
<span class="p_add">+		 * clears this CPU&#39;s bit in mm_cpumask().</span>
<span class="p_add">+		 */</span>
 		leave_mm(smp_processor_id());
 		return;
 	}
 
<span class="p_del">-	if (f-&gt;end == TLB_FLUSH_ALL) {</span>
<span class="p_del">-		local_flush_tlb();</span>
<span class="p_del">-		if (local)</span>
<span class="p_del">-			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);</span>
<span class="p_del">-		trace_tlb_flush(reason, TLB_FLUSH_ALL);</span>
<span class="p_del">-	} else {</span>
<span class="p_add">+	if (unlikely(local_tlb_gen == mm_tlb_gen)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * There&#39;s nothing to do: we&#39;re already up to date.  This can</span>
<span class="p_add">+		 * happen if two concurrent flushes happen -- the first flush to</span>
<span class="p_add">+		 * be handled can catch us all the way up, leaving no work for</span>
<span class="p_add">+		 * the second flush.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON_ONCE(local_tlb_gen &gt; mm_tlb_gen);</span>
<span class="p_add">+	WARN_ON_ONCE(f-&gt;new_tlb_gen &gt; mm_tlb_gen);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we get to this point, we know that our TLB is out of date.</span>
<span class="p_add">+	 * This does not strictly imply that we need to flush (it&#39;s</span>
<span class="p_add">+	 * possible that f-&gt;new_tlb_gen &lt;= local_tlb_gen), but we&#39;re</span>
<span class="p_add">+	 * going to need to flush in the very near future, so we might</span>
<span class="p_add">+	 * as well get it over with.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The only question is whether to do a full or partial flush.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We do a partial flush if requested and two extra conditions</span>
<span class="p_add">+	 * are met:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 1. f-&gt;new_tlb_gen == local_tlb_gen + 1.  We have an invariant that</span>
<span class="p_add">+	 *    we&#39;ve always done all needed flushes to catch up to</span>
<span class="p_add">+	 *    local_tlb_gen.  If, for example, local_tlb_gen == 2 and</span>
<span class="p_add">+	 *    f-&gt;new_tlb_gen == 3, then we know that the flush needed to bring</span>
<span class="p_add">+	 *    us up to date for tlb_gen 3 is the partial flush we&#39;re</span>
<span class="p_add">+	 *    processing.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *    As an example of why this check is needed, suppose that there</span>
<span class="p_add">+	 *    are two concurrent flushes.  The first is a full flush that</span>
<span class="p_add">+	 *    changes context.tlb_gen from 1 to 2.  The second is a partial</span>
<span class="p_add">+	 *    flush that changes context.tlb_gen from 2 to 3.  If they get</span>
<span class="p_add">+	 *    processed on this CPU in reverse order, we&#39;ll see</span>
<span class="p_add">+	 *     local_tlb_gen == 1, mm_tlb_gen == 3, and end != TLB_FLUSH_ALL.</span>
<span class="p_add">+	 *    If we were to use __flush_tlb_single() and set local_tlb_gen to</span>
<span class="p_add">+	 *    3, we&#39;d be break the invariant: we&#39;d update local_tlb_gen above</span>
<span class="p_add">+	 *    1 without the full flush that&#39;s needed for tlb_gen 2.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 2. f-&gt;new_tlb_gen == mm_tlb_gen.  This is purely an optimiation.</span>
<span class="p_add">+	 *    Partial TLB flushes are not all that much cheaper than full TLB</span>
<span class="p_add">+	 *    flushes, so it seems unlikely that it would be a performance win</span>
<span class="p_add">+	 *    to do a partial flush if that won&#39;t bring our TLB fully up to</span>
<span class="p_add">+	 *    date.  By doing a full flush instead, we can increase</span>
<span class="p_add">+	 *    local_tlb_gen all the way to mm_tlb_gen and we can probably</span>
<span class="p_add">+	 *    avoid another flush in the very near future.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (f-&gt;end != TLB_FLUSH_ALL &amp;&amp;</span>
<span class="p_add">+	    f-&gt;new_tlb_gen == local_tlb_gen + 1 &amp;&amp;</span>
<span class="p_add">+	    f-&gt;new_tlb_gen == mm_tlb_gen) {</span>
<span class="p_add">+		/* Partial flush */</span>
 		unsigned long addr;
 		unsigned long nr_pages = (f-&gt;end - f-&gt;start) &gt;&gt; PAGE_SHIFT;
<span class="p_add">+</span>
 		addr = f-&gt;start;
 		while (addr &lt; f-&gt;end) {
 			__flush_tlb_single(addr);
<span class="p_chunk">@@ -182,7 +261,16 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 		if (local)
 			count_vm_tlb_events(NR_TLB_LOCAL_FLUSH_ONE, nr_pages);
 		trace_tlb_flush(reason, nr_pages);
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Full flush. */</span>
<span class="p_add">+		local_flush_tlb();</span>
<span class="p_add">+		if (local)</span>
<span class="p_add">+			count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);</span>
<span class="p_add">+		trace_tlb_flush(reason, TLB_FLUSH_ALL);</span>
 	}
<span class="p_add">+</span>
<span class="p_add">+	/* Both paths above update our state to mm_tlb_gen. */</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, mm_tlb_gen);</span>
 }
 
 static void flush_tlb_func_local(void *info, enum tlb_flush_reason reason)
<span class="p_chunk">@@ -253,7 +341,7 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 	cpu = get_cpu();
 
 	/* This is also a barrier that synchronizes with switch_mm(). */
<span class="p_del">-	inc_mm_tlb_gen(mm);</span>
<span class="p_add">+	info.new_tlb_gen = inc_mm_tlb_gen(mm);</span>
 
 	/* Should we flush just the requested range? */
 	if ((end != TLB_FLUSH_ALL) &amp;&amp;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



