
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/3] mm, hugetlb: unclutter hugetlb allocation layers - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/3] mm, hugetlb: unclutter hugetlb allocation layers</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2017, 7:30 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170622193034.28972-2-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9805241/mbox/"
   >mbox</a>
|
   <a href="/patch/9805241/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9805241/">/patch/9805241/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	ABE8C60329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:30:51 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9208928682
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:30:51 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 90046286C6; Thu, 22 Jun 2017 19:30:51 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 98C30286C6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:30:49 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753810AbdFVTas (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Jun 2017 15:30:48 -0400
Received: from mail-wr0-f194.google.com ([209.85.128.194]:33524 &quot;EHLO
	mail-wr0-f194.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753664AbdFVTaq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Jun 2017 15:30:46 -0400
Received: by mail-wr0-f194.google.com with SMTP id x23so7049235wrb.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 22 Jun 2017 12:30:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=PdsC+t3rdPKo3m2jH7mM1e/2+S+mcHSfDxyfKuSZirc=;
	b=p3vyOTd+ZdNoVjpFa3+wUPTEwPNby6kJvQdahYJH5qnHqKSov8WvKkqavIqiM1gaVn
	BsRIecrz0lKv8TlgGysFJNXqwh1D6vFA67FNcxy0TsITeDJ5hRNqZPQJkzEbD7FZ0vG/
	3wKDal3dHHy9+jowGHFClrQrnlCMizwq/ptm6HzDN4tuVuVcuEinpaEYDzeCSMM+t62G
	85IPkDIByae1lHGRuFfUARC3h4eaiFXbQsu7fExt/eVTXK/lMKVD5h5Q0DvdkLuWedNb
	nzN1dBo2+VDp/QSWnLPWz70RVaeP3kp8HG2pUsR1VVPfzWJTiH+/tu0Z3WqMFDL1Ghp4
	c2kw==
X-Gm-Message-State: AKS2vOy+EQIkCVXQK/+iQN/Y+jam4K4WGEcWcXFi8UHpChvQuXlhoF1h
	65d838fXXW5DT3F3
X-Received: by 10.28.8.72 with SMTP id 69mr2937554wmi.102.1498159844551;
	Thu, 22 Jun 2017 12:30:44 -0700 (PDT)
Received: from tiehlicka.suse.cz (ip-89-177-66-30.net.upcbroadband.cz.
	[89.177.66.30]) by smtp.gmail.com with ESMTPSA id
	h16sm2595033wma.14.2017.06.22.12.30.43
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 22 Jun 2017 12:30:44 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: &lt;linux-mm@kvack.org&gt;, Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 1/3] mm, hugetlb: unclutter hugetlb allocation layers
Date: Thu, 22 Jun 2017 21:30:32 +0200
Message-Id: &lt;20170622193034.28972-2-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170622193034.28972-1-mhocko@kernel.org&gt;
References: &lt;20170622193034.28972-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 22, 2017, 7:30 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Hugetlb allocation path for fresh huge pages is unnecessarily complex
and it mixes different interfaces between layers. __alloc_buddy_huge_page
is the central place to perform a new allocation. It checks for the
hugetlb overcommit and then relies on __hugetlb_alloc_buddy_huge_page to
invoke the page allocator. This is all good except that
__alloc_buddy_huge_page pushes vma and address down the callchain and
so __hugetlb_alloc_buddy_huge_page has to deal with two different
allocation modes - one for memory policy and other node specific (or to
make it more obscure node non-specific) requests. This just screams for a
reorganization.

This patch pulls out all the vma specific handling up to
__alloc_buddy_huge_page_with_mpol where it belongs.
__alloc_buddy_huge_page will get nodemask argument and
__hugetlb_alloc_buddy_huge_page will become a trivial wrapper over the
page allocator.

In short:
__alloc_buddy_huge_page_with_mpol - memory policy handling
  __alloc_buddy_huge_page - overcommit handling and accounting
    __hugetlb_alloc_buddy_huge_page - page allocator layer

Also note that __hugetlb_alloc_buddy_huge_page and its cpuset retry loop
is not really needed because the page allocator already handles the
cpusets update.

Finally __hugetlb_alloc_buddy_huge_page had a special case for node
specific allocations (when no policy is applied and there is a node
given). This has relied on __GFP_THISNODE to not fallback to a different
node. alloc_huge_page_node is the only caller which relies on this
behavior so move the __GFP_THISNODE there.

Not only this removes quite some code it also should make those layers
easier to follow and clear wrt responsibilities.

Changes since v1
- pulled gfp mask out of __hugetlb_alloc_buddy_huge_page and make it an
  explicit argument to allow __GFP_THISNODE in alloc_huge_page_node per
  Vlastimil
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |   2 +-
 mm/hugetlb.c            | 133 +++++++++++-------------------------------------
 2 files changed, 30 insertions(+), 105 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - June 26, 2017, 6:30 p.m.</div>
<pre class="content">
On 06/22/2017 12:30 PM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hugetlb allocation path for fresh huge pages is unnecessarily complex</span>
<span class="quote">&gt; and it mixes different interfaces between layers. __alloc_buddy_huge_page</span>
<span class="quote">&gt; is the central place to perform a new allocation. It checks for the</span>
<span class="quote">&gt; hugetlb overcommit and then relies on __hugetlb_alloc_buddy_huge_page to</span>
<span class="quote">&gt; invoke the page allocator. This is all good except that</span>
<span class="quote">&gt; __alloc_buddy_huge_page pushes vma and address down the callchain and</span>
<span class="quote">&gt; so __hugetlb_alloc_buddy_huge_page has to deal with two different</span>
<span class="quote">&gt; allocation modes - one for memory policy and other node specific (or to</span>
<span class="quote">&gt; make it more obscure node non-specific) requests. This just screams for a</span>
<span class="quote">&gt; reorganization.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch pulls out all the vma specific handling up to</span>
<span class="quote">&gt; __alloc_buddy_huge_page_with_mpol where it belongs.</span>
<span class="quote">&gt; __alloc_buddy_huge_page will get nodemask argument and</span>
<span class="quote">&gt; __hugetlb_alloc_buddy_huge_page will become a trivial wrapper over the</span>
<span class="quote">&gt; page allocator.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In short:</span>
<span class="quote">&gt; __alloc_buddy_huge_page_with_mpol - memory policy handling</span>
<span class="quote">&gt;   __alloc_buddy_huge_page - overcommit handling and accounting</span>
<span class="quote">&gt;     __hugetlb_alloc_buddy_huge_page - page allocator layer</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also note that __hugetlb_alloc_buddy_huge_page and its cpuset retry loop</span>
<span class="quote">&gt; is not really needed because the page allocator already handles the</span>
<span class="quote">&gt; cpusets update.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Finally __hugetlb_alloc_buddy_huge_page had a special case for node</span>
<span class="quote">&gt; specific allocations (when no policy is applied and there is a node</span>
<span class="quote">&gt; given). This has relied on __GFP_THISNODE to not fallback to a different</span>
<span class="quote">&gt; node. alloc_huge_page_node is the only caller which relies on this</span>
<span class="quote">&gt; behavior so move the __GFP_THISNODE there.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not only this removes quite some code it also should make those layers</span>
<span class="quote">&gt; easier to follow and clear wrt responsibilities.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changes since v1</span>
<span class="quote">&gt; - pulled gfp mask out of __hugetlb_alloc_buddy_huge_page and make it an</span>
<span class="quote">&gt;   explicit argument to allow __GFP_THISNODE in alloc_huge_page_node per</span>
<span class="quote">&gt;   Vlastimil</span>
<span class="quote">&gt; </span>
<span class="reviewed-by">
Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="tested-by">Tested-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 8fd0725d3f30..66b621469f52 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 907786581812..fd6e0c50f949 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1521,82 +1521,19 @@</span> <span class="p_context"> int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 	return rc;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * There are 3 ways this can get called:</span>
<span class="p_del">- * 1. With vma+addr: we use the VMA&#39;s memory policy</span>
<span class="p_del">- * 2. With !vma, but nid=NUMA_NO_NODE:  We try to allocate a huge</span>
<span class="p_del">- *    page from any node, and let the buddy allocator itself figure</span>
<span class="p_del">- *    it out.</span>
<span class="p_del">- * 3. With !vma, but nid!=NUMA_NO_NODE.  We allocate a huge page</span>
<span class="p_del">- *    strictly from &#39;nid&#39;</span>
<span class="p_del">- */</span>
 static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,
<span class="p_del">-		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="p_add">+		gfp_t gfp_mask, int nid, nodemask_t *nmask)</span>
 {
 	int order = huge_page_order(h);
<span class="p_del">-	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="p_del">-	unsigned int cpuset_mems_cookie;</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We need a VMA to get a memory policy.  If we do not</span>
<span class="p_del">-	 * have one, we use the &#39;nid&#39; argument.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The mempolicy stuff below has some non-inlined bits</span>
<span class="p_del">-	 * and calls -&gt;vm_ops.  That makes it hard to optimize at</span>
<span class="p_del">-	 * compile-time, even when NUMA is off and it does</span>
<span class="p_del">-	 * nothing.  This helps the compiler optimize it out.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!IS_ENABLED(CONFIG_NUMA) || !vma) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If a specific node is requested, make sure to</span>
<span class="p_del">-		 * get memory from there, but only when a node</span>
<span class="p_del">-		 * is explicitly specified.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (nid != NUMA_NO_NODE)</span>
<span class="p_del">-			gfp |= __GFP_THISNODE;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Make sure to call something that can handle</span>
<span class="p_del">-		 * nid=NUMA_NO_NODE</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		return alloc_pages_node(nid, gfp, order);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * OK, so we have a VMA.  Fetch the mempolicy and try to</span>
<span class="p_del">-	 * allocate a huge page with it.  We will only reach this</span>
<span class="p_del">-	 * when CONFIG_NUMA=y.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		struct page *page;</span>
<span class="p_del">-		struct mempolicy *mpol;</span>
<span class="p_del">-		int nid;</span>
<span class="p_del">-		nodemask_t *nodemask;</span>
<span class="p_del">-</span>
<span class="p_del">-		cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_del">-		nid = huge_node(vma, addr, gfp, &amp;mpol, &amp;nodemask);</span>
<span class="p_del">-		mpol_cond_put(mpol);</span>
<span class="p_del">-		page = __alloc_pages_nodemask(gfp, order, nid, nodemask);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return page;</span>
<span class="p_del">-	} while (read_mems_allowed_retry(cpuset_mems_cookie));</span>
<span class="p_del">-</span>
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	gfp_mask |= __GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="p_add">+	if (nid == NUMA_NO_NODE)</span>
<span class="p_add">+		nid = numa_mem_id();</span>
<span class="p_add">+	return __alloc_pages_nodemask(gfp_mask, order, nid, nmask);</span>
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * There are two ways to allocate a huge page:</span>
<span class="p_del">- * 1. When you have a VMA and an address (like a fault)</span>
<span class="p_del">- * 2. When you have no VMA (like when setting /proc/.../nr_hugepages)</span>
<span class="p_del">- *</span>
<span class="p_del">- * &#39;vma&#39; and &#39;addr&#39; are only for (1).  &#39;nid&#39; is always NUMA_NO_NODE in</span>
<span class="p_del">- * this case which signifies that the allocation should be done with</span>
<span class="p_del">- * respect for the VMA&#39;s memory policy.</span>
<span class="p_del">- *</span>
<span class="p_del">- * For (2), we ignore &#39;vma&#39; and &#39;addr&#39; and use &#39;nid&#39; exclusively. This</span>
<span class="p_del">- * implies that memory policies will not be taken in to account.</span>
<span class="p_del">- */</span>
<span class="p_del">-static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_del">-		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="p_add">+static struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+		int nid, nodemask_t *nmask)</span>
 {
 	struct page *page;
 	unsigned int r_nid;
<span class="p_chunk">@@ -1605,15 +1542,6 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 		return NULL;
 
 	/*
<span class="p_del">-	 * Make sure that anyone specifying &#39;nid&#39; is not also specifying a VMA.</span>
<span class="p_del">-	 * This makes sure the caller is picking _one_ of the modes with which</span>
<span class="p_del">-	 * we can call this function, not both.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (vma || (addr != -1)) {</span>
<span class="p_del">-		VM_WARN_ON_ONCE(addr == -1);</span>
<span class="p_del">-		VM_WARN_ON_ONCE(nid != NUMA_NO_NODE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	/*</span>
 	 * Assume we will successfully allocate the surplus page to
 	 * prevent racing processes from causing the surplus to exceed
 	 * overcommit
<span class="p_chunk">@@ -1646,7 +1574,7 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, vma, addr, nid);</span>
<span class="p_add">+	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (page) {
<span class="p_chunk">@@ -1671,26 +1599,23 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 }
 
 /*
<span class="p_del">- * Allocate a huge page from &#39;nid&#39;.  Note, &#39;nid&#39; may be</span>
<span class="p_del">- * NUMA_NO_NODE, which means that it may be allocated</span>
<span class="p_del">- * anywhere.</span>
<span class="p_del">- */</span>
<span class="p_del">-static</span>
<span class="p_del">-struct page *__alloc_buddy_huge_page_no_mpol(struct hstate *h, int nid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long addr = -1;</span>
<span class="p_del">-</span>
<span class="p_del">-	return __alloc_buddy_huge_page(h, NULL, addr, nid);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.
  */
 static
 struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,
 		struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct mempolicy *mpol;</span>
<span class="p_add">+	gfp_t gfp_mask = htlb_alloc_mask(h);</span>
<span class="p_add">+	int nid;</span>
<span class="p_add">+	nodemask_t *nodemask;</span>
<span class="p_add">+</span>
<span class="p_add">+	nid = huge_node(vma, addr, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="p_add">+	page = __alloc_buddy_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="p_add">+	mpol_cond_put(mpol);</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
 }
 
 /*
<span class="p_chunk">@@ -1700,21 +1625,26 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
  */
 struct page *alloc_huge_page_node(struct hstate *h, int nid)
 {
<span class="p_add">+	gfp_t gfp_mask = htlb_alloc_mask(h);</span>
 	struct page *page = NULL;
 
<span class="p_add">+	if (nid != NUMA_NO_NODE)</span>
<span class="p_add">+		gfp_mask |= __GFP_THISNODE;</span>
<span class="p_add">+</span>
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0)
 		page = dequeue_huge_page_node(h, nid);
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, nid);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page(h, gfp_mask, nid, NULL);</span>
 
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
 {
<span class="p_add">+	gfp_t gfp_mask = htlb_alloc_mask(h);</span>
 	struct page *page = NULL;
 	int node;
 
<span class="p_chunk">@@ -1731,13 +1661,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
 		return page;
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return page;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, gfp_mask, NUMA_NO_NODE, nmask);</span>
 }
 
 /*
<span class="p_chunk">@@ -1765,7 +1689,8 @@</span> <span class="p_context"> static int gather_surplus_pages(struct hstate *h, int delta)</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, NUMA_NO_NODE);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page(h, htlb_alloc_mask(h),</span>
<span class="p_add">+				NUMA_NO_NODE, NULL);</span>
 		if (!page) {
 			alloc_ok = false;
 			break;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



