
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,HMM,CDM,3/3] mm/migrate: memory migration using a device DMA engine - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,HMM,CDM,3/3] mm/migrate: memory migration using a device DMA engine</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 7, 2017, 8:28 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1491596933-21669-4-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9670375/mbox/"
   >mbox</a>
|
   <a href="/patch/9670375/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9670375/">/patch/9670375/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	88F3B60364 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 20:30:02 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7B45C2865D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 20:30:02 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6BCD928662; Fri,  7 Apr 2017 20:30:02 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DB5372865D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 20:30:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756535AbdDGU3x (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 7 Apr 2017 16:29:53 -0400
Received: from mx1.redhat.com ([209.132.183.28]:45552 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755275AbdDGU3O (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 7 Apr 2017 16:29:14 -0400
Received: from smtp.corp.redhat.com
	(int-mx05.intmail.prod.int.phx2.redhat.com [10.5.11.15])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 7E2D76DDED;
	Fri,  7 Apr 2017 20:29:13 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 7E2D76DDED
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 7E2D76DDED
Received: from localhost.localdomain.com (ovpn-124-100.rdu2.redhat.com
	[10.10.124.100])
	by smtp.corp.redhat.com (Postfix) with ESMTP id E09B28F7FF;
	Fri,  7 Apr 2017 20:29:11 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;,
	Balbir Singh &lt;balbir@au1.ibm.com&gt;,
	Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	&quot;Paul E . McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;,
	Srikar Dronamraju &lt;srikar@linux.vnet.ibm.com&gt;,
	Haren Myneni &lt;haren@linux.vnet.ibm.com&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
Subject: [RFC HMM CDM 3/3] mm/migrate: memory migration using a device DMA
	engine
Date: Fri,  7 Apr 2017 16:28:53 -0400
Message-Id: &lt;1491596933-21669-4-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1491596933-21669-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1491596933-21669-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.15
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.28]);
	Fri, 07 Apr 2017 20:29:13 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - April 7, 2017, 8:28 p.m.</div>
<pre class="content">
This reuse most of migrate_vma() infrastructure and generalize it
so that you can move any array of page using device DMA.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
---
 include/linux/hmm.h     |   7 +-
 include/linux/migrate.h |  40 +++---
 mm/hmm.c                |  16 +--
 mm/migrate.c            | 364 +++++++++++++++++++++++++-----------------------
 4 files changed, 219 insertions(+), 208 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=165331">Reza Arbab</a> - April 10, 2017, 6:11 p.m.</div>
<pre class="content">
(Had sent this to you directly. Reposting for the whole cc list.)

On Fri, Apr 07, 2017 at 04:28:53PM -0400, Jérôme Glisse wrote:
<span class="quote">&gt;--- a/include/linux/migrate.h</span>
<span class="quote">&gt;+++ b/include/linux/migrate.h </span>
<span class="quote">&gt;@@ -212,28 +215,25 @@ static inline unsigned long migrate_pfn(unsigned long pfn)</span>
<span class="quote">&gt;  * THE finalize_and_map() CALLBACK MUST NOT CHANGE ANY OF THE SRC OR DST ARRAY</span>
<span class="quote">&gt;  * ENTRIES OR BAD THINGS WILL HAPPEN !</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt;-struct migrate_vma_ops {</span>
<span class="quote">&gt;-	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="quote">&gt;-			       const unsigned long *src,</span>
<span class="quote">&gt;-			       unsigned long *dst,</span>
<span class="quote">&gt;-			       unsigned long start,</span>
<span class="quote">&gt;-			       unsigned long end,</span>
<span class="quote">&gt;-			       void *private);</span>
<span class="quote">&gt;-	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="quote">&gt;-				 const unsigned long *src,</span>
<span class="quote">&gt;-				 const unsigned long *dst,</span>
<span class="quote">&gt;-				 unsigned long start,</span>
<span class="quote">&gt;-				 unsigned long end,</span>
<span class="quote">&gt;-				 void *private);</span>
<span class="quote">&gt;+struct migrate_dma_ops {</span>
<span class="quote">&gt;+	void (*alloc_and_copy)(struct migrate_dma_ctx *ctx);</span>
<span class="quote">&gt;+	void (*finalize_and_map)(struct migrate_dma_ctx *ctx);</span>
<span class="quote">&gt;+};</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+struct migrate_dma_ctx {</span>
<span class="quote">&gt;+	const struct migrate_dma_ops	*ops;</span>
<span class="quote">&gt;+	unsigned long			*dst;</span>
<span class="quote">&gt;+	unsigned long			*src;</span>
<span class="quote">&gt;+	unsigned long			cpages;</span>
<span class="quote">&gt;+	unsigned long			npages;</span>

Could you add this so we can still pass arguments to the callbacks?

	void				*private;
<span class="quote">
&gt; };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;-int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;+int migrate_vma(struct migrate_dma_ctx *ctx,</span>
<span class="quote">&gt; 		struct vm_area_struct *vma,</span>
<span class="quote">&gt; 		unsigned long start,</span>
<span class="quote">&gt;-		unsigned long end,</span>
<span class="quote">&gt;-		unsigned long *src,</span>
<span class="quote">&gt;-		unsigned long *dst,</span>
<span class="quote">&gt;-		void *private);</span>
<span class="quote">&gt;+		unsigned long end);</span>
<span class="quote">&gt;+int migrate_dma(struct migrate_dma_ctx *migrate_ctx);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; #endif /* CONFIG_MIGRATION */</span>
<span class="quote">&gt;</span>

...%&lt;...
<span class="quote">
&gt;--- a/mm/migrate.c</span>
<span class="quote">&gt;+++ b/mm/migrate.c</span>
<span class="quote">&gt;@@ -2803,16 +2761,76 @@ int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt; 	 * Note that migration can fail in migrate_vma_struct_page() for each</span>
<span class="quote">&gt; 	 * individual page.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt;-	ops-&gt;alloc_and_copy(vma, src, dst, start, end, private);</span>
<span class="quote">&gt;+	migrate_ctx-&gt;ops-&gt;alloc_and_copy(migrate_ctx);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	/* This does the real migration of struct page */</span>
<span class="quote">&gt;-	migrate_vma_pages(&amp;migrate);</span>
<span class="quote">&gt;+	migrate_dma_pages(migrate_ctx, vma, start, end);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;-	ops-&gt;finalize_and_map(vma, src, dst, start, end, private);</span>
<span class="quote">&gt;+	migrate_ctx-&gt;ops-&gt;finalize_and_map(migrate_ctx);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	/* Unlock and remap pages */</span>
<span class="quote">&gt;-	migrate_vma_finalize(&amp;migrate);</span>
<span class="quote">&gt;+	migrate_dma_finalize(migrate_ctx);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	return 0;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; EXPORT_SYMBOL(migrate_vma);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+/*</span>
<span class="quote">&gt;+ * migrate_dma() - migrate an array of pages using a device DMA engine</span>
<span class="quote">&gt;+ *</span>
<span class="quote">&gt;+ * @migrate_ctx: migrate context structure</span>
<span class="quote">&gt;+ *</span>
<span class="quote">&gt;+ * The context structure must have its src fields pointing to an array of</span>
<span class="quote">&gt;+ * migrate pfn entry each corresponding to a valid page and each page being</span>
<span class="quote">&gt;+ * lock. The dst entry must by an array as big as src, it will be use during</span>
<span class="quote">&gt;+ * migration to store the destination pfn.</span>
<span class="quote">&gt;+ *</span>
<span class="quote">&gt;+ */</span>
<span class="quote">&gt;+int migrate_dma(struct migrate_dma_ctx *migrate_ctx)</span>
<span class="quote">&gt;+{</span>
<span class="quote">&gt;+	unsigned long i;</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* Sanity check the arguments */</span>
<span class="quote">&gt;+	if (!migrate_ctx-&gt;ops || !migrate_ctx-&gt;src || !migrate_ctx-&gt;dst)</span>
<span class="quote">&gt;+		return -EINVAL;</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* Below code should be hidden behind some DEBUG config */</span>
<span class="quote">&gt;+	for (i = 0; i &lt; migrate_ctx-&gt;npages; ++i) {</span>
<span class="quote">&gt;+		const unsigned long mask = MIGRATE_PFN_VALID |</span>
<span class="quote">&gt;+					   MIGRATE_PFN_LOCKED;</span>

This line is before the pages are locked. I think it should be

					   MIGRATE_PFN_MIGRATE;
<span class="quote">
&gt;+</span>
<span class="quote">&gt;+		if (!(migrate_ctx-&gt;src[i] &amp; mask))</span>
<span class="quote">&gt;+			return -EINVAL;</span>
<span class="quote">&gt;+	}</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* Lock and isolate page */</span>
<span class="quote">&gt;+	migrate_dma_prepare(migrate_ctx);</span>
<span class="quote">&gt;+	if (!migrate_ctx-&gt;cpages)</span>
<span class="quote">&gt;+		return 0;</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* Unmap pages */</span>
<span class="quote">&gt;+	migrate_dma_unmap(migrate_ctx);</span>
<span class="quote">&gt;+	if (!migrate_ctx-&gt;cpages)</span>
<span class="quote">&gt;+		return 0;</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/*</span>
<span class="quote">&gt;+	 * At this point pages are locked and unmapped, and thus they have</span>
<span class="quote">&gt;+	 * stable content and can safely be copied to destination memory that</span>
<span class="quote">&gt;+	 * is allocated by the callback.</span>
<span class="quote">&gt;+	 *</span>
<span class="quote">&gt;+	 * Note that migration can fail in migrate_vma_struct_page() for each</span>
<span class="quote">&gt;+	 * individual page.</span>
<span class="quote">&gt;+	 */</span>
<span class="quote">&gt;+	migrate_ctx-&gt;ops-&gt;alloc_and_copy(migrate_ctx);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* This does the real migration of struct page */</span>
<span class="quote">&gt;+	migrate_dma_pages(migrate_ctx, NULL, 0, 0);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	migrate_ctx-&gt;ops-&gt;finalize_and_map(migrate_ctx);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	/* Unlock and remap pages */</span>
<span class="quote">&gt;+	migrate_dma_finalize(migrate_ctx);</span>
<span class="quote">&gt;+</span>
<span class="quote">&gt;+	return 0;</span>
<span class="quote">&gt;+}</span>
<span class="quote">&gt;+EXPORT_SYMBOL(migrate_dma);</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="p_header">index e4fda18..eff17d3 100644</span>
<span class="p_header">--- a/include/linux/hmm.h</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -398,14 +398,11 @@</span> <span class="p_context"> struct hmm_devmem *hmm_devmem_add_resource(const struct hmm_devmem_ops *ops,</span>
 void hmm_devmem_remove(struct hmm_devmem *devmem);
 
 int hmm_devmem_fault_range(struct hmm_devmem *devmem,
<span class="p_add">+			   struct migrate_dma_ctx *migrate_ctx,</span>
 			   struct vm_area_struct *vma,
<span class="p_del">-			   const struct migrate_vma_ops *ops,</span>
<span class="p_del">-			   unsigned long *src,</span>
<span class="p_del">-			   unsigned long *dst,</span>
 			   unsigned long start,
 			   unsigned long addr,
<span class="p_del">-			   unsigned long end,</span>
<span class="p_del">-			   void *private);</span>
<span class="p_add">+			   unsigned long end);</span>
 
 /*
  * hmm_devmem_page_set_drvdata - set per-page driver data field
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index 7dd875a..fa7f53a 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -141,7 +141,8 @@</span> <span class="p_context"> static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 #define MIGRATE_PFN_WRITE	(1UL &lt;&lt; 3)
 #define MIGRATE_PFN_DEVICE	(1UL &lt;&lt; 4)
 #define MIGRATE_PFN_ERROR	(1UL &lt;&lt; 5)
<span class="p_del">-#define MIGRATE_PFN_SHIFT	6</span>
<span class="p_add">+#define MIGRATE_PFN_LRU		(1UL &lt;&lt; 6)</span>
<span class="p_add">+#define MIGRATE_PFN_SHIFT	7</span>
 
 static inline struct page *migrate_pfn_to_page(unsigned long mpfn)
 {
<span class="p_chunk">@@ -155,8 +156,10 @@</span> <span class="p_context"> static inline unsigned long migrate_pfn(unsigned long pfn)</span>
 	return (pfn &lt;&lt; MIGRATE_PFN_SHIFT) | MIGRATE_PFN_VALID;
 }
 
<span class="p_add">+struct migrate_dma_ctx;</span>
<span class="p_add">+</span>
 /*
<span class="p_del">- * struct migrate_vma_ops - migrate operation callback</span>
<span class="p_add">+ * struct migrate_dma_ops - migrate operation callback</span>
  *
  * @alloc_and_copy: alloc destination memory and copy source memory to it
  * @finalize_and_map: allow caller to map the successfully migrated pages
<span class="p_chunk">@@ -212,28 +215,25 @@</span> <span class="p_context"> static inline unsigned long migrate_pfn(unsigned long pfn)</span>
  * THE finalize_and_map() CALLBACK MUST NOT CHANGE ANY OF THE SRC OR DST ARRAY
  * ENTRIES OR BAD THINGS WILL HAPPEN !
  */
<span class="p_del">-struct migrate_vma_ops {</span>
<span class="p_del">-	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="p_del">-			       const unsigned long *src,</span>
<span class="p_del">-			       unsigned long *dst,</span>
<span class="p_del">-			       unsigned long start,</span>
<span class="p_del">-			       unsigned long end,</span>
<span class="p_del">-			       void *private);</span>
<span class="p_del">-	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="p_del">-				 const unsigned long *src,</span>
<span class="p_del">-				 const unsigned long *dst,</span>
<span class="p_del">-				 unsigned long start,</span>
<span class="p_del">-				 unsigned long end,</span>
<span class="p_del">-				 void *private);</span>
<span class="p_add">+struct migrate_dma_ops {</span>
<span class="p_add">+	void (*alloc_and_copy)(struct migrate_dma_ctx *ctx);</span>
<span class="p_add">+	void (*finalize_and_map)(struct migrate_dma_ctx *ctx);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct migrate_dma_ctx {</span>
<span class="p_add">+	const struct migrate_dma_ops	*ops;</span>
<span class="p_add">+	unsigned long			*dst;</span>
<span class="p_add">+	unsigned long			*src;</span>
<span class="p_add">+	unsigned long			cpages;</span>
<span class="p_add">+	unsigned long			npages;</span>
 };
 
<span class="p_del">-int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+int migrate_vma(struct migrate_dma_ctx *ctx,</span>
 		struct vm_area_struct *vma,
 		unsigned long start,
<span class="p_del">-		unsigned long end,</span>
<span class="p_del">-		unsigned long *src,</span>
<span class="p_del">-		unsigned long *dst,</span>
<span class="p_del">-		void *private);</span>
<span class="p_add">+		unsigned long end);</span>
<span class="p_add">+int migrate_dma(struct migrate_dma_ctx *migrate_ctx);</span>
<span class="p_add">+</span>
 
 #endif /* CONFIG_MIGRATION */
 
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="p_header">index 28c7fcb..c14aca5 100644</span>
<span class="p_header">--- a/mm/hmm.c</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -1131,14 +1131,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(hmm_devmem_remove);</span>
  * hmm_devmem_fault_range() - migrate back a virtual range of memory
  *
  * @devmem: hmm_devmem struct use to track and manage the ZONE_DEVICE memory
<span class="p_add">+ * @migrate_ctx: migrate context structure</span>
  * @vma: virtual memory area containing the range to be migrated
<span class="p_del">- * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_del">- * @src: array of unsigned long containing source pfns</span>
<span class="p_del">- * @dst: array of unsigned long containing destination pfns</span>
  * @start: start address of the range to migrate (inclusive)
  * @addr: fault address (must be inside the range)
  * @end: end address of the range to migrate (exclusive)
<span class="p_del">- * @private: pointer passed back to each of the callback</span>
  * Returns: 0 on success, VM_FAULT_SIGBUS on error
  *
  * This is a wrapper around migrate_vma() which checks the migration status
<span class="p_chunk">@@ -1149,16 +1146,15 @@</span> <span class="p_context"> EXPORT_SYMBOL(hmm_devmem_remove);</span>
  * This is a helper intendend to be used by the ZONE_DEVICE fault handler.
  */
 int hmm_devmem_fault_range(struct hmm_devmem *devmem,
<span class="p_add">+			   struct migrate_dma_ctx *migrate_ctx,</span>
 			   struct vm_area_struct *vma,
<span class="p_del">-			   const struct migrate_vma_ops *ops,</span>
<span class="p_del">-			   unsigned long *src,</span>
<span class="p_del">-			   unsigned long *dst,</span>
 			   unsigned long start,
 			   unsigned long addr,
<span class="p_del">-			   unsigned long end,</span>
<span class="p_del">-			   void *private)</span>
<span class="p_add">+			   unsigned long end)</span>
 {
<span class="p_del">-	if (migrate_vma(ops, vma, start, end, src, dst, private))</span>
<span class="p_add">+	unsigned long *dst = migrate_ctx-&gt;dst;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (migrate_vma(migrate_ctx, vma, start, end))</span>
 		return VM_FAULT_SIGBUS;
 
 	if (dst[(addr - start) &gt;&gt; PAGE_SHIFT] &amp; MIGRATE_PFN_ERROR)
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 2497357..5f252d6 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -2100,27 +2100,17 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 #endif /* CONFIG_NUMA */
 
 
<span class="p_del">-struct migrate_vma {</span>
<span class="p_del">-	struct vm_area_struct	*vma;</span>
<span class="p_del">-	unsigned long		*dst;</span>
<span class="p_del">-	unsigned long		*src;</span>
<span class="p_del">-	unsigned long		cpages;</span>
<span class="p_del">-	unsigned long		npages;</span>
<span class="p_del">-	unsigned long		start;</span>
<span class="p_del">-	unsigned long		end;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 static int migrate_vma_collect_hole(unsigned long start,
 				    unsigned long end,
 				    struct mm_walk *walk)
 {
<span class="p_del">-	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct migrate_dma_ctx *migrate_ctx = walk-&gt;private;</span>
 	unsigned long addr;
 
 	for (addr = start &amp; PAGE_MASK; addr &lt; end; addr += PAGE_SIZE) {
<span class="p_del">-		migrate-&gt;cpages++;</span>
<span class="p_del">-		migrate-&gt;dst[migrate-&gt;npages] = 0;</span>
<span class="p_del">-		migrate-&gt;src[migrate-&gt;npages++] = 0;</span>
<span class="p_add">+		migrate_ctx-&gt;cpages++;</span>
<span class="p_add">+		migrate_ctx-&gt;dst[migrate_ctx-&gt;npages] = 0;</span>
<span class="p_add">+		migrate_ctx-&gt;src[migrate_ctx-&gt;npages++] = 0;</span>
 	}
 
 	return 0;
<span class="p_chunk">@@ -2131,7 +2121,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 				   unsigned long end,
 				   struct mm_walk *walk)
 {
<span class="p_del">-	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct migrate_dma_ctx *migrate_ctx = walk-&gt;private;</span>
 	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;
 	unsigned long addr = start, unmapped = 0;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -2155,7 +2145,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		pfn = pte_pfn(pte);
 
 		if (pte_none(pte)) {
<span class="p_del">-			migrate-&gt;cpages++;</span>
<span class="p_add">+			migrate_ctx-&gt;cpages++;</span>
 			mpfn = pfn = 0;
 			goto next;
 		}
<span class="p_chunk">@@ -2178,7 +2168,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 			if (is_write_device_entry(entry))
 				mpfn |= MIGRATE_PFN_WRITE;
 		} else {
<span class="p_del">-			page = vm_normal_page(migrate-&gt;vma, addr, pte);</span>
<span class="p_add">+			page = vm_normal_page(walk-&gt;vma, addr, pte);</span>
 			mpfn = migrate_pfn(pfn) | MIGRATE_PFN_MIGRATE;
 			mpfn |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;
 		}
<span class="p_chunk">@@ -2200,7 +2190,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		 * can&#39;t be dropped from it).
 		 */
 		get_page(page);
<span class="p_del">-		migrate-&gt;cpages++;</span>
<span class="p_add">+		migrate_ctx-&gt;cpages++;</span>
 
 		/*
 		 * Optimize for the common case where page is only mapped once
<span class="p_chunk">@@ -2231,8 +2221,8 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 		}
 
 next:
<span class="p_del">-		migrate-&gt;dst[migrate-&gt;npages] = 0;</span>
<span class="p_del">-		migrate-&gt;src[migrate-&gt;npages++] = mpfn;</span>
<span class="p_add">+		migrate_ctx-&gt;dst[migrate_ctx-&gt;npages] = 0;</span>
<span class="p_add">+		migrate_ctx-&gt;src[migrate_ctx-&gt;npages++] = mpfn;</span>
 	}
 	arch_leave_lazy_mmu_mode();
 	pte_unmap_unlock(ptep - 1, ptl);
<span class="p_chunk">@@ -2252,7 +2242,10 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
  * valid page, it updates the src array and takes a reference on the page, in
  * order to pin the page until we lock it and unmap it.
  */
<span class="p_del">-static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="p_add">+static void migrate_vma_collect(struct migrate_dma_ctx *migrate_ctx,</span>
<span class="p_add">+				struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end)</span>
 {
 	struct mm_walk mm_walk;
 
<span class="p_chunk">@@ -2261,30 +2254,24 @@</span> <span class="p_context"> static void migrate_vma_collect(struct migrate_vma *migrate)</span>
 	mm_walk.pte_hole = migrate_vma_collect_hole;
 	mm_walk.hugetlb_entry = NULL;
 	mm_walk.test_walk = NULL;
<span class="p_del">-	mm_walk.vma = migrate-&gt;vma;</span>
<span class="p_del">-	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_del">-	mm_walk.private = migrate;</span>
<span class="p_del">-</span>
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="p_del">-					    migrate-&gt;start,</span>
<span class="p_del">-					    migrate-&gt;end);</span>
<span class="p_del">-	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="p_del">-					  migrate-&gt;start,</span>
<span class="p_del">-					  migrate-&gt;end);</span>
<span class="p_del">-</span>
<span class="p_del">-	migrate-&gt;end = migrate-&gt;start + (migrate-&gt;npages &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+	mm_walk.vma = vma;</span>
<span class="p_add">+	mm_walk.mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	mm_walk.private = migrate_ctx;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm_walk.mm, start, end);</span>
<span class="p_add">+	walk_page_range(start, end, &amp;mm_walk);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm_walk.mm, start, end);</span>
 }
 
 /*
<span class="p_del">- * migrate_vma_check_page() - check if page is pinned or not</span>
<span class="p_add">+ * migrate_dma_check_page() - check if page is pinned or not</span>
  * @page: struct page to check
  *
  * Pinned pages cannot be migrated. This is the same test as in
  * migrate_page_move_mapping(), except that here we allow migration of a
  * ZONE_DEVICE page.
  */
<span class="p_del">-static bool migrate_vma_check_page(struct page *page)</span>
<span class="p_add">+static bool migrate_dma_check_page(struct page *page)</span>
 {
 	/*
 	 * One extra ref because caller holds an extra reference, either from
<span class="p_chunk">@@ -2318,34 +2305,31 @@</span> <span class="p_context"> static bool migrate_vma_check_page(struct page *page)</span>
 }
 
 /*
<span class="p_del">- * migrate_vma_prepare() - lock pages and isolate them from the lru</span>
<span class="p_del">- * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ * migrate_dma_prepare() - lock pages and isolate them from the lru</span>
<span class="p_add">+ * @migrate_ctx: migrate struct containing all migration information</span>
  *
  * This locks pages that have been collected by migrate_vma_collect(). Once each
  * page is locked it is isolated from the lru (for non-device pages). Finally,
  * the ref taken by migrate_vma_collect() is dropped, as locked pages cannot be
  * migrated by concurrent kernel threads.
  */
<span class="p_del">-static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
<span class="p_add">+static unsigned long migrate_dma_prepare(struct migrate_dma_ctx *migrate_ctx)</span>
 {
<span class="p_del">-	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_del">-	const unsigned long start = migrate-&gt;start;</span>
<span class="p_del">-	unsigned long addr, i, restore = 0;</span>
<span class="p_add">+	const unsigned long npages = migrate_ctx-&gt;npages;</span>
<span class="p_add">+	unsigned long i, restore = 0;</span>
 	bool allow_drain = true;
 
 	lru_add_drain();
 
 	for (i = 0; i &lt; npages; i++) {
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_del">-		bool remap = true;</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate_ctx-&gt;src[i]);</span>
 
 		if (!page)
 			continue;
 
<span class="p_del">-		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_LOCKED)) {</span>
<span class="p_del">-			remap = false;</span>
<span class="p_add">+		if (!(migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_LOCKED)) {</span>
 			lock_page(page);
<span class="p_del">-			migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+			migrate_ctx-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
 		}
 
 		/* ZONE_DEVICE pages are not on LRU */
<span class="p_chunk">@@ -2357,64 +2341,34 @@</span> <span class="p_context"> static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
 			}
 
 			if (isolate_lru_page(page)) {
<span class="p_del">-				if (remap) {</span>
<span class="p_del">-					migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_del">-					migrate-&gt;cpages--;</span>
<span class="p_del">-					restore++;</span>
<span class="p_del">-				} else {</span>
<span class="p_del">-					migrate-&gt;src[i] = 0;</span>
<span class="p_del">-					unlock_page(page);</span>
<span class="p_del">-					migrate-&gt;cpages--;</span>
<span class="p_del">-					put_page(page);</span>
<span class="p_del">-				}</span>
<span class="p_add">+				migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate_ctx-&gt;cpages--;</span>
<span class="p_add">+				restore++;</span>
 				continue;
 			}
 
 			/* Drop the reference we took in collect */
<span class="p_add">+			migrate_ctx-&gt;src[i] |= MIGRATE_PFN_LRU;</span>
 			put_page(page);
 		}
 
<span class="p_del">-		if (!migrate_vma_check_page(page)) {</span>
<span class="p_del">-			if (remap) {</span>
<span class="p_del">-				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_del">-				migrate-&gt;cpages--;</span>
<span class="p_del">-				restore++;</span>
<span class="p_del">-</span>
<span class="p_del">-				if (!is_zone_device_page(page)) {</span>
<span class="p_del">-					get_page(page);</span>
<span class="p_del">-					putback_lru_page(page);</span>
<span class="p_del">-				}</span>
<span class="p_del">-			} else {</span>
<span class="p_del">-				migrate-&gt;src[i] = 0;</span>
<span class="p_del">-				unlock_page(page);</span>
<span class="p_del">-				migrate-&gt;cpages--;</span>
<span class="p_del">-</span>
<span class="p_del">-				if (!is_zone_device_page(page))</span>
<span class="p_del">-					putback_lru_page(page);</span>
<span class="p_del">-				else</span>
<span class="p_del">-					put_page(page);</span>
<span class="p_del">-			}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is not the final check, it is an early check to avoid</span>
<span class="p_add">+		 * unecessary work if the page is pined.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!migrate_dma_check_page(page)) {</span>
<span class="p_add">+			migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+			migrate_ctx-&gt;cpages--;</span>
<span class="p_add">+			restore++;</span>
 		}
 	}
 
<span class="p_del">-	for (i = 0, addr = start; i &lt; npages &amp;&amp; restore; i++, addr += PAGE_SIZE) {</span>
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		remove_migration_pte(page, migrate-&gt;vma, addr, page);</span>
<span class="p_del">-</span>
<span class="p_del">-		migrate-&gt;src[i] = 0;</span>
<span class="p_del">-		unlock_page(page);</span>
<span class="p_del">-		put_page(page);</span>
<span class="p_del">-		restore--;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	return restore;</span>
 }
 
 /*
<span class="p_del">- * migrate_vma_unmap() - replace page mapping with special migration pte entry</span>
<span class="p_del">- * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ * migrate_dma_unmap() - replace page mapping with special migration pte entry</span>
<span class="p_add">+ * @migrate_ctx: migrate struct containing migration context informations</span>
  *
  * Replace page mapping (CPU page table pte) with a special migration pte entry
  * and check again if it has been pinned. Pinned pages are restored because we
<span class="p_chunk">@@ -2423,17 +2377,16 @@</span> <span class="p_context"> static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
  * This is the last step before we call the device driver callback to allocate
  * destination memory and copy contents of original page over to new page.
  */
<span class="p_del">-static void migrate_vma_unmap(struct migrate_vma *migrate)</span>
<span class="p_add">+static unsigned long migrate_dma_unmap(struct migrate_dma_ctx *migrate_ctx)</span>
 {
 	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;
<span class="p_del">-	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_del">-	const unsigned long start = migrate-&gt;start;</span>
<span class="p_del">-	unsigned long addr, i, restore = 0;</span>
<span class="p_add">+	const unsigned long npages = migrate_ctx-&gt;npages;</span>
<span class="p_add">+	unsigned long i, restore = 0;</span>
 
 	for (i = 0; i &lt; npages; i++) {
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate_ctx-&gt;src[i]);</span>
 
<span class="p_del">-		if (!page || !(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+		if (!page || !(migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
 			continue;
 
 		if (page_mapped(page)) {
<span class="p_chunk">@@ -2442,41 +2395,24 @@</span> <span class="p_context"> static void migrate_vma_unmap(struct migrate_vma *migrate)</span>
 				goto restore;
 		}
 
<span class="p_del">-		if (migrate_vma_check_page(page))</span>
<span class="p_add">+		if (migrate_dma_check_page(page))</span>
 			continue;
 
 restore:
<span class="p_del">-		migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_del">-		migrate-&gt;cpages--;</span>
<span class="p_add">+		migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+		migrate_ctx-&gt;cpages--;</span>
 		restore++;
 	}
 
<span class="p_del">-	for (addr = start, i = 0; i &lt; npages &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		remove_migration_ptes(page, page, false);</span>
<span class="p_del">-</span>
<span class="p_del">-		migrate-&gt;src[i] = 0;</span>
<span class="p_del">-		unlock_page(page);</span>
<span class="p_del">-		restore--;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (is_zone_device_page(page))</span>
<span class="p_del">-			put_page(page);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			putback_lru_page(page);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	return restore;</span>
 }
 
<span class="p_del">-static void migrate_vma_insert_page(struct migrate_vma *migrate,</span>
<span class="p_add">+static void migrate_vma_insert_page(struct vm_area_struct *vma,</span>
 				    unsigned long addr,
 				    struct page *page,
 				    unsigned long *src,
 				    unsigned long *dst)
 {
<span class="p_del">-	struct vm_area_struct *vma = migrate-&gt;vma;</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct mem_cgroup *memcg;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -2579,33 +2515,35 @@</span> <span class="p_context"> static void migrate_vma_insert_page(struct migrate_vma *migrate,</span>
 }
 
 /*
<span class="p_del">- * migrate_vma_pages() - migrate meta-data from src page to dst page</span>
<span class="p_del">- * @migrate: migrate struct containing all migration information</span>
<span class="p_add">+ * migrate_dma_pages() - migrate meta-data from src page to dst page</span>
<span class="p_add">+ * @migrate_ctx: migrate struct containing migration context informations</span>
  *
  * This migrates struct page meta-data from source struct page to destination
  * struct page. This effectively finishes the migration from source page to the
  * destination page.
  */
<span class="p_del">-static void migrate_vma_pages(struct migrate_vma *migrate)</span>
<span class="p_add">+static void migrate_dma_pages(struct migrate_dma_ctx *migrate_ctx,</span>
<span class="p_add">+			      struct vm_area_struct *vma,</span>
<span class="p_add">+			      unsigned long start,</span>
<span class="p_add">+			      unsigned long end)</span>
 {
<span class="p_del">-	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_del">-	const unsigned long start = migrate-&gt;start;</span>
<span class="p_add">+	const unsigned long npages = migrate_ctx-&gt;npages;</span>
 	unsigned long addr, i;
 
<span class="p_del">-	for (i = 0, addr = start; i &lt; npages; addr += PAGE_SIZE, i++) {</span>
<span class="p_del">-		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+	for (i = 0, addr = start; i &lt; npages; i++, addr += PAGE_SIZE) {</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate_ctx-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate_ctx-&gt;src[i]);</span>
 		struct address_space *mapping;
 		int r;
 
 		if (!newpage) {
<span class="p_del">-			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+			migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
 			continue;
<span class="p_del">-		} else if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE)) {</span>
<span class="p_add">+		} else if (vma &amp;&amp; !(migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE)) {</span>
 			if (!page)
<span class="p_del">-				migrate_vma_insert_page(migrate, addr, newpage,</span>
<span class="p_del">-							&amp;migrate-&gt;src[i],</span>
<span class="p_del">-							&amp;migrate-&gt;dst[i]);</span>
<span class="p_add">+				migrate_vma_insert_page(vma, addr, newpage,</span>
<span class="p_add">+							&amp;migrate_ctx-&gt;src[i],</span>
<span class="p_add">+							&amp;migrate_ctx-&gt;dst[i]);</span>
 			continue;
 		}
 
<span class="p_chunk">@@ -2618,7 +2556,7 @@</span> <span class="p_context"> static void migrate_vma_pages(struct migrate_vma *migrate)</span>
 				 * migrating to un-addressable device memory.
 				 */
 				if (mapping) {
<span class="p_del">-					migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+					migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
 					continue;
 				}
 			} else if (is_device_cache_coherent_page(newpage)) {
<span class="p_chunk">@@ -2632,19 +2570,19 @@</span> <span class="p_context"> static void migrate_vma_pages(struct migrate_vma *migrate)</span>
 				 * Other types of ZONE_DEVICE page are not
 				 * supported.
 				 */
<span class="p_del">-				migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+				migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
 				continue;
 			}
 		}
 
 		r = migrate_page(mapping, newpage, page, MIGRATE_SYNC_NO_COPY);
 		if (r != MIGRATEPAGE_SUCCESS)
<span class="p_del">-			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+			migrate_ctx-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
 	}
 }
 
 /*
<span class="p_del">- * migrate_vma_finalize() - restore CPU page table entry</span>
<span class="p_add">+ * migrate_dma_finalize() - restore CPU page table entry</span>
  * @migrate: migrate struct containing all migration information
  *
  * This replaces the special migration pte entry with either a mapping to the
<span class="p_chunk">@@ -2654,14 +2592,14 @@</span> <span class="p_context"> static void migrate_vma_pages(struct migrate_vma *migrate)</span>
  * This also unlocks the pages and puts them back on the lru, or drops the extra
  * refcount, for device pages.
  */
<span class="p_del">-static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
<span class="p_add">+static void migrate_dma_finalize(struct migrate_dma_ctx *migrate_ctx)</span>
 {
<span class="p_del">-	const unsigned long npages = migrate-&gt;npages;</span>
<span class="p_add">+	const unsigned long npages = migrate_ctx-&gt;npages;</span>
 	unsigned long i;
 
 	for (i = 0; i &lt; npages; i++) {
<span class="p_del">-		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_del">-		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate_ctx-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate_ctx-&gt;src[i]);</span>
 
 		if (!page) {
 			if (newpage) {
<span class="p_chunk">@@ -2671,7 +2609,7 @@</span> <span class="p_context"> static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
 			continue;
 		}
 
<span class="p_del">-		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE) || !newpage) {</span>
<span class="p_add">+		if (!(migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE) || !newpage) {</span>
 			if (newpage) {
 				unlock_page(newpage);
 				put_page(newpage);
<span class="p_chunk">@@ -2681,7 +2619,6 @@</span> <span class="p_context"> static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
 
 		remove_migration_ptes(page, newpage, false);
 		unlock_page(page);
<span class="p_del">-		migrate-&gt;cpages--;</span>
 
 		if (is_zone_device_page(page))
 			put_page(page);
<span class="p_chunk">@@ -2698,16 +2635,42 @@</span> <span class="p_context"> static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
 	}
 }
 
<span class="p_add">+static void migrate_vma_restore(struct migrate_dma_ctx *migrate_ctx,</span>
<span class="p_add">+				struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long restore,</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = start, i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; i &lt; migrate_ctx-&gt;npages &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		bool lru = migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_LRU;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+		page = migrate_pfn_to_page(migrate_ctx-&gt;src[i]);</span>
<span class="p_add">+		if (!page || (migrate_ctx-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate_ctx-&gt;src[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!lru)</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * migrate_vma() - migrate a range of memory inside vma
  *
<span class="p_del">- * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_add">+ * @migrate_ctx: migrate context structure</span>
  * @vma: virtual memory area containing the range to be migrated
  * @start: start address of the range to migrate (inclusive)
  * @end: end address of the range to migrate (exclusive)
<span class="p_del">- * @src: array of hmm_pfn_t containing source pfns</span>
<span class="p_del">- * @dst: array of hmm_pfn_t containing destination pfns</span>
<span class="p_del">- * @private: pointer passed back to each of the callback</span>
  * Returns: 0 on success, error code otherwise
  *
  * This function tries to migrate a range of memory virtual address range, using
<span class="p_chunk">@@ -2749,50 +2712,45 @@</span> <span class="p_context"> static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
  * Both src and dst array must be big enough for (end - start) &gt;&gt; PAGE_SHIFT
  * unsigned long entries.
  */
<span class="p_del">-int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+int migrate_vma(struct migrate_dma_ctx *migrate_ctx,</span>
 		struct vm_area_struct *vma,
 		unsigned long start,
<span class="p_del">-		unsigned long end,</span>
<span class="p_del">-		unsigned long *src,</span>
<span class="p_del">-		unsigned long *dst,</span>
<span class="p_del">-		void *private)</span>
<span class="p_add">+		unsigned long end)</span>
 {
<span class="p_del">-	struct migrate_vma migrate;</span>
<span class="p_add">+	unsigned long npages, restore;</span>
 
 	/* Sanity check the arguments */
 	start &amp;= PAGE_MASK;
 	end &amp;= PAGE_MASK;
 	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))
 		return -EINVAL;
<span class="p_del">-	if (!vma || !ops || !src || !dst || start &gt;= end)</span>
<span class="p_add">+	if (!vma || !migrate_ctx || !migrate_ctx-&gt;src || !migrate_ctx-&gt;dst)</span>
 		return -EINVAL;
<span class="p_del">-	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+	if (start &gt;= end || start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
 		return -EINVAL;
 	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)
 		return -EINVAL;
 
<span class="p_del">-	memset(src, 0, sizeof(*src) * ((end - start) &gt;&gt; PAGE_SHIFT));</span>
<span class="p_del">-	migrate.src = src;</span>
<span class="p_del">-	migrate.dst = dst;</span>
<span class="p_del">-	migrate.start = start;</span>
<span class="p_del">-	migrate.npages = 0;</span>
<span class="p_del">-	migrate.cpages = 0;</span>
<span class="p_del">-	migrate.end = end;</span>
<span class="p_del">-	migrate.vma = vma;</span>
<span class="p_add">+	migrate_ctx-&gt;npages = 0;</span>
<span class="p_add">+	migrate_ctx-&gt;cpages = 0;</span>
<span class="p_add">+	npages = (end - start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	memset(migrate_ctx-&gt;src, 0, sizeof(*migrate_ctx-&gt;src) * npages);</span>
 
 	/* Collect, and try to unmap source pages */
<span class="p_del">-	migrate_vma_collect(&amp;migrate);</span>
<span class="p_del">-	if (!migrate.cpages)</span>
<span class="p_add">+	migrate_vma_collect(migrate_ctx, vma, start, end);</span>
<span class="p_add">+	if (!migrate_ctx-&gt;cpages)</span>
 		return 0;
 
 	/* Lock and isolate page */
<span class="p_del">-	migrate_vma_prepare(&amp;migrate);</span>
<span class="p_del">-	if (!migrate.cpages)</span>
<span class="p_add">+	restore = migrate_dma_prepare(migrate_ctx);</span>
<span class="p_add">+	migrate_vma_restore(migrate_ctx, vma, restore, start, end);</span>
<span class="p_add">+	if (!migrate_ctx-&gt;cpages)</span>
 		return 0;
 
 	/* Unmap pages */
<span class="p_del">-	migrate_vma_unmap(&amp;migrate);</span>
<span class="p_del">-	if (!migrate.cpages)</span>
<span class="p_add">+	restore = migrate_dma_unmap(migrate_ctx);</span>
<span class="p_add">+	migrate_vma_restore(migrate_ctx, vma, restore, start, end);</span>
<span class="p_add">+	if (!migrate_ctx-&gt;cpages)</span>
 		return 0;
 
 	/*
<span class="p_chunk">@@ -2803,16 +2761,76 @@</span> <span class="p_context"> int migrate_vma(const struct migrate_vma_ops *ops,</span>
 	 * Note that migration can fail in migrate_vma_struct_page() for each
 	 * individual page.
 	 */
<span class="p_del">-	ops-&gt;alloc_and_copy(vma, src, dst, start, end, private);</span>
<span class="p_add">+	migrate_ctx-&gt;ops-&gt;alloc_and_copy(migrate_ctx);</span>
 
 	/* This does the real migration of struct page */
<span class="p_del">-	migrate_vma_pages(&amp;migrate);</span>
<span class="p_add">+	migrate_dma_pages(migrate_ctx, vma, start, end);</span>
 
<span class="p_del">-	ops-&gt;finalize_and_map(vma, src, dst, start, end, private);</span>
<span class="p_add">+	migrate_ctx-&gt;ops-&gt;finalize_and_map(migrate_ctx);</span>
 
 	/* Unlock and remap pages */
<span class="p_del">-	migrate_vma_finalize(&amp;migrate);</span>
<span class="p_add">+	migrate_dma_finalize(migrate_ctx);</span>
 
 	return 0;
 }
 EXPORT_SYMBOL(migrate_vma);
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_dma() - migrate an array of pages using a device DMA engine</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @migrate_ctx: migrate context structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The context structure must have its src fields pointing to an array of</span>
<span class="p_add">+ * migrate pfn entry each corresponding to a valid page and each page being</span>
<span class="p_add">+ * lock. The dst entry must by an array as big as src, it will be use during</span>
<span class="p_add">+ * migration to store the destination pfn.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+int migrate_dma(struct migrate_dma_ctx *migrate_ctx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long i;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity check the arguments */</span>
<span class="p_add">+	if (!migrate_ctx-&gt;ops || !migrate_ctx-&gt;src || !migrate_ctx-&gt;dst)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Below code should be hidden behind some DEBUG config */</span>
<span class="p_add">+	for (i = 0; i &lt; migrate_ctx-&gt;npages; ++i) {</span>
<span class="p_add">+		const unsigned long mask = MIGRATE_PFN_VALID |</span>
<span class="p_add">+					   MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!(migrate_ctx-&gt;src[i] &amp; mask))</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Lock and isolate page */</span>
<span class="p_add">+	migrate_dma_prepare(migrate_ctx);</span>
<span class="p_add">+	if (!migrate_ctx-&gt;cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unmap pages */</span>
<span class="p_add">+	migrate_dma_unmap(migrate_ctx);</span>
<span class="p_add">+	if (!migrate_ctx-&gt;cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point pages are locked and unmapped, and thus they have</span>
<span class="p_add">+	 * stable content and can safely be copied to destination memory that</span>
<span class="p_add">+	 * is allocated by the callback.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that migration can fail in migrate_vma_struct_page() for each</span>
<span class="p_add">+	 * individual page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	migrate_ctx-&gt;ops-&gt;alloc_and_copy(migrate_ctx);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This does the real migration of struct page */</span>
<span class="p_add">+	migrate_dma_pages(migrate_ctx, NULL, 0, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	migrate_ctx-&gt;ops-&gt;finalize_and_map(migrate_ctx);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unlock and remap pages */</span>
<span class="p_add">+	migrate_dma_finalize(migrate_ctx);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(migrate_dma);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



