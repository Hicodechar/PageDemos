
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv3,5/5] mm: use &#39;unsigned int&#39; for page order - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv3,5/5] mm: use &#39;unsigned int&#39; for page order</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 19, 2015, 9:21 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1439976106-137226-6-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7036521/mbox/"
   >mbox</a>
|
   <a href="/patch/7036521/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7036521/">/patch/7036521/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 57BFE9F344
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Aug 2015 09:22:33 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 39DE9204CF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Aug 2015 09:22:32 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id E5842205C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Aug 2015 09:22:30 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753541AbbHSJWV (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 19 Aug 2015 05:22:21 -0400
Received: from mga02.intel.com ([134.134.136.20]:58846 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753195AbbHSJWB (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 19 Aug 2015 05:22:01 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga101.jf.intel.com with ESMTP; 19 Aug 2015 02:22:02 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.15,709,1432623600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;771620021&quot;
Received: from black.fi.intel.com ([10.237.72.157])
	by fmsmga001.fm.intel.com with ESMTP; 19 Aug 2015 02:21:53 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 47C46251; Wed, 19 Aug 2015 12:21:49 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv3 5/5] mm: use &#39;unsigned int&#39; for page order
Date: Wed, 19 Aug 2015 12:21:46 +0300
Message-Id: &lt;1439976106-137226-6-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.5.0
In-Reply-To: &lt;1439976106-137226-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1439976106-137226-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Aug. 19, 2015, 9:21 a.m.</div>
<pre class="content">
Let&#39;s try to be consistent about data type of page order.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 include/linux/mm.h |  5 +++--
 mm/hugetlb.c       | 19 ++++++++++---------
 mm/internal.h      |  4 ++--
 mm/page_alloc.c    | 27 +++++++++++++++------------
 4 files changed, 30 insertions(+), 25 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 20, 2015, 8:32 a.m.</div>
<pre class="content">
On Wed 19-08-15 12:21:46, Kirill A. Shutemov wrote:
<span class="quote">&gt; Let&#39;s try to be consistent about data type of page order.</span>

Looks good to me.

We still have *_control::order but that is not directly related to this
patch series.
<span class="quote">
&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Thanks!
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  include/linux/mm.h |  5 +++--</span>
<span class="quote">&gt;  mm/hugetlb.c       | 19 ++++++++++---------</span>
<span class="quote">&gt;  mm/internal.h      |  4 ++--</span>
<span class="quote">&gt;  mm/page_alloc.c    | 27 +++++++++++++++------------</span>
<span class="quote">&gt;  4 files changed, 30 insertions(+), 25 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index a4c4b7d07473..a75bbb3f7142 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -557,7 +557,7 @@ static inline compound_page_dtor *get_compound_page_dtor(struct page *page)</span>
<span class="quote">&gt;  	return compound_page_dtors[page[1].compound_dtor];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline int compound_order(struct page *page)</span>
<span class="quote">&gt; +static inline unsigned int compound_order(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!PageHead(page))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -1718,7 +1718,8 @@ extern void si_meminfo(struct sysinfo * val);</span>
<span class="quote">&gt;  extern void si_meminfo_node(struct sysinfo *val, int nid);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern __printf(3, 4)</span>
<span class="quote">&gt; -void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...);</span>
<span class="quote">&gt; +void warn_alloc_failed(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; +		const char *fmt, ...);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void setup_per_cpu_pageset(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 53c0709fd87b..bf64bfebc473 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -817,7 +817,7 @@ static int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #if defined(CONFIG_CMA) &amp;&amp; defined(CONFIG_X86_64)</span>
<span class="quote">&gt;  static void destroy_compound_gigantic_page(struct page *page,</span>
<span class="quote">&gt; -					unsigned long order)</span>
<span class="quote">&gt; +					unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; @@ -832,7 +832,7 @@ static void destroy_compound_gigantic_page(struct page *page,</span>
<span class="quote">&gt;  	__ClearPageHead(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void free_gigantic_page(struct page *page, unsigned order)</span>
<span class="quote">&gt; +static void free_gigantic_page(struct page *page, unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	free_contig_range(page_to_pfn(page), 1 &lt;&lt; order);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -876,7 +876,7 @@ static bool zone_spans_last_pfn(const struct zone *zone,</span>
<span class="quote">&gt;  	return zone_spans_pfn(zone, last_pfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *alloc_gigantic_page(int nid, unsigned order)</span>
<span class="quote">&gt; +static struct page *alloc_gigantic_page(int nid, unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt;  	unsigned long ret, pfn, flags;</span>
<span class="quote">&gt; @@ -912,7 +912,7 @@ static struct page *alloc_gigantic_page(int nid, unsigned order)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void prep_new_huge_page(struct hstate *h, struct page *page, int nid);</span>
<span class="quote">&gt; -static void prep_compound_gigantic_page(struct page *page, unsigned long order);</span>
<span class="quote">&gt; +static void prep_compound_gigantic_page(struct page *page, unsigned int order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static struct page *alloc_fresh_gigantic_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -945,9 +945,9 @@ static int alloc_fresh_gigantic_page(struct hstate *h,</span>
<span class="quote">&gt;  static inline bool gigantic_page_supported(void) { return true; }</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline bool gigantic_page_supported(void) { return false; }</span>
<span class="quote">&gt; -static inline void free_gigantic_page(struct page *page, unsigned order) { }</span>
<span class="quote">&gt; +static inline void free_gigantic_page(struct page *page, unsigned int order) { }</span>
<span class="quote">&gt;  static inline void destroy_compound_gigantic_page(struct page *page,</span>
<span class="quote">&gt; -						unsigned long order) { }</span>
<span class="quote">&gt; +						unsigned int order) { }</span>
<span class="quote">&gt;  static inline int alloc_fresh_gigantic_page(struct hstate *h,</span>
<span class="quote">&gt;  					nodemask_t *nodes_allowed) { return 0; }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -1073,7 +1073,7 @@ static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)</span>
<span class="quote">&gt;  	put_page(page); /* free it into the hugepage allocator */</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void prep_compound_gigantic_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt; +static void prep_compound_gigantic_page(struct page *page, unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; @@ -1640,7 +1640,8 @@ found:</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void __init prep_compound_huge_page(struct page *page, int order)</span>
<span class="quote">&gt; +static void __init prep_compound_huge_page(struct page *page,</span>
<span class="quote">&gt; +		unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (unlikely(order &gt; (MAX_ORDER - 1)))</span>
<span class="quote">&gt;  		prep_compound_gigantic_page(page, order);</span>
<span class="quote">&gt; @@ -2351,7 +2352,7 @@ static int __init hugetlb_init(void)</span>
<span class="quote">&gt;  module_init(hugetlb_init);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* Should be called on processing a hugepagesz=... option */</span>
<span class="quote">&gt; -void __init hugetlb_add_hstate(unsigned order)</span>
<span class="quote">&gt; +void __init hugetlb_add_hstate(unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hstate *h;</span>
<span class="quote">&gt;  	unsigned long i;</span>
<span class="quote">&gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt; index 89e21a07080a..9a9fc497593f 100644</span>
<span class="quote">&gt; --- a/mm/internal.h</span>
<span class="quote">&gt; +++ b/mm/internal.h</span>
<span class="quote">&gt; @@ -157,7 +157,7 @@ __find_buddy_index(unsigned long page_idx, unsigned int order)</span>
<span class="quote">&gt;  extern int __isolate_free_page(struct page *page, unsigned int order);</span>
<span class="quote">&gt;  extern void __free_pages_bootmem(struct page *page, unsigned long pfn,</span>
<span class="quote">&gt;  					unsigned int order);</span>
<span class="quote">&gt; -extern void prep_compound_page(struct page *page, unsigned long order);</span>
<span class="quote">&gt; +extern void prep_compound_page(struct page *page, unsigned int order);</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;  extern bool is_free_buddy_page(struct page *page);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -214,7 +214,7 @@ int find_suitable_fallback(struct free_area *area, unsigned int order,</span>
<span class="quote">&gt;   * page cannot be allocated or merged in parallel. Alternatively, it must</span>
<span class="quote">&gt;   * handle invalid values gracefully, and use page_order_unsafe() below.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static inline unsigned long page_order(struct page *page)</span>
<span class="quote">&gt; +static inline unsigned int page_order(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/* PageBuddy() must be checked by the caller */</span>
<span class="quote">&gt;  	return page_private(page);</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 78859d47aaf4..347724850665 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -163,7 +163,7 @@ bool pm_suspended_storage(void)</span>
<span class="quote">&gt;  #endif /* CONFIG_PM_SLEEP */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE</span>
<span class="quote">&gt; -int pageblock_order __read_mostly;</span>
<span class="quote">&gt; +unsigned int pageblock_order __read_mostly;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __free_pages_ok(struct page *page, unsigned int order);</span>
<span class="quote">&gt; @@ -441,7 +441,7 @@ static void free_compound_page(struct page *page)</span>
<span class="quote">&gt;  	__free_pages_ok(page, compound_order(page));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void prep_compound_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt; +void prep_compound_page(struct page *page, unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	int nr_pages = 1 &lt;&lt; order;</span>
<span class="quote">&gt; @@ -641,7 +641,7 @@ static inline void __free_one_page(struct page *page,</span>
<span class="quote">&gt;  	unsigned long combined_idx;</span>
<span class="quote">&gt;  	unsigned long uninitialized_var(buddy_idx);</span>
<span class="quote">&gt;  	struct page *buddy;</span>
<span class="quote">&gt; -	int max_order = MAX_ORDER;</span>
<span class="quote">&gt; +	unsigned int max_order = MAX_ORDER;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_BUG_ON(!zone_is_initialized(zone));</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(page-&gt;flags &amp; PAGE_FLAGS_CHECK_AT_PREP, page);</span>
<span class="quote">&gt; @@ -1436,7 +1436,7 @@ int move_freepages(struct zone *zone,</span>
<span class="quote">&gt;  			  int migratetype)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; -	unsigned long order;</span>
<span class="quote">&gt; +	unsigned int order;</span>
<span class="quote">&gt;  	int pages_moved = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef CONFIG_HOLES_IN_ZONE</span>
<span class="quote">&gt; @@ -1550,7 +1550,7 @@ static bool can_steal_fallback(unsigned int order, int start_mt)</span>
<span class="quote">&gt;  static void steal_suitable_fallback(struct zone *zone, struct page *page,</span>
<span class="quote">&gt;  							  int start_type)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	int current_order = page_order(page);</span>
<span class="quote">&gt; +	unsigned int current_order = page_order(page);</span>
<span class="quote">&gt;  	int pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Take ownership for orders &gt;= pageblock_order */</span>
<span class="quote">&gt; @@ -2657,7 +2657,7 @@ static DEFINE_RATELIMIT_STATE(nopage_rs,</span>
<span class="quote">&gt;  		DEFAULT_RATELIMIT_INTERVAL,</span>
<span class="quote">&gt;  		DEFAULT_RATELIMIT_BURST);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt; +void warn_alloc_failed(gfp_t gfp_mask, unsigned int order, const char *fmt, ...)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int filter = SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2691,7 +2691,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;  		va_end(args);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pr_warn(&quot;%s: page allocation failure: order:%d, mode:0x%x\n&quot;,</span>
<span class="quote">&gt; +	pr_warn(&quot;%s: page allocation failure: order:%u, mode:0x%x\n&quot;,</span>
<span class="quote">&gt;  		current-&gt;comm, order, gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	dump_stack();</span>
<span class="quote">&gt; @@ -3450,7 +3450,8 @@ void free_kmem_pages(unsigned long addr, unsigned int order)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void *make_alloc_exact(unsigned long addr, unsigned order, size_t size)</span>
<span class="quote">&gt; +static void *make_alloc_exact(unsigned long addr, unsigned int order,</span>
<span class="quote">&gt; +		size_t size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (addr) {</span>
<span class="quote">&gt;  		unsigned long alloc_end = addr + (PAGE_SIZE &lt;&lt; order);</span>
<span class="quote">&gt; @@ -3502,7 +3503,7 @@ EXPORT_SYMBOL(alloc_pages_exact);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	unsigned order = get_order(size);</span>
<span class="quote">&gt; +	unsigned int order = get_order(size);</span>
<span class="quote">&gt;  	struct page *p = alloc_pages_node(nid, gfp_mask, order);</span>
<span class="quote">&gt;  	if (!p)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; @@ -3804,7 +3805,8 @@ void show_free_areas(unsigned int filter)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_populated_zone(zone) {</span>
<span class="quote">&gt; -		unsigned long nr[MAX_ORDER], flags, order, total = 0;</span>
<span class="quote">&gt; +		unsigned int order;</span>
<span class="quote">&gt; +		unsigned long nr[MAX_ORDER], flags, total = 0;</span>
<span class="quote">&gt;  		unsigned char types[MAX_ORDER];</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (skip_free_areas_node(filter, zone_to_nid(zone)))</span>
<span class="quote">&gt; @@ -4153,7 +4155,7 @@ static void build_zonelists(pg_data_t *pgdat)</span>
<span class="quote">&gt;  	nodemask_t used_mask;</span>
<span class="quote">&gt;  	int local_node, prev_node;</span>
<span class="quote">&gt;  	struct zonelist *zonelist;</span>
<span class="quote">&gt; -	int order = current_zonelist_order;</span>
<span class="quote">&gt; +	unsigned int order = current_zonelist_order;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* initialize zonelists */</span>
<span class="quote">&gt;  	for (i = 0; i &lt; MAX_ZONELISTS; i++) {</span>
<span class="quote">&gt; @@ -6818,7 +6820,8 @@ int alloc_contig_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  		       unsigned migratetype)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long outer_start, outer_end;</span>
<span class="quote">&gt; -	int ret = 0, order;</span>
<span class="quote">&gt; +	unsigned int order;</span>
<span class="quote">&gt; +	int ret = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct compact_control cc = {</span>
<span class="quote">&gt;  		.nr_migratepages = 0,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.5.0</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a4c4b7d07473..a75bbb3f7142 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -557,7 +557,7 @@</span> <span class="p_context"> static inline compound_page_dtor *get_compound_page_dtor(struct page *page)</span>
 	return compound_page_dtors[page[1].compound_dtor];
 }
 
<span class="p_del">-static inline int compound_order(struct page *page)</span>
<span class="p_add">+static inline unsigned int compound_order(struct page *page)</span>
 {
 	if (!PageHead(page))
 		return 0;
<span class="p_chunk">@@ -1718,7 +1718,8 @@</span> <span class="p_context"> extern void si_meminfo(struct sysinfo * val);</span>
 extern void si_meminfo_node(struct sysinfo *val, int nid);
 
 extern __printf(3, 4)
<span class="p_del">-void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...);</span>
<span class="p_add">+void warn_alloc_failed(gfp_t gfp_mask, unsigned int order,</span>
<span class="p_add">+		const char *fmt, ...);</span>
 
 extern void setup_per_cpu_pageset(void);
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 53c0709fd87b..bf64bfebc473 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -817,7 +817,7 @@</span> <span class="p_context"> static int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)</span>
 
 #if defined(CONFIG_CMA) &amp;&amp; defined(CONFIG_X86_64)
 static void destroy_compound_gigantic_page(struct page *page,
<span class="p_del">-					unsigned long order)</span>
<span class="p_add">+					unsigned int order)</span>
 {
 	int i;
 	int nr_pages = 1 &lt;&lt; order;
<span class="p_chunk">@@ -832,7 +832,7 @@</span> <span class="p_context"> static void destroy_compound_gigantic_page(struct page *page,</span>
 	__ClearPageHead(page);
 }
 
<span class="p_del">-static void free_gigantic_page(struct page *page, unsigned order)</span>
<span class="p_add">+static void free_gigantic_page(struct page *page, unsigned int order)</span>
 {
 	free_contig_range(page_to_pfn(page), 1 &lt;&lt; order);
 }
<span class="p_chunk">@@ -876,7 +876,7 @@</span> <span class="p_context"> static bool zone_spans_last_pfn(const struct zone *zone,</span>
 	return zone_spans_pfn(zone, last_pfn);
 }
 
<span class="p_del">-static struct page *alloc_gigantic_page(int nid, unsigned order)</span>
<span class="p_add">+static struct page *alloc_gigantic_page(int nid, unsigned int order)</span>
 {
 	unsigned long nr_pages = 1 &lt;&lt; order;
 	unsigned long ret, pfn, flags;
<span class="p_chunk">@@ -912,7 +912,7 @@</span> <span class="p_context"> static struct page *alloc_gigantic_page(int nid, unsigned order)</span>
 }
 
 static void prep_new_huge_page(struct hstate *h, struct page *page, int nid);
<span class="p_del">-static void prep_compound_gigantic_page(struct page *page, unsigned long order);</span>
<span class="p_add">+static void prep_compound_gigantic_page(struct page *page, unsigned int order);</span>
 
 static struct page *alloc_fresh_gigantic_page_node(struct hstate *h, int nid)
 {
<span class="p_chunk">@@ -945,9 +945,9 @@</span> <span class="p_context"> static int alloc_fresh_gigantic_page(struct hstate *h,</span>
 static inline bool gigantic_page_supported(void) { return true; }
 #else
 static inline bool gigantic_page_supported(void) { return false; }
<span class="p_del">-static inline void free_gigantic_page(struct page *page, unsigned order) { }</span>
<span class="p_add">+static inline void free_gigantic_page(struct page *page, unsigned int order) { }</span>
 static inline void destroy_compound_gigantic_page(struct page *page,
<span class="p_del">-						unsigned long order) { }</span>
<span class="p_add">+						unsigned int order) { }</span>
 static inline int alloc_fresh_gigantic_page(struct hstate *h,
 					nodemask_t *nodes_allowed) { return 0; }
 #endif
<span class="p_chunk">@@ -1073,7 +1073,7 @@</span> <span class="p_context"> static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)</span>
 	put_page(page); /* free it into the hugepage allocator */
 }
 
<span class="p_del">-static void prep_compound_gigantic_page(struct page *page, unsigned long order)</span>
<span class="p_add">+static void prep_compound_gigantic_page(struct page *page, unsigned int order)</span>
 {
 	int i;
 	int nr_pages = 1 &lt;&lt; order;
<span class="p_chunk">@@ -1640,7 +1640,8 @@</span> <span class="p_context"> found:</span>
 	return 1;
 }
 
<span class="p_del">-static void __init prep_compound_huge_page(struct page *page, int order)</span>
<span class="p_add">+static void __init prep_compound_huge_page(struct page *page,</span>
<span class="p_add">+		unsigned int order)</span>
 {
 	if (unlikely(order &gt; (MAX_ORDER - 1)))
 		prep_compound_gigantic_page(page, order);
<span class="p_chunk">@@ -2351,7 +2352,7 @@</span> <span class="p_context"> static int __init hugetlb_init(void)</span>
 module_init(hugetlb_init);
 
 /* Should be called on processing a hugepagesz=... option */
<span class="p_del">-void __init hugetlb_add_hstate(unsigned order)</span>
<span class="p_add">+void __init hugetlb_add_hstate(unsigned int order)</span>
 {
 	struct hstate *h;
 	unsigned long i;
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 89e21a07080a..9a9fc497593f 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -157,7 +157,7 @@</span> <span class="p_context"> __find_buddy_index(unsigned long page_idx, unsigned int order)</span>
 extern int __isolate_free_page(struct page *page, unsigned int order);
 extern void __free_pages_bootmem(struct page *page, unsigned long pfn,
 					unsigned int order);
<span class="p_del">-extern void prep_compound_page(struct page *page, unsigned long order);</span>
<span class="p_add">+extern void prep_compound_page(struct page *page, unsigned int order);</span>
 #ifdef CONFIG_MEMORY_FAILURE
 extern bool is_free_buddy_page(struct page *page);
 #endif
<span class="p_chunk">@@ -214,7 +214,7 @@</span> <span class="p_context"> int find_suitable_fallback(struct free_area *area, unsigned int order,</span>
  * page cannot be allocated or merged in parallel. Alternatively, it must
  * handle invalid values gracefully, and use page_order_unsafe() below.
  */
<span class="p_del">-static inline unsigned long page_order(struct page *page)</span>
<span class="p_add">+static inline unsigned int page_order(struct page *page)</span>
 {
 	/* PageBuddy() must be checked by the caller */
 	return page_private(page);
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 78859d47aaf4..347724850665 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -163,7 +163,7 @@</span> <span class="p_context"> bool pm_suspended_storage(void)</span>
 #endif /* CONFIG_PM_SLEEP */
 
 #ifdef CONFIG_HUGETLB_PAGE_SIZE_VARIABLE
<span class="p_del">-int pageblock_order __read_mostly;</span>
<span class="p_add">+unsigned int pageblock_order __read_mostly;</span>
 #endif
 
 static void __free_pages_ok(struct page *page, unsigned int order);
<span class="p_chunk">@@ -441,7 +441,7 @@</span> <span class="p_context"> static void free_compound_page(struct page *page)</span>
 	__free_pages_ok(page, compound_order(page));
 }
 
<span class="p_del">-void prep_compound_page(struct page *page, unsigned long order)</span>
<span class="p_add">+void prep_compound_page(struct page *page, unsigned int order)</span>
 {
 	int i;
 	int nr_pages = 1 &lt;&lt; order;
<span class="p_chunk">@@ -641,7 +641,7 @@</span> <span class="p_context"> static inline void __free_one_page(struct page *page,</span>
 	unsigned long combined_idx;
 	unsigned long uninitialized_var(buddy_idx);
 	struct page *buddy;
<span class="p_del">-	int max_order = MAX_ORDER;</span>
<span class="p_add">+	unsigned int max_order = MAX_ORDER;</span>
 
 	VM_BUG_ON(!zone_is_initialized(zone));
 	VM_BUG_ON_PAGE(page-&gt;flags &amp; PAGE_FLAGS_CHECK_AT_PREP, page);
<span class="p_chunk">@@ -1436,7 +1436,7 @@</span> <span class="p_context"> int move_freepages(struct zone *zone,</span>
 			  int migratetype)
 {
 	struct page *page;
<span class="p_del">-	unsigned long order;</span>
<span class="p_add">+	unsigned int order;</span>
 	int pages_moved = 0;
 
 #ifndef CONFIG_HOLES_IN_ZONE
<span class="p_chunk">@@ -1550,7 +1550,7 @@</span> <span class="p_context"> static bool can_steal_fallback(unsigned int order, int start_mt)</span>
 static void steal_suitable_fallback(struct zone *zone, struct page *page,
 							  int start_type)
 {
<span class="p_del">-	int current_order = page_order(page);</span>
<span class="p_add">+	unsigned int current_order = page_order(page);</span>
 	int pages;
 
 	/* Take ownership for orders &gt;= pageblock_order */
<span class="p_chunk">@@ -2657,7 +2657,7 @@</span> <span class="p_context"> static DEFINE_RATELIMIT_STATE(nopage_rs,</span>
 		DEFAULT_RATELIMIT_INTERVAL,
 		DEFAULT_RATELIMIT_BURST);
 
<span class="p_del">-void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="p_add">+void warn_alloc_failed(gfp_t gfp_mask, unsigned int order, const char *fmt, ...)</span>
 {
 	unsigned int filter = SHOW_MEM_FILTER_NODES;
 
<span class="p_chunk">@@ -2691,7 +2691,7 @@</span> <span class="p_context"> void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
 		va_end(args);
 	}
 
<span class="p_del">-	pr_warn(&quot;%s: page allocation failure: order:%d, mode:0x%x\n&quot;,</span>
<span class="p_add">+	pr_warn(&quot;%s: page allocation failure: order:%u, mode:0x%x\n&quot;,</span>
 		current-&gt;comm, order, gfp_mask);
 
 	dump_stack();
<span class="p_chunk">@@ -3450,7 +3450,8 @@</span> <span class="p_context"> void free_kmem_pages(unsigned long addr, unsigned int order)</span>
 	}
 }
 
<span class="p_del">-static void *make_alloc_exact(unsigned long addr, unsigned order, size_t size)</span>
<span class="p_add">+static void *make_alloc_exact(unsigned long addr, unsigned int order,</span>
<span class="p_add">+		size_t size)</span>
 {
 	if (addr) {
 		unsigned long alloc_end = addr + (PAGE_SIZE &lt;&lt; order);
<span class="p_chunk">@@ -3502,7 +3503,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(alloc_pages_exact);</span>
  */
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
<span class="p_del">-	unsigned order = get_order(size);</span>
<span class="p_add">+	unsigned int order = get_order(size);</span>
 	struct page *p = alloc_pages_node(nid, gfp_mask, order);
 	if (!p)
 		return NULL;
<span class="p_chunk">@@ -3804,7 +3805,8 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 	}
 
 	for_each_populated_zone(zone) {
<span class="p_del">-		unsigned long nr[MAX_ORDER], flags, order, total = 0;</span>
<span class="p_add">+		unsigned int order;</span>
<span class="p_add">+		unsigned long nr[MAX_ORDER], flags, total = 0;</span>
 		unsigned char types[MAX_ORDER];
 
 		if (skip_free_areas_node(filter, zone_to_nid(zone)))
<span class="p_chunk">@@ -4153,7 +4155,7 @@</span> <span class="p_context"> static void build_zonelists(pg_data_t *pgdat)</span>
 	nodemask_t used_mask;
 	int local_node, prev_node;
 	struct zonelist *zonelist;
<span class="p_del">-	int order = current_zonelist_order;</span>
<span class="p_add">+	unsigned int order = current_zonelist_order;</span>
 
 	/* initialize zonelists */
 	for (i = 0; i &lt; MAX_ZONELISTS; i++) {
<span class="p_chunk">@@ -6818,7 +6820,8 @@</span> <span class="p_context"> int alloc_contig_range(unsigned long start, unsigned long end,</span>
 		       unsigned migratetype)
 {
 	unsigned long outer_start, outer_end;
<span class="p_del">-	int ret = 0, order;</span>
<span class="p_add">+	unsigned int order;</span>
<span class="p_add">+	int ret = 0;</span>
 
 	struct compact_control cc = {
 		.nr_migratepages = 0,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



