
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>x86: Align TLB invalidation info - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    x86: Align TLB invalidation info</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 31, 2018, 8:11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180131201118.1694-1-namit@vmware.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10194759/mbox/"
   >mbox</a>
|
   <a href="/patch/10194759/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10194759/">/patch/10194759/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	93235601A0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 31 Jan 2018 20:11:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6A1BE2886D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 31 Jan 2018 20:11:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5EB9F28871; Wed, 31 Jan 2018 20:11:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AFB282886F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 31 Jan 2018 20:11:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751579AbeAaULf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 31 Jan 2018 15:11:35 -0500
Received: from ex13-edg-ou-001.vmware.com ([208.91.0.189]:44946 &quot;EHLO
	EX13-EDG-OU-001.vmware.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751296AbeAaULe (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 31 Jan 2018 15:11:34 -0500
Received: from sc9-mailhost3.vmware.com (10.113.161.73) by
	EX13-EDG-OU-001.vmware.com (10.113.208.155) with Microsoft SMTP
	Server id 15.0.1156.6; Wed, 31 Jan 2018 12:11:29 -0800
Received: from ubuntu.localdomain (unknown [10.2.101.129])
	by sc9-mailhost3.vmware.com (Postfix) with ESMTP id AE51A40C4F;
	Wed, 31 Jan 2018 12:11:33 -0800 (PST)
From: Nadav Amit &lt;namit@vmware.com&gt;
To: &lt;x86@kernel.org&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	Nadav Amit &lt;namit@vmware.com&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
Subject: [PATCH] x86: Align TLB invalidation info
Date: Wed, 31 Jan 2018 12:11:18 -0800
Message-ID: &lt;20180131201118.1694-1-namit@vmware.com&gt;
X-Mailer: git-send-email 2.14.1
MIME-Version: 1.0
Content-Type: text/plain
Received-SPF: None (EX13-EDG-OU-001.vmware.com: namit@vmware.com does not
	designate permitted sender hosts)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Jan. 31, 2018, 8:11 p.m.</div>
<pre class="content">
The TLB invalidation info is allocated on the stack, which might cause
it to be unaligned. Since this information may be transferred to
different cores for TLB shootdown, this might result in an additional
cache-line bouncing between the cores.

GCC provides a way to deal with it by using
__builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.
<span class="signed-off-by">
Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>

Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
---
 arch/x86/mm/tlb.c              | 21 +++++++++++----------
 include/linux/compiler-gcc.h   |  5 +++++
 include/linux/compiler_types.h |  4 ++++
 3 files changed, 20 insertions(+), 10 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Jan. 31, 2018, 8:24 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Jan 31, 2018, at 12:11 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt; </span>

Eww.  How about __aligned?
<span class="quote">

&gt; Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; arch/x86/mm/tlb.c              | 21 +++++++++++----------</span>
<span class="quote">&gt; include/linux/compiler-gcc.h   |  5 +++++</span>
<span class="quote">&gt; include/linux/compiler_types.h |  4 ++++</span>
<span class="quote">&gt; 3 files changed, 20 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; index 5bfe61a5e8e3..bab7bb5d982f 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; @@ -574,37 +574,38 @@ static unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;</span>
<span class="quote">&gt; void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt;                unsigned long end, unsigned long vmflag)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; +    struct flush_tlb_info *info;</span>
<span class="quote">&gt;    int cpu;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -    struct flush_tlb_info info = {</span>
<span class="quote">&gt; -        .mm = mm,</span>
<span class="quote">&gt; -    };</span>
<span class="quote">&gt; +    info = __alloca_with_align(sizeof(*info),</span>
<span class="quote">&gt; +                   SMP_CACHE_BYTES * BITS_PER_BYTE);</span>
<span class="quote">&gt; +    info-&gt;mm = mm;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    cpu = get_cpu();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    /* This is also a barrier that synchronizes with switch_mm(). */</span>
<span class="quote">&gt; -    info.new_tlb_gen = inc_mm_tlb_gen(mm);</span>
<span class="quote">&gt; +    info-&gt;new_tlb_gen = inc_mm_tlb_gen(mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    /* Should we flush just the requested range? */</span>
<span class="quote">&gt;    if ((end != TLB_FLUSH_ALL) &amp;&amp;</span>
<span class="quote">&gt;        !(vmflag &amp; VM_HUGETLB) &amp;&amp;</span>
<span class="quote">&gt;        ((end - start) &gt;&gt; PAGE_SHIFT) &lt;= tlb_single_page_flush_ceiling) {</span>
<span class="quote">&gt; -        info.start = start;</span>
<span class="quote">&gt; -        info.end = end;</span>
<span class="quote">&gt; +        info-&gt;start = start;</span>
<span class="quote">&gt; +        info-&gt;end = end;</span>
<span class="quote">&gt;    } else {</span>
<span class="quote">&gt; -        info.start = 0UL;</span>
<span class="quote">&gt; -        info.end = TLB_FLUSH_ALL;</span>
<span class="quote">&gt; +        info-&gt;start = 0UL;</span>
<span class="quote">&gt; +        info-&gt;end = TLB_FLUSH_ALL;</span>
<span class="quote">&gt;    }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    if (mm == this_cpu_read(cpu_tlbstate.loaded_mm)) {</span>
<span class="quote">&gt;        VM_WARN_ON(irqs_disabled());</span>
<span class="quote">&gt;        local_irq_disable();</span>
<span class="quote">&gt; -        flush_tlb_func_local(&amp;info, TLB_LOCAL_MM_SHOOTDOWN);</span>
<span class="quote">&gt; +        flush_tlb_func_local(info, TLB_LOCAL_MM_SHOOTDOWN);</span>
<span class="quote">&gt;        local_irq_enable();</span>
<span class="quote">&gt;    }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    if (cpumask_any_but(mm_cpumask(mm), cpu) &lt; nr_cpu_ids)</span>
<span class="quote">&gt; -        flush_tlb_others(mm_cpumask(mm), &amp;info);</span>
<span class="quote">&gt; +        flush_tlb_others(mm_cpumask(mm), info);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    put_cpu();</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h</span>
<span class="quote">&gt; index 631354acfa72..aea9a2e69417 100644</span>
<span class="quote">&gt; --- a/include/linux/compiler-gcc.h</span>
<span class="quote">&gt; +++ b/include/linux/compiler-gcc.h</span>
<span class="quote">&gt; @@ -314,6 +314,11 @@</span>
<span class="quote">&gt; #define __designated_init __attribute__((designated_init))</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#if GCC_VERSION &gt;= 60100</span>
<span class="quote">&gt; +#define __alloca_with_align(size, alignment)                \</span>
<span class="quote">&gt; +    __builtin_alloca_with_align(size, alignment)</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; #endif    /* gcc version &gt;= 40000 specific checks */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #if !defined(__noclone)</span>
<span class="quote">&gt; diff --git a/include/linux/compiler_types.h b/include/linux/compiler_types.h</span>
<span class="quote">&gt; index 6b79a9bba9a7..c71297d95c74 100644</span>
<span class="quote">&gt; --- a/include/linux/compiler_types.h</span>
<span class="quote">&gt; +++ b/include/linux/compiler_types.h</span>
<span class="quote">&gt; @@ -271,4 +271,8 @@ struct ftrace_likely_data {</span>
<span class="quote">&gt; # define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#ifndef __alloca_with_align</span>
<span class="quote">&gt; +#define __alloca_with_align(size, alignment) __builtin_alloca(size)</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; #endif /* __LINUX_COMPILER_TYPES_H */</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.14.1</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 31, 2018, 8:48 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Jan 31, 2018, at 12:11 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Eww.  How about __aligned?</span>

Err.. Stupid me. For some reason I remembered I tried it and it didn’t have
the desired effect, which caused me to assume it does not work for variables
on the stack. Anyhow, it does the work. I’ll submit v2.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Jan. 31, 2018, 8:49 p.m.</div>
<pre class="content">
On 01/31/2018 12:48 PM, Nadav Amit wrote:
<span class="quote">&gt;&gt;&gt; On Jan 31, 2018, at 12:11 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt;&gt; Eww.  How about __aligned?</span>
<span class="quote">&gt; Err.. Stupid me. For some reason I remembered I tried it and it didn’t have</span>
<span class="quote">&gt; the desired effect, which caused me to assume it does not work for variables</span>
<span class="quote">&gt; on the stack. Anyhow, it does the work. I’ll submit v2.</span>

Also, just curious, but did you find this situation by inspection or did
it show up in a profile somewhere?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 31, 2018, 8:52 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 01/31/2018 12:48 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Jan 31, 2018, at 12:11 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt;&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt;&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt;&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt;&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt;&gt;&gt; Eww.  How about __aligned?</span>
<span class="quote">&gt;&gt; Err.. Stupid me. For some reason I remembered I tried it and it didn’t have</span>
<span class="quote">&gt;&gt; the desired effect, which caused me to assume it does not work for variables</span>
<span class="quote">&gt;&gt; on the stack. Anyhow, it does the work. I’ll submit v2.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, just curious, but did you find this situation by inspection or did</span>
<span class="quote">&gt; it show up in a profile somewhere?</span>

Actually, not. I considered adding data to info for a different reason
(which eventually I deserted); I was assuming you will all kill me for
increasing the size, so this was a preemption step.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Jan. 31, 2018, 9 p.m.</div>
<pre class="content">
On Wed, Jan 31, 2018 at 12:48 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Jan 31, 2018, at 12:11 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Eww.  How about __aligned?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Err.. Stupid me. For some reason I remembered I tried it and it didn’t have</span>
<span class="quote">&gt; the desired effect, which caused me to assume it does not work for variables</span>
<span class="quote">&gt; on the stack. Anyhow, it does the work. I’ll submit v2.</span>
<span class="quote">&gt;</span>

You&#39;re probably remembering that __aligned(16) malfunctions on older
GCC versions.  But __aligned(64), which is what you want, has always
been okay AFAIK.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Jan. 31, 2018, 9:01 p.m.</div>
<pre class="content">
On 01/31/2018 12:11 PM, Nadav Amit wrote:
<span class="quote">&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>

It doesn&#39;t really *bounce*, though, does it?  I don&#39;t see any writes on
the remote side.  The remote use seems entirely read-only.

You also don&#39;t have to exhaustively test this, but I&#39;d love to see at
least a sanity check with a microbenchmark (or something) that, yes,
this does help *something*.  Maybe it makes the remote
flush_tlb_func_common() run faster because it&#39;s pulling in fewer lines,
or maybe you can even detect fewer misses in there.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 31, 2018, 9:09 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 01/31/2018 12:11 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It doesn&#39;t really *bounce*, though, does it?  I don&#39;t see any writes on</span>
<span class="quote">&gt; the remote side.  The remote use seems entirely read-only.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You also don&#39;t have to exhaustively test this, but I&#39;d love to see at</span>
<span class="quote">&gt; least a sanity check with a microbenchmark (or something) that, yes,</span>
<span class="quote">&gt; this does help *something*.  Maybe it makes the remote</span>
<span class="quote">&gt; flush_tlb_func_common() run faster because it&#39;s pulling in fewer lines,</span>
<span class="quote">&gt; or maybe you can even detect fewer misses in there.</span>

I agree that with the whole Meltdown/Spectre entry-cost it might not even be
measurable, at least on small ( &lt; 2 sockets) machines. But I do not think it
worth profiling. Basically, AFAIK, all the data structures that are used for
inter-processor communication by the kernel are aligned, and this is an
exception.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Jan. 31, 2018, 9:15 p.m.</div>
<pre class="content">
On Wed, Jan 31, 2018 at 1:09 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On 01/31/2018 12:11 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It doesn&#39;t really *bounce*, though, does it?  I don&#39;t see any writes on</span>
<span class="quote">&gt;&gt; the remote side.  The remote use seems entirely read-only.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; You also don&#39;t have to exhaustively test this, but I&#39;d love to see at</span>
<span class="quote">&gt;&gt; least a sanity check with a microbenchmark (or something) that, yes,</span>
<span class="quote">&gt;&gt; this does help *something*.  Maybe it makes the remote</span>
<span class="quote">&gt;&gt; flush_tlb_func_common() run faster because it&#39;s pulling in fewer lines,</span>
<span class="quote">&gt;&gt; or maybe you can even detect fewer misses in there.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I agree that with the whole Meltdown/Spectre entry-cost it might not even be</span>
<span class="quote">&gt; measurable, at least on small ( &lt; 2 sockets) machines. But I do not think it</span>
<span class="quote">&gt; worth profiling. Basically, AFAIK, all the data structures that are used for</span>
<span class="quote">&gt; inter-processor communication by the kernel are aligned, and this is an</span>
<span class="quote">&gt; exception.</span>
<span class="quote">&gt;</span>

This is only going to be measurable at all on NUMA, I suspect.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 31, 2018, 9:17 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">
&gt; On Wed, Jan 31, 2018 at 1:09 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On 01/31/2018 12:11 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; The TLB invalidation info is allocated on the stack, which might cause</span>
<span class="quote">&gt;&gt;&gt;&gt; it to be unaligned. Since this information may be transferred to</span>
<span class="quote">&gt;&gt;&gt;&gt; different cores for TLB shootdown, this might result in an additional</span>
<span class="quote">&gt;&gt;&gt;&gt; cache-line bouncing between the cores.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; GCC provides a way to deal with it by using</span>
<span class="quote">&gt;&gt;&gt;&gt; __builtin_alloca_with_align(). Use it to avoid the bouncing cache lines.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; It doesn&#39;t really *bounce*, though, does it?  I don&#39;t see any writes on</span>
<span class="quote">&gt;&gt;&gt; the remote side.  The remote use seems entirely read-only.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; You also don&#39;t have to exhaustively test this, but I&#39;d love to see at</span>
<span class="quote">&gt;&gt;&gt; least a sanity check with a microbenchmark (or something) that, yes,</span>
<span class="quote">&gt;&gt;&gt; this does help *something*.  Maybe it makes the remote</span>
<span class="quote">&gt;&gt;&gt; flush_tlb_func_common() run faster because it&#39;s pulling in fewer lines,</span>
<span class="quote">&gt;&gt;&gt; or maybe you can even detect fewer misses in there.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I agree that with the whole Meltdown/Spectre entry-cost it might not even be</span>
<span class="quote">&gt;&gt; measurable, at least on small ( &lt; 2 sockets) machines. But I do not think it</span>
<span class="quote">&gt;&gt; worth profiling. Basically, AFAIK, all the data structures that are used for</span>
<span class="quote">&gt;&gt; inter-processor communication by the kernel are aligned, and this is an</span>
<span class="quote">&gt;&gt; exception.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is only going to be measurable at all on NUMA, I suspect.</span>

Yes, I meant &lt;= 2 ...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 1, 2018, 5:38 a.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 01/31/2018 01:09 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt;&gt; You also don&#39;t have to exhaustively test this, but I&#39;d love to see at</span>
<span class="quote">&gt;&gt;&gt; least a sanity check with a microbenchmark (or something) that, yes,</span>
<span class="quote">&gt;&gt;&gt; this does help *something*.  Maybe it makes the remote</span>
<span class="quote">&gt;&gt;&gt; flush_tlb_func_common() run faster because it&#39;s pulling in fewer lines,</span>
<span class="quote">&gt;&gt;&gt; or maybe you can even detect fewer misses in there.</span>
<span class="quote">&gt;&gt; I agree that with the whole Meltdown/Spectre entry-cost it might not even be</span>
<span class="quote">&gt;&gt; measurable, at least on small ( &lt; 2 sockets) machines. But I do not think it</span>
<span class="quote">&gt;&gt; worth profiling. Basically, AFAIK, all the data structures that are used for</span>
<span class="quote">&gt;&gt; inter-processor communication by the kernel are aligned, and this is an</span>
<span class="quote">&gt;&gt; exception.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m certainly not nak&#39;ing this.  I think your patch is likely a good</span>
<span class="quote">&gt; idea.  But, could you please take ten or twenty minutes to go see if</span>
<span class="quote">&gt; practice matches your assumptions?  I&#39;d really appreciate it.  If you</span>
<span class="quote">&gt; can&#39;t measure it, then no biggie.</span>

[CC’ing the mailing list]

Per your request, I measured it (which perhaps I should have done before). I
caused a misalignment intentionally by adding some padding to flush_tlb_info
and compared it with an aligned version.

I used ftrace to measure the execution time of flush_tlb_func_remote() on a
2-socket Haswell machine, using a microbenchmark I wrote for some research
project.

It turns out that your skepticism may be correct - In both cases the
function execution time is roughly 400ns (2% improvement on the aligned case
which is probably noise).

So it is up to you whether you want to discard the patch.

Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Feb. 1, 2018, 9:38 a.m.</div>
<pre class="content">
On Wed, Jan 31, 2018 at 09:38:46PM -0800, Nadav Amit wrote:
<span class="quote">
&gt; I used ftrace to measure the execution time of flush_tlb_func_remote() on a</span>
<span class="quote">&gt; 2-socket Haswell machine, using a microbenchmark I wrote for some research</span>
<span class="quote">&gt; project.</span>

However cool ftrace is, it is _really_ bad for such uses. The cost of
using ftrace is many many time higher than any change you could affect
by this.

A microbench and/or perf is what you should use for this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 1, 2018, 6:45 p.m.</div>
<pre class="content">
Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">
&gt; On Wed, Jan 31, 2018 at 09:38:46PM -0800, Nadav Amit wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; I used ftrace to measure the execution time of flush_tlb_func_remote() on a</span>
<span class="quote">&gt;&gt; 2-socket Haswell machine, using a microbenchmark I wrote for some research</span>
<span class="quote">&gt;&gt; project.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; However cool ftrace is, it is _really_ bad for such uses. The cost of</span>
<span class="quote">&gt; using ftrace is many many time higher than any change you could affect</span>
<span class="quote">&gt; by this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A microbench and/or perf is what you should use for this.</span>

Don’t expect to see a remote NUMA access impact, whose cost are few 10s of
nanoseconds on microbenchmarks. (And indeed I did not.) Each iteration of
#PF - MADV_DONTNEED takes several microseconds, and the impact is lost in
the noise.

You are right in the fact that ftrace introduces overheads, but the variance
is relatively low. If I stretch the struct to 3 lines of cache, I see a 20ns
overhead. Anyhow, I think this line of code got more than its fair share of
attention.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 5bfe61a5e8e3..bab7bb5d982f 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -574,37 +574,38 @@</span> <span class="p_context"> static unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;</span>
 void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned long vmflag)
 {
<span class="p_add">+	struct flush_tlb_info *info;</span>
 	int cpu;
 
<span class="p_del">-	struct flush_tlb_info info = {</span>
<span class="p_del">-		.mm = mm,</span>
<span class="p_del">-	};</span>
<span class="p_add">+	info = __alloca_with_align(sizeof(*info),</span>
<span class="p_add">+				   SMP_CACHE_BYTES * BITS_PER_BYTE);</span>
<span class="p_add">+	info-&gt;mm = mm;</span>
 
 	cpu = get_cpu();
 
 	/* This is also a barrier that synchronizes with switch_mm(). */
<span class="p_del">-	info.new_tlb_gen = inc_mm_tlb_gen(mm);</span>
<span class="p_add">+	info-&gt;new_tlb_gen = inc_mm_tlb_gen(mm);</span>
 
 	/* Should we flush just the requested range? */
 	if ((end != TLB_FLUSH_ALL) &amp;&amp;
 	    !(vmflag &amp; VM_HUGETLB) &amp;&amp;
 	    ((end - start) &gt;&gt; PAGE_SHIFT) &lt;= tlb_single_page_flush_ceiling) {
<span class="p_del">-		info.start = start;</span>
<span class="p_del">-		info.end = end;</span>
<span class="p_add">+		info-&gt;start = start;</span>
<span class="p_add">+		info-&gt;end = end;</span>
 	} else {
<span class="p_del">-		info.start = 0UL;</span>
<span class="p_del">-		info.end = TLB_FLUSH_ALL;</span>
<span class="p_add">+		info-&gt;start = 0UL;</span>
<span class="p_add">+		info-&gt;end = TLB_FLUSH_ALL;</span>
 	}
 
 	if (mm == this_cpu_read(cpu_tlbstate.loaded_mm)) {
 		VM_WARN_ON(irqs_disabled());
 		local_irq_disable();
<span class="p_del">-		flush_tlb_func_local(&amp;info, TLB_LOCAL_MM_SHOOTDOWN);</span>
<span class="p_add">+		flush_tlb_func_local(info, TLB_LOCAL_MM_SHOOTDOWN);</span>
 		local_irq_enable();
 	}
 
 	if (cpumask_any_but(mm_cpumask(mm), cpu) &lt; nr_cpu_ids)
<span class="p_del">-		flush_tlb_others(mm_cpumask(mm), &amp;info);</span>
<span class="p_add">+		flush_tlb_others(mm_cpumask(mm), info);</span>
 
 	put_cpu();
 }
<span class="p_header">diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h</span>
<span class="p_header">index 631354acfa72..aea9a2e69417 100644</span>
<span class="p_header">--- a/include/linux/compiler-gcc.h</span>
<span class="p_header">+++ b/include/linux/compiler-gcc.h</span>
<span class="p_chunk">@@ -314,6 +314,11 @@</span> <span class="p_context"></span>
 #define __designated_init __attribute__((designated_init))
 #endif
 
<span class="p_add">+#if GCC_VERSION &gt;= 60100</span>
<span class="p_add">+#define __alloca_with_align(size, alignment)				\</span>
<span class="p_add">+	__builtin_alloca_with_align(size, alignment)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif	/* gcc version &gt;= 40000 specific checks */
 
 #if !defined(__noclone)
<span class="p_header">diff --git a/include/linux/compiler_types.h b/include/linux/compiler_types.h</span>
<span class="p_header">index 6b79a9bba9a7..c71297d95c74 100644</span>
<span class="p_header">--- a/include/linux/compiler_types.h</span>
<span class="p_header">+++ b/include/linux/compiler_types.h</span>
<span class="p_chunk">@@ -271,4 +271,8 @@</span> <span class="p_context"> struct ftrace_likely_data {</span>
 # define __native_word(t) (sizeof(t) == sizeof(char) || sizeof(t) == sizeof(short) || sizeof(t) == sizeof(int) || sizeof(t) == sizeof(long))
 #endif
 
<span class="p_add">+#ifndef __alloca_with_align</span>
<span class="p_add">+#define __alloca_with_align(size, alignment) __builtin_alloca(size)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* __LINUX_COMPILER_TYPES_H */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



