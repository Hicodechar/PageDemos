
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm, oom: allow oom reaper to race with exit_mmap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm, oom: allow oom reaper to race with exit_mmap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 27, 2017, 6:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170727065023.GB20970@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9866347/mbox/"
   >mbox</a>
|
   <a href="/patch/9866347/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9866347/">/patch/9866347/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D971660382 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 27 Jul 2017 06:50:35 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C9A44287F0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 27 Jul 2017 06:50:35 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BE68D287F8; Thu, 27 Jul 2017 06:50:35 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C7DD8287F0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 27 Jul 2017 06:50:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751632AbdG0Gu3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 27 Jul 2017 02:50:29 -0400
Received: from mx2.suse.de ([195.135.220.15]:40495 &quot;EHLO mx1.suse.de&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1751454AbdG0Gu2 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 27 Jul 2017 02:50:28 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx1.suse.de (Postfix) with ESMTP id A83B0AE4F;
	Thu, 27 Jul 2017 06:50:26 +0000 (UTC)
Date: Thu, 27 Jul 2017 08:50:24 +0200
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;, linux-mm@kvack.org,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: Re: [PATCH] mm, oom: allow oom reaper to race with exit_mmap
Message-ID: &lt;20170727065023.GB20970@dhcp22.suse.cz&gt;
References: &lt;20170724145142.i5xqpie3joyxbnck@node.shutemov.name&gt;
	&lt;20170724161146.GQ25221@dhcp22.suse.cz&gt;
	&lt;20170725142626.GJ26723@dhcp22.suse.cz&gt;
	&lt;20170725151754.3txp44a2kbffsxdg@node.shutemov.name&gt;
	&lt;20170725152300.GM26723@dhcp22.suse.cz&gt;
	&lt;20170725153110.qzfz7wpnxkjwh5bc@node.shutemov.name&gt;
	&lt;20170725160359.GO26723@dhcp22.suse.cz&gt;
	&lt;20170725191952.GR29716@redhat.com&gt;
	&lt;20170726054557.GB960@dhcp22.suse.cz&gt;
	&lt;20170726162912.GA29716@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20170726162912.GA29716@redhat.com&gt;
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 27, 2017, 6:50 a.m.</div>
<pre class="content">
On Wed 26-07-17 18:29:12, Andrea Arcangeli wrote:
<span class="quote">&gt; On Wed, Jul 26, 2017 at 07:45:57AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Tue 25-07-17 21:19:52, Andrea Arcangeli wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tue, Jul 25, 2017 at 06:04:00PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; -	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (tsk_is_oom_victim(current))</span>
<span class="quote">&gt; &gt; &gt; &gt; +		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; &gt; &gt;  	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);</span>
<span class="quote">&gt; &gt; &gt; &gt;  	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt; &gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -3012,7 +3014,8 @@ void exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; &gt; &gt;  	}</span>
<span class="quote">&gt; &gt; &gt; &gt;  	mm-&gt;mmap = NULL;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	vm_unacct_memory(nr_accounted);</span>
<span class="quote">&gt; &gt; &gt; &gt; -	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (tsk_is_oom_victim(current))</span>
<span class="quote">&gt; &gt; &gt; &gt; +		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; How is this possibly safe? mark_oom_victim can run while exit_mmap is</span>
<span class="quote">&gt; &gt; &gt; running.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I believe it cannot. We always call mark_oom_victim (on !current) with</span>
<span class="quote">&gt; &gt; task_lock held and check task-&gt;mm != NULL and we call do_exit-&gt;mmput after</span>
<span class="quote">&gt; &gt; mm is set to NULL under the same lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Holding the mmap_sem for writing and setting mm-&gt;mmap to NULL to</span>
<span class="quote">&gt; filter which tasks already released the mmap_sem for writing post</span>
<span class="quote">&gt; free_pgtables still look unnecessary to solve this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using MMF_OOM_SKIP as flag had side effects of oom_badness() skipping</span>
<span class="quote">&gt; it, but we can use the same tsk_is_oom_victim instead and relay on the</span>
<span class="quote">&gt; locking in mark_oom_victim you pointed out above instead of the</span>
<span class="quote">&gt; test_and_set_bit of my patch, because current-&gt;mm is already NULL at</span>
<span class="quote">&gt; that point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A race at the light of the above now is, because current-&gt;mm is NULL by the</span>
<span class="quote">&gt; time mmput is called, how can you start the oom_reap_task on a process</span>
<span class="quote">&gt; with current-&gt;mm NULL that called the last mmput and is blocked</span>
<span class="quote">&gt; in exit_aio?</span>

Because we have that mm available. See tsk-&gt;signal-&gt;oom_mm in
oom_reap_task
<span class="quote">
&gt; It looks like no false positive can get fixed until this</span>
<span class="quote">&gt; is solved first because </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Isn&#39;t this enough? If this is enough it avoids other modification to</span>
<span class="quote">&gt; the exit_mmap runtime that looks unnecessary: mm-&gt;mmap = NULL replaced</span>
<span class="quote">&gt; by MMF_OOM_SKIP that has to be set anyway by __mmput later and one</span>
<span class="quote">&gt; unnecessary branch to call the up_write.</span>
<span class="quote">&gt; </span>
[...]
<span class="quote">&gt; diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="quote">&gt; index f19efcf75418..bdab595ce25c 100644</span>
<span class="quote">&gt; --- a/mm/mmap.c</span>
<span class="quote">&gt; +++ b/mm/mmap.c</span>
<span class="quote">&gt; @@ -2993,6 +2993,23 @@ void exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt;  	/* Use -1 here to ensure all VMAs in the mm are unmapped */</span>
<span class="quote">&gt;  	unmap_vmas(&amp;tlb, vma, 0, -1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags);</span>
<span class="quote">&gt; +	if (tsk_is_oom_victim(current)) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Wait for oom_reap_task() to stop working on this</span>
<span class="quote">&gt; +		 * mm. Because MMF_OOM_SKIP is already set before</span>
<span class="quote">&gt; +		 * calling down_read(), oom_reap_task() will not run</span>
<span class="quote">&gt; +		 * on this &quot;mm&quot; post up_write().</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * tsk_is_oom_victim() cannot be set from under us</span>
<span class="quote">&gt; +		 * either because current-&gt;mm is already set to NULL</span>
<span class="quote">&gt; +		 * under task_lock before calling mmput and oom_mm is</span>
<span class="quote">&gt; +		 * set not NULL by the OOM killer only if current-&gt;mm</span>
<span class="quote">&gt; +		 * is found not NULL while holding the task_lock.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, 0, -1);</span>

Yes this will work and it won&#39;t depend on the oom_lock. But isn&#39;t it
just more ugly than simply doing

	if (tsk_is_oom_victim) {
		down_write(&amp;mm-&gt;mmap_sem);
		locked = true;
	}
	free_pgtables(...)
	[...]
	if (locked)
		down_up(&amp;mm-&gt;mmap_sem);

in general I do not like empty locked sections much, to be honest. Now
with the conditional locking my patch looks as follows. It should be
pretty much equivalent to your patch. Would that be acceptable for you
or do you think there is a strong reason to go with yours?
---
From 2198654be88d11efb1f372e8579761f65e219206 Mon Sep 17 00:00:00 2001
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Date: Thu, 27 Jul 2017 08:48:15 +0200
Subject: [PATCH] mm, oom: allow oom reaper to race with exit_mmap

David has noticed that the oom killer might kill additional tasks while
the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks
the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down
to 0. The race is as follows

oom_reap_task				do_exit
					  exit_mm
  __oom_reap_task_mm
					    mmput
					      __mmput
    mmget_not_zero # fails
    						exit_mmap # frees memory
  set_bit(MMF_OOM_SKIP)

The victim is still visible to the OOM killer until it is unhashed.

Currently we try to reduce a risk of this race by taking oom_lock
and wait for out_of_memory sleep while holding the lock to give the
victim some time to exit. This is quite suboptimal approach because
there is no guarantee the victim (especially a large one) will manage
to unmap its address space and free enough memory to the particular oom
domain which needs a memory (e.g. a specific NUMA node).

Fix this problem by allowing __oom_reap_task_mm and __mmput path to
race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed
to run in parallel with other unmappers (hence the mmap_sem for read).

The only tricky part is to exclude page tables tear down and all
operations which modify the address space in the __mmput path. exit_mmap
doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing
really forbids us to use mmap_sem for write, though. In fact we are
already relying on this lock earlier in the __mmput path to synchronize
with ksm and khugepaged.

Take the exclusive mmap_sem when calling free_pgtables and destroying
vmas to sync with __oom_reap_task_mm which take the lock for read. All
other operations can safely race with the parallel unmap.

Changes v1
- bail on null mm-&gt;mmap early as per David Rientjes
- take exclusive mmap_sem in exit_mmap only for oom victims to reduce
  the lock overhead

Reported-by: David Rientjes &lt;rientjes@google.com&gt;
Fixes: 26db62f179d1 (&quot;oom: keep mm of the killed task available&quot;)
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 mm/mmap.c     | 16 ++++++++++++++++
 mm/oom_kill.c | 47 ++++++++---------------------------------------
 2 files changed, 24 insertions(+), 39 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 27, 2017, 2:55 p.m.</div>
<pre class="content">
On Thu, Jul 27, 2017 at 08:50:24AM +0200, Michal Hocko wrote:
<span class="quote">&gt; Yes this will work and it won&#39;t depend on the oom_lock. But isn&#39;t it</span>
<span class="quote">&gt; just more ugly than simply doing</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (tsk_is_oom_victim) {</span>
<span class="quote">&gt; 		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; 		locked = true;</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; 	free_pgtables(...)</span>
<span class="quote">&gt; 	[...]</span>
<span class="quote">&gt; 	if (locked)</span>
<span class="quote">&gt; 		down_up(&amp;mm-&gt;mmap_sem);</span>

To me not doing if (tsk_is_oom...) { down_write; up_write } is by
default a confusing implementation, because it&#39;s not strict and not
strict code is not self documenting and you&#39;ve to think twice of why
you&#39;re doing something the way you&#39;re doing it.

The doubt on what was the point to hold the mmap_sem during
free_pgtables is precisely why I started digging into this issue
because it didn&#39;t look possible you could truly benefit from holding
the mmap_sem during free_pgtables.

I also don&#39;t like having a new invariant that your solution relies on,
that is mm-&gt;mmap = NULL, when we can make just set the MMF_OOM_SKIP a
bit earlier that it gets set anyway and use that to control the other
side of the race.

I like strict code that uses as fewer invariants as possible and that
never holds a lock for any instruction more than it is required (again
purely for self documenting reasons, the CPU won&#39;t notice much one
instruction more or less).

Even with your patch the two branches are unnecessary, that may not be
measurable, but it&#39;s still wasted CPU. It&#39;s all about setting mm-&gt;mmap
before the up_write. In fact my patch should at least put an incremental
unlikely around my single branch added to exit_mmap.

I see the {down_write;up_write} Hugh&#39;s ksm_exit-like as a strict
solution to this issue and I wrote it specifically while trying to
research a way to be more strict because from the start it didn&#39;t look
the holding of the mmap_sem during free_pgtables was necessary.

I&#39;m also fine to drop the oom_lock but I think it can be done
incrementally as it&#39;s a separate issue, my second patch should allow
for it with no adverse side effects.

All I care about is the exit_mmap path because it runs too many times
not to pay deep attention to every bit of it ;).

Thanks,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 28, 2017, 6:23 a.m.</div>
<pre class="content">
On Thu 27-07-17 16:55:59, Andrea Arcangeli wrote:
<span class="quote">&gt; On Thu, Jul 27, 2017 at 08:50:24AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; Yes this will work and it won&#39;t depend on the oom_lock. But isn&#39;t it</span>
<span class="quote">&gt; &gt; just more ugly than simply doing</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (tsk_is_oom_victim) {</span>
<span class="quote">&gt; &gt; 		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; 		locked = true;</span>
<span class="quote">&gt; &gt; 	}</span>
<span class="quote">&gt; &gt; 	free_pgtables(...)</span>
<span class="quote">&gt; &gt; 	[...]</span>
<span class="quote">&gt; &gt; 	if (locked)</span>
<span class="quote">&gt; &gt; 		down_up(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To me not doing if (tsk_is_oom...) { down_write; up_write } is by</span>
<span class="quote">&gt; default a confusing implementation, because it&#39;s not strict and not</span>
<span class="quote">&gt; strict code is not self documenting and you&#39;ve to think twice of why</span>
<span class="quote">&gt; you&#39;re doing something the way you&#39;re doing it.</span>

I disagree. But this is a matter of taste, I guess. I prefer when
locking is explicit and clear about the scope. An empty locked region
used for synchronization is just obscure and you have to think much more
what it means when you can see what the lock actually protects. It is
also less error prone I believe.
<span class="quote">
&gt; The doubt on what was the point to hold the mmap_sem during</span>
<span class="quote">&gt; free_pgtables is precisely why I started digging into this issue</span>
<span class="quote">&gt; because it didn&#39;t look possible you could truly benefit from holding</span>
<span class="quote">&gt; the mmap_sem during free_pgtables.</span>

The point is that you cannot remove page tables while somebody is
walking them. This is enforced in all other paths except for exit which
was kind of special because nobody could race there.
<span class="quote">
&gt; I also don&#39;t like having a new invariant that your solution relies on,</span>
<span class="quote">&gt; that is mm-&gt;mmap = NULL, when we can make just set the MMF_OOM_SKIP a</span>
<span class="quote">&gt; bit earlier that it gets set anyway and use that to control the other</span>
<span class="quote">&gt; side of the race.</span>

Well, again a matter of taste. I prefer making it clear that mm-&gt;mmap is
no longer valid because it points to a freed pointer. Relying on
MMF_OOM_SKIP for the handshake is just abusing the flag for a different
purpose than it was intended.
<span class="quote"> 
&gt; I like strict code that uses as fewer invariants as possible and that</span>
<span class="quote">&gt; never holds a lock for any instruction more than it is required (again</span>
<span class="quote">&gt; purely for self documenting reasons, the CPU won&#39;t notice much one</span>
<span class="quote">&gt; instruction more or less).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Even with your patch the two branches are unnecessary, that may not be</span>
<span class="quote">&gt; measurable, but it&#39;s still wasted CPU. It&#39;s all about setting mm-&gt;mmap</span>
<span class="quote">&gt; before the up_write. In fact my patch should at least put an incremental</span>
<span class="quote">&gt; unlikely around my single branch added to exit_mmap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I see the {down_write;up_write} Hugh&#39;s ksm_exit-like as a strict</span>
<span class="quote">&gt; solution to this issue and I wrote it specifically while trying to</span>
<span class="quote">&gt; research a way to be more strict because from the start it didn&#39;t look</span>
<span class="quote">&gt; the holding of the mmap_sem during free_pgtables was necessary.</span>

While we have discussed that with Hugh he has shown an interest to
actually get rid of this (peculiar) construct which would be possible if
the down_write was implicit in exit_mmap [1].
<span class="quote">
&gt; I&#39;m also fine to drop the oom_lock but I think it can be done</span>
<span class="quote">&gt; incrementally as it&#39;s a separate issue, my second patch should allow</span>
<span class="quote">&gt; for it with no adverse side effects.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; All I care about is the exit_mmap path because it runs too many times</span>
<span class="quote">&gt; not to pay deep attention to every bit of it ;).</span>

Well, all I care about is to fix this over-eager oom killing due to oom
reaper setting MMF_OOM_SKIP too early. That is a regression. I think we
can agree that both proposed solutions are functionally equivalent. I do
not want to push mine too hard so if you _believe_ that yours is really
better feel free to post it for inclusion.

[1] http://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 24e9261bdcc0..822e8860b9d2 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/userfaultfd_k.h&gt;
 #include &lt;linux/moduleparam.h&gt;
 #include &lt;linux/pkeys.h&gt;
<span class="p_add">+#include &lt;linux/oom.h&gt;</span>
 
 #include &lt;linux/uaccess.h&gt;
 #include &lt;asm/cacheflush.h&gt;
<span class="p_chunk">@@ -2967,6 +2968,7 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	unsigned long nr_accounted = 0;
<span class="p_add">+	bool locked = false;</span>
 
 	/* mm&#39;s last user has gone, and its about to be pulled down */
 	mmu_notifier_release(mm);
<span class="p_chunk">@@ -2993,6 +2995,17 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * oom reaper might race with exit_mmap so make sure we won&#39;t free</span>
<span class="p_add">+	 * page tables or unmap VMAs under its feet</span>
<span class="p_add">+	 * Please note that mark_oom_victim is always called under task_lock</span>
<span class="p_add">+	 * with tsk-&gt;mm != NULL checked on !current tasks which synchronizes</span>
<span class="p_add">+	 * with exit_mm and so we cannot race here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (tsk_is_oom_victim(current)) {</span>
<span class="p_add">+		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		locked = true;</span>
<span class="p_add">+	}</span>
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 
<span class="p_chunk">@@ -3005,7 +3018,10 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
<span class="p_add">+	mm-&gt;mmap = NULL;</span>
 	vm_unacct_memory(nr_accounted);
<span class="p_add">+	if (locked)</span>
<span class="p_add">+		up_write(&amp;mm-&gt;mmap_sem);</span>
 }
 
 /* Insert vm structure into process list sorted by address
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f030c1c..b1c96e1910f2 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -470,40 +470,15 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
<span class="p_del">-	bool ret = true;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We have to make sure to not race with the victim exit path</span>
<span class="p_del">-	 * and cause premature new oom victim selection:</span>
<span class="p_del">-	 * __oom_reap_task_mm		exit_mm</span>
<span class="p_del">-	 *   mmget_not_zero</span>
<span class="p_del">-	 *				  mmput</span>
<span class="p_del">-	 *				    atomic_dec_and_test</span>
<span class="p_del">-	 *				  exit_oom_victim</span>
<span class="p_del">-	 *				[...]</span>
<span class="p_del">-	 *				out_of_memory</span>
<span class="p_del">-	 *				  select_bad_process</span>
<span class="p_del">-	 *				    # no TIF_MEMDIE task selects new victim</span>
<span class="p_del">-	 *  unmap_page_range # frees some memory</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mutex_lock(&amp;oom_lock);</span>
 
 	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {
<span class="p_del">-		ret = false;</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		return false;</span>
 	}
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * increase mm_users only after we know we will reap something so</span>
<span class="p_del">-	 * that the mmput_async is called only when we have reaped something</span>
<span class="p_del">-	 * and delayed __mmput doesn&#39;t matter that much</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!mmget_not_zero(mm)) {</span>
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		trace_skip_task_reaping(tsk-&gt;pid);</span>
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/* There is nothing to reap so bail out without signs in the log */</span>
<span class="p_add">+	if (!mm-&gt;mmap)</span>
<span class="p_add">+		goto unlock;</span>
 
 	trace_start_task_reaping(tsk-&gt;pid);
 
<span class="p_chunk">@@ -540,18 +515,12 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Drop our reference but make sure the mmput slow path is called from a</span>
<span class="p_del">-	 * different context because we shouldn&#39;t risk we get stuck there and</span>
<span class="p_del">-	 * put the oom_reaper out of the way.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mmput_async(mm);</span>
 	trace_finish_task_reaping(tsk-&gt;pid);
<span class="p_del">-unlock_oom:</span>
<span class="p_del">-	mutex_unlock(&amp;oom_lock);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
 }
 
 #define MAX_OOM_REAP_RETRIES 10

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



