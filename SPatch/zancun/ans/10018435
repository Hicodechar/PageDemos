
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/1] mm:hugetlbfs: Fix hwpoison reserve accounting - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/1] mm:hugetlbfs: Fix hwpoison reserve accounting</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 19, 2017, 11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171019230007.17043-2-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10018435/mbox/"
   >mbox</a>
|
   <a href="/patch/10018435/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10018435/">/patch/10018435/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	132EA602C8 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 19 Oct 2017 23:00:51 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 060EB28E66
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 19 Oct 2017 23:00:51 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EEBA828E6D; Thu, 19 Oct 2017 23:00:50 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5D1DC28E66
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 19 Oct 2017 23:00:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752229AbdJSXAs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 19 Oct 2017 19:00:48 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:32709 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751003AbdJSXAr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 19 Oct 2017 19:00:47 -0400
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id v9JN0cRw001922
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 19 Oct 2017 23:00:38 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id v9JN0cSB029195
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 19 Oct 2017 23:00:38 GMT
Received: from abhmp0012.oracle.com (abhmp0012.oracle.com [141.146.116.18])
	by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id
	v9JN0aa6032155; Thu, 19 Oct 2017 23:00:36 GMT
Received: from monkey.oracle.com (/174.25.111.33)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 19 Oct 2017 16:00:36 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;, stable@vger.kernel.org
Subject: [PATCH 1/1] mm:hugetlbfs: Fix hwpoison reserve accounting
Date: Thu, 19 Oct 2017 16:00:07 -0700
Message-Id: &lt;20171019230007.17043-2-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.13.6
In-Reply-To: &lt;20171019230007.17043-1-mike.kravetz@oracle.com&gt;
References: &lt;20171019230007.17043-1-mike.kravetz@oracle.com&gt;
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 19, 2017, 11 p.m.</div>
<pre class="content">
Calling madvise(MADV_HWPOISON) on a hugetlbfs page will result in
bad (negative) reserved huge page counts.  This may not happen
immediately, but may happen later when the underlying file is
removed or filesystem unmounted.  For example:
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
HugePages_Total:       1
HugePages_Free:        0
HugePages_Rsvd:    18446744073709551615
HugePages_Surp:        0
Hugepagesize:       2048 kB

In routine hugetlbfs_error_remove_page(), hugetlb_fix_reserve_counts
is called after remove_huge_page.  hugetlb_fix_reserve_counts is
designed to only be called/used only if a failure is returned from
hugetlb_unreserve_pages.  Therefore, call hugetlb_unreserve_pages
as required and only call hugetlb_fix_reserve_counts in the unlikely
event that hugetlb_unreserve_pages returns an error.

Fixes: 78bb920344b8 (&quot;mm: hwpoison: dissolve in-use hugepage in unrecoverable memory error&quot;)
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: Michal Hocko &lt;mhocko@kernel.org&gt;
Cc: Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Cc: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: &lt;stable@vger.kernel.org&gt;
<span class="signed-off-by">Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 fs/hugetlbfs/inode.c | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Oct. 20, 2017, 2:30 a.m.</div>
<pre class="content">
On Thu, Oct 19, 2017 at 04:00:07PM -0700, Mike Kravetz wrote:
<span class="quote">&gt; Calling madvise(MADV_HWPOISON) on a hugetlbfs page will result in</span>
<span class="quote">&gt; bad (negative) reserved huge page counts.  This may not happen</span>
<span class="quote">&gt; immediately, but may happen later when the underlying file is</span>
<span class="quote">&gt; removed or filesystem unmounted.  For example:</span>
<span class="quote">&gt; AnonHugePages:         0 kB</span>
<span class="quote">&gt; ShmemHugePages:        0 kB</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        0</span>
<span class="quote">&gt; HugePages_Rsvd:    18446744073709551615</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In routine hugetlbfs_error_remove_page(), hugetlb_fix_reserve_counts</span>
<span class="quote">&gt; is called after remove_huge_page.  hugetlb_fix_reserve_counts is</span>
<span class="quote">&gt; designed to only be called/used only if a failure is returned from</span>
<span class="quote">&gt; hugetlb_unreserve_pages.  Therefore, call hugetlb_unreserve_pages</span>
<span class="quote">&gt; as required and only call hugetlb_fix_reserve_counts in the unlikely</span>
<span class="quote">&gt; event that hugetlb_unreserve_pages returns an error.</span>

Hi Mike,

Thank you for addressing this. The patch itself looks good to me, but
the reported issue (negative reserve count) doesn&#39;t reproduce in my trial
with v4.14-rc5, so could you share the exact procedure for this issue?

When error handler runs over a huge page, the reserve count is incremented
so I&#39;m not sure why the reserve count goes negative. My operation is like below:

  $ sysctl vm.nr_hugepages=10
  $ grep HugePages_ /proc/meminfo
  HugePages_Total:      10
  HugePages_Free:       10
  HugePages_Rsvd:        0
  HugePages_Surp:        0
  $ ./test_alloc_generic -B hugetlb_file -N1 -L &quot;mmap access memory_error_injection:error_type=madv_hard&quot;  // allocate a 2MB file on hugetlbfs, then madvise(MADV_HWPOISON) on it.
  $ grep HugePages_ /proc/meminfo
  HugePages_Total:      10
  HugePages_Free:        9
  HugePages_Rsvd:        1  // reserve count is incremented
  HugePages_Surp:        0
  $ rm work/hugetlbfs/testfile
  $ grep HugePages_ /proc/meminfo
  HugePages_Total:      10
  HugePages_Free:        9
  HugePages_Rsvd:        0  // reserve count is gone
  HugePages_Surp:        0
  $ /src/linux-dev/tools/vm/page-types -b hwpoison -x // unpoison the huge page
  $ grep HugePages_ /proc/meminfo
  HugePages_Total:      10
  HugePages_Free:       10  // all huge pages are free (back to the beginning)
  HugePages_Rsvd:        0
  HugePages_Surp:        0

Thanks,
Naoya Horiguchi
<span class="quote">
&gt;</span>
<span class="quote">&gt; Fixes: 78bb920344b8 (&quot;mm: hwpoison: dissolve in-use hugepage in unrecoverable memory error&quot;)</span>
<span class="quote">&gt; Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; Cc: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: &lt;stable@vger.kernel.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  fs/hugetlbfs/inode.c | 5 ++++-</span>
<span class="quote">&gt;  1 file changed, 4 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; index 59073e9f01a4..ed113ea17aff 100644</span>
<span class="quote">&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; @@ -842,9 +842,12 @@ static int hugetlbfs_error_remove_page(struct address_space *mapping,</span>
<span class="quote">&gt;  				struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct inode *inode = mapping-&gt;host;</span>
<span class="quote">&gt; +	pgoff_t index = page-&gt;index;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	remove_huge_page(page);</span>
<span class="quote">&gt; -	hugetlb_fix_reserve_counts(inode);</span>
<span class="quote">&gt; +	if (unlikely(hugetlb_unreserve_pages(inode, index, index + 1, 1)))</span>
<span class="quote">&gt; +		hugetlb_fix_reserve_counts(inode);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.13.6</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 20, 2017, 5:49 p.m.</div>
<pre class="content">
On 10/19/2017 07:30 PM, Naoya Horiguchi wrote:
<span class="quote">&gt; On Thu, Oct 19, 2017 at 04:00:07PM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thank you for addressing this. The patch itself looks good to me, but</span>
<span class="quote">&gt; the reported issue (negative reserve count) doesn&#39;t reproduce in my trial</span>
<span class="quote">&gt; with v4.14-rc5, so could you share the exact procedure for this issue?</span>

Sure, but first one question on your test scenario below.
<span class="quote">
&gt; </span>
<span class="quote">&gt; When error handler runs over a huge page, the reserve count is incremented</span>
<span class="quote">&gt; so I&#39;m not sure why the reserve count goes negative.</span>

I&#39;m not sure I follow.  What specific code is incrementing the reserve
count?  
<span class="quote">
&gt;                                                      My operation is like below:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   $ sysctl vm.nr_hugepages=10</span>
<span class="quote">&gt;   $ grep HugePages_ /proc/meminfo</span>
<span class="quote">&gt;   HugePages_Total:      10</span>
<span class="quote">&gt;   HugePages_Free:       10</span>
<span class="quote">&gt;   HugePages_Rsvd:        0</span>
<span class="quote">&gt;   HugePages_Surp:        0</span>
<span class="quote">&gt;   $ ./test_alloc_generic -B hugetlb_file -N1 -L &quot;mmap access memory_error_injection:error_type=madv_hard&quot;  // allocate a 2MB file on hugetlbfs, then madvise(MADV_HWPOISON) on it.</span>
<span class="quote">&gt;   $ grep HugePages_ /proc/meminfo</span>
<span class="quote">&gt;   HugePages_Total:      10</span>
<span class="quote">&gt;   HugePages_Free:        9</span>
<span class="quote">&gt;   HugePages_Rsvd:        1  // reserve count is incremented</span>
<span class="quote">&gt;   HugePages_Surp:        0</span>

This is confusing to me.  I can not create a test where there is a reserve
count after poisoning page.

I tried to recreate your test.  Running unmodified 4.14.0-rc5.

Before test
-----------
HugePages_Total:       1
HugePages_Free:        1
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB

After open(creat) and mmap of 2MB hugetlbfs file
------------------------------------------------
HugePages_Total:       1
HugePages_Free:        1
HugePages_Rsvd:        1
HugePages_Surp:        0
Hugepagesize:       2048 kB

Reserve count is 1 as expected/normal

After madvise(MADV_HWPOISON) of the single huge page in mapping/file
--------------------------------------------------------------------
HugePages_Total:       1
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB

In this case, the reserve (and free) count were decremented.  Note that
before the poison operation the page was not associated with the mapping/
file.  I did not look closely at the code, but assume the madvise may
cause the page to be &#39;faulted in&#39;.

The counts remain the same when the program exits
-------------------------------------------------
HugePages_Total:       1
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB

Remove the file (rm /var/opt/oracle/hugepool/foo)
-------------------------------------------------
HugePages_Total:       1
HugePages_Free:        0
HugePages_Rsvd:    18446744073709551615
HugePages_Surp:        0
Hugepagesize:       2048 kB

I am still confused about how your test maintains a reserve count after
poisoning.  It may be a good idea for you to test my patch with your
test scenario as I can not recreate here.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Oct. 23, 2017, 7:32 a.m.</div>
<pre class="content">
On Fri, Oct 20, 2017 at 10:49:46AM -0700, Mike Kravetz wrote:
<span class="quote">&gt; On 10/19/2017 07:30 PM, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; On Thu, Oct 19, 2017 at 04:00:07PM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Thank you for addressing this. The patch itself looks good to me, but</span>
<span class="quote">&gt; &gt; the reported issue (negative reserve count) doesn&#39;t reproduce in my trial</span>
<span class="quote">&gt; &gt; with v4.14-rc5, so could you share the exact procedure for this issue?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sure, but first one question on your test scenario below.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; When error handler runs over a huge page, the reserve count is incremented</span>
<span class="quote">&gt; &gt; so I&#39;m not sure why the reserve count goes negative.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m not sure I follow.  What specific code is incrementing the reserve</span>
<span class="quote">&gt; count?</span>

The call path is like below:

  hugetlbfs_error_remove_page
    hugetlb_fix_reserve_counts
      hugepage_subpool_get_pages(spool, 1)
        hugetlb_acct_memory(h, 1);
          gather_surplus_pages
            h-&gt;resv_huge_pages += delta;
<span class="quote">
&gt;</span>
<span class="quote">&gt; &gt;                                                      My operation is like below:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;   $ sysctl vm.nr_hugepages=10</span>
<span class="quote">&gt; &gt;   $ grep HugePages_ /proc/meminfo</span>
<span class="quote">&gt; &gt;   HugePages_Total:      10</span>
<span class="quote">&gt; &gt;   HugePages_Free:       10</span>
<span class="quote">&gt; &gt;   HugePages_Rsvd:        0</span>
<span class="quote">&gt; &gt;   HugePages_Surp:        0</span>
<span class="quote">&gt; &gt;   $ ./test_alloc_generic -B hugetlb_file -N1 -L &quot;mmap access memory_error_injection:error_type=madv_hard&quot;  // allocate a 2MB file on hugetlbfs, then madvise(MADV_HWPOISON) on it.</span>
<span class="quote">&gt; &gt;   $ grep HugePages_ /proc/meminfo</span>
<span class="quote">&gt; &gt;   HugePages_Total:      10</span>
<span class="quote">&gt; &gt;   HugePages_Free:        9</span>
<span class="quote">&gt; &gt;   HugePages_Rsvd:        1  // reserve count is incremented</span>
<span class="quote">&gt; &gt;   HugePages_Surp:        0</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is confusing to me.  I can not create a test where there is a reserve</span>
<span class="quote">&gt; count after poisoning page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I tried to recreate your test.  Running unmodified 4.14.0-rc5.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Before test</span>
<span class="quote">&gt; -----------</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        1</span>
<span class="quote">&gt; HugePages_Rsvd:        0</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; After open(creat) and mmap of 2MB hugetlbfs file</span>
<span class="quote">&gt; ------------------------------------------------</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        1</span>
<span class="quote">&gt; HugePages_Rsvd:        1</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reserve count is 1 as expected/normal</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; After madvise(MADV_HWPOISON) of the single huge page in mapping/file</span>
<span class="quote">&gt; --------------------------------------------------------------------</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        0</span>
<span class="quote">&gt; HugePages_Rsvd:        0</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In this case, the reserve (and free) count were decremented.  Note that</span>
<span class="quote">&gt; before the poison operation the page was not associated with the mapping/</span>
<span class="quote">&gt; file.  I did not look closely at the code, but assume the madvise may</span>
<span class="quote">&gt; cause the page to be &#39;faulted in&#39;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The counts remain the same when the program exits</span>
<span class="quote">&gt; -------------------------------------------------</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        0</span>
<span class="quote">&gt; HugePages_Rsvd:        0</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Remove the file (rm /var/opt/oracle/hugepool/foo)</span>
<span class="quote">&gt; -------------------------------------------------</span>
<span class="quote">&gt; HugePages_Total:       1</span>
<span class="quote">&gt; HugePages_Free:        0</span>
<span class="quote">&gt; HugePages_Rsvd:    18446744073709551615</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am still confused about how your test maintains a reserve count after</span>
<span class="quote">&gt; poisoning.  It may be a good idea for you to test my patch with your</span>
<span class="quote">&gt; test scenario as I can not recreate here.</span>

Interestingly, I found that this reproduces if all hugetlb pages are
reserved when poisoning.
Your testing meets the condition, and mine doesn&#39;t.

In gather_surplus_pages() we determine whether we extend hugetlb pool
with surplus pages like below:

    needed = (h-&gt;resv_huge_pages + delta) - h-&gt;free_huge_pages;
    if (needed &lt;= 0) {
            h-&gt;resv_huge_pages += delta;
            return 0;
    }
    ...

needed is 1 if h-&gt;resv_huge_pages == h-&gt;free_huge_pages, and then
the reserve count gets inconsistent.
I confirmed that your patch fixes the issue, so I&#39;m OK with it.
<span class="acked-by">
Acked-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

Thanks,
Naoya Horiguchi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 23, 2017, 6:20 p.m.</div>
<pre class="content">
On 10/23/2017 12:32 AM, Naoya Horiguchi wrote:
<span class="quote">&gt; On Fri, Oct 20, 2017 at 10:49:46AM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt; On 10/19/2017 07:30 PM, Naoya Horiguchi wrote:</span>
<span class="quote">&gt;&gt;&gt; On Thu, Oct 19, 2017 at 04:00:07PM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Thank you for addressing this. The patch itself looks good to me, but</span>
<span class="quote">&gt;&gt;&gt; the reported issue (negative reserve count) doesn&#39;t reproduce in my trial</span>
<span class="quote">&gt;&gt;&gt; with v4.14-rc5, so could you share the exact procedure for this issue?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure, but first one question on your test scenario below.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; When error handler runs over a huge page, the reserve count is incremented</span>
<span class="quote">&gt;&gt;&gt; so I&#39;m not sure why the reserve count goes negative.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not sure I follow.  What specific code is incrementing the reserve</span>
<span class="quote">&gt;&gt; count?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The call path is like below:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   hugetlbfs_error_remove_page</span>
<span class="quote">&gt;     hugetlb_fix_reserve_counts</span>
<span class="quote">&gt;       hugepage_subpool_get_pages(spool, 1)</span>
<span class="quote">&gt;         hugetlb_acct_memory(h, 1);</span>
<span class="quote">&gt;           gather_surplus_pages</span>
<span class="quote">&gt;             h-&gt;resv_huge_pages += delta;</span>
<span class="quote">&gt; </span>

Ah OK.  This is a result of call to hugetlb_fix_reserve_counts which
I believe is incorrect in most instances, and is unlikely to happen 
with my patch.
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt; Remove the file (rm /var/opt/oracle/hugepool/foo)</span>
<span class="quote">&gt;&gt; -------------------------------------------------</span>
<span class="quote">&gt;&gt; HugePages_Total:       1</span>
<span class="quote">&gt;&gt; HugePages_Free:        0</span>
<span class="quote">&gt;&gt; HugePages_Rsvd:    18446744073709551615</span>
<span class="quote">&gt;&gt; HugePages_Surp:        0</span>
<span class="quote">&gt;&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I am still confused about how your test maintains a reserve count after</span>
<span class="quote">&gt;&gt; poisoning.  It may be a good idea for you to test my patch with your</span>
<span class="quote">&gt;&gt; test scenario as I can not recreate here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Interestingly, I found that this reproduces if all hugetlb pages are</span>
<span class="quote">&gt; reserved when poisoning.</span>
<span class="quote">&gt; Your testing meets the condition, and mine doesn&#39;t.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In gather_surplus_pages() we determine whether we extend hugetlb pool</span>
<span class="quote">&gt; with surplus pages like below:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     needed = (h-&gt;resv_huge_pages + delta) - h-&gt;free_huge_pages;</span>
<span class="quote">&gt;     if (needed &lt;= 0) {</span>
<span class="quote">&gt;             h-&gt;resv_huge_pages += delta;</span>
<span class="quote">&gt;             return 0;</span>
<span class="quote">&gt;     }</span>
<span class="quote">&gt;     ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; needed is 1 if h-&gt;resv_huge_pages == h-&gt;free_huge_pages, and then</span>
<span class="quote">&gt; the reserve count gets inconsistent.</span>
<span class="quote">&gt; I confirmed that your patch fixes the issue, so I&#39;m OK with it.</span>

Thanks.  That now makes sense to me.

hugetlb_fix_reserve_counts (which results in gather_surplus_pages being
called), is only designed to be called in the extremely rare cases when
we have free&#39;ed a huge page but are unable to free the reservation entry.

Just curious, when the hugetlb_fix_reserve_counts call was added to
hugetlbfs_error_remove_page, was the intention to preserve the original
reservation?  I remember thinking hard about that for the hole punch
case and came to the conclusion that it was easier and less error prone
to remove the reservation as well.  That will also happen in the error
case with the patch I provided.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Oct. 24, 2017, 12:46 a.m.</div>
<pre class="content">
On Mon, Oct 23, 2017 at 11:20:02AM -0700, Mike Kravetz wrote:
<span class="quote">&gt; On 10/23/2017 12:32 AM, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; On Fri, Oct 20, 2017 at 10:49:46AM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt;&gt; On 10/19/2017 07:30 PM, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; On Thu, Oct 19, 2017 at 04:00:07PM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Thank you for addressing this. The patch itself looks good to me, but</span>
<span class="quote">&gt; &gt;&gt;&gt; the reported issue (negative reserve count) doesn&#39;t reproduce in my trial</span>
<span class="quote">&gt; &gt;&gt;&gt; with v4.14-rc5, so could you share the exact procedure for this issue?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Sure, but first one question on your test scenario below.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; When error handler runs over a huge page, the reserve count is incremented</span>
<span class="quote">&gt; &gt;&gt;&gt; so I&#39;m not sure why the reserve count goes negative.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I&#39;m not sure I follow.  What specific code is incrementing the reserve</span>
<span class="quote">&gt; &gt;&gt; count?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The call path is like below:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   hugetlbfs_error_remove_page</span>
<span class="quote">&gt; &gt;     hugetlb_fix_reserve_counts</span>
<span class="quote">&gt; &gt;       hugepage_subpool_get_pages(spool, 1)</span>
<span class="quote">&gt; &gt;         hugetlb_acct_memory(h, 1);</span>
<span class="quote">&gt; &gt;           gather_surplus_pages</span>
<span class="quote">&gt; &gt;             h-&gt;resv_huge_pages += delta;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah OK.  This is a result of call to hugetlb_fix_reserve_counts which</span>
<span class="quote">&gt; I believe is incorrect in most instances, and is unlikely to happen </span>
<span class="quote">&gt; with my patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Remove the file (rm /var/opt/oracle/hugepool/foo)</span>
<span class="quote">&gt; &gt;&gt; -------------------------------------------------</span>
<span class="quote">&gt; &gt;&gt; HugePages_Total:       1</span>
<span class="quote">&gt; &gt;&gt; HugePages_Free:        0</span>
<span class="quote">&gt; &gt;&gt; HugePages_Rsvd:    18446744073709551615</span>
<span class="quote">&gt; &gt;&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; &gt;&gt; Hugepagesize:       2048 kB</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I am still confused about how your test maintains a reserve count after</span>
<span class="quote">&gt; &gt;&gt; poisoning.  It may be a good idea for you to test my patch with your</span>
<span class="quote">&gt; &gt;&gt; test scenario as I can not recreate here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Interestingly, I found that this reproduces if all hugetlb pages are</span>
<span class="quote">&gt; &gt; reserved when poisoning.</span>
<span class="quote">&gt; &gt; Your testing meets the condition, and mine doesn&#39;t.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In gather_surplus_pages() we determine whether we extend hugetlb pool</span>
<span class="quote">&gt; &gt; with surplus pages like below:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;     needed = (h-&gt;resv_huge_pages + delta) - h-&gt;free_huge_pages;</span>
<span class="quote">&gt; &gt;     if (needed &lt;= 0) {</span>
<span class="quote">&gt; &gt;             h-&gt;resv_huge_pages += delta;</span>
<span class="quote">&gt; &gt;             return 0;</span>
<span class="quote">&gt; &gt;     }</span>
<span class="quote">&gt; &gt;     ...</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; needed is 1 if h-&gt;resv_huge_pages == h-&gt;free_huge_pages, and then</span>
<span class="quote">&gt; &gt; the reserve count gets inconsistent.</span>
<span class="quote">&gt; &gt; I confirmed that your patch fixes the issue, so I&#39;m OK with it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks.  That now makes sense to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hugetlb_fix_reserve_counts (which results in gather_surplus_pages being</span>
<span class="quote">&gt; called), is only designed to be called in the extremely rare cases when</span>
<span class="quote">&gt; we have free&#39;ed a huge page but are unable to free the reservation entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just curious, when the hugetlb_fix_reserve_counts call was added to</span>
<span class="quote">&gt; hugetlbfs_error_remove_page, was the intention to preserve the original</span>
<span class="quote">&gt; reservation? </span>

No, the intention was to remove the reservation of the error hugepage
which was unmapped and isolated from normal hugepage&#39;s lifecycle.
The error hugepage is not freed back to hugepage pool, but it should be
handled in the same manner as freeing from the perspective of reserve count.

When I was writing commit 78bb920344b8, I experienced some reserve count
mismatch, and wrongly borrowed the code from truncation code.
<span class="quote">
&gt; I remember thinking hard about that for the hole punch</span>
<span class="quote">&gt; case and came to the conclusion that it was easier and less error prone</span>
<span class="quote">&gt; to remove the reservation as well.  That will also happen in the error</span>
<span class="quote">&gt; case with the patch I provided.</span>

Yes, hole punching seems sililar to poisoning except that the final destination
of the target page differs. So we can make the same conclusion here.

Thanks,
Naoya Horiguchi
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 59073e9f01a4..ed113ea17aff 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -842,9 +842,12 @@</span> <span class="p_context"> static int hugetlbfs_error_remove_page(struct address_space *mapping,</span>
 				struct page *page)
 {
 	struct inode *inode = mapping-&gt;host;
<span class="p_add">+	pgoff_t index = page-&gt;index;</span>
 
 	remove_huge_page(page);
<span class="p_del">-	hugetlb_fix_reserve_counts(inode);</span>
<span class="p_add">+	if (unlikely(hugetlb_unreserve_pages(inode, index, index + 1, 1)))</span>
<span class="p_add">+		hugetlb_fix_reserve_counts(inode);</span>
<span class="p_add">+</span>
 	return 0;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



