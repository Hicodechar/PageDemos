
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[6/7] mm: convert generic code to 5-level paging - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [6/7] mm: convert generic code to 5-level paging</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 6, 2017, 8:45 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170306204514.1852-7-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9607609/mbox/"
   >mbox</a>
|
   <a href="/patch/9607609/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9607609/">/patch/9607609/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	68F2B6046A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Mar 2017 20:48:44 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6B566283C9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Mar 2017 20:48:44 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5FD432841A; Mon,  6 Mar 2017 20:48:44 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E7211283C9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Mar 2017 20:48:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932580AbdCFUsk (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Mar 2017 15:48:40 -0500
Received: from mga03.intel.com ([134.134.136.65]:65131 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932614AbdCFUrq (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Mar 2017 15:47:46 -0500
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
	by orsmga103.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	06 Mar 2017 12:46:10 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.35,255,1484035200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1138694672&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga002.fm.intel.com with ESMTP; 06 Mar 2017 12:46:06 -0800
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 2401A1D0; Mon,  6 Mar 2017 22:45:31 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCH 6/7] mm: convert generic code to 5-level paging
Date: Mon,  6 Mar 2017 23:45:13 +0300
Message-Id: &lt;20170306204514.1852-7-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170306204514.1852-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170306204514.1852-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - March 6, 2017, 8:45 p.m.</div>
<pre class="content">
Convert all non-architecture-specific code to 5-level paging.

It&#39;s mostly mechanical adding handling one more page table level in
places where we deal with pud_t.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 drivers/misc/sgi-gru/grufault.c |   9 +-
 fs/userfaultfd.c                |   6 +-
 include/asm-generic/pgtable.h   |  48 +++++++++-
 include/linux/hugetlb.h         |   5 +-
 include/linux/kasan.h           |   1 +
 include/linux/mm.h              |  31 ++++--
 lib/ioremap.c                   |  39 +++++++-
 mm/gup.c                        |  46 +++++++--
 mm/huge_memory.c                |   7 +-
 mm/hugetlb.c                    |  29 +++---
 mm/kasan/kasan_init.c           |  35 ++++++-
 mm/memory.c                     | 207 +++++++++++++++++++++++++++++++++-------
 mm/mlock.c                      |   1 +
 mm/mprotect.c                   |  26 ++++-
 mm/mremap.c                     |  13 ++-
 mm/page_vma_mapped.c            |   6 +-
 mm/pagewalk.c                   |  32 ++++++-
 mm/pgtable-generic.c            |   6 ++
 mm/rmap.c                       |   7 +-
 mm/sparse-vmemmap.c             |  22 ++++-
 mm/swapfile.c                   |  26 ++++-
 mm/userfaultfd.c                |  23 +++--
 mm/vmalloc.c                    |  81 ++++++++++++----
 23 files changed, 586 insertions(+), 120 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 8, 2017, 1:57 p.m.</div>
<pre class="content">
On Mon 06-03-17 23:45:13, Kirill A. Shutemov wrote:
<span class="quote">&gt; Convert all non-architecture-specific code to 5-level paging.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s mostly mechanical adding handling one more page table level in</span>
<span class="quote">&gt; places where we deal with pud_t.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>

OK, I haven&#39;t spotted anything major. I am just scratching my head about
the __ARCH_HAS_5LEVEL_HACK leak into kasan_init.c (see below). Why do we
need it?  It looks more than ugly but I am not familiar with kasan so
maybe this is really necessary.

Other than that free to to add
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

The rest of the series look good (as good as all the pte hackery can get
;)) as well.

[...]
<span class="quote">&gt; diff --git a/mm/kasan/kasan_init.c b/mm/kasan/kasan_init.c</span>
<span class="quote">&gt; index 31238dad85fb..7870ad44ee20 100644</span>
<span class="quote">&gt; --- a/mm/kasan/kasan_init.c</span>
<span class="quote">&gt; +++ b/mm/kasan/kasan_init.c</span>
<span class="quote">&gt; @@ -30,6 +30,9 @@</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 4</span>
<span class="quote">&gt; +p4d_t kasan_zero_p4d[PTRS_PER_P4D] __page_aligned_bss;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  #if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt;  pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt;  #endif</span>
[...]
<span class="quote">&gt; @@ -136,8 +157,12 @@ void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
<span class="quote">&gt;  			 * puds,pmds, so pgd_populate(), pud_populate()</span>
<span class="quote">&gt;  			 * is noops.</span>
<span class="quote">&gt;  			 */</span>
<span class="quote">&gt; -			pgd_populate(&amp;init_mm, pgd, lm_alias(kasan_zero_pud));</span>
<span class="quote">&gt; -			pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +#ifndef __ARCH_HAS_5LEVEL_HACK</span>
<span class="quote">&gt; +			pgd_populate(&amp;init_mm, pgd, lm_alias(kasan_zero_p4d));</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +			p4d = p4d_offset(pgd, addr);</span>
<span class="quote">&gt; +			p4d_populate(&amp;init_mm, p4d, lm_alias(kasan_zero_pud));</span>
<span class="quote">&gt; +			pud = pud_offset(p4d, addr);</span>
<span class="quote">&gt;  			pud_populate(&amp;init_mm, pud, lm_alias(kasan_zero_pmd));</span>
<span class="quote">&gt;  			pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt;  			pmd_populate_kernel(&amp;init_mm, pmd, lm_alias(kasan_zero_pte));</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - March 8, 2017, 3:21 p.m.</div>
<pre class="content">
On Wed, Mar 08, 2017 at 02:57:35PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Mon 06-03-17 23:45:13, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; Convert all non-architecture-specific code to 5-level paging.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s mostly mechanical adding handling one more page table level in</span>
<span class="quote">&gt; &gt; places where we deal with pud_t.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, I haven&#39;t spotted anything major. I am just scratching my head about</span>
<span class="quote">&gt; the __ARCH_HAS_5LEVEL_HACK leak into kasan_init.c (see below). Why do we</span>
<span class="quote">&gt; need it?  It looks more than ugly but I am not familiar with kasan so</span>
<span class="quote">&gt; maybe this is really necessary.</span>

Yeah ugly.

kasan_zero_p4d is only defined if we have real page table level. It&#39;s okay
if the page table level is folded properly -- using pgtable-nop4d.h -- in
this case pgd_populate() is nop and we don&#39;t reference kasan_zero_p4d.

With 5level-fixup.h, pgd_populate() is not nop, so we would reference
kasan_zero_p4d and build breaks. We don&#39;t need this as p4d_populate()
would do what we really need in this case.

We can drop the hack once all architectures that support kasan would be
converted to pgtable-nop4d.h -- amd64 and x86 at the moment.

Makes sense?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 9, 2017, 9:54 a.m.</div>
<pre class="content">
On Wed 08-03-17 18:21:30, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Wed, Mar 08, 2017 at 02:57:35PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 06-03-17 23:45:13, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; Convert all non-architecture-specific code to 5-level paging.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It&#39;s mostly mechanical adding handling one more page table level in</span>
<span class="quote">&gt; &gt; &gt; places where we deal with pud_t.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; OK, I haven&#39;t spotted anything major. I am just scratching my head about</span>
<span class="quote">&gt; &gt; the __ARCH_HAS_5LEVEL_HACK leak into kasan_init.c (see below). Why do we</span>
<span class="quote">&gt; &gt; need it?  It looks more than ugly but I am not familiar with kasan so</span>
<span class="quote">&gt; &gt; maybe this is really necessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah ugly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; kasan_zero_p4d is only defined if we have real page table level. It&#39;s okay</span>
<span class="quote">&gt; if the page table level is folded properly -- using pgtable-nop4d.h -- in</span>
<span class="quote">&gt; this case pgd_populate() is nop and we don&#39;t reference kasan_zero_p4d.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With 5level-fixup.h, pgd_populate() is not nop, so we would reference</span>
<span class="quote">&gt; kasan_zero_p4d and build breaks. We don&#39;t need this as p4d_populate()</span>
<span class="quote">&gt; would do what we really need in this case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can drop the hack once all architectures that support kasan would be</span>
<span class="quote">&gt; converted to pgtable-nop4d.h -- amd64 and x86 at the moment.</span>

But those architectures even do not enable kasan
$ git grep &quot;select *HAVE_ARCH_KASAN&quot;
arch/arm64/Kconfig:     select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP &amp;&amp; !(ARM64_16K_PAGES &amp;&amp; ARM64_VA_BITS_48)
arch/x86/Kconfig:       select HAVE_ARCH_KASAN                  if X86_64 &amp;&amp; SPARSEMEM_VMEMMAP

both arm64 and x86 (64b) do compile fine without the ifdef... So I guess
we should be fine without it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 9, 2017, 11:47 a.m.</div>
<pre class="content">
On Thu, Mar 09, 2017 at 10:54:15AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 08-03-17 18:21:30, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Wed, Mar 08, 2017 at 02:57:35PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Mon 06-03-17 23:45:13, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; Convert all non-architecture-specific code to 5-level paging.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; It&#39;s mostly mechanical adding handling one more page table level in</span>
<span class="quote">&gt; &gt; &gt; &gt; places where we deal with pud_t.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; OK, I haven&#39;t spotted anything major. I am just scratching my head about</span>
<span class="quote">&gt; &gt; &gt; the __ARCH_HAS_5LEVEL_HACK leak into kasan_init.c (see below). Why do we</span>
<span class="quote">&gt; &gt; &gt; need it?  It looks more than ugly but I am not familiar with kasan so</span>
<span class="quote">&gt; &gt; &gt; maybe this is really necessary.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yeah ugly.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; kasan_zero_p4d is only defined if we have real page table level. It&#39;s okay</span>
<span class="quote">&gt; &gt; if the page table level is folded properly -- using pgtable-nop4d.h -- in</span>
<span class="quote">&gt; &gt; this case pgd_populate() is nop and we don&#39;t reference kasan_zero_p4d.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; With 5level-fixup.h, pgd_populate() is not nop, so we would reference</span>
<span class="quote">&gt; &gt; kasan_zero_p4d and build breaks. We don&#39;t need this as p4d_populate()</span>
<span class="quote">&gt; &gt; would do what we really need in this case.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We can drop the hack once all architectures that support kasan would be</span>
<span class="quote">&gt; &gt; converted to pgtable-nop4d.h -- amd64 and x86 at the moment.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But those architectures even do not enable kasan</span>
<span class="quote">&gt; $ git grep &quot;select *HAVE_ARCH_KASAN&quot;</span>
<span class="quote">&gt; arch/arm64/Kconfig:     select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP &amp;&amp; !(ARM64_16K_PAGES &amp;&amp; ARM64_VA_BITS_48)</span>
<span class="quote">&gt; arch/x86/Kconfig:       select HAVE_ARCH_KASAN                  if X86_64 &amp;&amp; SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; both arm64 and x86 (64b) do compile fine without the ifdef... So I guess</span>
<span class="quote">&gt; we should be fine without it.</span>

Have you build the image to the final linking? lm_alias() hides the error
until later.

x86-64 allmodconfig without the #ifndef:

  MODPOST vmlinux.o
mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:
(.init.text+0xb72b): undefined reference to `kasan_zero_p4d&#39;
Makefile:983: recipe for target &#39;vmlinux&#39; failed
make: *** [vmlinux] Error 1
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 9, 2017, 12:20 p.m.</div>
<pre class="content">
On Thu 09-03-17 14:47:16, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, Mar 09, 2017 at 10:54:15AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 08-03-17 18:21:30, Kirill A. Shutemov wrote:</span>
[...]
<span class="quote">&gt; &gt; &gt; We can drop the hack once all architectures that support kasan would be</span>
<span class="quote">&gt; &gt; &gt; converted to pgtable-nop4d.h -- amd64 and x86 at the moment.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But those architectures even do not enable kasan</span>
<span class="quote">&gt; &gt; $ git grep &quot;select *HAVE_ARCH_KASAN&quot;</span>
<span class="quote">&gt; &gt; arch/arm64/Kconfig:     select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP &amp;&amp; !(ARM64_16K_PAGES &amp;&amp; ARM64_VA_BITS_48)</span>
<span class="quote">&gt; &gt; arch/x86/Kconfig:       select HAVE_ARCH_KASAN                  if X86_64 &amp;&amp; SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; both arm64 and x86 (64b) do compile fine without the ifdef... So I guess</span>
<span class="quote">&gt; &gt; we should be fine without it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Have you build the image to the final linking? lm_alias() hides the error</span>
<span class="quote">&gt; until later.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; x86-64 allmodconfig without the #ifndef:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   MODPOST vmlinux.o</span>
<span class="quote">&gt; mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:</span>
<span class="quote">&gt; (.init.text+0xb72b): undefined reference to `kasan_zero_p4d&#39;</span>
<span class="quote">&gt; Makefile:983: recipe for target &#39;vmlinux&#39; failed</span>
<span class="quote">&gt; make: *** [vmlinux] Error 1</span>

Interesting
arm64 cross compile:
$ grep CONFIG_KASAN .config
CONFIG_KASAN=y
CONFIG_KASAN_OUTLINE=y
# CONFIG_KASAN_INLINE is not set

Compiling for arm64 with aarch64-linux using gcc 4.9.0
[...]
  LD      vmlinux.o
  MODPOST vmlinux.o
  KSYM    .tmp_kallsyms1.o
  KSYM    .tmp_kallsyms2.o
  LD      vmlinux
  SORTEX  vmlinux
  SYSMAP  System.map

x86_64 crosscompile with the same version to rule out gcc version
changes

$ grep CONFIG_KASAN .config
CONFIG_KASAN_SHADOW_OFFSET=0xdffffc0000000000
CONFIG_KASAN=y
CONFIG_KASAN_OUTLINE=y
# CONFIG_KASAN_INLINE is not set

[...]
  LD      init/built-in.o
  LD      vmlinux.o
  MODPOST vmlinux.o
mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:
(.init.text+0x84e5): undefined reference to `kasan_zero_p4d&#39;
Makefile:983: recipe for target &#39;vmlinux&#39; failed

no idea why arm64 build was OK.

Anyway I am not insisting on removing this ifdef it is just too ugly to
spread __ARCH_HAS_5LEVEL_HACK outside of the arch code. We have few more
in the mm code but those look much more understandable. Maybe a short
comment explaining the ifdef would be better.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 9, 2017, 1:59 p.m.</div>
<pre class="content">
On Thu, Mar 09, 2017 at 01:20:30PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Thu 09-03-17 14:47:16, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Thu, Mar 09, 2017 at 10:54:15AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Wed 08-03-17 18:21:30, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; We can drop the hack once all architectures that support kasan would be</span>
<span class="quote">&gt; &gt; &gt; &gt; converted to pgtable-nop4d.h -- amd64 and x86 at the moment.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; But those architectures even do not enable kasan</span>
<span class="quote">&gt; &gt; &gt; $ git grep &quot;select *HAVE_ARCH_KASAN&quot;</span>
<span class="quote">&gt; &gt; &gt; arch/arm64/Kconfig:     select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP &amp;&amp; !(ARM64_16K_PAGES &amp;&amp; ARM64_VA_BITS_48)</span>
<span class="quote">&gt; &gt; &gt; arch/x86/Kconfig:       select HAVE_ARCH_KASAN                  if X86_64 &amp;&amp; SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; both arm64 and x86 (64b) do compile fine without the ifdef... So I guess</span>
<span class="quote">&gt; &gt; &gt; we should be fine without it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Have you build the image to the final linking? lm_alias() hides the error</span>
<span class="quote">&gt; &gt; until later.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; x86-64 allmodconfig without the #ifndef:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   MODPOST vmlinux.o</span>
<span class="quote">&gt; &gt; mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:</span>
<span class="quote">&gt; &gt; (.init.text+0xb72b): undefined reference to `kasan_zero_p4d&#39;</span>
<span class="quote">&gt; &gt; Makefile:983: recipe for target &#39;vmlinux&#39; failed</span>
<span class="quote">&gt; &gt; make: *** [vmlinux] Error 1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Interesting</span>
<span class="quote">&gt; arm64 cross compile:</span>
<span class="quote">&gt; $ grep CONFIG_KASAN .config</span>
<span class="quote">&gt; CONFIG_KASAN=y</span>
<span class="quote">&gt; CONFIG_KASAN_OUTLINE=y</span>
<span class="quote">&gt; # CONFIG_KASAN_INLINE is not set</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compiling for arm64 with aarch64-linux using gcc 4.9.0</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;   LD      vmlinux.o</span>
<span class="quote">&gt;   MODPOST vmlinux.o</span>
<span class="quote">&gt;   KSYM    .tmp_kallsyms1.o</span>
<span class="quote">&gt;   KSYM    .tmp_kallsyms2.o</span>
<span class="quote">&gt;   LD      vmlinux</span>
<span class="quote">&gt;   SORTEX  vmlinux</span>
<span class="quote">&gt;   SYSMAP  System.map</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; x86_64 crosscompile with the same version to rule out gcc version</span>
<span class="quote">&gt; changes</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; $ grep CONFIG_KASAN .config</span>
<span class="quote">&gt; CONFIG_KASAN_SHADOW_OFFSET=0xdffffc0000000000</span>
<span class="quote">&gt; CONFIG_KASAN=y</span>
<span class="quote">&gt; CONFIG_KASAN_OUTLINE=y</span>
<span class="quote">&gt; # CONFIG_KASAN_INLINE is not set</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;   LD      init/built-in.o</span>
<span class="quote">&gt;   LD      vmlinux.o</span>
<span class="quote">&gt;   MODPOST vmlinux.o</span>
<span class="quote">&gt; mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:</span>
<span class="quote">&gt; (.init.text+0x84e5): undefined reference to `kasan_zero_p4d&#39;</span>
<span class="quote">&gt; Makefile:983: recipe for target &#39;vmlinux&#39; failed</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; no idea why arm64 build was OK.</span>

allmodconfig on amd64 enables 3-level paging, whcih produces nop
pgd_populate().

defconfig + enabling CONFIG_KASAN would trigger the issue:

mm/built-in.o: In function `kasan_populate_zero_shadow&#39;:
/home/kas/linux/la57/mm/kasan/kasan_init.c:161: undefined reference to `kasan_zero_p4d&#39;
/home/kas/linux/la57/mm/kasan/kasan_init.c:161: undefined reference to `kasan_zero_p4d&#39;
Makefile:983: recipe for target &#39;vmlinux&#39; failed
make: *** [vmlinux] Error 1
<span class="quote">
&gt; Anyway I am not insisting on removing this ifdef it is just too ugly to</span>
<span class="quote">&gt; spread __ARCH_HAS_5LEVEL_HACK outside of the arch code. We have few more</span>
<span class="quote">&gt; in the mm code but those look much more understandable. Maybe a short</span>
<span class="quote">&gt; comment explaining the ifdef would be better.</span>

Sure.

Thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/misc/sgi-gru/grufault.c b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">index 6fb773dbcd0c..93be82fc338a 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_chunk">@@ -219,15 +219,20 @@</span> <span class="p_context"> static int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,</span>
 	int write, unsigned long *paddr, int *pageshift)
 {
 	pgd_t *pgdp;
<span class="p_del">-	pmd_t *pmdp;</span>
<span class="p_add">+	p4d_t *p4dp;</span>
 	pud_t *pudp;
<span class="p_add">+	pmd_t *pmdp;</span>
 	pte_t pte;
 
 	pgdp = pgd_offset(vma-&gt;vm_mm, vaddr);
 	if (unlikely(pgd_none(*pgdp)))
 		goto err;
 
<span class="p_del">-	pudp = pud_offset(pgdp, vaddr);</span>
<span class="p_add">+	p4dp = p4d_offset(pgdp, vaddr);</span>
<span class="p_add">+	if (unlikely(p4d_none(*p4dp)))</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+	pudp = pud_offset(p4dp, vaddr);</span>
 	if (unlikely(pud_none(*pudp)))
 		goto err;
 
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 973607df579d..02ce3944d0f5 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -267,6 +267,7 @@</span> <span class="p_context"> static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,</span>
 {
 	struct mm_struct *mm = ctx-&gt;mm;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -277,7 +278,10 @@</span> <span class="p_context"> static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,</span>
 	pgd = pgd_offset(mm, address);
 	if (!pgd_present(*pgd))
 		goto out;
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		goto out;
 	pmd = pmd_offset(pud, address);
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index f4ca23b158b3..1fad160f35de 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -10,9 +10,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/bug.h&gt;
 #include &lt;linux/errno.h&gt;
 
<span class="p_del">-#if 4 - defined(__PAGETABLE_PUD_FOLDED) - defined(__PAGETABLE_PMD_FOLDED) != \</span>
<span class="p_del">-	CONFIG_PGTABLE_LEVELS</span>
<span class="p_del">-#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{PUD,PMD}_FOLDED</span>
<span class="p_add">+#if 5 - defined(__PAGETABLE_P4D_FOLDED) - defined(__PAGETABLE_PUD_FOLDED) - \</span>
<span class="p_add">+	defined(__PAGETABLE_PMD_FOLDED) != CONFIG_PGTABLE_LEVELS</span>
<span class="p_add">+#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{P4D,PUD,PMD}_FOLDED</span>
 #endif
 
 /*
<span class="p_chunk">@@ -424,6 +424,13 @@</span> <span class="p_context"> static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)</span>
 	(__boundary - 1 &lt; (end) - 1)? __boundary: (end);		\
 })
 
<span class="p_add">+#ifndef p4d_addr_end</span>
<span class="p_add">+#define p4d_addr_end(addr, end)						\</span>
<span class="p_add">+({	unsigned long __boundary = ((addr) + P4D_SIZE) &amp; P4D_MASK;	\</span>
<span class="p_add">+	(__boundary - 1 &lt; (end) - 1)? __boundary: (end);		\</span>
<span class="p_add">+})</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef pud_addr_end
 #define pud_addr_end(addr, end)						\
 ({	unsigned long __boundary = ((addr) + PUD_SIZE) &amp; PUD_MASK;	\
<span class="p_chunk">@@ -444,6 +451,7 @@</span> <span class="p_context"> static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)</span>
  * Do the tests inline, but report and clear the bad entry in mm/memory.c.
  */
 void pgd_clear_bad(pgd_t *);
<span class="p_add">+void p4d_clear_bad(p4d_t *);</span>
 void pud_clear_bad(pud_t *);
 void pmd_clear_bad(pmd_t *);
 
<span class="p_chunk">@@ -458,6 +466,17 @@</span> <span class="p_context"> static inline int pgd_none_or_clear_bad(pgd_t *pgd)</span>
 	return 0;
 }
 
<span class="p_add">+static inline int p4d_none_or_clear_bad(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	if (unlikely(p4d_bad(*p4d))) {</span>
<span class="p_add">+		p4d_clear_bad(p4d);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int pud_none_or_clear_bad(pud_t *pud)
 {
 	if (pud_none(*pud))
<span class="p_chunk">@@ -844,11 +863,30 @@</span> <span class="p_context"> static inline int pmd_protnone(pmd_t pmd)</span>
 #endif /* CONFIG_MMU */
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_P4D_FOLDED</span>
<span class="p_add">+int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot);</span>
<span class="p_add">+int p4d_clear_huge(p4d_t *p4d);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int p4d_clear_huge(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* !__PAGETABLE_P4D_FOLDED */</span>
<span class="p_add">+</span>
 int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot);
 int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot);
 int pud_clear_huge(pud_t *pud);
 int pmd_clear_huge(pmd_t *pmd);
 #else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_add">+static inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 static inline int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)
 {
 	return 0;
<span class="p_chunk">@@ -857,6 +895,10 @@</span> <span class="p_context"> static inline int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)</span>
 {
 	return 0;
 }
<span class="p_add">+static inline int p4d_clear_huge(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 static inline int pud_clear_huge(pud_t *pud)
 {
 	return 0;
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 503099d8aada..b857fc8cc2ec 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -122,7 +122,7 @@</span> <span class="p_context"> struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,</span>
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int flags);
 int pmd_huge(pmd_t pmd);
<span class="p_del">-int pud_huge(pud_t pmd);</span>
<span class="p_add">+int pud_huge(pud_t pud);</span>
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
<span class="p_chunk">@@ -197,6 +197,9 @@</span> <span class="p_context"> static inline void __unmap_hugepage_range(struct mmu_gather *tlb,</span>
 #ifndef pgd_huge
 #define pgd_huge(x)	0
 #endif
<span class="p_add">+#ifndef p4d_huge</span>
<span class="p_add">+#define p4d_huge(x)	0</span>
<span class="p_add">+#endif</span>
 
 #ifndef pgd_write
 static inline int pgd_write(pgd_t pgd)
<span class="p_header">diff --git a/include/linux/kasan.h b/include/linux/kasan.h</span>
<span class="p_header">index ceb3fe78a0d3..1c823bef4c15 100644</span>
<span class="p_header">--- a/include/linux/kasan.h</span>
<span class="p_header">+++ b/include/linux/kasan.h</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"> extern unsigned char kasan_zero_page[PAGE_SIZE];</span>
 extern pte_t kasan_zero_pte[PTRS_PER_PTE];
 extern pmd_t kasan_zero_pmd[PTRS_PER_PMD];
 extern pud_t kasan_zero_pud[PTRS_PER_PUD];
<span class="p_add">+extern p4d_t kasan_zero_p4d[PTRS_PER_P4D];</span>
 
 void kasan_populate_zero_shadow(const void *shadow_start,
 				const void *shadow_end);
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index be1fe264eb37..5f01c88f0800 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1560,14 +1560,24 @@</span> <span class="p_context"> static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,</span>
 	return ptep;
 }
 
<span class="p_add">+#ifdef __PAGETABLE_P4D_FOLDED</span>
<span class="p_add">+static inline int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+						unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef __PAGETABLE_PUD_FOLDED
<span class="p_del">-static inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static inline int __pud_alloc(struct mm_struct *mm, p4d_t *p4d,</span>
 						unsigned long address)
 {
 	return 0;
 }
 #else
<span class="p_del">-int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);</span>
<span class="p_add">+int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);</span>
 #endif
 
 #if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)
<span class="p_chunk">@@ -1621,10 +1631,18 @@</span> <span class="p_context"> int __pte_alloc_kernel(pmd_t *pmd, unsigned long address);</span>
 #if defined(CONFIG_MMU) &amp;&amp; !defined(__ARCH_HAS_4LEVEL_HACK)
 
 #ifndef __ARCH_HAS_5LEVEL_HACK
<span class="p_del">-static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
<span class="p_add">+static inline p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+		unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unlikely(pgd_none(*pgd)) &amp;&amp; __p4d_alloc(mm, pgd, address)) ?</span>
<span class="p_add">+		NULL : p4d_offset(pgd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,</span>
<span class="p_add">+		unsigned long address)</span>
 {
<span class="p_del">-	return (unlikely(pgd_none(*pgd)) &amp;&amp; __pud_alloc(mm, pgd, address))?</span>
<span class="p_del">-		NULL: pud_offset(pgd, address);</span>
<span class="p_add">+	return (unlikely(p4d_none(*p4d)) &amp;&amp; __pud_alloc(mm, p4d, address)) ?</span>
<span class="p_add">+		NULL : pud_offset(p4d, address);</span>
 }
 #endif /* !__ARCH_HAS_5LEVEL_HACK */
 
<span class="p_chunk">@@ -2388,7 +2406,8 @@</span> <span class="p_context"> void sparse_mem_maps_populate_node(struct page **map_map,</span>
 
 struct page *sparse_mem_map_populate(unsigned long pnum, int nid);
 pgd_t *vmemmap_pgd_populate(unsigned long addr, int node);
<span class="p_del">-pud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);</span>
<span class="p_add">+p4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);</span>
<span class="p_add">+pud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);</span>
 pmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);
 pte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);
 void *vmemmap_alloc_block(unsigned long size, int node);
<span class="p_header">diff --git a/lib/ioremap.c b/lib/ioremap.c</span>
<span class="p_header">index a3e14ce92a56..4bb30206b942 100644</span>
<span class="p_header">--- a/lib/ioremap.c</span>
<span class="p_header">+++ b/lib/ioremap.c</span>
<span class="p_chunk">@@ -14,6 +14,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
<span class="p_add">+static int __read_mostly ioremap_p4d_capable;</span>
 static int __read_mostly ioremap_pud_capable;
 static int __read_mostly ioremap_pmd_capable;
 static int __read_mostly ioremap_huge_disabled;
<span class="p_chunk">@@ -35,6 +36,11 @@</span> <span class="p_context"> void __init ioremap_huge_init(void)</span>
 	}
 }
 
<span class="p_add">+static inline int ioremap_p4d_enabled(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ioremap_p4d_capable;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int ioremap_pud_enabled(void)
 {
 	return ioremap_pud_capable;
<span class="p_chunk">@@ -46,6 +52,7 @@</span> <span class="p_context"> static inline int ioremap_pmd_enabled(void)</span>
 }
 
 #else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_add">+static inline int ioremap_p4d_enabled(void) { return 0; }</span>
 static inline int ioremap_pud_enabled(void) { return 0; }
 static inline int ioremap_pmd_enabled(void) { return 0; }
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_chunk">@@ -94,14 +101,14 @@</span> <span class="p_context"> static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,</span>
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
 
 	phys_addr -= addr;
<span class="p_del">-	pud = pud_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(&amp;init_mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -120,6 +127,32 @@</span> <span class="p_context"> static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	phys_addr -= addr;</span>
<span class="p_add">+	p4d = p4d_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ioremap_p4d_enabled() &amp;&amp;</span>
<span class="p_add">+		    ((next - addr) == P4D_SIZE) &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(phys_addr + addr, P4D_SIZE)) {</span>
<span class="p_add">+			if (p4d_set_huge(p4d, phys_addr + addr, prot))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ioremap_pud_range(p4d, addr, next, phys_addr + addr, prot))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int ioremap_page_range(unsigned long addr,
 		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
<span class="p_chunk">@@ -135,7 +168,7 @@</span> <span class="p_context"> int ioremap_page_range(unsigned long addr,</span>
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);</span>
<span class="p_add">+		err = ioremap_p4d_range(pgd, addr, next, phys_addr+addr, prot);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 9c047e951aa3..c74bad1bf6e8 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -226,6 +226,7 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 			      unsigned int *page_mask)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -243,8 +244,13 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 	pgd = pgd_offset(mm, address);
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 		return no_page_table(vma, flags);
<span class="p_del">-</span>
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_huge(*p4d));</span>
<span class="p_add">+	if (unlikely(p4d_bad(*p4d)))</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (pud_none(*pud))
 		return no_page_table(vma, flags);
 	if (pud_huge(*pud) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB) {
<span class="p_chunk">@@ -325,6 +331,7 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 		struct page **page)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -338,7 +345,9 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 	else
 		pgd = pgd_offset_gate(mm, address);
 	BUG_ON(pgd_none(*pgd));
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	BUG_ON(p4d_none(*p4d));</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	BUG_ON(pud_none(*pud));
 	pmd = pmd_offset(pud, address);
 	if (pmd_none(*pmd))
<span class="p_chunk">@@ -1400,13 +1409,13 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 	return 1;
 }
 
<span class="p_del">-static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,</span>
 			 int write, struct page **pages, int *nr)
 {
 	unsigned long next;
 	pud_t *pudp;
 
<span class="p_del">-	pudp = pud_offset(&amp;pgd, addr);</span>
<span class="p_add">+	pudp = pud_offset(&amp;p4d, addr);</span>
 	do {
 		pud_t pud = READ_ONCE(*pudp);
 
<span class="p_chunk">@@ -1428,6 +1437,31 @@</span> <span class="p_context"> static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
 	return 1;
 }
 
<span class="p_add">+static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			 int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	p4d_t *p4dp;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4dp = p4d_offset(&amp;pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		p4d_t p4d = READ_ONCE(*p4dp);</span>
<span class="p_add">+</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none(p4d))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		BUILD_BUG_ON(p4d_huge(p4d));</span>
<span class="p_add">+		if (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {</span>
<span class="p_add">+			if (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,</span>
<span class="p_add">+					 P4D_SHIFT, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		} else if (!gup_p4d_range(p4d, addr, next, write, pages, nr))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+	} while (p4dp++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Like get_user_pages_fast() except it&#39;s IRQ-safe in that it won&#39;t fall back to
  * the regular GUP. It will only return non-negative values.
<span class="p_chunk">@@ -1478,7 +1512,7 @@</span> <span class="p_context"> int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
 					 PGDIR_SHIFT, next, write, pages, &amp;nr))
 				break;
<span class="p_del">-		} else if (!gup_pud_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_add">+		} else if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
 			break;
 	} while (pgdp++, addr = next, addr != end);
 	local_irq_restore(flags);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index d36b2af4d1bf..e4766de25709 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2048,6 +2048,7 @@</span> <span class="p_context"> void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,</span>
 		bool freeze, struct page *page)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_chunk">@@ -2055,7 +2056,11 @@</span> <span class="p_context"> void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,</span>
 	if (!pgd_present(*pgd))
 		return;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		return;
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a7aa811b7d14..3d0aab9ee80d 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4555,7 +4555,8 @@</span> <span class="p_context"> pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)</span>
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
 {
 	pgd_t *pgd = pgd_offset(mm, *addr);
<span class="p_del">-	pud_t *pud = pud_offset(pgd, *addr);</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, *addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, *addr);</span>
 
 	BUG_ON(page_count(virt_to_page(ptep)) == 0);
 	if (page_count(virt_to_page(ptep)) == 1)
<span class="p_chunk">@@ -4586,11 +4587,13 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm,</span>
 			unsigned long addr, unsigned long sz)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pte_t *pte = NULL;
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (pud) {
 		if (sz == PUD_SIZE) {
 			pte = (pte_t *)pud;
<span class="p_chunk">@@ -4610,18 +4613,22 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm,</span>
 pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd = NULL;</span>
<span class="p_add">+	pmd_t *pmd;</span>
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	if (pgd_present(*pgd)) {</span>
<span class="p_del">-		pud = pud_offset(pgd, addr);</span>
<span class="p_del">-		if (pud_present(*pud)) {</span>
<span class="p_del">-			if (pud_huge(*pud))</span>
<span class="p_del">-				return (pte_t *)pud;</span>
<span class="p_del">-			pmd = pmd_offset(pud, addr);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!pgd_present(*pgd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
<span class="p_add">+	if (!pud_present(*pud))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	if (pud_huge(*pud))</span>
<span class="p_add">+		return (pte_t *)pud;</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
 	return (pte_t *) pmd;
 }
 
<span class="p_header">diff --git a/mm/kasan/kasan_init.c b/mm/kasan/kasan_init.c</span>
<span class="p_header">index 31238dad85fb..7870ad44ee20 100644</span>
<span class="p_header">--- a/mm/kasan/kasan_init.c</span>
<span class="p_header">+++ b/mm/kasan/kasan_init.c</span>
<span class="p_chunk">@@ -30,6 +30,9 @@</span> <span class="p_context"></span>
  */
 unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;
 
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 4</span>
<span class="p_add">+p4d_t kasan_zero_p4d[PTRS_PER_P4D] __page_aligned_bss;</span>
<span class="p_add">+#endif</span>
 #if CONFIG_PGTABLE_LEVELS &gt; 3
 pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;
 #endif
<span class="p_chunk">@@ -82,10 +85,10 @@</span> <span class="p_context"> static void __init zero_pmd_populate(pud_t *pud, unsigned long addr,</span>
 	} while (pmd++, addr = next, addr != end);
 }
 
<span class="p_del">-static void __init zero_pud_populate(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static void __init zero_pud_populate(p4d_t *p4d, unsigned long addr,</span>
 				unsigned long end)
 {
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	unsigned long next;
 
 	do {
<span class="p_chunk">@@ -107,6 +110,23 @@</span> <span class="p_context"> static void __init zero_pud_populate(pgd_t *pgd, unsigned long addr,</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_add">+static void __init zero_p4d_populate(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+				unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (p4d_none(*p4d)) {</span>
<span class="p_add">+			p4d_populate(&amp;init_mm, p4d,</span>
<span class="p_add">+				early_alloc(PAGE_SIZE, NUMA_NO_NODE));</span>
<span class="p_add">+		}</span>
<span class="p_add">+		zero_pud_populate(p4d, addr, next);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * kasan_populate_zero_shadow - populate shadow memory region with
  *                               kasan_zero_page
<span class="p_chunk">@@ -125,6 +145,7 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 		next = pgd_addr_end(addr, end);
 
 		if (IS_ALIGNED(addr, PGDIR_SIZE) &amp;&amp; end - addr &gt;= PGDIR_SIZE) {
<span class="p_add">+			p4d_t *p4d;</span>
 			pud_t *pud;
 			pmd_t *pmd;
 
<span class="p_chunk">@@ -136,8 +157,12 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 			 * puds,pmds, so pgd_populate(), pud_populate()
 			 * is noops.
 			 */
<span class="p_del">-			pgd_populate(&amp;init_mm, pgd, lm_alias(kasan_zero_pud));</span>
<span class="p_del">-			pud = pud_offset(pgd, addr);</span>
<span class="p_add">+#ifndef __ARCH_HAS_5LEVEL_HACK</span>
<span class="p_add">+			pgd_populate(&amp;init_mm, pgd, lm_alias(kasan_zero_p4d));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+			p4d_populate(&amp;init_mm, p4d, lm_alias(kasan_zero_pud));</span>
<span class="p_add">+			pud = pud_offset(p4d, addr);</span>
 			pud_populate(&amp;init_mm, pud, lm_alias(kasan_zero_pmd));
 			pmd = pmd_offset(pud, addr);
 			pmd_populate_kernel(&amp;init_mm, pmd, lm_alias(kasan_zero_pte));
<span class="p_chunk">@@ -148,6 +173,6 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 			pgd_populate(&amp;init_mm, pgd,
 				early_alloc(PAGE_SIZE, NUMA_NO_NODE));
 		}
<span class="p_del">-		zero_pud_populate(pgd, addr, next);</span>
<span class="p_add">+		zero_p4d_populate(pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index a97a4cec2e1f..7f1c2163b3ce 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -445,7 +445,7 @@</span> <span class="p_context"> static inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,</span>
 	mm_dec_nr_pmds(tlb-&gt;mm);
 }
 
<span class="p_del">-static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
<span class="p_add">+static inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				unsigned long floor, unsigned long ceiling)
 {
<span class="p_chunk">@@ -454,7 +454,7 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 	unsigned long start;
 
 	start = addr;
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -462,6 +462,39 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 		free_pmd_range(tlb, pud, addr, next, floor, ceiling);
 	} while (pud++, addr = next, addr != end);
 
<span class="p_add">+	start &amp;= P4D_MASK;</span>
<span class="p_add">+	if (start &lt; floor)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (ceiling) {</span>
<span class="p_add">+		ceiling &amp;= P4D_MASK;</span>
<span class="p_add">+		if (!ceiling)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (end - 1 &gt; ceiling - 1)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, start);</span>
<span class="p_add">+	p4d_clear(p4d);</span>
<span class="p_add">+	pud_free_tlb(tlb, pud, start);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				unsigned long floor, unsigned long ceiling)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = addr;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		free_pud_range(tlb, p4d, addr, next, floor, ceiling);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
 	start &amp;= PGDIR_MASK;
 	if (start &lt; floor)
 		return;
<span class="p_chunk">@@ -473,9 +506,9 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 	if (end - 1 &gt; ceiling - 1)
 		return;
 
<span class="p_del">-	pud = pud_offset(pgd, start);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, start);</span>
 	pgd_clear(pgd);
<span class="p_del">-	pud_free_tlb(tlb, pud, start);</span>
<span class="p_add">+	p4d_free_tlb(tlb, p4d, start);</span>
 }
 
 /*
<span class="p_chunk">@@ -539,7 +572,7 @@</span> <span class="p_context"> void free_pgd_range(struct mmu_gather *tlb,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		free_pud_range(tlb, pgd, addr, next, floor, ceiling);</span>
<span class="p_add">+		free_p4d_range(tlb, pgd, addr, next, floor, ceiling);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -658,7 +691,8 @@</span> <span class="p_context"> static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,</span>
 			  pte_t pte, struct page *page)
 {
 	pgd_t *pgd = pgd_offset(vma-&gt;vm_mm, addr);
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	pmd_t *pmd = pmd_offset(pud, addr);
 	struct address_space *mapping;
 	pgoff_t index;
<span class="p_chunk">@@ -1023,16 +1057,16 @@</span> <span class="p_context"> static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 }
 
 static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
<span class="p_del">-		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,</span>
<span class="p_add">+		p4d_t *dst_p4d, p4d_t *src_p4d, struct vm_area_struct *vma,</span>
 		unsigned long addr, unsigned long end)
 {
 	pud_t *src_pud, *dst_pud;
 	unsigned long next;
 
<span class="p_del">-	dst_pud = pud_alloc(dst_mm, dst_pgd, addr);</span>
<span class="p_add">+	dst_pud = pud_alloc(dst_mm, dst_p4d, addr);</span>
 	if (!dst_pud)
 		return -ENOMEM;
<span class="p_del">-	src_pud = pud_offset(src_pgd, addr);</span>
<span class="p_add">+	src_pud = pud_offset(src_p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
<span class="p_chunk">@@ -1056,6 +1090,28 @@</span> <span class="p_context"> static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 	return 0;
 }
 
<span class="p_add">+static inline int copy_p4d_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="p_add">+		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *src_p4d, *dst_p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	dst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);</span>
<span class="p_add">+	if (!dst_p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	src_p4d = p4d_offset(src_pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(src_p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (copy_pud_range(dst_mm, src_mm, dst_p4d, src_p4d,</span>
<span class="p_add">+						vma, addr, next))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (dst_p4d++, src_p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		struct vm_area_struct *vma)
 {
<span class="p_chunk">@@ -1111,7 +1167,7 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(src_pgd))
 			continue;
<span class="p_del">-		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,</span>
<span class="p_add">+		if (unlikely(copy_p4d_range(dst_mm, src_mm, dst_pgd, src_pgd,</span>
 					    vma, addr, next))) {
 			ret = -ENOMEM;
 			break;
<span class="p_chunk">@@ -1267,14 +1323,14 @@</span> <span class="p_context"> static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,</span>
 }
 
 static inline unsigned long zap_pud_range(struct mmu_gather *tlb,
<span class="p_del">-				struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				struct vm_area_struct *vma, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				struct zap_details *details)
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_trans_huge(*pud) || pud_devmap(*pud)) {
<span class="p_chunk">@@ -1295,6 +1351,25 @@</span> <span class="p_context"> static inline unsigned long zap_pud_range(struct mmu_gather *tlb,</span>
 	return addr;
 }
 
<span class="p_add">+static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,</span>
<span class="p_add">+				struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				struct zap_details *details)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		next = zap_pud_range(tlb, vma, p4d, addr, next, details);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void unmap_page_range(struct mmu_gather *tlb,
 			     struct vm_area_struct *vma,
 			     unsigned long addr, unsigned long end,
<span class="p_chunk">@@ -1310,7 +1385,7 @@</span> <span class="p_context"> void unmap_page_range(struct mmu_gather *tlb,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		next = zap_pud_range(tlb, vma, pgd, addr, next, details);</span>
<span class="p_add">+		next = zap_p4d_range(tlb, vma, pgd, addr, next, details);</span>
 	} while (pgd++, addr = next, addr != end);
 	tlb_end_vma(tlb, vma);
 }
<span class="p_chunk">@@ -1465,16 +1540,24 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(zap_vma_ptes);</span>
 pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
 			spinlock_t **ptl)
 {
<span class="p_del">-	pgd_t *pgd = pgd_offset(mm, addr);</span>
<span class="p_del">-	pud_t *pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_del">-	if (pud) {</span>
<span class="p_del">-		pmd_t *pmd = pmd_alloc(mm, pud, addr);</span>
<span class="p_del">-		if (pmd) {</span>
<span class="p_del">-			VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="p_del">-			return pte_alloc_map_lock(mm, pmd, addr, ptl);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset(mm, addr);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pmd = pmd_alloc(mm, pud, addr);</span>
<span class="p_add">+	if (!pmd)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="p_add">+	return pte_alloc_map_lock(mm, pmd, addr, ptl);</span>
 }
 
 /*
<span class="p_chunk">@@ -1740,7 +1823,7 @@</span> <span class="p_context"> static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,</span>
 			unsigned long addr, unsigned long end,
 			unsigned long pfn, pgprot_t prot)
 {
<span class="p_chunk">@@ -1748,7 +1831,7 @@</span> <span class="p_context"> static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	unsigned long next;
 
 	pfn -= addr &gt;&gt; PAGE_SHIFT;
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -1760,6 +1843,26 @@</span> <span class="p_context"> static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+			unsigned long addr, unsigned long end,</span>
<span class="p_add">+			unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pfn -= addr &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (remap_pud_range(mm, p4d, addr, next,</span>
<span class="p_add">+				pfn + (addr &gt;&gt; PAGE_SHIFT), prot))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * remap_pfn_range - remap kernel memory to userspace
  * @vma: user vma to map to
<span class="p_chunk">@@ -1816,7 +1919,7 @@</span> <span class="p_context"> int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,</span>
 	flush_cache_range(vma, addr, end);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = remap_pud_range(mm, pgd, addr, next,</span>
<span class="p_add">+		err = remap_p4d_range(mm, pgd, addr, next,</span>
 				pfn + (addr &gt;&gt; PAGE_SHIFT), prot);
 		if (err)
 			break;
<span class="p_chunk">@@ -1932,7 +2035,7 @@</span> <span class="p_context"> static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,</span>
 	return err;
 }
 
<span class="p_del">-static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,</span>
 				     unsigned long addr, unsigned long end,
 				     pte_fn_t fn, void *data)
 {
<span class="p_chunk">@@ -1940,7 +2043,7 @@</span> <span class="p_context"> static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	unsigned long next;
 	int err;
 
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -1952,6 +2055,26 @@</span> <span class="p_context"> static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	return err;
 }
 
<span class="p_add">+static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+				     unsigned long addr, unsigned long end,</span>
<span class="p_add">+				     pte_fn_t fn, void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		err = apply_to_pud_range(mm, p4d, addr, next, fn, data);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Scan a region of virtual memory, filling in page tables as necessary
  * and calling a provided function on each leaf page table.
<span class="p_chunk">@@ -1970,7 +2093,7 @@</span> <span class="p_context"> int apply_to_page_range(struct mm_struct *mm, unsigned long addr,</span>
 	pgd = pgd_offset(mm, addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = apply_to_pud_range(mm, pgd, addr, next, fn, data);</span>
<span class="p_add">+		err = apply_to_p4d_range(mm, pgd, addr, next, fn, data);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_chunk">@@ -3653,11 +3776,15 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	};
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	int ret;
 
 	pgd = pgd_offset(mm, address);
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return VM_FAULT_OOM;</span>
 
<span class="p_del">-	vmf.pud = pud_alloc(mm, pgd, address);</span>
<span class="p_add">+	vmf.pud = pud_alloc(mm, p4d, address);</span>
 	if (!vmf.pud)
 		return VM_FAULT_OOM;
 	if (pud_none(*vmf.pud) &amp;&amp; transparent_hugepage_enabled(vma)) {
<span class="p_chunk">@@ -3784,7 +3911,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(handle_mm_fault);</span>
  * Allocate page upper directory.
  * We&#39;ve already handled the fast-path in-line.
  */
<span class="p_del">-int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
<span class="p_add">+int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)</span>
 {
 	pud_t *new = pud_alloc_one(mm, address);
 	if (!new)
<span class="p_chunk">@@ -3793,10 +3920,17 @@</span> <span class="p_context"> int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
 	smp_wmb(); /* See comment in __pte_alloc */
 
 	spin_lock(&amp;mm-&gt;page_table_lock);
<span class="p_del">-	if (pgd_present(*pgd))		/* Another has populated it */</span>
<span class="p_add">+#ifndef __ARCH_HAS_5LEVEL_HACK</span>
<span class="p_add">+	if (p4d_present(*p4d))		/* Another has populated it */</span>
<span class="p_add">+		pud_free(mm, new);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		p4d_populate(mm, p4d, new);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	if (pgd_present(*p4d))		/* Another has populated it */</span>
 		pud_free(mm, new);
 	else
<span class="p_del">-		pgd_populate(mm, pgd, new);</span>
<span class="p_add">+		pgd_populate(mm, p4d, new);</span>
<span class="p_add">+#endif /* __ARCH_HAS_5LEVEL_HACK */</span>
 	spin_unlock(&amp;mm-&gt;page_table_lock);
 	return 0;
 }
<span class="p_chunk">@@ -3839,6 +3973,7 @@</span> <span class="p_context"> static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,</span>
 		pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *ptep;
<span class="p_chunk">@@ -3847,7 +3982,11 @@</span> <span class="p_context"> static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,</span>
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 		goto out;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
 		goto out;
 
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index 1050511f8b2b..945edac46810 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -380,6 +380,7 @@</span> <span class="p_context"> static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,</span>
 	pte = get_locked_pte(vma-&gt;vm_mm, start,	&amp;ptl);
 	/* Make sure we do not cross the page table boundary */
 	end = pgd_addr_end(start, end);
<span class="p_add">+	end = p4d_addr_end(start, end);</span>
 	end = pud_addr_end(start, end);
 	end = pmd_addr_end(start, end);
 
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 848e946b08e5..8edd0d576254 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -193,14 +193,14 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 }
 
 static inline unsigned long change_pud_range(struct vm_area_struct *vma,
<span class="p_del">-		pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 		pgprot_t newprot, int dirty_accountable, int prot_numa)
 {
 	pud_t *pud;
 	unsigned long next;
 	unsigned long pages = 0;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -212,6 +212,26 @@</span> <span class="p_context"> static inline unsigned long change_pud_range(struct vm_area_struct *vma,</span>
 	return pages;
 }
 
<span class="p_add">+static inline unsigned long change_p4d_range(struct vm_area_struct *vma,</span>
<span class="p_add">+		pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		pgprot_t newprot, int dirty_accountable, int prot_numa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long pages = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		pages += change_pud_range(vma, p4d, addr, next, newprot,</span>
<span class="p_add">+				 dirty_accountable, prot_numa);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return pages;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static unsigned long change_protection_range(struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end, pgprot_t newprot,
 		int dirty_accountable, int prot_numa)
<span class="p_chunk">@@ -230,7 +250,7 @@</span> <span class="p_context"> static unsigned long change_protection_range(struct vm_area_struct *vma,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		pages += change_pud_range(vma, pgd, addr, next, newprot,</span>
<span class="p_add">+		pages += change_p4d_range(vma, pgd, addr, next, newprot,</span>
 				 dirty_accountable, prot_numa);
 	} while (pgd++, addr = next, addr != end);
 
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 8233b0105c82..cd8a1b199ef9 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_chunk">@@ -39,7 +40,11 @@</span> <span class="p_context"> static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)</span>
 	if (pgd_none_or_clear_bad(pgd))
 		return NULL;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	if (pud_none_or_clear_bad(pud))
 		return NULL;
 
<span class="p_chunk">@@ -54,11 +59,15 @@</span> <span class="p_context"> static pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			    unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return NULL;
 
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index a23001a22c15..c4c9def8ffea 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -104,6 +104,7 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	struct mm_struct *mm = pvmw-&gt;vma-&gt;vm_mm;
 	struct page *page = pvmw-&gt;page;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 
 	/* The only possible pmd mapping has been handled on last iteration */
<span class="p_chunk">@@ -133,7 +134,10 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	pgd = pgd_offset(mm, pvmw-&gt;address);
 	if (!pgd_present(*pgd))
 		return false;
<span class="p_del">-	pud = pud_offset(pgd, pvmw-&gt;address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, pvmw-&gt;address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	pud = pud_offset(p4d, pvmw-&gt;address);</span>
 	if (!pud_present(*pud))
 		return false;
 	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 03761577ae86..60f7856e508f 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -69,14 +69,14 @@</span> <span class="p_context"> static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
 	return err;
 }
 
<span class="p_del">-static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 			  struct mm_walk *walk)
 {
 	pud_t *pud;
 	unsigned long next;
 	int err = 0;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
  again:
 		next = pud_addr_end(addr, end);
<span class="p_chunk">@@ -113,6 +113,32 @@</span> <span class="p_context"> static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 	return err;
 }
 
<span class="p_add">+static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			  struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int err = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d)) {</span>
<span class="p_add">+			if (walk-&gt;pte_hole)</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+			if (err)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)</span>
<span class="p_add">+			err = walk_pud_range(p4d, addr, next, walk);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int walk_pgd_range(unsigned long addr, unsigned long end,
 			  struct mm_walk *walk)
 {
<span class="p_chunk">@@ -131,7 +157,7 @@</span> <span class="p_context"> static int walk_pgd_range(unsigned long addr, unsigned long end,</span>
 			continue;
 		}
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
<span class="p_del">-			err = walk_pud_range(pgd, addr, next, walk);</span>
<span class="p_add">+			err = walk_p4d_range(pgd, addr, next, walk);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 4ed5908c65b0..c99d9512a45b 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -22,6 +22,12 @@</span> <span class="p_context"> void pgd_clear_bad(pgd_t *pgd)</span>
 	pgd_clear(pgd);
 }
 
<span class="p_add">+void p4d_clear_bad(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_ERROR(*p4d);</span>
<span class="p_add">+	p4d_clear(p4d);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void pud_clear_bad(pud_t *pud)
 {
 	pud_ERROR(*pud);
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 2da487d6cea8..2984403a2424 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -684,6 +684,7 @@</span> <span class="p_context"> unsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)</span>
 pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd = NULL;
 	pmd_t pmde;
<span class="p_chunk">@@ -692,7 +693,11 @@</span> <span class="p_context"> pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)</span>
 	if (!pgd_present(*pgd))
 		goto out;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		goto out;
 
<span class="p_header">diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c</span>
<span class="p_header">index 574c67b663fe..a56c3989f773 100644</span>
<span class="p_header">--- a/mm/sparse-vmemmap.c</span>
<span class="p_header">+++ b/mm/sparse-vmemmap.c</span>
<span class="p_chunk">@@ -196,9 +196,9 @@</span> <span class="p_context"> pmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)</span>
 	return pmd;
 }
 
<span class="p_del">-pud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)</span>
<span class="p_add">+pud_t * __meminit vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node)</span>
 {
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	if (pud_none(*pud)) {
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
 		if (!p)
<span class="p_chunk">@@ -208,6 +208,18 @@</span> <span class="p_context"> pud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)</span>
 	return pud;
 }
 
<span class="p_add">+p4d_t * __meminit vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		void *p = vmemmap_alloc_block(PAGE_SIZE, node);</span>
<span class="p_add">+		if (!p)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		p4d_populate(&amp;init_mm, p4d, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return p4d;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 pgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)
 {
 	pgd_t *pgd = pgd_offset_k(addr);
<span class="p_chunk">@@ -225,6 +237,7 @@</span> <span class="p_context"> int __meminit vmemmap_populate_basepages(unsigned long start,</span>
 {
 	unsigned long addr = start;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -233,7 +246,10 @@</span> <span class="p_context"> int __meminit vmemmap_populate_basepages(unsigned long start,</span>
 		pgd = vmemmap_pgd_populate(addr, node);
 		if (!pgd)
 			return -ENOMEM;
<span class="p_del">-		pud = vmemmap_pud_populate(pgd, addr, node);</span>
<span class="p_add">+		p4d = vmemmap_p4d_populate(pgd, addr, node);</span>
<span class="p_add">+		if (!p4d)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		pud = vmemmap_pud_populate(p4d, addr, node);</span>
 		if (!pud)
 			return -ENOMEM;
 		pmd = vmemmap_pmd_populate(pud, addr, node);
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 521ef9b6064f..178130880b90 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1517,7 +1517,7 @@</span> <span class="p_context"> static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+static inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
 {
<span class="p_chunk">@@ -1525,7 +1525,7 @@</span> <span class="p_context"> static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
 	unsigned long next;
 	int ret;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -1537,6 +1537,26 @@</span> <span class="p_context"> static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				swp_entry_t entry, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		ret = unuse_pud_range(vma, p4d, addr, next, entry, page);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int unuse_vma(struct vm_area_struct *vma,
 				swp_entry_t entry, struct page *page)
 {
<span class="p_chunk">@@ -1560,7 +1580,7 @@</span> <span class="p_context"> static int unuse_vma(struct vm_area_struct *vma,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		ret = unuse_pud_range(vma, pgd, addr, next, entry, page);</span>
<span class="p_add">+		ret = unuse_p4d_range(vma, pgd, addr, next, entry, page);</span>
 		if (ret)
 			return ret;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index 479e631d43c2..8bcb501bce60 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -128,19 +128,22 @@</span> <span class="p_context"> static int mfill_zeropage_pte(struct mm_struct *dst_mm,</span>
 static pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd = NULL;</span>
 
 	pgd = pgd_offset(mm, address);
<span class="p_del">-	pud = pud_alloc(mm, pgd, address);</span>
<span class="p_del">-	if (pud)</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Note that we didn&#39;t run this because the pmd was</span>
<span class="p_del">-		 * missing, the *pmd may be already established and in</span>
<span class="p_del">-		 * turn it may also be a trans_huge_pmd.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = pmd_alloc(mm, pud, address);</span>
<span class="p_del">-	return pmd;</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, address);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note that we didn&#39;t run this because the pmd was</span>
<span class="p_add">+	 * missing, the *pmd may be already established and in</span>
<span class="p_add">+	 * turn it may also be a trans_huge_pmd.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return pmd_alloc(mm, pud, address);</span>
 }
 
 #ifdef CONFIG_HUGETLB_PAGE
<span class="p_header">diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="p_header">index b4024d688f38..0dd80222b20b 100644</span>
<span class="p_header">--- a/mm/vmalloc.c</span>
<span class="p_header">+++ b/mm/vmalloc.c</span>
<span class="p_chunk">@@ -86,12 +86,12 @@</span> <span class="p_context"> static void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end)</span>
 	} while (pmd++, addr = next, addr != end);
 }
 
<span class="p_del">-static void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="p_add">+static void vunmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end)</span>
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_clear_huge(pud))
<span class="p_chunk">@@ -102,6 +102,22 @@</span> <span class="p_context"> static void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_add">+static void vunmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_clear_huge(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		vunmap_pud_range(p4d, addr, next);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void vunmap_page_range(unsigned long addr, unsigned long end)
 {
 	pgd_t *pgd;
<span class="p_chunk">@@ -113,7 +129,7 @@</span> <span class="p_context"> static void vunmap_page_range(unsigned long addr, unsigned long end)</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		vunmap_pud_range(pgd, addr, next);</span>
<span class="p_add">+		vunmap_p4d_range(pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -160,13 +176,13 @@</span> <span class="p_context"> static int vmap_pmd_range(pud_t *pud, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_del">-static int vmap_pud_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static int vmap_pud_range(p4d_t *p4d, unsigned long addr,</span>
 		unsigned long end, pgprot_t prot, struct page **pages, int *nr)
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(&amp;init_mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -177,6 +193,23 @@</span> <span class="p_context"> static int vmap_pud_range(pgd_t *pgd, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+static int vmap_p4d_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, pgprot_t prot, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (vmap_pud_range(p4d, addr, next, prot, pages, nr))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Set up page tables in kva (addr, end). The ptes shall have prot &quot;prot&quot;, and
  * will have pfns corresponding to the &quot;pages&quot; array.
<span class="p_chunk">@@ -196,7 +229,7 @@</span> <span class="p_context"> static int vmap_page_range_noflush(unsigned long start, unsigned long end,</span>
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = vmap_pud_range(pgd, addr, next, prot, pages, &amp;nr);</span>
<span class="p_add">+		err = vmap_p4d_range(pgd, addr, next, prot, pages, &amp;nr);</span>
 		if (err)
 			return err;
 	} while (pgd++, addr = next, addr != end);
<span class="p_chunk">@@ -237,6 +270,10 @@</span> <span class="p_context"> struct page *vmalloc_to_page(const void *vmalloc_addr)</span>
 	unsigned long addr = (unsigned long) vmalloc_addr;
 	struct page *page = NULL;
 	pgd_t *pgd = pgd_offset_k(addr);
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *ptep, pte;</span>
 
 	/*
 	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
<span class="p_chunk">@@ -244,21 +281,23 @@</span> <span class="p_context"> struct page *vmalloc_to_page(const void *vmalloc_addr)</span>
 	 */
 	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
 
<span class="p_del">-	if (!pgd_none(*pgd)) {</span>
<span class="p_del">-		pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_del">-		if (!pud_none(*pud)) {</span>
<span class="p_del">-			pmd_t *pmd = pmd_offset(pud, addr);</span>
<span class="p_del">-			if (!pmd_none(*pmd)) {</span>
<span class="p_del">-				pte_t *ptep, pte;</span>
<span class="p_del">-</span>
<span class="p_del">-				ptep = pte_offset_map(pmd, addr);</span>
<span class="p_del">-				pte = *ptep;</span>
<span class="p_del">-				if (pte_present(pte))</span>
<span class="p_del">-					page = pte_page(pte);</span>
<span class="p_del">-				pte_unmap(ptep);</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (pgd_none(*pgd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
<span class="p_add">+	if (pud_none(*pud))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	if (pmd_none(*pmd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = pte_offset_map(pmd, addr);</span>
<span class="p_add">+	pte = *ptep;</span>
<span class="p_add">+	if (pte_present(pte))</span>
<span class="p_add">+		page = pte_page(pte);</span>
<span class="p_add">+	pte_unmap(ptep);</span>
 	return page;
 }
 EXPORT_SYMBOL(vmalloc_to_page);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



