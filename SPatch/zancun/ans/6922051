
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 fixes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 fixes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 1, 2015, 8:44 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150801084456.GA13068@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6922051/mbox/"
   >mbox</a>
|
   <a href="/patch/6922051/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6922051/">/patch/6922051/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 64A459F39B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  1 Aug 2015 08:45:30 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 796EA20549
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  1 Aug 2015 08:45:28 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 579FF2053E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  1 Aug 2015 08:45:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751191AbbHAIpK (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 1 Aug 2015 04:45:10 -0400
Received: from mail-wi0-f182.google.com ([209.85.212.182]:38846 &quot;EHLO
	mail-wi0-f182.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750993AbbHAIpE (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 1 Aug 2015 04:45:04 -0400
Received: by wibxm9 with SMTP id xm9so57711700wib.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sat, 01 Aug 2015 01:45:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version:content-type
	:content-disposition:user-agent;
	bh=rMFDrVrZgxFRJkGQqbgwdjqp4J3PAvzXeLFLCgno1lg=;
	b=nnY8aOU8ge0IeZvFRJ635cU342n95OzDVXGQgWLc56V/cAHIEIe/AiHP1mllZVYuwF
	qLWRRzMzgSBzgT1B6vvTxq7AqQF9EmSy6Uqj7riplFcTcC3pq6LtJ4UvaPdPgUcZcelo
	VKQtFHLiFVQueuMeVBAgAm3FrW8wyDOyNTUapwYBZj5Qy2IWY62ORGX4Oa4swRww9HHN
	LAtWL/PS2mkMQ7HmsQf3ux2LahZv1Q1naGki/wuiRYK3Q3HqqWLjI+pIYNGXztNn31gG
	KG3NcuvBfncD8utWPPwU2MFcXjU2yDpKBwYr1kaw0SDhS5NNmQ3ZKf8Cje5pfqFffw3F
	H7DA==
X-Received: by 10.181.12.40 with SMTP id en8mr16106323wid.10.1438418702448; 
	Sat, 01 Aug 2015 01:45:02 -0700 (PDT)
Received: from gmail.com (54033495.catv.pool.telekom.hu. [84.3.52.149])
	by smtp.gmail.com with ESMTPSA id
	dn8sm2009956wib.12.2015.08.01.01.44.58
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Sat, 01 Aug 2015 01:44:59 -0700 (PDT)
Date: Sat, 1 Aug 2015 10:44:56 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86 fixes
Message-ID: &lt;20150801084456.GA13068@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	FSL_HELO_FAKE,RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Aug. 1, 2015, 8:44 a.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-urgent-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-urgent-for-linus

   # HEAD: 37868fe113ff2ba814b3b4eb12df214df555f8dc x86/ldt: Make modify_ldt synchronous

Fallout from the recent NMI fixes: make x86 LDT handling more robust.
Also some EFI fixes.

  out-of-topic modifications in x86-urgent-for-linus:
  -----------------------------------------------------
  drivers/firmware/efi/efi.c         # 9115c7589b11: efi: Check for NULL efi kern

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (2):
      x86/xen: Probe target addresses in set_aliased_prot() before the hypercall
      x86/ldt: Make modify_ldt synchronous

Dmitry Skorodumov (1):
      x86/efi: Use all 64 bit of efi_memmap in setup_e820()

Jiang Liu (1):
      x86/irq: Use the caller provided polarity setting in mp_check_pin_attr()

Ricardo Neri (1):
      efi: Check for NULL efi kernel parameters


 arch/x86/boot/compressed/eboot.c   |   4 +
 arch/x86/include/asm/desc.h        |  15 ---
 arch/x86/include/asm/mmu.h         |   3 +-
 arch/x86/include/asm/mmu_context.h |  54 +++++++-
 arch/x86/kernel/apic/io_apic.c     |   2 +-
 arch/x86/kernel/cpu/common.c       |   4 +-
 arch/x86/kernel/cpu/perf_event.c   |  12 +-
 arch/x86/kernel/ldt.c              | 262 ++++++++++++++++++++-----------------
 arch/x86/kernel/process_64.c       |   4 +-
 arch/x86/kernel/step.c             |   6 +-
 arch/x86/platform/efi/efi.c        |   5 +
 arch/x86/power/cpu.c               |   3 +-
 arch/x86/xen/enlighten.c           |  40 ++++++
 drivers/firmware/efi/efi.c         |   5 +
 14 files changed, 265 insertions(+), 154 deletions(-)

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/boot/compressed/eboot.c b/arch/x86/boot/compressed/eboot.c</span>
<span class="p_header">index 2c82bd150d43..7d69afd8b6fa 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/eboot.c</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/eboot.c</span>
<span class="p_chunk">@@ -1193,6 +1193,10 @@</span> <span class="p_context"> static efi_status_t setup_e820(struct boot_params *params,</span>
 		unsigned int e820_type = 0;
 		unsigned long m = efi-&gt;efi_memmap;
 
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		m |= (u64)efi-&gt;efi_memmap_hi &lt;&lt; 32;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 		d = (efi_memory_desc_t *)(m + (i * efi-&gt;efi_memdesc_size));
 		switch (d-&gt;type) {
 		case EFI_RESERVED_TYPE:
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index a0bf89fd2647..4e10d73cf018 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -280,21 +280,6 @@</span> <span class="p_context"> static inline void clear_LDT(void)</span>
 	set_ldt(NULL, 0);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * load one particular LDT into the current CPU</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void load_LDT_nolock(mm_context_t *pc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt, pc-&gt;size);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void load_LDT(mm_context_t *pc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	load_LDT_nolock(pc);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline unsigned long get_desc_base(const struct desc_struct *desc)
 {
 	return (unsigned)(desc-&gt;base0 | ((desc-&gt;base1) &lt;&lt; 16) | ((desc-&gt;base2) &lt;&lt; 24));
<span class="p_header">diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="p_header">index 09b9620a73b4..364d27481a52 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -9,8 +9,7 @@</span> <span class="p_context"></span>
  * we put the segment information here.
  */
 typedef struct {
<span class="p_del">-	void *ldt;</span>
<span class="p_del">-	int size;</span>
<span class="p_add">+	struct ldt_struct *ldt;</span>
 
 #ifdef CONFIG_X86_64
 	/* True if mm supports a task running in 32 bit compatibility mode. */
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 804a3a6030ca..984abfe47edc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -34,6 +34,50 @@</span> <span class="p_context"> static inline void load_mm_cr4(struct mm_struct *mm) {}</span>
 #endif
 
 /*
<span class="p_add">+ * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="p_add">+ * modified while live.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct ldt_struct {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Xen requires page-aligned LDTs with special permissions.  This is</span>
<span class="p_add">+	 * needed to prevent us from installing evil descriptors such as</span>
<span class="p_add">+	 * call gates.  On native, we could merge the ldt_struct and LDT</span>
<span class="p_add">+	 * allocations, but it&#39;s not worth trying to optimize.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct desc_struct *entries;</span>
<span class="p_add">+	int size;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void load_mm_ldt(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct ldt_struct *ldt;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* lockless_dereference synchronizes with smp_store_release */</span>
<span class="p_add">+	ldt = lockless_dereference(mm-&gt;context.ldt);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any change to mm-&gt;context.ldt is followed by an IPI to all</span>
<span class="p_add">+	 * CPUs with the mm active.  The LDT will not be freed until</span>
<span class="p_add">+	 * after the IPI is handled by all such CPUs.  This means that,</span>
<span class="p_add">+	 * if the ldt_struct changes before we return, the values we see</span>
<span class="p_add">+	 * will be safe, and the new values will be loaded before we run</span>
<span class="p_add">+	 * any user code.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * NB: don&#39;t try to convert this to use RCU without extreme care.</span>
<span class="p_add">+	 * We would still need IRQs off, because we don&#39;t want to change</span>
<span class="p_add">+	 * the local LDT after an IPI loaded a newer value than the one</span>
<span class="p_add">+	 * that we can see.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(ldt))</span>
<span class="p_add">+		set_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		clear_LDT();</span>
<span class="p_add">+</span>
<span class="p_add">+	DEBUG_LOCKS_WARN_ON(preemptible());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Used for LDT copy/destruction.
  */
 int init_new_context(struct task_struct *tsk, struct mm_struct *mm);
<span class="p_chunk">@@ -78,12 +122,12 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 		 * was called and then modify_ldt changed
 		 * prev-&gt;context.ldt but suppressed an IPI to this CPU.
 		 * In this case, prev-&gt;context.ldt != NULL, because we
<span class="p_del">-		 * never free an LDT while the mm still exists.  That</span>
<span class="p_del">-		 * means that next-&gt;context.ldt != prev-&gt;context.ldt,</span>
<span class="p_del">-		 * because mms never share an LDT.</span>
<span class="p_add">+		 * never set context.ldt to NULL while the mm still</span>
<span class="p_add">+		 * exists.  That means that next-&gt;context.ldt !=</span>
<span class="p_add">+		 * prev-&gt;context.ldt, because mms never share an LDT.</span>
 		 */
 		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))
<span class="p_del">-			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
 	}
 #ifdef CONFIG_SMP
 	  else {
<span class="p_chunk">@@ -106,7 +150,7 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 			load_cr3(next-&gt;pgd);
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 			load_mm_cr4(next);
<span class="p_del">-			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
 		}
 	}
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_header">index 845dc0df2002..206052e55517 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/io_apic.c</span>
<span class="p_chunk">@@ -943,7 +943,7 @@</span> <span class="p_context"> static bool mp_check_pin_attr(int irq, struct irq_alloc_info *info)</span>
 	 */
 	if (irq &lt; nr_legacy_irqs() &amp;&amp; data-&gt;count == 1) {
 		if (info-&gt;ioapic_trigger != data-&gt;trigger)
<span class="p_del">-			mp_register_handler(irq, data-&gt;trigger);</span>
<span class="p_add">+			mp_register_handler(irq, info-&gt;ioapic_trigger);</span>
 		data-&gt;entry.trigger = data-&gt;trigger = info-&gt;ioapic_trigger;
 		data-&gt;entry.polarity = data-&gt;polarity = info-&gt;ioapic_polarity;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 922c5e0cea4c..cb9e5df42dd2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1410,7 +1410,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	load_sp0(t, &amp;current-&gt;thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
<span class="p_del">-	load_LDT(&amp;init_mm.context);</span>
<span class="p_add">+	load_mm_ldt(&amp;init_mm);</span>
 
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
<span class="p_chunk">@@ -1459,7 +1459,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	load_sp0(t, thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
<span class="p_del">-	load_LDT(&amp;init_mm.context);</span>
<span class="p_add">+	load_mm_ldt(&amp;init_mm);</span>
 
 	t-&gt;x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">index 3658de47900f..9469dfa55607 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_chunk">@@ -2179,21 +2179,25 @@</span> <span class="p_context"> static unsigned long get_segment_base(unsigned int segment)</span>
 	int idx = segment &gt;&gt; 3;
 
 	if ((segment &amp; SEGMENT_TI_MASK) == SEGMENT_LDT) {
<span class="p_add">+		struct ldt_struct *ldt;</span>
<span class="p_add">+</span>
 		if (idx &gt; LDT_ENTRIES)
 			return 0;
 
<span class="p_del">-		if (idx &gt; current-&gt;active_mm-&gt;context.size)</span>
<span class="p_add">+		/* IRQs are off, so this synchronizes with smp_store_release */</span>
<span class="p_add">+		ldt = lockless_dereference(current-&gt;active_mm-&gt;context.ldt);</span>
<span class="p_add">+		if (!ldt || idx &gt; ldt-&gt;size)</span>
 			return 0;
 
<span class="p_del">-		desc = current-&gt;active_mm-&gt;context.ldt;</span>
<span class="p_add">+		desc = &amp;ldt-&gt;entries[idx];</span>
 	} else {
 		if (idx &gt; GDT_ENTRIES)
 			return 0;
 
<span class="p_del">-		desc = raw_cpu_ptr(gdt_page.gdt);</span>
<span class="p_add">+		desc = raw_cpu_ptr(gdt_page.gdt) + idx;</span>
 	}
 
<span class="p_del">-	return get_desc_base(desc + idx);</span>
<span class="p_add">+	return get_desc_base(desc);</span>
 }
 
 #ifdef CONFIG_COMPAT
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index c37886d759cc..2bcc0525f1c1 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/string.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/smp.h&gt;
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/uaccess.h&gt;
 
<span class="p_chunk">@@ -20,82 +21,82 @@</span> <span class="p_context"></span>
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
 
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_add">+/* context.lock is held for us, so we don&#39;t need any locking. */</span>
 static void flush_ldt(void *current_mm)
 {
<span class="p_del">-	if (current-&gt;active_mm == current_mm)</span>
<span class="p_del">-		load_LDT(&amp;current-&gt;active_mm-&gt;context);</span>
<span class="p_add">+	mm_context_t *pc;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (current-&gt;active_mm != current_mm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pc = &amp;current-&gt;active_mm-&gt;context;</span>
<span class="p_add">+	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;size);</span>
 }
<span class="p_del">-#endif</span>
 
<span class="p_del">-static int alloc_ldt(mm_context_t *pc, int mincount, int reload)</span>
<span class="p_add">+/* The caller must call finalize_ldt_struct on the result. LDT starts zeroed. */</span>
<span class="p_add">+static struct ldt_struct *alloc_ldt_struct(int size)</span>
 {
<span class="p_del">-	void *oldldt, *newldt;</span>
<span class="p_del">-	int oldsize;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (mincount &lt;= pc-&gt;size)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	oldsize = pc-&gt;size;</span>
<span class="p_del">-	mincount = (mincount + (PAGE_SIZE / LDT_ENTRY_SIZE - 1)) &amp;</span>
<span class="p_del">-			(~(PAGE_SIZE / LDT_ENTRY_SIZE - 1));</span>
<span class="p_del">-	if (mincount * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-		newldt = vmalloc(mincount * LDT_ENTRY_SIZE);</span>
<span class="p_add">+	struct ldt_struct *new_ldt;</span>
<span class="p_add">+	int alloc_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (size &gt; LDT_ENTRIES)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	new_ldt = kmalloc(sizeof(struct ldt_struct), GFP_KERNEL);</span>
<span class="p_add">+	if (!new_ldt)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(LDT_ENTRY_SIZE != sizeof(struct desc_struct));</span>
<span class="p_add">+	alloc_size = size * LDT_ENTRY_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Xen is very picky: it requires a page-aligned LDT that has no</span>
<span class="p_add">+	 * trailing nonzero bytes in any page that contains LDT descriptors.</span>
<span class="p_add">+	 * Keep it simple: zero the whole allocation and never allocate less</span>
<span class="p_add">+	 * than PAGE_SIZE.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (alloc_size &gt; PAGE_SIZE)</span>
<span class="p_add">+		new_ldt-&gt;entries = vzalloc(alloc_size);</span>
 	else
<span class="p_del">-		newldt = (void *)__get_free_page(GFP_KERNEL);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!newldt)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+		new_ldt-&gt;entries = kzalloc(PAGE_SIZE, GFP_KERNEL);</span>
 
<span class="p_del">-	if (oldsize)</span>
<span class="p_del">-		memcpy(newldt, pc-&gt;ldt, oldsize * LDT_ENTRY_SIZE);</span>
<span class="p_del">-	oldldt = pc-&gt;ldt;</span>
<span class="p_del">-	memset(newldt + oldsize * LDT_ENTRY_SIZE, 0,</span>
<span class="p_del">-	       (mincount - oldsize) * LDT_ENTRY_SIZE);</span>
<span class="p_add">+	if (!new_ldt-&gt;entries) {</span>
<span class="p_add">+		kfree(new_ldt);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	paravirt_alloc_ldt(newldt, mincount);</span>
<span class="p_add">+	new_ldt-&gt;size = size;</span>
<span class="p_add">+	return new_ldt;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	/* CHECKME: Do we really need this ? */</span>
<span class="p_del">-	wmb();</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	pc-&gt;ldt = newldt;</span>
<span class="p_del">-	wmb();</span>
<span class="p_del">-	pc-&gt;size = mincount;</span>
<span class="p_del">-	wmb();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (reload) {</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-		preempt_disable();</span>
<span class="p_del">-		load_LDT(pc);</span>
<span class="p_del">-		if (!cpumask_equal(mm_cpumask(current-&gt;mm),</span>
<span class="p_del">-				   cpumask_of(smp_processor_id())))</span>
<span class="p_del">-			smp_call_function(flush_ldt, current-&gt;mm, 1);</span>
<span class="p_del">-		preempt_enable();</span>
<span class="p_del">-#else</span>
<span class="p_del">-		load_LDT(pc);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if (oldsize) {</span>
<span class="p_del">-		paravirt_free_ldt(oldldt, oldsize);</span>
<span class="p_del">-		if (oldsize * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-			vfree(oldldt);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			put_page(virt_to_page(oldldt));</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_add">+/* After calling this, the LDT is immutable. */</span>
<span class="p_add">+static void finalize_ldt_struct(struct ldt_struct *ldt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	paravirt_alloc_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
 }
 
<span class="p_del">-static inline int copy_ldt(mm_context_t *new, mm_context_t *old)</span>
<span class="p_add">+/* context.lock is held */</span>
<span class="p_add">+static void install_ldt(struct mm_struct *current_mm,</span>
<span class="p_add">+			struct ldt_struct *ldt)</span>
 {
<span class="p_del">-	int err = alloc_ldt(new, old-&gt;size, 0);</span>
<span class="p_del">-	int i;</span>
<span class="p_add">+	/* Synchronizes with lockless_dereference in load_mm_ldt. */</span>
<span class="p_add">+	smp_store_release(&amp;current_mm-&gt;context.ldt, ldt);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Activate the LDT for all CPUs using current_mm. */</span>
<span class="p_add">+	on_each_cpu_mask(mm_cpumask(current_mm), flush_ldt, current_mm, true);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (err &lt; 0)</span>
<span class="p_del">-		return err;</span>
<span class="p_add">+static void free_ldt_struct(struct ldt_struct *ldt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (likely(!ldt))</span>
<span class="p_add">+		return;</span>
 
<span class="p_del">-	for (i = 0; i &lt; old-&gt;size; i++)</span>
<span class="p_del">-		write_ldt_entry(new-&gt;ldt, i, old-&gt;ldt + i * LDT_ENTRY_SIZE);</span>
<span class="p_del">-	return 0;</span>
<span class="p_add">+	paravirt_free_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="p_add">+	if (ldt-&gt;size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_add">+		vfree(ldt-&gt;entries);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		kfree(ldt-&gt;entries);</span>
<span class="p_add">+	kfree(ldt);</span>
 }
 
 /*
<span class="p_chunk">@@ -104,17 +105,37 @@</span> <span class="p_context"> static inline int copy_ldt(mm_context_t *new, mm_context_t *old)</span>
  */
 int init_new_context(struct task_struct *tsk, struct mm_struct *mm)
 {
<span class="p_add">+	struct ldt_struct *new_ldt;</span>
 	struct mm_struct *old_mm;
 	int retval = 0;
 
 	mutex_init(&amp;mm-&gt;context.lock);
<span class="p_del">-	mm-&gt;context.size = 0;</span>
 	old_mm = current-&gt;mm;
<span class="p_del">-	if (old_mm &amp;&amp; old_mm-&gt;context.size &gt; 0) {</span>
<span class="p_del">-		mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_del">-		retval = copy_ldt(&amp;mm-&gt;context, &amp;old_mm-&gt;context);</span>
<span class="p_del">-		mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_add">+	if (!old_mm) {</span>
<span class="p_add">+		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+		return 0;</span>
 	}
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_add">+	if (!old_mm-&gt;context.ldt) {</span>
<span class="p_add">+		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;size);</span>
<span class="p_add">+	if (!new_ldt) {</span>
<span class="p_add">+		retval = -ENOMEM;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,</span>
<span class="p_add">+	       new_ldt-&gt;size * LDT_ENTRY_SIZE);</span>
<span class="p_add">+	finalize_ldt_struct(new_ldt);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm-&gt;context.ldt = new_ldt;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -125,53 +146,47 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
  */
 void destroy_context(struct mm_struct *mm)
 {
<span class="p_del">-	if (mm-&gt;context.size) {</span>
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-		/* CHECKME: Can this ever happen ? */</span>
<span class="p_del">-		if (mm == current-&gt;active_mm)</span>
<span class="p_del">-			clear_LDT();</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		paravirt_free_ldt(mm-&gt;context.ldt, mm-&gt;context.size);</span>
<span class="p_del">-		if (mm-&gt;context.size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-			vfree(mm-&gt;context.ldt);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			put_page(virt_to_page(mm-&gt;context.ldt));</span>
<span class="p_del">-		mm-&gt;context.size = 0;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	free_ldt_struct(mm-&gt;context.ldt);</span>
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
 }
 
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
<span class="p_del">-	int err;</span>
<span class="p_add">+	int retval;</span>
 	unsigned long size;
 	struct mm_struct *mm = current-&gt;mm;
 
<span class="p_del">-	if (!mm-&gt;context.size)</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm-&gt;context.ldt) {</span>
<span class="p_add">+		retval = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (bytecount &gt; LDT_ENTRY_SIZE * LDT_ENTRIES)
 		bytecount = LDT_ENTRY_SIZE * LDT_ENTRIES;
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	size = mm-&gt;context.size * LDT_ENTRY_SIZE;</span>
<span class="p_add">+	size = mm-&gt;context.ldt-&gt;size * LDT_ENTRY_SIZE;</span>
 	if (size &gt; bytecount)
 		size = bytecount;
 
<span class="p_del">-	err = 0;</span>
<span class="p_del">-	if (copy_to_user(ptr, mm-&gt;context.ldt, size))</span>
<span class="p_del">-		err = -EFAULT;</span>
<span class="p_del">-	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	if (err &lt; 0)</span>
<span class="p_del">-		goto error_return;</span>
<span class="p_add">+	if (copy_to_user(ptr, mm-&gt;context.ldt-&gt;entries, size)) {</span>
<span class="p_add">+		retval = -EFAULT;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (size != bytecount) {
<span class="p_del">-		/* zero-fill the rest */</span>
<span class="p_del">-		if (clear_user(ptr + size, bytecount - size) != 0) {</span>
<span class="p_del">-			err = -EFAULT;</span>
<span class="p_del">-			goto error_return;</span>
<span class="p_add">+		/* Zero-fill the rest and pretend we read bytecount bytes. */</span>
<span class="p_add">+		if (clear_user(ptr + size, bytecount - size)) {</span>
<span class="p_add">+			retval = -EFAULT;</span>
<span class="p_add">+			goto out_unlock;</span>
 		}
 	}
<span class="p_del">-	return bytecount;</span>
<span class="p_del">-error_return:</span>
<span class="p_del">-	return err;</span>
<span class="p_add">+	retval = bytecount;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	return retval;</span>
 }
 
 static int read_default_ldt(void __user *ptr, unsigned long bytecount)
<span class="p_chunk">@@ -195,6 +210,8 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 	struct desc_struct ldt;
 	int error;
 	struct user_desc ldt_info;
<span class="p_add">+	int oldsize, newsize;</span>
<span class="p_add">+	struct ldt_struct *new_ldt, *old_ldt;</span>
 
 	error = -EINVAL;
 	if (bytecount != sizeof(ldt_info))
<span class="p_chunk">@@ -213,34 +230,39 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 			goto out;
 	}
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	if (ldt_info.entry_number &gt;= mm-&gt;context.size) {</span>
<span class="p_del">-		error = alloc_ldt(&amp;current-&gt;mm-&gt;context,</span>
<span class="p_del">-				  ldt_info.entry_number + 1, 1);</span>
<span class="p_del">-		if (error &lt; 0)</span>
<span class="p_del">-			goto out_unlock;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Allow LDTs to be cleared by the user. */</span>
<span class="p_del">-	if (ldt_info.base_addr == 0 &amp;&amp; ldt_info.limit == 0) {</span>
<span class="p_del">-		if (oldmode || LDT_empty(&amp;ldt_info)) {</span>
<span class="p_del">-			memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="p_del">-			goto install;</span>
<span class="p_add">+	if ((oldmode &amp;&amp; !ldt_info.base_addr &amp;&amp; !ldt_info.limit) ||</span>
<span class="p_add">+	    LDT_empty(&amp;ldt_info)) {</span>
<span class="p_add">+		/* The user wants to clear the entry. */</span>
<span class="p_add">+		memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="p_add">+			error = -EINVAL;</span>
<span class="p_add">+			goto out;</span>
 		}
<span class="p_add">+</span>
<span class="p_add">+		fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="p_add">+		if (oldmode)</span>
<span class="p_add">+			ldt.avl = 0;</span>
 	}
 
<span class="p_del">-	if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="p_del">-		error = -EINVAL;</span>
<span class="p_add">+	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	old_ldt = mm-&gt;context.ldt;</span>
<span class="p_add">+	oldsize = old_ldt ? old_ldt-&gt;size : 0;</span>
<span class="p_add">+	newsize = max((int)(ldt_info.entry_number + 1), oldsize);</span>
<span class="p_add">+</span>
<span class="p_add">+	error = -ENOMEM;</span>
<span class="p_add">+	new_ldt = alloc_ldt_struct(newsize);</span>
<span class="p_add">+	if (!new_ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
<span class="p_del">-	fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="p_del">-	if (oldmode)</span>
<span class="p_del">-		ldt.avl = 0;</span>
<span class="p_add">+	if (old_ldt)</span>
<span class="p_add">+		memcpy(new_ldt-&gt;entries, old_ldt-&gt;entries, oldsize * LDT_ENTRY_SIZE);</span>
<span class="p_add">+	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;</span>
<span class="p_add">+	finalize_ldt_struct(new_ldt);</span>
 
<span class="p_del">-	/* Install the new entry ...  */</span>
<span class="p_del">-install:</span>
<span class="p_del">-	write_ldt_entry(mm-&gt;context.ldt, ldt_info.entry_number, &amp;ldt);</span>
<span class="p_add">+	install_ldt(mm, new_ldt);</span>
<span class="p_add">+	free_ldt_struct(old_ldt);</span>
 	error = 0;
 
 out_unlock:
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index 71d7849a07f7..f6b916387590 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -121,11 +121,11 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task-&gt;mm) {
<span class="p_del">-		if (dead_task-&gt;mm-&gt;context.size) {</span>
<span class="p_add">+		if (dead_task-&gt;mm-&gt;context.ldt) {</span>
 			pr_warn(&quot;WARNING: dead process %s still has LDT? &lt;%p/%d&gt;\n&quot;,
 				dead_task-&gt;comm,
 				dead_task-&gt;mm-&gt;context.ldt,
<span class="p_del">-				dead_task-&gt;mm-&gt;context.size);</span>
<span class="p_add">+				dead_task-&gt;mm-&gt;context.ldt-&gt;size);</span>
 			BUG();
 		}
 	}
<span class="p_header">diff --git a/arch/x86/kernel/step.c b/arch/x86/kernel/step.c</span>
<span class="p_header">index 9b4d51d0c0d0..6273324186ac 100644</span>
<span class="p_header">--- a/arch/x86/kernel/step.c</span>
<span class="p_header">+++ b/arch/x86/kernel/step.c</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/ptrace.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
 
 unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)
 {
<span class="p_chunk">@@ -30,10 +31,11 @@</span> <span class="p_context"> unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *re</span>
 		seg &amp;= ~7UL;
 
 		mutex_lock(&amp;child-&gt;mm-&gt;context.lock);
<span class="p_del">-		if (unlikely((seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.size))</span>
<span class="p_add">+		if (unlikely(!child-&gt;mm-&gt;context.ldt ||</span>
<span class="p_add">+			     (seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.ldt-&gt;size))</span>
 			addr = -1L; /* bogus selector, access would fault */
 		else {
<span class="p_del">-			desc = child-&gt;mm-&gt;context.ldt + seg;</span>
<span class="p_add">+			desc = &amp;child-&gt;mm-&gt;context.ldt-&gt;entries[seg];</span>
 			base = get_desc_base(desc);
 
 			/* 16-bit code segment? */
<span class="p_header">diff --git a/arch/x86/platform/efi/efi.c b/arch/x86/platform/efi/efi.c</span>
<span class="p_header">index cfba30f27392..e4308fe6afe8 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi.c</span>
<span class="p_chunk">@@ -972,6 +972,11 @@</span> <span class="p_context"> u64 efi_mem_attributes(unsigned long phys_addr)</span>
 
 static int __init arch_parse_efi_cmdline(char *str)
 {
<span class="p_add">+	if (!str) {</span>
<span class="p_add">+		pr_warn(&quot;need at least one option\n&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (parse_option_str(str, &quot;old_map&quot;))
 		set_bit(EFI_OLD_MEMMAP, &amp;efi.flags);
 	if (parse_option_str(str, &quot;debug&quot;))
<span class="p_header">diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="p_header">index 0d7dd1f5ac36..9ab52791fed5 100644</span>
<span class="p_header">--- a/arch/x86/power/cpu.c</span>
<span class="p_header">+++ b/arch/x86/power/cpu.c</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/fpu/internal.h&gt;
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/cpu.h&gt;
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
 
 #ifdef CONFIG_X86_32
 __visible unsigned long saved_context_ebx;
<span class="p_chunk">@@ -153,7 +154,7 @@</span> <span class="p_context"> static void fix_processor_context(void)</span>
 	syscall_init();				/* This sets MSR_*STAR and related */
 #endif
 	load_TR_desc();				/* This does ltr */
<span class="p_del">-	load_LDT(&amp;current-&gt;active_mm-&gt;context);	/* This does lldt */</span>
<span class="p_add">+	load_mm_ldt(current-&gt;active_mm);	/* This does lldt */</span>
 
 	fpu__resume_cpu();
 }
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index 0b95c9b8283f..11d6fb4e8483 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -483,6 +483,7 @@</span> <span class="p_context"> static void set_aliased_prot(void *v, pgprot_t prot)</span>
 	pte_t pte;
 	unsigned long pfn;
 	struct page *page;
<span class="p_add">+	unsigned char dummy;</span>
 
 	ptep = lookup_address((unsigned long)v, &amp;level);
 	BUG_ON(ptep == NULL);
<span class="p_chunk">@@ -492,6 +493,32 @@</span> <span class="p_context"> static void set_aliased_prot(void *v, pgprot_t prot)</span>
 
 	pte = pfn_pte(pfn, prot);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Careful: update_va_mapping() will fail if the virtual address</span>
<span class="p_add">+	 * we&#39;re poking isn&#39;t populated in the page tables.  We don&#39;t</span>
<span class="p_add">+	 * need to worry about the direct map (that&#39;s always in the page</span>
<span class="p_add">+	 * tables), but we need to be careful about vmap space.  In</span>
<span class="p_add">+	 * particular, the top level page table can lazily propagate</span>
<span class="p_add">+	 * entries between processes, so if we&#39;ve switched mms since we</span>
<span class="p_add">+	 * vmapped the target in the first place, we might not have the</span>
<span class="p_add">+	 * top-level page table entry populated.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We disable preemption because we want the same mm active when</span>
<span class="p_add">+	 * we probe the target and when we issue the hypercall.  We&#39;ll</span>
<span class="p_add">+	 * have the same nominal mm, but if we&#39;re a kernel thread, lazy</span>
<span class="p_add">+	 * mm dropping could change our pgd.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Out of an abundance of caution, this uses __get_user() to fault</span>
<span class="p_add">+	 * in the target address just in case there&#39;s some obscure case</span>
<span class="p_add">+	 * in which the target address isn&#39;t readable.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();	/* Avoid warnings due to being atomic. */</span>
<span class="p_add">+	__get_user(dummy, (unsigned char __user __force *)v);</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+</span>
 	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
 		BUG();
 
<span class="p_chunk">@@ -503,6 +530,8 @@</span> <span class="p_context"> static void set_aliased_prot(void *v, pgprot_t prot)</span>
 				BUG();
 	} else
 		kmap_flush_unused();
<span class="p_add">+</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
 static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)
<span class="p_chunk">@@ -510,6 +539,17 @@</span> <span class="p_context"> static void xen_alloc_ldt(struct desc_struct *ldt, unsigned entries)</span>
 	const unsigned entries_per_page = PAGE_SIZE / LDT_ENTRY_SIZE;
 	int i;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We need to mark the all aliases of the LDT pages RO.  We</span>
<span class="p_add">+	 * don&#39;t need to call vm_flush_aliases(), though, since that&#39;s</span>
<span class="p_add">+	 * only responsible for flushing aliases out the TLBs, not the</span>
<span class="p_add">+	 * page tables, and Xen will flush the TLB for us if needed.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * To avoid confusing future readers: none of this is necessary</span>
<span class="p_add">+	 * to load the LDT.  The hypervisor only checks this when the</span>
<span class="p_add">+	 * LDT is faulted in due to subsequent descriptor access.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
 	for(i = 0; i &lt; entries; i += entries_per_page)
 		set_aliased_prot(ldt + i, PAGE_KERNEL_RO);
 }
<span class="p_header">diff --git a/drivers/firmware/efi/efi.c b/drivers/firmware/efi/efi.c</span>
<span class="p_header">index 9fa8084a7c8d..d6144e3b97c5 100644</span>
<span class="p_header">--- a/drivers/firmware/efi/efi.c</span>
<span class="p_header">+++ b/drivers/firmware/efi/efi.c</span>
<span class="p_chunk">@@ -58,6 +58,11 @@</span> <span class="p_context"> bool efi_runtime_disabled(void)</span>
 
 static int __init parse_efi_cmdline(char *str)
 {
<span class="p_add">+	if (!str) {</span>
<span class="p_add">+		pr_warn(&quot;need at least one option\n&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (parse_option_str(str, &quot;noruntime&quot;))
 		disable_runtime = true;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



