
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv3,06/33] mm: convert generic code to 5-level paging - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv3,06/33] mm: convert generic code to 5-level paging</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 17, 2017, 2:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170217141328.164563-7-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9579899/mbox/"
   >mbox</a>
|
   <a href="/patch/9579899/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9579899/">/patch/9579899/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AA840600C5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:16:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 97823286E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:16:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8C363286E4; Fri, 17 Feb 2017 14:16:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2C205286E3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:16:38 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934534AbdBQOQg (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Feb 2017 09:16:36 -0500
Received: from mga01.intel.com ([192.55.52.88]:36851 &quot;EHLO mga01.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934405AbdBQOOe (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Feb 2017 09:14:34 -0500
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
	by fmsmga101.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	17 Feb 2017 06:14:28 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.35,172,1484035200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1130250289&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga002.fm.intel.com with ESMTP; 17 Feb 2017 06:14:19 -0800
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 9D1AB5F5; Fri, 17 Feb 2017 16:13:44 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv3 06/33] mm: convert generic code to 5-level paging
Date: Fri, 17 Feb 2017 17:13:01 +0300
Message-Id: &lt;20170217141328.164563-7-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170217141328.164563-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170217141328.164563-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Feb. 17, 2017, 2:13 p.m.</div>
<pre class="content">
Convert all non-architecture-specific code to 5-level paging.

It&#39;s mostly mechanical adding handling one more page table level in
places where we deal with pud_t.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 drivers/misc/sgi-gru/grufault.c |   9 +-
 fs/userfaultfd.c                |   6 +-
 include/asm-generic/pgtable.h   |  48 +++++++++-
 include/linux/hugetlb.h         |   5 +-
 include/linux/kasan.h           |   1 +
 include/linux/mm.h              |  31 ++++--
 lib/ioremap.c                   |  39 +++++++-
 mm/gup.c                        |  46 +++++++--
 mm/huge_memory.c                |   7 +-
 mm/hugetlb.c                    |  29 +++---
 mm/kasan/kasan_init.c           |  35 ++++++-
 mm/memory.c                     | 207 +++++++++++++++++++++++++++++++++-------
 mm/mlock.c                      |   1 +
 mm/mprotect.c                   |  26 ++++-
 mm/mremap.c                     |  13 ++-
 mm/pagewalk.c                   |  32 ++++++-
 mm/pgtable-generic.c            |   6 ++
 mm/rmap.c                       |  13 ++-
 mm/sparse-vmemmap.c             |  22 ++++-
 mm/swapfile.c                   |  26 ++++-
 mm/userfaultfd.c                |  23 +++--
 mm/vmalloc.c                    |  81 ++++++++++++----
 22 files changed, 586 insertions(+), 120 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/misc/sgi-gru/grufault.c b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">index 6fb773dbcd0c..93be82fc338a 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_chunk">@@ -219,15 +219,20 @@</span> <span class="p_context"> static int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,</span>
 	int write, unsigned long *paddr, int *pageshift)
 {
 	pgd_t *pgdp;
<span class="p_del">-	pmd_t *pmdp;</span>
<span class="p_add">+	p4d_t *p4dp;</span>
 	pud_t *pudp;
<span class="p_add">+	pmd_t *pmdp;</span>
 	pte_t pte;
 
 	pgdp = pgd_offset(vma-&gt;vm_mm, vaddr);
 	if (unlikely(pgd_none(*pgdp)))
 		goto err;
 
<span class="p_del">-	pudp = pud_offset(pgdp, vaddr);</span>
<span class="p_add">+	p4dp = p4d_offset(pgdp, vaddr);</span>
<span class="p_add">+	if (unlikely(p4d_none(*p4dp)))</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+	pudp = pud_offset(p4dp, vaddr);</span>
 	if (unlikely(pud_none(*pudp)))
 		goto err;
 
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 43953e03c356..84589fca7f5b 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -202,6 +202,7 @@</span> <span class="p_context"> static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,</span>
 {
 	struct mm_struct *mm = ctx-&gt;mm;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -212,7 +213,10 @@</span> <span class="p_context"> static inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,</span>
 	pgd = pgd_offset(mm, address);
 	if (!pgd_present(*pgd))
 		goto out;
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		goto out;
 	pmd = pmd_offset(pud, address);
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index 18af2bcefe6a..dc6a54318cf5 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -10,9 +10,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/bug.h&gt;
 #include &lt;linux/errno.h&gt;
 
<span class="p_del">-#if 4 - defined(__PAGETABLE_PUD_FOLDED) - defined(__PAGETABLE_PMD_FOLDED) != \</span>
<span class="p_del">-	CONFIG_PGTABLE_LEVELS</span>
<span class="p_del">-#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{PUD,PMD}_FOLDED</span>
<span class="p_add">+#if 5 - defined(__PAGETABLE_P4D_FOLDED) - defined(__PAGETABLE_PUD_FOLDED) - \</span>
<span class="p_add">+	defined(__PAGETABLE_PMD_FOLDED) != CONFIG_PGTABLE_LEVELS</span>
<span class="p_add">+#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{P4D,PUD,PMD}_FOLDED</span>
 #endif
 
 /*
<span class="p_chunk">@@ -339,6 +339,13 @@</span> <span class="p_context"> static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)</span>
 	(__boundary - 1 &lt; (end) - 1)? __boundary: (end);		\
 })
 
<span class="p_add">+#ifndef p4d_addr_end</span>
<span class="p_add">+#define p4d_addr_end(addr, end)						\</span>
<span class="p_add">+({	unsigned long __boundary = ((addr) + P4D_SIZE) &amp; P4D_MASK;	\</span>
<span class="p_add">+	(__boundary - 1 &lt; (end) - 1)? __boundary: (end);		\</span>
<span class="p_add">+})</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef pud_addr_end
 #define pud_addr_end(addr, end)						\
 ({	unsigned long __boundary = ((addr) + PUD_SIZE) &amp; PUD_MASK;	\
<span class="p_chunk">@@ -359,6 +366,7 @@</span> <span class="p_context"> static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)</span>
  * Do the tests inline, but report and clear the bad entry in mm/memory.c.
  */
 void pgd_clear_bad(pgd_t *);
<span class="p_add">+void p4d_clear_bad(p4d_t *);</span>
 void pud_clear_bad(pud_t *);
 void pmd_clear_bad(pmd_t *);
 
<span class="p_chunk">@@ -373,6 +381,17 @@</span> <span class="p_context"> static inline int pgd_none_or_clear_bad(pgd_t *pgd)</span>
 	return 0;
 }
 
<span class="p_add">+static inline int p4d_none_or_clear_bad(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	if (unlikely(p4d_bad(*p4d))) {</span>
<span class="p_add">+		p4d_clear_bad(p4d);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int pud_none_or_clear_bad(pud_t *pud)
 {
 	if (pud_none(*pud))
<span class="p_chunk">@@ -750,11 +769,30 @@</span> <span class="p_context"> static inline int pmd_protnone(pmd_t pmd)</span>
 #endif /* CONFIG_MMU */
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_P4D_FOLDED</span>
<span class="p_add">+int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot);</span>
<span class="p_add">+int p4d_clear_huge(p4d_t *p4d);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline int p4d_clear_huge(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* !__PAGETABLE_P4D_FOLDED */</span>
<span class="p_add">+</span>
 int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot);
 int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot);
 int pud_clear_huge(pud_t *pud);
 int pmd_clear_huge(pmd_t *pmd);
 #else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_add">+static inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 static inline int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)
 {
 	return 0;
<span class="p_chunk">@@ -763,6 +801,10 @@</span> <span class="p_context"> static inline int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)</span>
 {
 	return 0;
 }
<span class="p_add">+static inline int p4d_clear_huge(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 static inline int pud_clear_huge(pud_t *pud)
 {
 	return 0;
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 48c76d612d40..1b45ac594b54 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -116,7 +116,7 @@</span> <span class="p_context"> struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,</span>
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
 				pud_t *pud, int flags);
 int pmd_huge(pmd_t pmd);
<span class="p_del">-int pud_huge(pud_t pmd);</span>
<span class="p_add">+int pud_huge(pud_t pud);</span>
 unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 		unsigned long address, unsigned long end, pgprot_t newprot);
 
<span class="p_chunk">@@ -189,6 +189,9 @@</span> <span class="p_context"> static inline void __unmap_hugepage_range(struct mmu_gather *tlb,</span>
 #ifndef pgd_huge
 #define pgd_huge(x)	0
 #endif
<span class="p_add">+#ifndef p4d_huge</span>
<span class="p_add">+#define p4d_huge(x)	0</span>
<span class="p_add">+#endif</span>
 
 #ifndef pgd_write
 static inline int pgd_write(pgd_t pgd)
<span class="p_header">diff --git a/include/linux/kasan.h b/include/linux/kasan.h</span>
<span class="p_header">index 820c0ad54a01..8e52fe609220 100644</span>
<span class="p_header">--- a/include/linux/kasan.h</span>
<span class="p_header">+++ b/include/linux/kasan.h</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"> extern unsigned char kasan_zero_page[PAGE_SIZE];</span>
 extern pte_t kasan_zero_pte[PTRS_PER_PTE];
 extern pmd_t kasan_zero_pmd[PTRS_PER_PMD];
 extern pud_t kasan_zero_pud[PTRS_PER_PUD];
<span class="p_add">+extern p4d_t kasan_zero_p4d[PTRS_PER_P4D];</span>
 
 void kasan_populate_zero_shadow(const void *shadow_start,
 				const void *shadow_end);
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a953e87183ea..96456b3f9524 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1515,14 +1515,24 @@</span> <span class="p_context"> static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,</span>
 	return ptep;
 }
 
<span class="p_add">+#ifdef __PAGETABLE_P4D_FOLDED</span>
<span class="p_add">+static inline int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+						unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef __PAGETABLE_PUD_FOLDED
<span class="p_del">-static inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static inline int __pud_alloc(struct mm_struct *mm, p4d_t *p4d,</span>
 						unsigned long address)
 {
 	return 0;
 }
 #else
<span class="p_del">-int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);</span>
<span class="p_add">+int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);</span>
 #endif
 
 #if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)
<span class="p_chunk">@@ -1576,10 +1586,18 @@</span> <span class="p_context"> int __pte_alloc_kernel(pmd_t *pmd, unsigned long address);</span>
 #if defined(CONFIG_MMU) &amp;&amp; !defined(__ARCH_HAS_4LEVEL_HACK)
 
 #ifndef __ARCH_HAS_5LEVEL_HACK
<span class="p_del">-static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
<span class="p_add">+static inline p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+		unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unlikely(pgd_none(*pgd)) &amp;&amp; __p4d_alloc(mm, pgd, address)) ?</span>
<span class="p_add">+		NULL : p4d_offset(pgd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,</span>
<span class="p_add">+		unsigned long address)</span>
 {
<span class="p_del">-	return (unlikely(pgd_none(*pgd)) &amp;&amp; __pud_alloc(mm, pgd, address))?</span>
<span class="p_del">-		NULL: pud_offset(pgd, address);</span>
<span class="p_add">+	return (unlikely(p4d_none(*p4d)) &amp;&amp; __pud_alloc(mm, p4d, address)) ?</span>
<span class="p_add">+		NULL : pud_offset(p4d, address);</span>
 }
 #endif /* !__ARCH_HAS_5LEVEL_HACK */
 
<span class="p_chunk">@@ -2318,7 +2336,8 @@</span> <span class="p_context"> void sparse_mem_maps_populate_node(struct page **map_map,</span>
 
 struct page *sparse_mem_map_populate(unsigned long pnum, int nid);
 pgd_t *vmemmap_pgd_populate(unsigned long addr, int node);
<span class="p_del">-pud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);</span>
<span class="p_add">+p4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);</span>
<span class="p_add">+pud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);</span>
 pmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);
 pte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);
 void *vmemmap_alloc_block(unsigned long size, int node);
<span class="p_header">diff --git a/lib/ioremap.c b/lib/ioremap.c</span>
<span class="p_header">index a3e14ce92a56..4bb30206b942 100644</span>
<span class="p_header">--- a/lib/ioremap.c</span>
<span class="p_header">+++ b/lib/ioremap.c</span>
<span class="p_chunk">@@ -14,6 +14,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
<span class="p_add">+static int __read_mostly ioremap_p4d_capable;</span>
 static int __read_mostly ioremap_pud_capable;
 static int __read_mostly ioremap_pmd_capable;
 static int __read_mostly ioremap_huge_disabled;
<span class="p_chunk">@@ -35,6 +36,11 @@</span> <span class="p_context"> void __init ioremap_huge_init(void)</span>
 	}
 }
 
<span class="p_add">+static inline int ioremap_p4d_enabled(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ioremap_p4d_capable;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int ioremap_pud_enabled(void)
 {
 	return ioremap_pud_capable;
<span class="p_chunk">@@ -46,6 +52,7 @@</span> <span class="p_context"> static inline int ioremap_pmd_enabled(void)</span>
 }
 
 #else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_add">+static inline int ioremap_p4d_enabled(void) { return 0; }</span>
 static inline int ioremap_pud_enabled(void) { return 0; }
 static inline int ioremap_pmd_enabled(void) { return 0; }
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
<span class="p_chunk">@@ -94,14 +101,14 @@</span> <span class="p_context"> static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,</span>
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
 
 	phys_addr -= addr;
<span class="p_del">-	pud = pud_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(&amp;init_mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -120,6 +127,32 @@</span> <span class="p_context"> static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	phys_addr -= addr;</span>
<span class="p_add">+	p4d = p4d_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ioremap_p4d_enabled() &amp;&amp;</span>
<span class="p_add">+		    ((next - addr) == P4D_SIZE) &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(phys_addr + addr, P4D_SIZE)) {</span>
<span class="p_add">+			if (p4d_set_huge(p4d, phys_addr + addr, prot))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ioremap_pud_range(p4d, addr, next, phys_addr + addr, prot))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int ioremap_page_range(unsigned long addr,
 		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
<span class="p_chunk">@@ -135,7 +168,7 @@</span> <span class="p_context"> int ioremap_page_range(unsigned long addr,</span>
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);</span>
<span class="p_add">+		err = ioremap_p4d_range(pgd, addr, next, phys_addr+addr, prot);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 55315555489d..4660b7ee2088 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -226,6 +226,7 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 			      unsigned int *page_mask)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -243,8 +244,13 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 	pgd = pgd_offset(mm, address);
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 		return no_page_table(vma, flags);
<span class="p_del">-</span>
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_huge(*p4d));</span>
<span class="p_add">+	if (unlikely(p4d_bad(*p4d)))</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (pud_none(*pud))
 		return no_page_table(vma, flags);
 	if (pud_huge(*pud) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB) {
<span class="p_chunk">@@ -317,6 +323,7 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 		struct page **page)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -330,7 +337,9 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 	else
 		pgd = pgd_offset_gate(mm, address);
 	BUG_ON(pgd_none(*pgd));
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	BUG_ON(p4d_none(*p4d));</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	BUG_ON(pud_none(*pud));
 	pmd = pmd_offset(pud, address);
 	if (pmd_none(*pmd))
<span class="p_chunk">@@ -1392,13 +1401,13 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 	return 1;
 }
 
<span class="p_del">-static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,</span>
 			 int write, struct page **pages, int *nr)
 {
 	unsigned long next;
 	pud_t *pudp;
 
<span class="p_del">-	pudp = pud_offset(&amp;pgd, addr);</span>
<span class="p_add">+	pudp = pud_offset(&amp;p4d, addr);</span>
 	do {
 		pud_t pud = READ_ONCE(*pudp);
 
<span class="p_chunk">@@ -1420,6 +1429,31 @@</span> <span class="p_context"> static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
 	return 1;
 }
 
<span class="p_add">+static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			 int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	p4d_t *p4dp;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4dp = p4d_offset(&amp;pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		p4d_t p4d = READ_ONCE(*p4dp);</span>
<span class="p_add">+</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none(p4d))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		BUILD_BUG_ON(p4d_huge(p4d));</span>
<span class="p_add">+		if (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {</span>
<span class="p_add">+			if (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,</span>
<span class="p_add">+					 P4D_SHIFT, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		} else if (!gup_p4d_range(p4d, addr, next, write, pages, nr))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+	} while (p4dp++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Like get_user_pages_fast() except it&#39;s IRQ-safe in that it won&#39;t fall back to
  * the regular GUP. It will only return non-negative values.
<span class="p_chunk">@@ -1470,7 +1504,7 @@</span> <span class="p_context"> int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
 					 PGDIR_SHIFT, next, write, pages, &amp;nr))
 				break;
<span class="p_del">-		} else if (!gup_pud_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_add">+		} else if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
 			break;
 	} while (pgdp++, addr = next, addr != end);
 	local_irq_restore(flags);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 5f3ad65c85de..4a35ac48b446 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1795,6 +1795,7 @@</span> <span class="p_context"> void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,</span>
 		bool freeze, struct page *page)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_chunk">@@ -1802,7 +1803,11 @@</span> <span class="p_context"> void split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,</span>
 	if (!pgd_present(*pgd))
 		return;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		return;
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index c7025c132670..3ccde37734d2 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4385,7 +4385,8 @@</span> <span class="p_context"> pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)</span>
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
 {
 	pgd_t *pgd = pgd_offset(mm, *addr);
<span class="p_del">-	pud_t *pud = pud_offset(pgd, *addr);</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, *addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, *addr);</span>
 
 	BUG_ON(page_count(virt_to_page(ptep)) == 0);
 	if (page_count(virt_to_page(ptep)) == 1)
<span class="p_chunk">@@ -4416,11 +4417,13 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm,</span>
 			unsigned long addr, unsigned long sz)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pte_t *pte = NULL;
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (pud) {
 		if (sz == PUD_SIZE) {
 			pte = (pte_t *)pud;
<span class="p_chunk">@@ -4440,18 +4443,22 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm,</span>
 pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd = NULL;</span>
<span class="p_add">+	pmd_t *pmd;</span>
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	if (pgd_present(*pgd)) {</span>
<span class="p_del">-		pud = pud_offset(pgd, addr);</span>
<span class="p_del">-		if (pud_present(*pud)) {</span>
<span class="p_del">-			if (pud_huge(*pud))</span>
<span class="p_del">-				return (pte_t *)pud;</span>
<span class="p_del">-			pmd = pmd_offset(pud, addr);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!pgd_present(*pgd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
<span class="p_add">+	if (!pud_present(*pud))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	if (pud_huge(*pud))</span>
<span class="p_add">+		return (pte_t *)pud;</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
 	return (pte_t *) pmd;
 }
 
<span class="p_header">diff --git a/mm/kasan/kasan_init.c b/mm/kasan/kasan_init.c</span>
<span class="p_header">index 3f9a41cf0ac6..0d4ee78796fc 100644</span>
<span class="p_header">--- a/mm/kasan/kasan_init.c</span>
<span class="p_header">+++ b/mm/kasan/kasan_init.c</span>
<span class="p_chunk">@@ -29,6 +29,9 @@</span> <span class="p_context"></span>
  */
 unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;
 
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 4</span>
<span class="p_add">+p4d_t kasan_zero_p4d[PTRS_PER_P4D] __page_aligned_bss;</span>
<span class="p_add">+#endif</span>
 #if CONFIG_PGTABLE_LEVELS &gt; 3
 pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;
 #endif
<span class="p_chunk">@@ -81,10 +84,10 @@</span> <span class="p_context"> static void __init zero_pmd_populate(pud_t *pud, unsigned long addr,</span>
 	} while (pmd++, addr = next, addr != end);
 }
 
<span class="p_del">-static void __init zero_pud_populate(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static void __init zero_pud_populate(p4d_t *p4d, unsigned long addr,</span>
 				unsigned long end)
 {
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	unsigned long next;
 
 	do {
<span class="p_chunk">@@ -106,6 +109,23 @@</span> <span class="p_context"> static void __init zero_pud_populate(pgd_t *pgd, unsigned long addr,</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_add">+static void __init zero_p4d_populate(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+				unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (p4d_none(*p4d)) {</span>
<span class="p_add">+			p4d_populate(&amp;init_mm, p4d,</span>
<span class="p_add">+				early_alloc(PAGE_SIZE, NUMA_NO_NODE));</span>
<span class="p_add">+		}</span>
<span class="p_add">+		zero_pud_populate(p4d, addr, next);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * kasan_populate_zero_shadow - populate shadow memory region with
  *                               kasan_zero_page
<span class="p_chunk">@@ -124,6 +144,7 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 		next = pgd_addr_end(addr, end);
 
 		if (IS_ALIGNED(addr, PGDIR_SIZE) &amp;&amp; end - addr &gt;= PGDIR_SIZE) {
<span class="p_add">+			p4d_t *p4d;</span>
 			pud_t *pud;
 			pmd_t *pmd;
 
<span class="p_chunk">@@ -135,8 +156,12 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 			 * puds,pmds, so pgd_populate(), pud_populate()
 			 * is noops.
 			 */
<span class="p_del">-			pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="p_del">-			pud = pud_offset(pgd, addr);</span>
<span class="p_add">+#ifndef __ARCH_HAS_5LEVEL_HACK</span>
<span class="p_add">+			pgd_populate(&amp;init_mm, pgd, kasan_zero_p4d);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+			p4d_populate(&amp;init_mm, p4d, kasan_zero_pud);</span>
<span class="p_add">+			pud = pud_offset(p4d, addr);</span>
 			pud_populate(&amp;init_mm, pud, kasan_zero_pmd);
 			pmd = pmd_offset(pud, addr);
 			pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);
<span class="p_chunk">@@ -147,6 +172,6 @@</span> <span class="p_context"> void __init kasan_populate_zero_shadow(const void *shadow_start,</span>
 			pgd_populate(&amp;init_mm, pgd,
 				early_alloc(PAGE_SIZE, NUMA_NO_NODE));
 		}
<span class="p_del">-		zero_pud_populate(pgd, addr, next);</span>
<span class="p_add">+		zero_p4d_populate(pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 6bf2b471e30c..e4a37c340a56 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -442,7 +442,7 @@</span> <span class="p_context"> static inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,</span>
 	mm_dec_nr_pmds(tlb-&gt;mm);
 }
 
<span class="p_del">-static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
<span class="p_add">+static inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				unsigned long floor, unsigned long ceiling)
 {
<span class="p_chunk">@@ -451,7 +451,7 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 	unsigned long start;
 
 	start = addr;
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -459,6 +459,39 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 		free_pmd_range(tlb, pud, addr, next, floor, ceiling);
 	} while (pud++, addr = next, addr != end);
 
<span class="p_add">+	start &amp;= P4D_MASK;</span>
<span class="p_add">+	if (start &lt; floor)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (ceiling) {</span>
<span class="p_add">+		ceiling &amp;= P4D_MASK;</span>
<span class="p_add">+		if (!ceiling)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (end - 1 &gt; ceiling - 1)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, start);</span>
<span class="p_add">+	p4d_clear(p4d);</span>
<span class="p_add">+	pud_free_tlb(tlb, pud, start);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				unsigned long floor, unsigned long ceiling)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = addr;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		free_pud_range(tlb, p4d, addr, next, floor, ceiling);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
 	start &amp;= PGDIR_MASK;
 	if (start &lt; floor)
 		return;
<span class="p_chunk">@@ -470,9 +503,9 @@</span> <span class="p_context"> static inline void free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 	if (end - 1 &gt; ceiling - 1)
 		return;
 
<span class="p_del">-	pud = pud_offset(pgd, start);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, start);</span>
 	pgd_clear(pgd);
<span class="p_del">-	pud_free_tlb(tlb, pud, start);</span>
<span class="p_add">+	p4d_free_tlb(tlb, p4d, start);</span>
 }
 
 /*
<span class="p_chunk">@@ -536,7 +569,7 @@</span> <span class="p_context"> void free_pgd_range(struct mmu_gather *tlb,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		free_pud_range(tlb, pgd, addr, next, floor, ceiling);</span>
<span class="p_add">+		free_p4d_range(tlb, pgd, addr, next, floor, ceiling);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -655,7 +688,8 @@</span> <span class="p_context"> static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,</span>
 			  pte_t pte, struct page *page)
 {
 	pgd_t *pgd = pgd_offset(vma-&gt;vm_mm, addr);
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	pmd_t *pmd = pmd_offset(pud, addr);
 	struct address_space *mapping;
 	pgoff_t index;
<span class="p_chunk">@@ -1020,16 +1054,16 @@</span> <span class="p_context"> static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 }
 
 static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
<span class="p_del">-		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,</span>
<span class="p_add">+		p4d_t *dst_p4d, p4d_t *src_p4d, struct vm_area_struct *vma,</span>
 		unsigned long addr, unsigned long end)
 {
 	pud_t *src_pud, *dst_pud;
 	unsigned long next;
 
<span class="p_del">-	dst_pud = pud_alloc(dst_mm, dst_pgd, addr);</span>
<span class="p_add">+	dst_pud = pud_alloc(dst_mm, dst_p4d, addr);</span>
 	if (!dst_pud)
 		return -ENOMEM;
<span class="p_del">-	src_pud = pud_offset(src_pgd, addr);</span>
<span class="p_add">+	src_pud = pud_offset(src_p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(src_pud))
<span class="p_chunk">@@ -1041,6 +1075,28 @@</span> <span class="p_context"> static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 	return 0;
 }
 
<span class="p_add">+static inline int copy_p4d_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="p_add">+		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *src_p4d, *dst_p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	dst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);</span>
<span class="p_add">+	if (!dst_p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	src_p4d = p4d_offset(src_pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(src_p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (copy_pud_range(dst_mm, src_mm, dst_p4d, src_p4d,</span>
<span class="p_add">+						vma, addr, next))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (dst_p4d++, src_p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		struct vm_area_struct *vma)
 {
<span class="p_chunk">@@ -1096,7 +1152,7 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(src_pgd))
 			continue;
<span class="p_del">-		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,</span>
<span class="p_add">+		if (unlikely(copy_p4d_range(dst_mm, src_mm, dst_pgd, src_pgd,</span>
 					    vma, addr, next))) {
 			ret = -ENOMEM;
 			break;
<span class="p_chunk">@@ -1259,14 +1315,14 @@</span> <span class="p_context"> static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,</span>
 }
 
 static inline unsigned long zap_pud_range(struct mmu_gather *tlb,
<span class="p_del">-				struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				struct vm_area_struct *vma, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				struct zap_details *details)
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -1277,6 +1333,25 @@</span> <span class="p_context"> static inline unsigned long zap_pud_range(struct mmu_gather *tlb,</span>
 	return addr;
 }
 
<span class="p_add">+static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,</span>
<span class="p_add">+				struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				struct zap_details *details)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		next = zap_pud_range(tlb, vma, p4d, addr, next, details);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void unmap_page_range(struct mmu_gather *tlb,
 			     struct vm_area_struct *vma,
 			     unsigned long addr, unsigned long end,
<span class="p_chunk">@@ -1292,7 +1367,7 @@</span> <span class="p_context"> void unmap_page_range(struct mmu_gather *tlb,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		next = zap_pud_range(tlb, vma, pgd, addr, next, details);</span>
<span class="p_add">+		next = zap_p4d_range(tlb, vma, pgd, addr, next, details);</span>
 	} while (pgd++, addr = next, addr != end);
 	tlb_end_vma(tlb, vma);
 }
<span class="p_chunk">@@ -1448,16 +1523,24 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(zap_vma_ptes);</span>
 pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
 			spinlock_t **ptl)
 {
<span class="p_del">-	pgd_t * pgd = pgd_offset(mm, addr);</span>
<span class="p_del">-	pud_t * pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_del">-	if (pud) {</span>
<span class="p_del">-		pmd_t * pmd = pmd_alloc(mm, pud, addr);</span>
<span class="p_del">-		if (pmd) {</span>
<span class="p_del">-			VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="p_del">-			return pte_alloc_map_lock(mm, pmd, addr, ptl);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset(mm, addr);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pmd = pmd_alloc(mm, pud, addr);</span>
<span class="p_add">+	if (!pmd)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="p_add">+	return pte_alloc_map_lock(mm, pmd, addr, ptl);</span>
 }
 
 /*
<span class="p_chunk">@@ -1723,7 +1806,7 @@</span> <span class="p_context"> static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,</span>
 			unsigned long addr, unsigned long end,
 			unsigned long pfn, pgprot_t prot)
 {
<span class="p_chunk">@@ -1731,7 +1814,7 @@</span> <span class="p_context"> static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	unsigned long next;
 
 	pfn -= addr &gt;&gt; PAGE_SHIFT;
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -1743,6 +1826,26 @@</span> <span class="p_context"> static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+			unsigned long addr, unsigned long end,</span>
<span class="p_add">+			unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pfn -= addr &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (remap_pud_range(mm, p4d, addr, next,</span>
<span class="p_add">+				pfn + (addr &gt;&gt; PAGE_SHIFT), prot))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * remap_pfn_range - remap kernel memory to userspace
  * @vma: user vma to map to
<span class="p_chunk">@@ -1799,7 +1902,7 @@</span> <span class="p_context"> int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,</span>
 	flush_cache_range(vma, addr, end);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = remap_pud_range(mm, pgd, addr, next,</span>
<span class="p_add">+		err = remap_p4d_range(mm, pgd, addr, next,</span>
 				pfn + (addr &gt;&gt; PAGE_SHIFT), prot);
 		if (err)
 			break;
<span class="p_chunk">@@ -1915,7 +2018,7 @@</span> <span class="p_context"> static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,</span>
 	return err;
 }
 
<span class="p_del">-static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,</span>
 				     unsigned long addr, unsigned long end,
 				     pte_fn_t fn, void *data)
 {
<span class="p_chunk">@@ -1923,7 +2026,7 @@</span> <span class="p_context"> static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	unsigned long next;
 	int err;
 
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -1935,6 +2038,26 @@</span> <span class="p_context"> static int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,</span>
 	return err;
 }
 
<span class="p_add">+static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,</span>
<span class="p_add">+				     unsigned long addr, unsigned long end,</span>
<span class="p_add">+				     pte_fn_t fn, void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		err = apply_to_pud_range(mm, p4d, addr, next, fn, data);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Scan a region of virtual memory, filling in page tables as necessary
  * and calling a provided function on each leaf page table.
<span class="p_chunk">@@ -1953,7 +2076,7 @@</span> <span class="p_context"> int apply_to_page_range(struct mm_struct *mm, unsigned long addr,</span>
 	pgd = pgd_offset(mm, addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = apply_to_pud_range(mm, pgd, addr, next, fn, data);</span>
<span class="p_add">+		err = apply_to_p4d_range(mm, pgd, addr, next, fn, data);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_chunk">@@ -3615,10 +3738,14 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	};
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 
 	pgd = pgd_offset(mm, address);
<span class="p_del">-	pud = pud_alloc(mm, pgd, address);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return VM_FAULT_OOM;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, address);</span>
 	if (!pud)
 		return VM_FAULT_OOM;
 	vmf.pmd = pmd_alloc(mm, pud, address);
<span class="p_chunk">@@ -3722,7 +3849,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(handle_mm_fault);</span>
  * Allocate page upper directory.
  * We&#39;ve already handled the fast-path in-line.
  */
<span class="p_del">-int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
<span class="p_add">+int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)</span>
 {
 	pud_t *new = pud_alloc_one(mm, address);
 	if (!new)
<span class="p_chunk">@@ -3731,10 +3858,17 @@</span> <span class="p_context"> int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)</span>
 	smp_wmb(); /* See comment in __pte_alloc */
 
 	spin_lock(&amp;mm-&gt;page_table_lock);
<span class="p_del">-	if (pgd_present(*pgd))		/* Another has populated it */</span>
<span class="p_add">+#ifndef __ARCH_HAS_5LEVEL_HACK</span>
<span class="p_add">+	if (p4d_present(*p4d))		/* Another has populated it */</span>
<span class="p_add">+		pud_free(mm, new);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		p4d_populate(mm, p4d, new);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	if (pgd_present(*p4d))		/* Another has populated it */</span>
 		pud_free(mm, new);
 	else
<span class="p_del">-		pgd_populate(mm, pgd, new);</span>
<span class="p_add">+		pgd_populate(mm, p4d, new);</span>
<span class="p_add">+#endif /* __ARCH_HAS_5LEVEL_HACK */</span>
 	spin_unlock(&amp;mm-&gt;page_table_lock);
 	return 0;
 }
<span class="p_chunk">@@ -3776,6 +3910,7 @@</span> <span class="p_context"> static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,</span>
 		pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *ptep;
<span class="p_chunk">@@ -3784,7 +3919,11 @@</span> <span class="p_context"> static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,</span>
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 		goto out;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
 		goto out;
 
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index cdbed8aaa426..81fe0c18e29d 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -379,6 +379,7 @@</span> <span class="p_context"> static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,</span>
 	pte = get_locked_pte(vma-&gt;vm_mm, start,	&amp;ptl);
 	/* Make sure we do not cross the page table boundary */
 	end = pgd_addr_end(start, end);
<span class="p_add">+	end = p4d_addr_end(start, end);</span>
 	end = pud_addr_end(start, end);
 	end = pmd_addr_end(start, end);
 
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index f9c07f54dd62..af677fe954bc 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -209,14 +209,14 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 }
 
 static inline unsigned long change_pud_range(struct vm_area_struct *vma,
<span class="p_del">-		pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 		pgprot_t newprot, int dirty_accountable, int prot_numa)
 {
 	pud_t *pud;
 	unsigned long next;
 	unsigned long pages = 0;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -228,6 +228,26 @@</span> <span class="p_context"> static inline unsigned long change_pud_range(struct vm_area_struct *vma,</span>
 	return pages;
 }
 
<span class="p_add">+static inline unsigned long change_p4d_range(struct vm_area_struct *vma,</span>
<span class="p_add">+		pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		pgprot_t newprot, int dirty_accountable, int prot_numa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long pages = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		pages += change_pud_range(vma, p4d, addr, next, newprot,</span>
<span class="p_add">+				 dirty_accountable, prot_numa);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return pages;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static unsigned long change_protection_range(struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end, pgprot_t newprot,
 		int dirty_accountable, int prot_numa)
<span class="p_chunk">@@ -246,7 +266,7 @@</span> <span class="p_context"> static unsigned long change_protection_range(struct vm_area_struct *vma,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		pages += change_pud_range(vma, pgd, addr, next, newprot,</span>
<span class="p_add">+		pages += change_p4d_range(vma, pgd, addr, next, newprot,</span>
 				 dirty_accountable, prot_numa);
 	} while (pgd++, addr = next, addr != end);
 
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 30d7d2482eea..2b3bfcd51c75 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -31,6 +31,7 @@</span> <span class="p_context"></span>
 static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_chunk">@@ -38,7 +39,11 @@</span> <span class="p_context"> static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)</span>
 	if (pgd_none_or_clear_bad(pgd))
 		return NULL;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	if (pud_none_or_clear_bad(pud))
 		return NULL;
 
<span class="p_chunk">@@ -53,11 +58,15 @@</span> <span class="p_context"> static pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			    unsigned long addr)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 
 	pgd = pgd_offset(mm, addr);
<span class="p_del">-	pud = pud_alloc(mm, pgd, addr);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, addr);</span>
 	if (!pud)
 		return NULL;
 
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 207244489a68..0020f340abfd 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -69,14 +69,14 @@</span> <span class="p_context"> static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
 	return err;
 }
 
<span class="p_del">-static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 			  struct mm_walk *walk)
 {
 	pud_t *pud;
 	unsigned long next;
 	int err = 0;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud)) {
<span class="p_chunk">@@ -95,6 +95,32 @@</span> <span class="p_context"> static int walk_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 	return err;
 }
 
<span class="p_add">+static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			  struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int err = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d)) {</span>
<span class="p_add">+			if (walk-&gt;pte_hole)</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+			if (err)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)</span>
<span class="p_add">+			err = walk_pud_range(p4d, addr, next, walk);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int walk_pgd_range(unsigned long addr, unsigned long end,
 			  struct mm_walk *walk)
 {
<span class="p_chunk">@@ -113,7 +139,7 @@</span> <span class="p_context"> static int walk_pgd_range(unsigned long addr, unsigned long end,</span>
 			continue;
 		}
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
<span class="p_del">-			err = walk_pud_range(pgd, addr, next, walk);</span>
<span class="p_add">+			err = walk_p4d_range(pgd, addr, next, walk);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 71c5f9109f2a..738e278b48c1 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -22,6 +22,12 @@</span> <span class="p_context"> void pgd_clear_bad(pgd_t *pgd)</span>
 	pgd_clear(pgd);
 }
 
<span class="p_add">+void p4d_clear_bad(p4d_t *p4d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_ERROR(*p4d);</span>
<span class="p_add">+	p4d_clear(p4d);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void pud_clear_bad(pud_t *pud)
 {
 	pud_ERROR(*pud);
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 91619fd70939..257cb67892ee 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -684,6 +684,7 @@</span> <span class="p_context"> unsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)</span>
 pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd = NULL;
 	pmd_t pmde;
<span class="p_chunk">@@ -692,7 +693,11 @@</span> <span class="p_context"> pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)</span>
 	if (!pgd_present(*pgd))
 		goto out;
 
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		goto out;
 
<span class="p_chunk">@@ -797,6 +802,7 @@</span> <span class="p_context"> bool page_check_address_transhuge(struct page *page, struct mm_struct *mm,</span>
 				  pte_t **ptep, spinlock_t **ptlp)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -816,7 +822,10 @@</span> <span class="p_context"> bool page_check_address_transhuge(struct page *page, struct mm_struct *mm,</span>
 	pgd = pgd_offset(mm, address);
 	if (!pgd_present(*pgd))
 		return false;
<span class="p_del">-	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	p4d = p4d_offset(pgd, address);</span>
<span class="p_add">+	if (!p4d_present(*p4d))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
 	if (!pud_present(*pud))
 		return false;
 	pmd = pmd_offset(pud, address);
<span class="p_header">diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c</span>
<span class="p_header">index 574c67b663fe..a56c3989f773 100644</span>
<span class="p_header">--- a/mm/sparse-vmemmap.c</span>
<span class="p_header">+++ b/mm/sparse-vmemmap.c</span>
<span class="p_chunk">@@ -196,9 +196,9 @@</span> <span class="p_context"> pmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)</span>
 	return pmd;
 }
 
<span class="p_del">-pud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)</span>
<span class="p_add">+pud_t * __meminit vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node)</span>
 {
<span class="p_del">-	pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud_t *pud = pud_offset(p4d, addr);</span>
 	if (pud_none(*pud)) {
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
 		if (!p)
<span class="p_chunk">@@ -208,6 +208,18 @@</span> <span class="p_context"> pud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)</span>
 	return pud;
 }
 
<span class="p_add">+p4d_t * __meminit vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		void *p = vmemmap_alloc_block(PAGE_SIZE, node);</span>
<span class="p_add">+		if (!p)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		p4d_populate(&amp;init_mm, p4d, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return p4d;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 pgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)
 {
 	pgd_t *pgd = pgd_offset_k(addr);
<span class="p_chunk">@@ -225,6 +237,7 @@</span> <span class="p_context"> int __meminit vmemmap_populate_basepages(unsigned long start,</span>
 {
 	unsigned long addr = start;
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -233,7 +246,10 @@</span> <span class="p_context"> int __meminit vmemmap_populate_basepages(unsigned long start,</span>
 		pgd = vmemmap_pgd_populate(addr, node);
 		if (!pgd)
 			return -ENOMEM;
<span class="p_del">-		pud = vmemmap_pud_populate(pgd, addr, node);</span>
<span class="p_add">+		p4d = vmemmap_p4d_populate(pgd, addr, node);</span>
<span class="p_add">+		if (!p4d)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		pud = vmemmap_pud_populate(p4d, addr, node);</span>
 		if (!pud)
 			return -ENOMEM;
 		pmd = vmemmap_pmd_populate(pud, addr, node);
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 4761701d1721..3d2c1c33fd6c 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1259,7 +1259,7 @@</span> <span class="p_context"> static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,</span>
 	return 0;
 }
 
<span class="p_del">-static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+static inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,</span>
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
 {
<span class="p_chunk">@@ -1267,7 +1267,7 @@</span> <span class="p_context"> static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
 	unsigned long next;
 	int ret;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
<span class="p_chunk">@@ -1279,6 +1279,26 @@</span> <span class="p_context"> static inline int unuse_pud_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
 	return 0;
 }
 
<span class="p_add">+static inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,</span>
<span class="p_add">+				unsigned long addr, unsigned long end,</span>
<span class="p_add">+				swp_entry_t entry, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		ret = unuse_pud_range(vma, p4d, addr, next, entry, page);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int unuse_vma(struct vm_area_struct *vma,
 				swp_entry_t entry, struct page *page)
 {
<span class="p_chunk">@@ -1302,7 +1322,7 @@</span> <span class="p_context"> static int unuse_vma(struct vm_area_struct *vma,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		ret = unuse_pud_range(vma, pgd, addr, next, entry, page);</span>
<span class="p_add">+		ret = unuse_p4d_range(vma, pgd, addr, next, entry, page);</span>
 		if (ret)
 			return ret;
 	} while (pgd++, addr = next, addr != end);
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index af817e5060fb..721681deba9f 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -124,19 +124,22 @@</span> <span class="p_context"> static int mfill_zeropage_pte(struct mm_struct *dst_mm,</span>
 static pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)
 {
 	pgd_t *pgd;
<span class="p_add">+	p4d_t *p4d;</span>
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd = NULL;</span>
 
 	pgd = pgd_offset(mm, address);
<span class="p_del">-	pud = pud_alloc(mm, pgd, address);</span>
<span class="p_del">-	if (pud)</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Note that we didn&#39;t run this because the pmd was</span>
<span class="p_del">-		 * missing, the *pmd may be already established and in</span>
<span class="p_del">-		 * turn it may also be a trans_huge_pmd.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmd = pmd_alloc(mm, pud, address);</span>
<span class="p_del">-	return pmd;</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, address);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note that we didn&#39;t run this because the pmd was</span>
<span class="p_add">+	 * missing, the *pmd may be already established and in</span>
<span class="p_add">+	 * turn it may also be a trans_huge_pmd.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return pmd_alloc(mm, pud, address);</span>
 }
 
 static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,
<span class="p_header">diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="p_header">index 3ca82d44edd3..35ff0d0e6400 100644</span>
<span class="p_header">--- a/mm/vmalloc.c</span>
<span class="p_header">+++ b/mm/vmalloc.c</span>
<span class="p_chunk">@@ -86,12 +86,12 @@</span> <span class="p_context"> static void vunmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end)</span>
 	} while (pmd++, addr = next, addr != end);
 }
 
<span class="p_del">-static void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="p_add">+static void vunmap_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end)</span>
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_clear_huge(pud))
<span class="p_chunk">@@ -102,6 +102,22 @@</span> <span class="p_context"> static void vunmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_add">+static void vunmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_clear_huge(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (p4d_none_or_clear_bad(p4d))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		vunmap_pud_range(p4d, addr, next);</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void vunmap_page_range(unsigned long addr, unsigned long end)
 {
 	pgd_t *pgd;
<span class="p_chunk">@@ -113,7 +129,7 @@</span> <span class="p_context"> static void vunmap_page_range(unsigned long addr, unsigned long end)</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
<span class="p_del">-		vunmap_pud_range(pgd, addr, next);</span>
<span class="p_add">+		vunmap_p4d_range(pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -160,13 +176,13 @@</span> <span class="p_context"> static int vmap_pmd_range(pud_t *pud, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_del">-static int vmap_pud_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+static int vmap_pud_range(p4d_t *p4d, unsigned long addr,</span>
 		unsigned long end, pgprot_t prot, struct page **pages, int *nr)
 {
 	pud_t *pud;
 	unsigned long next;
 
<span class="p_del">-	pud = pud_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	pud = pud_alloc(&amp;init_mm, p4d, addr);</span>
 	if (!pud)
 		return -ENOMEM;
 	do {
<span class="p_chunk">@@ -177,6 +193,23 @@</span> <span class="p_context"> static int vmap_pud_range(pgd_t *pgd, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+static int vmap_p4d_range(pgd_t *pgd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, pgprot_t prot, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(&amp;init_mm, pgd, addr);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (vmap_pud_range(p4d, addr, next, prot, pages, nr))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} while (p4d++, addr = next, addr != end);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Set up page tables in kva (addr, end). The ptes shall have prot &quot;prot&quot;, and
  * will have pfns corresponding to the &quot;pages&quot; array.
<span class="p_chunk">@@ -196,7 +229,7 @@</span> <span class="p_context"> static int vmap_page_range_noflush(unsigned long start, unsigned long end,</span>
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
<span class="p_del">-		err = vmap_pud_range(pgd, addr, next, prot, pages, &amp;nr);</span>
<span class="p_add">+		err = vmap_p4d_range(pgd, addr, next, prot, pages, &amp;nr);</span>
 		if (err)
 			return err;
 	} while (pgd++, addr = next, addr != end);
<span class="p_chunk">@@ -237,6 +270,10 @@</span> <span class="p_context"> struct page *vmalloc_to_page(const void *vmalloc_addr)</span>
 	unsigned long addr = (unsigned long) vmalloc_addr;
 	struct page *page = NULL;
 	pgd_t *pgd = pgd_offset_k(addr);
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *ptep, pte;</span>
 
 	/*
 	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
<span class="p_chunk">@@ -244,21 +281,23 @@</span> <span class="p_context"> struct page *vmalloc_to_page(const void *vmalloc_addr)</span>
 	 */
 	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
 
<span class="p_del">-	if (!pgd_none(*pgd)) {</span>
<span class="p_del">-		pud_t *pud = pud_offset(pgd, addr);</span>
<span class="p_del">-		if (!pud_none(*pud)) {</span>
<span class="p_del">-			pmd_t *pmd = pmd_offset(pud, addr);</span>
<span class="p_del">-			if (!pmd_none(*pmd)) {</span>
<span class="p_del">-				pte_t *ptep, pte;</span>
<span class="p_del">-</span>
<span class="p_del">-				ptep = pte_offset_map(pmd, addr);</span>
<span class="p_del">-				pte = *ptep;</span>
<span class="p_del">-				if (pte_present(pte))</span>
<span class="p_del">-					page = pte_page(pte);</span>
<span class="p_del">-				pte_unmap(ptep);</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (pgd_none(*pgd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+	if (p4d_none(*p4d))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pud = pud_offset(p4d, addr);</span>
<span class="p_add">+	if (pud_none(*pud))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	if (pmd_none(*pmd))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = pte_offset_map(pmd, addr);</span>
<span class="p_add">+	pte = *ptep;</span>
<span class="p_add">+	if (pte_present(pte))</span>
<span class="p_add">+		page = pte_page(pte);</span>
<span class="p_add">+	pte_unmap(ptep);</span>
 	return page;
 }
 EXPORT_SYMBOL(vmalloc_to_page);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



