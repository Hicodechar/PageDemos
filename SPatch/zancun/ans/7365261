
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,11/20] kvm: rename pfn_t to kvm_pfn_t - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,11/20] kvm: rename pfn_t to kvm_pfn_t</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=320">Dan Williams</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 10, 2015, 12:56 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20151010005622.17221.44373.stgit@dwillia2-desk3.jf.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7365261/mbox/"
   >mbox</a>
|
   <a href="/patch/7365261/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7365261/">/patch/7365261/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id C9AE59F1D5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 10 Oct 2015 01:05:39 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 05A7F208B5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 10 Oct 2015 01:05:36 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5E0442060D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 10 Oct 2015 01:05:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752104AbbJJBF0 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 9 Oct 2015 21:05:26 -0400
Received: from mga03.intel.com ([134.134.136.65]:38966 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751132AbbJJBCH (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 9 Oct 2015 21:02:07 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga103.jf.intel.com with ESMTP; 09 Oct 2015 18:02:05 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,660,1437462000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;807676183&quot;
Received: from dwillia2-desk3.jf.intel.com ([10.54.39.39])
	by fmsmga001.fm.intel.com with ESMTP; 09 Oct 2015 18:02:05 -0700
Subject: [PATCH v2 11/20] kvm: rename pfn_t to kvm_pfn_t
From: Dan Williams &lt;dan.j.williams@intel.com&gt;
To: linux-nvdimm@lists.01.org
Cc: Dave Hansen &lt;dave@sr71.net&gt;, Russell King &lt;linux@arm.linux.org.uk&gt;,
	linux-mm@kvack.org, Gleb Natapov &lt;gleb@kernel.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	linux-kernel@vger.kernel.org, Ralf Baechle &lt;ralf@linux-mips.org&gt;,
	Marc Zyngier &lt;marc.zyngier@arm.com&gt;, Paul Mackerras &lt;paulus@samba.org&gt;,
	Christoffer Dall &lt;christoffer.dall@linaro.org&gt;,
	Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	ross.zwisler@linux.intel.com, hch@lst.de, Alexander Graf &lt;agraf@suse.com&gt;
Date: Fri, 09 Oct 2015 20:56:22 -0400
Message-ID: &lt;20151010005622.17221.44373.stgit@dwillia2-desk3.jf.intel.com&gt;
In-Reply-To: &lt;20151010005522.17221.87557.stgit@dwillia2-desk3.jf.intel.com&gt;
References: &lt;20151010005522.17221.87557.stgit@dwillia2-desk3.jf.intel.com&gt;
User-Agent: StGit/0.17.1-9-g687f
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Oct. 10, 2015, 12:56 a.m.</div>
<pre class="content">
The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing
pfn_t in KVM to kvm_pfn_t [2].

[1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html
[2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html

Cc: Dave Hansen &lt;dave@sr71.net&gt;
Cc: Gleb Natapov &lt;gleb@kernel.org&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
Cc: Russell King &lt;linux@arm.linux.org.uk&gt;
Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
Cc: Will Deacon &lt;will.deacon@arm.com&gt;
Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;
Cc: Alexander Graf &lt;agraf@suse.com&gt;
Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;
Cc: Paul Mackerras &lt;paulus@samba.org&gt;
<span class="signed-off-by">Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
---
 arch/arm/include/asm/kvm_mmu.h        |    5 ++--
 arch/arm/kvm/mmu.c                    |   10 ++++---
 arch/arm64/include/asm/kvm_mmu.h      |    3 +-
 arch/mips/include/asm/kvm_host.h      |    6 ++--
 arch/mips/kvm/emulate.c               |    2 +
 arch/mips/kvm/tlb.c                   |   14 +++++-----
 arch/powerpc/include/asm/kvm_book3s.h |    4 +--
 arch/powerpc/include/asm/kvm_ppc.h    |    2 +
 arch/powerpc/kvm/book3s.c             |    6 ++--
 arch/powerpc/kvm/book3s_32_mmu_host.c |    2 +
 arch/powerpc/kvm/book3s_64_mmu_host.c |    2 +
 arch/powerpc/kvm/e500.h               |    2 +
 arch/powerpc/kvm/e500_mmu_host.c      |    8 +++---
 arch/powerpc/kvm/trace_pr.h           |    2 +
 arch/x86/kvm/iommu.c                  |   11 ++++----
 arch/x86/kvm/mmu.c                    |   37 +++++++++++++-------------
 arch/x86/kvm/mmu_audit.c              |    2 +
 arch/x86/kvm/paging_tmpl.h            |    6 ++--
 arch/x86/kvm/vmx.c                    |    2 +
 arch/x86/kvm/x86.c                    |    2 +
 include/linux/kvm_host.h              |   37 +++++++++++++-------------
 include/linux/kvm_types.h             |    2 +
 virt/kvm/kvm_main.c                   |   47 +++++++++++++++++----------------
 23 files changed, 110 insertions(+), 104 deletions(-)


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68151">Christoffer Dall</a> - Oct. 10, 2015, 3:35 p.m.</div>
<pre class="content">
On Fri, Oct 09, 2015 at 08:56:22PM -0400, Dan Williams wrote:
<span class="quote">&gt; The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing</span>
<span class="quote">&gt; pfn_t in KVM to kvm_pfn_t [2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html</span>
<span class="quote">&gt; [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Dave Hansen &lt;dave@sr71.net&gt;</span>
<span class="quote">&gt; Cc: Gleb Natapov &lt;gleb@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt; Cc: Russell King &lt;linux@arm.linux.org.uk&gt;</span>
<span class="quote">&gt; Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; Cc: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;</span>
<span class="quote">&gt; Cc: Alexander Graf &lt;agraf@suse.com&gt;</span>
<span class="quote">&gt; Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;</span>
<span class="quote">&gt; Cc: Paul Mackerras &lt;paulus@samba.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/kvm_mmu.h        |    5 ++--</span>
<span class="quote">&gt;  arch/arm/kvm/mmu.c                    |   10 ++++---</span>
<span class="quote">&gt;  arch/arm64/include/asm/kvm_mmu.h      |    3 +-</span>
<span class="quote">&gt;  arch/mips/include/asm/kvm_host.h      |    6 ++--</span>
<span class="quote">&gt;  arch/mips/kvm/emulate.c               |    2 +</span>
<span class="quote">&gt;  arch/mips/kvm/tlb.c                   |   14 +++++-----</span>
<span class="quote">&gt;  arch/powerpc/include/asm/kvm_book3s.h |    4 +--</span>
<span class="quote">&gt;  arch/powerpc/include/asm/kvm_ppc.h    |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s.c             |    6 ++--</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_32_mmu_host.c |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_64_mmu_host.c |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/e500.h               |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/e500_mmu_host.c      |    8 +++---</span>
<span class="quote">&gt;  arch/powerpc/kvm/trace_pr.h           |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/iommu.c                  |   11 ++++----</span>
<span class="quote">&gt;  arch/x86/kvm/mmu.c                    |   37 +++++++++++++-------------</span>
<span class="quote">&gt;  arch/x86/kvm/mmu_audit.c              |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/paging_tmpl.h            |    6 ++--</span>
<span class="quote">&gt;  arch/x86/kvm/vmx.c                    |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/x86.c                    |    2 +</span>
<span class="quote">&gt;  include/linux/kvm_host.h              |   37 +++++++++++++-------------</span>
<span class="quote">&gt;  include/linux/kvm_types.h             |    2 +</span>
<span class="quote">&gt;  virt/kvm/kvm_main.c                   |   47 +++++++++++++++++----------------</span>
<span class="quote">&gt;  23 files changed, 110 insertions(+), 104 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index 405aa1883307..8ebd282dfc2b 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -182,7 +182,8 @@ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	return (vcpu-&gt;arch.cp15[c1_SCTLR] &amp; 0b101) == 0b101;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt; +					       kvm_pfn_t pfn,</span>
<span class="quote">&gt;  					       unsigned long size,</span>
<span class="quote">&gt;  					       bool ipa_uncached)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -246,7 +247,7 @@ static inline void __kvm_flush_dcache_pte(pte_t pte)</span>
<span class="quote">&gt;  static inline void __kvm_flush_dcache_pmd(pmd_t pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long size = PMD_SIZE;</span>
<span class="quote">&gt; -	pfn_t pfn = pmd_pfn(pmd);</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = pmd_pfn(pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (size) {</span>
<span class="quote">&gt;  		void *va = kmap_atomic_pfn(pfn);</span>
<span class="quote">&gt; diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; index 6984342da13d..e2dcbfdc4a8c 100644</span>
<span class="quote">&gt; --- a/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; @@ -988,9 +988,9 @@ out:</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static bool transparent_hugepage_adjust(pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="quote">&gt; +static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn = *pfnp;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = *pfnp;</span>
<span class="quote">&gt;  	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; @@ -1202,7 +1202,7 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,</span>
<span class="quote">&gt;  	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, kvm_pfn_t pfn,</span>
<span class="quote">&gt;  				      unsigned long size, bool uncached)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	__coherent_cache_guest_page(vcpu, pfn, size, uncached);</span>
<span class="quote">&gt; @@ -1219,7 +1219,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;  	struct kvm *kvm = vcpu-&gt;kvm;</span>
<span class="quote">&gt;  	struct kvm_mmu_memory_cache *memcache = &amp;vcpu-&gt;arch.mmu_page_cache;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	pgprot_t mem_type = PAGE_S2;</span>
<span class="quote">&gt;  	bool fault_ipa_uncached;</span>
<span class="quote">&gt;  	bool logging_active = memslot_is_logging(memslot);</span>
<span class="quote">&gt; @@ -1347,7 +1347,7 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t *pmd;</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	bool pfn_valid = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	trace_kvm_access_fault(fault_ipa);</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index 61505676d085..385fc8cef82d 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -230,7 +230,8 @@ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	return (vcpu_sys_reg(vcpu, SCTLR_EL1) &amp; 0b101) == 0b101;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt; +					       kvm_pfn_t pfn,</span>
<span class="quote">&gt;  					       unsigned long size,</span>
<span class="quote">&gt;  					       bool ipa_uncached)</span>
<span class="quote">&gt;  {</span>
[...]

For the arm/arm64 part:
<span class="acked-by">Acked-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - Oct. 10, 2015, 8:35 p.m.</div>
<pre class="content">
On 10/10/2015 02:56, Dan Williams wrote:
<span class="quote">&gt; The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing</span>
<span class="quote">&gt; pfn_t in KVM to kvm_pfn_t [2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html</span>
<span class="quote">&gt; [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html</span>

Can you please change also the other types in include/linux/kvm_types.h?

Thanks,

Paolo
<span class="quote">
&gt; Cc: Dave Hansen &lt;dave@sr71.net&gt;</span>
<span class="quote">&gt; Cc: Gleb Natapov &lt;gleb@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt; Cc: Russell King &lt;linux@arm.linux.org.uk&gt;</span>
<span class="quote">&gt; Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; Cc: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;</span>
<span class="quote">&gt; Cc: Alexander Graf &lt;agraf@suse.com&gt;</span>
<span class="quote">&gt; Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;</span>
<span class="quote">&gt; Cc: Paul Mackerras &lt;paulus@samba.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/include/asm/kvm_mmu.h        |    5 ++--</span>
<span class="quote">&gt;  arch/arm/kvm/mmu.c                    |   10 ++++---</span>
<span class="quote">&gt;  arch/arm64/include/asm/kvm_mmu.h      |    3 +-</span>
<span class="quote">&gt;  arch/mips/include/asm/kvm_host.h      |    6 ++--</span>
<span class="quote">&gt;  arch/mips/kvm/emulate.c               |    2 +</span>
<span class="quote">&gt;  arch/mips/kvm/tlb.c                   |   14 +++++-----</span>
<span class="quote">&gt;  arch/powerpc/include/asm/kvm_book3s.h |    4 +--</span>
<span class="quote">&gt;  arch/powerpc/include/asm/kvm_ppc.h    |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s.c             |    6 ++--</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_32_mmu_host.c |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_64_mmu_host.c |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/e500.h               |    2 +</span>
<span class="quote">&gt;  arch/powerpc/kvm/e500_mmu_host.c      |    8 +++---</span>
<span class="quote">&gt;  arch/powerpc/kvm/trace_pr.h           |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/iommu.c                  |   11 ++++----</span>
<span class="quote">&gt;  arch/x86/kvm/mmu.c                    |   37 +++++++++++++-------------</span>
<span class="quote">&gt;  arch/x86/kvm/mmu_audit.c              |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/paging_tmpl.h            |    6 ++--</span>
<span class="quote">&gt;  arch/x86/kvm/vmx.c                    |    2 +</span>
<span class="quote">&gt;  arch/x86/kvm/x86.c                    |    2 +</span>
<span class="quote">&gt;  include/linux/kvm_host.h              |   37 +++++++++++++-------------</span>
<span class="quote">&gt;  include/linux/kvm_types.h             |    2 +</span>
<span class="quote">&gt;  virt/kvm/kvm_main.c                   |   47 +++++++++++++++++----------------</span>
<span class="quote">&gt;  23 files changed, 110 insertions(+), 104 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index 405aa1883307..8ebd282dfc2b 100644</span>
<span class="quote">&gt; --- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -182,7 +182,8 @@ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	return (vcpu-&gt;arch.cp15[c1_SCTLR] &amp; 0b101) == 0b101;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt; +					       kvm_pfn_t pfn,</span>
<span class="quote">&gt;  					       unsigned long size,</span>
<span class="quote">&gt;  					       bool ipa_uncached)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -246,7 +247,7 @@ static inline void __kvm_flush_dcache_pte(pte_t pte)</span>
<span class="quote">&gt;  static inline void __kvm_flush_dcache_pmd(pmd_t pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long size = PMD_SIZE;</span>
<span class="quote">&gt; -	pfn_t pfn = pmd_pfn(pmd);</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = pmd_pfn(pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (size) {</span>
<span class="quote">&gt;  		void *va = kmap_atomic_pfn(pfn);</span>
<span class="quote">&gt; diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; index 6984342da13d..e2dcbfdc4a8c 100644</span>
<span class="quote">&gt; --- a/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; @@ -988,9 +988,9 @@ out:</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static bool transparent_hugepage_adjust(pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="quote">&gt; +static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn = *pfnp;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = *pfnp;</span>
<span class="quote">&gt;  	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; @@ -1202,7 +1202,7 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,</span>
<span class="quote">&gt;  	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, kvm_pfn_t pfn,</span>
<span class="quote">&gt;  				      unsigned long size, bool uncached)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	__coherent_cache_guest_page(vcpu, pfn, size, uncached);</span>
<span class="quote">&gt; @@ -1219,7 +1219,7 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
<span class="quote">&gt;  	struct kvm *kvm = vcpu-&gt;kvm;</span>
<span class="quote">&gt;  	struct kvm_mmu_memory_cache *memcache = &amp;vcpu-&gt;arch.mmu_page_cache;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	pgprot_t mem_type = PAGE_S2;</span>
<span class="quote">&gt;  	bool fault_ipa_uncached;</span>
<span class="quote">&gt;  	bool logging_active = memslot_is_logging(memslot);</span>
<span class="quote">&gt; @@ -1347,7 +1347,7 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t *pmd;</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	bool pfn_valid = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	trace_kvm_access_fault(fault_ipa);</span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; index 61505676d085..385fc8cef82d 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="quote">&gt; @@ -230,7 +230,8 @@ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	return (vcpu_sys_reg(vcpu, SCTLR_EL1) &amp; 0b101) == 0b101;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="quote">&gt; +static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt; +					       kvm_pfn_t pfn,</span>
<span class="quote">&gt;  					       unsigned long size,</span>
<span class="quote">&gt;  					       bool ipa_uncached)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h</span>
<span class="quote">&gt; index 5a1a882e0a75..9c67f05a0a1b 100644</span>
<span class="quote">&gt; --- a/arch/mips/include/asm/kvm_host.h</span>
<span class="quote">&gt; +++ b/arch/mips/include/asm/kvm_host.h</span>
<span class="quote">&gt; @@ -101,9 +101,9 @@</span>
<span class="quote">&gt;  #define CAUSEF_DC			(_ULCAST_(1) &lt;&lt; 27)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern atomic_t kvm_mips_instance;</span>
<span class="quote">&gt; -extern pfn_t(*kvm_mips_gfn_to_pfn) (struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; -extern void (*kvm_mips_release_pfn_clean) (pfn_t pfn);</span>
<span class="quote">&gt; -extern bool(*kvm_mips_is_error_pfn) (pfn_t pfn);</span>
<span class="quote">&gt; +extern kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; +extern void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);</span>
<span class="quote">&gt; +extern bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct kvm_vm_stat {</span>
<span class="quote">&gt;  	u32 remote_tlb_flush;</span>
<span class="quote">&gt; diff --git a/arch/mips/kvm/emulate.c b/arch/mips/kvm/emulate.c</span>
<span class="quote">&gt; index d5fa3eaf39a1..476296cf37d3 100644</span>
<span class="quote">&gt; --- a/arch/mips/kvm/emulate.c</span>
<span class="quote">&gt; +++ b/arch/mips/kvm/emulate.c</span>
<span class="quote">&gt; @@ -1525,7 +1525,7 @@ int kvm_mips_sync_icache(unsigned long va, struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	struct kvm *kvm = vcpu-&gt;kvm;</span>
<span class="quote">&gt;  	unsigned long pa;</span>
<span class="quote">&gt;  	gfn_t gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	gfn = va &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/mips/kvm/tlb.c b/arch/mips/kvm/tlb.c</span>
<span class="quote">&gt; index aed0ac2a4972..570479c03bdc 100644</span>
<span class="quote">&gt; --- a/arch/mips/kvm/tlb.c</span>
<span class="quote">&gt; +++ b/arch/mips/kvm/tlb.c</span>
<span class="quote">&gt; @@ -38,13 +38,13 @@ atomic_t kvm_mips_instance;</span>
<span class="quote">&gt;  EXPORT_SYMBOL(kvm_mips_instance);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* These function pointers are initialized once the KVM module is loaded */</span>
<span class="quote">&gt; -pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt;  EXPORT_SYMBOL(kvm_mips_gfn_to_pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void (*kvm_mips_release_pfn_clean)(pfn_t pfn);</span>
<span class="quote">&gt; +void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  EXPORT_SYMBOL(kvm_mips_release_pfn_clean);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -bool (*kvm_mips_is_error_pfn)(pfn_t pfn);</span>
<span class="quote">&gt; +bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  EXPORT_SYMBOL(kvm_mips_is_error_pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  uint32_t kvm_mips_get_kernel_asid(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; @@ -144,7 +144,7 @@ EXPORT_SYMBOL(kvm_mips_dump_guest_tlbs);</span>
<span class="quote">&gt;  static int kvm_mips_map_page(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int srcu_idx, err = 0;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (kvm-&gt;arch.guest_pmap[gfn] != KVM_INVALID_PAGE)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -262,7 +262,7 @@ int kvm_mips_handle_kseg0_tlb_fault(unsigned long badvaddr,</span>
<span class="quote">&gt;  				    struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	gfn_t gfn;</span>
<span class="quote">&gt; -	pfn_t pfn0, pfn1;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn0, pfn1;</span>
<span class="quote">&gt;  	unsigned long vaddr = 0;</span>
<span class="quote">&gt;  	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;</span>
<span class="quote">&gt;  	int even;</span>
<span class="quote">&gt; @@ -313,7 +313,7 @@ EXPORT_SYMBOL(kvm_mips_handle_kseg0_tlb_fault);</span>
<span class="quote">&gt;  int kvm_mips_handle_commpage_tlb_fault(unsigned long badvaddr,</span>
<span class="quote">&gt;  	struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn0, pfn1;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn0, pfn1;</span>
<span class="quote">&gt;  	unsigned long flags, old_entryhi = 0, vaddr = 0;</span>
<span class="quote">&gt;  	unsigned long entrylo0 = 0, entrylo1 = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -360,7 +360,7 @@ int kvm_mips_handle_mapped_seg_tlb_fault(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;</span>
<span class="quote">&gt;  	struct kvm *kvm = vcpu-&gt;kvm;</span>
<span class="quote">&gt; -	pfn_t pfn0, pfn1;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn0, pfn1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((tlb-&gt;tlb_hi &amp; VPN2_MASK) == 0) {</span>
<span class="quote">&gt;  		pfn0 = 0;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="quote">&gt; index 9fac01cb89c1..8f39796c9da8 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="quote">&gt; @@ -154,8 +154,8 @@ extern void kvmppc_set_bat(struct kvm_vcpu *vcpu, struct kvmppc_bat *bat,</span>
<span class="quote">&gt;  			   bool upper, u32 val);</span>
<span class="quote">&gt;  extern void kvmppc_giveup_ext(struct kvm_vcpu *vcpu, ulong msr);</span>
<span class="quote">&gt;  extern int kvmppc_emulate_paired_single(struct kvm_run *run, struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt; -extern pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="quote">&gt; -			bool *writable);</span>
<span class="quote">&gt; +extern kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa,</span>
<span class="quote">&gt; +			bool writing, bool *writable);</span>
<span class="quote">&gt;  extern void kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,</span>
<span class="quote">&gt;  			unsigned long *rmap, long pte_index, int realmode);</span>
<span class="quote">&gt;  extern void kvmppc_update_rmap_change(unsigned long *rmap, unsigned long psize);</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="quote">&gt; index c6ef05bd0765..2241d5357129 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="quote">&gt; @@ -515,7 +515,7 @@ void kvmppc_claim_lpid(long lpid);</span>
<span class="quote">&gt;  void kvmppc_free_lpid(long lpid);</span>
<span class="quote">&gt;  void kvmppc_init_lpid(unsigned long nr_lpids);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void kvmppc_mmu_flush_icache(pfn_t pfn)</span>
<span class="quote">&gt; +static inline void kvmppc_mmu_flush_icache(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/book3s.c b/arch/powerpc/kvm/book3s.c</span>
<span class="quote">&gt; index 099c79d8c160..638c6d9be9e0 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/book3s.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/book3s.c</span>
<span class="quote">&gt; @@ -366,7 +366,7 @@ int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvmppc_core_prepare_to_enter);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="quote">&gt; +kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="quote">&gt;  			bool *writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	ulong mp_pa = vcpu-&gt;arch.magic_page_pa &amp; KVM_PAM;</span>
<span class="quote">&gt; @@ -379,9 +379,9 @@ pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="quote">&gt;  	gpa &amp;= ~0xFFFULL;</span>
<span class="quote">&gt;  	if (unlikely(mp_pa) &amp;&amp; unlikely((gpa &amp; KVM_PAM) == mp_pa)) {</span>
<span class="quote">&gt;  		ulong shared_page = ((ulong)vcpu-&gt;arch.shared) &amp; PAGE_MASK;</span>
<span class="quote">&gt; -		pfn_t pfn;</span>
<span class="quote">&gt; +		kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		pfn = (pfn_t)virt_to_phys((void*)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		pfn = (kvm_pfn_t)virt_to_phys((void*)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  		get_page(pfn_to_page(pfn));</span>
<span class="quote">&gt;  		if (writable)</span>
<span class="quote">&gt;  			*writable = true;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/book3s_32_mmu_host.c b/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="quote">&gt; index d5c9bfeb0c9c..55c4d51ea3e2 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="quote">&gt; @@ -142,7 +142,7 @@ extern char etext[];</span>
<span class="quote">&gt;  int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,</span>
<span class="quote">&gt;  			bool iswrite)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t hpaddr;</span>
<span class="quote">&gt; +	kvm_pfn_t hpaddr;</span>
<span class="quote">&gt;  	u64 vpn;</span>
<span class="quote">&gt;  	u64 vsid;</span>
<span class="quote">&gt;  	struct kvmppc_sid_map *map;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/book3s_64_mmu_host.c b/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="quote">&gt; index 79ad35abd196..913cd2198fa6 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="quote">&gt; @@ -83,7 +83,7 @@ int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,</span>
<span class="quote">&gt;  			bool iswrite)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long vpn;</span>
<span class="quote">&gt; -	pfn_t hpaddr;</span>
<span class="quote">&gt; +	kvm_pfn_t hpaddr;</span>
<span class="quote">&gt;  	ulong hash, hpteg;</span>
<span class="quote">&gt;  	u64 vsid;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/e500.h b/arch/powerpc/kvm/e500.h</span>
<span class="quote">&gt; index 72920bed3ac6..94f04fcb373e 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/e500.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/e500.h</span>
<span class="quote">&gt; @@ -41,7 +41,7 @@ enum vcpu_ftr {</span>
<span class="quote">&gt;  #define E500_TLB_MAS2_ATTR	(0x7f)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct tlbe_ref {</span>
<span class="quote">&gt; -	pfn_t pfn;		/* valid only for TLB0, except briefly */</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;		/* valid only for TLB0, except briefly */</span>
<span class="quote">&gt;  	unsigned int flags;	/* E500_TLB_* */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/e500_mmu_host.c b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="quote">&gt; index 4d33e199edcc..8a5bb6dfcc2d 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="quote">&gt; @@ -163,9 +163,9 @@ void kvmppc_map_magic(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	struct kvm_book3e_206_tlb_entry magic;</span>
<span class="quote">&gt;  	ulong shared_page = ((ulong)vcpu-&gt;arch.shared) &amp; PAGE_MASK;</span>
<span class="quote">&gt;  	unsigned int stid;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pfn = (pfn_t)virt_to_phys((void *)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +	pfn = (kvm_pfn_t)virt_to_phys((void *)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	get_page(pfn_to_page(pfn));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt; @@ -246,7 +246,7 @@ static inline int tlbe_is_writable(struct kvm_book3e_206_tlb_entry *tlbe)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void kvmppc_e500_ref_setup(struct tlbe_ref *ref,</span>
<span class="quote">&gt;  					 struct kvm_book3e_206_tlb_entry *gtlbe,</span>
<span class="quote">&gt; -					 pfn_t pfn, unsigned int wimg)</span>
<span class="quote">&gt; +					 kvm_pfn_t pfn, unsigned int wimg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	ref-&gt;pfn = pfn;</span>
<span class="quote">&gt;  	ref-&gt;flags = E500_TLB_VALID;</span>
<span class="quote">&gt; @@ -309,7 +309,7 @@ static void kvmppc_e500_setup_stlbe(</span>
<span class="quote">&gt;  	int tsize, struct tlbe_ref *ref, u64 gvaddr,</span>
<span class="quote">&gt;  	struct kvm_book3e_206_tlb_entry *stlbe)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn = ref-&gt;pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = ref-&gt;pfn;</span>
<span class="quote">&gt;  	u32 pr = vcpu-&gt;arch.shared-&gt;msr &amp; MSR_PR;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	BUG_ON(!(ref-&gt;flags &amp; E500_TLB_VALID));</span>
<span class="quote">&gt; diff --git a/arch/powerpc/kvm/trace_pr.h b/arch/powerpc/kvm/trace_pr.h</span>
<span class="quote">&gt; index 810507cb688a..d44f324184fb 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/kvm/trace_pr.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/kvm/trace_pr.h</span>
<span class="quote">&gt; @@ -30,7 +30,7 @@ TRACE_EVENT(kvm_book3s_reenter,</span>
<span class="quote">&gt;  #ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  TRACE_EVENT(kvm_book3s_64_mmu_map,</span>
<span class="quote">&gt; -	TP_PROTO(int rflags, ulong hpteg, ulong va, pfn_t hpaddr,</span>
<span class="quote">&gt; +	TP_PROTO(int rflags, ulong hpteg, ulong va, kvm_pfn_t hpaddr,</span>
<span class="quote">&gt;  		 struct kvmppc_pte *orig_pte),</span>
<span class="quote">&gt;  	TP_ARGS(rflags, hpteg, va, hpaddr, orig_pte),</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/iommu.c b/arch/x86/kvm/iommu.c</span>
<span class="quote">&gt; index 5c520ebf6343..a22a488b4622 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/iommu.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/iommu.c</span>
<span class="quote">&gt; @@ -43,11 +43,11 @@ static int kvm_iommu_unmap_memslots(struct kvm *kvm);</span>
<span class="quote">&gt;  static void kvm_iommu_put_pages(struct kvm *kvm,</span>
<span class="quote">&gt;  				gfn_t base_gfn, unsigned long npages);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt; +static kvm_pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt;  			   unsigned long npages)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	gfn_t end_gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pfn     = gfn_to_pfn_memslot(slot, gfn);</span>
<span class="quote">&gt;  	end_gfn = gfn + npages;</span>
<span class="quote">&gt; @@ -62,7 +62,8 @@ static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt;  	return pfn;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)</span>
<span class="quote">&gt; +static void kvm_unpin_pages(struct kvm *kvm, kvm_pfn_t pfn,</span>
<span class="quote">&gt; +		unsigned long npages)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long i;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -73,7 +74,7 @@ static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)</span>
<span class="quote">&gt;  int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	gfn_t gfn, end_gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	int r = 0;</span>
<span class="quote">&gt;  	struct iommu_domain *domain = kvm-&gt;arch.iommu_domain;</span>
<span class="quote">&gt;  	int flags;</span>
<span class="quote">&gt; @@ -275,7 +276,7 @@ static void kvm_iommu_put_pages(struct kvm *kvm,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct iommu_domain *domain;</span>
<span class="quote">&gt;  	gfn_t end_gfn, gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	u64 phys;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	domain  = kvm-&gt;arch.iommu_domain;</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; index ff606f507913..6ab963ae0427 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; @@ -259,7 +259,7 @@ static unsigned get_mmio_spte_access(u64 spte)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static bool set_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,</span>
<span class="quote">&gt; -			  pfn_t pfn, unsigned access)</span>
<span class="quote">&gt; +			  kvm_pfn_t pfn, unsigned access)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (unlikely(is_noslot_pfn(pfn))) {</span>
<span class="quote">&gt;  		mark_mmio_spte(vcpu, sptep, gfn, access);</span>
<span class="quote">&gt; @@ -325,7 +325,7 @@ static int is_last_spte(u64 pte, int level)</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static pfn_t spte_to_pfn(u64 pte)</span>
<span class="quote">&gt; +static kvm_pfn_t spte_to_pfn(u64 pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return (pte &amp; PT64_BASE_ADDR_MASK) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -587,7 +587,7 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int mmu_spte_clear_track_bits(u64 *sptep)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	u64 old_spte = *sptep;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!spte_has_volatile_bits(old_spte))</span>
<span class="quote">&gt; @@ -1369,7 +1369,7 @@ static int kvm_set_pte_rmapp(struct kvm *kvm, unsigned long *rmapp,</span>
<span class="quote">&gt;  	int need_flush = 0;</span>
<span class="quote">&gt;  	u64 new_spte;</span>
<span class="quote">&gt;  	pte_t *ptep = (pte_t *)data;</span>
<span class="quote">&gt; -	pfn_t new_pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t new_pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	WARN_ON(pte_huge(*ptep));</span>
<span class="quote">&gt;  	new_pfn = pte_pfn(*ptep);</span>
<span class="quote">&gt; @@ -2456,7 +2456,7 @@ static int mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static bool kvm_is_mmio_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (pfn_valid(pfn))</span>
<span class="quote">&gt;  		return !is_zero_pfn(pfn) &amp;&amp; PageReserved(pfn_to_page(pfn));</span>
<span class="quote">&gt; @@ -2466,7 +2466,7 @@ static bool kvm_is_mmio_pfn(pfn_t pfn)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,</span>
<span class="quote">&gt;  		    unsigned pte_access, int level,</span>
<span class="quote">&gt; -		    gfn_t gfn, pfn_t pfn, bool speculative,</span>
<span class="quote">&gt; +		    gfn_t gfn, kvm_pfn_t pfn, bool speculative,</span>
<span class="quote">&gt;  		    bool can_unsync, bool host_writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	u64 spte;</span>
<span class="quote">&gt; @@ -2546,7 +2546,7 @@ done:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,</span>
<span class="quote">&gt;  			 unsigned pte_access, int write_fault, int *emulate,</span>
<span class="quote">&gt; -			 int level, gfn_t gfn, pfn_t pfn, bool speculative,</span>
<span class="quote">&gt; +			 int level, gfn_t gfn, kvm_pfn_t pfn, bool speculative,</span>
<span class="quote">&gt;  			 bool host_writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int was_rmapped = 0;</span>
<span class="quote">&gt; @@ -2606,7 +2606,7 @@ static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,</span>
<span class="quote">&gt;  	kvm_release_pfn_clean(pfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
<span class="quote">&gt; +static kvm_pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
<span class="quote">&gt;  				     bool no_dirty_log)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm_memory_slot *slot;</span>
<span class="quote">&gt; @@ -2689,7 +2689,7 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,</span>
<span class="quote">&gt; -			int map_writable, int level, gfn_t gfn, pfn_t pfn,</span>
<span class="quote">&gt; +			int map_writable, int level, gfn_t gfn, kvm_pfn_t pfn,</span>
<span class="quote">&gt;  			bool prefault)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm_shadow_walk_iterator iterator;</span>
<span class="quote">&gt; @@ -2739,7 +2739,7 @@ static void kvm_send_hwpoison_signal(unsigned long address, struct task_struct *</span>
<span class="quote">&gt;  	send_sig_info(SIGBUS, &amp;info, tsk);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, pfn_t pfn)</span>
<span class="quote">&gt; +static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Do not cache the mmio info caused by writing the readonly gfn</span>
<span class="quote">&gt; @@ -2759,9 +2759,10 @@ static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, pfn_t pfn)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt; -					gfn_t *gfnp, pfn_t *pfnp, int *levelp)</span>
<span class="quote">&gt; +					gfn_t *gfnp, kvm_pfn_t *pfnp,</span>
<span class="quote">&gt; +					int *levelp)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn = *pfnp;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = *pfnp;</span>
<span class="quote">&gt;  	gfn_t gfn = *gfnp;</span>
<span class="quote">&gt;  	int level = *levelp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2800,7 +2801,7 @@ static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,</span>
<span class="quote">&gt; -				pfn_t pfn, unsigned access, int *ret_val)</span>
<span class="quote">&gt; +				kvm_pfn_t pfn, unsigned access, int *ret_val)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	bool ret = true;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2954,7 +2955,7 @@ exit:</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,</span>
<span class="quote">&gt; -			 gva_t gva, pfn_t *pfn, bool write, bool *writable);</span>
<span class="quote">&gt; +			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable);</span>
<span class="quote">&gt;  static void make_mmu_pages_available(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,</span>
<span class="quote">&gt; @@ -2963,7 +2964,7 @@ static int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt;  	int level;</span>
<span class="quote">&gt;  	int force_pt_level;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	unsigned long mmu_seq;</span>
<span class="quote">&gt;  	bool map_writable, write = error_code &amp; PFERR_WRITE_MASK;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -3435,7 +3436,7 @@ static bool can_do_async_pf(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,</span>
<span class="quote">&gt; -			 gva_t gva, pfn_t *pfn, bool write, bool *writable)</span>
<span class="quote">&gt; +			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm_memory_slot *slot;</span>
<span class="quote">&gt;  	bool async;</span>
<span class="quote">&gt; @@ -3473,7 +3474,7 @@ check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)</span>
<span class="quote">&gt;  static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,</span>
<span class="quote">&gt;  			  bool prefault)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt;  	int level;</span>
<span class="quote">&gt;  	int force_pt_level;</span>
<span class="quote">&gt; @@ -4627,7 +4628,7 @@ static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,</span>
<span class="quote">&gt;  	u64 *sptep;</span>
<span class="quote">&gt;  	struct rmap_iterator iter;</span>
<span class="quote">&gt;  	int need_tlb_flush = 0;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	struct kvm_mmu_page *sp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  restart:</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/mmu_audit.c b/arch/x86/kvm/mmu_audit.c</span>
<span class="quote">&gt; index 03d518e499a6..37a4d14115c0 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/mmu_audit.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/mmu_audit.c</span>
<span class="quote">&gt; @@ -97,7 +97,7 @@ static void audit_mappings(struct kvm_vcpu *vcpu, u64 *sptep, int level)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm_mmu_page *sp;</span>
<span class="quote">&gt;  	gfn_t gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	hpa_t hpa;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	sp = page_header(__pa(sptep));</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h</span>
<span class="quote">&gt; index 736e6ab8784d..9dd02cb74724 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/paging_tmpl.h</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/paging_tmpl.h</span>
<span class="quote">&gt; @@ -456,7 +456,7 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned pte_access;</span>
<span class="quote">&gt;  	gfn_t gfn;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt; @@ -551,7 +551,7 @@ static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,</span>
<span class="quote">&gt;  static int FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,</span>
<span class="quote">&gt;  			 struct guest_walker *gw,</span>
<span class="quote">&gt;  			 int write_fault, int hlevel,</span>
<span class="quote">&gt; -			 pfn_t pfn, bool map_writable, bool prefault)</span>
<span class="quote">&gt; +			 kvm_pfn_t pfn, bool map_writable, bool prefault)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm_mmu_page *sp = NULL;</span>
<span class="quote">&gt;  	struct kvm_shadow_walk_iterator it;</span>
<span class="quote">&gt; @@ -696,7 +696,7 @@ static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,</span>
<span class="quote">&gt;  	int user_fault = error_code &amp; PFERR_USER_MASK;</span>
<span class="quote">&gt;  	struct guest_walker walker;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  	int level = PT_PAGE_TABLE_LEVEL;</span>
<span class="quote">&gt;  	int force_pt_level;</span>
<span class="quote">&gt;  	unsigned long mmu_seq;</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; index 06ef4908ba61..d401ed6874bd 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; @@ -4046,7 +4046,7 @@ out:</span>
<span class="quote">&gt;  static int init_rmode_identity_map(struct kvm *kvm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int i, idx, r = 0;</span>
<span class="quote">&gt; -	pfn_t identity_map_pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t identity_map_pfn;</span>
<span class="quote">&gt;  	u32 tmp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!enable_ept)</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; index 92511d4b7236..8fc5ca584edf 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; @@ -4935,7 +4935,7 @@ static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t cr2,</span>
<span class="quote">&gt;  				  int emulation_type)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	gpa_t gpa = cr2;</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (emulation_type &amp; EMULTYPE_NO_REEXECUTE)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt; diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="quote">&gt; index 1bef9e21e725..2420b43f3acc 100644</span>
<span class="quote">&gt; --- a/include/linux/kvm_host.h</span>
<span class="quote">&gt; +++ b/include/linux/kvm_host.h</span>
<span class="quote">&gt; @@ -65,7 +65,7 @@</span>
<span class="quote">&gt;   * error pfns indicate that the gfn is in slot but faild to</span>
<span class="quote">&gt;   * translate it to pfn on host.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static inline bool is_error_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +static inline bool is_error_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return !!(pfn &amp; KVM_PFN_ERR_MASK);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -75,13 +75,13 @@ static inline bool is_error_pfn(pfn_t pfn)</span>
<span class="quote">&gt;   * translated to pfn - it is not in slot or failed to</span>
<span class="quote">&gt;   * translate it to pfn.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static inline bool is_error_noslot_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +static inline bool is_error_noslot_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return !!(pfn &amp; KVM_PFN_ERR_NOSLOT_MASK);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* noslot pfn indicates that the gfn is not in slot. */</span>
<span class="quote">&gt; -static inline bool is_noslot_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +static inline bool is_noslot_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return pfn == KVM_PFN_NOSLOT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -569,19 +569,20 @@ void kvm_release_page_clean(struct page *page);</span>
<span class="quote">&gt;  void kvm_release_page_dirty(struct page *page);</span>
<span class="quote">&gt;  void kvm_set_page_accessed(struct page *page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="quote">&gt;  		      bool *writable);</span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
<span class="quote">&gt; -			   bool *async, bool write_fault, bool *writable);</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt; +			       bool atomic, bool *async, bool write_fault,</span>
<span class="quote">&gt; +			       bool *writable);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void kvm_release_pfn_clean(pfn_t pfn);</span>
<span class="quote">&gt; -void kvm_set_pfn_dirty(pfn_t pfn);</span>
<span class="quote">&gt; -void kvm_set_pfn_accessed(pfn_t pfn);</span>
<span class="quote">&gt; -void kvm_get_pfn(pfn_t pfn);</span>
<span class="quote">&gt; +void kvm_release_pfn_clean(kvm_pfn_t pfn);</span>
<span class="quote">&gt; +void kvm_set_pfn_dirty(kvm_pfn_t pfn);</span>
<span class="quote">&gt; +void kvm_set_pfn_accessed(kvm_pfn_t pfn);</span>
<span class="quote">&gt; +void kvm_get_pfn(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,</span>
<span class="quote">&gt;  			int len);</span>
<span class="quote">&gt; @@ -607,8 +608,8 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt; -pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt; +kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt;  struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt;  unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="quote">&gt;  unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);</span>
<span class="quote">&gt; @@ -789,7 +790,7 @@ void kvm_arch_sync_events(struct kvm *kvm);</span>
<span class="quote">&gt;  int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  void kvm_vcpu_kick(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -bool kvm_is_reserved_pfn(pfn_t pfn);</span>
<span class="quote">&gt; +bool kvm_is_reserved_pfn(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct kvm_irq_ack_notifier {</span>
<span class="quote">&gt;  	struct hlist_node link;</span>
<span class="quote">&gt; @@ -940,7 +941,7 @@ static inline gfn_t gpa_to_gfn(gpa_t gpa)</span>
<span class="quote">&gt;  	return (gfn_t)(gpa &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline hpa_t pfn_to_hpa(pfn_t pfn)</span>
<span class="quote">&gt; +static inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return (hpa_t)pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/include/linux/kvm_types.h b/include/linux/kvm_types.h</span>
<span class="quote">&gt; index 1b47a185c2f0..8bf259dae9f6 100644</span>
<span class="quote">&gt; --- a/include/linux/kvm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/kvm_types.h</span>
<span class="quote">&gt; @@ -53,7 +53,7 @@ typedef unsigned long  hva_t;</span>
<span class="quote">&gt;  typedef u64            hpa_t;</span>
<span class="quote">&gt;  typedef u64            hfn_t;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -typedef hfn_t pfn_t;</span>
<span class="quote">&gt; +typedef hfn_t kvm_pfn_t;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct gfn_to_hva_cache {</span>
<span class="quote">&gt;  	u64 generation;</span>
<span class="quote">&gt; diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; index 8db1d9361993..02cd2eddd3ff 100644</span>
<span class="quote">&gt; --- a/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; +++ b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; @@ -111,7 +111,7 @@ static void hardware_disable_all(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void kvm_io_bus_destroy(struct kvm_io_bus *bus);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void kvm_release_pfn_dirty(pfn_t pfn);</span>
<span class="quote">&gt; +static void kvm_release_pfn_dirty(kvm_pfn_t pfn);</span>
<span class="quote">&gt;  static void mark_page_dirty_in_slot(struct kvm_memory_slot *memslot, gfn_t gfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  __visible bool kvm_rebooting;</span>
<span class="quote">&gt; @@ -119,7 +119,7 @@ EXPORT_SYMBOL_GPL(kvm_rebooting);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static bool largepages_enabled = true;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -bool kvm_is_reserved_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +bool kvm_is_reserved_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (pfn_valid(pfn))</span>
<span class="quote">&gt;  		return PageReserved(pfn_to_page(pfn));</span>
<span class="quote">&gt; @@ -1296,7 +1296,7 @@ static inline int check_user_page_hwpoison(unsigned long addr)</span>
<span class="quote">&gt;   * true indicates success, otherwise false is returned.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt; -			    bool write_fault, bool *writable, pfn_t *pfn)</span>
<span class="quote">&gt; +			    bool write_fault, bool *writable, kvm_pfn_t *pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page[1];</span>
<span class="quote">&gt;  	int npages;</span>
<span class="quote">&gt; @@ -1329,7 +1329,7 @@ static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt;   * 1 indicates success, -errno is returned if error is detected.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,</span>
<span class="quote">&gt; -			   bool *writable, pfn_t *pfn)</span>
<span class="quote">&gt; +			   bool *writable, kvm_pfn_t *pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page[1];</span>
<span class="quote">&gt;  	int npages = 0;</span>
<span class="quote">&gt; @@ -1393,11 +1393,11 @@ static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
<span class="quote">&gt;   * 2): @write_fault = false &amp;&amp; @writable, @writable will tell the caller</span>
<span class="quote">&gt;   *     whether the mapping is writable.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt; +static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt;  			bool write_fault, bool *writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; -	pfn_t pfn = 0;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn = 0;</span>
<span class="quote">&gt;  	int npages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* we can do it either atomically or asynchronously, not both */</span>
<span class="quote">&gt; @@ -1438,8 +1438,9 @@ exit:</span>
<span class="quote">&gt;  	return pfn;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
<span class="quote">&gt; -			   bool *async, bool write_fault, bool *writable)</span>
<span class="quote">&gt; +kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt; +			       bool atomic, bool *async, bool write_fault,</span>
<span class="quote">&gt; +			       bool *writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1460,7 +1461,7 @@ pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="quote">&gt;  		      bool *writable)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,</span>
<span class="quote">&gt; @@ -1468,37 +1469,37 @@ pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return gfn_to_pfn_memslot_atomic(gfn_to_memslot(kvm, gfn), gfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_atomic);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_pfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="quote">&gt; +kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1521,7 +1522,7 @@ int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *kvm_pfn_to_page(pfn_t pfn)</span>
<span class="quote">&gt; +static struct page *kvm_pfn_to_page(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (is_error_noslot_pfn(pfn))</span>
<span class="quote">&gt;  		return KVM_ERR_PTR_BAD_PAGE;</span>
<span class="quote">&gt; @@ -1536,7 +1537,7 @@ static struct page *kvm_pfn_to_page(pfn_t pfn)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pfn = gfn_to_pfn(kvm, gfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1546,7 +1547,7 @@ EXPORT_SYMBOL_GPL(gfn_to_page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pfn_t pfn;</span>
<span class="quote">&gt; +	kvm_pfn_t pfn;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pfn = kvm_vcpu_gfn_to_pfn(vcpu, gfn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1562,7 +1563,7 @@ void kvm_release_page_clean(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvm_release_page_clean);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void kvm_release_pfn_clean(pfn_t pfn)</span>
<span class="quote">&gt; +void kvm_release_pfn_clean(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!is_error_noslot_pfn(pfn) &amp;&amp; !kvm_is_reserved_pfn(pfn))</span>
<span class="quote">&gt;  		put_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; @@ -1577,13 +1578,13 @@ void kvm_release_page_dirty(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvm_release_page_dirty);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void kvm_release_pfn_dirty(pfn_t pfn)</span>
<span class="quote">&gt; +static void kvm_release_pfn_dirty(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	kvm_set_pfn_dirty(pfn);</span>
<span class="quote">&gt;  	kvm_release_pfn_clean(pfn);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void kvm_set_pfn_dirty(pfn_t pfn)</span>
<span class="quote">&gt; +void kvm_set_pfn_dirty(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!kvm_is_reserved_pfn(pfn)) {</span>
<span class="quote">&gt;  		struct page *page = pfn_to_page(pfn);</span>
<span class="quote">&gt; @@ -1594,14 +1595,14 @@ void kvm_set_pfn_dirty(pfn_t pfn)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void kvm_set_pfn_accessed(pfn_t pfn)</span>
<span class="quote">&gt; +void kvm_set_pfn_accessed(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!kvm_is_reserved_pfn(pfn))</span>
<span class="quote">&gt;  		mark_page_accessed(pfn_to_page(pfn));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void kvm_get_pfn(pfn_t pfn)</span>
<span class="quote">&gt; +void kvm_get_pfn(kvm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!kvm_is_reserved_pfn(pfn))</span>
<span class="quote">&gt;  		get_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt; </span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Oct. 10, 2015, 8:57 p.m.</div>
<pre class="content">
On Sat, Oct 10, 2015 at 1:35 PM, Paolo Bonzini &lt;pbonzini@redhat.com&gt; wrote:
<span class="quote">&gt; On 10/10/2015 02:56, Dan Williams wrote:</span>
<span class="quote">&gt;&gt; The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing</span>
<span class="quote">&gt;&gt; pfn_t in KVM to kvm_pfn_t [2].</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html</span>
<span class="quote">&gt;&gt; [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can you please change also the other types in include/linux/kvm_types.h?</span>

Hmm, all those seem kvm specific already.  I&#39;d only prefix them with
kvm_ if they collided with a &quot;core&quot; type.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - Oct. 12, 2015, 12:51 p.m.</div>
<pre class="content">
On 10/10/2015 22:57, Dan Williams wrote:
<span class="quote">&gt; On Sat, Oct 10, 2015 at 1:35 PM, Paolo Bonzini &lt;pbonzini@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 10/10/2015 02:56, Dan Williams wrote:</span>
<span class="quote">&gt;&gt;&gt; The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing</span>
<span class="quote">&gt;&gt;&gt; pfn_t in KVM to kvm_pfn_t [2].</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html</span>
<span class="quote">&gt;&gt;&gt; [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Can you please change also the other types in include/linux/kvm_types.h?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, all those seem kvm specific already.  I&#39;d only prefix them with</span>
<span class="quote">&gt; kvm_ if they collided with a &quot;core&quot; type.</span>

But they are all related and the code becomes uglier if you only prefix
one of them.  If you don&#39;t convert all of them, I will do it anyway as
soon as this patch get in.

Since it touches a lot of KVM files, we should synchronize in order to
avoid conflicts and gnashing of teeth.  What tree is this patch going
in?  You could provide me a commit SHA1 for this patch (well, its
definitive version) based on Linus&#39;s tree (so that I can merge it in my
tree as well), or I could commit it and provide the SHA1 to the
maintainer of said tree.

Paolo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Oct. 12, 2015, 4:16 p.m.</div>
<pre class="content">
On Mon, Oct 12, 2015 at 5:51 AM, Paolo Bonzini &lt;pbonzini@redhat.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 10/10/2015 22:57, Dan Williams wrote:</span>
<span class="quote">&gt;&gt; On Sat, Oct 10, 2015 at 1:35 PM, Paolo Bonzini &lt;pbonzini@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 10/10/2015 02:56, Dan Williams wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; The core has developed a need for a &quot;pfn_t&quot; type [1].  Move the existing</span>
<span class="quote">&gt;&gt;&gt;&gt; pfn_t in KVM to kvm_pfn_t [2].</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; [1]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002199.html</span>
<span class="quote">&gt;&gt;&gt;&gt; [2]: https://lists.01.org/pipermail/linux-nvdimm/2015-September/002218.html</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Can you please change also the other types in include/linux/kvm_types.h?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hmm, all those seem kvm specific already.  I&#39;d only prefix them with</span>
<span class="quote">&gt;&gt; kvm_ if they collided with a &quot;core&quot; type.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But they are all related and the code becomes uglier if you only prefix</span>
<span class="quote">&gt; one of them.  If you don&#39;t convert all of them, I will do it anyway as</span>
<span class="quote">&gt; soon as this patch get in.</span>

Ok.
<span class="quote">
&gt; Since it touches a lot of KVM files, we should synchronize in order to</span>
<span class="quote">&gt; avoid conflicts and gnashing of teeth.  What tree is this patch going</span>
<span class="quote">&gt; in?  You could provide me a commit SHA1 for this patch (well, its</span>
<span class="quote">&gt; definitive version) based on Linus&#39;s tree (so that I can merge it in my</span>
<span class="quote">&gt; tree as well), or I could commit it and provide the SHA1 to the</span>
<span class="quote">&gt; maintainer of said tree.</span>

The kvm_pfn_t conversion is only needed if the new pfn_t
infrastructure moves forward, and at this point it still needs some
review feedback.

How about this, care to send conversion patches for the rest? ...based on:

    https://git.kernel.org/cgit/linux/kernel/git/djbw/nvdimm.git/log/?h=libnvdimm-pending

When/if the new pfn_t bits move forward I&#39;ll carry them in the same
pull request through the nvdimm.git tree.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">index 405aa1883307..8ebd282dfc2b 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -182,7 +182,8 @@</span> <span class="p_context"> static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
 	return (vcpu-&gt;arch.cp15[c1_SCTLR] &amp; 0b101) == 0b101;
 }
 
<span class="p_del">-static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="p_add">+static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="p_add">+					       kvm_pfn_t pfn,</span>
 					       unsigned long size,
 					       bool ipa_uncached)
 {
<span class="p_chunk">@@ -246,7 +247,7 @@</span> <span class="p_context"> static inline void __kvm_flush_dcache_pte(pte_t pte)</span>
 static inline void __kvm_flush_dcache_pmd(pmd_t pmd)
 {
 	unsigned long size = PMD_SIZE;
<span class="p_del">-	pfn_t pfn = pmd_pfn(pmd);</span>
<span class="p_add">+	kvm_pfn_t pfn = pmd_pfn(pmd);</span>
 
 	while (size) {
 		void *va = kmap_atomic_pfn(pfn);
<span class="p_header">diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="p_header">index 6984342da13d..e2dcbfdc4a8c 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmu.c</span>
<span class="p_chunk">@@ -988,9 +988,9 @@</span> <span class="p_context"> out:</span>
 	return ret;
 }
 
<span class="p_del">-static bool transparent_hugepage_adjust(pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="p_add">+static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
 {
<span class="p_del">-	pfn_t pfn = *pfnp;</span>
<span class="p_add">+	kvm_pfn_t pfn = *pfnp;</span>
 	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;
 
 	if (PageTransCompound(pfn_to_page(pfn))) {
<span class="p_chunk">@@ -1202,7 +1202,7 @@</span> <span class="p_context"> void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,</span>
 	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
 }
 
<span class="p_del">-static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="p_add">+static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, kvm_pfn_t pfn,</span>
 				      unsigned long size, bool uncached)
 {
 	__coherent_cache_guest_page(vcpu, pfn, size, uncached);
<span class="p_chunk">@@ -1219,7 +1219,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 	struct kvm *kvm = vcpu-&gt;kvm;
 	struct kvm_mmu_memory_cache *memcache = &amp;vcpu-&gt;arch.mmu_page_cache;
 	struct vm_area_struct *vma;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	pgprot_t mem_type = PAGE_S2;
 	bool fault_ipa_uncached;
 	bool logging_active = memslot_is_logging(memslot);
<span class="p_chunk">@@ -1347,7 +1347,7 @@</span> <span class="p_context"> static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
 {
 	pmd_t *pmd;
 	pte_t *pte;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	bool pfn_valid = false;
 
 	trace_kvm_access_fault(fault_ipa);
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">index 61505676d085..385fc8cef82d 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_mmu.h</span>
<span class="p_chunk">@@ -230,7 +230,8 @@</span> <span class="p_context"> static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)</span>
 	return (vcpu_sys_reg(vcpu, SCTLR_EL1) &amp; 0b101) == 0b101;
 }
 
<span class="p_del">-static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,</span>
<span class="p_add">+static inline void __coherent_cache_guest_page(struct kvm_vcpu *vcpu,</span>
<span class="p_add">+					       kvm_pfn_t pfn,</span>
 					       unsigned long size,
 					       bool ipa_uncached)
 {
<span class="p_header">diff --git a/arch/mips/include/asm/kvm_host.h b/arch/mips/include/asm/kvm_host.h</span>
<span class="p_header">index 5a1a882e0a75..9c67f05a0a1b 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -101,9 +101,9 @@</span> <span class="p_context"></span>
 #define CAUSEF_DC			(_ULCAST_(1) &lt;&lt; 27)
 
 extern atomic_t kvm_mips_instance;
<span class="p_del">-extern pfn_t(*kvm_mips_gfn_to_pfn) (struct kvm *kvm, gfn_t gfn);</span>
<span class="p_del">-extern void (*kvm_mips_release_pfn_clean) (pfn_t pfn);</span>
<span class="p_del">-extern bool(*kvm_mips_is_error_pfn) (pfn_t pfn);</span>
<span class="p_add">+extern kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_add">+extern void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);</span>
<span class="p_add">+extern bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);</span>
 
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
<span class="p_header">diff --git a/arch/mips/kvm/emulate.c b/arch/mips/kvm/emulate.c</span>
<span class="p_header">index d5fa3eaf39a1..476296cf37d3 100644</span>
<span class="p_header">--- a/arch/mips/kvm/emulate.c</span>
<span class="p_header">+++ b/arch/mips/kvm/emulate.c</span>
<span class="p_chunk">@@ -1525,7 +1525,7 @@</span> <span class="p_context"> int kvm_mips_sync_icache(unsigned long va, struct kvm_vcpu *vcpu)</span>
 	struct kvm *kvm = vcpu-&gt;kvm;
 	unsigned long pa;
 	gfn_t gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	gfn = va &gt;&gt; PAGE_SHIFT;
 
<span class="p_header">diff --git a/arch/mips/kvm/tlb.c b/arch/mips/kvm/tlb.c</span>
<span class="p_header">index aed0ac2a4972..570479c03bdc 100644</span>
<span class="p_header">--- a/arch/mips/kvm/tlb.c</span>
<span class="p_header">+++ b/arch/mips/kvm/tlb.c</span>
<span class="p_chunk">@@ -38,13 +38,13 @@</span> <span class="p_context"> atomic_t kvm_mips_instance;</span>
 EXPORT_SYMBOL(kvm_mips_instance);
 
 /* These function pointers are initialized once the KVM module is loaded */
<span class="p_del">-pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t (*kvm_mips_gfn_to_pfn)(struct kvm *kvm, gfn_t gfn);</span>
 EXPORT_SYMBOL(kvm_mips_gfn_to_pfn);
 
<span class="p_del">-void (*kvm_mips_release_pfn_clean)(pfn_t pfn);</span>
<span class="p_add">+void (*kvm_mips_release_pfn_clean)(kvm_pfn_t pfn);</span>
 EXPORT_SYMBOL(kvm_mips_release_pfn_clean);
 
<span class="p_del">-bool (*kvm_mips_is_error_pfn)(pfn_t pfn);</span>
<span class="p_add">+bool (*kvm_mips_is_error_pfn)(kvm_pfn_t pfn);</span>
 EXPORT_SYMBOL(kvm_mips_is_error_pfn);
 
 uint32_t kvm_mips_get_kernel_asid(struct kvm_vcpu *vcpu)
<span class="p_chunk">@@ -144,7 +144,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(kvm_mips_dump_guest_tlbs);</span>
 static int kvm_mips_map_page(struct kvm *kvm, gfn_t gfn)
 {
 	int srcu_idx, err = 0;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	if (kvm-&gt;arch.guest_pmap[gfn] != KVM_INVALID_PAGE)
 		return 0;
<span class="p_chunk">@@ -262,7 +262,7 @@</span> <span class="p_context"> int kvm_mips_handle_kseg0_tlb_fault(unsigned long badvaddr,</span>
 				    struct kvm_vcpu *vcpu)
 {
 	gfn_t gfn;
<span class="p_del">-	pfn_t pfn0, pfn1;</span>
<span class="p_add">+	kvm_pfn_t pfn0, pfn1;</span>
 	unsigned long vaddr = 0;
 	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;
 	int even;
<span class="p_chunk">@@ -313,7 +313,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(kvm_mips_handle_kseg0_tlb_fault);</span>
 int kvm_mips_handle_commpage_tlb_fault(unsigned long badvaddr,
 	struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	pfn_t pfn0, pfn1;</span>
<span class="p_add">+	kvm_pfn_t pfn0, pfn1;</span>
 	unsigned long flags, old_entryhi = 0, vaddr = 0;
 	unsigned long entrylo0 = 0, entrylo1 = 0;
 
<span class="p_chunk">@@ -360,7 +360,7 @@</span> <span class="p_context"> int kvm_mips_handle_mapped_seg_tlb_fault(struct kvm_vcpu *vcpu,</span>
 {
 	unsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;
 	struct kvm *kvm = vcpu-&gt;kvm;
<span class="p_del">-	pfn_t pfn0, pfn1;</span>
<span class="p_add">+	kvm_pfn_t pfn0, pfn1;</span>
 
 	if ((tlb-&gt;tlb_hi &amp; VPN2_MASK) == 0) {
 		pfn0 = 0;
<span class="p_header">diff --git a/arch/powerpc/include/asm/kvm_book3s.h b/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="p_header">index 9fac01cb89c1..8f39796c9da8 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/kvm_book3s.h</span>
<span class="p_chunk">@@ -154,8 +154,8 @@</span> <span class="p_context"> extern void kvmppc_set_bat(struct kvm_vcpu *vcpu, struct kvmppc_bat *bat,</span>
 			   bool upper, u32 val);
 extern void kvmppc_giveup_ext(struct kvm_vcpu *vcpu, ulong msr);
 extern int kvmppc_emulate_paired_single(struct kvm_run *run, struct kvm_vcpu *vcpu);
<span class="p_del">-extern pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="p_del">-			bool *writable);</span>
<span class="p_add">+extern kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa,</span>
<span class="p_add">+			bool writing, bool *writable);</span>
 extern void kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,
 			unsigned long *rmap, long pte_index, int realmode);
 extern void kvmppc_update_rmap_change(unsigned long *rmap, unsigned long psize);
<span class="p_header">diff --git a/arch/powerpc/include/asm/kvm_ppc.h b/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="p_header">index c6ef05bd0765..2241d5357129 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/kvm_ppc.h</span>
<span class="p_chunk">@@ -515,7 +515,7 @@</span> <span class="p_context"> void kvmppc_claim_lpid(long lpid);</span>
 void kvmppc_free_lpid(long lpid);
 void kvmppc_init_lpid(unsigned long nr_lpids);
 
<span class="p_del">-static inline void kvmppc_mmu_flush_icache(pfn_t pfn)</span>
<span class="p_add">+static inline void kvmppc_mmu_flush_icache(kvm_pfn_t pfn)</span>
 {
 	struct page *page;
 	/*
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s.c b/arch/powerpc/kvm/book3s.c</span>
<span class="p_header">index 099c79d8c160..638c6d9be9e0 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s.c</span>
<span class="p_chunk">@@ -366,7 +366,7 @@</span> <span class="p_context"> int kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu)</span>
 }
 EXPORT_SYMBOL_GPL(kvmppc_core_prepare_to_enter);
 
<span class="p_del">-pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
<span class="p_add">+kvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
 			bool *writable)
 {
 	ulong mp_pa = vcpu-&gt;arch.magic_page_pa &amp; KVM_PAM;
<span class="p_chunk">@@ -379,9 +379,9 @@</span> <span class="p_context"> pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,</span>
 	gpa &amp;= ~0xFFFULL;
 	if (unlikely(mp_pa) &amp;&amp; unlikely((gpa &amp; KVM_PAM) == mp_pa)) {
 		ulong shared_page = ((ulong)vcpu-&gt;arch.shared) &amp; PAGE_MASK;
<span class="p_del">-		pfn_t pfn;</span>
<span class="p_add">+		kvm_pfn_t pfn;</span>
 
<span class="p_del">-		pfn = (pfn_t)virt_to_phys((void*)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		pfn = (kvm_pfn_t)virt_to_phys((void*)shared_page) &gt;&gt; PAGE_SHIFT;</span>
 		get_page(pfn_to_page(pfn));
 		if (writable)
 			*writable = true;
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_32_mmu_host.c b/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="p_header">index d5c9bfeb0c9c..55c4d51ea3e2 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_32_mmu_host.c</span>
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> extern char etext[];</span>
 int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,
 			bool iswrite)
 {
<span class="p_del">-	pfn_t hpaddr;</span>
<span class="p_add">+	kvm_pfn_t hpaddr;</span>
 	u64 vpn;
 	u64 vsid;
 	struct kvmppc_sid_map *map;
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_host.c b/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="p_header">index 79ad35abd196..913cd2198fa6 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_host.c</span>
<span class="p_chunk">@@ -83,7 +83,7 @@</span> <span class="p_context"> int kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte,</span>
 			bool iswrite)
 {
 	unsigned long vpn;
<span class="p_del">-	pfn_t hpaddr;</span>
<span class="p_add">+	kvm_pfn_t hpaddr;</span>
 	ulong hash, hpteg;
 	u64 vsid;
 	int ret;
<span class="p_header">diff --git a/arch/powerpc/kvm/e500.h b/arch/powerpc/kvm/e500.h</span>
<span class="p_header">index 72920bed3ac6..94f04fcb373e 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/e500.h</span>
<span class="p_header">+++ b/arch/powerpc/kvm/e500.h</span>
<span class="p_chunk">@@ -41,7 +41,7 @@</span> <span class="p_context"> enum vcpu_ftr {</span>
 #define E500_TLB_MAS2_ATTR	(0x7f)
 
 struct tlbe_ref {
<span class="p_del">-	pfn_t pfn;		/* valid only for TLB0, except briefly */</span>
<span class="p_add">+	kvm_pfn_t pfn;		/* valid only for TLB0, except briefly */</span>
 	unsigned int flags;	/* E500_TLB_* */
 };
 
<span class="p_header">diff --git a/arch/powerpc/kvm/e500_mmu_host.c b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">index 4d33e199edcc..8a5bb6dfcc2d 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_chunk">@@ -163,9 +163,9 @@</span> <span class="p_context"> void kvmppc_map_magic(struct kvm_vcpu *vcpu)</span>
 	struct kvm_book3e_206_tlb_entry magic;
 	ulong shared_page = ((ulong)vcpu-&gt;arch.shared) &amp; PAGE_MASK;
 	unsigned int stid;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
<span class="p_del">-	pfn = (pfn_t)virt_to_phys((void *)shared_page) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	pfn = (kvm_pfn_t)virt_to_phys((void *)shared_page) &gt;&gt; PAGE_SHIFT;</span>
 	get_page(pfn_to_page(pfn));
 
 	preempt_disable();
<span class="p_chunk">@@ -246,7 +246,7 @@</span> <span class="p_context"> static inline int tlbe_is_writable(struct kvm_book3e_206_tlb_entry *tlbe)</span>
 
 static inline void kvmppc_e500_ref_setup(struct tlbe_ref *ref,
 					 struct kvm_book3e_206_tlb_entry *gtlbe,
<span class="p_del">-					 pfn_t pfn, unsigned int wimg)</span>
<span class="p_add">+					 kvm_pfn_t pfn, unsigned int wimg)</span>
 {
 	ref-&gt;pfn = pfn;
 	ref-&gt;flags = E500_TLB_VALID;
<span class="p_chunk">@@ -309,7 +309,7 @@</span> <span class="p_context"> static void kvmppc_e500_setup_stlbe(</span>
 	int tsize, struct tlbe_ref *ref, u64 gvaddr,
 	struct kvm_book3e_206_tlb_entry *stlbe)
 {
<span class="p_del">-	pfn_t pfn = ref-&gt;pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn = ref-&gt;pfn;</span>
 	u32 pr = vcpu-&gt;arch.shared-&gt;msr &amp; MSR_PR;
 
 	BUG_ON(!(ref-&gt;flags &amp; E500_TLB_VALID));
<span class="p_header">diff --git a/arch/powerpc/kvm/trace_pr.h b/arch/powerpc/kvm/trace_pr.h</span>
<span class="p_header">index 810507cb688a..d44f324184fb 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/trace_pr.h</span>
<span class="p_header">+++ b/arch/powerpc/kvm/trace_pr.h</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> TRACE_EVENT(kvm_book3s_reenter,</span>
 #ifdef CONFIG_PPC_BOOK3S_64
 
 TRACE_EVENT(kvm_book3s_64_mmu_map,
<span class="p_del">-	TP_PROTO(int rflags, ulong hpteg, ulong va, pfn_t hpaddr,</span>
<span class="p_add">+	TP_PROTO(int rflags, ulong hpteg, ulong va, kvm_pfn_t hpaddr,</span>
 		 struct kvmppc_pte *orig_pte),
 	TP_ARGS(rflags, hpteg, va, hpaddr, orig_pte),
 
<span class="p_header">diff --git a/arch/x86/kvm/iommu.c b/arch/x86/kvm/iommu.c</span>
<span class="p_header">index 5c520ebf6343..a22a488b4622 100644</span>
<span class="p_header">--- a/arch/x86/kvm/iommu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/iommu.c</span>
<span class="p_chunk">@@ -43,11 +43,11 @@</span> <span class="p_context"> static int kvm_iommu_unmap_memslots(struct kvm *kvm);</span>
 static void kvm_iommu_put_pages(struct kvm *kvm,
 				gfn_t base_gfn, unsigned long npages);
 
<span class="p_del">-static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="p_add">+static kvm_pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
 			   unsigned long npages)
 {
 	gfn_t end_gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	pfn     = gfn_to_pfn_memslot(slot, gfn);
 	end_gfn = gfn + npages;
<span class="p_chunk">@@ -62,7 +62,8 @@</span> <span class="p_context"> static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,</span>
 	return pfn;
 }
 
<span class="p_del">-static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)</span>
<span class="p_add">+static void kvm_unpin_pages(struct kvm *kvm, kvm_pfn_t pfn,</span>
<span class="p_add">+		unsigned long npages)</span>
 {
 	unsigned long i;
 
<span class="p_chunk">@@ -73,7 +74,7 @@</span> <span class="p_context"> static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)</span>
 int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
 	gfn_t gfn, end_gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	int r = 0;
 	struct iommu_domain *domain = kvm-&gt;arch.iommu_domain;
 	int flags;
<span class="p_chunk">@@ -275,7 +276,7 @@</span> <span class="p_context"> static void kvm_iommu_put_pages(struct kvm *kvm,</span>
 {
 	struct iommu_domain *domain;
 	gfn_t end_gfn, gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	u64 phys;
 
 	domain  = kvm-&gt;arch.iommu_domain;
<span class="p_header">diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="p_header">index ff606f507913..6ab963ae0427 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu.c</span>
<span class="p_chunk">@@ -259,7 +259,7 @@</span> <span class="p_context"> static unsigned get_mmio_spte_access(u64 spte)</span>
 }
 
 static bool set_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
<span class="p_del">-			  pfn_t pfn, unsigned access)</span>
<span class="p_add">+			  kvm_pfn_t pfn, unsigned access)</span>
 {
 	if (unlikely(is_noslot_pfn(pfn))) {
 		mark_mmio_spte(vcpu, sptep, gfn, access);
<span class="p_chunk">@@ -325,7 +325,7 @@</span> <span class="p_context"> static int is_last_spte(u64 pte, int level)</span>
 	return 0;
 }
 
<span class="p_del">-static pfn_t spte_to_pfn(u64 pte)</span>
<span class="p_add">+static kvm_pfn_t spte_to_pfn(u64 pte)</span>
 {
 	return (pte &amp; PT64_BASE_ADDR_MASK) &gt;&gt; PAGE_SHIFT;
 }
<span class="p_chunk">@@ -587,7 +587,7 @@</span> <span class="p_context"> static bool mmu_spte_update(u64 *sptep, u64 new_spte)</span>
  */
 static int mmu_spte_clear_track_bits(u64 *sptep)
 {
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	u64 old_spte = *sptep;
 
 	if (!spte_has_volatile_bits(old_spte))
<span class="p_chunk">@@ -1369,7 +1369,7 @@</span> <span class="p_context"> static int kvm_set_pte_rmapp(struct kvm *kvm, unsigned long *rmapp,</span>
 	int need_flush = 0;
 	u64 new_spte;
 	pte_t *ptep = (pte_t *)data;
<span class="p_del">-	pfn_t new_pfn;</span>
<span class="p_add">+	kvm_pfn_t new_pfn;</span>
 
 	WARN_ON(pte_huge(*ptep));
 	new_pfn = pte_pfn(*ptep);
<span class="p_chunk">@@ -2456,7 +2456,7 @@</span> <span class="p_context"> static int mmu_need_write_protect(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
 	return 0;
 }
 
<span class="p_del">-static bool kvm_is_mmio_pfn(pfn_t pfn)</span>
<span class="p_add">+static bool kvm_is_mmio_pfn(kvm_pfn_t pfn)</span>
 {
 	if (pfn_valid(pfn))
 		return !is_zero_pfn(pfn) &amp;&amp; PageReserved(pfn_to_page(pfn));
<span class="p_chunk">@@ -2466,7 +2466,7 @@</span> <span class="p_context"> static bool kvm_is_mmio_pfn(pfn_t pfn)</span>
 
 static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 		    unsigned pte_access, int level,
<span class="p_del">-		    gfn_t gfn, pfn_t pfn, bool speculative,</span>
<span class="p_add">+		    gfn_t gfn, kvm_pfn_t pfn, bool speculative,</span>
 		    bool can_unsync, bool host_writable)
 {
 	u64 spte;
<span class="p_chunk">@@ -2546,7 +2546,7 @@</span> <span class="p_context"> done:</span>
 
 static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 			 unsigned pte_access, int write_fault, int *emulate,
<span class="p_del">-			 int level, gfn_t gfn, pfn_t pfn, bool speculative,</span>
<span class="p_add">+			 int level, gfn_t gfn, kvm_pfn_t pfn, bool speculative,</span>
 			 bool host_writable)
 {
 	int was_rmapped = 0;
<span class="p_chunk">@@ -2606,7 +2606,7 @@</span> <span class="p_context"> static void mmu_set_spte(struct kvm_vcpu *vcpu, u64 *sptep,</span>
 	kvm_release_pfn_clean(pfn);
 }
 
<span class="p_del">-static pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
<span class="p_add">+static kvm_pfn_t pte_prefetch_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn,</span>
 				     bool no_dirty_log)
 {
 	struct kvm_memory_slot *slot;
<span class="p_chunk">@@ -2689,7 +2689,7 @@</span> <span class="p_context"> static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)</span>
 }
 
 static int __direct_map(struct kvm_vcpu *vcpu, gpa_t v, int write,
<span class="p_del">-			int map_writable, int level, gfn_t gfn, pfn_t pfn,</span>
<span class="p_add">+			int map_writable, int level, gfn_t gfn, kvm_pfn_t pfn,</span>
 			bool prefault)
 {
 	struct kvm_shadow_walk_iterator iterator;
<span class="p_chunk">@@ -2739,7 +2739,7 @@</span> <span class="p_context"> static void kvm_send_hwpoison_signal(unsigned long address, struct task_struct *</span>
 	send_sig_info(SIGBUS, &amp;info, tsk);
 }
 
<span class="p_del">-static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, pfn_t pfn)</span>
<span class="p_add">+static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)</span>
 {
 	/*
 	 * Do not cache the mmio info caused by writing the readonly gfn
<span class="p_chunk">@@ -2759,9 +2759,10 @@</span> <span class="p_context"> static int kvm_handle_bad_page(struct kvm_vcpu *vcpu, gfn_t gfn, pfn_t pfn)</span>
 }
 
 static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,
<span class="p_del">-					gfn_t *gfnp, pfn_t *pfnp, int *levelp)</span>
<span class="p_add">+					gfn_t *gfnp, kvm_pfn_t *pfnp,</span>
<span class="p_add">+					int *levelp)</span>
 {
<span class="p_del">-	pfn_t pfn = *pfnp;</span>
<span class="p_add">+	kvm_pfn_t pfn = *pfnp;</span>
 	gfn_t gfn = *gfnp;
 	int level = *levelp;
 
<span class="p_chunk">@@ -2800,7 +2801,7 @@</span> <span class="p_context"> static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,</span>
 }
 
 static bool handle_abnormal_pfn(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn,
<span class="p_del">-				pfn_t pfn, unsigned access, int *ret_val)</span>
<span class="p_add">+				kvm_pfn_t pfn, unsigned access, int *ret_val)</span>
 {
 	bool ret = true;
 
<span class="p_chunk">@@ -2954,7 +2955,7 @@</span> <span class="p_context"> exit:</span>
 }
 
 static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
<span class="p_del">-			 gva_t gva, pfn_t *pfn, bool write, bool *writable);</span>
<span class="p_add">+			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable);</span>
 static void make_mmu_pages_available(struct kvm_vcpu *vcpu);
 
 static int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,
<span class="p_chunk">@@ -2963,7 +2964,7 @@</span> <span class="p_context"> static int nonpaging_map(struct kvm_vcpu *vcpu, gva_t v, u32 error_code,</span>
 	int r;
 	int level;
 	int force_pt_level;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	unsigned long mmu_seq;
 	bool map_writable, write = error_code &amp; PFERR_WRITE_MASK;
 
<span class="p_chunk">@@ -3435,7 +3436,7 @@</span> <span class="p_context"> static bool can_do_async_pf(struct kvm_vcpu *vcpu)</span>
 }
 
 static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
<span class="p_del">-			 gva_t gva, pfn_t *pfn, bool write, bool *writable)</span>
<span class="p_add">+			 gva_t gva, kvm_pfn_t *pfn, bool write, bool *writable)</span>
 {
 	struct kvm_memory_slot *slot;
 	bool async;
<span class="p_chunk">@@ -3473,7 +3474,7 @@</span> <span class="p_context"> check_hugepage_cache_consistency(struct kvm_vcpu *vcpu, gfn_t gfn, int level)</span>
 static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 			  bool prefault)
 {
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	int r;
 	int level;
 	int force_pt_level;
<span class="p_chunk">@@ -4627,7 +4628,7 @@</span> <span class="p_context"> static bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,</span>
 	u64 *sptep;
 	struct rmap_iterator iter;
 	int need_tlb_flush = 0;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	struct kvm_mmu_page *sp;
 
 restart:
<span class="p_header">diff --git a/arch/x86/kvm/mmu_audit.c b/arch/x86/kvm/mmu_audit.c</span>
<span class="p_header">index 03d518e499a6..37a4d14115c0 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu_audit.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu_audit.c</span>
<span class="p_chunk">@@ -97,7 +97,7 @@</span> <span class="p_context"> static void audit_mappings(struct kvm_vcpu *vcpu, u64 *sptep, int level)</span>
 {
 	struct kvm_mmu_page *sp;
 	gfn_t gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	hpa_t hpa;
 
 	sp = page_header(__pa(sptep));
<span class="p_header">diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h</span>
<span class="p_header">index 736e6ab8784d..9dd02cb74724 100644</span>
<span class="p_header">--- a/arch/x86/kvm/paging_tmpl.h</span>
<span class="p_header">+++ b/arch/x86/kvm/paging_tmpl.h</span>
<span class="p_chunk">@@ -456,7 +456,7 @@</span> <span class="p_context"> FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,</span>
 {
 	unsigned pte_access;
 	gfn_t gfn;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	if (FNAME(prefetch_invalid_gpte)(vcpu, sp, spte, gpte))
 		return false;
<span class="p_chunk">@@ -551,7 +551,7 @@</span> <span class="p_context"> static void FNAME(pte_prefetch)(struct kvm_vcpu *vcpu, struct guest_walker *gw,</span>
 static int FNAME(fetch)(struct kvm_vcpu *vcpu, gva_t addr,
 			 struct guest_walker *gw,
 			 int write_fault, int hlevel,
<span class="p_del">-			 pfn_t pfn, bool map_writable, bool prefault)</span>
<span class="p_add">+			 kvm_pfn_t pfn, bool map_writable, bool prefault)</span>
 {
 	struct kvm_mmu_page *sp = NULL;
 	struct kvm_shadow_walk_iterator it;
<span class="p_chunk">@@ -696,7 +696,7 @@</span> <span class="p_context"> static int FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code,</span>
 	int user_fault = error_code &amp; PFERR_USER_MASK;
 	struct guest_walker walker;
 	int r;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 	int level = PT_PAGE_TABLE_LEVEL;
 	int force_pt_level;
 	unsigned long mmu_seq;
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 06ef4908ba61..d401ed6874bd 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -4046,7 +4046,7 @@</span> <span class="p_context"> out:</span>
 static int init_rmode_identity_map(struct kvm *kvm)
 {
 	int i, idx, r = 0;
<span class="p_del">-	pfn_t identity_map_pfn;</span>
<span class="p_add">+	kvm_pfn_t identity_map_pfn;</span>
 	u32 tmp;
 
 	if (!enable_ept)
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 92511d4b7236..8fc5ca584edf 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -4935,7 +4935,7 @@</span> <span class="p_context"> static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t cr2,</span>
 				  int emulation_type)
 {
 	gpa_t gpa = cr2;
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	if (emulation_type &amp; EMULTYPE_NO_REEXECUTE)
 		return false;
<span class="p_header">diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="p_header">index 1bef9e21e725..2420b43f3acc 100644</span>
<span class="p_header">--- a/include/linux/kvm_host.h</span>
<span class="p_header">+++ b/include/linux/kvm_host.h</span>
<span class="p_chunk">@@ -65,7 +65,7 @@</span> <span class="p_context"></span>
  * error pfns indicate that the gfn is in slot but faild to
  * translate it to pfn on host.
  */
<span class="p_del">-static inline bool is_error_pfn(pfn_t pfn)</span>
<span class="p_add">+static inline bool is_error_pfn(kvm_pfn_t pfn)</span>
 {
 	return !!(pfn &amp; KVM_PFN_ERR_MASK);
 }
<span class="p_chunk">@@ -75,13 +75,13 @@</span> <span class="p_context"> static inline bool is_error_pfn(pfn_t pfn)</span>
  * translated to pfn - it is not in slot or failed to
  * translate it to pfn.
  */
<span class="p_del">-static inline bool is_error_noslot_pfn(pfn_t pfn)</span>
<span class="p_add">+static inline bool is_error_noslot_pfn(kvm_pfn_t pfn)</span>
 {
 	return !!(pfn &amp; KVM_PFN_ERR_NOSLOT_MASK);
 }
 
 /* noslot pfn indicates that the gfn is not in slot. */
<span class="p_del">-static inline bool is_noslot_pfn(pfn_t pfn)</span>
<span class="p_add">+static inline bool is_noslot_pfn(kvm_pfn_t pfn)</span>
 {
 	return pfn == KVM_PFN_NOSLOT;
 }
<span class="p_chunk">@@ -569,19 +569,20 @@</span> <span class="p_context"> void kvm_release_page_clean(struct page *page);</span>
 void kvm_release_page_dirty(struct page *page);
 void kvm_set_page_accessed(struct page *page);
 
<span class="p_del">-pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_del">-pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_del">-pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
 		      bool *writable);
<span class="p_del">-pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="p_del">-pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="p_del">-pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
<span class="p_del">-			   bool *async, bool write_fault, bool *writable);</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="p_add">+			       bool atomic, bool *async, bool write_fault,</span>
<span class="p_add">+			       bool *writable);</span>
 
<span class="p_del">-void kvm_release_pfn_clean(pfn_t pfn);</span>
<span class="p_del">-void kvm_set_pfn_dirty(pfn_t pfn);</span>
<span class="p_del">-void kvm_set_pfn_accessed(pfn_t pfn);</span>
<span class="p_del">-void kvm_get_pfn(pfn_t pfn);</span>
<span class="p_add">+void kvm_release_pfn_clean(kvm_pfn_t pfn);</span>
<span class="p_add">+void kvm_set_pfn_dirty(kvm_pfn_t pfn);</span>
<span class="p_add">+void kvm_set_pfn_accessed(kvm_pfn_t pfn);</span>
<span class="p_add">+void kvm_get_pfn(kvm_pfn_t pfn);</span>
 
 int kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,
 			int len);
<span class="p_chunk">@@ -607,8 +608,8 @@</span> <span class="p_context"> void mark_page_dirty(struct kvm *kvm, gfn_t gfn);</span>
 
 struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
<span class="p_del">-pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="p_del">-pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
<span class="p_add">+kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);</span>
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);
 unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);
<span class="p_chunk">@@ -789,7 +790,7 @@</span> <span class="p_context"> void kvm_arch_sync_events(struct kvm *kvm);</span>
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
 
<span class="p_del">-bool kvm_is_reserved_pfn(pfn_t pfn);</span>
<span class="p_add">+bool kvm_is_reserved_pfn(kvm_pfn_t pfn);</span>
 
 struct kvm_irq_ack_notifier {
 	struct hlist_node link;
<span class="p_chunk">@@ -940,7 +941,7 @@</span> <span class="p_context"> static inline gfn_t gpa_to_gfn(gpa_t gpa)</span>
 	return (gfn_t)(gpa &gt;&gt; PAGE_SHIFT);
 }
 
<span class="p_del">-static inline hpa_t pfn_to_hpa(pfn_t pfn)</span>
<span class="p_add">+static inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)</span>
 {
 	return (hpa_t)pfn &lt;&lt; PAGE_SHIFT;
 }
<span class="p_header">diff --git a/include/linux/kvm_types.h b/include/linux/kvm_types.h</span>
<span class="p_header">index 1b47a185c2f0..8bf259dae9f6 100644</span>
<span class="p_header">--- a/include/linux/kvm_types.h</span>
<span class="p_header">+++ b/include/linux/kvm_types.h</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> typedef unsigned long  hva_t;</span>
 typedef u64            hpa_t;
 typedef u64            hfn_t;
 
<span class="p_del">-typedef hfn_t pfn_t;</span>
<span class="p_add">+typedef hfn_t kvm_pfn_t;</span>
 
 struct gfn_to_hva_cache {
 	u64 generation;
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 8db1d9361993..02cd2eddd3ff 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -111,7 +111,7 @@</span> <span class="p_context"> static void hardware_disable_all(void);</span>
 
 static void kvm_io_bus_destroy(struct kvm_io_bus *bus);
 
<span class="p_del">-static void kvm_release_pfn_dirty(pfn_t pfn);</span>
<span class="p_add">+static void kvm_release_pfn_dirty(kvm_pfn_t pfn);</span>
 static void mark_page_dirty_in_slot(struct kvm_memory_slot *memslot, gfn_t gfn);
 
 __visible bool kvm_rebooting;
<span class="p_chunk">@@ -119,7 +119,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(kvm_rebooting);</span>
 
 static bool largepages_enabled = true;
 
<span class="p_del">-bool kvm_is_reserved_pfn(pfn_t pfn)</span>
<span class="p_add">+bool kvm_is_reserved_pfn(kvm_pfn_t pfn)</span>
 {
 	if (pfn_valid(pfn))
 		return PageReserved(pfn_to_page(pfn));
<span class="p_chunk">@@ -1296,7 +1296,7 @@</span> <span class="p_context"> static inline int check_user_page_hwpoison(unsigned long addr)</span>
  * true indicates success, otherwise false is returned.
  */
 static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,
<span class="p_del">-			    bool write_fault, bool *writable, pfn_t *pfn)</span>
<span class="p_add">+			    bool write_fault, bool *writable, kvm_pfn_t *pfn)</span>
 {
 	struct page *page[1];
 	int npages;
<span class="p_chunk">@@ -1329,7 +1329,7 @@</span> <span class="p_context"> static bool hva_to_pfn_fast(unsigned long addr, bool atomic, bool *async,</span>
  * 1 indicates success, -errno is returned if error is detected.
  */
 static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
<span class="p_del">-			   bool *writable, pfn_t *pfn)</span>
<span class="p_add">+			   bool *writable, kvm_pfn_t *pfn)</span>
 {
 	struct page *page[1];
 	int npages = 0;
<span class="p_chunk">@@ -1393,11 +1393,11 @@</span> <span class="p_context"> static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
  * 2): @write_fault = false &amp;&amp; @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
<span class="p_del">-static pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="p_add">+static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 			bool write_fault, bool *writable)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	pfn_t pfn = 0;</span>
<span class="p_add">+	kvm_pfn_t pfn = 0;</span>
 	int npages;
 
 	/* we can do it either atomically or asynchronously, not both */
<span class="p_chunk">@@ -1438,8 +1438,9 @@</span> <span class="p_context"> exit:</span>
 	return pfn;
 }
 
<span class="p_del">-pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
<span class="p_del">-			   bool *async, bool write_fault, bool *writable)</span>
<span class="p_add">+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,</span>
<span class="p_add">+			       bool atomic, bool *async, bool write_fault,</span>
<span class="p_add">+			       bool *writable)</span>
 {
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
<span class="p_chunk">@@ -1460,7 +1461,7 @@</span> <span class="p_context"> pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn, bool atomic,</span>
 }
 EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);
 
<span class="p_del">-pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
 		      bool *writable)
 {
 	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,
<span class="p_chunk">@@ -1468,37 +1469,37 @@</span> <span class="p_context"> pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,</span>
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
<span class="p_del">-pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)</span>
 {
 	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
<span class="p_del">-pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)</span>
 {
 	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);
 
<span class="p_del">-pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn_atomic(struct kvm *kvm, gfn_t gfn)</span>
 {
 	return gfn_to_pfn_memslot_atomic(gfn_to_memslot(kvm, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_atomic);
 
<span class="p_del">-pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
 {
 	return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_atomic);
 
<span class="p_del">-pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)</span>
 {
 	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn);
 
<span class="p_del">-pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
<span class="p_add">+kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)</span>
 {
 	return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
 }
<span class="p_chunk">@@ -1521,7 +1522,7 @@</span> <span class="p_context"> int gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,</span>
 }
 EXPORT_SYMBOL_GPL(gfn_to_page_many_atomic);
 
<span class="p_del">-static struct page *kvm_pfn_to_page(pfn_t pfn)</span>
<span class="p_add">+static struct page *kvm_pfn_to_page(kvm_pfn_t pfn)</span>
 {
 	if (is_error_noslot_pfn(pfn))
 		return KVM_ERR_PTR_BAD_PAGE;
<span class="p_chunk">@@ -1536,7 +1537,7 @@</span> <span class="p_context"> static struct page *kvm_pfn_to_page(pfn_t pfn)</span>
 
 struct page *gfn_to_page(struct kvm *kvm, gfn_t gfn)
 {
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	pfn = gfn_to_pfn(kvm, gfn);
 
<span class="p_chunk">@@ -1546,7 +1547,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gfn_to_page);</span>
 
 struct page *kvm_vcpu_gfn_to_page(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
<span class="p_del">-	pfn_t pfn;</span>
<span class="p_add">+	kvm_pfn_t pfn;</span>
 
 	pfn = kvm_vcpu_gfn_to_pfn(vcpu, gfn);
 
<span class="p_chunk">@@ -1562,7 +1563,7 @@</span> <span class="p_context"> void kvm_release_page_clean(struct page *page)</span>
 }
 EXPORT_SYMBOL_GPL(kvm_release_page_clean);
 
<span class="p_del">-void kvm_release_pfn_clean(pfn_t pfn)</span>
<span class="p_add">+void kvm_release_pfn_clean(kvm_pfn_t pfn)</span>
 {
 	if (!is_error_noslot_pfn(pfn) &amp;&amp; !kvm_is_reserved_pfn(pfn))
 		put_page(pfn_to_page(pfn));
<span class="p_chunk">@@ -1577,13 +1578,13 @@</span> <span class="p_context"> void kvm_release_page_dirty(struct page *page)</span>
 }
 EXPORT_SYMBOL_GPL(kvm_release_page_dirty);
 
<span class="p_del">-static void kvm_release_pfn_dirty(pfn_t pfn)</span>
<span class="p_add">+static void kvm_release_pfn_dirty(kvm_pfn_t pfn)</span>
 {
 	kvm_set_pfn_dirty(pfn);
 	kvm_release_pfn_clean(pfn);
 }
 
<span class="p_del">-void kvm_set_pfn_dirty(pfn_t pfn)</span>
<span class="p_add">+void kvm_set_pfn_dirty(kvm_pfn_t pfn)</span>
 {
 	if (!kvm_is_reserved_pfn(pfn)) {
 		struct page *page = pfn_to_page(pfn);
<span class="p_chunk">@@ -1594,14 +1595,14 @@</span> <span class="p_context"> void kvm_set_pfn_dirty(pfn_t pfn)</span>
 }
 EXPORT_SYMBOL_GPL(kvm_set_pfn_dirty);
 
<span class="p_del">-void kvm_set_pfn_accessed(pfn_t pfn)</span>
<span class="p_add">+void kvm_set_pfn_accessed(kvm_pfn_t pfn)</span>
 {
 	if (!kvm_is_reserved_pfn(pfn))
 		mark_page_accessed(pfn_to_page(pfn));
 }
 EXPORT_SYMBOL_GPL(kvm_set_pfn_accessed);
 
<span class="p_del">-void kvm_get_pfn(pfn_t pfn)</span>
<span class="p_add">+void kvm_get_pfn(kvm_pfn_t pfn)</span>
 {
 	if (!kvm_is_reserved_pfn(pfn))
 		get_page(pfn_to_page(pfn));

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



