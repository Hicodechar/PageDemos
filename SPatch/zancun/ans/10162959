
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] x86: Avoid CR3 load on compatibility mode with PTI - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] x86: Avoid CR3 load on compatibility mode with PTI</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 14, 2018, 8:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180114201306.3554-1-namit@vmware.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10162959/mbox/"
   >mbox</a>
|
   <a href="/patch/10162959/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10162959/">/patch/10162959/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D23C8601C0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 03:40:54 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BC57A285FB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 03:40:54 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AED9728671; Mon, 15 Jan 2018 03:40:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-5.4 required=2.0 tests=BAYES_00, DATE_IN_PAST_06_12,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E9A90285FB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 Jan 2018 03:40:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753612AbeAODks (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 14 Jan 2018 22:40:48 -0500
Received: from ex13-edg-ou-001.vmware.com ([208.91.0.189]:43408 &quot;EHLO
	EX13-EDG-OU-001.vmware.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752490AbeAODkr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 14 Jan 2018 22:40:47 -0500
X-Greylist: delayed 904 seconds by postgrey-1.27 at vger.kernel.org;
	Sun, 14 Jan 2018 22:40:46 EST
Received: from sc9-mailhost1.vmware.com (10.113.161.71) by
	EX13-EDG-OU-001.vmware.com (10.113.208.155) with Microsoft SMTP
	Server id 15.0.1156.6; Sun, 14 Jan 2018 19:25:13 -0800
Received: from sc2-haas01-esx0118.eng.vmware.com
	(sc2-haas01-esx0118.eng.vmware.com [10.172.44.118])
	by sc9-mailhost1.vmware.com (Postfix) with ESMTP id 575E31970B;
	Sun, 14 Jan 2018 19:25:42 -0800 (PST)
From: Nadav Amit &lt;namit@vmware.com&gt;
To: &lt;linux-kernel@vger.kernel.org&gt;
CC: &lt;dave.hansen@linux.intel.com&gt;, &lt;luto@kernel.org&gt;,
	&lt;tglx@linutronix.de&gt;, &lt;mingo@redhat.com&gt;, &lt;hpa@zytor.com&gt;,
	&lt;x86@kernel.org&gt;, &lt;nadav.amit@gmail.com&gt;, &lt;w@1wt.eu&gt;,
	Nadav Amit &lt;namit@vmware.com&gt;
Subject: [RFC] x86: Avoid CR3 load on compatibility mode with PTI
Date: Sun, 14 Jan 2018 12:13:06 -0800
Message-ID: &lt;20180114201306.3554-1-namit@vmware.com&gt;
X-Mailer: git-send-email 2.14.1
MIME-Version: 1.0
Content-Type: text/plain
Received-SPF: None (EX13-EDG-OU-001.vmware.com: namit@vmware.com does not
	designate permitted sender hosts)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Jan. 14, 2018, 8:13 p.m.</div>
<pre class="content">
Currently, when page-table isolation is on to prevent the Meltdown bug
(CVE-2017-5754), CR3 is always loaded on system-call and interrupt.

However, it appears that this is an unnecessary measure when programs
run in compatibility mode. In this mode only 32-bit registers are
available, which means that there *should* be no way for the CPU to
access, even speculatively, memory that belongs to the kernel, which
sits in high addresses.

While this may seem as an &quot;uninteresting&quot; case, it opens the possibility
to run I/O intensive applications in compatibility mode with improved
performance relatively to their 64-bit counterpart. These applications
are arguably also the ones that are least affected by the less efficient
32-code.

For example, on Dell R630 (Intel E5-2670 v3)

Redis (redis-benchmark)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Jan. 15, 2018, 5:20 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt; sits in high addresses.</span>

You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.

I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.

Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Jan. 15, 2018, 5:42 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt;&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt;&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt;&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt;&gt; sits in high addresses.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.</span>

I noticed it doesn’t. I thought the removing/restoring the __USER_CS
descriptor on context switch, based on TIF_IA32, would be enough.
modify_ldt() always keeps the descriptor l-bit clear. I will review the
other GDT descriptors, and if needed, create two GDTs. Let me know if I
missed anything else.
<span class="quote">
&gt; Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.</span>

If SMEP is not supported, compatibility mode would still require page-table
isolation.

Thanks for the feedback. I still look for an ack for the basic idea of
disabling page-table isolation on compatibility mode.

Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Jan. 15, 2018, 5:45 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Jan 15, 2018, at 9:42 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt;&gt;&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt;&gt;&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt;&gt;&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt;&gt;&gt; sits in high addresses.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I noticed it doesn’t. I thought the removing/restoring the __USER_CS</span>
<span class="quote">&gt; descriptor on context switch, based on TIF_IA32, would be enough.</span>
<span class="quote">&gt; modify_ldt() always keeps the descriptor l-bit clear. I will review the</span>
<span class="quote">&gt; other GDT descriptors, and if needed, create two GDTs. Let me know if I</span>
<span class="quote">&gt; missed anything else.</span>

There world need to be some opt-in control, I think, for CRIU if nothing else.

Also, on Xen PV, it&#39;s a complete nonstarter.  We don&#39;t have enough control over the GDT unless someone knows otherwise.  But there&#39;s no PTI on Xen PV either.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If SMEP is not supported, compatibility mode would still require page-table</span>
<span class="quote">&gt; isolation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for the feedback. I still look for an ack for the basic idea of</span>
<span class="quote">&gt; disabling page-table isolation on compatibility mode.</span>
<span class="quote">&gt; </span>

I&#39;m still not really convinced this is worth it.  It will send a bad message and get people to run critical stuff compiled for 32-bit, which has its own downsides.
<span class="quote">
&gt; Regards,</span>
<span class="quote">&gt; Nadav</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Jan. 15, 2018, 5:50 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Jan 15, 2018, at 9:42 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt;&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt;&gt;&gt;&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt;&gt;&gt;&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt;&gt;&gt;&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt;&gt;&gt;&gt; sits in high addresses.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I noticed it doesn’t. I thought the removing/restoring the __USER_CS</span>
<span class="quote">&gt;&gt; descriptor on context switch, based on TIF_IA32, would be enough.</span>
<span class="quote">&gt;&gt; modify_ldt() always keeps the descriptor l-bit clear. I will review the</span>
<span class="quote">&gt;&gt; other GDT descriptors, and if needed, create two GDTs. Let me know if I</span>
<span class="quote">&gt;&gt; missed anything else.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There world need to be some opt-in control, I think, for CRIU if nothing else.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, on Xen PV, it&#39;s a complete nonstarter.  We don&#39;t have enough control over the GDT unless someone knows otherwise.  But there&#39;s no PTI on Xen PV either.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; If SMEP is not supported, compatibility mode would still require page-table</span>
<span class="quote">&gt;&gt; isolation.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Thanks for the feedback. I still look for an ack for the basic idea of</span>
<span class="quote">&gt;&gt; disabling page-table isolation on compatibility mode.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m still not really convinced this is worth it.  It will send a bad message and get people to run critical stuff compiled for 32-bit, which has its own downsides.</span>

I can handle #GP gracefully if __USER_CS is loaded so PTI would be required
again. Doing so would eliminate the need for an opt-in, and preserve the
current semantics.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Jan. 15, 2018, 6:04 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Jan 15, 2018, at 9:50 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; On Jan 15, 2018, at 9:42 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; sits in high addresses.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; I noticed it doesn’t. I thought the removing/restoring the __USER_CS</span>
<span class="quote">&gt;&gt;&gt; descriptor on context switch, based on TIF_IA32, would be enough.</span>
<span class="quote">&gt;&gt;&gt; modify_ldt() always keeps the descriptor l-bit clear. I will review the</span>
<span class="quote">&gt;&gt;&gt; other GDT descriptors, and if needed, create two GDTs. Let me know if I</span>
<span class="quote">&gt;&gt;&gt; missed anything else.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; There world need to be some opt-in control, I think, for CRIU if nothing else.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Also, on Xen PV, it&#39;s a complete nonstarter.  We don&#39;t have enough control over the GDT unless someone knows otherwise.  But there&#39;s no PTI on Xen PV either.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; If SMEP is not supported, compatibility mode would still require page-table</span>
<span class="quote">&gt;&gt;&gt; isolation.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Thanks for the feedback. I still look for an ack for the basic idea of</span>
<span class="quote">&gt;&gt;&gt; disabling page-table isolation on compatibility mode.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I&#39;m still not really convinced this is worth it.  It will send a bad message and get people to run critical stuff compiled for 32-bit, which has its own downsides.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can handle #GP gracefully if __USER_CS is loaded so PTI would be required</span>
<span class="quote">&gt; again. Doing so would eliminate the need for an opt-in, and preserve the</span>
<span class="quote">&gt; current semantics.</span>
<span class="quote">&gt; </span>

Not if someone used LAR, a la the sigreturn_32 test.  Not necessarily a showstopper, though.

You&#39;d also have to figure out how to do PTI per-thread, which Linus doesn&#39;t like.  See Willy&#39;s PTI opt-out thread.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Jan. 15, 2018, 6:50 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Jan 15, 2018, at 9:50 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; On Jan 15, 2018, at 9:42 AM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On Jan 14, 2018, at 12:13 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; However, it appears that this is an unnecessary measure when programs</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; run in compatibility mode. In this mode only 32-bit registers are</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; available, which means that there *should* be no way for the CPU to</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; access, even speculatively, memory that belongs to the kernel, which</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; sits in high addresses.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; You&#39;re assuming that TIF_IA32 prevents the execution of 64-bit code.  It doesn&#39;t.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I&#39;ve occasionally considered adding an opt-in hardening mechanism to enforce 32-bit or 64-bit execution, but we don&#39;t have this now.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; I noticed it doesn’t. I thought the removing/restoring the __USER_CS</span>
<span class="quote">&gt;&gt;&gt;&gt; descriptor on context switch, based on TIF_IA32, would be enough.</span>
<span class="quote">&gt;&gt;&gt;&gt; modify_ldt() always keeps the descriptor l-bit clear. I will review the</span>
<span class="quote">&gt;&gt;&gt;&gt; other GDT descriptors, and if needed, create two GDTs. Let me know if I</span>
<span class="quote">&gt;&gt;&gt;&gt; missed anything else.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; There world need to be some opt-in control, I think, for CRIU if nothing else.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Also, on Xen PV, it&#39;s a complete nonstarter.  We don&#39;t have enough control over the GDT unless someone knows otherwise.  But there&#39;s no PTI on Xen PV either.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Anything like this would also need to spend on SMEP, I think -- the pseudo-SMEP granted by PTI is too valuable to give up on old boxes, I think.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; If SMEP is not supported, compatibility mode would still require page-table</span>
<span class="quote">&gt;&gt;&gt;&gt; isolation.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Thanks for the feedback. I still look for an ack for the basic idea of</span>
<span class="quote">&gt;&gt;&gt;&gt; disabling page-table isolation on compatibility mode.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; I&#39;m still not really convinced this is worth it.  It will send a bad message and get people to run critical stuff compiled for 32-bit, which has its own downsides.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I can handle #GP gracefully if __USER_CS is loaded so PTI would be required</span>
<span class="quote">&gt;&gt; again. Doing so would eliminate the need for an opt-in, and preserve the</span>
<span class="quote">&gt;&gt; current semantics.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not if someone used LAR, a la the sigreturn_32 test.  Not necessarily a showstopper, though.</span>

Thanks for pointing it out. Actually, I think that since
GDT_ENTRY_DEFAULT_USER_DS and GDT_ENTRY_DEFAULT_USER_CS are the last set
entries in the GDT, I can just play with the GDT limit (lower it on IA32),
and get LAR working as well.
<span class="quote">
&gt; You&#39;d also have to figure out how to do PTI per-thread, which Linus doesn&#39;t like.  See Willy&#39;s PTI opt-out thread.</span>

Maybe I read it wrong, but I think Linus&#39;s main objections are for
dynamically enabling/disabling PTI and for not having clear protection
guarantees. I don’t think that disabling PTI on compatibility mode suffers
from these limitations. (But then again…)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Jan. 15, 2018, 7:49 p.m.</div>
<pre class="content">
On 01/14/2018 12:13 PM, Nadav Amit wrote:
<span class="quote">&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>

I think of PTI as being a defense against bad stuff that happens from
the kernel being mapped into the user address space, with Meltdown being
the most obvious &quot;bad&quot; thing.

What you&#39;re saying here is that since a 32-bit program can&#39;t address the
kernel sitting at a &gt;32-bit address, it does not need to unmap the
kernel.  As Andy pointed out, there are a few holes with that assumption.

IMNHO, any PTI-disabling mechanisms better be rock-solid, and easy to
convince ourselves that they do the right thing.  For instance, the
per-process PTI stuff is going to make the decision quite close to a
capability check, which makes it fairly easy to get right.

If we start disabling PTI willy nilly at points _away_ from the
capability checks (like for 32-bit binaries, say), then it gets really
hard to decide if we are doing the right things.

Also, what&#39;s the end goal here?  Run old 32-bit binaries better?  You
want to weaken the security of the whole implementation to do that?
Sounds like a bad tradeoff to me.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - Jan. 15, 2018, 7:52 p.m.</div>
<pre class="content">
On Mon, Jan 15, 2018 at 11:49:19AM -0800, Dave Hansen wrote:
<span class="quote">&gt; If we start disabling PTI willy nilly at points _away_ from the</span>
<span class="quote">&gt; capability checks (like for 32-bit binaries, say), then it gets really</span>
<span class="quote">&gt; hard to decide if we are doing the right things.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, what&#39;s the end goal here?  Run old 32-bit binaries better?  You</span>
<span class="quote">&gt; want to weaken the security of the whole implementation to do that?</span>
<span class="quote">&gt; Sounds like a bad tradeoff to me.</span>

In fact I understand it differently, which is that by running 32-bit,
he can recover the original performance without sacrifying security.
It&#39;s not that bad actually when you think about it since the vast
majority of performance-sensitive software doesn&#39;t need to access
even one GB of data.

Willy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 15, 2018, 8:09 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 01/14/2018 12:13 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Currently, when page-table isolation is on to prevent the Meltdown bug</span>
<span class="quote">&gt;&gt; (CVE-2017-5754), CR3 is always loaded on system-call and interrupt.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think of PTI as being a defense against bad stuff that happens from</span>
<span class="quote">&gt; the kernel being mapped into the user address space, with Meltdown being</span>
<span class="quote">&gt; the most obvious &quot;bad&quot; thing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What you&#39;re saying here is that since a 32-bit program can&#39;t address the</span>
<span class="quote">&gt; kernel sitting at a &gt;32-bit address, it does not need to unmap the</span>
<span class="quote">&gt; kernel.  As Andy pointed out, there are a few holes with that assumption.</span>

I think that Andy pointed out that my RFC may break existing programs, but
it should be relatively easy to fix. I don’t think there is any problem with
the assumption.
<span class="quote">
&gt; IMNHO, any PTI-disabling mechanisms better be rock-solid, and easy to</span>
<span class="quote">&gt; convince ourselves that they do the right thing.  For instance, the</span>
<span class="quote">&gt; per-process PTI stuff is going to make the decision quite close to a</span>
<span class="quote">&gt; capability check, which makes it fairly easy to get right.</span>

Per-process PTI stuff is an easy solution, but it just pushes the decision
to the user, who is likely to disable PTI without thinking twice, especially
since he got no other option.
<span class="quote">
&gt; If we start disabling PTI willy nilly at points _away_ from the</span>
<span class="quote">&gt; capability checks (like for 32-bit binaries, say), then it gets really</span>
<span class="quote">&gt; hard to decide if we are doing the right things.</span>

Eventually it comes down to the question: what does the CPU do? I was
assuming that Intel can figure it out. If it is just about being “paranoid”,
I presume some paranoid knob should control this behavior.
<span class="quote">
&gt; Also, what&#39;s the end goal here?  Run old 32-bit binaries better?  You</span>
<span class="quote">&gt; want to weaken the security of the whole implementation to do that?</span>
<span class="quote">&gt; Sounds like a bad tradeoff to me.</span>

As Willy noted in this thread, I think that some users may be interested in
running 32-bit Apache/Nginx/Redis to get the performance back without
sacrificing security.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 16, 2018, 12:41 a.m.</div>
<pre class="content">
* Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">
&gt; &gt; Also, what&#39;s the end goal here?  Run old 32-bit binaries better?  You</span>
<span class="quote">&gt; &gt; want to weaken the security of the whole implementation to do that?</span>
<span class="quote">&gt; &gt; Sounds like a bad tradeoff to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As Willy noted in this thread, I think that some users may be interested in </span>
<span class="quote">&gt; running 32-bit Apache/Nginx/Redis to get the performance back without </span>
<span class="quote">&gt; sacrificing security.</span>

Note that it is a flawed assumption to think that this is possible, as they might 
in many cases not be getting their performance back: 32-bit binaries for the same 
general CPU bound computation can easily be 5% slower than 64-bit binaries (as 
long as the larger cache footprint of 64-bit data doesn&#39;t fall out of key caches), 
but can be up to 30% slower for certain computations.

In fact, depending on how kernel heavy the web workload is (for example how much 
CGI processing versus IO it does, etc.), a 32-bit binary could be distinctly 
_slower_ than even a PTI-enabled 64-bit binary.

So we are trading a 5-15% slowdown (PTI) for another 5-15% slowdown, plus we are
losing the soft-SMEP feature on older CPUs that PTI enables, which is a pretty 
powerful mitigation technique.

Yes, I suspect in some (maybe many) cases it would be a speedup, but I really 
don&#39;t like the underlying assumptions and tradeoffs here. (Not that I like any of 
this whole Meltdown debacle TBH.)

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Jan. 16, 2018, 3:49 a.m.</div>
<pre class="content">
Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; * Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; Also, what&#39;s the end goal here?  Run old 32-bit binaries better?  You</span>
<span class="quote">&gt;&gt;&gt; want to weaken the security of the whole implementation to do that?</span>
<span class="quote">&gt;&gt;&gt; Sounds like a bad tradeoff to me.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; As Willy noted in this thread, I think that some users may be interested in </span>
<span class="quote">&gt;&gt; running 32-bit Apache/Nginx/Redis to get the performance back without </span>
<span class="quote">&gt;&gt; sacrificing security.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note that it is a flawed assumption to think that this is possible, as they might </span>
<span class="quote">&gt; in many cases not be getting their performance back: 32-bit binaries for the same </span>
<span class="quote">&gt; general CPU bound computation can easily be 5% slower than 64-bit binaries (as </span>
<span class="quote">&gt; long as the larger cache footprint of 64-bit data doesn&#39;t fall out of key caches), </span>
<span class="quote">&gt; but can be up to 30% slower for certain computations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In fact, depending on how kernel heavy the web workload is (for example how much </span>
<span class="quote">&gt; CGI processing versus IO it does, etc.), a 32-bit binary could be distinctly </span>
<span class="quote">&gt; _slower_ than even a PTI-enabled 64-bit binary.</span>

Obviously you are right - I didn’t argue otherwise - and I think it is also
reflected in the results (Redis LRANGE results). Yet, arguably the workloads
that are affected the most by PTI are those with a high number of syscalls
and interrupts, in which user computation time is relatively small.
<span class="quote">
&gt; So we are trading a 5-15% slowdown (PTI) for another 5-15% slowdown, plus we are</span>
<span class="quote">&gt; losing the soft-SMEP feature on older CPUs that PTI enables, which is a pretty </span>
<span class="quote">&gt; powerful mitigation technique.</span>

This soft-SMEP can be kept by keeping PTI if SMEP is unsupported. Although
we trade slowdowns, they are different ones, which allows the user to make
his best decision.
<span class="quote">
&gt; Yes, I suspect in some (maybe many) cases it would be a speedup, but I really </span>
<span class="quote">&gt; don&#39;t like the underlying assumptions and tradeoffs here. (Not that I like any of </span>
<span class="quote">&gt; this whole Meltdown debacle TBH.)</span>

To make sure that I understand correctly - the assumptions are that
disabling PTI on compatibility mode would: (1) Benefit some workloads; (2)
Be useful, even if we only consider CPUs with SMEP; and (3) Secure.

Under these assumptions, the tradeoff is slightly greater code complexity
for considerably better performance of 32-bit code; in some common cases
this makes 32-bit code to perform significantly better than 64-bit code.

Am I missing something? My main concern was initially security, but so far
from your aggregated feedback I did not see something concrete which cannot
relatively easily be addressed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 20, 2018, 2:26 p.m.</div>
<pre class="content">
* Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">
&gt; &gt; So we are trading a 5-15% slowdown (PTI) for another 5-15% slowdown, plus we </span>
<span class="quote">&gt; &gt; are losing the soft-SMEP feature on older CPUs that PTI enables, which is a </span>
<span class="quote">&gt; &gt; pretty powerful mitigation technique.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This soft-SMEP can be kept by keeping PTI if SMEP is unsupported. Although we </span>
<span class="quote">&gt; trade slowdowns, they are different ones, which allows the user to make his best </span>
<span class="quote">&gt; decision.</span>

Indeed, not allowing PTI to be disabled if SMEP is unavailable might be a 
solution.
<span class="quote">
&gt; &gt; Yes, I suspect in some (maybe many) cases it would be a speedup, but I really </span>
<span class="quote">&gt; &gt; don&#39;t like the underlying assumptions and tradeoffs here. (Not that I like any </span>
<span class="quote">&gt; &gt; of this whole Meltdown debacle TBH.)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To make sure that I understand correctly - the assumptions are that disabling </span>
<span class="quote">&gt; PTI on compatibility mode would: (1) Benefit some workloads; (2) Be useful, even </span>
<span class="quote">&gt; if we only consider CPUs with SMEP; and (3) Secure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Under these assumptions, the tradeoff is slightly greater code complexity for </span>
<span class="quote">&gt; considerably better performance of 32-bit code; in some common cases this makes </span>
<span class="quote">&gt; 32-bit code to perform significantly better than 64-bit code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Am I missing something? My main concern was initially security, but so far from </span>
<span class="quote">&gt; your aggregated feedback I did not see something concrete which cannot </span>
<span class="quote">&gt; relatively easily be addressed.</span>

Yes, I suppose.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - Jan. 20, 2018, 4:31 p.m.</div>
<pre class="content">
On Sat, Jan 20, 2018 at 03:26:27PM +0100, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; So we are trading a 5-15% slowdown (PTI) for another 5-15% slowdown, plus we </span>
<span class="quote">&gt; &gt; &gt; are losing the soft-SMEP feature on older CPUs that PTI enables, which is a </span>
<span class="quote">&gt; &gt; &gt; pretty powerful mitigation technique.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This soft-SMEP can be kept by keeping PTI if SMEP is unsupported. Although we </span>
<span class="quote">&gt; &gt; trade slowdowns, they are different ones, which allows the user to make his best </span>
<span class="quote">&gt; &gt; decision.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed, not allowing PTI to be disabled if SMEP is unavailable might be a </span>
<span class="quote">&gt; solution.</span>

Well, I do not agree with this, for the simple reason that the SMEP-like
protection provided by PTI was in fact a byproduct of the Meltdown
mitigation, eventhough quite a valuable one. For me, disabling PTI means
&quot;I want to recover the performance I had on this workload before the PTI
fixes because I value performance over security&quot;. By doing it per process
we&#39;ll allow users to have both performance for a few processes and
protection (including SMEP-like) for the rest of the system. Their only
other choice will be to completely disable PTI, thus removing all
protection and losing the SMEP emulation.

Best regards,
Willy
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
======================
                x86_64          i386            improvement
                -------------------------------------------
PING_INLINE:    103519.66       122249.39       18.09%
PING_BULK:      104493.2        120918.98       15.72%
SET:            86580.09        126103.41       45.65%
GET:            87719.3         124533          41.97%
INCR:           88573.96        123915.73       39.90%
LPUSH:          88731.15        99502.48        12.14%
RPUSH:          88417.33        99108.03        12.09%
LPOP:           88261.25        99304.87        12.51%
RPOP:           88183.43        98716.68        11.94%
SADD:           88028.16        98911.97        12.36%
SPOP:           88261.25        97465.89        10.43%
LPUSH           87873.46        99108.03        12.78%
LRANGE_100      51572.98        47326.08        -8.23%
LRANGE_300      20383.2         16528.93        -18.91%
LRANGE_500      13259.08        10617.97        -19.92%
LRANGE_600      10246.95        8389.26         -18.13%
MSET            89847.26        93370.68        3.92%

Apache (wrk -c 8 -t 4 --latency -d 30s http://localhost)
========================================================
Latency:        137.11us        312.18us        -56%
Req/Sec:        17.19k          17.02k          -1%

This patch provides a rough implementation. It is mainly intended to
provide the idea and gather feedback whether compatibility mode can be
considered safe. It does overlap with Willy Tarreau work and indeed the
NX-bit should be resolved more nicely.

Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;
<span class="p_del">---</span>
 arch/x86/entry/calling.h         | 29 +++++++++++++++++++++++++++++
 arch/x86/entry/entry_64_compat.S |  4 ++--
 arch/x86/include/asm/desc.h      | 17 +++++++++++++++++
 arch/x86/include/asm/tlbflush.h  |  9 ++++++++-
 arch/x86/kernel/process_64.c     | 11 +++++++++++
 arch/x86/mm/pti.c                | 17 -----------------
 6 files changed, 67 insertions(+), 20 deletions(-)

<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 45a63e00a6af..9b0190f4a96b 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -213,7 +213,14 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
 	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not switch on compatibility mode.</span>
<span class="p_add">+	 */</span>
 	mov	%cr3, \scratch_reg
<span class="p_add">+	testq	$PTI_SWITCH_MASK, \scratch_reg</span>
<span class="p_add">+	jz	.Lend_\@</span>
<span class="p_add">+</span>
 	ADJUST_KERNEL_CR3 \scratch_reg
 	mov	\scratch_reg, %cr3
 .Lend_\@:
<span class="p_chunk">@@ -224,6 +231,16 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 .macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req
 	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not switch on compatibility mode. If there is no need for a</span>
<span class="p_add">+	 * flush, run lfence to avoid speculative execution returning to user</span>
<span class="p_add">+	 * with the wrong CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq    PER_CPU_VAR(current_task), \scratch_reg</span>
<span class="p_add">+	testl   $_TIF_IA32, TASK_TI_flags(\scratch_reg)</span>
<span class="p_add">+	jnz     .Lno_spec_\@</span>
<span class="p_add">+</span>
 	mov	%cr3, \scratch_reg
 
 	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID
<span class="p_chunk">@@ -241,6 +258,10 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	movq	\scratch_reg2, \scratch_reg
 	jmp	.Lwrcr3_\@
 
<span class="p_add">+.Lno_spec_\@:</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	.Lend_\@</span>
<span class="p_add">+</span>
 .Lnoflush_\@:
 	movq	\scratch_reg2, \scratch_reg
 	SET_NOFLUSH_BIT \scratch_reg
<span class="p_chunk">@@ -286,6 +307,10 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID
 
<span class="p_add">+	movq    PER_CPU_VAR(current_task), \scratch_reg</span>
<span class="p_add">+	testl   $_TIF_IA32, TASK_TI_flags(\scratch_reg)</span>
<span class="p_add">+	jnz	.Lno_spec_\@</span>
<span class="p_add">+</span>
 	/*
 	 * KERNEL pages can always resume with NOFLUSH as we do
 	 * explicit flushes.
<span class="p_chunk">@@ -305,6 +330,10 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask
 	jmp	.Lwrcr3_\@
 
<span class="p_add">+.Lno_spec_\@:</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	.Lend_\@</span>
<span class="p_add">+</span>
 .Lnoflush_\@:
 	SET_NOFLUSH_BIT \save_reg
 
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 98d5358e4041..0d520c80ef36 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -4,7 +4,7 @@</span> <span class="p_context"></span>
  *
  * Copyright 2000-2002 Andi Kleen, SuSE Labs.
  */
<span class="p_del">-#include &quot;calling.h&quot;</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
 #include &lt;asm/asm-offsets.h&gt;
 #include &lt;asm/current.h&gt;
 #include &lt;asm/errno.h&gt;
<span class="p_chunk">@@ -14,8 +14,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/irqflags.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_del">-#include &lt;linux/linkage.h&gt;</span>
 #include &lt;linux/err.h&gt;
<span class="p_add">+#include &quot;calling.h&quot;</span>
 
 	.section .entry.text, &quot;ax&quot;
 
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 13c5ee878a47..5d139f915b2e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -431,6 +431,23 @@</span> <span class="p_context"> static inline void load_current_idt(void)</span>
 		load_idt((const struct desc_ptr *)&amp;idt_descr);
 }
 
<span class="p_add">+static inline void remove_user_cs64(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct desc_struct *d = get_cpu_gdt_rw(smp_processor_id());</span>
<span class="p_add">+	struct desc_struct user_cs = {0};</span>
<span class="p_add">+</span>
<span class="p_add">+	write_gdt_entry(d, GDT_ENTRY_DEFAULT_USER_CS, &amp;user_cs, DESCTYPE_S);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void restore_user_cs64(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct desc_struct *d = get_cpu_gdt_rw(smp_processor_id());</span>
<span class="p_add">+	struct desc_struct user_cs = GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff);</span>
<span class="p_add">+</span>
<span class="p_add">+	write_gdt_entry(d, GDT_ENTRY_DEFAULT_USER_CS, &amp;user_cs, DESCTYPE_S);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 extern void idt_setup_early_handler(void);
 extern void idt_setup_early_traps(void);
 extern void idt_setup_traps(void);
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4a08dd2ab32a..d11480130ef1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -355,7 +355,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 	 */
 	WARN_ON_ONCE(preemptible());
 
<span class="p_del">-	invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="p_add">+	if (!(current-&gt;mm &amp;&amp; test_thread_flag(TIF_IA32)))</span>
<span class="p_add">+		invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
 
 	/* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */
 	native_write_cr3(__native_read_cr3());
<span class="p_chunk">@@ -407,6 +408,9 @@</span> <span class="p_context"> static inline void __native_flush_tlb_single(unsigned long addr)</span>
 	if (!static_cpu_has(X86_FEATURE_PTI))
 		return;
 
<span class="p_add">+	if (current-&gt;mm &amp;&amp; test_thread_flag(TIF_IA32))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	/*
 	 * Some platforms #GP if we call invpcid(type=1/2) before CR4.PCIDE=1.
 	 * Just use invalidate_user_asid() in case we are called early.
<span class="p_chunk">@@ -443,6 +447,9 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
 	if (!static_cpu_has(X86_FEATURE_PTI))
 		return;
 
<span class="p_add">+	if (current-&gt;mm &amp;&amp; test_thread_flag(TIF_IA32))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	/*
 	 * __flush_tlb_single() will have cleared the TLB entry for this ASID,
 	 * but since kernel space is replicated across all, we must also
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index c75466232016..01235402cbbe 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -473,6 +473,17 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 		     task_thread_info(prev_p)-&gt;flags &amp; _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
<span class="p_add">+#if defined(CONFIG_PAGE_TABLE_ISOLATION) &amp;&amp; defined(CONFIG_IA32_EMULATION)</span>
<span class="p_add">+	if (unlikely(static_cpu_has(X86_FEATURE_PTI) &amp;&amp;</span>
<span class="p_add">+		     (task_thread_info(next_p)-&gt;flags ^</span>
<span class="p_add">+		      task_thread_info(prev_p)-&gt;flags) &amp; _TIF_IA32)) {</span>
<span class="p_add">+		if (task_thread_info(next_p)-&gt;flags &amp; _TIF_IA32)</span>
<span class="p_add">+			remove_user_cs64();</span>
<span class="p_add">+		else</span>
<span class="p_add">+			restore_user_cs64();</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_XEN_PV
 	/*
 	 * On Xen PV, IOPL bits in pt_regs-&gt;flags have no effect, and
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index 43d4a4a29037..c7d7f5a35d8c 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -122,23 +122,6 @@</span> <span class="p_context"> pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
 	 */
 	kernel_to_user_pgdp(pgdp)-&gt;pgd = pgd.pgd;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If this is normal user memory, make it NX in the kernel</span>
<span class="p_del">-	 * pagetables so that, if we somehow screw up and return to</span>
<span class="p_del">-	 * usermode with the kernel CR3 loaded, we&#39;ll get a page fault</span>
<span class="p_del">-	 * instead of allowing user code to execute with the wrong CR3.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * As exceptions, we don&#39;t set NX if:</span>
<span class="p_del">-	 *  - _PAGE_USER is not set.  This could be an executable</span>
<span class="p_del">-	 *     EFI runtime mapping or something similar, and the kernel</span>
<span class="p_del">-	 *     may execute from it</span>
<span class="p_del">-	 *  - we don&#39;t have NX support</span>
<span class="p_del">-	 *  - we&#39;re clearing the PGD (i.e. the new pgd is not present).</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if ((pgd.pgd &amp; (_PAGE_USER|_PAGE_PRESENT)) == (_PAGE_USER|_PAGE_PRESENT) &amp;&amp;</span>
<span class="p_del">-	    (__supported_pte_mask &amp; _PAGE_NX))</span>
<span class="p_del">-		pgd.pgd |= _PAGE_NX;</span>
<span class="p_del">-</span>
 	/* return the copy of the PGD we want the kernel to use: */
 	return pgd;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



