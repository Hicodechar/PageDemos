
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V3] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V3] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 16, 2017, 10:05 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170516100509.20122-1-khandual@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9728799/mbox/"
   >mbox</a>
|
   <a href="/patch/9728799/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9728799/">/patch/9728799/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E827E6028A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 10:06:24 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D7B51289FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 10:06:24 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C8E3728A16; Tue, 16 May 2017 10:06:24 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AA2E7289FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 10:06:23 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752514AbdEPKGO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 16 May 2017 06:06:14 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:48971 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1751080AbdEPKGN (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 16 May 2017 06:06:13 -0400
Received: from pps.filterd (m0098417.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v4GA3nPa141750
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 06:06:12 -0400
Received: from e23smtp07.au.ibm.com (e23smtp07.au.ibm.com [202.81.31.140])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2aft10xc6t-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 06:06:12 -0400
Received: from localhost
	by e23smtp07.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;khandual@linux.vnet.ibm.com&gt;;
	Tue, 16 May 2017 20:06:09 +1000
Received: from d23relay06.au.ibm.com (202.81.31.225)
	by e23smtp07.au.ibm.com (202.81.31.204) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Tue, 16 May 2017 20:06:07 +1000
Received: from d23av02.au.ibm.com (d23av02.au.ibm.com [9.190.235.138])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	v4GA5xCo19398656
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 20:06:07 +1000
Received: from d23av02.au.ibm.com (localhost [127.0.0.1])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	v4GA5Ski025930
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 20:05:28 +1000
Received: from localhost.in.ibm.com ([9.124.220.25])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	v4GA5Op2025192; Tue, 16 May 2017 20:05:26 +1000
From: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: akpm@linux-foundation.org
Subject: [PATCH V3] mm/madvise: Enable (soft|hard) offline of HugeTLB pages
	at PGD level
Date: Tue, 16 May 2017 15:35:09 +0530
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;20170426035731.6924-1-khandual@linux.vnet.ibm.com&gt;
References: &lt;20170426035731.6924-1-khandual@linux.vnet.ibm.com&gt;
X-TM-AS-MML: disable
x-cbid: 17051610-0044-0000-0000-000002579CE6
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17051610-0045-0000-0000-000006E5FA8B
Message-Id: &lt;20170516100509.20122-1-khandual@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-05-16_03:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1705160087
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - May 16, 2017, 10:05 a.m.</div>
<pre class="content">
Though migrating gigantic HugeTLB pages does not sound much like real
world use case, they can be affected by memory errors. Hence migration
at the PGD level HugeTLB pages should be supported just to enable soft
and hard offline use cases.

While allocating the new gigantic HugeTLB page, it should not matter
whether new page comes from the same node or not. There would be very
few gigantic pages on the system afterall, we should not be bothered
about node locality when trying to save a big page from crashing.

This change renames dequeu_huge_page_node() function as dequeue_huge
_page_node_exact() preserving it&#39;s original functionality. Now the new
dequeue_huge_page_node() function scans through all available online
nodes to allocate a huge page for the NUMA_NO_NODE case and just falls
back calling dequeu_huge_page_node_exact() for all other cases.
<span class="signed-off-by">
Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
---
Changes in V3:
* Dropped alloc_huge_page_nonid() as per Andrew
* Changed dequeue_huge_page_node() to accommodate NUMA_NO_NODE as per Andrew
* Added dequeue_huge_page_node_exact() which implements functionality for the
  previous dequeue_huge_page_node() function

Changes in V2:
 * Added hstate_is_gigantic() definition when !CONFIG_HUGETLB_PAGE
   which takes care of the build failure reported earlier.

 include/linux/hugetlb.h |  7 ++++++-
 mm/hugetlb.c            | 18 +++++++++++++++++-
 mm/memory-failure.c     | 13 +++++++++----
 3 files changed, 32 insertions(+), 6 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 28, 2017, 12:49 a.m.</div>
<pre class="content">
On 05/16/2017 03:05 AM, Anshuman Khandual wrote:
<span class="quote">&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt; and hard offline use cases.</span>

Hi Anshuman,

Sorry for the late question, but I just stumbled on this code when
looking at something else.

It appears the primary motivation for these changes is to handle
memory errors in gigantic pages.  In this case, you migrate to
another gigantic page.  However, doesn&#39;t this assume that there is
a pre-allocated gigantic page sitting unused that will be the target
of the migration?  alloc_huge_page_node will not allocate a gigantic
page.  Or, am I missing something?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - July 28, 2017, 5:53 a.m.</div>
<pre class="content">
On 07/28/2017 06:19 AM, Mike Kravetz wrote:
<span class="quote">&gt; On 05/16/2017 03:05 AM, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt;&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt;&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt;&gt; and hard offline use cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Anshuman,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry for the late question, but I just stumbled on this code when</span>
<span class="quote">&gt; looking at something else.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It appears the primary motivation for these changes is to handle</span>
<span class="quote">&gt; memory errors in gigantic pages.  In this case, you migrate to</span>

Right.
<span class="quote">
&gt; another gigantic page.  However, doesn&#39;t this assume that there is</span>

Right.
<span class="quote">
&gt; a pre-allocated gigantic page sitting unused that will be the target</span>
<span class="quote">&gt; of the migration?  alloc_huge_page_node will not allocate a gigantic</span>
<span class="quote">&gt; page.  Or, am I missing something?</span>

Yes, its in the context of 16GB pages on POWER8 system where all the
gigantic pages are pre allocated from the platform and passed on to
the kernel through the device tree. We dont allocate these gigantic
pages on runtime.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index b857fc8cc2ec..614a0a40f1ef 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -466,7 +466,11 @@</span> <span class="p_context"> extern int dissolve_free_huge_pages(unsigned long start_pfn,</span>
 static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
<span class="p_del">-	return huge_page_shift(h) == PMD_SHIFT;</span>
<span class="p_add">+	if ((huge_page_shift(h) == PMD_SHIFT) ||</span>
<span class="p_add">+		(huge_page_shift(h) == PGDIR_SHIFT))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return false;</span>
 #else
 	return false;
 #endif
<span class="p_chunk">@@ -518,6 +522,7 @@</span> <span class="p_context"> struct hstate {};</span>
 #define vma_mmu_pagesize(v) PAGE_SIZE
 #define huge_page_order(h) 0
 #define huge_page_shift(h) PAGE_SHIFT
<span class="p_add">+#define hstate_is_gigantic(h) 0</span>
 static inline unsigned int pages_per_huge_page(struct hstate *h)
 {
 	return 1;
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index e5828875f7bb..7cd0f09b8dd0 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -867,7 +867,7 @@</span> <span class="p_context"> static void enqueue_huge_page(struct hstate *h, struct page *page)</span>
 	h-&gt;free_huge_pages_node[nid]++;
 }
 
<span class="p_del">-static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="p_add">+static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
 {
 	struct page *page;
 
<span class="p_chunk">@@ -887,6 +887,22 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_add">+static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int node;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (nid != NUMA_NO_NODE)</span>
<span class="p_add">+		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_online_node(node) {</span>
<span class="p_add">+		page = dequeue_huge_page_node_exact(h, node);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Movability of hugepages depends on migration support. */
 static inline gfp_t htlb_alloc_mask(struct hstate *h)
 {
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index 2527dfeddb00..f71efae2e494 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -1489,11 +1489,16 @@</span> <span class="p_context"> EXPORT_SYMBOL(unpoison_memory);</span>
 static struct page *new_page(struct page *p, unsigned long private, int **x)
 {
 	int nid = page_to_nid(p);
<span class="p_del">-	if (PageHuge(p))</span>
<span class="p_del">-		return alloc_huge_page_node(page_hstate(compound_head(p)),</span>
<span class="p_del">-						   nid);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (PageHuge(p)) {</span>
<span class="p_add">+		struct hstate *hstate = page_hstate(compound_head(p));</span>
<span class="p_add">+</span>
<span class="p_add">+		if (hstate_is_gigantic(hstate))</span>
<span class="p_add">+			return alloc_huge_page_node(hstate, NUMA_NO_NODE);</span>
<span class="p_add">+</span>
<span class="p_add">+		return alloc_huge_page_node(hstate, nid);</span>
<span class="p_add">+	} else {</span>
 		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);
<span class="p_add">+	}</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



