
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] remove unnecessary condition in remove_inode_hugepages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] remove unnecessary condition in remove_inode_hugepages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=146341">zhong jiang</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 23, 2016, 1:53 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;57E48B30.2000303@huawei.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9347295/mbox/"
   >mbox</a>
|
   <a href="/patch/9347295/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9347295/">/patch/9347295/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BEE90607EE for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 23 Sep 2016 01:57:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 905B62AC88
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 23 Sep 2016 01:57:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6ED3E2ACAF; Fri, 23 Sep 2016 01:57:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E5D752AC88
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 23 Sep 2016 01:57:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1034065AbcIWB5l (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Sep 2016 21:57:41 -0400
Received: from szxga02-in.huawei.com ([119.145.14.65]:57460 &quot;EHLO
	szxga02-in.huawei.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S965135AbcIWB5k (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Sep 2016 21:57:40 -0400
Received: from 172.24.1.60 (EHLO szxeml422-hub.china.huawei.com)
	([172.24.1.60])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id DNN93860; Fri, 23 Sep 2016 09:54:05 +0800 (CST)
Received: from [127.0.0.1] (10.177.29.68) by szxeml422-hub.china.huawei.com
	(10.82.67.152) with Microsoft SMTP Server id 14.3.235.1;
	Fri, 23 Sep 2016 09:54:03 +0800
Message-ID: &lt;57E48B30.2000303@huawei.com&gt;
Date: Fri, 23 Sep 2016 09:53:52 +0800
From: zhong jiang &lt;zhongjiang@huawei.com&gt;
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64;
	rv:12.0) Gecko/20120428 Thunderbird/12.0.1
MIME-Version: 1.0
To: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	&quot;Hugh Dickins&quot; &lt;hughd@google.com&gt;
CC: Linux Memory Management List &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: [RFC] remove unnecessary condition in remove_inode_hugepages
Content-Type: text/plain; charset=&quot;ISO-8859-1&quot;
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.177.29.68]
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020203.57E48B3F.00A3, ss=1, re=0.000, recu=0.000,
	reip=0.000, cl=1, cld=1, fgs=0, ip=0.0.0.0,
	so=2013-06-18 04:22:30, dmn=2013-03-21 17:37:32
X-Mirapoint-Loop-Id: 85e4e7b75070b26fc63764715fc3b9fb
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - Sept. 23, 2016, 1:53 a.m.</div>
<pre class="content">
At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,
and PagePrivate will decide hugetlb reserves counts.

we obtain the page from page cache. and use page both lock_page and mutex_lock.
alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate
before unlock page. 

but I&#39; m not sure  it is right  or I miss the points.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Sept. 23, 2016, 8:18 a.m.</div>
<pre class="content">
[CC Naoya]

On Fri 23-09-16 09:53:52, zhong jiang wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,</span>
<span class="quote">&gt; and PagePrivate will decide hugetlb reserves counts.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; we obtain the page from page cache. and use page both lock_page and mutex_lock.</span>
<span class="quote">&gt; alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate</span>
<span class="quote">&gt; before unlock page. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; but I&#39; m not sure  it is right  or I miss the points.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; index 4ea71eb..010723b 100644</span>
<span class="quote">&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; @@ -462,14 +462,12 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;                          * of error.</span>
<span class="quote">&gt;                          */</span>
<span class="quote">&gt; -                       rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt;                         remove_huge_page(page);</span>
<span class="quote">&gt;                         freed++;</span>
<span class="quote">&gt;                         if (!truncate_op) {</span>
<span class="quote">&gt;                                 if (unlikely(hugetlb_unreserve_pages(inode,</span>
<span class="quote">&gt;                                                         next, next + 1, 1)))</span>
<span class="quote">&gt; -                                       hugetlb_fix_reserve_counts(inode,</span>
<span class="quote">&gt; -                                                               rsv_on_error);</span>
<span class="quote">&gt; +                                       hugetlb_fix_reserve_counts(inode)</span>
<span class="quote">&gt;                         }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                         unlock_page(page);</span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index c26d463..d2e0fc5 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -90,7 +90,7 @@ int dequeue_hwpoisoned_huge_page(struct page *page);</span>
<span class="quote">&gt;  bool isolate_huge_page(struct page *page, struct list_head *list);</span>
<span class="quote">&gt;  void putback_active_hugepage(struct page *page);</span>
<span class="quote">&gt;  void free_huge_page(struct page *page);</span>
<span class="quote">&gt; -void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve);</span>
<span class="quote">&gt; +void hugetlb_fix_reserve_counts(struct inode *inode);</span>
<span class="quote">&gt;  extern struct mutex *hugetlb_fault_mutex_table;</span>
<span class="quote">&gt;  u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,</span>
<span class="quote">&gt;                                 struct vm_area_struct *vma,</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 87e11d8..28a079a 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -567,13 +567,13 @@ retry:</span>
<span class="quote">&gt;   * appear as a &quot;reserved&quot; entry instead of simply dangling with incorrect</span>
<span class="quote">&gt;   * counts.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve)</span>
<span class="quote">&gt; +void hugetlb_fix_reserve_counts(struct inode *inode)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct hugepage_subpool *spool = subpool_inode(inode);</span>
<span class="quote">&gt;         long rsv_adjust;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         rsv_adjust = hugepage_subpool_get_pages(spool, 1);</span>
<span class="quote">&gt; -       if (restore_reserve &amp;&amp; rsv_adjust) {</span>
<span class="quote">&gt; +       if (rsv_adjust) {</span>
<span class="quote">&gt;                 struct hstate *h = hstate_inode(inode);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                 hugetlb_acct_memory(h, 1);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Sept. 23, 2016, 5:19 p.m.</div>
<pre class="content">
On 09/22/2016 06:53 PM, zhong jiang wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,</span>
<span class="quote">&gt; and PagePrivate will decide hugetlb reserves counts.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; we obtain the page from page cache. and use page both lock_page and mutex_lock.</span>
<span class="quote">&gt; alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate</span>
<span class="quote">&gt; before unlock page. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; but I&#39; m not sure  it is right  or I miss the points.</span>

Let me try to explain the code you suggest is unnecessary.

The PagePrivate flag is used in huge page allocation/deallocation to
indicate that the page was globally reserved.  For example, in
dequeue_huge_page_vma() there is this code:

                        if (page) {
                                if (avoid_reserve)
                                        break;
                                if (!vma_has_reserves(vma, chg))
                                        break;

                                SetPagePrivate(page);
                                h-&gt;resv_huge_pages--;
                                break;
                        }

and in free_huge_page():

        restore_reserve = PagePrivate(page);
        ClearPagePrivate(page);
	.
	&lt;snip&gt;
	.
        if (restore_reserve)
                h-&gt;resv_huge_pages++;

This helps maintains the global huge page reserve count.

In addition to the global reserve count, there are per VMA reservation
structures.  Unfortunately, these structures have different meanings
depending on the context in which they are used.

If there is a VMA reservation entry for a page, and the page has not
been instantiated in the VMA this indicates there is a huge page reserved
and the global resv_huge_pages count reflects that reservation.  Even
if a page was not reserved, a VMA reservation entry is added when a page
is instantiated in the VMA.

With that background, let&#39;s look at the existing code/proposed changes.
<span class="quote">
&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; index 4ea71eb..010723b 100644</span>
<span class="quote">&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; @@ -462,14 +462,12 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;                          * of error.</span>
<span class="quote">&gt;                          */</span>
<span class="quote">&gt; -                       rsv_on_error = !PagePrivate(page);</span>

This rsv_on_error flag indicates that when the huge page was allocated,
it was NOT counted against the global reserve count.  So, when
remove_huge_page eventually calls free_huge_page(), the global count
resv_huge_pages is not incremented.  So far, no problem.
<span class="quote">
&gt;                         remove_huge_page(page);</span>
<span class="quote">&gt;                         freed++;</span>
<span class="quote">&gt;                         if (!truncate_op) {</span>
<span class="quote">&gt;                                 if (unlikely(hugetlb_unreserve_pages(inode,</span>
<span class="quote">&gt;                                                         next, next + 1, 1)))</span>

We now have this VERY unlikely situation that hugetlb_unreserve_pages fails.
This means that the VMA reservation entry for the page was not removed.
So, we are in a bit of a mess.  The page has already been removed, but the
VMA reservation entry can not.  This LOOKS like there is a reservation for
the page in the VMA reservation structure.  But, the global count
resv_huge_pages does not reflect this reservation.

If we do nothing, when the VMA is eventually removed the VMA reservation
structure will be completely removed and the global count resv_huge_pages
will be decremented for each entry in the structure.  Since, there is a
VMA reservation entry without a corresponding global count, the global
count will be one less than it should (will eventually go to -1).

To &#39;fix&#39; this, hugetlb_fix_reserve_counts is called.  In this case, it will
increment the global count so that it is consistent with the entries in
the VMA reservation structure.

This is all quite confusing and really unlikely to happen.  I tried to
explain in code comments:

Before removing the page:
                        /*
                         * We must free the huge page and remove from page
                         * cache (remove_huge_page) BEFORE removing the
                         * region/reserve map (hugetlb_unreserve_pages).  In
                         * rare out of memory conditions, removal of the
                         * region/reserve map could fail.  Before free&#39;ing
                         * the page, note PagePrivate which is used in case
                         * of error.
                         */

And, the routine hugetlb_fix_reserve_counts:
/*
 * A rare out of memory error was encountered which prevented removal of
 * the reserve map region for a page.  The huge page itself was free&#39;ed
 * and removed from the page cache.  This routine will adjust the subpool
 * usage count, and the global reserve count if needed.  By incrementing
 * these counts, the reserve map entry which could not be deleted will
 * appear as a &quot;reserved&quot; entry instead of simply dangling with incorrect
 * counts.
 */
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - Sept. 24, 2016, 2:56 a.m.</div>
<pre class="content">
On 2016/9/24 1:19, Mike Kravetz wrote:
<span class="quote">&gt; On 09/22/2016 06:53 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt; At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,</span>
<span class="quote">&gt;&gt; and PagePrivate will decide hugetlb reserves counts.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; we obtain the page from page cache. and use page both lock_page and mutex_lock.</span>
<span class="quote">&gt;&gt; alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate</span>
<span class="quote">&gt;&gt; before unlock page. </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; but I&#39; m not sure  it is right  or I miss the points.</span>
<span class="quote">&gt; Let me try to explain the code you suggest is unnecessary.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The PagePrivate flag is used in huge page allocation/deallocation to</span>
<span class="quote">&gt; indicate that the page was globally reserved.  For example, in</span>
<span class="quote">&gt; dequeue_huge_page_vma() there is this code:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                         if (page) {</span>
<span class="quote">&gt;                                 if (avoid_reserve)</span>
<span class="quote">&gt;                                         break;</span>
<span class="quote">&gt;                                 if (!vma_has_reserves(vma, chg))</span>
<span class="quote">&gt;                                         break;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                                 SetPagePrivate(page);</span>
<span class="quote">&gt;                                 h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt;                                 break;</span>
<span class="quote">&gt;                         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; and in free_huge_page():</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         restore_reserve = PagePrivate(page);</span>
<span class="quote">&gt;         ClearPagePrivate(page);</span>
<span class="quote">&gt; 	.</span>
<span class="quote">&gt; 	&lt;snip&gt;</span>
<span class="quote">&gt; 	.</span>
<span class="quote">&gt;         if (restore_reserve)</span>
<span class="quote">&gt;                 h-&gt;resv_huge_pages++;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This helps maintains the global huge page reserve count.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In addition to the global reserve count, there are per VMA reservation</span>
<span class="quote">&gt; structures.  Unfortunately, these structures have different meanings</span>
<span class="quote">&gt; depending on the context in which they are used.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If there is a VMA reservation entry for a page, and the page has not</span>
<span class="quote">&gt; been instantiated in the VMA this indicates there is a huge page reserved</span>
<span class="quote">&gt; and the global resv_huge_pages count reflects that reservation.  Even</span>
<span class="quote">&gt; if a page was not reserved, a VMA reservation entry is added when a page</span>
<span class="quote">&gt; is instantiated in the VMA.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; With that background, let&#39;s look at the existing code/proposed changes.</span>
 Clearly. 
<span class="quote">&gt;&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; index 4ea71eb..010723b 100644</span>
<span class="quote">&gt;&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; @@ -462,14 +462,12 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;&gt;                          * of error.</span>
<span class="quote">&gt;&gt;                          */</span>
<span class="quote">&gt;&gt; -                       rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt; This rsv_on_error flag indicates that when the huge page was allocated,</span>
   yes
<span class="quote">&gt; it was NOT counted against the global reserve count.  So, when</span>
<span class="quote">&gt; remove_huge_page eventually calls free_huge_page(), the global count</span>
<span class="quote">&gt; resv_huge_pages is not incremented.  So far, no problem.</span>
 but the page comes from the page cache.  if it is.  it should implement
 ClearPageprivate(page) when lock page.   This condition always true.

  The key point is why it need still check the PagePrivate(page) when page from
  page cache and hold lock.

  Thanks you
 zhongjiang
<span class="quote">&gt;&gt;                         remove_huge_page(page);</span>
<span class="quote">&gt;&gt;                         freed++;</span>
<span class="quote">&gt;&gt;                         if (!truncate_op) {</span>
<span class="quote">&gt;&gt;                                 if (unlikely(hugetlb_unreserve_pages(inode,</span>
<span class="quote">&gt;&gt;                                                         next, next + 1, 1)))</span>
<span class="quote">&gt; We now have this VERY unlikely situation that hugetlb_unreserve_pages fails.</span>
<span class="quote">&gt; This means that the VMA reservation entry for the page was not removed.</span>
<span class="quote">&gt; So, we are in a bit of a mess.  The page has already been removed, but the</span>
<span class="quote">&gt; VMA reservation entry can not.  This LOOKS like there is a reservation for</span>
<span class="quote">&gt; the page in the VMA reservation structure.  But, the global count</span>
<span class="quote">&gt; resv_huge_pages does not reflect this reservation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If we do nothing, when the VMA is eventually removed the VMA reservation</span>
<span class="quote">&gt; structure will be completely removed and the global count resv_huge_pages</span>
<span class="quote">&gt; will be decremented for each entry in the structure.  Since, there is a</span>
<span class="quote">&gt; VMA reservation entry without a corresponding global count, the global</span>
<span class="quote">&gt; count will be one less than it should (will eventually go to -1).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To &#39;fix&#39; this, hugetlb_fix_reserve_counts is called.  In this case, it will</span>
<span class="quote">&gt; increment the global count so that it is consistent with the entries in</span>
<span class="quote">&gt; the VMA reservation structure.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is all quite confusing and really unlikely to happen.  I tried to</span>
<span class="quote">&gt; explain in code comments:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Before removing the page:</span>
<span class="quote">&gt;                         /*</span>
<span class="quote">&gt;                          * We must free the huge page and remove from page</span>
<span class="quote">&gt;                          * cache (remove_huge_page) BEFORE removing the</span>
<span class="quote">&gt;                          * region/reserve map (hugetlb_unreserve_pages).  In</span>
<span class="quote">&gt;                          * rare out of memory conditions, removal of the</span>
<span class="quote">&gt;                          * region/reserve map could fail.  Before free&#39;ing</span>
<span class="quote">&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;                          * of error.</span>
<span class="quote">&gt;                          */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And, the routine hugetlb_fix_reserve_counts:</span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt;  * A rare out of memory error was encountered which prevented removal of</span>
<span class="quote">&gt;  * the reserve map region for a page.  The huge page itself was free&#39;ed</span>
<span class="quote">&gt;  * and removed from the page cache.  This routine will adjust the subpool</span>
<span class="quote">&gt;  * usage count, and the global reserve count if needed.  By incrementing</span>
<span class="quote">&gt;  * these counts, the reserve map entry which could not be deleted will</span>
<span class="quote">&gt;  * appear as a &quot;reserved&quot; entry instead of simply dangling with incorrect</span>
<span class="quote">&gt;  * counts.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Sept. 25, 2016, 12:06 a.m.</div>
<pre class="content">
On 09/23/2016 07:56 PM, zhong jiang wrote:
<span class="quote">&gt; On 2016/9/24 1:19, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt; On 09/22/2016 06:53 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt;&gt; At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,</span>
<span class="quote">&gt;&gt;&gt; and PagePrivate will decide hugetlb reserves counts.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; we obtain the page from page cache. and use page both lock_page and mutex_lock.</span>
<span class="quote">&gt;&gt;&gt; alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate</span>
<span class="quote">&gt;&gt;&gt; before unlock page. </span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; but I&#39; m not sure  it is right  or I miss the points.</span>
<span class="quote">&gt;&gt; Let me try to explain the code you suggest is unnecessary.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The PagePrivate flag is used in huge page allocation/deallocation to</span>
<span class="quote">&gt;&gt; indicate that the page was globally reserved.  For example, in</span>
<span class="quote">&gt;&gt; dequeue_huge_page_vma() there is this code:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;                         if (page) {</span>
<span class="quote">&gt;&gt;                                 if (avoid_reserve)</span>
<span class="quote">&gt;&gt;                                         break;</span>
<span class="quote">&gt;&gt;                                 if (!vma_has_reserves(vma, chg))</span>
<span class="quote">&gt;&gt;                                         break;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;                                 SetPagePrivate(page);</span>
<span class="quote">&gt;&gt;                                 h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt;&gt;                                 break;</span>
<span class="quote">&gt;&gt;                         }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; and in free_huge_page():</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;         restore_reserve = PagePrivate(page);</span>
<span class="quote">&gt;&gt;         ClearPagePrivate(page);</span>
<span class="quote">&gt;&gt; 	.</span>
<span class="quote">&gt;&gt; 	&lt;snip&gt;</span>
<span class="quote">&gt;&gt; 	.</span>
<span class="quote">&gt;&gt;         if (restore_reserve)</span>
<span class="quote">&gt;&gt;                 h-&gt;resv_huge_pages++;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This helps maintains the global huge page reserve count.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In addition to the global reserve count, there are per VMA reservation</span>
<span class="quote">&gt;&gt; structures.  Unfortunately, these structures have different meanings</span>
<span class="quote">&gt;&gt; depending on the context in which they are used.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If there is a VMA reservation entry for a page, and the page has not</span>
<span class="quote">&gt;&gt; been instantiated in the VMA this indicates there is a huge page reserved</span>
<span class="quote">&gt;&gt; and the global resv_huge_pages count reflects that reservation.  Even</span>
<span class="quote">&gt;&gt; if a page was not reserved, a VMA reservation entry is added when a page</span>
<span class="quote">&gt;&gt; is instantiated in the VMA.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; With that background, let&#39;s look at the existing code/proposed changes.</span>
<span class="quote">&gt;  Clearly. </span>
<span class="quote">&gt;&gt;&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt; index 4ea71eb..010723b 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt; @@ -462,14 +462,12 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;&gt;&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;&gt;&gt;                          * of error.</span>
<span class="quote">&gt;&gt;&gt;                          */</span>
<span class="quote">&gt;&gt;&gt; -                       rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt;&gt; This rsv_on_error flag indicates that when the huge page was allocated,</span>
<span class="quote">&gt;    yes</span>
<span class="quote">&gt;&gt; it was NOT counted against the global reserve count.  So, when</span>
<span class="quote">&gt;&gt; remove_huge_page eventually calls free_huge_page(), the global count</span>
<span class="quote">&gt;&gt; resv_huge_pages is not incremented.  So far, no problem.</span>
<span class="quote">&gt;  but the page comes from the page cache.  if it is.  it should implement</span>
<span class="quote">&gt;  ClearPageprivate(page) when lock page.   This condition always true.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   The key point is why it need still check the PagePrivate(page) when page from</span>
<span class="quote">&gt;   page cache and hold lock.</span>

You are correct.  My apologies for not seeing your point in the original
post.

When the huge page is added to the page cache (huge_add_to_page_cache),
the Page Private flag will be cleared.  Since this code
(remove_inode_hugepages) will only be called for pages in the page cache,
PagePrivate(page) will always be false.

The comments in this area should be changed along with the code.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - Sept. 25, 2016, 6:40 a.m.</div>
<pre class="content">
On 2016/9/25 8:06, Mike Kravetz wrote:
<span class="quote">&gt; On 09/23/2016 07:56 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt; On 2016/9/24 1:19, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt;&gt; On 09/22/2016 06:53 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; At present, we need to call hugetlb_fix_reserve_count when hugetlb_unrserve_pages fails,</span>
<span class="quote">&gt;&gt;&gt;&gt; and PagePrivate will decide hugetlb reserves counts.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; we obtain the page from page cache. and use page both lock_page and mutex_lock.</span>
<span class="quote">&gt;&gt;&gt;&gt; alloc_huge_page add page to page chace always hold lock page, then bail out clearpageprivate</span>
<span class="quote">&gt;&gt;&gt;&gt; before unlock page. </span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; but I&#39; m not sure  it is right  or I miss the points.</span>
<span class="quote">&gt;&gt;&gt; Let me try to explain the code you suggest is unnecessary.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The PagePrivate flag is used in huge page allocation/deallocation to</span>
<span class="quote">&gt;&gt;&gt; indicate that the page was globally reserved.  For example, in</span>
<span class="quote">&gt;&gt;&gt; dequeue_huge_page_vma() there is this code:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;                         if (page) {</span>
<span class="quote">&gt;&gt;&gt;                                 if (avoid_reserve)</span>
<span class="quote">&gt;&gt;&gt;                                         break;</span>
<span class="quote">&gt;&gt;&gt;                                 if (!vma_has_reserves(vma, chg))</span>
<span class="quote">&gt;&gt;&gt;                                         break;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;                                 SetPagePrivate(page);</span>
<span class="quote">&gt;&gt;&gt;                                 h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt;&gt;&gt;                                 break;</span>
<span class="quote">&gt;&gt;&gt;                         }</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; and in free_huge_page():</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;         restore_reserve = PagePrivate(page);</span>
<span class="quote">&gt;&gt;&gt;         ClearPagePrivate(page);</span>
<span class="quote">&gt;&gt;&gt; 	.</span>
<span class="quote">&gt;&gt;&gt; 	&lt;snip&gt;</span>
<span class="quote">&gt;&gt;&gt; 	.</span>
<span class="quote">&gt;&gt;&gt;         if (restore_reserve)</span>
<span class="quote">&gt;&gt;&gt;                 h-&gt;resv_huge_pages++;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This helps maintains the global huge page reserve count.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; In addition to the global reserve count, there are per VMA reservation</span>
<span class="quote">&gt;&gt;&gt; structures.  Unfortunately, these structures have different meanings</span>
<span class="quote">&gt;&gt;&gt; depending on the context in which they are used.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; If there is a VMA reservation entry for a page, and the page has not</span>
<span class="quote">&gt;&gt;&gt; been instantiated in the VMA this indicates there is a huge page reserved</span>
<span class="quote">&gt;&gt;&gt; and the global resv_huge_pages count reflects that reservation.  Even</span>
<span class="quote">&gt;&gt;&gt; if a page was not reserved, a VMA reservation entry is added when a page</span>
<span class="quote">&gt;&gt;&gt; is instantiated in the VMA.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; With that background, let&#39;s look at the existing code/proposed changes.</span>
<span class="quote">&gt;&gt;  Clearly. </span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt;&gt; index 4ea71eb..010723b 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -462,14 +462,12 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;&gt;&gt;&gt;                          * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt;&gt;&gt;&gt;                          * of error.</span>
<span class="quote">&gt;&gt;&gt;&gt;                          */</span>
<span class="quote">&gt;&gt;&gt;&gt; -                       rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt;&gt;&gt; This rsv_on_error flag indicates that when the huge page was allocated,</span>
<span class="quote">&gt;&gt;    yes</span>
<span class="quote">&gt;&gt;&gt; it was NOT counted against the global reserve count.  So, when</span>
<span class="quote">&gt;&gt;&gt; remove_huge_page eventually calls free_huge_page(), the global count</span>
<span class="quote">&gt;&gt;&gt; resv_huge_pages is not incremented.  So far, no problem.</span>
<span class="quote">&gt;&gt;  but the page comes from the page cache.  if it is.  it should implement</span>
<span class="quote">&gt;&gt;  ClearPageprivate(page) when lock page.   This condition always true.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   The key point is why it need still check the PagePrivate(page) when page from</span>
<span class="quote">&gt;&gt;   page cache and hold lock.</span>
<span class="quote">&gt; You are correct.  My apologies for not seeing your point in the original</span>
<span class="quote">&gt; post.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; When the huge page is added to the page cache (huge_add_to_page_cache),</span>
<span class="quote">&gt; the Page Private flag will be cleared.  Since this code</span>
<span class="quote">&gt; (remove_inode_hugepages) will only be called for pages in the page cache,</span>
<span class="quote">&gt; PagePrivate(page) will always be false.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The comments in this area should be changed along with the code.</span>
<span class="quote">&gt;</span>
 Thanks, I will resend the patch.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 4ea71eb..010723b 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -462,14 +462,12 @@</span> <span class="p_context"> static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
                         * the page, note PagePrivate which is used in case
                         * of error.
                         */
<span class="p_del">-                       rsv_on_error = !PagePrivate(page);</span>
                        remove_huge_page(page);
                        freed++;
                        if (!truncate_op) {
                                if (unlikely(hugetlb_unreserve_pages(inode,
                                                        next, next + 1, 1)))
<span class="p_del">-                                       hugetlb_fix_reserve_counts(inode,</span>
<span class="p_del">-                                                               rsv_on_error);</span>
<span class="p_add">+                                       hugetlb_fix_reserve_counts(inode)</span>
                        }

                        unlock_page(page);
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index c26d463..d2e0fc5 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -90,7 +90,7 @@</span> <span class="p_context"> int dequeue_hwpoisoned_huge_page(struct page *page);</span>
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
 void free_huge_page(struct page *page);
<span class="p_del">-void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve);</span>
<span class="p_add">+void hugetlb_fix_reserve_counts(struct inode *inode);</span>
 extern struct mutex *hugetlb_fault_mutex_table;
 u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
                                struct vm_area_struct *vma,
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 87e11d8..28a079a 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -567,13 +567,13 @@</span> <span class="p_context"> retry:</span>
  * appear as a &quot;reserved&quot; entry instead of simply dangling with incorrect
  * counts.
  */
<span class="p_del">-void hugetlb_fix_reserve_counts(struct inode *inode, bool restore_reserve)</span>
<span class="p_add">+void hugetlb_fix_reserve_counts(struct inode *inode)</span>
 {
        struct hugepage_subpool *spool = subpool_inode(inode);
        long rsv_adjust;

        rsv_adjust = hugepage_subpool_get_pages(spool, 1);
<span class="p_del">-       if (restore_reserve &amp;&amp; rsv_adjust) {</span>
<span class="p_add">+       if (rsv_adjust) {</span>
                struct hstate *h = hstate_inode(inode);

                hugetlb_acct_memory(h, 1);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



