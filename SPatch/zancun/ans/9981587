
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,04/31] KVM: arm/arm64: Abstract stage-2 MMU state into a separate structure - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,04/31] KVM: arm/arm64: Abstract stage-2 MMU state into a separate structure</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=174677">Jintack Lim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 3, 2017, 3:10 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1507000273-3735-2-git-send-email-jintack.lim@linaro.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9981587/mbox/"
   >mbox</a>
|
   <a href="/patch/9981587/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9981587/">/patch/9981587/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	3F0C6602B8 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:19:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2DBE320881
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:19:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2229928735; Tue,  3 Oct 2017 03:19:09 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,RCVD_IN_DNSWL_HI,RCVD_IN_SORBS_SPAM
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 88C2320881
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Oct 2017 03:19:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751776AbdJCDLp (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 2 Oct 2017 23:11:45 -0400
Received: from mail-io0-f175.google.com ([209.85.223.175]:49204 &quot;EHLO
	mail-io0-f175.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751478AbdJCDLm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 2 Oct 2017 23:11:42 -0400
Received: by mail-io0-f175.google.com with SMTP id 21so6448095iof.6
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 02 Oct 2017 20:11:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=linaro.org; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=Nr8sDV6nQiJGavOYM7VW9hXtKu1vTPVRjeDWk/MOEEA=;
	b=Ot8v+n40N3LhxU8F7z7UrmY4T/mlfU+QdJrhMGHxJAcMdDfBzSCdIFE6kHwy8TsyjU
	ALUoEtLQI+5XrSBaXe8f/qrss17C7aB1BceqdyooBmVuUFWF1iV49m+COiW+qHNWLAi+
	izsD7qYiB9rc1PFwO+JG9pMHDGq/por8+wkq4=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=Nr8sDV6nQiJGavOYM7VW9hXtKu1vTPVRjeDWk/MOEEA=;
	b=qshTQJR13auI9UOf/WR73xUCcIltG2buhuTrFwGx2jF8K9P40iw7N2C83w4bW093bW
	fE4xlGy732p91gf+Uf1DRw3o2rwwNewmbX5xS2j9DAHmOOgIkd68gKocl+VoiWw1kyMM
	KVfgLZb/3ybt5mw09N6abGT4UB55QDQ1cYtBGW0HPfoE/MA6C4vkEj48TyZ0hoewaDma
	fSPXMcTwmVXa4lXgJ0a/cyHE15D1QDVm9Jh6tEb9CBspzLAgPIgKjB0xIyTaKE6/ZINW
	y+0fTgiL7hBJELR4NZ0HaBkA1SCqa6H9guoCrfujIv1C2v0UUJnjk8DYDXyKNLSGA3al
	vwAg==
X-Gm-Message-State: AMCzsaWyggm1aj1GxmlTXVGFWJO5bPJY34PjIm+1En1LOotBtDX0KXDJ
	dq7EYj+njEbsNF8V28UsPq+VSw==
X-Google-Smtp-Source: AOwi7QAjlCsycDl1Vd0YYu9k+6ikCZdZwv3NR/etmFSWjN3k3wUSL7W7rig0+GCzZIztPyE8Dvwyew==
X-Received: by 10.107.173.34 with SMTP id w34mr27554716ioe.14.1507000300915; 
	Mon, 02 Oct 2017 20:11:40 -0700 (PDT)
Received: from node.jintackl-qv28633.kvmarm-pg0.wisc.cloudlab.us
	(c220g1-031126.wisc.cloudlab.us. [128.104.222.76])
	by smtp.gmail.com with ESMTPSA id
	h84sm5367193iod.72.2017.10.02.20.11.39
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Mon, 02 Oct 2017 20:11:40 -0700 (PDT)
From: Jintack Lim &lt;jintack.lim@linaro.org&gt;
To: christoffer.dall@linaro.org, marc.zyngier@arm.com,
	kvmarm@lists.cs.columbia.edu
Cc: jintack@cs.columbia.edu, pbonzini@redhat.com, rkrcmar@redhat.com,
	catalin.marinas@arm.com, will.deacon@arm.com,
	linux@armlinux.org.uk, mark.rutland@arm.com,
	linux-arm-kernel@lists.infradead.org, kvm@vger.kernel.org,
	linux-kernel@vger.kernel.org, Jintack Lim &lt;jintack.lim@linaro.org&gt;
Subject: [RFC PATCH v2 04/31] KVM: arm/arm64: Abstract stage-2 MMU state
	into a separate structure
Date: Mon,  2 Oct 2017 22:10:46 -0500
Message-Id: &lt;1507000273-3735-2-git-send-email-jintack.lim@linaro.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1507000273-3735-1-git-send-email-jintack.lim@linaro.org&gt;
References: &lt;1507000273-3735-1-git-send-email-jintack.lim@linaro.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174677">Jintack Lim</a> - Oct. 3, 2017, 3:10 a.m.</div>
<pre class="content">
<span class="from">From: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>

Abstract stage-2 MMU state into a separate structure and change all
callers referring to page tables, VMIDs, and the VTTBR to use this new
indirection.

This is about to become very handy when using shadow stage-2 page
tables.
<span class="signed-off-by">
Signed-off-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Jintack Lim &lt;jintack.lim@linaro.org&gt;</span>
---
 arch/arm/include/asm/kvm_asm.h    |   7 +-
 arch/arm/include/asm/kvm_host.h   |  26 +++++---
 arch/arm/kvm/hyp/switch.c         |   5 +-
 arch/arm/kvm/hyp/tlb.c            |  18 ++---
 arch/arm64/include/asm/kvm_asm.h  |   7 +-
 arch/arm64/include/asm/kvm_host.h |  10 ++-
 arch/arm64/kvm/hyp/switch.c       |   5 +-
 arch/arm64/kvm/hyp/tlb.c          |  38 +++++------
 virt/kvm/arm/arm.c                |  34 +++++-----
 virt/kvm/arm/mmu.c                | 137 +++++++++++++++++++++-----------------
 10 files changed, 163 insertions(+), 124 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_asm.h b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">index 14d68a4..71b7255 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -57,6 +57,7 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;
<span class="p_add">+struct kvm_s2_mmu;</span>
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
<span class="p_chunk">@@ -64,9 +65,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm *kvm);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">index 7e9e6c8..78d826e 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -53,9 +53,21 @@</span> <span class="p_context"></span>
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 void kvm_reset_coprocs(struct kvm_vcpu *vcpu);
 
<span class="p_del">-struct kvm_arch {</span>
<span class="p_del">-	/* VTTBR value associated with below pgd and vmid */</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
<span class="p_add">+	/* The VMID generation used for the virt. memory system */</span>
<span class="p_add">+	u64    vmid_gen;</span>
<span class="p_add">+	u32    vmid;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Stage-2 page table */</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* VTTBR value associated with above pgd and vmid */</span>
 	u64    vttbr;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_arch {</span>
<span class="p_add">+	/* Stage 2 paging state for the VM */</span>
<span class="p_add">+	struct kvm_s2_mmu mmu;</span>
 
 	/* The last vcpu id that ran on each physical CPU */
 	int __percpu *last_vcpu_ran;
<span class="p_chunk">@@ -65,13 +77,6 @@</span> <span class="p_context"> struct kvm_arch {</span>
 	 * here.
 	 */
 
<span class="p_del">-	/* The VMID generation used for the virt. memory system */</span>
<span class="p_del">-	u64    vmid_gen;</span>
<span class="p_del">-	u32    vmid;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Stage-2 page table */</span>
<span class="p_del">-	pgd_t *pgd;</span>
<span class="p_del">-</span>
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
 	int max_vcpus;
<span class="p_chunk">@@ -185,6 +190,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Detect first run of a vcpu */
 	bool has_run_once;
<span class="p_add">+</span>
<span class="p_add">+	/* Stage 2 paging state used by the hardware on next switch */</span>
<span class="p_add">+	struct kvm_s2_mmu *hw_mmu;</span>
 };
 
 struct kvm_vm_stat {
<span class="p_header">diff --git a/arch/arm/kvm/hyp/switch.c b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">index ebd2dd4..4814671 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -75,8 +75,9 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(vcpu-&gt;kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	write_sysreg(vcpu-&gt;arch.midr, VPIDR);
 }
 
<span class="p_header">diff --git a/arch/arm/kvm/hyp/tlb.c b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">index 6d810af..56f0a49 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -34,13 +34,13 @@</span> <span class="p_context"></span>
  * As v7 does not support flushing per IPA, just nuke the whole TLB
  * instead, ignoring the ipa value.
  */
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALLIS);
<span class="p_chunk">@@ -50,17 +50,17 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
 	write_sysreg(0, VTTBR);
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+					 phys_addr_t ipa)</span>
 {
<span class="p_del">-	__kvm_tlb_flush_vmid(kvm);</span>
<span class="p_add">+	__kvm_tlb_flush_vmid(mmu);</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(kern_hyp_va(vcpu)-&gt;kvm);</span>
<span class="p_del">-</span>
 	/* Switch to requested VMID */
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALL);
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">index 26a64d0..ff6244f 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;
<span class="p_add">+struct kvm_s2_mmu;</span>
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
<span class="p_chunk">@@ -51,9 +52,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm *kvm);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">index 373235c..e7e9f70 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -50,7 +50,7 @@</span> <span class="p_context"></span>
 int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
<span class="p_del">-struct kvm_arch {</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
 	/* The VMID generation used for the virt. memory system */
 	u64    vmid_gen;
 	u32    vmid;
<span class="p_chunk">@@ -61,6 +61,11 @@</span> <span class="p_context"> struct kvm_arch {</span>
 
 	/* VTTBR value associated with above pgd and vmid */
 	u64    vttbr;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_arch {</span>
<span class="p_add">+	/* Stage 2 paging state for the VM */</span>
<span class="p_add">+	struct kvm_s2_mmu mmu;</span>
 
 	/* The last vcpu id that ran on each physical CPU */
 	int __percpu *last_vcpu_ran;
<span class="p_chunk">@@ -329,6 +334,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Detect first run of a vcpu */
 	bool has_run_once;
<span class="p_add">+</span>
<span class="p_add">+	/* Stage 2 paging state used by the hardware on next switch */</span>
<span class="p_add">+	struct kvm_s2_mmu *hw_mmu;</span>
 };
 
 #define vcpu_gp_regs(v)		(&amp;(v)-&gt;arch.ctxt.gp_regs)
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">index 2a64a5c..8b1b3e9 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -181,8 +181,9 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(vcpu-&gt;kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 }
 
 static void __hyp_text __deactivate_vm(struct kvm_vcpu *vcpu)
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">index 73464a9..0897678 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/kvm_hyp.h&gt;
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm *kvm)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm_s2_mmu *mmu)</span>
 {
 	u64 val;
 
<span class="p_chunk">@@ -29,16 +29,16 @@</span> <span class="p_context"> static void __hyp_text __tlb_switch_to_guest_vhe(struct kvm *kvm)</span>
 	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so
 	 * let&#39;s flip TGE before executing the TLB operation.
 	 */
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 	val = read_sysreg(hcr_el2);
 	val &amp;= ~HCR_TGE;
 	write_sysreg(val, hcr_el2);
 	isb();
 }
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm *kvm)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_guest_nvhe(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 	isb();
 }
 
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> static hyp_alternate_select(__tlb_switch_to_guest,</span>
 			    __tlb_switch_to_guest_vhe,
 			    ARM64_HAS_VIRT_HOST_EXTN);
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_host_vhe(struct kvm *kvm)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_host_vhe(struct kvm_s2_mmu *mmu)</span>
 {
 	/*
 	 * We&#39;re done with the TLB operation, let&#39;s restore the host&#39;s
<span class="p_chunk">@@ -57,7 +57,7 @@</span> <span class="p_context"> static void __hyp_text __tlb_switch_to_host_vhe(struct kvm *kvm)</span>
 	write_sysreg(HCR_HOST_VHE_FLAGS, hcr_el2);
 }
 
<span class="p_del">-static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm *kvm)</span>
<span class="p_add">+static void __hyp_text __tlb_switch_to_host_nvhe(struct kvm_s2_mmu *mmu)</span>
 {
 	write_sysreg(0, vttbr_el2);
 }
<span class="p_chunk">@@ -67,13 +67,14 @@</span> <span class="p_context"> static hyp_alternate_select(__tlb_switch_to_host,</span>
 			    __tlb_switch_to_host_vhe,
 			    ARM64_HAS_VIRT_HOST_EXTN);
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+					 phys_addr_t ipa)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	__tlb_switch_to_guest()(kvm);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(mmu);</span>
 
 	/*
 	 * We could do so much better if we had the VA as well.
<span class="p_chunk">@@ -116,36 +117,35 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
 	if (!has_vhe() &amp;&amp; icache_is_vpipt())
 		__flush_icache_all();
 
<span class="p_del">-	__tlb_switch_to_host()(kvm);</span>
<span class="p_add">+	__tlb_switch_to_host()(mmu);</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	__tlb_switch_to_guest()(kvm);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(mmu);</span>
 
 	__tlbi(vmalls12e1is);
 	dsb(ish);
 	isb();
 
<span class="p_del">-	__tlb_switch_to_host()(kvm);</span>
<span class="p_add">+	__tlb_switch_to_host()(mmu);</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(kern_hyp_va(vcpu)-&gt;kvm);</span>
<span class="p_del">-</span>
 	/* Switch to requested VMID */
<span class="p_del">-	__tlb_switch_to_guest()(kvm);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	__tlb_switch_to_guest()(mmu);</span>
 
 	__tlbi(vmalle1);
 	dsb(nsh);
 	isb();
 
<span class="p_del">-	__tlb_switch_to_host()(kvm);</span>
<span class="p_add">+	__tlb_switch_to_host()(mmu);</span>
 }
 
 void __hyp_text __kvm_flush_vm_context(void)
<span class="p_header">diff --git a/virt/kvm/arm/arm.c b/virt/kvm/arm/arm.c</span>
<span class="p_header">index 0ff2997..bee27bb 100644</span>
<span class="p_header">--- a/virt/kvm/arm/arm.c</span>
<span class="p_header">+++ b/virt/kvm/arm/arm.c</span>
<span class="p_chunk">@@ -138,7 +138,7 @@</span> <span class="p_context"> int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)</span>
 	kvm_vgic_early_init(kvm);
 
 	/* Mark the initial VMID generation invalid */
<span class="p_del">-	kvm-&gt;arch.vmid_gen = 0;</span>
<span class="p_add">+	kvm-&gt;arch.mmu.vmid_gen = 0;</span>
 
 	/* The maximum number of VCPUs is limited by the host&#39;s GIC model */
 	kvm-&gt;arch.max_vcpus = vgic_present ?
<span class="p_chunk">@@ -334,6 +334,8 @@</span> <span class="p_context"> int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)</span>
 
 	kvm_arm_reset_debug_ptr(vcpu);
 
<span class="p_add">+	vcpu-&gt;arch.hw_mmu = &amp;vcpu-&gt;kvm-&gt;arch.mmu;</span>
<span class="p_add">+</span>
 	return kvm_vgic_vcpu_init(vcpu);
 }
 
<span class="p_chunk">@@ -348,7 +350,7 @@</span> <span class="p_context"> void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	 * over-invalidation doesn&#39;t affect correctness.
 	 */
 	if (*last_ran != vcpu-&gt;vcpu_id) {
<span class="p_del">-		kvm_call_hyp(__kvm_tlb_flush_local_vmid, vcpu);</span>
<span class="p_add">+		kvm_call_hyp(__kvm_tlb_flush_local_vmid, &amp;vcpu-&gt;kvm-&gt;arch.mmu);</span>
 		*last_ran = vcpu-&gt;vcpu_id;
 	}
 
<span class="p_chunk">@@ -442,25 +444,26 @@</span> <span class="p_context"> void force_vm_exit(const cpumask_t *mask)</span>
  * VMID for the new generation, we must flush necessary caches and TLBs on all
  * CPUs.
  */
<span class="p_del">-static bool need_new_vmid_gen(struct kvm *kvm)</span>
<span class="p_add">+static bool need_new_vmid_gen(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	return unlikely(kvm-&gt;arch.vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
<span class="p_add">+	return unlikely(mmu-&gt;vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
 }
 
 /**
  * update_vttbr - Update the VTTBR with a valid VMID before the guest runs
<span class="p_del">- * @kvm	The guest that we are about to run</span>
<span class="p_add">+ * @kvm: The guest that we are about to run</span>
<span class="p_add">+ * @mmu: The stage-2 translation context to update</span>
  *
  * Called from kvm_arch_vcpu_ioctl_run before entering the guest to ensure the
  * VM has a valid VMID, otherwise assigns a new one and flushes corresponding
  * caches and TLBs.
  */
<span class="p_del">-static void update_vttbr(struct kvm *kvm)</span>
<span class="p_add">+static void update_vttbr(struct kvm *kvm, struct kvm_s2_mmu *mmu)</span>
 {
 	phys_addr_t pgd_phys;
 	u64 vmid;
 
<span class="p_del">-	if (!need_new_vmid_gen(kvm))</span>
<span class="p_add">+	if (!need_new_vmid_gen(mmu))</span>
 		return;
 
 	spin_lock(&amp;kvm_vmid_lock);
<span class="p_chunk">@@ -470,7 +473,7 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm)</span>
 	 * already allocated a valid vmid for this vm, then this vcpu should
 	 * use the same vmid.
 	 */
<span class="p_del">-	if (!need_new_vmid_gen(kvm)) {</span>
<span class="p_add">+	if (!need_new_vmid_gen(mmu)) {</span>
 		spin_unlock(&amp;kvm_vmid_lock);
 		return;
 	}
<span class="p_chunk">@@ -494,16 +497,17 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm)</span>
 		kvm_call_hyp(__kvm_flush_vm_context);
 	}
 
<span class="p_del">-	kvm-&gt;arch.vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_del">-	kvm-&gt;arch.vmid = kvm_next_vmid;</span>
<span class="p_add">+	mmu-&gt;vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_add">+	mmu-&gt;vmid = kvm_next_vmid;</span>
 	kvm_next_vmid++;
 	kvm_next_vmid &amp;= (1 &lt;&lt; kvm_vmid_bits) - 1;
 
 	/* update vttbr to be used with the new vmid */
<span class="p_del">-	pgd_phys = virt_to_phys(kvm-&gt;arch.pgd);</span>
<span class="p_add">+	pgd_phys = virt_to_phys(mmu-&gt;pgd);</span>
 	BUG_ON(pgd_phys &amp; ~VTTBR_BADDR_MASK);
<span class="p_del">-	vmid = ((u64)(kvm-&gt;arch.vmid) &lt;&lt; VTTBR_VMID_SHIFT) &amp; VTTBR_VMID_MASK(kvm_vmid_bits);</span>
<span class="p_del">-	kvm-&gt;arch.vttbr = pgd_phys | vmid;</span>
<span class="p_add">+	vmid = ((u64)(mmu-&gt;vmid) &lt;&lt; VTTBR_VMID_SHIFT) &amp;</span>
<span class="p_add">+	       VTTBR_VMID_MASK(kvm_vmid_bits);</span>
<span class="p_add">+	mmu-&gt;vttbr = pgd_phys | vmid;</span>
 
 	spin_unlock(&amp;kvm_vmid_lock);
 }
<span class="p_chunk">@@ -638,7 +642,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 */
 		cond_resched();
 
<span class="p_del">-		update_vttbr(vcpu-&gt;kvm);</span>
<span class="p_add">+		update_vttbr(vcpu-&gt;kvm, vcpu-&gt;arch.hw_mmu);</span>
 
 		check_vcpu_requests(vcpu);
 
<span class="p_chunk">@@ -677,7 +681,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 */
 		smp_store_mb(vcpu-&gt;mode, IN_GUEST_MODE);
 
<span class="p_del">-		if (ret &lt;= 0 || need_new_vmid_gen(vcpu-&gt;kvm) ||</span>
<span class="p_add">+		if (ret &lt;= 0 || need_new_vmid_gen(vcpu-&gt;arch.hw_mmu) ||</span>
 		    kvm_request_pending(vcpu)) {
 			vcpu-&gt;mode = OUTSIDE_GUEST_MODE;
 			local_irq_enable();
<span class="p_header">diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c</span>
<span class="p_header">index 0a5f5ca..d8ea1f9 100644</span>
<span class="p_header">--- a/virt/kvm/arm/mmu.c</span>
<span class="p_header">+++ b/virt/kvm/arm/mmu.c</span>
<span class="p_chunk">@@ -64,9 +64,9 @@</span> <span class="p_context"> void kvm_flush_remote_tlbs(struct kvm *kvm)</span>
 	kvm_call_hyp(__kvm_tlb_flush_vmid, kvm);
 }
 
<span class="p_del">-static void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+static void kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa)</span>
 {
<span class="p_del">-	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, kvm, ipa);</span>
<span class="p_add">+	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ipa);</span>
 }
 
 /*
<span class="p_chunk">@@ -103,13 +103,14 @@</span> <span class="p_context"> static bool kvm_is_device_pfn(unsigned long pfn)</span>
  * Function clears a PMD entry, flushes addr 1st and 2nd stage TLBs. Marks all
  * pages in the range dirty.
  */
<span class="p_del">-static void stage2_dissolve_pmd(struct kvm *kvm, phys_addr_t addr, pmd_t *pmd)</span>
<span class="p_add">+static void stage2_dissolve_pmd(struct kvm_s2_mmu *mmu, phys_addr_t addr,</span>
<span class="p_add">+				pmd_t *pmd)</span>
 {
 	if (!pmd_thp_or_huge(*pmd))
 		return;
 
 	pmd_clear(pmd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	put_page(virt_to_page(pmd));
 }
 
<span class="p_chunk">@@ -145,31 +146,34 @@</span> <span class="p_context"> static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)</span>
 	return p;
 }
 
<span class="p_del">-static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pgd_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pgd_t *pgd, phys_addr_t addr)</span>
 {
 	pud_t *pud_table __maybe_unused = stage2_pud_offset(pgd, 0UL);
 	stage2_pgd_clear(pgd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	stage2_pud_free(pud_table);
 	put_page(virt_to_page(pgd));
 }
 
<span class="p_del">-static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pud_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pud_t *pud, phys_addr_t addr)</span>
 {
 	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);
 	VM_BUG_ON(stage2_pud_huge(*pud));
 	stage2_pud_clear(pud);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	stage2_pmd_free(pmd_table);
 	put_page(virt_to_page(pud));
 }
 
<span class="p_del">-static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pmd_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pmd_t *pmd, phys_addr_t addr)</span>
 {
 	pte_t *pte_table = pte_offset_kernel(pmd, 0);
 	VM_BUG_ON(pmd_thp_or_huge(*pmd));
 	pmd_clear(pmd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	pte_free_kernel(NULL, pte_table);
 	put_page(virt_to_page(pmd));
 }
<span class="p_chunk">@@ -194,7 +198,7 @@</span> <span class="p_context"> static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr</span>
  * the corresponding TLBs, we call kvm_flush_dcache_p*() to make sure
  * the IO subsystem will never hit in the cache.
  */
<span class="p_del">-static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="p_add">+static void unmap_stage2_ptes(struct kvm_s2_mmu *mmu, pmd_t *pmd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t start_addr = addr;
<span class="p_chunk">@@ -206,7 +210,7 @@</span> <span class="p_context"> static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 			pte_t old_pte = *pte;
 
 			kvm_set_pte(pte, __pte(0));
<span class="p_del">-			kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+			kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 
 			/* No need to invalidate the cache for device mappings */
 			if (!kvm_is_device_pfn(pte_pfn(old_pte)))
<span class="p_chunk">@@ -217,10 +221,10 @@</span> <span class="p_context"> static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
 	if (stage2_pte_table_empty(start_pte))
<span class="p_del">-		clear_stage2_pmd_entry(kvm, pmd, start_addr);</span>
<span class="p_add">+		clear_stage2_pmd_entry(mmu, pmd, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="p_add">+static void unmap_stage2_pmds(struct kvm_s2_mmu *mmu, pud_t *pud,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
<span class="p_chunk">@@ -234,22 +238,22 @@</span> <span class="p_context"> static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
 				pmd_t old_pmd = *pmd;
 
 				pmd_clear(pmd);
<span class="p_del">-				kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+				kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 
 				kvm_flush_dcache_pmd(old_pmd);
 
 				put_page(virt_to_page(pmd));
 			} else {
<span class="p_del">-				unmap_stage2_ptes(kvm, pmd, addr, next);</span>
<span class="p_add">+				unmap_stage2_ptes(mmu, pmd, addr, next);</span>
 			}
 		}
 	} while (pmd++, addr = next, addr != end);
 
 	if (stage2_pmd_table_empty(start_pmd))
<span class="p_del">-		clear_stage2_pud_entry(kvm, pud, start_addr);</span>
<span class="p_add">+		clear_stage2_pud_entry(mmu, pud, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
<span class="p_add">+static void unmap_stage2_puds(struct kvm_s2_mmu *mmu, pgd_t *pgd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
<span class="p_chunk">@@ -263,17 +267,17 @@</span> <span class="p_context"> static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
 				pud_t old_pud = *pud;
 
 				stage2_pud_clear(pud);
<span class="p_del">-				kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+				kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 				kvm_flush_dcache_pud(old_pud);
 				put_page(virt_to_page(pud));
 			} else {
<span class="p_del">-				unmap_stage2_pmds(kvm, pud, addr, next);</span>
<span class="p_add">+				unmap_stage2_pmds(mmu, pud, addr, next);</span>
 			}
 		}
 	} while (pud++, addr = next, addr != end);
 
 	if (stage2_pud_table_empty(start_pud))
<span class="p_del">-		clear_stage2_pgd_entry(kvm, pgd, start_addr);</span>
<span class="p_add">+		clear_stage2_pgd_entry(mmu, pgd, start_addr);</span>
 }
 
 /**
<span class="p_chunk">@@ -292,20 +296,21 @@</span> <span class="p_context"> static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
 	pgd_t *pgd;
 	phys_addr_t addr = start, end = start + size;
 	phys_addr_t next;
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
 
 	assert_spin_locked(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		/*
 		 * Make sure the page table is still active, as another thread
 		 * could have possibly freed the page table, while we released
 		 * the lock.
 		 */
<span class="p_del">-		if (!READ_ONCE(kvm-&gt;arch.pgd))</span>
<span class="p_add">+		if (!READ_ONCE(mmu-&gt;pgd))</span>
 			break;
 		next = stage2_pgd_addr_end(addr, end);
 		if (!stage2_pgd_none(*pgd))
<span class="p_del">-			unmap_stage2_puds(kvm, pgd, addr, next);</span>
<span class="p_add">+			unmap_stage2_puds(mmu, pgd, addr, next);</span>
 		/*
 		 * If the range is too large, release the kvm-&gt;mmu_lock
 		 * to prevent starvation and lockup detector warnings.
<span class="p_chunk">@@ -360,7 +365,7 @@</span> <span class="p_context"> static void stage2_flush_puds(pgd_t *pgd, phys_addr_t addr, phys_addr_t end)</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_del">-static void stage2_flush_memslot(struct kvm *kvm,</span>
<span class="p_add">+static void stage2_flush_memslot(struct kvm_s2_mmu *mmu,</span>
 				 struct kvm_memory_slot *memslot)
 {
 	phys_addr_t addr = memslot-&gt;base_gfn &lt;&lt; PAGE_SHIFT;
<span class="p_chunk">@@ -368,7 +373,7 @@</span> <span class="p_context"> static void stage2_flush_memslot(struct kvm *kvm,</span>
 	phys_addr_t next;
 	pgd_t *pgd;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		next = stage2_pgd_addr_end(addr, end);
 		stage2_flush_puds(pgd, addr, next);
<span class="p_chunk">@@ -393,7 +398,7 @@</span> <span class="p_context"> static void stage2_flush_vm(struct kvm *kvm)</span>
 
 	slots = kvm_memslots(kvm);
 	kvm_for_each_memslot(memslot, slots)
<span class="p_del">-		stage2_flush_memslot(kvm, memslot);</span>
<span class="p_add">+		stage2_flush_memslot(&amp;kvm-&gt;arch.mmu, memslot);</span>
 
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	srcu_read_unlock(&amp;kvm-&gt;srcu, idx);
<span class="p_chunk">@@ -745,8 +750,9 @@</span> <span class="p_context"> int create_hyp_io_mappings(void *from, void *to, phys_addr_t phys_addr)</span>
 int kvm_alloc_stage2_pgd(struct kvm *kvm)
 {
 	pgd_t *pgd;
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
 
<span class="p_del">-	if (kvm-&gt;arch.pgd != NULL) {</span>
<span class="p_add">+	if (mmu-&gt;pgd != NULL) {</span>
 		kvm_err(&quot;kvm_arch already initialized?\n&quot;);
 		return -EINVAL;
 	}
<span class="p_chunk">@@ -756,7 +762,8 @@</span> <span class="p_context"> int kvm_alloc_stage2_pgd(struct kvm *kvm)</span>
 	if (!pgd)
 		return -ENOMEM;
 
<span class="p_del">-	kvm-&gt;arch.pgd = pgd;</span>
<span class="p_add">+	mmu-&gt;pgd = pgd;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -831,19 +838,20 @@</span> <span class="p_context"> void stage2_unmap_vm(struct kvm *kvm)</span>
  * kvm_free_stage2_pgd - free all stage-2 tables
  * @kvm:	The KVM struct pointer for the VM.
  *
<span class="p_del">- * Walks the level-1 page table pointed to by kvm-&gt;arch.pgd and frees all</span>
<span class="p_add">+ * Walks the level-1 page table pointed to by kvm-&gt;arch.mmu.pgd and frees all</span>
  * underlying level-2 and level-3 tables before freeing the actual level-1 table
  * and setting the struct pointer to NULL.
  */
 void kvm_free_stage2_pgd(struct kvm *kvm)
 {
 	void *pgd = NULL;
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	if (kvm-&gt;arch.pgd) {</span>
<span class="p_add">+	if (mmu-&gt;pgd) {</span>
 		unmap_stage2_range(kvm, 0, KVM_PHYS_SIZE);
<span class="p_del">-		pgd = READ_ONCE(kvm-&gt;arch.pgd);</span>
<span class="p_del">-		kvm-&gt;arch.pgd = NULL;</span>
<span class="p_add">+		pgd = READ_ONCE(mmu-&gt;pgd);</span>
<span class="p_add">+		mmu-&gt;pgd = NULL;</span>
 	}
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 
<span class="p_chunk">@@ -852,13 +860,14 @@</span> <span class="p_context"> void kvm_free_stage2_pgd(struct kvm *kvm)</span>
 		free_pages_exact(pgd, S2_PGD_SIZE);
 }
 
<span class="p_del">-static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static pud_t *stage2_get_pud(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			     struct kvm_mmu_memory_cache *cache,</span>
 			     phys_addr_t addr)
 {
 	pgd_t *pgd;
 	pud_t *pud;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	if (WARN_ON(stage2_pgd_none(*pgd))) {
 		if (!cache)
 			return NULL;
<span class="p_chunk">@@ -870,13 +879,14 @@</span> <span class="p_context"> static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
 	return stage2_pud_offset(pgd, addr);
 }
 
<span class="p_del">-static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static pmd_t *stage2_get_pmd(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			     struct kvm_mmu_memory_cache *cache,</span>
 			     phys_addr_t addr)
 {
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_del">-	pud = stage2_get_pud(kvm, cache, addr);</span>
<span class="p_add">+	pud = stage2_get_pud(mmu, cache, addr);</span>
 	if (!pud)
 		return NULL;
 
<span class="p_chunk">@@ -891,12 +901,13 @@</span> <span class="p_context"> static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
 	return stage2_pmd_offset(pud, addr);
 }
 
<span class="p_del">-static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
<span class="p_add">+static int stage2_set_pmd_huge(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			       struct kvm_mmu_memory_cache</span>
 			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
 {
 	pmd_t *pmd, old_pmd;
 
<span class="p_del">-	pmd = stage2_get_pmd(kvm, cache, addr);</span>
<span class="p_add">+	pmd = stage2_get_pmd(mmu, cache, addr);</span>
 	VM_BUG_ON(!pmd);
 
 	/*
<span class="p_chunk">@@ -913,7 +924,7 @@</span> <span class="p_context"> static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
 	old_pmd = *pmd;
 	if (pmd_present(old_pmd)) {
 		pmd_clear(pmd);
<span class="p_del">-		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+		kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	} else {
 		get_page(virt_to_page(pmd));
 	}
<span class="p_chunk">@@ -922,7 +933,8 @@</span> <span class="p_context"> static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
 	return 0;
 }
 
<span class="p_del">-static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static int stage2_set_pte(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			  struct kvm_mmu_memory_cache *cache,</span>
 			  phys_addr_t addr, const pte_t *new_pte,
 			  unsigned long flags)
 {
<span class="p_chunk">@@ -934,7 +946,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	VM_BUG_ON(logging_active &amp;&amp; !cache);
 
 	/* Create stage-2 page table mapping - Levels 0 and 1 */
<span class="p_del">-	pmd = stage2_get_pmd(kvm, cache, addr);</span>
<span class="p_add">+	pmd = stage2_get_pmd(mmu, cache, addr);</span>
 	if (!pmd) {
 		/*
 		 * Ignore calls from kvm_set_spte_hva for unallocated
<span class="p_chunk">@@ -948,7 +960,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	 * allocate page.
 	 */
 	if (logging_active)
<span class="p_del">-		stage2_dissolve_pmd(kvm, addr, pmd);</span>
<span class="p_add">+		stage2_dissolve_pmd(mmu, addr, pmd);</span>
 
 	/* Create stage-2 page mappings - Level 2 */
 	if (pmd_none(*pmd)) {
<span class="p_chunk">@@ -968,7 +980,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	old_pte = *pte;
 	if (pte_present(old_pte)) {
 		kvm_set_pte(pte, __pte(0));
<span class="p_del">-		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+		kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	} else {
 		get_page(virt_to_page(pte));
 	}
<span class="p_chunk">@@ -1028,7 +1040,7 @@</span> <span class="p_context"> int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,</span>
 		if (ret)
 			goto out;
 		spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-		ret = stage2_set_pte(kvm, &amp;cache, addr, &amp;pte,</span>
<span class="p_add">+		ret = stage2_set_pte(&amp;kvm-&gt;arch.mmu, &amp;cache, addr, &amp;pte,</span>
 						KVM_S2PTE_FLAG_IS_IOMAP);
 		spin_unlock(&amp;kvm-&gt;mmu_lock);
 		if (ret)
<span class="p_chunk">@@ -1166,12 +1178,13 @@</span> <span class="p_context"> static void  stage2_wp_puds(pgd_t *pgd, phys_addr_t addr, phys_addr_t end)</span>
  * @addr:	Start address of range
  * @end:	End address of range
  */
<span class="p_del">-static void stage2_wp_range(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)</span>
<span class="p_add">+static void stage2_wp_range(struct kvm *kvm, struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			    phys_addr_t addr, phys_addr_t end)</span>
 {
 	pgd_t *pgd;
 	phys_addr_t next;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		/*
 		 * Release kvm_mmu_lock periodically if the memory region is
<span class="p_chunk">@@ -1183,7 +1196,7 @@</span> <span class="p_context"> static void stage2_wp_range(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)</span>
 		 * the lock.
 		 */
 		cond_resched_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-		if (!READ_ONCE(kvm-&gt;arch.pgd))</span>
<span class="p_add">+		if (!READ_ONCE(mmu-&gt;pgd))</span>
 			break;
 		next = stage2_pgd_addr_end(addr, end);
 		if (stage2_pgd_present(*pgd))
<span class="p_chunk">@@ -1212,7 +1225,7 @@</span> <span class="p_context"> void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot)</span>
 	phys_addr_t end = (memslot-&gt;base_gfn + memslot-&gt;npages) &lt;&lt; PAGE_SHIFT;
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	stage2_wp_range(kvm, start, end);</span>
<span class="p_add">+	stage2_wp_range(kvm, &amp;kvm-&gt;arch.mmu, start, end);</span>
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	kvm_flush_remote_tlbs(kvm);
 }
<span class="p_chunk">@@ -1236,7 +1249,7 @@</span> <span class="p_context"> static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,</span>
 	phys_addr_t start = (base_gfn +  __ffs(mask)) &lt;&lt; PAGE_SHIFT;
 	phys_addr_t end = (base_gfn + __fls(mask) + 1) &lt;&lt; PAGE_SHIFT;
 
<span class="p_del">-	stage2_wp_range(kvm, start, end);</span>
<span class="p_add">+	stage2_wp_range(kvm, &amp;kvm-&gt;arch.mmu, start, end);</span>
 }
 
 /*
<span class="p_chunk">@@ -1292,6 +1305,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 	pgprot_t mem_type = PAGE_S2;
 	bool logging_active = memslot_is_logging(memslot);
 	unsigned long flags = 0;
<span class="p_add">+	struct kvm_s2_mmu *mmu = vcpu-&gt;arch.hw_mmu;</span>
 
 	write_fault = kvm_is_write_fault(vcpu);
 	if (fault_status == FSC_PERM &amp;&amp; !write_fault) {
<span class="p_chunk">@@ -1388,7 +1402,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 			kvm_set_pfn_dirty(pfn);
 		}
 		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE);
<span class="p_del">-		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &amp;new_pmd);</span>
<span class="p_add">+		ret = stage2_set_pmd_huge(mmu, memcache, fault_ipa, &amp;new_pmd);</span>
 	} else {
 		pte_t new_pte = pfn_pte(pfn, mem_type);
 
<span class="p_chunk">@@ -1398,7 +1412,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 			mark_page_dirty(kvm, gfn);
 		}
 		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE);
<span class="p_del">-		ret = stage2_set_pte(kvm, memcache, fault_ipa, &amp;new_pte, flags);</span>
<span class="p_add">+		ret = stage2_set_pte(mmu, memcache, fault_ipa, &amp;new_pte, flags);</span>
 	}
 
 out_unlock:
<span class="p_chunk">@@ -1426,7 +1440,7 @@</span> <span class="p_context"> static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
 
 	spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
 
<span class="p_del">-	pmd = stage2_get_pmd(vcpu-&gt;kvm, NULL, fault_ipa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(vcpu-&gt;arch.hw_mmu, NULL, fault_ipa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		goto out;
 
<span class="p_chunk">@@ -1594,7 +1608,7 @@</span> <span class="p_context"> int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)</span>
 {
 	unsigned long end = hva + PAGE_SIZE;
 
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return 0;
 
 	trace_kvm_unmap_hva(hva);
<span class="p_chunk">@@ -1605,7 +1619,7 @@</span> <span class="p_context"> int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)</span>
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end)
 {
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return 0;
 
 	trace_kvm_unmap_hva_range(start, end);
<span class="p_chunk">@@ -1625,7 +1639,7 @@</span> <span class="p_context"> static int kvm_set_spte_handler(struct kvm *kvm, gpa_t gpa, u64 size, void *data</span>
 	 * therefore stage2_set_pte() never needs to clear out a huge PMD
 	 * through this calling path.
 	 */
<span class="p_del">-	stage2_set_pte(kvm, NULL, gpa, pte, 0);</span>
<span class="p_add">+	stage2_set_pte(&amp;kvm-&gt;arch.mmu, NULL, gpa, pte, 0);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1635,7 +1649,7 @@</span> <span class="p_context"> void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)</span>
 	unsigned long end = hva + PAGE_SIZE;
 	pte_t stage2_pte;
 
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return;
 
 	trace_kvm_set_spte_hva(hva);
<span class="p_chunk">@@ -1649,7 +1663,7 @@</span> <span class="p_context"> static int kvm_age_hva_handler(struct kvm *kvm, gpa_t gpa, u64 size, void *data)</span>
 	pte_t *pte;
 
 	WARN_ON(size != PAGE_SIZE &amp;&amp; size != PMD_SIZE);
<span class="p_del">-	pmd = stage2_get_pmd(kvm, NULL, gpa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(&amp;kvm-&gt;arch.mmu, NULL, gpa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		return 0;
 
<span class="p_chunk">@@ -1669,7 +1683,7 @@</span> <span class="p_context"> static int kvm_test_age_hva_handler(struct kvm *kvm, gpa_t gpa, u64 size, void *</span>
 	pte_t *pte;
 
 	WARN_ON(size != PAGE_SIZE &amp;&amp; size != PMD_SIZE);
<span class="p_del">-	pmd = stage2_get_pmd(kvm, NULL, gpa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(&amp;kvm-&gt;arch.mmu, NULL, gpa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		return 0;
 
<span class="p_chunk">@@ -1898,9 +1912,10 @@</span> <span class="p_context"> int kvm_arch_prepare_memory_region(struct kvm *kvm,</span>
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
 	if (ret)
<span class="p_del">-		unmap_stage2_range(kvm, mem-&gt;guest_phys_addr, mem-&gt;memory_size);</span>
<span class="p_add">+		unmap_stage2_range(kvm, mem-&gt;guest_phys_addr,</span>
<span class="p_add">+				   mem-&gt;memory_size);</span>
 	else
<span class="p_del">-		stage2_flush_memslot(kvm, memslot);</span>
<span class="p_add">+		stage2_flush_memslot(&amp;kvm-&gt;arch.mmu, memslot);</span>
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 out:
 	up_read(&amp;current-&gt;mm-&gt;mmap_sem);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



