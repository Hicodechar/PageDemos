
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2] Linux VM workaround for Knights Landing A/D leak - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2] Linux VM workaround for Knights Landing A/D leak</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 14, 2016, 5:01 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465923672-14232-1-git-send-email-lukasz.anaczkowski@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9176307/mbox/"
   >mbox</a>
|
   <a href="/patch/9176307/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9176307/">/patch/9176307/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D2B3F6021C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 17:01:43 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C2847265F9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 17:01:43 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B744828047; Tue, 14 Jun 2016 17:01:43 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1166B2793B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Jun 2016 17:01:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752817AbcFNRB1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Jun 2016 13:01:27 -0400
Received: from mga14.intel.com ([192.55.52.115]:63067 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751932AbcFNRBZ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Jun 2016 13:01:25 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by fmsmga103.fm.intel.com with ESMTP; 14 Jun 2016 10:01:26 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.26,471,1459839600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;987332586&quot;
Received: from gklab-125-033.igk.intel.com ([10.91.125.33])
	by fmsmga001.fm.intel.com with ESMTP; 14 Jun 2016 10:01:21 -0700
From: Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	tglx@linutronix.de, mingo@redhat.com, dave.hansen@linux.intel.com,
	ak@linux.intel.com, kirill.shutemov@linux.intel.com,
	mhocko@suse.com, akpm@linux-foundation.org, hpa@zytor.com
Cc: lukasz.anaczkowski@intel.com, harish.srinivasappa@intel.com,
	lukasz.odzioba@intel.com, grzegorz.andrejczuk@intel.com,
	lukasz.daniluk@intel.com
Subject: [PATCH v2] Linux VM workaround for Knights Landing A/D leak
Date: Tue, 14 Jun 2016 19:01:12 +0200
Message-Id: &lt;1465923672-14232-1-git-send-email-lukasz.anaczkowski@intel.com&gt;
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: &lt;7FB15233-B347-4A87-9506-A9E10D331292@gmail.com&gt;
References: &lt;7FB15233-B347-4A87-9506-A9E10D331292@gmail.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 14, 2016, 5:01 p.m.</div>
<pre class="content">
<span class="from">From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>

Knights Landing has a issue that a thread setting A or D bits
may not do so atomically against checking the present bit.
A thread which is going to page fault may still set those
bits, even though the present bit was already atomically cleared.

This implies that when the kernel clears present atomically,
some time later the supposed to be zero entry could be corrupted
with stray A or D bits.

Since the PTE could be already used for storing a swap index,
or a NUMA migration index, this cannot be tolerated. Most
of the time the kernel detects the problem, but in some
rare cases it may not.

This patch enforces that the page unmap path in vmscan/direct reclaim
always flushes other CPUs after clearing each page, and also
clears the PTE again after the flush.

For reclaim this brings the performance back to before Mel&#39;s
flushing changes, but for unmap it disables batching.

This makes sure any leaked A/D bits are immediately cleared before the entry
is used for something else.

Any parallel faults that check for entry is zero may loop,
but they should eventually recover after the entry is written.

Also other users may spin in the page table lock until we
&quot;fixed&quot; the PTE. This is ensured by always taking the page table lock
even for the swap cache case. Previously this was only done
on architectures with non atomic PTE accesses (such as 32bit PTE),
but now it is also done when this bug workaround is active.

I audited apply_pte_range and other users of arch_enter_lazy...
and they seem to all not clear the present bit.

Right now the extra flush is done in the low level
architecture code, while the higher level code still
does batched TLB flush. This means there is always one extra
unnecessary TLB flush now. As a followon optimization
this could be avoided by telling the callers that
the flush already happenend.

v2 (Lukasz Anaczkowski):
    () added call to smp_mb__after_atomic() to synchornize with
       switch_mm, based on Nadav&#39;s comment
    () fixed compilation breakage
<span class="signed-off-by">
Signed-off-by: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt;</span>
---
 arch/x86/include/asm/cpufeatures.h |  1 +
 arch/x86/include/asm/hugetlb.h     |  9 ++++++++-
 arch/x86/include/asm/pgtable.h     |  5 +++++
 arch/x86/include/asm/pgtable_64.h  |  6 ++++++
 arch/x86/kernel/cpu/intel.c        | 10 ++++++++++
 arch/x86/mm/tlb.c                  | 22 ++++++++++++++++++++++
 include/linux/mm.h                 |  4 ++++
 mm/memory.c                        |  3 ++-
 8 files changed, 58 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 14, 2016, 5:24 p.m.</div>
<pre class="content">
On 06/14/2016 10:01 AM, Lukasz Anaczkowski wrote:
<span class="quote">&gt; v2 (Lukasz Anaczkowski):</span>
<span class="quote">&gt;     () fixed compilation breakage</span>
...

By unconditionally defining the workaround code, even on kernels where
there is no chance of ever hitting this bug.  I think that&#39;s a pretty
poor way to do it.

Can we please stick this in one of the intel.c files, so it&#39;s only
present on CPU_SUP_INTEL builds?

Which reminds me...
<span class="quote">
&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -178,6 +178,12 @@ extern void cleanup_highmap(void);</span>
<span class="quote">&gt;  extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="quote">&gt; +static inline bool arch_needs_swap_ptl(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="quote">&gt; +}</span>

Does this *REALLY* only affect 64-bit kernels?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 14, 2016, 6:10 p.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 07:01:12PM +0200, Lukasz Anaczkowski wrote:
<span class="quote">&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Knights Landing has a issue that a thread setting A or D bits</span>
<span class="quote">&gt; may not do so atomically against checking the present bit.</span>
<span class="quote">&gt; A thread which is going to page fault may still set those</span>
<span class="quote">&gt; bits, even though the present bit was already atomically cleared.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This implies that when the kernel clears present atomically,</span>
<span class="quote">&gt; some time later the supposed to be zero entry could be corrupted</span>
<span class="quote">&gt; with stray A or D bits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since the PTE could be already used for storing a swap index,</span>
<span class="quote">&gt; or a NUMA migration index, this cannot be tolerated. Most</span>
<span class="quote">&gt; of the time the kernel detects the problem, but in some</span>
<span class="quote">&gt; rare cases it may not.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch enforces that the page unmap path in vmscan/direct reclaim</span>
<span class="quote">&gt; always flushes other CPUs after clearing each page, and also</span>
<span class="quote">&gt; clears the PTE again after the flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For reclaim this brings the performance back to before Mel&#39;s</span>
<span class="quote">&gt; flushing changes, but for unmap it disables batching.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This makes sure any leaked A/D bits are immediately cleared before the entry</span>
<span class="quote">&gt; is used for something else.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Any parallel faults that check for entry is zero may loop,</span>
<span class="quote">&gt; but they should eventually recover after the entry is written.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also other users may spin in the page table lock until we</span>
<span class="quote">&gt; &quot;fixed&quot; the PTE. This is ensured by always taking the page table lock</span>
<span class="quote">&gt; even for the swap cache case. Previously this was only done</span>
<span class="quote">&gt; on architectures with non atomic PTE accesses (such as 32bit PTE),</span>
<span class="quote">&gt; but now it is also done when this bug workaround is active.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I audited apply_pte_range and other users of arch_enter_lazy...</span>
<span class="quote">&gt; and they seem to all not clear the present bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right now the extra flush is done in the low level</span>
<span class="quote">&gt; architecture code, while the higher level code still</span>
<span class="quote">&gt; does batched TLB flush. This means there is always one extra</span>
<span class="quote">&gt; unnecessary TLB flush now. As a followon optimization</span>
<span class="quote">&gt; this could be avoided by telling the callers that</span>
<span class="quote">&gt; the flush already happenend.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v2 (Lukasz Anaczkowski):</span>
<span class="quote">&gt;     () added call to smp_mb__after_atomic() to synchornize with</span>
<span class="quote">&gt;        switch_mm, based on Nadav&#39;s comment</span>
<span class="quote">&gt;     () fixed compilation breakage</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/cpufeatures.h |  1 +</span>
<span class="quote">&gt;  arch/x86/include/asm/hugetlb.h     |  9 ++++++++-</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable.h     |  5 +++++</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h  |  6 ++++++</span>
<span class="quote">&gt;  arch/x86/kernel/cpu/intel.c        | 10 ++++++++++</span>
<span class="quote">&gt;  arch/x86/mm/tlb.c                  | 22 ++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm.h                 |  4 ++++</span>
<span class="quote">&gt;  mm/memory.c                        |  3 ++-</span>
<span class="quote">&gt;  8 files changed, 58 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="quote">&gt; index 4a41348..2c48011 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="quote">&gt; @@ -303,6 +303,7 @@</span>
<span class="quote">&gt;  #define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */</span>
<span class="quote">&gt;  #define X86_BUG_NULL_SEG	X86_BUG(9) /* Nulling a selector preserves the base */</span>
<span class="quote">&gt;  #define X86_BUG_SWAPGS_FENCE	X86_BUG(10) /* SWAPGS without input dep on GS */</span>
<span class="quote">&gt; +#define X86_BUG_PTE_LEAK        X86_BUG(11) /* PTE may leak A/D bits after clear */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_X86_32</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/hugetlb.h b/arch/x86/include/asm/hugetlb.h</span>
<span class="quote">&gt; index 3a10616..58e1ca9 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/hugetlb.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/hugetlb.h</span>
<span class="quote">&gt; @@ -41,10 +41,17 @@ static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  	set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +			 pte_t *ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,</span>
<span class="quote">&gt;  					    unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>

static_cpu_has_bug()
<span class="quote">
&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; index 1a27396..9769355 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable.h</span>
<span class="quote">&gt; @@ -794,11 +794,16 @@ extern int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  extern int ptep_clear_flush_young(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				  unsigned long address, pte_t *ptep);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +			 pte_t *ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define __HAVE_ARCH_PTEP_GET_AND_CLEAR</span>
<span class="quote">&gt;  static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  				       pte_t *ptep)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t pte = native_ptep_get_and_clear(ptep);</span>
<span class="quote">&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>

static_cpu_has_bug()
<span class="quote">
&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt;  	pte_update(mm, addr, ptep);</span>
<span class="quote">&gt;  	return pte;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index 2ee7811..6fa4079 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -178,6 +178,12 @@ extern void cleanup_highmap(void);</span>
<span class="quote">&gt;  extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="quote">&gt; +static inline bool arch_needs_swap_ptl(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>

static_cpu_has_bug()
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #endif /* !__ASSEMBLY__ */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* _ASM_X86_PGTABLE_64_H */</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; index 6e2ffbe..f499513 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="quote">&gt; @@ -181,6 +181,16 @@ static void early_init_intel(struct cpuinfo_x86 *c)</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (c-&gt;x86_model == 87) {</span>

That should be INTEL_FAM6_XEON_PHI_KNL, AFAICT.
<span class="quote">
&gt; +		static bool printed;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!printed) {</span>
<span class="quote">&gt; +			pr_info(&quot;Enabling PTE leaking workaround\n&quot;);</span>
<span class="quote">&gt; +			printed = true;</span>
<span class="quote">&gt; +		}</span>

pr_info_once
<span class="quote">
&gt; +		set_cpu_bug(c, X86_BUG_PTE_LEAK);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Intel Quark Core DevMan_001.pdf section 6.4.11</span>
<span class="quote">&gt;  	 * &quot;The operating system also is required to invalidate (i.e., flush)</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80821">One Thousand Gnomes</a> - June 14, 2016, 6:34 p.m.</div>
<pre class="content">
On Tue, 14 Jun 2016 10:24:16 -0700
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 06/14/2016 10:01 AM, Lukasz Anaczkowski wrote:</span>
<span class="quote">&gt; &gt; v2 (Lukasz Anaczkowski):</span>
<span class="quote">&gt; &gt;     () fixed compilation breakage  </span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; By unconditionally defining the workaround code, even on kernels where</span>
<span class="quote">&gt; there is no chance of ever hitting this bug.  I think that&#39;s a pretty</span>
<span class="quote">&gt; poor way to do it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we please stick this in one of the intel.c files, so it&#39;s only</span>
<span class="quote">&gt; present on CPU_SUP_INTEL builds?</span>

Can we please make it use alternatives or somesuch so that it just goes
away at boot if its not a Knights Landing box ?

Alan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 14, 2016, 6:38 p.m.</div>
<pre class="content">
Lukasz Anaczkowski &lt;lukasz.anaczkowski@intel.com&gt; wrote:
<span class="quote">
&gt; From: Andi Kleen &lt;ak@linux.intel.com&gt;</span>
<span class="quote">
&gt; static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,</span>
<span class="quote">&gt; 					    unsigned long addr, pte_t *ptep)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; -	return ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt; }</span>

I missed it on the previous iteration: ptep_get_and_clear already calls 
fix_pte_leak when needed. So do you need to call it again here?

Thanks,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 14, 2016, 6:54 p.m.</div>
<pre class="content">
On 06/14/2016 11:34 AM, One Thousand Gnomes wrote:
<span class="quote">&gt; On Tue, 14 Jun 2016 10:24:16 -0700</span>
<span class="quote">&gt; Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On 06/14/2016 10:01 AM, Lukasz Anaczkowski wrote:</span>
<span class="quote">&gt;&gt;&gt; v2 (Lukasz Anaczkowski):</span>
<span class="quote">&gt;&gt;&gt;     () fixed compilation breakage  </span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; By unconditionally defining the workaround code, even on kernels where</span>
<span class="quote">&gt;&gt; there is no chance of ever hitting this bug.  I think that&#39;s a pretty</span>
<span class="quote">&gt;&gt; poor way to do it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Can we please stick this in one of the intel.c files, so it&#39;s only</span>
<span class="quote">&gt;&gt; present on CPU_SUP_INTEL builds?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we please make it use alternatives or somesuch so that it just goes</span>
<span class="quote">&gt; away at boot if its not a Knights Landing box ?</span>

Lukasz, Borislav suggested using static_cpu_has_bug(), which will do the
alternatives patching.  It&#39;s definitely the right thing to use here.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 14, 2016, 7:19 p.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 11:54:24AM -0700, Dave Hansen wrote:
<span class="quote">&gt; Lukasz, Borislav suggested using static_cpu_has_bug(), which will do the</span>
<span class="quote">&gt; alternatives patching.  It&#39;s definitely the right thing to use here.</span>

Yeah, either that or do an

alternative_call(null_func, fix_pte_peak, X86_BUG_PTE_LEAK, ...)

or so and you&#39;ll need a dummy function to call on !X86_BUG_PTE_LEAK
CPUs.

The static_cpu_has_bug() thing should be most likely a penalty
of a single JMP (I have to look at the asm) but then since the
callers are inlined, you&#39;ll have to patch all those places where
*ptep_get_and_clear() get inlined.

Shouldn&#39;t be a big deal still but...

&quot;debug-alternative&quot; and a kvm guest should help you there to get a quick
idea.

HTH.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - June 14, 2016, 8:20 p.m.</div>
<pre class="content">
On 06/14/16 12:19, Borislav Petkov wrote:
<span class="quote">&gt; On Tue, Jun 14, 2016 at 11:54:24AM -0700, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt; Lukasz, Borislav suggested using static_cpu_has_bug(), which will do the</span>
<span class="quote">&gt;&gt; alternatives patching.  It&#39;s definitely the right thing to use here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, either that or do an</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alternative_call(null_func, fix_pte_peak, X86_BUG_PTE_LEAK, ...)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; or so and you&#39;ll need a dummy function to call on !X86_BUG_PTE_LEAK</span>
<span class="quote">&gt; CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The static_cpu_has_bug() thing should be most likely a penalty</span>
<span class="quote">&gt; of a single JMP (I have to look at the asm) but then since the</span>
<span class="quote">&gt; callers are inlined, you&#39;ll have to patch all those places where</span>
<span class="quote">&gt; *ptep_get_and_clear() get inlined.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Shouldn&#39;t be a big deal still but...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;debug-alternative&quot; and a kvm guest should help you there to get a quick</span>
<span class="quote">&gt; idea.</span>
<span class="quote">&gt; </span>

static_cpu_has_bug() should turn into 5-byte NOP in the common (bugless)
case.

	-hpa
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 14, 2016, 8:47 p.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 01:20:06PM -0700, H. Peter Anvin wrote:
<span class="quote">&gt; static_cpu_has_bug() should turn into 5-byte NOP in the common (bugless)</span>
<span class="quote">&gt; case.</span>

Yeah, it does. I looked at the asm.

I wasn&#39;t 100% sure because I vaguely remember gcc reordering things in
some pathological case but I&#39;m most likely remembering wrong because if
it were doing that, then the whole nopping out won&#39;t work. F&#39;get about
it. :)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - June 14, 2016, 8:54 p.m.</div>
<pre class="content">
On 06/14/16 13:47, Borislav Petkov wrote:
<span class="quote">&gt; On Tue, Jun 14, 2016 at 01:20:06PM -0700, H. Peter Anvin wrote:</span>
<span class="quote">&gt;&gt; static_cpu_has_bug() should turn into 5-byte NOP in the common (bugless)</span>
<span class="quote">&gt;&gt; case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, it does. I looked at the asm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I wasn&#39;t 100% sure because I vaguely remember gcc reordering things in</span>
<span class="quote">&gt; some pathological case but I&#39;m most likely remembering wrong because if</span>
<span class="quote">&gt; it were doing that, then the whole nopping out won&#39;t work. F&#39;get about</span>
<span class="quote">&gt; it. :)</span>
<span class="quote">&gt; </span>

There was that.  It is still possible that we end up with NOP a JMP
right before another JMP; we could perhaps make the patching code
smarter and see if we have a JMP immediately after.

	-hpa
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 14, 2016, 9:02 p.m.</div>
<pre class="content">
On Tue, Jun 14, 2016 at 01:54:25PM -0700, H. Peter Anvin wrote:
<span class="quote">&gt; There was that.  It is still possible that we end up with NOP a JMP</span>
<span class="quote">&gt; right before another JMP; we could perhaps make the patching code</span>
<span class="quote">&gt; smarter and see if we have a JMP immediately after.</span>

Yeah, I still can&#39;t get reproduce that reliably - I remember seeing it
at some point but then dismissing it for another, higher-prio thing. And
now the whole memory is hazy at best.

But, you&#39;re giving me a great idea right now - I have this kernel
disassembler tool which dumps alternative sections already and I could
teach it to look for pathological cases around the patching sites and
scream.

Something for my TODO list when I get a quiet moment.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - June 14, 2016, 9:08 p.m.</div>
<pre class="content">
On 06/14/16 14:02, Borislav Petkov wrote:
<span class="quote">&gt; On Tue, Jun 14, 2016 at 01:54:25PM -0700, H. Peter Anvin wrote:</span>
<span class="quote">&gt;&gt; There was that.  It is still possible that we end up with NOP a JMP</span>
<span class="quote">&gt;&gt; right before another JMP; we could perhaps make the patching code</span>
<span class="quote">&gt;&gt; smarter and see if we have a JMP immediately after.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, I still can&#39;t get reproduce that reliably - I remember seeing it</span>
<span class="quote">&gt; at some point but then dismissing it for another, higher-prio thing. And</span>
<span class="quote">&gt; now the whole memory is hazy at best.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But, you&#39;re giving me a great idea right now - I have this kernel</span>
<span class="quote">&gt; disassembler tool which dumps alternative sections already and I could</span>
<span class="quote">&gt; teach it to look for pathological cases around the patching sites and</span>
<span class="quote">&gt; scream.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Something for my TODO list when I get a quiet moment.</span>
<span class="quote">&gt; </span>

It&#39;s not really pathological; the issue is that asm goto() with an
unreachable clause after it doesn&#39;t tell gcc that a certain code path
ought to be linear, so we tell it to fall through.  However, if gcc then
wants to have a jump there for whatever reason (perhaps it is part of a
loop) we end up with a redundant jump, so a patch site followed by a JMP
is entirely reasonable.

	-hpa
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - June 14, 2016, 9:13 p.m.</div>
<pre class="content">
On June 14, 2016 2:02:55 PM PDT, Borislav Petkov &lt;bp@alien8.de&gt; wrote:
<span class="quote">&gt;On Tue, Jun 14, 2016 at 01:54:25PM -0700, H. Peter Anvin wrote:</span>
<span class="quote">&gt;&gt; There was that.  It is still possible that we end up with NOP a JMP</span>
<span class="quote">&gt;&gt; right before another JMP; we could perhaps make the patching code</span>
<span class="quote">&gt;&gt; smarter and see if we have a JMP immediately after.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Yeah, I still can&#39;t get reproduce that reliably - I remember seeing it</span>
<span class="quote">&gt;at some point but then dismissing it for another, higher-prio thing.</span>
<span class="quote">&gt;And</span>
<span class="quote">&gt;now the whole memory is hazy at best.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;But, you&#39;re giving me a great idea right now - I have this kernel</span>
<span class="quote">&gt;disassembler tool which dumps alternative sections already and I could</span>
<span class="quote">&gt;teach it to look for pathological cases around the patching sites and</span>
<span class="quote">&gt;scream.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Something for my TODO list when I get a quiet moment.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Thanks!</span>

We talked with the GCC people about always bias asm goto toward the first label even if followed by __builtin_unreachable().  I don&#39;t know if that happened; if so we should probably insert the unreachable for those versions of gcc only.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 15, 2016, 1:12 p.m.</div>
<pre class="content">
<span class="from">From: Borislav Petkov [mailto:bp@alien8.de] </span>
Sent: Tuesday, June 14, 2016 8:10 PM
<span class="quote">
&gt;&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static_cpu_has_bug()</span>
<span class="quote">
&gt;&gt; +	if (c-&gt;x86_model == 87) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That should be INTEL_FAM6_XEON_PHI_KNL, AFAICT.</span>
<span class="quote">
&gt;&gt; +		static bool printed;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (!printed) {</span>
<span class="quote">&gt;&gt; +			pr_info(&quot;Enabling PTE leaking workaround\n&quot;);</span>
<span class="quote">&gt;&gt; +			printed = true;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; pr_info_once</span>

Thanks, Boris! This is very valuable. I&#39;ll address  those comments in next version of the patch.

Cheers,
Lukasz
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=115261">lukasz.anaczkowski@intel.com</a> - June 15, 2016, 1:12 p.m.</div>
<pre class="content">
<span class="from">From: Nadav Amit [mailto:nadav.amit@gmail.com] </span>
Sent: Tuesday, June 14, 2016 8:38 PM
<span class="quote">
&gt;&gt; +	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt;&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt;&gt; +	return pte;</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I missed it on the previous iteration: ptep_get_and_clear already calls </span>
<span class="quote">&gt; fix_pte_leak when needed. So do you need to call it again here?</span>

You&#39;re right, Nadav. Not needing this. Will be removed in next version of the patch.

Cheers,
Lukasz
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 15, 2016, 8:04 p.m.</div>
<pre class="content">
Lukasz &lt;lukasz.anaczkowski@intel.com&gt; wrote:
<span class="quote">
&gt; From: Nadav Amit [mailto:nadav.amit@gmail.com] </span>
<span class="quote">&gt; Sent: Tuesday, June 14, 2016 8:38 PM</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="quote">&gt;&gt;&gt; +		fix_pte_leak(mm, addr, ptep);</span>
<span class="quote">&gt;&gt;&gt; +	return pte;</span>
<span class="quote">&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I missed it on the previous iteration: ptep_get_and_clear already calls </span>
<span class="quote">&gt;&gt; fix_pte_leak when needed. So do you need to call it again here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You&#39;re right, Nadav. Not needing this. Will be removed in next version of the patch.</span>

Be careful here. According to the SDM when invalidating a huge-page,
each 4KB page needs to be invalidated separately. In practice, when
Linux invalidates 2MB/1GB pages it performs a full TLB flush. The
full flush may not be required on knights landing, and specifically
for the workaround, but you should check.  

Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - June 15, 2016, 8:10 p.m.</div>
<pre class="content">
On 06/15/2016 01:04 PM, Nadav Amit wrote:
<span class="quote">&gt; Be careful here. According to the SDM when invalidating a huge-page,</span>
<span class="quote">&gt; each 4KB page needs to be invalidated separately. In practice, when</span>
<span class="quote">&gt; Linux invalidates 2MB/1GB pages it performs a full TLB flush. The</span>
<span class="quote">&gt; full flush may not be required on knights landing, and specifically</span>
<span class="quote">&gt; for the workaround, but you should check.  </span>

Where do you get that?  The SDM says: &quot;they (TLB invalidation operations
invalidate all TLB entries corresponding to the translation specified by
the paging structures.&quot;

Here&#39;s the full paragraph from the SDM

... some processors may choose to cache multiple smaller-page TLB
entries for a translation specified by the paging structures to use a
page larger than 4 KBytes. There is no way for software to be aware
that multiple translations for smaller pages have been used for a large
page. The INVLPG instruction and page faults provide the same assurances
that they provide when a single TLB entry is used: they invalidate all
TLB entries corresponding to the translation specified by the paging
structures.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - June 15, 2016, 8:26 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 06/15/2016 01:04 PM, Nadav Amit wrote:</span>
<span class="quote">&gt;&gt; Be careful here. According to the SDM when invalidating a huge-page,</span>
<span class="quote">&gt;&gt; each 4KB page needs to be invalidated separately. In practice, when</span>
<span class="quote">&gt;&gt; Linux invalidates 2MB/1GB pages it performs a full TLB flush. The</span>
<span class="quote">&gt;&gt; full flush may not be required on knights landing, and specifically</span>
<span class="quote">&gt;&gt; for the workaround, but you should check.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Where do you get that?  The SDM says: &quot;they (TLB invalidation operations</span>
<span class="quote">&gt; invalidate all TLB entries corresponding to the translation specified by</span>
<span class="quote">&gt; the paging structures.”</span>

You are absolutely correct. Last time I write something based on my
recollection of the SDM without re-reading again. Sorry.

Nadav
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 4a41348..2c48011 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -303,6 +303,7 @@</span> <span class="p_context"></span>
 #define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */
 #define X86_BUG_NULL_SEG	X86_BUG(9) /* Nulling a selector preserves the base */
 #define X86_BUG_SWAPGS_FENCE	X86_BUG(10) /* SWAPGS without input dep on GS */
<span class="p_add">+#define X86_BUG_PTE_LEAK        X86_BUG(11) /* PTE may leak A/D bits after clear */</span>
 
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/hugetlb.h b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">index 3a10616..58e1ca9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hugetlb.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hugetlb.h</span>
<span class="p_chunk">@@ -41,10 +41,17 @@</span> <span class="p_context"> static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 	set_pte_at(mm, addr, ptep, pte);
 }
 
<span class="p_add">+extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+			 pte_t *ptep);</span>
<span class="p_add">+</span>
 static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
 					    unsigned long addr, pte_t *ptep)
 {
<span class="p_del">-	return ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+	pte_t pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+		fix_pte_leak(mm, addr, ptep);</span>
<span class="p_add">+	return pte;</span>
 }
 
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 1a27396..9769355 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -794,11 +794,16 @@</span> <span class="p_context"> extern int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
 extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 				  unsigned long address, pte_t *ptep);
 
<span class="p_add">+extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+			 pte_t *ptep);</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PTEP_GET_AND_CLEAR
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
 	pte_t pte = native_ptep_get_and_clear(ptep);
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+		fix_pte_leak(mm, addr, ptep);</span>
 	pte_update(mm, addr, ptep);
 	return pte;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 2ee7811..6fa4079 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -178,6 +178,12 @@</span> <span class="p_context"> extern void cleanup_highmap(void);</span>
 extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
 extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
 
<span class="p_add">+#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+       return boot_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_PGTABLE_64_H */
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel.c b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">index 6e2ffbe..f499513 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c</span>
<span class="p_chunk">@@ -181,6 +181,16 @@</span> <span class="p_context"> static void early_init_intel(struct cpuinfo_x86 *c)</span>
 		}
 	}
 
<span class="p_add">+	if (c-&gt;x86_model == 87) {</span>
<span class="p_add">+		static bool printed;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!printed) {</span>
<span class="p_add">+			pr_info(&quot;Enabling PTE leaking workaround\n&quot;);</span>
<span class="p_add">+			printed = true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_cpu_bug(c, X86_BUG_PTE_LEAK);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Intel Quark Core DevMan_001.pdf section 6.4.11
 	 * &quot;The operating system also is required to invalidate (i.e., flush)
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 5643fd0..9b4c575 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -469,3 +469,25 @@</span> <span class="p_context"> static int __init create_tlb_single_page_flush_ceiling(void)</span>
 late_initcall(create_tlb_single_page_flush_ceiling);
 
 #endif /* CONFIG_SMP */
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Workaround for KNL issue:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A thread that is going to page fault due to P=0, may still</span>
<span class="p_add">+ * non atomically set A or D bits, which could corrupt swap entries.</span>
<span class="p_add">+ * Always flush the other CPUs and clear the PTE again to avoid</span>
<span class="p_add">+ * this leakage. We are excluded using the pagetable lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_mb__after_atomic();</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);</span>
<span class="p_add">+		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="p_add">+				 addr + PAGE_SIZE);</span>
<span class="p_add">+		mb();</span>
<span class="p_add">+		set_pte(ptep, __pte(0));</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 5df5feb..5c80fe09 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2404,6 +2404,10 @@</span> <span class="p_context"> static inline bool debug_guardpage_enabled(void) { return false; }</span>
 static inline bool page_is_guard(struct page *page) { return false; }
 #endif /* CONFIG_DEBUG_PAGEALLOC */
 
<span class="p_add">+#ifndef ARCH_HAS_NEEDS_SWAP_PTL</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void) { return false; }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #if MAX_NUMNODES &gt; 1
 void __init setup_nr_node_ids(void);
 #else
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 15322b7..0d6ef39 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1960,7 +1960,8 @@</span> <span class="p_context"> static inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,</span>
 {
 	int same = 1;
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
<span class="p_del">-	if (sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
<span class="p_add">+	if (arch_needs_swap_ptl() ||</span>
<span class="p_add">+	    sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
 		spinlock_t *ptl = pte_lockptr(mm, pmd);
 		spin_lock(ptl);
 		same = pte_same(*page_table, orig_pte);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



