
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,06/11] x86/mm: Rework lazy TLB mode and TLB freshness tracking - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,06/11] x86/mm: Rework lazy TLB mode and TLB freshness tracking</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 21, 2017, 5:22 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;70f3a61658aa7c1c89f4db6a4f81d8df9e396ade.1498022414.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9800735/mbox/"
   >mbox</a>
|
   <a href="/patch/9800735/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9800735/">/patch/9800735/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EFD5960234 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 05:23:41 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DECFD28510
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 05:23:41 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D2CC12851A; Wed, 21 Jun 2017 05:23:41 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8A4E228510
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 05:23:40 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753322AbdFUFXb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 21 Jun 2017 01:23:31 -0400
Received: from mail.kernel.org ([198.145.29.99]:55766 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753209AbdFUFWa (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 21 Jun 2017 01:22:30 -0400
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 3A03D23A12;
	Wed, 21 Jun 2017 05:22:24 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 3A03D23A12
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Andrew Banman &lt;abanman@sgi.com&gt;, Mike Travis &lt;travis@sgi.com&gt;,
	Dimitri Sivanich &lt;sivanich@sgi.com&gt;, Juergen Gross &lt;jgross@suse.com&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
Subject: [PATCH v3 06/11] x86/mm: Rework lazy TLB mode and TLB freshness
	tracking
Date: Tue, 20 Jun 2017 22:22:12 -0700
Message-Id: &lt;70f3a61658aa7c1c89f4db6a4f81d8df9e396ade.1498022414.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.9.4
In-Reply-To: &lt;cover.1498022414.git.luto@kernel.org&gt;
References: &lt;cover.1498022414.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1498022414.git.luto@kernel.org&gt;
References: &lt;cover.1498022414.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 21, 2017, 5:22 a.m.</div>
<pre class="content">
x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to
init_mm the first time it tried to flush a lazy TLB.  This meant an
unnecessary CR3 write and, if the flush was remote, an unnecessary
IPI.

Rewrite it entirely.  When we enter lazy mode, we simply remove the
cpu from mm_cpumask.  This means that we need a way to figure out
whether we&#39;ve missed a flush when we switch back out of lazy mode.
I use the tlb_gen machinery to track whether a context is up to
date.

Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m
using an array of length 1 containing (ctx_id, tlb_gen) rather than
just storing tlb_gen, and making it at array isn&#39;t necessary yet.
I&#39;m doing this because the next few patches add PCID support, and,
with PCID, we need ctx_id, and the array will end up with a length
greater than 1.  Making it an array now means that there will be
less churn and therefore less stress on your eyeballs.

NB: This is dubious but, AFAICT, still correct on Xen and UV.
xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this
patch changes the way that mm_cpumask() works.  This should be okay,
since Xen *also* iterates all online CPUs to find all the CPUs it
needs to twiddle.

The UV tlbflush code is rather dated and should be changed.

Cc: Andrew Banman &lt;abanman@sgi.com&gt;
Cc: Mike Travis &lt;travis@sgi.com&gt;
Cc: Dimitri Sivanich &lt;sivanich@sgi.com&gt;
Cc: Juergen Gross &lt;jgross@suse.com&gt;
Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/mmu_context.h |   6 +-
 arch/x86/include/asm/tlbflush.h    |   4 -
 arch/x86/mm/init.c                 |   1 -
 arch/x86/mm/tlb.c                  | 227 +++++++++++++++++++------------------
 arch/x86/xen/mmu_pv.c              |   3 +-
 5 files changed, 119 insertions(+), 122 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - June 21, 2017, 9:01 a.m.</div>
<pre class="content">
On Tue, 20 Jun 2017, Andy Lutomirski wrote:
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * The flush IPI assumes that a thread switch happens in this order:</span>
<span class="quote">&gt; - * [cpu0: the cpu that switches]</span>
<span class="quote">&gt; - * 1) switch_mm() either 1a) or 1b)</span>
<span class="quote">&gt; - * 1a) thread switch to a different mm</span>
<span class="quote">&gt; - * 1a1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="quote">&gt; - *	Now the tlb flush NMI handler flush_tlb_func won&#39;t call leave_mm</span>
<span class="quote">&gt; - *	if cpu0 was in lazy tlb mode.</span>
<span class="quote">&gt; - * 1a2) update cpu active_mm</span>
<span class="quote">&gt; - *	Now cpu0 accepts tlb flushes for the new mm.</span>
<span class="quote">&gt; - * 1a3) cpu_set(cpu, new_mm-&gt;cpu_vm_mask);</span>
<span class="quote">&gt; - *	Now the other cpus will send tlb flush ipis.</span>
<span class="quote">&gt; - * 1a4) change cr3.</span>
<span class="quote">&gt; - * 1a5) cpu_clear(cpu, old_mm-&gt;cpu_vm_mask);</span>
<span class="quote">&gt; - *	Stop ipi delivery for the old mm. This is not synchronized with</span>
<span class="quote">&gt; - *	the other cpus, but flush_tlb_func ignore flush ipis for the wrong</span>
<span class="quote">&gt; - *	mm, and in the worst case we perform a superfluous tlb flush.</span>
<span class="quote">&gt; - * 1b) thread switch without mm change</span>
<span class="quote">&gt; - *	cpu active_mm is correct, cpu0 already handles flush ipis.</span>
<span class="quote">&gt; - * 1b1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="quote">&gt; - * 1b2) test_and_set the cpu bit in cpu_vm_mask.</span>
<span class="quote">&gt; - *	Atomically set the bit [other cpus will start sending flush ipis],</span>
<span class="quote">&gt; - *	and test the bit.</span>
<span class="quote">&gt; - * 1b3) if the bit was 0: leave_mm was called, flush the tlb.</span>
<span class="quote">&gt; - * 2) switch %%esp, ie current</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * The interrupt must handle 2 special cases:</span>
<span class="quote">&gt; - * - cr3 is changed before %%esp, ie. it cannot use current-&gt;{active_,}mm.</span>
<span class="quote">&gt; - * - the cpu performs speculative tlb reads, i.e. even if the cpu only</span>
<span class="quote">&gt; - *   runs in kernel space, the cpu could load tlb entries for user space</span>
<span class="quote">&gt; - *   pages.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * The good news is that cpu_tlbstate is local to each cpu, no</span>
<span class="quote">&gt; - * write/read ordering problems.</span>

While the new code is really well commented, it would be a good thing to
have a single place where all of this including the ordering constraints
are documented.
<span class="quote">
&gt; @@ -215,12 +200,13 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
<span class="quote">&gt;  	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="quote">&gt;  		   loaded_mm-&gt;context.ctx_id);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (this_cpu_read(cpu_tlbstate.state) != TLBSTATE_OK) {</span>
<span class="quote">&gt; +	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -		 * leave_mm() is adequate to handle any type of flush, and</span>
<span class="quote">&gt; -		 * we would prefer not to receive further IPIs.</span>
<span class="quote">&gt; +		 * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="quote">&gt; +		 * remote flushes due to races and on local flushes if a</span>
<span class="quote">&gt; +		 * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="quote">&gt; +		 * still using.</span>

Ok. That&#39;s more informative.
<span class="reviewed-by">
Reviewed-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 21, 2017, 4:04 p.m.</div>
<pre class="content">
On Wed, Jun 21, 2017 at 2:01 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Tue, 20 Jun 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; -/*</span>
<span class="quote">&gt;&gt; - * The flush IPI assumes that a thread switch happens in this order:</span>
<span class="quote">&gt;&gt; - * [cpu0: the cpu that switches]</span>
<span class="quote">&gt;&gt; - * 1) switch_mm() either 1a) or 1b)</span>
<span class="quote">&gt;&gt; - * 1a) thread switch to a different mm</span>
<span class="quote">&gt;&gt; - * 1a1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="quote">&gt;&gt; - *   Now the tlb flush NMI handler flush_tlb_func won&#39;t call leave_mm</span>
<span class="quote">&gt;&gt; - *   if cpu0 was in lazy tlb mode.</span>
<span class="quote">&gt;&gt; - * 1a2) update cpu active_mm</span>
<span class="quote">&gt;&gt; - *   Now cpu0 accepts tlb flushes for the new mm.</span>
<span class="quote">&gt;&gt; - * 1a3) cpu_set(cpu, new_mm-&gt;cpu_vm_mask);</span>
<span class="quote">&gt;&gt; - *   Now the other cpus will send tlb flush ipis.</span>
<span class="quote">&gt;&gt; - * 1a4) change cr3.</span>
<span class="quote">&gt;&gt; - * 1a5) cpu_clear(cpu, old_mm-&gt;cpu_vm_mask);</span>
<span class="quote">&gt;&gt; - *   Stop ipi delivery for the old mm. This is not synchronized with</span>
<span class="quote">&gt;&gt; - *   the other cpus, but flush_tlb_func ignore flush ipis for the wrong</span>
<span class="quote">&gt;&gt; - *   mm, and in the worst case we perform a superfluous tlb flush.</span>
<span class="quote">&gt;&gt; - * 1b) thread switch without mm change</span>
<span class="quote">&gt;&gt; - *   cpu active_mm is correct, cpu0 already handles flush ipis.</span>
<span class="quote">&gt;&gt; - * 1b1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="quote">&gt;&gt; - * 1b2) test_and_set the cpu bit in cpu_vm_mask.</span>
<span class="quote">&gt;&gt; - *   Atomically set the bit [other cpus will start sending flush ipis],</span>
<span class="quote">&gt;&gt; - *   and test the bit.</span>
<span class="quote">&gt;&gt; - * 1b3) if the bit was 0: leave_mm was called, flush the tlb.</span>
<span class="quote">&gt;&gt; - * 2) switch %%esp, ie current</span>
<span class="quote">&gt;&gt; - *</span>
<span class="quote">&gt;&gt; - * The interrupt must handle 2 special cases:</span>
<span class="quote">&gt;&gt; - * - cr3 is changed before %%esp, ie. it cannot use current-&gt;{active_,}mm.</span>
<span class="quote">&gt;&gt; - * - the cpu performs speculative tlb reads, i.e. even if the cpu only</span>
<span class="quote">&gt;&gt; - *   runs in kernel space, the cpu could load tlb entries for user space</span>
<span class="quote">&gt;&gt; - *   pages.</span>
<span class="quote">&gt;&gt; - *</span>
<span class="quote">&gt;&gt; - * The good news is that cpu_tlbstate is local to each cpu, no</span>
<span class="quote">&gt;&gt; - * write/read ordering problems.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; While the new code is really well commented, it would be a good thing to</span>
<span class="quote">&gt; have a single place where all of this including the ordering constraints</span>
<span class="quote">&gt; are documented.</span>

I&#39;ll look at the end of the whole series and see if I can come up with
something good.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; @@ -215,12 +200,13 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
<span class="quote">&gt;&gt;       VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="quote">&gt;&gt;                  loaded_mm-&gt;context.ctx_id);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     if (this_cpu_read(cpu_tlbstate.state) != TLBSTATE_OK) {</span>
<span class="quote">&gt;&gt; +     if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
<span class="quote">&gt;&gt;               /*</span>
<span class="quote">&gt;&gt; -              * leave_mm() is adequate to handle any type of flush, and</span>
<span class="quote">&gt;&gt; -              * we would prefer not to receive further IPIs.</span>
<span class="quote">&gt;&gt; +              * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="quote">&gt;&gt; +              * remote flushes due to races and on local flushes if a</span>
<span class="quote">&gt;&gt; +              * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="quote">&gt;&gt; +              * still using.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok. That&#39;s more informative.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reviewed-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 21, 2017, 5:29 p.m.</div>
<pre class="content">
On Wed, Jun 21, 2017 at 09:04:48AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; I&#39;ll look at the end of the whole series and see if I can come up with</span>
<span class="quote">&gt; something good.</span>

... along with the logic what we flush when, please. I.e., the text in
struct flush_tlb_info.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 22, 2017, 2:50 p.m.</div>
<pre class="content">
On Tue, Jun 20, 2017 at 10:22:12PM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; x86&#39;s lazy TLB mode used to be fairly weak -- it would switch to</span>
<span class="quote">&gt; init_mm the first time it tried to flush a lazy TLB.  This meant an</span>
<span class="quote">&gt; unnecessary CR3 write and, if the flush was remote, an unnecessary</span>
<span class="quote">&gt; IPI.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>

s/cpu/CPU/
<span class="quote">
&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt; date.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt; needs to twiddle.</span>

This whole text should be under the &quot;---&quot; line below if we don&#39;t want it
in the commit message.
<span class="quote">
&gt; </span>
<span class="quote">&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Andrew Banman &lt;abanman@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Mike Travis &lt;travis@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Dimitri Sivanich &lt;sivanich@sgi.com&gt;</span>
<span class="quote">&gt; Cc: Juergen Gross &lt;jgross@suse.com&gt;</span>
<span class="quote">&gt; Cc: Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/mmu_context.h |   6 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/tlbflush.h    |   4 -</span>
<span class="quote">&gt;  arch/x86/mm/init.c                 |   1 -</span>
<span class="quote">&gt;  arch/x86/mm/tlb.c                  | 227 +++++++++++++++++++------------------</span>
<span class="quote">&gt;  arch/x86/xen/mmu_pv.c              |   3 +-</span>
<span class="quote">&gt;  5 files changed, 119 insertions(+), 122 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; index e5295d485899..69a4f1ee86ac 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -125,8 +125,10 @@ static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt; -		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);</span>
<span class="quote">&gt; +	int cpu = smp_processor_id();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt; +		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>

It seems we haz a helper for that: cpumask_test_and_clear_cpu() which
does BTR straightaway.
<span class="quote">
&gt;  extern atomic64_t last_mm_ctx_id;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; index 4f6c30d6ec39..87b13e51e867 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; @@ -95,7 +95,6 @@ struct tlb_state {</span>
<span class="quote">&gt;  	 * mode even if we&#39;ve already switched back to swapper_pg_dir.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	struct mm_struct *loaded_mm;</span>
<span class="quote">&gt; -	int state;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Access to this CR4 shadow and to H/W CR4 is protected by</span>
<span class="quote">&gt; @@ -310,9 +309,6 @@ static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
<span class="quote">&gt;  void native_flush_tlb_others(const struct cpumask *cpumask,</span>
<span class="quote">&gt;  			     const struct flush_tlb_info *info);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define TLBSTATE_OK	1</span>
<span class="quote">&gt; -#define TLBSTATE_LAZY	2</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,</span>
<span class="quote">&gt;  					struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="quote">&gt; index 88ee942cb47d..7d6fa4676af9 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/init.c</span>
<span class="quote">&gt; @@ -812,7 +812,6 @@ void __init zone_sizes_init(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
<span class="quote">&gt;  	.loaded_mm = &amp;init_mm,</span>
<span class="quote">&gt; -	.state = 0,</span>
<span class="quote">&gt;  	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(cpu_tlbstate);</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; index 9f5ef7a5e74a..fea2b07ac7d8 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; @@ -45,8 +45,8 @@ void leave_mm(int cpu)</span>
<span class="quote">&gt;  	if (loaded_mm == &amp;init_mm)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt; -		BUG();</span>
<span class="quote">&gt; +	/* Warn if we&#39;re not lazy. */</span>
<span class="quote">&gt; +	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>

We don&#39;t BUG() anymore?
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  	switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -67,133 +67,118 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned cpu = smp_processor_id();</span>
<span class="quote">&gt;  	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);</span>
<span class="quote">&gt; +	u64 next_tlb_gen;</span>

Please sort function local variables declaration in a reverse christmas
tree order:

	&lt;type&gt; longest_variable_name;
	&lt;type&gt; shorter_var_name;
	&lt;type&gt; even_shorter;
	&lt;type&gt; i;
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * NB: The scheduler will call us with prev == next when</span>
<span class="quote">&gt; -	 * switching from lazy TLB mode to normal mode if active_mm</span>
<span class="quote">&gt; -	 * isn&#39;t changing.  When this happens, there is no guarantee</span>
<span class="quote">&gt; -	 * that CR3 (and hence cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt; +	 * NB: The scheduler will call us with prev == next when switching</span>
<span class="quote">&gt; +	 * from lazy TLB mode to normal mode if active_mm isn&#39;t changing.</span>
<span class="quote">&gt; +	 * When this happens, we don&#39;t assume that CR3 (and hence</span>
<span class="quote">&gt; +	 * cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt;  	 *</span>
<span class="quote">&gt;  	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="quote">&gt; +	/* We don&#39;t want flush_tlb_func_* to run concurrently with us. */</span>
<span class="quote">&gt; +	if (IS_ENABLED(CONFIG_PROVE_LOCKING))</span>
<span class="quote">&gt; +		WARN_ON_ONCE(!irqs_disabled());</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>

Why do we need that check? Can that ever happen?
<span class="quote">
&gt;  	if (real_prev == next) {</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * There&#39;s nothing to do: we always keep the per-mm control</span>
<span class="quote">&gt; -		 * regs in sync with cpu_tlbstate.loaded_mm.  Just</span>
<span class="quote">&gt; -		 * sanity-check mm_cpumask.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="quote">&gt; -			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="quote">&gt; +			 * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="quote">&gt; +			 * anything, nor do we need to update CR3, CR4, or</span>
<span class="quote">&gt; +			 * LDTR.</span>
<span class="quote">&gt; +			 */</span>

Nice comment.
<span class="quote">
&gt; +			return;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="quote">&gt; +		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; +		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="quote">&gt; +			  next-&gt;context.ctx_id);</span>

I guess this check should be right under the if (real_prev == next), right?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="quote">&gt; +		    next_tlb_gen) {</span>

Yeah, let it stick out - that trailing &#39;&lt;&#39; doesn&#39;t make it any nicer.
<span class="quote">
&gt; +			/*</span>
<span class="quote">&gt; +			 * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="quote">&gt; +			 * takes the known CR3 value as input.  This would</span>
<span class="quote">&gt; +			 * be faster on Xen PV and on hypothetical CPUs</span>
<span class="quote">&gt; +			 * on which INVPCID is fast.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; +				       next_tlb_gen);</span>
<span class="quote">&gt; +			write_cr3(__pa(next-&gt;pgd));</span>

&lt;---- newline here.
<span class="quote">
&gt; +			/*</span>
<span class="quote">&gt; +			 * This gets called via leave_mm() in the idle path</span>
<span class="quote">&gt; +			 * where RCU functions differently.  Tracing normally</span>
<span class="quote">&gt; +			 * uses RCU, so we have to call the tracepoint</span>
<span class="quote">&gt; +			 * specially here.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt; +						TLB_FLUSH_ALL);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -		 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt; -		 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt; -		 * map it.</span>
<span class="quote">&gt; +		 * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="quote">&gt; +		 * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="quote">&gt; +		 * are not reflected in tlb_gen.)</span>

We need that comment because... ? I mean, we do update CR4/LDTR at the
end of the function.
<span class="quote">
&gt;  		 */</span>
<span class="quote">&gt; -		unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt; +			 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt; +			 * map it.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			unsigned int stack_pgd_index =</span>

Shorten that var name and make it fit into 80ish cols so that the
linebreak is gone.
<span class="quote">
&gt; +				pgd_index(current_stack_pointer());</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt; +				set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt; +		/* Stop remote flushes for the previous mm */</span>
<span class="quote">&gt; +		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="quote">&gt; +			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>

cpumask_test_and_clear_cpu()
<span class="quote">
&gt;  </span>
<span class="quote">&gt; -		if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt; -			set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +		WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>

We warn if the next task is not lazy because...?
<span class="quote">
&gt; -	this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt; -	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="quote">&gt; -	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; -		       atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Start remote flushes and then read tlb_gen.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; +		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt; -	cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; +		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="quote">&gt; +			  next-&gt;context.ctx_id);</span>

Also put it as the first statement after the else { ?
<span class="quote">
&gt; -	/*</span>
<span class="quote">&gt; -	 * Re-load page tables.</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * This logic has an ordering constraint:</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="quote">&gt; -	 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="quote">&gt; -	 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="quote">&gt; -	 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="quote">&gt; -	 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="quote">&gt; -	 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="quote">&gt; -	 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="quote">&gt; -	 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="quote">&gt; -	 * execute full barriers to prevent this from happening.</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * Thus, switch_mm needs a full barrier between the</span>
<span class="quote">&gt; -	 * store to mm_cpumask and any operation that could load</span>
<span class="quote">&gt; -	 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="quote">&gt; -	 * due to instruction fetches or for no reason at all,</span>
<span class="quote">&gt; -	 * and neither LOCK nor MFENCE orders them.</span>
<span class="quote">&gt; -	 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="quote">&gt; -	 * ordering guarantee we need.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	load_cr3(next-&gt;pgd);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt; -	 * functions differently.  Tracing normally uses RCU, so we have to</span>
<span class="quote">&gt; -	 * call the tracepoint specially here.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="quote">&gt; +		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="quote">&gt; +			       next-&gt;context.ctx_id);</span>
<span class="quote">&gt; +		this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; +			       next_tlb_gen);</span>

Yeah, just let all those three stick out.

Also, no:

                if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;
                    next_tlb_gen) {

check?
<span class="quote">
&gt; +		this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt; +		write_cr3(__pa(next-&gt;pgd));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Stop flush ipis for the previous mm */</span>
<span class="quote">&gt; -	WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="quote">&gt; -		     real_prev != &amp;init_mm);</span>
<span class="quote">&gt; -	cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt; +		 * functions differently.  Tracing normally uses RCU, so we</span>
<span class="quote">&gt; +		 * have to call the tracepoint specially here.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt; +					TLB_FLUSH_ALL);</span>
<span class="quote">&gt; +	}</span>

That&#39;s repeated as in the if-branch above. Move it out I guess.
<span class="quote">
&gt;  </span>
<span class="quote">&gt; -	/* Load per-mm CR4 and LDTR state */</span>
<span class="quote">&gt;  	load_mm_cr4(next);</span>
<span class="quote">&gt;  	switch_ldt(real_prev, next);</span>
<span class="quote">&gt;  }</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 22, 2017, 5:47 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 7:50 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:
<span class="quote">&gt; On Tue, Jun 20, 2017 at 10:22:12PM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt;&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; s/cpu/CPU/</span>

Done.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt;&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt;&gt; date.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt;&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt;&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt;&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt;&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt;&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt;&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt;&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt;&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt;&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt;&gt; needs to twiddle.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This whole text should be under the &quot;---&quot; line below if we don&#39;t want it</span>
<span class="quote">&gt; in the commit message.</span>

I figured that some future reader of this patch might actually want to
see this text, though.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The UV tlbflush code is rather dated and should be changed.</span>

And I&#39;d definitely like the UV maintainers to notice this part, now or
in the future :)  I don&#39;t want to personally touch the UV code with a
ten-foot pole, but it really should be updated by someone who has a
chance of getting it right and being able to test it.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt;&gt; +             cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It seems we haz a helper for that: cpumask_test_and_clear_cpu() which</span>
<span class="quote">&gt; does BTR straightaway.</span>

Yeah, but I&#39;m doing this for performance.  I think that all the
various one-line helpers do a LOCKed op right away, and I think it&#39;s
faster to see if we can avoid the LOCKed op by trying an ordinary read
first.  OTOH, maybe this is misguided -- if the cacheline lives
somewhere else and we do end up needing to update it, we&#39;ll end up
first sharing it and then making it exclusive, which increases the
amount of cache coherency traffic, so maybe I&#39;m optimizing for the
wrong thing.  What do you think?
<span class="quote">
&gt;&gt; -     if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt;&gt; -             BUG();</span>
<span class="quote">&gt;&gt; +     /* Warn if we&#39;re not lazy. */</span>
<span class="quote">&gt;&gt; +     WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We don&#39;t BUG() anymore?</span>

We could.  But, when the whole patch series is applied, the only
caller left is a somewhat dubious Xen optimization, and if we blindly
continue executing, I think the worst that happens is that we OOPS
later or that we get segfaults when we shouldn&#39;t get segfaults.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -67,133 +67,118 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;       unsigned cpu = smp_processor_id();</span>
<span class="quote">&gt;&gt;       struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);</span>
<span class="quote">&gt;&gt; +     u64 next_tlb_gen;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please sort function local variables declaration in a reverse christmas</span>
<span class="quote">&gt; tree order:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         &lt;type&gt; longest_variable_name;</span>
<span class="quote">&gt;         &lt;type&gt; shorter_var_name;</span>
<span class="quote">&gt;         &lt;type&gt; even_shorter;</span>
<span class="quote">&gt;         &lt;type&gt; i;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       /*</span>
<span class="quote">&gt;&gt; -      * NB: The scheduler will call us with prev == next when</span>
<span class="quote">&gt;&gt; -      * switching from lazy TLB mode to normal mode if active_mm</span>
<span class="quote">&gt;&gt; -      * isn&#39;t changing.  When this happens, there is no guarantee</span>
<span class="quote">&gt;&gt; -      * that CR3 (and hence cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt;&gt; +      * NB: The scheduler will call us with prev == next when switching</span>
<span class="quote">&gt;&gt; +      * from lazy TLB mode to normal mode if active_mm isn&#39;t changing.</span>
<span class="quote">&gt;&gt; +      * When this happens, we don&#39;t assume that CR3 (and hence</span>
<span class="quote">&gt;&gt; +      * cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt;&gt;        *</span>
<span class="quote">&gt;&gt;        * NB: leave_mm() calls us with prev == NULL and tsk == NULL.</span>
<span class="quote">&gt;&gt;        */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="quote">&gt;&gt; +     /* We don&#39;t want flush_tlb_func_* to run concurrently with us. */</span>
<span class="quote">&gt;&gt; +     if (IS_ENABLED(CONFIG_PROVE_LOCKING))</span>
<span class="quote">&gt;&gt; +             WARN_ON_ONCE(!irqs_disabled());</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why do we need that check? Can that ever happen?</span>

It did in one particular buggy incarnation.  It would also trigger if,
say, suspend/resume corrupts CR3.  Admittedly this is unlikely, but
I&#39;d rather catch it.  Once PCID is on, corruption seems a bit less
farfetched -- this assertion will catch anyone who accidentally does
write_cr3(read_cr3_pa()).
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;       if (real_prev == next) {</span>
<span class="quote">&gt;&gt; -             /*</span>
<span class="quote">&gt;&gt; -              * There&#39;s nothing to do: we always keep the per-mm control</span>
<span class="quote">&gt;&gt; -              * regs in sync with cpu_tlbstate.loaded_mm.  Just</span>
<span class="quote">&gt;&gt; -              * sanity-check mm_cpumask.</span>
<span class="quote">&gt;&gt; -              */</span>
<span class="quote">&gt;&gt; -             if (WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="quote">&gt;&gt; -                     cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt;&gt; -             return;</span>
<span class="quote">&gt;&gt; -     }</span>
<span class="quote">&gt;&gt; +             if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="quote">&gt;&gt; +                     /*</span>
<span class="quote">&gt;&gt; +                      * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="quote">&gt;&gt; +                      * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="quote">&gt;&gt; +                      * anything, nor do we need to update CR3, CR4, or</span>
<span class="quote">&gt;&gt; +                      * LDTR.</span>
<span class="quote">&gt;&gt; +                      */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Nice comment.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +                     return;</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             /* Resume remote flushes and then read tlb_gen. */</span>
<span class="quote">&gt;&gt; +             cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt;&gt; +             next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="quote">&gt;&gt; +                       next-&gt;context.ctx_id);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I guess this check should be right under the if (real_prev == next), right?</span>

Moved.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="quote">&gt;&gt; +                 next_tlb_gen) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, let it stick out - that trailing &#39;&lt;&#39; doesn&#39;t make it any nicer.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +                     /*</span>
<span class="quote">&gt;&gt; +                      * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="quote">&gt;&gt; +                      * takes the known CR3 value as input.  This would</span>
<span class="quote">&gt;&gt; +                      * be faster on Xen PV and on hypothetical CPUs</span>
<span class="quote">&gt;&gt; +                      * on which INVPCID is fast.</span>
<span class="quote">&gt;&gt; +                      */</span>
<span class="quote">&gt;&gt; +                     this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt;&gt; +                                    next_tlb_gen);</span>
<span class="quote">&gt;&gt; +                     write_cr3(__pa(next-&gt;pgd));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &lt;---- newline here.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +                     /*</span>
<span class="quote">&gt;&gt; +                      * This gets called via leave_mm() in the idle path</span>
<span class="quote">&gt;&gt; +                      * where RCU functions differently.  Tracing normally</span>
<span class="quote">&gt;&gt; +                      * uses RCU, so we have to call the tracepoint</span>
<span class="quote">&gt;&gt; +                      * specially here.</span>
<span class="quote">&gt;&gt; +                      */</span>
<span class="quote">&gt;&gt; +                     trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt;&gt; +                                             TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt;&gt;               /*</span>
<span class="quote">&gt;&gt; -              * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt;&gt; -              * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt;&gt; -              * map it.</span>
<span class="quote">&gt;&gt; +              * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="quote">&gt;&gt; +              * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="quote">&gt;&gt; +              * are not reflected in tlb_gen.)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We need that comment because... ? I mean, we do update CR4/LDTR at the</span>
<span class="quote">&gt; end of the function.</span>

I&#39;m trying to explain to the potentially confused reader why we fall
through and update CR4 and LDTR even if we decided not to update the
TLB.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;                */</span>
<span class="quote">&gt;&gt; -             unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="quote">&gt;&gt; +     } else {</span>
<span class="quote">&gt;&gt; +             if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt;&gt; +                     /*</span>
<span class="quote">&gt;&gt; +                      * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt;&gt; +                      * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt;&gt; +                      * map it.</span>
<span class="quote">&gt;&gt; +                      */</span>
<span class="quote">&gt;&gt; +                     unsigned int stack_pgd_index =</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Shorten that var name and make it fit into 80ish cols so that the</span>
<span class="quote">&gt; linebreak is gone.</span>

Done.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +                             pgd_index(current_stack_pointer());</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +                     pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +                     if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt;&gt; +                             set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -             pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt;&gt; +             /* Stop remote flushes for the previous mm */</span>
<span class="quote">&gt;&gt; +             if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="quote">&gt;&gt; +                     cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; cpumask_test_and_clear_cpu()</span>

Same as before.  I can change this, but it&#39;ll have different
performance characteristics.  This one here will optimize the case
where we go lazy and then switch away.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -             if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt;&gt; -                     set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt;&gt; -     }</span>
<span class="quote">&gt;&gt; +             WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We warn if the next task is not lazy because...?</span>

The next task had better not be in mm_cpumask(), but I agree that this
is minor.  I&#39;ll change it to VM_WARN_ON_ONCE.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; -     this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt;&gt; -     this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="quote">&gt;&gt; -     this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt;&gt; -                    atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
<span class="quote">&gt;&gt; +             /*</span>
<span class="quote">&gt;&gt; +              * Start remote flushes and then read tlb_gen.</span>
<span class="quote">&gt;&gt; +              */</span>
<span class="quote">&gt;&gt; +             cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt;&gt; +             next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt;&gt; -     cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt;&gt; +             VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="quote">&gt;&gt; +                       next-&gt;context.ctx_id);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also put it as the first statement after the else { ?</span>

Done.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; -     /*</span>
<span class="quote">&gt;&gt; -      * Re-load page tables.</span>
<span class="quote">&gt;&gt; -      *</span>
<span class="quote">&gt;&gt; -      * This logic has an ordering constraint:</span>
<span class="quote">&gt;&gt; -      *</span>
<span class="quote">&gt;&gt; -      *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="quote">&gt;&gt; -      *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="quote">&gt;&gt; -      *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="quote">&gt;&gt; -      *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="quote">&gt;&gt; -      *</span>
<span class="quote">&gt;&gt; -      * We need to prevent an outcome in which CPU 1 observes</span>
<span class="quote">&gt;&gt; -      * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="quote">&gt;&gt; -      * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="quote">&gt;&gt; -      * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="quote">&gt;&gt; -      *</span>
<span class="quote">&gt;&gt; -      * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="quote">&gt;&gt; -      * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="quote">&gt;&gt; -      * execute full barriers to prevent this from happening.</span>
<span class="quote">&gt;&gt; -      *</span>
<span class="quote">&gt;&gt; -      * Thus, switch_mm needs a full barrier between the</span>
<span class="quote">&gt;&gt; -      * store to mm_cpumask and any operation that could load</span>
<span class="quote">&gt;&gt; -      * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="quote">&gt;&gt; -      * due to instruction fetches or for no reason at all,</span>
<span class="quote">&gt;&gt; -      * and neither LOCK nor MFENCE orders them.</span>
<span class="quote">&gt;&gt; -      * Fortunately, load_cr3() is serializing and gives the</span>
<span class="quote">&gt;&gt; -      * ordering guarantee we need.</span>
<span class="quote">&gt;&gt; -      */</span>
<span class="quote">&gt;&gt; -     load_cr3(next-&gt;pgd);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -     /*</span>
<span class="quote">&gt;&gt; -      * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt;&gt; -      * functions differently.  Tracing normally uses RCU, so we have to</span>
<span class="quote">&gt;&gt; -      * call the tracepoint specially here.</span>
<span class="quote">&gt;&gt; -      */</span>
<span class="quote">&gt;&gt; -     trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt; +             this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="quote">&gt;&gt; +                            next-&gt;context.ctx_id);</span>
<span class="quote">&gt;&gt; +             this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt;&gt; +                            next_tlb_gen);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, just let all those three stick out.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also, no:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="quote">&gt;                     next_tlb_gen) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; check?</span>

What would the check do?  Without PCID, we don&#39;t have a choice as to
whether we flush.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +             this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt;&gt; +             write_cr3(__pa(next-&gt;pgd));</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     /* Stop flush ipis for the previous mm */</span>
<span class="quote">&gt;&gt; -     WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="quote">&gt;&gt; -                  real_prev != &amp;init_mm);</span>
<span class="quote">&gt;&gt; -     cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="quote">&gt;&gt; +             /*</span>
<span class="quote">&gt;&gt; +              * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt;&gt; +              * functions differently.  Tracing normally uses RCU, so we</span>
<span class="quote">&gt;&gt; +              * have to call the tracepoint specially here.</span>
<span class="quote">&gt;&gt; +              */</span>
<span class="quote">&gt;&gt; +             trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt;&gt; +                                     TLB_FLUSH_ALL);</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s repeated as in the if-branch above. Move it out I guess.</span>

I would have, but it changes later in the series and this reduces churn.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - June 22, 2017, 7:05 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 10:47:29AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; I figured that some future reader of this patch might actually want to</span>
<span class="quote">&gt; see this text, though.</span>

Oh, don&#39;t get me wrong: with commit messages more is more, in the
general case. That&#39;s why I said &quot;if&quot;.
<span class="quote">
&gt; &gt;&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And I&#39;d definitely like the UV maintainers to notice this part, now or</span>
<span class="quote">&gt; in the future :)  I don&#39;t want to personally touch the UV code with a</span>
<span class="quote">&gt; ten-foot pole, but it really should be updated by someone who has a</span>
<span class="quote">&gt; chance of getting it right and being able to test it.</span>

Ah, could be because they moved recently and have hpe addresses now.
Lemme add them.
<span class="quote">
&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +     if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt; &gt;&gt; +             cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It seems we haz a helper for that: cpumask_test_and_clear_cpu() which</span>
<span class="quote">&gt; &gt; does BTR straightaway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, but I&#39;m doing this for performance.  I think that all the</span>
<span class="quote">&gt; various one-line helpers do a LOCKed op right away, and I think it&#39;s</span>
<span class="quote">&gt; faster to see if we can avoid the LOCKed op by trying an ordinary read</span>
<span class="quote">&gt; first.</span>

Right, the test part of the operation is unlocked so if that is the
likely case, it is a win.
<span class="quote">
&gt; OTOH, maybe this is misguided -- if the cacheline lives somewhere else</span>
<span class="quote">&gt; and we do end up needing to update it, we&#39;ll end up first sharing it</span>
<span class="quote">&gt; and then making it exclusive, which increases the amount of cache</span>
<span class="quote">&gt; coherency traffic, so maybe I&#39;m optimizing for the wrong thing. What</span>
<span class="quote">&gt; do you think?</span>

Yeah, but we&#39;ll have to do that anyway for the locked operation. Ok,
let&#39;s leave it split like it is.
<span class="quote">
&gt; It did in one particular buggy incarnation.  It would also trigger if,</span>
<span class="quote">&gt; say, suspend/resume corrupts CR3.  Admittedly this is unlikely, but</span>
<span class="quote">&gt; I&#39;d rather catch it.  Once PCID is on, corruption seems a bit less</span>
<span class="quote">&gt; farfetched -- this assertion will catch anyone who accidentally does</span>
<span class="quote">&gt; write_cr3(read_cr3_pa()).</span>

Ok, but let&#39;s put a comment over it pls as it is not obvious when
something like that can happen.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - June 23, 2017, 1:34 p.m.</div>
<pre class="content">
<span class="quote">&gt; diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt; index 1d7a7213a310..f5df56fb8b5c 100644</span>
<span class="quote">&gt; --- a/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt; +++ b/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt; @@ -1005,8 +1005,7 @@ static void xen_drop_mm_ref(struct mm_struct *mm)</span>
<span class="quote">&gt;   	/* Get the &quot;official&quot; set of cpus referring to our pagetable. */</span>
<span class="quote">&gt;   	if (!alloc_cpumask_var(&amp;mask, GFP_ATOMIC)) {</span>
<span class="quote">&gt;   		for_each_online_cpu(cpu) {</span>
<span class="quote">&gt; -			if (!cpumask_test_cpu(cpu, mm_cpumask(mm))</span>
<span class="quote">&gt; -			    &amp;&amp; per_cpu(xen_current_cr3, cpu) != __pa(mm-&gt;pgd))</span>
<span class="quote">&gt; +			if (per_cpu(xen_current_cr3, cpu) != __pa(mm-&gt;pgd))</span>
<span class="quote">&gt;   				continue;</span>
<span class="quote">&gt;   			smp_call_function_single(cpu, drop_mm_ref_this_cpu, mm, 1);</span>
<span class="quote">&gt;   		}</span>
<span class="quote">&gt; </span>


I wonder then whether
	cpumask_copy(mask, mm_cpumask(mm));
immediately below is needed.

-boris
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 23, 2017, 3:22 p.m.</div>
<pre class="content">
On Fri, Jun 23, 2017 at 6:34 AM, Boris Ostrovsky
&lt;boris.ostrovsky@oracle.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt;&gt; index 1d7a7213a310..f5df56fb8b5c 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/xen/mmu_pv.c</span>
<span class="quote">&gt;&gt; @@ -1005,8 +1005,7 @@ static void xen_drop_mm_ref(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;         /* Get the &quot;official&quot; set of cpus referring to our pagetable. */</span>
<span class="quote">&gt;&gt;         if (!alloc_cpumask_var(&amp;mask, GFP_ATOMIC)) {</span>
<span class="quote">&gt;&gt;                 for_each_online_cpu(cpu) {</span>
<span class="quote">&gt;&gt; -                       if (!cpumask_test_cpu(cpu, mm_cpumask(mm))</span>
<span class="quote">&gt;&gt; -                           &amp;&amp; per_cpu(xen_current_cr3, cpu) !=</span>
<span class="quote">&gt;&gt; __pa(mm-&gt;pgd))</span>
<span class="quote">&gt;&gt; +                       if (per_cpu(xen_current_cr3, cpu) !=</span>
<span class="quote">&gt;&gt; __pa(mm-&gt;pgd))</span>
<span class="quote">&gt;&gt;                                 continue;</span>
<span class="quote">&gt;&gt;                         smp_call_function_single(cpu,</span>
<span class="quote">&gt;&gt; drop_mm_ref_this_cpu, mm, 1);</span>
<span class="quote">&gt;&gt;                 }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I wonder then whether</span>
<span class="quote">&gt;         cpumask_copy(mask, mm_cpumask(mm));</span>
<span class="quote">&gt; immediately below is needed.</span>

Probably not.  I&#39;ll change it to cpumask_clear().  Then the two cases
in that function match better.
<span class="quote">
&gt;</span>
<span class="quote">&gt; -boris</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172707">Andrew Banman</a> - July 27, 2017, 7:53 p.m.</div>
<pre class="content">
On Thu, Jun 22, 2017 at 10:47:29AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; On Thu, Jun 22, 2017 at 7:50 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:</span>
<span class="quote">&gt; &gt; On Tue, Jun 20, 2017 at 10:22:12PM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt; &gt;&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; s/cpu/CPU/</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Done.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt; &gt;&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt; &gt;&gt; date.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt; &gt;&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt; &gt;&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt; &gt;&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt; &gt;&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt; &gt;&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt; &gt;&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt; &gt;&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt; &gt;&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt; &gt;&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt; &gt;&gt; needs to twiddle.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This whole text should be under the &quot;---&quot; line below if we don&#39;t want it</span>
<span class="quote">&gt; &gt; in the commit message.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I figured that some future reader of this patch might actually want to</span>
<span class="quote">&gt; see this text, though.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And I&#39;d definitely like the UV maintainers to notice this part, now or</span>
<span class="quote">&gt; in the future :)  I don&#39;t want to personally touch the UV code with a</span>
<span class="quote">&gt; ten-foot pole, but it really should be updated by someone who has a</span>
<span class="quote">&gt; chance of getting it right and being able to test it.</span>

Noticed! We&#39;re aware of these changes and we&#39;re planning on updating this
code in the future. Presently the BAU tlb shootdown feature is working well
on our recent hardware.

Thank you,

Andrew
HPE &lt;abanman@hpe.com&gt;
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +     if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="quote">&gt; &gt;&gt; +             cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It seems we haz a helper for that: cpumask_test_and_clear_cpu() which</span>
<span class="quote">&gt; &gt; does BTR straightaway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, but I&#39;m doing this for performance.  I think that all the</span>
<span class="quote">&gt; various one-line helpers do a LOCKed op right away, and I think it&#39;s</span>
<span class="quote">&gt; faster to see if we can avoid the LOCKed op by trying an ordinary read</span>
<span class="quote">&gt; first.  OTOH, maybe this is misguided -- if the cacheline lives</span>
<span class="quote">&gt; somewhere else and we do end up needing to update it, we&#39;ll end up</span>
<span class="quote">&gt; first sharing it and then making it exclusive, which increases the</span>
<span class="quote">&gt; amount of cache coherency traffic, so maybe I&#39;m optimizing for the</span>
<span class="quote">&gt; wrong thing.  What do you think?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; -     if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="quote">&gt; &gt;&gt; -             BUG();</span>
<span class="quote">&gt; &gt;&gt; +     /* Warn if we&#39;re not lazy. */</span>
<span class="quote">&gt; &gt;&gt; +     WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; We don&#39;t BUG() anymore?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We could.  But, when the whole patch series is applied, the only</span>
<span class="quote">&gt; caller left is a somewhat dubious Xen optimization, and if we blindly</span>
<span class="quote">&gt; continue executing, I think the worst that happens is that we OOPS</span>
<span class="quote">&gt; later or that we get segfaults when we shouldn&#39;t get segfaults.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;       switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="quote">&gt; &gt;&gt;  }</span>
<span class="quote">&gt; &gt;&gt; @@ -67,133 +67,118 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt; &gt;&gt;  {</span>
<span class="quote">&gt; &gt;&gt;       unsigned cpu = smp_processor_id();</span>
<span class="quote">&gt; &gt;&gt;       struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);</span>
<span class="quote">&gt; &gt;&gt; +     u64 next_tlb_gen;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Please sort function local variables declaration in a reverse christmas</span>
<span class="quote">&gt; &gt; tree order:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         &lt;type&gt; longest_variable_name;</span>
<span class="quote">&gt; &gt;         &lt;type&gt; shorter_var_name;</span>
<span class="quote">&gt; &gt;         &lt;type&gt; even_shorter;</span>
<span class="quote">&gt; &gt;         &lt;type&gt; i;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;       /*</span>
<span class="quote">&gt; &gt;&gt; -      * NB: The scheduler will call us with prev == next when</span>
<span class="quote">&gt; &gt;&gt; -      * switching from lazy TLB mode to normal mode if active_mm</span>
<span class="quote">&gt; &gt;&gt; -      * isn&#39;t changing.  When this happens, there is no guarantee</span>
<span class="quote">&gt; &gt;&gt; -      * that CR3 (and hence cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt; &gt;&gt; +      * NB: The scheduler will call us with prev == next when switching</span>
<span class="quote">&gt; &gt;&gt; +      * from lazy TLB mode to normal mode if active_mm isn&#39;t changing.</span>
<span class="quote">&gt; &gt;&gt; +      * When this happens, we don&#39;t assume that CR3 (and hence</span>
<span class="quote">&gt; &gt;&gt; +      * cpu_tlbstate.loaded_mm) matches next.</span>
<span class="quote">&gt; &gt;&gt;        *</span>
<span class="quote">&gt; &gt;&gt;        * NB: leave_mm() calls us with prev == NULL and tsk == NULL.</span>
<span class="quote">&gt; &gt;&gt;        */</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -     this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="quote">&gt; &gt;&gt; +     /* We don&#39;t want flush_tlb_func_* to run concurrently with us. */</span>
<span class="quote">&gt; &gt;&gt; +     if (IS_ENABLED(CONFIG_PROVE_LOCKING))</span>
<span class="quote">&gt; &gt;&gt; +             WARN_ON_ONCE(!irqs_disabled());</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +     VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Why do we need that check? Can that ever happen?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It did in one particular buggy incarnation.  It would also trigger if,</span>
<span class="quote">&gt; say, suspend/resume corrupts CR3.  Admittedly this is unlikely, but</span>
<span class="quote">&gt; I&#39;d rather catch it.  Once PCID is on, corruption seems a bit less</span>
<span class="quote">&gt; farfetched -- this assertion will catch anyone who accidentally does</span>
<span class="quote">&gt; write_cr3(read_cr3_pa()).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;       if (real_prev == next) {</span>
<span class="quote">&gt; &gt;&gt; -             /*</span>
<span class="quote">&gt; &gt;&gt; -              * There&#39;s nothing to do: we always keep the per-mm control</span>
<span class="quote">&gt; &gt;&gt; -              * regs in sync with cpu_tlbstate.loaded_mm.  Just</span>
<span class="quote">&gt; &gt;&gt; -              * sanity-check mm_cpumask.</span>
<span class="quote">&gt; &gt;&gt; -              */</span>
<span class="quote">&gt; &gt;&gt; -             if (WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="quote">&gt; &gt;&gt; -                     cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; &gt;&gt; -             return;</span>
<span class="quote">&gt; &gt;&gt; -     }</span>
<span class="quote">&gt; &gt;&gt; +             if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="quote">&gt; &gt;&gt; +                     /*</span>
<span class="quote">&gt; &gt;&gt; +                      * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="quote">&gt; &gt;&gt; +                      * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="quote">&gt; &gt;&gt; +                      * anything, nor do we need to update CR3, CR4, or</span>
<span class="quote">&gt; &gt;&gt; +                      * LDTR.</span>
<span class="quote">&gt; &gt;&gt; +                      */</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Nice comment.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +                     return;</span>
<span class="quote">&gt; &gt;&gt; +             }</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +             /* Resume remote flushes and then read tlb_gen. */</span>
<span class="quote">&gt; &gt;&gt; +             cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; &gt;&gt; +             next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +             VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="quote">&gt; &gt;&gt; +                       next-&gt;context.ctx_id);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I guess this check should be right under the if (real_prev == next), right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Moved.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +             if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="quote">&gt; &gt;&gt; +                 next_tlb_gen) {</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yeah, let it stick out - that trailing &#39;&lt;&#39; doesn&#39;t make it any nicer.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +                     /*</span>
<span class="quote">&gt; &gt;&gt; +                      * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="quote">&gt; &gt;&gt; +                      * takes the known CR3 value as input.  This would</span>
<span class="quote">&gt; &gt;&gt; +                      * be faster on Xen PV and on hypothetical CPUs</span>
<span class="quote">&gt; &gt;&gt; +                      * on which INVPCID is fast.</span>
<span class="quote">&gt; &gt;&gt; +                      */</span>
<span class="quote">&gt; &gt;&gt; +                     this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; &gt;&gt; +                                    next_tlb_gen);</span>
<span class="quote">&gt; &gt;&gt; +                     write_cr3(__pa(next-&gt;pgd));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &lt;---- newline here.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +                     /*</span>
<span class="quote">&gt; &gt;&gt; +                      * This gets called via leave_mm() in the idle path</span>
<span class="quote">&gt; &gt;&gt; +                      * where RCU functions differently.  Tracing normally</span>
<span class="quote">&gt; &gt;&gt; +                      * uses RCU, so we have to call the tracepoint</span>
<span class="quote">&gt; &gt;&gt; +                      * specially here.</span>
<span class="quote">&gt; &gt;&gt; +                      */</span>
<span class="quote">&gt; &gt;&gt; +                     trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt; &gt;&gt; +                                             TLB_FLUSH_ALL);</span>
<span class="quote">&gt; &gt;&gt; +             }</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -     if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt; &gt;&gt;               /*</span>
<span class="quote">&gt; &gt;&gt; -              * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt; &gt;&gt; -              * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt; &gt;&gt; -              * map it.</span>
<span class="quote">&gt; &gt;&gt; +              * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="quote">&gt; &gt;&gt; +              * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="quote">&gt; &gt;&gt; +              * are not reflected in tlb_gen.)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; We need that comment because... ? I mean, we do update CR4/LDTR at the</span>
<span class="quote">&gt; &gt; end of the function.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m trying to explain to the potentially confused reader why we fall</span>
<span class="quote">&gt; through and update CR4 and LDTR even if we decided not to update the</span>
<span class="quote">&gt; TLB.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;                */</span>
<span class="quote">&gt; &gt;&gt; -             unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="quote">&gt; &gt;&gt; +     } else {</span>
<span class="quote">&gt; &gt;&gt; +             if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="quote">&gt; &gt;&gt; +                     /*</span>
<span class="quote">&gt; &gt;&gt; +                      * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="quote">&gt; &gt;&gt; +                      * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="quote">&gt; &gt;&gt; +                      * map it.</span>
<span class="quote">&gt; &gt;&gt; +                      */</span>
<span class="quote">&gt; &gt;&gt; +                     unsigned int stack_pgd_index =</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Shorten that var name and make it fit into 80ish cols so that the</span>
<span class="quote">&gt; &gt; linebreak is gone.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Done.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +                             pgd_index(current_stack_pointer());</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +                     pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +                     if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt; &gt;&gt; +                             set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt; &gt;&gt; +             }</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -             pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="quote">&gt; &gt;&gt; +             /* Stop remote flushes for the previous mm */</span>
<span class="quote">&gt; &gt;&gt; +             if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="quote">&gt; &gt;&gt; +                     cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; cpumask_test_and_clear_cpu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same as before.  I can change this, but it&#39;ll have different</span>
<span class="quote">&gt; performance characteristics.  This one here will optimize the case</span>
<span class="quote">&gt; where we go lazy and then switch away.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -             if (unlikely(pgd_none(*pgd)))</span>
<span class="quote">&gt; &gt;&gt; -                     set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="quote">&gt; &gt;&gt; -     }</span>
<span class="quote">&gt; &gt;&gt; +             WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; We warn if the next task is not lazy because...?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The next task had better not be in mm_cpumask(), but I agree that this</span>
<span class="quote">&gt; is minor.  I&#39;ll change it to VM_WARN_ON_ONCE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; -     this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt; &gt;&gt; -     this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="quote">&gt; &gt;&gt; -     this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; &gt;&gt; -                    atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
<span class="quote">&gt; &gt;&gt; +             /*</span>
<span class="quote">&gt; &gt;&gt; +              * Start remote flushes and then read tlb_gen.</span>
<span class="quote">&gt; &gt;&gt; +              */</span>
<span class="quote">&gt; &gt;&gt; +             cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; &gt;&gt; +             next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -     WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt; &gt;&gt; -     cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; &gt;&gt; +             VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="quote">&gt; &gt;&gt; +                       next-&gt;context.ctx_id);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Also put it as the first statement after the else { ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Done.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; -     /*</span>
<span class="quote">&gt; &gt;&gt; -      * Re-load page tables.</span>
<span class="quote">&gt; &gt;&gt; -      *</span>
<span class="quote">&gt; &gt;&gt; -      * This logic has an ordering constraint:</span>
<span class="quote">&gt; &gt;&gt; -      *</span>
<span class="quote">&gt; &gt;&gt; -      *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="quote">&gt; &gt;&gt; -      *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="quote">&gt; &gt;&gt; -      *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="quote">&gt; &gt;&gt; -      *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="quote">&gt; &gt;&gt; -      *</span>
<span class="quote">&gt; &gt;&gt; -      * We need to prevent an outcome in which CPU 1 observes</span>
<span class="quote">&gt; &gt;&gt; -      * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="quote">&gt; &gt;&gt; -      * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="quote">&gt; &gt;&gt; -      * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="quote">&gt; &gt;&gt; -      *</span>
<span class="quote">&gt; &gt;&gt; -      * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="quote">&gt; &gt;&gt; -      * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="quote">&gt; &gt;&gt; -      * execute full barriers to prevent this from happening.</span>
<span class="quote">&gt; &gt;&gt; -      *</span>
<span class="quote">&gt; &gt;&gt; -      * Thus, switch_mm needs a full barrier between the</span>
<span class="quote">&gt; &gt;&gt; -      * store to mm_cpumask and any operation that could load</span>
<span class="quote">&gt; &gt;&gt; -      * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="quote">&gt; &gt;&gt; -      * due to instruction fetches or for no reason at all,</span>
<span class="quote">&gt; &gt;&gt; -      * and neither LOCK nor MFENCE orders them.</span>
<span class="quote">&gt; &gt;&gt; -      * Fortunately, load_cr3() is serializing and gives the</span>
<span class="quote">&gt; &gt;&gt; -      * ordering guarantee we need.</span>
<span class="quote">&gt; &gt;&gt; -      */</span>
<span class="quote">&gt; &gt;&gt; -     load_cr3(next-&gt;pgd);</span>
<span class="quote">&gt; &gt;&gt; -</span>
<span class="quote">&gt; &gt;&gt; -     /*</span>
<span class="quote">&gt; &gt;&gt; -      * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt; &gt;&gt; -      * functions differently.  Tracing normally uses RCU, so we have to</span>
<span class="quote">&gt; &gt;&gt; -      * call the tracepoint specially here.</span>
<span class="quote">&gt; &gt;&gt; -      */</span>
<span class="quote">&gt; &gt;&gt; -     trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="quote">&gt; &gt;&gt; +             this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="quote">&gt; &gt;&gt; +                            next-&gt;context.ctx_id);</span>
<span class="quote">&gt; &gt;&gt; +             this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="quote">&gt; &gt;&gt; +                            next_tlb_gen);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yeah, just let all those three stick out.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Also, no:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;                 if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="quote">&gt; &gt;                     next_tlb_gen) {</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; check?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What would the check do?  Without PCID, we don&#39;t have a choice as to</span>
<span class="quote">&gt; whether we flush.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +             this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="quote">&gt; &gt;&gt; +             write_cr3(__pa(next-&gt;pgd));</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; -     /* Stop flush ipis for the previous mm */</span>
<span class="quote">&gt; &gt;&gt; -     WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="quote">&gt; &gt;&gt; -                  real_prev != &amp;init_mm);</span>
<span class="quote">&gt; &gt;&gt; -     cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="quote">&gt; &gt;&gt; +             /*</span>
<span class="quote">&gt; &gt;&gt; +              * This gets called via leave_mm() in the idle path where RCU</span>
<span class="quote">&gt; &gt;&gt; +              * functions differently.  Tracing normally uses RCU, so we</span>
<span class="quote">&gt; &gt;&gt; +              * have to call the tracepoint specially here.</span>
<span class="quote">&gt; &gt;&gt; +              */</span>
<span class="quote">&gt; &gt;&gt; +             trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="quote">&gt; &gt;&gt; +                                     TLB_FLUSH_ALL);</span>
<span class="quote">&gt; &gt;&gt; +     }</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; That&#39;s repeated as in the if-branch above. Move it out I guess.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would have, but it changes later in the series and this reduces churn.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - July 28, 2017, 2:05 a.m.</div>
<pre class="content">
<span class="quote">&gt; On Jul 27, 2017, at 3:53 PM, Andrew Banman &lt;abanman@hpe.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Thu, Jun 22, 2017 at 10:47:29AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; On Thu, Jun 22, 2017 at 7:50 AM, Borislav Petkov &lt;bp@alien8.de&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, Jun 20, 2017 at 10:22:12PM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Rewrite it entirely.  When we enter lazy mode, we simply remove the</span>
<span class="quote">&gt;&gt;&gt;&gt; cpu from mm_cpumask.  This means that we need a way to figure out</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; s/cpu/CPU/</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Done.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; whether we&#39;ve missed a flush when we switch back out of lazy mode.</span>
<span class="quote">&gt;&gt;&gt;&gt; I use the tlb_gen machinery to track whether a context is up to</span>
<span class="quote">&gt;&gt;&gt;&gt; date.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Note to reviewers: this patch, my itself, looks a bit odd.  I&#39;m</span>
<span class="quote">&gt;&gt;&gt;&gt; using an array of length 1 containing (ctx_id, tlb_gen) rather than</span>
<span class="quote">&gt;&gt;&gt;&gt; just storing tlb_gen, and making it at array isn&#39;t necessary yet.</span>
<span class="quote">&gt;&gt;&gt;&gt; I&#39;m doing this because the next few patches add PCID support, and,</span>
<span class="quote">&gt;&gt;&gt;&gt; with PCID, we need ctx_id, and the array will end up with a length</span>
<span class="quote">&gt;&gt;&gt;&gt; greater than 1.  Making it an array now means that there will be</span>
<span class="quote">&gt;&gt;&gt;&gt; less churn and therefore less stress on your eyeballs.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; NB: This is dubious but, AFAICT, still correct on Xen and UV.</span>
<span class="quote">&gt;&gt;&gt;&gt; xen_exit_mmap() uses mm_cpumask() for nefarious purposes and this</span>
<span class="quote">&gt;&gt;&gt;&gt; patch changes the way that mm_cpumask() works.  This should be okay,</span>
<span class="quote">&gt;&gt;&gt;&gt; since Xen *also* iterates all online CPUs to find all the CPUs it</span>
<span class="quote">&gt;&gt;&gt;&gt; needs to twiddle.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This whole text should be under the &quot;---&quot; line below if we don&#39;t want it</span>
<span class="quote">&gt;&gt;&gt; in the commit message.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I figured that some future reader of this patch might actually want to</span>
<span class="quote">&gt;&gt; see this text, though.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; The UV tlbflush code is rather dated and should be changed.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And I&#39;d definitely like the UV maintainers to notice this part, now or</span>
<span class="quote">&gt;&gt; in the future :)  I don&#39;t want to personally touch the UV code with a</span>
<span class="quote">&gt;&gt; ten-foot pole, but it really should be updated by someone who has a</span>
<span class="quote">&gt;&gt; chance of getting it right and being able to test it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Noticed! We&#39;re aware of these changes and we&#39;re planning on updating this</span>
<span class="quote">&gt; code in the future. Presently the BAU tlb shootdown feature is working well</span>
<span class="quote">&gt; on our recent hardware.</span>

:)

I would suggest reworking it to hook the SMP function call
infrastructure instead of the TLB shootdown code.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index e5295d485899..69a4f1ee86ac 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -125,8 +125,10 @@</span> <span class="p_context"> static inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)</span>
 
 static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 {
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);</span>
<span class="p_add">+	int cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, mm_cpumask(mm)))</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(mm));</span>
 }
 
 extern atomic64_t last_mm_ctx_id;
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4f6c30d6ec39..87b13e51e867 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -95,7 +95,6 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 * mode even if we&#39;ve already switched back to swapper_pg_dir.
 	 */
 	struct mm_struct *loaded_mm;
<span class="p_del">-	int state;</span>
 
 	/*
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
<span class="p_chunk">@@ -310,9 +309,6 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
 void native_flush_tlb_others(const struct cpumask *cpumask,
 			     const struct flush_tlb_info *info);
 
<span class="p_del">-#define TLBSTATE_OK	1</span>
<span class="p_del">-#define TLBSTATE_LAZY	2</span>
<span class="p_del">-</span>
 static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 					struct mm_struct *mm)
 {
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 88ee942cb47d..7d6fa4676af9 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -812,7 +812,6 @@</span> <span class="p_context"> void __init zone_sizes_init(void)</span>
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {
 	.loaded_mm = &amp;init_mm,
<span class="p_del">-	.state = 0,</span>
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
 };
 EXPORT_SYMBOL_GPL(cpu_tlbstate);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 9f5ef7a5e74a..fea2b07ac7d8 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -45,8 +45,8 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 	if (loaded_mm == &amp;init_mm)
 		return;
 
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)</span>
<span class="p_del">-		BUG();</span>
<span class="p_add">+	/* Warn if we&#39;re not lazy. */</span>
<span class="p_add">+	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));</span>
 
 	switch_mm(NULL, &amp;init_mm, NULL);
 }
<span class="p_chunk">@@ -67,133 +67,118 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 {
 	unsigned cpu = smp_processor_id();
 	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_add">+	u64 next_tlb_gen;</span>
 
 	/*
<span class="p_del">-	 * NB: The scheduler will call us with prev == next when</span>
<span class="p_del">-	 * switching from lazy TLB mode to normal mode if active_mm</span>
<span class="p_del">-	 * isn&#39;t changing.  When this happens, there is no guarantee</span>
<span class="p_del">-	 * that CR3 (and hence cpu_tlbstate.loaded_mm) matches next.</span>
<span class="p_add">+	 * NB: The scheduler will call us with prev == next when switching</span>
<span class="p_add">+	 * from lazy TLB mode to normal mode if active_mm isn&#39;t changing.</span>
<span class="p_add">+	 * When this happens, we don&#39;t assume that CR3 (and hence</span>
<span class="p_add">+	 * cpu_tlbstate.loaded_mm) matches next.</span>
 	 *
 	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
 	 */
 
<span class="p_del">-	this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+	/* We don&#39;t want flush_tlb_func_* to run concurrently with us. */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PROVE_LOCKING))</span>
<span class="p_add">+		WARN_ON_ONCE(!irqs_disabled());</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>
 
 	if (real_prev == next) {
<span class="p_del">-		/*</span>
<span class="p_del">-		 * There&#39;s nothing to do: we always keep the per-mm control</span>
<span class="p_del">-		 * regs in sync with cpu_tlbstate.loaded_mm.  Just</span>
<span class="p_del">-		 * sanity-check mm_cpumask.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(next))))</span>
<span class="p_del">-			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_add">+		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * There&#39;s nothing to do: we weren&#39;t lazy, and we</span>
<span class="p_add">+			 * aren&#39;t changing our mm.  We don&#39;t need to flush</span>
<span class="p_add">+			 * anything, nor do we need to update CR3, CR4, or</span>
<span class="p_add">+			 * LDTR.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="p_add">+			  next-&gt;context.ctx_id);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="p_add">+		    next_tlb_gen) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Ideally, we&#39;d have a flush_tlb() variant that</span>
<span class="p_add">+			 * takes the known CR3 value as input.  This would</span>
<span class="p_add">+			 * be faster on Xen PV and on hypothetical CPUs</span>
<span class="p_add">+			 * on which INVPCID is fast.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_add">+				       next_tlb_gen);</span>
<span class="p_add">+			write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This gets called via leave_mm() in the idle path</span>
<span class="p_add">+			 * where RCU functions differently.  Tracing normally</span>
<span class="p_add">+			 * uses RCU, so we have to call the tracepoint</span>
<span class="p_add">+			 * specially here.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_add">+						TLB_FLUSH_ALL);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-	if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
 		/*
<span class="p_del">-		 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="p_del">-		 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="p_del">-		 * map it.</span>
<span class="p_add">+		 * We just exited lazy mode, which means that CR4 and/or LDTR</span>
<span class="p_add">+		 * may be stale.  (Changes to the required CR4 and LDTR states</span>
<span class="p_add">+		 * are not reflected in tlb_gen.)</span>
 		 */
<span class="p_del">-		unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="p_add">+			 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="p_add">+			 * map it.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			unsigned int stack_pgd_index =</span>
<span class="p_add">+				pgd_index(current_stack_pointer());</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(pgd_none(*pgd)))</span>
<span class="p_add">+				set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-		pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="p_add">+		/* Stop remote flushes for the previous mm */</span>
<span class="p_add">+		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))</span>
<span class="p_add">+			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
 
<span class="p_del">-		if (unlikely(pgd_none(*pgd)))</span>
<span class="p_del">-			set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="p_del">-	}</span>
<span class="p_add">+		WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
 
<span class="p_del">-	this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_del">-	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, next-&gt;context.ctx_id);</span>
<span class="p_del">-	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_del">-		       atomic64_read(&amp;next-&gt;context.tlb_gen));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Start remote flushes and then read tlb_gen.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
 
<span class="p_del">-	WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="p_del">-	cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="p_add">+			  next-&gt;context.ctx_id);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Re-load page tables.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This logic has an ordering constraint:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_del">-	 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_del">-	 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_del">-	 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_del">-	 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_del">-	 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_del">-	 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_del">-	 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_del">-	 * execute full barriers to prevent this from happening.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_del">-	 * store to mm_cpumask and any operation that could load</span>
<span class="p_del">-	 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_del">-	 * due to instruction fetches or for no reason at all,</span>
<span class="p_del">-	 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_del">-	 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_del">-	 * ordering guarantee we need.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	load_cr3(next-&gt;pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="p_del">-	 * functions differently.  Tracing normally uses RCU, so we have to</span>
<span class="p_del">-	 * call the tracepoint specially here.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="p_add">+			       next-&gt;context.ctx_id);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_add">+			       next_tlb_gen);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_add">+		write_cr3(__pa(next-&gt;pgd));</span>
 
<span class="p_del">-	/* Stop flush ipis for the previous mm */</span>
<span class="p_del">-	WARN_ON_ONCE(!cpumask_test_cpu(cpu, mm_cpumask(real_prev)) &amp;&amp;</span>
<span class="p_del">-		     real_prev != &amp;init_mm);</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, mm_cpumask(real_prev));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This gets called via leave_mm() in the idle path where RCU</span>
<span class="p_add">+		 * functions differently.  Tracing normally uses RCU, so we</span>
<span class="p_add">+		 * have to call the tracepoint specially here.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_add">+					TLB_FLUSH_ALL);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	/* Load per-mm CR4 and LDTR state */</span>
 	load_mm_cr4(next);
 	switch_ldt(real_prev, next);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * The flush IPI assumes that a thread switch happens in this order:</span>
<span class="p_del">- * [cpu0: the cpu that switches]</span>
<span class="p_del">- * 1) switch_mm() either 1a) or 1b)</span>
<span class="p_del">- * 1a) thread switch to a different mm</span>
<span class="p_del">- * 1a1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="p_del">- *	Now the tlb flush NMI handler flush_tlb_func won&#39;t call leave_mm</span>
<span class="p_del">- *	if cpu0 was in lazy tlb mode.</span>
<span class="p_del">- * 1a2) update cpu active_mm</span>
<span class="p_del">- *	Now cpu0 accepts tlb flushes for the new mm.</span>
<span class="p_del">- * 1a3) cpu_set(cpu, new_mm-&gt;cpu_vm_mask);</span>
<span class="p_del">- *	Now the other cpus will send tlb flush ipis.</span>
<span class="p_del">- * 1a4) change cr3.</span>
<span class="p_del">- * 1a5) cpu_clear(cpu, old_mm-&gt;cpu_vm_mask);</span>
<span class="p_del">- *	Stop ipi delivery for the old mm. This is not synchronized with</span>
<span class="p_del">- *	the other cpus, but flush_tlb_func ignore flush ipis for the wrong</span>
<span class="p_del">- *	mm, and in the worst case we perform a superfluous tlb flush.</span>
<span class="p_del">- * 1b) thread switch without mm change</span>
<span class="p_del">- *	cpu active_mm is correct, cpu0 already handles flush ipis.</span>
<span class="p_del">- * 1b1) set cpu_tlbstate to TLBSTATE_OK</span>
<span class="p_del">- * 1b2) test_and_set the cpu bit in cpu_vm_mask.</span>
<span class="p_del">- *	Atomically set the bit [other cpus will start sending flush ipis],</span>
<span class="p_del">- *	and test the bit.</span>
<span class="p_del">- * 1b3) if the bit was 0: leave_mm was called, flush the tlb.</span>
<span class="p_del">- * 2) switch %%esp, ie current</span>
<span class="p_del">- *</span>
<span class="p_del">- * The interrupt must handle 2 special cases:</span>
<span class="p_del">- * - cr3 is changed before %%esp, ie. it cannot use current-&gt;{active_,}mm.</span>
<span class="p_del">- * - the cpu performs speculative tlb reads, i.e. even if the cpu only</span>
<span class="p_del">- *   runs in kernel space, the cpu could load tlb entries for user space</span>
<span class="p_del">- *   pages.</span>
<span class="p_del">- *</span>
<span class="p_del">- * The good news is that cpu_tlbstate is local to each cpu, no</span>
<span class="p_del">- * write/read ordering problems.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
 static void flush_tlb_func_common(const struct flush_tlb_info *f,
 				  bool local, enum tlb_flush_reason reason)
 {
<span class="p_chunk">@@ -215,12 +200,13 @@</span> <span class="p_context"> static void flush_tlb_func_common(const struct flush_tlb_info *f,</span>
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=
 		   loaded_mm-&gt;context.ctx_id);
 
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) != TLBSTATE_OK) {</span>
<span class="p_add">+	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {</span>
 		/*
<span class="p_del">-		 * leave_mm() is adequate to handle any type of flush, and</span>
<span class="p_del">-		 * we would prefer not to receive further IPIs.</span>
<span class="p_add">+		 * We&#39;re in lazy mode -- don&#39;t flush.  We can get here on</span>
<span class="p_add">+		 * remote flushes due to races and on local flushes if a</span>
<span class="p_add">+		 * kernel thread coincidentally flushes the mm it&#39;s lazily</span>
<span class="p_add">+		 * still using.</span>
 		 */
<span class="p_del">-		leave_mm(smp_processor_id());</span>
 		return;
 	}
 
<span class="p_chunk">@@ -317,6 +303,21 @@</span> <span class="p_context"> void native_flush_tlb_others(const struct cpumask *cpumask,</span>
 				(info-&gt;end - info-&gt;start) &gt;&gt; PAGE_SHIFT);
 
 	if (is_uv_system()) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This whole special case is confused.  UV has a &quot;Broadcast</span>
<span class="p_add">+		 * Assist Unit&quot;, which seems to be a fancy way to send IPIs.</span>
<span class="p_add">+		 * Back when x86 used an explicit TLB flush IPI, UV was</span>
<span class="p_add">+		 * optimized to use its own mechanism.  These days, x86 uses</span>
<span class="p_add">+		 * smp_call_function_many(), but UV still uses a manual IPI,</span>
<span class="p_add">+		 * and that IPI&#39;s action is out of date -- it does a manual</span>
<span class="p_add">+		 * flush instead of calling flush_tlb_func_remote().  This</span>
<span class="p_add">+		 * means that the percpu tlb_gen variables won&#39;t be updated</span>
<span class="p_add">+		 * and we&#39;ll do pointless flushes on future context switches.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Rather than hooking native_flush_tlb_others() here, I think</span>
<span class="p_add">+		 * that UV should be updated so that smp_call_function_many(),</span>
<span class="p_add">+		 * etc, are optimal on UV.</span>
<span class="p_add">+		 */</span>
 		unsigned int cpu;
 
 		cpu = smp_processor_id();
<span class="p_chunk">@@ -375,6 +376,7 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 
 	if (cpumask_any_but(mm_cpumask(mm), cpu) &lt; nr_cpu_ids)
 		flush_tlb_others(mm_cpumask(mm), &amp;info);
<span class="p_add">+</span>
 	put_cpu();
 }
 
<span class="p_chunk">@@ -383,8 +385,6 @@</span> <span class="p_context"> static void do_flush_tlb_all(void *info)</span>
 {
 	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
 	__flush_tlb_all();
<span class="p_del">-	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_LAZY)</span>
<span class="p_del">-		leave_mm(smp_processor_id());</span>
 }
 
 void flush_tlb_all(void)
<span class="p_chunk">@@ -436,6 +436,7 @@</span> <span class="p_context"> void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch)</span>
 
 	if (cpumask_any_but(&amp;batch-&gt;cpumask, cpu) &lt; nr_cpu_ids)
 		flush_tlb_others(&amp;batch-&gt;cpumask, &amp;info);
<span class="p_add">+</span>
 	cpumask_clear(&amp;batch-&gt;cpumask);
 
 	put_cpu();
<span class="p_header">diff --git a/arch/x86/xen/mmu_pv.c b/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">index 1d7a7213a310..f5df56fb8b5c 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu_pv.c</span>
<span class="p_chunk">@@ -1005,8 +1005,7 @@</span> <span class="p_context"> static void xen_drop_mm_ref(struct mm_struct *mm)</span>
 	/* Get the &quot;official&quot; set of cpus referring to our pagetable. */
 	if (!alloc_cpumask_var(&amp;mask, GFP_ATOMIC)) {
 		for_each_online_cpu(cpu) {
<span class="p_del">-			if (!cpumask_test_cpu(cpu, mm_cpumask(mm))</span>
<span class="p_del">-			    &amp;&amp; per_cpu(xen_current_cr3, cpu) != __pa(mm-&gt;pgd))</span>
<span class="p_add">+			if (per_cpu(xen_current_cr3, cpu) != __pa(mm-&gt;pgd))</span>
 				continue;
 			smp_call_function_single(cpu, drop_mm_ref_this_cpu, mm, 1);
 		}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



