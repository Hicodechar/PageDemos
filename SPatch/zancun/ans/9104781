
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] scheduler changes for v4.7 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] scheduler changes for v4.7</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 16, 2016, 5:08 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160516170812.GA4284@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9104781/mbox/"
   >mbox</a>
|
   <a href="/patch/9104781/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9104781/">/patch/9104781/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id F1782BF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:08:45 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id EFD39202C8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:08:37 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 88904202A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 16 May 2016 17:08:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753927AbcEPRIY (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 16 May 2016 13:08:24 -0400
Received: from mail-wm0-f65.google.com ([74.125.82.65]:35565 &quot;EHLO
	mail-wm0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753274AbcEPRIT (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 16 May 2016 13:08:19 -0400
Received: by mail-wm0-f65.google.com with SMTP id e201so19502751wme.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 16 May 2016 10:08:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=t0P5mbJo3lkCbSZrQnXZLh3XP8jghvW7v3V2LQnbzDQ=;
	b=dc3Od+7NP0qUYu8W5mfcqVLl0cBZTgaaSsKMiO4eBvCW+vN8nSMpAk+OnC5Z48/iOJ
	LyVNOUeJUAY8kGHdoXS/+5nlQPlUdVLH9J/U+nYTdakHmbfw+caFMwBl5EwGuZXNj9Ik
	xpVWG+oFPqah1g1mW5sGpKbxbSdjSgY+A6oBXHwFz6ZVudH6Lds/TM4IGOTAvCyMX/Og
	5Xa7YlHf+x9rEuv42NELbZkELTSoTXUtclZAuZZaf7V5wi0a01NM6MKYau4zjMCsCPKu
	8lb3/AC3Oon4omGCpYZZvqqcHMSbHDj5NhlDt3jfOaaa0jA5zS6B84f/ymBaaTDtwjSa
	ZGxg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=t0P5mbJo3lkCbSZrQnXZLh3XP8jghvW7v3V2LQnbzDQ=;
	b=AINaNIIC6YVHd/1gVLbw/+ZFFrloJFt3UzU0cmwA6WUjfFQSIy4U1Ny3GB9oFznAon
	juuXxr/BIgDzncmbOhBz1EWTOQJXh/shlAHyZlii0ljFg34K9eIZIlLDjj8jA8WpN45M
	X8yn8Gh4M49DD7Z53I0miKerL6qTosZzV3UYPpjS1VivboqVX2pjWhQgfF86Pua0DVIg
	pv4QDArwtIQpUXmCase1PI8pJHaGWVuqGOptP8bR5exEHubXZFsnTSdKh6WB5Esu2fb4
	jawygpTPUtGoCTpWyDW14ioesSTWbwUA4KM+Z329lLSdD9z7yMhdIVr0EAUTG9ZxEBSJ
	Vcwg==
X-Gm-Message-State: AOPr4FU7mFxUm0y3I3g7LzRLitoFndnISqgd+/8SZZzo+9uVOnTVIR84BjQ3nUm0WD/g3A==
X-Received: by 10.194.246.138 with SMTP id
	xw10mr28939842wjc.99.1463418497206; 
	Mon, 16 May 2016 10:08:17 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	b22sm19515512wmb.9.2016.05.16.10.08.15
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 16 May 2016 10:08:15 -0700 (PDT)
Date: Mon, 16 May 2016 19:08:12 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Mike Galbraith &lt;efault@gmx.de&gt;, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] scheduler changes for v4.7
Message-ID: &lt;20160516170812.GA4284@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.2 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=ham version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - May 16, 2016, 5:08 p.m.</div>
<pre class="content">
Linus,

Please pull the latest sched-core-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git sched-core-for-linus

   # HEAD: ef0491ea17f8019821c7e9c8e801184ecf17f85a ARM: Hide finish_arch_post_lock_switch() from modules

 - massive CPU hotplug rework (Thomas Gleixner)

 - improve migration fairness (Peter Zijlstra)

 - CPU load calculation updates/cleanups (Yuyang Du)

 - cpufreq updates (Steve Muckle)

 - nohz optimizations (Frederic Weisbecker)

 - switch_mm() micro-optimization on x86 (Andy Lutomirski)

 - ... lots of other enhancements, fixes and cleanups.

 Thanks,

	Ingo

------------------&gt;
Alexander Shishkin (1):
      perf/core, sched: Don&#39;t use clock function pointer to determine clock

Andy Lutomirski (5):
      sched/core, ARM: Include linux/preempt.h from asm/mmu_context.h
      sched/core: Add switch_mm_irqs_off() and use it in the scheduler
      x86/mm: Build arch/x86/mm/tlb.c even on !SMP
      x86/mm, sched/core: Uninline switch_mm()
      x86/mm, sched/core: Turn off IRQs in switch_mm()

Anton Blanchard (1):
      sched/cpuacct: Check for NULL when using task_pt_regs()

Daniel Lezcano (2):
      sched/clock: Remove pointless test in cpu_clock/local_clock
      sched/clock: Make local_clock()/cpu_clock() inline

Davidlohr Bueso (1):
      sched/core: Fix comment typo in wake_q_add()

Dietmar Eggemann (2):
      sched/fair: Remove stale power aware scheduling comments
      sched/fair: Fix comment in calculate_imbalance()

Dongsheng Yang (1):
      sched/cpuacct: Split usage accounting into user_usage and sys_usage

Frederic Weisbecker (3):
      sched/fair: Gather CPU load functions under a more conventional namespace
      sched/fair: Correctly handle nohz ticks CPU load accounting
      sched/fair: Optimize !CONFIG_NO_HZ_COMMON CPU load updates

Ingo Molnar (1):
      mm/mmu_context, sched/core: Fix mmu_context.h assumption

Matt Fleming (1):
      sched/fair: Update rq clock before updating nohz CPU load

Morten Rasmussen (1):
      sched/fair: Correct unit of load_above_capacity

Muhammad Falak R Wani (1):
      sched/core: Remove unused variable

Peter Zijlstra (10):
      sched/core: Move task_rq_lock() out of line
      sched/core: Introduce &#39;struct rq_flags&#39;
      locking/lockdep, sched/core: Implement a better lock pinning scheme
      sched/core: Enable increased load resolution on 64-bit kernels
      sched/hotplug: Move sync_rcu to be with set_cpu_active(false)
      sched/fair: Move record_wakee()
      sched/fair: Prepare to fix fairness problems on migration
      sched/core: Kill sched_class::task_waking to clean up the migration logic
      sched/fair: Fix fairness issue on migration
      sched/fair: Clean up scale confusion

Peter Zijlstra (Intel) (1):
      sched: Allow per-cpu kernel threads to run on online &amp;&amp; !active

Rabin Vincent (1):
      sched/debug: Don&#39;t dump sched debug info in SysRq-W

Srikar Dronamraju (2):
      sched/fair: Reset nr_balance_failed after active balancing
      sched/fair: Fix asym packing to select correct CPU

Steve Muckle (3):
      sched/fair: Move cpufreq hook to update_cfs_rq_load_avg()
      sched/fair: Do not call cpufreq hook unless util changed
      sched/fair: Call cpufreq hook in additional paths

Steven Rostedt (2):
      sched/core: Add preempt checks in preempt_schedule() code
      ARM: Hide finish_arch_post_lock_switch() from modules

Thomas Gleixner (14):
      sched: Make set_cpu_rq_start_time() a built in hotplug state
      sched: Allow hotplug notifiers to be setup early
      sched: Consolidate the notifier maze
      sched: Move sched_domains_numa_masks_clear() to DOWN_PREPARE
      sched/hotplug: Convert cpu_[in]active notifiers to state machine
      sched/migration: Move prepare transition to SCHED_STARTING state
      sched/migration: Move calc_load_migrate() into CPU_DYING
      sched/migration: Move CPU_ONLINE into scheduler state
      sched/hotplug: Move migration CPU_DYING to sched_cpu_dying()
      sched/hotplug: Make activate() the last hotplug step
      sched/fair: Make ilb_notifier an explicit call
      sched: Make hrtick_notifier an explicit call
      sched/core: Use tsk_cpus_allowed() instead of accessing -&gt;cpus_allowed
      sched/core: Provide a tsk_nr_cpus_allowed() helper

Tim Chen (1):
      sched/numa: Remove unnecessary NUMA dequeue update from non-SMP kernels

Vik Heyndrickx (1):
      sched/loadavg: Fix loadavg artifacts on fully idle and on fully loaded systems

Wanpeng Li (3):
      sched/cpufreq: Optimize cpufreq update kicker to avoid update multiple times
      sched/debug: Print out idle balance values even on !CONFIG_SCHEDSTATS kernels
      sched/nohz: Fix affine unpinned timers mess

Xunlei Pang (1):
      sched/deadline: Fix a bug in dl_overflow()

Yuyang Du (6):
      sched/fair: Update comments after a variable rename
      sched/fair: Initiate a new task&#39;s util avg to a bounded value
      sched/fair: Generalize the load/util averages resolution definition
      sched/fair: Rename SCHED_LOAD_SHIFT to NICE_0_LOAD_SHIFT and remove SCHED_LOAD_SCALE
      sched/fair: Add detailed description to the sched load avg metrics
      sched/fair: Optimize sum computation with a lookup table

Zhao Lei (1):
      sched/cpuacct: Show all possible CPUs in cpuacct output


 Documentation/trace/ftrace.txt     |  10 +-
 arch/arm/include/asm/mmu_context.h |   3 +
 arch/powerpc/kernel/smp.c          |   2 +-
 arch/s390/kernel/smp.c             |   2 +-
 arch/x86/events/core.c             |   2 +-
 arch/x86/include/asm/mmu_context.h | 101 +----
 arch/x86/mm/Makefile               |   3 +-
 arch/x86/mm/tlb.c                  | 116 ++++++
 include/linux/cpu.h                |  18 -
 include/linux/cpuhotplug.h         |   2 +
 include/linux/cpumask.h            |   6 +-
 include/linux/lockdep.h            |  23 +-
 include/linux/mmu_context.h        |   7 +
 include/linux/sched.h              | 124 +++++-
 kernel/cpu.c                       |  32 +-
 kernel/locking/lockdep.c           |  71 +++-
 kernel/sched/clock.c               |  48 +--
 kernel/sched/core.c                | 749 +++++++++++++++++++++----------------
 kernel/sched/cpuacct.c             | 147 ++++++--
 kernel/sched/cpudeadline.c         |   4 +-
 kernel/sched/cpupri.c              |   4 +-
 kernel/sched/deadline.c            |  55 +--
 kernel/sched/debug.c               |  10 +-
 kernel/sched/fair.c                | 513 ++++++++++++++++---------
 kernel/sched/idle_task.c           |   2 +-
 kernel/sched/loadavg.c             |  11 +-
 kernel/sched/rt.c                  |  38 +-
 kernel/sched/sched.h               | 140 +++----
 kernel/sched/stop_task.c           |   2 +-
 kernel/time/tick-sched.c           |   9 +-
 mm/mmu_context.c                   |   2 +-
 31 files changed, 1329 insertions(+), 927 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/trace/ftrace.txt b/Documentation/trace/ftrace.txt</span>
<span class="p_header">index f52f297cb406..9857606dd7b7 100644</span>
<span class="p_header">--- a/Documentation/trace/ftrace.txt</span>
<span class="p_header">+++ b/Documentation/trace/ftrace.txt</span>
<span class="p_chunk">@@ -1562,12 +1562,12 @@</span> <span class="p_context"> Doing the same with chrt -r 5 and function-trace set.</span>
   &lt;idle&gt;-0       3dN.1   12us : menu_hrtimer_cancel &lt;-tick_nohz_idle_exit
   &lt;idle&gt;-0       3dN.1   12us : ktime_get &lt;-tick_nohz_idle_exit
   &lt;idle&gt;-0       3dN.1   12us : tick_do_update_jiffies64 &lt;-tick_nohz_idle_exit
<span class="p_del">-  &lt;idle&gt;-0       3dN.1   13us : update_cpu_load_nohz &lt;-tick_nohz_idle_exit</span>
<span class="p_del">-  &lt;idle&gt;-0       3dN.1   13us : _raw_spin_lock &lt;-update_cpu_load_nohz</span>
<span class="p_add">+  &lt;idle&gt;-0       3dN.1   13us : cpu_load_update_nohz &lt;-tick_nohz_idle_exit</span>
<span class="p_add">+  &lt;idle&gt;-0       3dN.1   13us : _raw_spin_lock &lt;-cpu_load_update_nohz</span>
   &lt;idle&gt;-0       3dN.1   13us : add_preempt_count &lt;-_raw_spin_lock
<span class="p_del">-  &lt;idle&gt;-0       3dN.2   13us : __update_cpu_load &lt;-update_cpu_load_nohz</span>
<span class="p_del">-  &lt;idle&gt;-0       3dN.2   14us : sched_avg_update &lt;-__update_cpu_load</span>
<span class="p_del">-  &lt;idle&gt;-0       3dN.2   14us : _raw_spin_unlock &lt;-update_cpu_load_nohz</span>
<span class="p_add">+  &lt;idle&gt;-0       3dN.2   13us : __cpu_load_update &lt;-cpu_load_update_nohz</span>
<span class="p_add">+  &lt;idle&gt;-0       3dN.2   14us : sched_avg_update &lt;-__cpu_load_update</span>
<span class="p_add">+  &lt;idle&gt;-0       3dN.2   14us : _raw_spin_unlock &lt;-cpu_load_update_nohz</span>
   &lt;idle&gt;-0       3dN.2   14us : sub_preempt_count &lt;-_raw_spin_unlock
   &lt;idle&gt;-0       3dN.1   15us : calc_load_exit_idle &lt;-tick_nohz_idle_exit
   &lt;idle&gt;-0       3dN.1   15us : touch_softlockup_watchdog &lt;-tick_nohz_idle_exit
<span class="p_header">diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h</span>
<span class="p_header">index fa5b42d44985..3cc14dd8587c 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/compiler.h&gt;
 #include &lt;linux/sched.h&gt;
<span class="p_add">+#include &lt;linux/preempt.h&gt;</span>
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/cachetype.h&gt;
 #include &lt;asm/proc-fns.h&gt;
<span class="p_chunk">@@ -66,6 +67,7 @@</span> <span class="p_context"> static inline void check_and_switch_context(struct mm_struct *mm,</span>
 		cpu_switch_mm(mm-&gt;pgd, mm);
 }
 
<span class="p_add">+#ifndef MODULE</span>
 #define finish_arch_post_lock_switch \
 	finish_arch_post_lock_switch
 static inline void finish_arch_post_lock_switch(void)
<span class="p_chunk">@@ -87,6 +89,7 @@</span> <span class="p_context"> static inline void finish_arch_post_lock_switch(void)</span>
 		preempt_enable_no_resched();
 	}
 }
<span class="p_add">+#endif /* !MODULE */</span>
 
 #endif	/* CONFIG_MMU */
 
<span class="p_header">diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c</span>
<span class="p_header">index 8cac1eb41466..55c924b65f71 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/smp.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/smp.c</span>
<span class="p_chunk">@@ -565,7 +565,7 @@</span> <span class="p_context"> int __cpu_up(unsigned int cpu, struct task_struct *tidle)</span>
 		smp_ops-&gt;give_timebase();
 
 	/* Wait until cpu puts itself in the online &amp; active maps */
<span class="p_del">-	while (!cpu_online(cpu) || !cpu_active(cpu))</span>
<span class="p_add">+	while (!cpu_online(cpu))</span>
 		cpu_relax();
 
 	return 0;
<span class="p_header">diff --git a/arch/s390/kernel/smp.c b/arch/s390/kernel/smp.c</span>
<span class="p_header">index 40a6b4f9c36c..7b89a7572100 100644</span>
<span class="p_header">--- a/arch/s390/kernel/smp.c</span>
<span class="p_header">+++ b/arch/s390/kernel/smp.c</span>
<span class="p_chunk">@@ -832,7 +832,7 @@</span> <span class="p_context"> int __cpu_up(unsigned int cpu, struct task_struct *tidle)</span>
 	pcpu_attach_task(pcpu, tidle);
 	pcpu_start_fn(pcpu, smp_start_secondary, NULL);
 	/* Wait until cpu puts itself in the online &amp; active maps */
<span class="p_del">-	while (!cpu_online(cpu) || !cpu_active(cpu))</span>
<span class="p_add">+	while (!cpu_online(cpu))</span>
 		cpu_relax();
 	return 0;
 }
<span class="p_header">diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c</span>
<span class="p_header">index 041e442a3e28..dd39fde66b54 100644</span>
<span class="p_header">--- a/arch/x86/events/core.c</span>
<span class="p_header">+++ b/arch/x86/events/core.c</span>
<span class="p_chunk">@@ -2177,7 +2177,7 @@</span> <span class="p_context"> void arch_perf_update_userpage(struct perf_event *event,</span>
 	 * cap_user_time_zero doesn&#39;t make sense when we&#39;re using a different
 	 * time base for the records.
 	 */
<span class="p_del">-	if (event-&gt;clock == &amp;local_clock) {</span>
<span class="p_add">+	if (!event-&gt;attr.use_clockid) {</span>
 		userpg-&gt;cap_user_time_zero = 1;
 		userpg-&gt;time_zero = data-&gt;cyc2ns_offset;
 	}
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 84280029cafd..396348196aa7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -115,103 +115,12 @@</span> <span class="p_context"> static inline void destroy_context(struct mm_struct *mm)</span>
 	destroy_context_ldt(mm);
 }
 
<span class="p_del">-static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-			     struct task_struct *tsk)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned cpu = smp_processor_id();</span>
<span class="p_add">+extern void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+		      struct task_struct *tsk);</span>
 
<span class="p_del">-	if (likely(prev != next)) {</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.active_mm, next);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Re-load page tables.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * This logic has an ordering constraint:</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_del">-		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_del">-		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_del">-		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_del">-		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_del">-		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_del">-		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_del">-		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_del">-		 * execute full barriers to prevent this from happening.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_del">-		 * store to mm_cpumask and any operation that could load</span>
<span class="p_del">-		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_del">-		 * due to instruction fetches or for no reason at all,</span>
<span class="p_del">-		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_del">-		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_del">-		 * ordering guarantee we need.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		load_cr3(next-&gt;pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Stop flush ipis for the previous mm */</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, mm_cpumask(prev));</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Load per-mm CR4 state */</span>
<span class="p_del">-		load_mm_cr4(next);</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_MODIFY_LDT_SYSCALL</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Load the LDT, if the LDT is different.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * It&#39;s possible that prev-&gt;context.ldt doesn&#39;t match</span>
<span class="p_del">-		 * the LDT register.  This can happen if leave_mm(prev)</span>
<span class="p_del">-		 * was called and then modify_ldt changed</span>
<span class="p_del">-		 * prev-&gt;context.ldt but suppressed an IPI to this CPU.</span>
<span class="p_del">-		 * In this case, prev-&gt;context.ldt != NULL, because we</span>
<span class="p_del">-		 * never set context.ldt to NULL while the mm still</span>
<span class="p_del">-		 * exists.  That means that next-&gt;context.ldt !=</span>
<span class="p_del">-		 * prev-&gt;context.ldt, because mms never share an LDT.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))</span>
<span class="p_del">-			load_mm_ldt(next);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	}</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-	  else {</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_del">-		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * On established mms, the mm_cpumask is only changed</span>
<span class="p_del">-			 * from irq context, from ptep_clear_flush() while in</span>
<span class="p_del">-			 * lazy tlb mode, and here. Irqs are blocked during</span>
<span class="p_del">-			 * schedule, protecting us from simultaneous changes.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_del">-</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * We were in lazy tlb mode and leave_mm disabled</span>
<span class="p_del">-			 * tlb flush IPI delivery. We must reload CR3</span>
<span class="p_del">-			 * to make sure to use no freed page tables.</span>
<span class="p_del">-			 *</span>
<span class="p_del">-			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_del">-			 * fills with respect to the mm_cpumask write.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			load_cr3(next-&gt;pgd);</span>
<span class="p_del">-			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_del">-			load_mm_cr4(next);</span>
<span class="p_del">-			load_mm_ldt(next);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_add">+extern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			       struct task_struct *tsk);</span>
<span class="p_add">+#define switch_mm_irqs_off switch_mm_irqs_off</span>
 
 #define activate_mm(prev, next)			\
 do {						\
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index f98913258c63..62c0043a5fd5 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 KCOV_INSTRUMENT_tlb.o	:= n
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_del">-	    pat.o pgtable.o physaddr.o gup.o setup_nx.o</span>
<span class="p_add">+	    pat.o pgtable.o physaddr.o gup.o setup_nx.o tlb.o</span>
 
 # Make sure __phys_addr has no stackprotector
 nostackp := $(call cc-option, -fno-stack-protector)
<span class="p_chunk">@@ -12,7 +12,6 @@</span> <span class="p_context"> CFLAGS_setup_nx.o		:= $(nostackp)</span>
 CFLAGS_fault.o := -I$(src)/../include/asm/trace
 
 obj-$(CONFIG_X86_PAT)		+= pat_rbtree.o
<span class="p_del">-obj-$(CONFIG_SMP)		+= tlb.o</span>
 
 obj-$(CONFIG_X86_32)		+= pgtable_32.o iomap_32.o
 
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index fe9b9f776361..5643fd0b1a7d 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -28,6 +28,8 @@</span> <span class="p_context"></span>
  *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi
  */
 
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+</span>
 struct flush_tlb_info {
 	struct mm_struct *flush_mm;
 	unsigned long flush_start;
<span class="p_chunk">@@ -57,6 +59,118 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 }
 EXPORT_SYMBOL_GPL(leave_mm);
 
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+	       struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	switch_mm_irqs_off(prev, next, tsk);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (likely(prev != next)) {</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.active_mm, next);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Re-load page tables.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * This logic has an ordering constraint:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_add">+		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_add">+		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_add">+		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_add">+		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_add">+		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_add">+		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_add">+		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_add">+		 * execute full barriers to prevent this from happening.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_add">+		 * store to mm_cpumask and any operation that could load</span>
<span class="p_add">+		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_add">+		 * due to instruction fetches or for no reason at all,</span>
<span class="p_add">+		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_add">+		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_add">+		 * ordering guarantee we need.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		load_cr3(next-&gt;pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Stop flush ipis for the previous mm */</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, mm_cpumask(prev));</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Load per-mm CR4 state */</span>
<span class="p_add">+		load_mm_cr4(next);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MODIFY_LDT_SYSCALL</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Load the LDT, if the LDT is different.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * It&#39;s possible that prev-&gt;context.ldt doesn&#39;t match</span>
<span class="p_add">+		 * the LDT register.  This can happen if leave_mm(prev)</span>
<span class="p_add">+		 * was called and then modify_ldt changed</span>
<span class="p_add">+		 * prev-&gt;context.ldt but suppressed an IPI to this CPU.</span>
<span class="p_add">+		 * In this case, prev-&gt;context.ldt != NULL, because we</span>
<span class="p_add">+		 * never set context.ldt to NULL while the mm still</span>
<span class="p_add">+		 * exists.  That means that next-&gt;context.ldt !=</span>
<span class="p_add">+		 * prev-&gt;context.ldt, because mms never share an LDT.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))</span>
<span class="p_add">+			load_mm_ldt(next);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	}</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	  else {</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);</span>
<span class="p_add">+		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!cpumask_test_cpu(cpu, mm_cpumask(next))) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * On established mms, the mm_cpumask is only changed</span>
<span class="p_add">+			 * from irq context, from ptep_clear_flush() while in</span>
<span class="p_add">+			 * lazy tlb mode, and here. Irqs are blocked during</span>
<span class="p_add">+			 * schedule, protecting us from simultaneous changes.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We were in lazy tlb mode and leave_mm disabled</span>
<span class="p_add">+			 * tlb flush IPI delivery. We must reload CR3</span>
<span class="p_add">+			 * to make sure to use no freed page tables.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_add">+			 * fills with respect to the mm_cpumask write.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			load_cr3(next-&gt;pgd);</span>
<span class="p_add">+			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_add">+			load_mm_cr4(next);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+</span>
 /*
  * The flush IPI assumes that a thread switch happens in this order:
  * [cpu0: the cpu that switches]
<span class="p_chunk">@@ -353,3 +467,5 @@</span> <span class="p_context"> static int __init create_tlb_single_page_flush_ceiling(void)</span>
 	return 0;
 }
 late_initcall(create_tlb_single_page_flush_ceiling);
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_header">diff --git a/include/linux/cpu.h b/include/linux/cpu.h</span>
<span class="p_header">index f9b1fab4388a..21597dcac0e2 100644</span>
<span class="p_header">--- a/include/linux/cpu.h</span>
<span class="p_header">+++ b/include/linux/cpu.h</span>
<span class="p_chunk">@@ -59,25 +59,7 @@</span> <span class="p_context"> struct notifier_block;</span>
  * CPU notifier priorities.
  */
 enum {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * SCHED_ACTIVE marks a cpu which is coming up active during</span>
<span class="p_del">-	 * CPU_ONLINE and CPU_DOWN_FAILED and must be the first</span>
<span class="p_del">-	 * notifier.  CPUSET_ACTIVE adjusts cpuset according to</span>
<span class="p_del">-	 * cpu_active mask right after SCHED_ACTIVE.  During</span>
<span class="p_del">-	 * CPU_DOWN_PREPARE, SCHED_INACTIVE and CPUSET_INACTIVE are</span>
<span class="p_del">-	 * ordered in the similar way.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This ordering guarantees consistent cpu_active mask and</span>
<span class="p_del">-	 * migration behavior to all cpu notifiers.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	CPU_PRI_SCHED_ACTIVE	= INT_MAX,</span>
<span class="p_del">-	CPU_PRI_CPUSET_ACTIVE	= INT_MAX - 1,</span>
<span class="p_del">-	CPU_PRI_SCHED_INACTIVE	= INT_MIN + 1,</span>
<span class="p_del">-	CPU_PRI_CPUSET_INACTIVE	= INT_MIN,</span>
<span class="p_del">-</span>
<span class="p_del">-	/* migration should happen before other stuff but after perf */</span>
 	CPU_PRI_PERF		= 20,
<span class="p_del">-	CPU_PRI_MIGRATION	= 10,</span>
 
 	/* bring up workqueues before normal notifiers and down after */
 	CPU_PRI_WORKQUEUE_UP	= 5,
<span class="p_header">diff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h</span>
<span class="p_header">index 5d68e15e46b7..386374d19987 100644</span>
<span class="p_header">--- a/include/linux/cpuhotplug.h</span>
<span class="p_header">+++ b/include/linux/cpuhotplug.h</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"> enum cpuhp_state {</span>
 	CPUHP_BRINGUP_CPU,
 	CPUHP_AP_IDLE_DEAD,
 	CPUHP_AP_OFFLINE,
<span class="p_add">+	CPUHP_AP_SCHED_STARTING,</span>
 	CPUHP_AP_NOTIFY_STARTING,
 	CPUHP_AP_ONLINE,
 	CPUHP_TEARDOWN_CPU,
<span class="p_chunk">@@ -16,6 +17,7 @@</span> <span class="p_context"> enum cpuhp_state {</span>
 	CPUHP_AP_NOTIFY_ONLINE,
 	CPUHP_AP_ONLINE_DYN,
 	CPUHP_AP_ONLINE_DYN_END		= CPUHP_AP_ONLINE_DYN + 30,
<span class="p_add">+	CPUHP_AP_ACTIVE,</span>
 	CPUHP_ONLINE,
 };
 
<span class="p_header">diff --git a/include/linux/cpumask.h b/include/linux/cpumask.h</span>
<span class="p_header">index 40cee6b77a93..e828cf65d7df 100644</span>
<span class="p_header">--- a/include/linux/cpumask.h</span>
<span class="p_header">+++ b/include/linux/cpumask.h</span>
<span class="p_chunk">@@ -743,12 +743,10 @@</span> <span class="p_context"> set_cpu_present(unsigned int cpu, bool present)</span>
 static inline void
 set_cpu_online(unsigned int cpu, bool online)
 {
<span class="p_del">-	if (online) {</span>
<span class="p_add">+	if (online)</span>
 		cpumask_set_cpu(cpu, &amp;__cpu_online_mask);
<span class="p_del">-		cpumask_set_cpu(cpu, &amp;__cpu_active_mask);</span>
<span class="p_del">-	} else {</span>
<span class="p_add">+	else</span>
 		cpumask_clear_cpu(cpu, &amp;__cpu_online_mask);
<span class="p_del">-	}</span>
 }
 
 static inline void
<span class="p_header">diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h</span>
<span class="p_header">index d10ef06971b5..fb7d87e45fbe 100644</span>
<span class="p_header">--- a/include/linux/lockdep.h</span>
<span class="p_header">+++ b/include/linux/lockdep.h</span>
<span class="p_chunk">@@ -356,8 +356,13 @@</span> <span class="p_context"> extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);</span>
 extern void lockdep_clear_current_reclaim_state(void);
 extern void lockdep_trace_alloc(gfp_t mask);
 
<span class="p_del">-extern void lock_pin_lock(struct lockdep_map *lock);</span>
<span class="p_del">-extern void lock_unpin_lock(struct lockdep_map *lock);</span>
<span class="p_add">+struct pin_cookie { unsigned int val; };</span>
<span class="p_add">+</span>
<span class="p_add">+#define NIL_COOKIE (struct pin_cookie){ .val = 0U, }</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);</span>
<span class="p_add">+extern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);</span>
<span class="p_add">+extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);</span>
 
 # define INIT_LOCKDEP				.lockdep_recursion = 0, .lockdep_reclaim_gfp = 0,
 
<span class="p_chunk">@@ -373,8 +378,9 @@</span> <span class="p_context"> extern void lock_unpin_lock(struct lockdep_map *lock);</span>
 
 #define lockdep_recursing(tsk)	((tsk)-&gt;lockdep_recursion)
 
<span class="p_del">-#define lockdep_pin_lock(l)		lock_pin_lock(&amp;(l)-&gt;dep_map)</span>
<span class="p_del">-#define lockdep_unpin_lock(l)	lock_unpin_lock(&amp;(l)-&gt;dep_map)</span>
<span class="p_add">+#define lockdep_pin_lock(l)	lock_pin_lock(&amp;(l)-&gt;dep_map)</span>
<span class="p_add">+#define lockdep_repin_lock(l,c)	lock_repin_lock(&amp;(l)-&gt;dep_map, (c))</span>
<span class="p_add">+#define lockdep_unpin_lock(l,c)	lock_unpin_lock(&amp;(l)-&gt;dep_map, (c))</span>
 
 #else /* !CONFIG_LOCKDEP */
 
<span class="p_chunk">@@ -427,8 +433,13 @@</span> <span class="p_context"> struct lock_class_key { };</span>
 
 #define lockdep_recursing(tsk)			(0)
 
<span class="p_del">-#define lockdep_pin_lock(l)				do { (void)(l); } while (0)</span>
<span class="p_del">-#define lockdep_unpin_lock(l)			do { (void)(l); } while (0)</span>
<span class="p_add">+struct pin_cookie { };</span>
<span class="p_add">+</span>
<span class="p_add">+#define NIL_COOKIE (struct pin_cookie){ }</span>
<span class="p_add">+</span>
<span class="p_add">+#define lockdep_pin_lock(l)			({ struct pin_cookie cookie; cookie; })</span>
<span class="p_add">+#define lockdep_repin_lock(l, c)		do { (void)(l); (void)(c); } while (0)</span>
<span class="p_add">+#define lockdep_unpin_lock(l, c)		do { (void)(l); (void)(c); } while (0)</span>
 
 #endif /* !LOCKDEP */
 
<span class="p_header">diff --git a/include/linux/mmu_context.h b/include/linux/mmu_context.h</span>
<span class="p_header">index 70fffeba7495..a4441784503b 100644</span>
<span class="p_header">--- a/include/linux/mmu_context.h</span>
<span class="p_header">+++ b/include/linux/mmu_context.h</span>
<span class="p_chunk">@@ -1,9 +1,16 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_MMU_CONTEXT_H
 #define _LINUX_MMU_CONTEXT_H
 
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+</span>
 struct mm_struct;
 
 void use_mm(struct mm_struct *mm);
 void unuse_mm(struct mm_struct *mm);
 
<span class="p_add">+/* Architectures that care about IRQ state in switch_mm can override this. */</span>
<span class="p_add">+#ifndef switch_mm_irqs_off</span>
<span class="p_add">+# define switch_mm_irqs_off switch_mm</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 52c4847b05e2..38526b67e787 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -178,9 +178,11 @@</span> <span class="p_context"> extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);</span>
 extern void calc_global_load(unsigned long ticks);
 
 #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_NO_HZ_COMMON)
<span class="p_del">-extern void update_cpu_load_nohz(int active);</span>
<span class="p_add">+extern void cpu_load_update_nohz_start(void);</span>
<span class="p_add">+extern void cpu_load_update_nohz_stop(void);</span>
 #else
<span class="p_del">-static inline void update_cpu_load_nohz(int active) { }</span>
<span class="p_add">+static inline void cpu_load_update_nohz_start(void) { }</span>
<span class="p_add">+static inline void cpu_load_update_nohz_stop(void) { }</span>
 #endif
 
 extern void dump_cpu_task(int cpu);
<span class="p_chunk">@@ -372,6 +374,15 @@</span> <span class="p_context"> extern void cpu_init (void);</span>
 extern void trap_init(void);
 extern void update_process_times(int user);
 extern void scheduler_tick(void);
<span class="p_add">+extern int sched_cpu_starting(unsigned int cpu);</span>
<span class="p_add">+extern int sched_cpu_activate(unsigned int cpu);</span>
<span class="p_add">+extern int sched_cpu_deactivate(unsigned int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_add">+extern int sched_cpu_dying(unsigned int cpu);</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define sched_cpu_dying	NULL</span>
<span class="p_add">+#endif</span>
 
 extern void sched_show_task(struct task_struct *p);
 
<span class="p_chunk">@@ -935,9 +946,19 @@</span> <span class="p_context"> enum cpu_idle_type {</span>
 };
 
 /*
<span class="p_add">+ * Integer metrics need fixed point arithmetic, e.g., sched/fair</span>
<span class="p_add">+ * has a few: load, load_avg, util_avg, freq, and capacity.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We define a basic fixed point arithmetic range, and then formalize</span>
<span class="p_add">+ * all these metrics based on that basic range.</span>
<span class="p_add">+ */</span>
<span class="p_add">+# define SCHED_FIXEDPOINT_SHIFT	10</span>
<span class="p_add">+# define SCHED_FIXEDPOINT_SCALE	(1L &lt;&lt; SCHED_FIXEDPOINT_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Increase resolution of cpu_capacity calculations
  */
<span class="p_del">-#define SCHED_CAPACITY_SHIFT	10</span>
<span class="p_add">+#define SCHED_CAPACITY_SHIFT	SCHED_FIXEDPOINT_SHIFT</span>
 #define SCHED_CAPACITY_SCALE	(1L &lt;&lt; SCHED_CAPACITY_SHIFT)
 
 /*
<span class="p_chunk">@@ -1199,18 +1220,56 @@</span> <span class="p_context"> struct load_weight {</span>
 };
 
 /*
<span class="p_del">- * The load_avg/util_avg accumulates an infinite geometric series.</span>
<span class="p_del">- * 1) load_avg factors frequency scaling into the amount of time that a</span>
<span class="p_del">- * sched_entity is runnable on a rq into its weight. For cfs_rq, it is the</span>
<span class="p_del">- * aggregated such weights of all runnable and blocked sched_entities.</span>
<span class="p_del">- * 2) util_avg factors frequency and cpu scaling into the amount of time</span>
<span class="p_del">- * that a sched_entity is running on a CPU, in the range [0..SCHED_LOAD_SCALE].</span>
<span class="p_del">- * For cfs_rq, it is the aggregated such times of all runnable and</span>
<span class="p_add">+ * The load_avg/util_avg accumulates an infinite geometric series</span>
<span class="p_add">+ * (see __update_load_avg() in kernel/sched/fair.c).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * [load_avg definition]</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   load_avg = runnable% * scale_load_down(load)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * where runnable% is the time ratio that a sched_entity is runnable.</span>
<span class="p_add">+ * For cfs_rq, it is the aggregated load_avg of all runnable and</span>
  * blocked sched_entities.
<span class="p_del">- * The 64 bit load_sum can:</span>
<span class="p_del">- * 1) for cfs_rq, afford 4353082796 (=2^64/47742/88761) entities with</span>
<span class="p_del">- * the highest weight (=88761) always runnable, we should not overflow</span>
<span class="p_del">- * 2) for entity, support any load.weight always runnable</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * load_avg may also take frequency scaling into account:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   load_avg = runnable% * scale_load_down(load) * freq%</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * where freq% is the CPU frequency normalized to the highest frequency.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * [util_avg definition]</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   util_avg = running% * SCHED_CAPACITY_SCALE</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * where running% is the time ratio that a sched_entity is running on</span>
<span class="p_add">+ * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable</span>
<span class="p_add">+ * and blocked sched_entities.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * util_avg may also factor frequency scaling and CPU capacity scaling:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * where freq% is the same as above, and capacity% is the CPU capacity</span>
<span class="p_add">+ * normalized to the greatest capacity (due to uarch differences, etc).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * N.B., the above ratios (runnable%, running%, freq%, and capacity%)</span>
<span class="p_add">+ * themselves are in the range of [0, 1]. To do fixed point arithmetics,</span>
<span class="p_add">+ * we therefore scale them to as large a range as necessary. This is for</span>
<span class="p_add">+ * example reflected by util_avg&#39;s SCHED_CAPACITY_SCALE.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * [Overflow issue]</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities</span>
<span class="p_add">+ * with the highest load (=88761), always runnable on a single cfs_rq,</span>
<span class="p_add">+ * and should not overflow as the number already hits PID_MAX_LIMIT.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For all other cases (including 32-bit kernels), struct load_weight&#39;s</span>
<span class="p_add">+ * weight will overflow first before we do, because:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *    Max(load_avg) &lt;= Max(load.weight)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Then it is the load_weight&#39;s responsibility to consider overflow</span>
<span class="p_add">+ * issues.</span>
  */
 struct sched_avg {
 	u64 last_update_time, load_sum;
<span class="p_chunk">@@ -1871,6 +1930,11 @@</span> <span class="p_context"> extern int arch_task_struct_size __read_mostly;</span>
 /* Future-safe accessor for struct task_struct&#39;s cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&amp;(tsk)-&gt;cpus_allowed)
 
<span class="p_add">+static inline int tsk_nr_cpus_allowed(struct task_struct *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return p-&gt;nr_cpus_allowed;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04
<span class="p_chunk">@@ -2303,8 +2367,6 @@</span> <span class="p_context"> extern unsigned long long notrace sched_clock(void);</span>
 /*
  * See the comment in kernel/sched/clock.c
  */
<span class="p_del">-extern u64 cpu_clock(int cpu);</span>
<span class="p_del">-extern u64 local_clock(void);</span>
 extern u64 running_clock(void);
 extern u64 sched_clock_cpu(int cpu);
 
<span class="p_chunk">@@ -2323,6 +2385,16 @@</span> <span class="p_context"> static inline void sched_clock_idle_sleep_event(void)</span>
 static inline void sched_clock_idle_wakeup_event(u64 delta_ns)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline u64 cpu_clock(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sched_clock();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u64 local_clock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sched_clock();</span>
<span class="p_add">+}</span>
 #else
 /*
  * Architectures can set this to 1 if they have specified
<span class="p_chunk">@@ -2337,6 +2409,26 @@</span> <span class="p_context"> extern void clear_sched_clock_stable(void);</span>
 extern void sched_clock_tick(void);
 extern void sched_clock_idle_sleep_event(void);
 extern void sched_clock_idle_wakeup_event(u64 delta_ns);
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * As outlined in clock.c, provides a fast, high resolution, nanosecond</span>
<span class="p_add">+ * time source that is monotonic per cpu argument and has bounded drift</span>
<span class="p_add">+ * between cpus.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ######################### BIG FAT WARNING ##########################</span>
<span class="p_add">+ * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #</span>
<span class="p_add">+ * # go backwards !!                                                  #</span>
<span class="p_add">+ * ####################################################################</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline u64 cpu_clock(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sched_clock_cpu(cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u64 local_clock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sched_clock_cpu(raw_smp_processor_id());</span>
<span class="p_add">+}</span>
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
<span class="p_header">diff --git a/kernel/cpu.c b/kernel/cpu.c</span>
<span class="p_header">index 3e3f6e49eabb..d948e44c471e 100644</span>
<span class="p_header">--- a/kernel/cpu.c</span>
<span class="p_header">+++ b/kernel/cpu.c</span>
<span class="p_chunk">@@ -703,21 +703,6 @@</span> <span class="p_context"> static int takedown_cpu(unsigned int cpu)</span>
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);
 	int err;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * By now we&#39;ve cleared cpu_active_mask, wait for all preempt-disabled</span>
<span class="p_del">-	 * and RCU users of this state to go away such that all new such users</span>
<span class="p_del">-	 * will observe it.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * For CONFIG_PREEMPT we have preemptible RCU and its sync_rcu() might</span>
<span class="p_del">-	 * not imply sync_sched(), so wait for both.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Do sync before park smpboot threads to take care the rcu boost case.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_PREEMPT))</span>
<span class="p_del">-		synchronize_rcu_mult(call_rcu, call_rcu_sched);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		synchronize_rcu();</span>
<span class="p_del">-</span>
 	/* Park the smpboot threads */
 	kthread_park(per_cpu_ptr(&amp;cpuhp_state, cpu)-&gt;thread);
 	smpboot_park_threads(cpu);
<span class="p_chunk">@@ -923,8 +908,6 @@</span> <span class="p_context"> void cpuhp_online_idle(enum cpuhp_state state)</span>
 
 	st-&gt;state = CPUHP_AP_ONLINE_IDLE;
 
<span class="p_del">-	/* The cpu is marked online, set it active now */</span>
<span class="p_del">-	set_cpu_active(cpu, true);</span>
 	/* Unpark the stopper thread and the hotplug thread of this cpu */
 	stop_machine_unpark(cpu);
 	kthread_unpark(st-&gt;thread);
<span class="p_chunk">@@ -1236,6 +1219,12 @@</span> <span class="p_context"> static struct cpuhp_step cpuhp_ap_states[] = {</span>
 		.name			= &quot;ap:offline&quot;,
 		.cant_stop		= true,
 	},
<span class="p_add">+	/* First state is scheduler control. Interrupts are disabled */</span>
<span class="p_add">+	[CPUHP_AP_SCHED_STARTING] = {</span>
<span class="p_add">+		.name			= &quot;sched:starting&quot;,</span>
<span class="p_add">+		.startup		= sched_cpu_starting,</span>
<span class="p_add">+		.teardown		= sched_cpu_dying,</span>
<span class="p_add">+	},</span>
 	/*
 	 * Low level startup/teardown notifiers. Run with interrupts
 	 * disabled. Will be removed once the notifiers are converted to
<span class="p_chunk">@@ -1274,6 +1263,15 @@</span> <span class="p_context"> static struct cpuhp_step cpuhp_ap_states[] = {</span>
 	 * The dynamically registered state space is here
 	 */
 
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	/* Last state is scheduler control setting the cpu active */</span>
<span class="p_add">+	[CPUHP_AP_ACTIVE] = {</span>
<span class="p_add">+		.name			= &quot;sched:active&quot;,</span>
<span class="p_add">+		.startup		= sched_cpu_activate,</span>
<span class="p_add">+		.teardown		= sched_cpu_deactivate,</span>
<span class="p_add">+	},</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* CPU is fully up and running. */
 	[CPUHP_ONLINE] = {
 		.name			= &quot;online&quot;,
<span class="p_header">diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="p_header">index 78c1c0ee6dc1..68bc6a654ca3 100644</span>
<span class="p_header">--- a/kernel/locking/lockdep.c</span>
<span class="p_header">+++ b/kernel/locking/lockdep.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/bitops.h&gt;
 #include &lt;linux/gfp.h&gt;
 #include &lt;linux/kmemcheck.h&gt;
<span class="p_add">+#include &lt;linux/random.h&gt;</span>
 
 #include &lt;asm/sections.h&gt;
 
<span class="p_chunk">@@ -3585,7 +3586,35 @@</span> <span class="p_context"> static int __lock_is_held(struct lockdep_map *lock)</span>
 	return 0;
 }
 
<span class="p_del">-static void __lock_pin_lock(struct lockdep_map *lock)</span>
<span class="p_add">+static struct pin_cookie __lock_pin_lock(struct lockdep_map *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pin_cookie cookie = NIL_COOKIE;</span>
<span class="p_add">+	struct task_struct *curr = current;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!debug_locks))</span>
<span class="p_add">+		return cookie;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; curr-&gt;lockdep_depth; i++) {</span>
<span class="p_add">+		struct held_lock *hlock = curr-&gt;held_locks + i;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (match_held_lock(hlock, lock)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Grab 16bits of randomness; this is sufficient to not</span>
<span class="p_add">+			 * be guessable and still allows some pin nesting in</span>
<span class="p_add">+			 * our u32 pin_count.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			cookie.val = 1 + (prandom_u32() &gt;&gt; 16);</span>
<span class="p_add">+			hlock-&gt;pin_count += cookie.val;</span>
<span class="p_add">+			return cookie;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN(1, &quot;pinning an unheld lock\n&quot;);</span>
<span class="p_add">+	return cookie;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)</span>
 {
 	struct task_struct *curr = current;
 	int i;
<span class="p_chunk">@@ -3597,7 +3626,7 @@</span> <span class="p_context"> static void __lock_pin_lock(struct lockdep_map *lock)</span>
 		struct held_lock *hlock = curr-&gt;held_locks + i;
 
 		if (match_held_lock(hlock, lock)) {
<span class="p_del">-			hlock-&gt;pin_count++;</span>
<span class="p_add">+			hlock-&gt;pin_count += cookie.val;</span>
 			return;
 		}
 	}
<span class="p_chunk">@@ -3605,7 +3634,7 @@</span> <span class="p_context"> static void __lock_pin_lock(struct lockdep_map *lock)</span>
 	WARN(1, &quot;pinning an unheld lock\n&quot;);
 }
 
<span class="p_del">-static void __lock_unpin_lock(struct lockdep_map *lock)</span>
<span class="p_add">+static void __lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)</span>
 {
 	struct task_struct *curr = current;
 	int i;
<span class="p_chunk">@@ -3620,7 +3649,11 @@</span> <span class="p_context"> static void __lock_unpin_lock(struct lockdep_map *lock)</span>
 			if (WARN(!hlock-&gt;pin_count, &quot;unpinning an unpinned lock\n&quot;))
 				return;
 
<span class="p_del">-			hlock-&gt;pin_count--;</span>
<span class="p_add">+			hlock-&gt;pin_count -= cookie.val;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (WARN((int)hlock-&gt;pin_count &lt; 0, &quot;pin count corrupted\n&quot;))</span>
<span class="p_add">+				hlock-&gt;pin_count = 0;</span>
<span class="p_add">+</span>
 			return;
 		}
 	}
<span class="p_chunk">@@ -3751,24 +3784,44 @@</span> <span class="p_context"> int lock_is_held(struct lockdep_map *lock)</span>
 }
 EXPORT_SYMBOL_GPL(lock_is_held);
 
<span class="p_del">-void lock_pin_lock(struct lockdep_map *lock)</span>
<span class="p_add">+struct pin_cookie lock_pin_lock(struct lockdep_map *lock)</span>
 {
<span class="p_add">+	struct pin_cookie cookie = NIL_COOKIE;</span>
 	unsigned long flags;
 
 	if (unlikely(current-&gt;lockdep_recursion))
<span class="p_del">-		return;</span>
<span class="p_add">+		return cookie;</span>
 
 	raw_local_irq_save(flags);
 	check_flags(flags);
 
 	current-&gt;lockdep_recursion = 1;
<span class="p_del">-	__lock_pin_lock(lock);</span>
<span class="p_add">+	cookie = __lock_pin_lock(lock);</span>
 	current-&gt;lockdep_recursion = 0;
 	raw_local_irq_restore(flags);
<span class="p_add">+</span>
<span class="p_add">+	return cookie;</span>
 }
 EXPORT_SYMBOL_GPL(lock_pin_lock);
 
<span class="p_del">-void lock_unpin_lock(struct lockdep_map *lock)</span>
<span class="p_add">+void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(current-&gt;lockdep_recursion))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_local_irq_save(flags);</span>
<span class="p_add">+	check_flags(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	current-&gt;lockdep_recursion = 1;</span>
<span class="p_add">+	__lock_repin_lock(lock, cookie);</span>
<span class="p_add">+	current-&gt;lockdep_recursion = 0;</span>
<span class="p_add">+	raw_local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(lock_repin_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie)</span>
 {
 	unsigned long flags;
 
<span class="p_chunk">@@ -3779,7 +3832,7 @@</span> <span class="p_context"> void lock_unpin_lock(struct lockdep_map *lock)</span>
 	check_flags(flags);
 
 	current-&gt;lockdep_recursion = 1;
<span class="p_del">-	__lock_unpin_lock(lock);</span>
<span class="p_add">+	__lock_unpin_lock(lock, cookie);</span>
 	current-&gt;lockdep_recursion = 0;
 	raw_local_irq_restore(flags);
 }
<span class="p_header">diff --git a/kernel/sched/clock.c b/kernel/sched/clock.c</span>
<span class="p_header">index fedb967a9841..e85a725e5c34 100644</span>
<span class="p_header">--- a/kernel/sched/clock.c</span>
<span class="p_header">+++ b/kernel/sched/clock.c</span>
<span class="p_chunk">@@ -318,6 +318,7 @@</span> <span class="p_context"> u64 sched_clock_cpu(int cpu)</span>
 
 	return clock;
 }
<span class="p_add">+EXPORT_SYMBOL_GPL(sched_clock_cpu);</span>
 
 void sched_clock_tick(void)
 {
<span class="p_chunk">@@ -363,39 +364,6 @@</span> <span class="p_context"> void sched_clock_idle_wakeup_event(u64 delta_ns)</span>
 }
 EXPORT_SYMBOL_GPL(sched_clock_idle_wakeup_event);
 
<span class="p_del">-/*</span>
<span class="p_del">- * As outlined at the top, provides a fast, high resolution, nanosecond</span>
<span class="p_del">- * time source that is monotonic per cpu argument and has bounded drift</span>
<span class="p_del">- * between cpus.</span>
<span class="p_del">- *</span>
<span class="p_del">- * ######################### BIG FAT WARNING ##########################</span>
<span class="p_del">- * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #</span>
<span class="p_del">- * # go backwards !!                                                  #</span>
<span class="p_del">- * ####################################################################</span>
<span class="p_del">- */</span>
<span class="p_del">-u64 cpu_clock(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!sched_clock_stable())</span>
<span class="p_del">-		return sched_clock_cpu(cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Similar to cpu_clock() for the current cpu. Time will only be observed</span>
<span class="p_del">- * to be monotonic if care is taken to only compare timestampt taken on the</span>
<span class="p_del">- * same CPU.</span>
<span class="p_del">- *</span>
<span class="p_del">- * See cpu_clock().</span>
<span class="p_del">- */</span>
<span class="p_del">-u64 local_clock(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!sched_clock_stable())</span>
<span class="p_del">-		return sched_clock_cpu(raw_smp_processor_id());</span>
<span class="p_del">-</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #else /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
 
 void sched_clock_init(void)
<span class="p_chunk">@@ -410,22 +378,8 @@</span> <span class="p_context"> u64 sched_clock_cpu(int cpu)</span>
 
 	return sched_clock();
 }
<span class="p_del">-</span>
<span class="p_del">-u64 cpu_clock(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-u64 local_clock(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */
 
<span class="p_del">-EXPORT_SYMBOL_GPL(cpu_clock);</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(local_clock);</span>
<span class="p_del">-</span>
 /*
  * Running clock - returns the time that has elapsed while a guest has been
  * running.
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index d1f7149f8704..404c0784b1fc 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/init.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/highmem.h&gt;
<span class="p_del">-#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_context.h&gt;</span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/capability.h&gt;
 #include &lt;linux/completion.h&gt;
<span class="p_chunk">@@ -170,6 +170,71 @@</span> <span class="p_context"> static struct rq *this_rq_lock(void)</span>
 	return rq;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * __task_rq_lock - lock the rq @p resides on.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
<span class="p_add">+	__acquires(rq-&gt;lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq;</span>
<span class="p_add">+</span>
<span class="p_add">+	lockdep_assert_held(&amp;p-&gt;pi_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		rq = task_rq(p);</span>
<span class="p_add">+		raw_spin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		if (likely(rq == task_rq(p) &amp;&amp; !task_on_rq_migrating(p))) {</span>
<span class="p_add">+			rf-&gt;cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+			return rq;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		raw_spin_unlock(&amp;rq-&gt;lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		while (unlikely(task_on_rq_migrating(p)))</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * task_rq_lock - lock p-&gt;pi_lock and lock the rq @p resides on.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
<span class="p_add">+	__acquires(p-&gt;pi_lock)</span>
<span class="p_add">+	__acquires(rq-&gt;lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
<span class="p_add">+		rq = task_rq(p);</span>
<span class="p_add">+		raw_spin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 *	move_queued_task()		task_rq_lock()</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 *	ACQUIRE (rq-&gt;lock)</span>
<span class="p_add">+		 *	[S] -&gt;on_rq = MIGRATING		[L] rq = task_rq()</span>
<span class="p_add">+		 *	WMB (__set_task_cpu())		ACQUIRE (rq-&gt;lock);</span>
<span class="p_add">+		 *	[S] -&gt;cpu = new_cpu		[L] task_rq()</span>
<span class="p_add">+		 *					[L] -&gt;on_rq</span>
<span class="p_add">+		 *	RELEASE (rq-&gt;lock)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If we observe the old cpu in task_rq_lock, the acquire of</span>
<span class="p_add">+		 * the old rq-&gt;lock will fully serialize against the stores.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * If we observe the new cpu in task_rq_lock, the acquire will</span>
<span class="p_add">+		 * pair with the WMB to ensure we must then also see migrating.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (likely(rq == task_rq(p) &amp;&amp; !task_on_rq_migrating(p))) {</span>
<span class="p_add">+			rf-&gt;cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+			return rq;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		raw_spin_unlock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
<span class="p_add">+</span>
<span class="p_add">+		while (unlikely(task_on_rq_migrating(p)))</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_SCHED_HRTICK
 /*
  * Use HR-timers to deliver accurate preemption points.
<span class="p_chunk">@@ -249,29 +314,6 @@</span> <span class="p_context"> void hrtick_start(struct rq *rq, u64 delay)</span>
 	}
 }
 
<span class="p_del">-static int</span>
<span class="p_del">-hotplug_hrtick(struct notifier_block *nfb, unsigned long action, void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = (int)(long)hcpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (action) {</span>
<span class="p_del">-	case CPU_UP_CANCELED:</span>
<span class="p_del">-	case CPU_UP_CANCELED_FROZEN:</span>
<span class="p_del">-	case CPU_DOWN_PREPARE:</span>
<span class="p_del">-	case CPU_DOWN_PREPARE_FROZEN:</span>
<span class="p_del">-	case CPU_DEAD:</span>
<span class="p_del">-	case CPU_DEAD_FROZEN:</span>
<span class="p_del">-		hrtick_clear(cpu_rq(cpu));</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return NOTIFY_DONE;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __init void init_hrtick(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	hotcpu_notifier(hotplug_hrtick, 0);</span>
<span class="p_del">-}</span>
 #else
 /*
  * Called to set the hrtick timer state.
<span class="p_chunk">@@ -288,10 +330,6 @@</span> <span class="p_context"> void hrtick_start(struct rq *rq, u64 delay)</span>
 	hrtimer_start(&amp;rq-&gt;hrtick_timer, ns_to_ktime(delay),
 		      HRTIMER_MODE_REL_PINNED);
 }
<span class="p_del">-</span>
<span class="p_del">-static inline void init_hrtick(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
 #endif /* CONFIG_SMP */
 
 static void init_rq_hrtick(struct rq *rq)
<span class="p_chunk">@@ -315,10 +353,6 @@</span> <span class="p_context"> static inline void hrtick_clear(struct rq *rq)</span>
 static inline void init_rq_hrtick(struct rq *rq)
 {
 }
<span class="p_del">-</span>
<span class="p_del">-static inline void init_hrtick(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
 #endif	/* CONFIG_SCHED_HRTICK */
 
 /*
<span class="p_chunk">@@ -400,7 +434,7 @@</span> <span class="p_context"> void wake_q_add(struct wake_q_head *head, struct task_struct *task)</span>
 	 * wakeup due to that.
 	 *
 	 * This cmpxchg() implies a full barrier, which pairs with the write
<span class="p_del">-	 * barrier implied by the wakeup in wake_up_list().</span>
<span class="p_add">+	 * barrier implied by the wakeup in wake_up_q().</span>
 	 */
 	if (cmpxchg(&amp;node-&gt;next, NULL, WAKE_Q_TAIL))
 		return;
<span class="p_chunk">@@ -499,7 +533,10 @@</span> <span class="p_context"> int get_nohz_timer_target(void)</span>
 	rcu_read_lock();
 	for_each_domain(cpu, sd) {
 		for_each_cpu(i, sched_domain_span(sd)) {
<span class="p_del">-			if (!idle_cpu(i) &amp;&amp; is_housekeeping_cpu(cpu)) {</span>
<span class="p_add">+			if (cpu == i)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!idle_cpu(i) &amp;&amp; is_housekeeping_cpu(i)) {</span>
 				cpu = i;
 				goto unlock;
 			}
<span class="p_chunk">@@ -1085,12 +1122,20 @@</span> <span class="p_context"> void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
 static int __set_cpus_allowed_ptr(struct task_struct *p,
 				  const struct cpumask *new_mask, bool check)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	struct rq *rq;</span>
<span class="p_add">+	const struct cpumask *cpu_valid_mask = cpu_active_mask;</span>
 	unsigned int dest_cpu;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 	int ret = 0;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p-&gt;flags &amp; PF_KTHREAD) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Kernel threads are allowed on online &amp;&amp; !active CPUs</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cpu_valid_mask = cpu_online_mask;</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * Must re-check here, to close a race against __kthread_bind(),
<span class="p_chunk">@@ -1104,22 +1149,32 @@</span> <span class="p_context"> static int __set_cpus_allowed_ptr(struct task_struct *p,</span>
 	if (cpumask_equal(&amp;p-&gt;cpus_allowed, new_mask))
 		goto out;
 
<span class="p_del">-	if (!cpumask_intersects(new_mask, cpu_active_mask)) {</span>
<span class="p_add">+	if (!cpumask_intersects(new_mask, cpu_valid_mask)) {</span>
 		ret = -EINVAL;
 		goto out;
 	}
 
 	do_set_cpus_allowed(p, new_mask);
 
<span class="p_add">+	if (p-&gt;flags &amp; PF_KTHREAD) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For kernel threads that do indeed end up on online &amp;&amp;</span>
<span class="p_add">+		 * !active we want to ensure they are strict per-cpu threads.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(cpumask_intersects(new_mask, cpu_online_mask) &amp;&amp;</span>
<span class="p_add">+			!cpumask_intersects(new_mask, cpu_active_mask) &amp;&amp;</span>
<span class="p_add">+			p-&gt;nr_cpus_allowed != 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Can the task run on the task&#39;s current CPU? If so, we&#39;re done */
 	if (cpumask_test_cpu(task_cpu(p), new_mask))
 		goto out;
 
<span class="p_del">-	dest_cpu = cpumask_any_and(cpu_active_mask, new_mask);</span>
<span class="p_add">+	dest_cpu = cpumask_any_and(cpu_valid_mask, new_mask);</span>
 	if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {
 		struct migration_arg arg = { p, dest_cpu };
 		/* Need help from migration thread: drop lock and wait. */
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);
 		tlb_migrate_finish(p-&gt;mm);
 		return 0;
<span class="p_chunk">@@ -1128,12 +1183,12 @@</span> <span class="p_context"> static int __set_cpus_allowed_ptr(struct task_struct *p,</span>
 		 * OK, since we&#39;re going to drop the lock immediately
 		 * afterwards anyway.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 		rq = move_queued_task(rq, p, dest_cpu);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 	}
 out:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1317,8 +1372,8 @@</span> <span class="p_context"> int migrate_swap(struct task_struct *cur, struct task_struct *p)</span>
  */
 unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 {
<span class="p_del">-	unsigned long flags;</span>
 	int running, queued;
<span class="p_add">+	struct rq_flags rf;</span>
 	unsigned long ncsw;
 	struct rq *rq;
 
<span class="p_chunk">@@ -1353,14 +1408,14 @@</span> <span class="p_context"> unsigned long wait_task_inactive(struct task_struct *p, long match_state)</span>
 		 * lock now, to be *sure*. If we&#39;re wrong, we&#39;ll
 		 * just go back and repeat.
 		 */
<span class="p_del">-		rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+		rq = task_rq_lock(p, &amp;rf);</span>
 		trace_sched_wait_task(p);
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
 		if (!match_state || p-&gt;state == match_state)
 			ncsw = p-&gt;nvcsw | LONG_MIN; /* sets MSB */
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 
 		/*
 		 * If it changed from the expected state, bail out now.
<span class="p_chunk">@@ -1434,6 +1489,25 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(kick_process);</span>
 
 /*
  * -&gt;cpus_allowed is protected by both rq-&gt;lock and p-&gt;pi_lock
<span class="p_add">+ *</span>
<span class="p_add">+ * A few notes on cpu_active vs cpu_online:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  - cpu_active must be a subset of cpu_online</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  - on cpu-up we allow per-cpu kthreads on the online &amp;&amp; !active cpu,</span>
<span class="p_add">+ *    see __set_cpus_allowed_ptr(). At this point the newly online</span>
<span class="p_add">+ *    cpu isn&#39;t yet part of the sched domains, and balancing will not</span>
<span class="p_add">+ *    see it.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  - on cpu-down we clear cpu_active() to mask the sched domains and</span>
<span class="p_add">+ *    avoid the load balancer to place new tasks on the to be removed</span>
<span class="p_add">+ *    cpu. Existing tasks will remain running there and will be taken</span>
<span class="p_add">+ *    off.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This means that fallback selection must not select !active CPUs.</span>
<span class="p_add">+ * And can assume that any active CPU must be online. Conversely</span>
<span class="p_add">+ * select_task_rq() below may allow selection of !active CPUs in order</span>
<span class="p_add">+ * to satisfy the above rules.</span>
  */
 static int select_fallback_rq(int cpu, struct task_struct *p)
 {
<span class="p_chunk">@@ -1452,8 +1526,6 @@</span> <span class="p_context"> static int select_fallback_rq(int cpu, struct task_struct *p)</span>
 
 		/* Look for allowed, online CPU in same node. */
 		for_each_cpu(dest_cpu, nodemask) {
<span class="p_del">-			if (!cpu_online(dest_cpu))</span>
<span class="p_del">-				continue;</span>
 			if (!cpu_active(dest_cpu))
 				continue;
 			if (cpumask_test_cpu(dest_cpu, tsk_cpus_allowed(p)))
<span class="p_chunk">@@ -1464,8 +1536,6 @@</span> <span class="p_context"> static int select_fallback_rq(int cpu, struct task_struct *p)</span>
 	for (;;) {
 		/* Any allowed, online CPU? */
 		for_each_cpu(dest_cpu, tsk_cpus_allowed(p)) {
<span class="p_del">-			if (!cpu_online(dest_cpu))</span>
<span class="p_del">-				continue;</span>
 			if (!cpu_active(dest_cpu))
 				continue;
 			goto out;
<span class="p_chunk">@@ -1515,8 +1585,10 @@</span> <span class="p_context"> int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags)</span>
 {
 	lockdep_assert_held(&amp;p-&gt;pi_lock);
 
<span class="p_del">-	if (p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		cpu = p-&gt;sched_class-&gt;select_task_rq(p, cpu, sd_flags, wake_flags);
<span class="p_add">+	else</span>
<span class="p_add">+		cpu = cpumask_any(tsk_cpus_allowed(p));</span>
 
 	/*
 	 * In order not to call set_task_cpu() on a blocking task we need
<span class="p_chunk">@@ -1604,8 +1676,8 @@</span> <span class="p_context"> static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_fl</span>
 /*
  * Mark the task runnable and perform wakeup-preemption.
  */
<span class="p_del">-static void</span>
<span class="p_del">-ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)</span>
<span class="p_add">+static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,</span>
<span class="p_add">+			   struct pin_cookie cookie)</span>
 {
 	check_preempt_curr(rq, p, wake_flags);
 	p-&gt;state = TASK_RUNNING;
<span class="p_chunk">@@ -1617,9 +1689,9 @@</span> <span class="p_context"> ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)</span>
 		 * Our task @p is fully woken up and running; so its safe to
 		 * drop the rq-&gt;lock, hereafter rq is only used for statistics.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		p-&gt;sched_class-&gt;task_woken(rq, p);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, cookie);</span>
 	}
 
 	if (rq-&gt;idle_stamp) {
<span class="p_chunk">@@ -1637,17 +1709,23 @@</span> <span class="p_context"> ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)</span>
 }
 
 static void
<span class="p_del">-ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags)</span>
<span class="p_add">+ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags,</span>
<span class="p_add">+		 struct pin_cookie cookie)</span>
 {
<span class="p_add">+	int en_flags = ENQUEUE_WAKEUP;</span>
<span class="p_add">+</span>
 	lockdep_assert_held(&amp;rq-&gt;lock);
 
 #ifdef CONFIG_SMP
 	if (p-&gt;sched_contributes_to_load)
 		rq-&gt;nr_uninterruptible--;
<span class="p_add">+</span>
<span class="p_add">+	if (wake_flags &amp; WF_MIGRATED)</span>
<span class="p_add">+		en_flags |= ENQUEUE_MIGRATED;</span>
 #endif
 
<span class="p_del">-	ttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_WAKING);</span>
<span class="p_del">-	ttwu_do_wakeup(rq, p, wake_flags);</span>
<span class="p_add">+	ttwu_activate(rq, p, en_flags);</span>
<span class="p_add">+	ttwu_do_wakeup(rq, p, wake_flags, cookie);</span>
 }
 
 /*
<span class="p_chunk">@@ -1658,17 +1736,18 @@</span> <span class="p_context"> ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags)</span>
  */
 static int ttwu_remote(struct task_struct *p, int wake_flags)
 {
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 	int ret = 0;
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 	if (task_on_rq_queued(p)) {
 		/* check_preempt_curr() may use rq clock */
 		update_rq_clock(rq);
<span class="p_del">-		ttwu_do_wakeup(rq, p, wake_flags);</span>
<span class="p_add">+		ttwu_do_wakeup(rq, p, wake_flags, rf.cookie);</span>
 		ret = 1;
 	}
<span class="p_del">-	__task_rq_unlock(rq);</span>
<span class="p_add">+	__task_rq_unlock(rq, &amp;rf);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1678,6 +1757,7 @@</span> <span class="p_context"> void sched_ttwu_pending(void)</span>
 {
 	struct rq *rq = this_rq();
 	struct llist_node *llist = llist_del_all(&amp;rq-&gt;wake_list);
<span class="p_add">+	struct pin_cookie cookie;</span>
 	struct task_struct *p;
 	unsigned long flags;
 
<span class="p_chunk">@@ -1685,15 +1765,19 @@</span> <span class="p_context"> void sched_ttwu_pending(void)</span>
 		return;
 
 	raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);
<span class="p_del">-	lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
 
 	while (llist) {
 		p = llist_entry(llist, struct task_struct, wake_entry);
 		llist = llist_next(llist);
<span class="p_del">-		ttwu_do_activate(rq, p, 0);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * See ttwu_queue(); we only call ttwu_queue_remote() when</span>
<span class="p_add">+		 * its a x-cpu wakeup.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ttwu_do_activate(rq, p, WF_MIGRATED, cookie);</span>
 	}
 
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 	raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);
 }
 
<span class="p_chunk">@@ -1777,9 +1861,10 @@</span> <span class="p_context"> bool cpus_share_cache(int this_cpu, int that_cpu)</span>
 }
 #endif /* CONFIG_SMP */
 
<span class="p_del">-static void ttwu_queue(struct task_struct *p, int cpu)</span>
<span class="p_add">+static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags)</span>
 {
 	struct rq *rq = cpu_rq(cpu);
<span class="p_add">+	struct pin_cookie cookie;</span>
 
 #if defined(CONFIG_SMP)
 	if (sched_feat(TTWU_QUEUE) &amp;&amp; !cpus_share_cache(smp_processor_id(), cpu)) {
<span class="p_chunk">@@ -1790,9 +1875,9 @@</span> <span class="p_context"> static void ttwu_queue(struct task_struct *p, int cpu)</span>
 #endif
 
 	raw_spin_lock(&amp;rq-&gt;lock);
<span class="p_del">-	lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-	ttwu_do_activate(rq, p, 0);</span>
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	ttwu_do_activate(rq, p, wake_flags, cookie);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 	raw_spin_unlock(&amp;rq-&gt;lock);
 }
 
<span class="p_chunk">@@ -1961,9 +2046,6 @@</span> <span class="p_context"> try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)</span>
 	p-&gt;sched_contributes_to_load = !!task_contributes_to_load(p);
 	p-&gt;state = TASK_WAKING;
 
<span class="p_del">-	if (p-&gt;sched_class-&gt;task_waking)</span>
<span class="p_del">-		p-&gt;sched_class-&gt;task_waking(p);</span>
<span class="p_del">-</span>
 	cpu = select_task_rq(p, p-&gt;wake_cpu, SD_BALANCE_WAKE, wake_flags);
 	if (task_cpu(p) != cpu) {
 		wake_flags |= WF_MIGRATED;
<span class="p_chunk">@@ -1971,7 +2053,7 @@</span> <span class="p_context"> try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)</span>
 	}
 #endif /* CONFIG_SMP */
 
<span class="p_del">-	ttwu_queue(p, cpu);</span>
<span class="p_add">+	ttwu_queue(p, cpu, wake_flags);</span>
 stat:
 	if (schedstat_enabled())
 		ttwu_stat(p, cpu, wake_flags);
<span class="p_chunk">@@ -1989,7 +2071,7 @@</span> <span class="p_context"> try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)</span>
  * ensure that this_rq() is locked, @p is bound to this_rq() and not
  * the current task.
  */
<span class="p_del">-static void try_to_wake_up_local(struct task_struct *p)</span>
<span class="p_add">+static void try_to_wake_up_local(struct task_struct *p, struct pin_cookie cookie)</span>
 {
 	struct rq *rq = task_rq(p);
 
<span class="p_chunk">@@ -2006,11 +2088,11 @@</span> <span class="p_context"> static void try_to_wake_up_local(struct task_struct *p)</span>
 		 * disabled avoiding further scheduler activity on it and we&#39;ve
 		 * not yet picked a replacement task.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		raw_spin_unlock(&amp;rq-&gt;lock);
 		raw_spin_lock(&amp;p-&gt;pi_lock);
 		raw_spin_lock(&amp;rq-&gt;lock);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, cookie);</span>
 	}
 
 	if (!(p-&gt;state &amp; TASK_NORMAL))
<span class="p_chunk">@@ -2021,7 +2103,7 @@</span> <span class="p_context"> static void try_to_wake_up_local(struct task_struct *p)</span>
 	if (!task_on_rq_queued(p))
 		ttwu_activate(rq, p, ENQUEUE_WAKEUP);
 
<span class="p_del">-	ttwu_do_wakeup(rq, p, 0);</span>
<span class="p_add">+	ttwu_do_wakeup(rq, p, 0, cookie);</span>
 	if (schedstat_enabled())
 		ttwu_stat(p, smp_processor_id(), 0);
 out:
<span class="p_chunk">@@ -2381,7 +2463,8 @@</span> <span class="p_context"> static int dl_overflow(struct task_struct *p, int policy,</span>
 	u64 new_bw = dl_policy(policy) ? to_ratio(period, runtime) : 0;
 	int cpus, err = -1;
 
<span class="p_del">-	if (new_bw == p-&gt;dl.dl_bw)</span>
<span class="p_add">+	/* !deadline task may carry old deadline bandwidth */</span>
<span class="p_add">+	if (new_bw == p-&gt;dl.dl_bw &amp;&amp; task_has_dl_policy(p))</span>
 		return 0;
 
 	/*
<span class="p_chunk">@@ -2420,12 +2503,12 @@</span> <span class="p_context"> extern void init_dl_bw(struct dl_bw *dl_b);</span>
  */
 void wake_up_new_task(struct task_struct *p)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, flags);</span>
 	/* Initialize new task&#39;s runnable average */
 	init_entity_runnable_average(&amp;p-&gt;se);
<span class="p_add">+	raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, rf.flags);</span>
 #ifdef CONFIG_SMP
 	/*
 	 * Fork balancing, do it here and not earlier because:
<span class="p_chunk">@@ -2434,8 +2517,10 @@</span> <span class="p_context"> void wake_up_new_task(struct task_struct *p)</span>
 	 */
 	set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0));
 #endif
<span class="p_add">+	/* Post initialize new task&#39;s util average when its cfs_rq is set */</span>
<span class="p_add">+	post_init_entity_util_avg(&amp;p-&gt;se);</span>
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 	activate_task(rq, p, 0);
 	p-&gt;on_rq = TASK_ON_RQ_QUEUED;
 	trace_sched_wakeup_new(p);
<span class="p_chunk">@@ -2446,12 +2531,12 @@</span> <span class="p_context"> void wake_up_new_task(struct task_struct *p)</span>
 		 * Nothing relies on rq-&gt;lock after this, so its fine to
 		 * drop it.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 		p-&gt;sched_class-&gt;task_woken(rq, p);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 	}
 #endif
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
<span class="p_chunk">@@ -2713,7 +2798,7 @@</span> <span class="p_context"> asmlinkage __visible void schedule_tail(struct task_struct *prev)</span>
  */
 static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
<span class="p_del">-	       struct task_struct *next)</span>
<span class="p_add">+	       struct task_struct *next, struct pin_cookie cookie)</span>
 {
 	struct mm_struct *mm, *oldmm;
 
<span class="p_chunk">@@ -2733,7 +2818,7 @@</span> <span class="p_context"> context_switch(struct rq *rq, struct task_struct *prev,</span>
 		atomic_inc(&amp;oldmm-&gt;mm_count);
 		enter_lazy_tlb(oldmm, next);
 	} else
<span class="p_del">-		switch_mm(oldmm, mm, next);</span>
<span class="p_add">+		switch_mm_irqs_off(oldmm, mm, next);</span>
 
 	if (!prev-&gt;mm) {
 		prev-&gt;active_mm = NULL;
<span class="p_chunk">@@ -2745,7 +2830,7 @@</span> <span class="p_context"> context_switch(struct rq *rq, struct task_struct *prev,</span>
 	 * of the scheduler it&#39;s an obvious special-case), so we
 	 * do an early lockdep release here:
 	 */
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 	spin_release(&amp;rq-&gt;lock.dep_map, 1, _THIS_IP_);
 
 	/* Here we just switch the register state and the stack. */
<span class="p_chunk">@@ -2867,7 +2952,7 @@</span> <span class="p_context"> EXPORT_PER_CPU_SYMBOL(kernel_cpustat);</span>
  */
 unsigned long long task_sched_runtime(struct task_struct *p)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 	u64 ns;
 
<span class="p_chunk">@@ -2887,7 +2972,7 @@</span> <span class="p_context"> unsigned long long task_sched_runtime(struct task_struct *p)</span>
 		return p-&gt;se.sum_exec_runtime;
 #endif
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	/*
 	 * Must be -&gt;curr _and_ -&gt;on_rq.  If dequeued, we would
 	 * project cycles that may never be accounted to this
<span class="p_chunk">@@ -2898,7 +2983,7 @@</span> <span class="p_context"> unsigned long long task_sched_runtime(struct task_struct *p)</span>
 		p-&gt;sched_class-&gt;update_curr(rq);
 	}
 	ns = p-&gt;se.sum_exec_runtime;
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	return ns;
 }
<span class="p_chunk">@@ -2918,7 +3003,7 @@</span> <span class="p_context"> void scheduler_tick(void)</span>
 	raw_spin_lock(&amp;rq-&gt;lock);
 	update_rq_clock(rq);
 	curr-&gt;sched_class-&gt;task_tick(rq, curr, 0);
<span class="p_del">-	update_cpu_load_active(rq);</span>
<span class="p_add">+	cpu_load_update_active(rq);</span>
 	calc_global_load_tick(rq);
 	raw_spin_unlock(&amp;rq-&gt;lock);
 
<span class="p_chunk">@@ -2961,6 +3046,20 @@</span> <span class="p_context"> u64 scheduler_tick_max_deferment(void)</span>
 
 #if defined(CONFIG_PREEMPT) &amp;&amp; (defined(CONFIG_DEBUG_PREEMPT) || \
 				defined(CONFIG_PREEMPT_TRACER))
<span class="p_add">+/*</span>
<span class="p_add">+ * If the value passed in is equal to the current preempt count</span>
<span class="p_add">+ * then we just disabled preemption. Start timing the latency.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void preempt_latency_start(int val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (preempt_count() == val) {</span>
<span class="p_add">+		unsigned long ip = get_lock_parent_ip();</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_PREEMPT</span>
<span class="p_add">+		current-&gt;preempt_disable_ip = ip;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		trace_preempt_off(CALLER_ADDR0, ip);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
 
 void preempt_count_add(int val)
 {
<span class="p_chunk">@@ -2979,17 +3078,21 @@</span> <span class="p_context"> void preempt_count_add(int val)</span>
 	DEBUG_LOCKS_WARN_ON((preempt_count() &amp; PREEMPT_MASK) &gt;=
 				PREEMPT_MASK - 10);
 #endif
<span class="p_del">-	if (preempt_count() == val) {</span>
<span class="p_del">-		unsigned long ip = get_lock_parent_ip();</span>
<span class="p_del">-#ifdef CONFIG_DEBUG_PREEMPT</span>
<span class="p_del">-		current-&gt;preempt_disable_ip = ip;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		trace_preempt_off(CALLER_ADDR0, ip);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	preempt_latency_start(val);</span>
 }
 EXPORT_SYMBOL(preempt_count_add);
 NOKPROBE_SYMBOL(preempt_count_add);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If the value passed in equals to the current preempt count</span>
<span class="p_add">+ * then we just enabled preemption. Stop timing the latency.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void preempt_latency_stop(int val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (preempt_count() == val)</span>
<span class="p_add">+		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void preempt_count_sub(int val)
 {
 #ifdef CONFIG_DEBUG_PREEMPT
<span class="p_chunk">@@ -3006,13 +3109,15 @@</span> <span class="p_context"> void preempt_count_sub(int val)</span>
 		return;
 #endif
 
<span class="p_del">-	if (preempt_count() == val)</span>
<span class="p_del">-		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());</span>
<span class="p_add">+	preempt_latency_stop(val);</span>
 	__preempt_count_sub(val);
 }
 EXPORT_SYMBOL(preempt_count_sub);
 NOKPROBE_SYMBOL(preempt_count_sub);
 
<span class="p_add">+#else</span>
<span class="p_add">+static inline void preempt_latency_start(int val) { }</span>
<span class="p_add">+static inline void preempt_latency_stop(int val) { }</span>
 #endif
 
 /*
<span class="p_chunk">@@ -3065,7 +3170,7 @@</span> <span class="p_context"> static inline void schedule_debug(struct task_struct *prev)</span>
  * Pick up the highest-prio task:
  */
 static inline struct task_struct *
<span class="p_del">-pick_next_task(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+pick_next_task(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	const struct sched_class *class = &amp;fair_sched_class;
 	struct task_struct *p;
<span class="p_chunk">@@ -3076,20 +3181,20 @@</span> <span class="p_context"> pick_next_task(struct rq *rq, struct task_struct *prev)</span>
 	 */
 	if (likely(prev-&gt;sched_class == class &amp;&amp;
 		   rq-&gt;nr_running == rq-&gt;cfs.h_nr_running)) {
<span class="p_del">-		p = fair_sched_class.pick_next_task(rq, prev);</span>
<span class="p_add">+		p = fair_sched_class.pick_next_task(rq, prev, cookie);</span>
 		if (unlikely(p == RETRY_TASK))
 			goto again;
 
 		/* assumes fair_sched_class-&gt;next == idle_sched_class */
 		if (unlikely(!p))
<span class="p_del">-			p = idle_sched_class.pick_next_task(rq, prev);</span>
<span class="p_add">+			p = idle_sched_class.pick_next_task(rq, prev, cookie);</span>
 
 		return p;
 	}
 
 again:
 	for_each_class(class) {
<span class="p_del">-		p = class-&gt;pick_next_task(rq, prev);</span>
<span class="p_add">+		p = class-&gt;pick_next_task(rq, prev, cookie);</span>
 		if (p) {
 			if (unlikely(p == RETRY_TASK))
 				goto again;
<span class="p_chunk">@@ -3143,6 +3248,7 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 {
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
<span class="p_add">+	struct pin_cookie cookie;</span>
 	struct rq *rq;
 	int cpu;
 
<span class="p_chunk">@@ -3176,7 +3282,7 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 	 */
 	smp_mb__before_spinlock();
 	raw_spin_lock(&amp;rq-&gt;lock);
<span class="p_del">-	lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
 
 	rq-&gt;clock_skip_update &lt;&lt;= 1; /* promote REQ to ACT */
 
<span class="p_chunk">@@ -3198,7 +3304,7 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 
 				to_wakeup = wq_worker_sleeping(prev);
 				if (to_wakeup)
<span class="p_del">-					try_to_wake_up_local(to_wakeup);</span>
<span class="p_add">+					try_to_wake_up_local(to_wakeup, cookie);</span>
 			}
 		}
 		switch_count = &amp;prev-&gt;nvcsw;
<span class="p_chunk">@@ -3207,7 +3313,7 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 	if (task_on_rq_queued(prev))
 		update_rq_clock(rq);
 
<span class="p_del">-	next = pick_next_task(rq, prev);</span>
<span class="p_add">+	next = pick_next_task(rq, prev, cookie);</span>
 	clear_tsk_need_resched(prev);
 	clear_preempt_need_resched();
 	rq-&gt;clock_skip_update = 0;
<span class="p_chunk">@@ -3218,9 +3324,9 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 		++*switch_count;
 
 		trace_sched_switch(preempt, prev, next);
<span class="p_del">-		rq = context_switch(rq, prev, next); /* unlocks the rq */</span>
<span class="p_add">+		rq = context_switch(rq, prev, next, cookie); /* unlocks the rq */</span>
 	} else {
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		raw_spin_unlock_irq(&amp;rq-&gt;lock);
 	}
 
<span class="p_chunk">@@ -3287,8 +3393,23 @@</span> <span class="p_context"> void __sched schedule_preempt_disabled(void)</span>
 static void __sched notrace preempt_schedule_common(void)
 {
 	do {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Because the function tracer can trace preempt_count_sub()</span>
<span class="p_add">+		 * and it also uses preempt_enable/disable_notrace(), if</span>
<span class="p_add">+		 * NEED_RESCHED is set, the preempt_enable_notrace() called</span>
<span class="p_add">+		 * by the function tracer will call this function again and</span>
<span class="p_add">+		 * cause infinite recursion.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Preemption must be disabled here before the function</span>
<span class="p_add">+		 * tracer can trace. Break up preempt_disable() into two</span>
<span class="p_add">+		 * calls. One to disable preemption without fear of being</span>
<span class="p_add">+		 * traced. The other to still record the preemption latency,</span>
<span class="p_add">+		 * which can also be traced by the function tracer.</span>
<span class="p_add">+		 */</span>
 		preempt_disable_notrace();
<span class="p_add">+		preempt_latency_start(1);</span>
 		__schedule(true);
<span class="p_add">+		preempt_latency_stop(1);</span>
 		preempt_enable_no_resched_notrace();
 
 		/*
<span class="p_chunk">@@ -3340,7 +3461,21 @@</span> <span class="p_context"> asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)</span>
 		return;
 
 	do {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Because the function tracer can trace preempt_count_sub()</span>
<span class="p_add">+		 * and it also uses preempt_enable/disable_notrace(), if</span>
<span class="p_add">+		 * NEED_RESCHED is set, the preempt_enable_notrace() called</span>
<span class="p_add">+		 * by the function tracer will call this function again and</span>
<span class="p_add">+		 * cause infinite recursion.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Preemption must be disabled here before the function</span>
<span class="p_add">+		 * tracer can trace. Break up preempt_disable() into two</span>
<span class="p_add">+		 * calls. One to disable preemption without fear of being</span>
<span class="p_add">+		 * traced. The other to still record the preemption latency,</span>
<span class="p_add">+		 * which can also be traced by the function tracer.</span>
<span class="p_add">+		 */</span>
 		preempt_disable_notrace();
<span class="p_add">+		preempt_latency_start(1);</span>
 		/*
 		 * Needs preempt disabled in case user_exit() is traced
 		 * and the tracer calls preempt_enable_notrace() causing
<span class="p_chunk">@@ -3350,6 +3485,7 @@</span> <span class="p_context"> asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)</span>
 		__schedule(true);
 		exception_exit(prev_ctx);
 
<span class="p_add">+		preempt_latency_stop(1);</span>
 		preempt_enable_no_resched_notrace();
 	} while (need_resched());
 }
<span class="p_chunk">@@ -3406,12 +3542,13 @@</span> <span class="p_context"> EXPORT_SYMBOL(default_wake_function);</span>
 void rt_mutex_setprio(struct task_struct *p, int prio)
 {
 	int oldprio, queued, running, queue_flag = DEQUEUE_SAVE | DEQUEUE_MOVE;
<span class="p_del">-	struct rq *rq;</span>
 	const struct sched_class *prev_class;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 
 	BUG_ON(prio &gt; MAX_PRIO);
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * Idle task boosting is a nono in general. There is one
<span class="p_chunk">@@ -3487,7 +3624,7 @@</span> <span class="p_context"> void rt_mutex_setprio(struct task_struct *p, int prio)</span>
 	check_class_changed(rq, p, prev_class, oldprio);
 out_unlock:
 	preempt_disable(); /* avoid rq from going away on us */
<span class="p_del">-	__task_rq_unlock(rq);</span>
<span class="p_add">+	__task_rq_unlock(rq, &amp;rf);</span>
 
 	balance_callback(rq);
 	preempt_enable();
<span class="p_chunk">@@ -3497,7 +3634,7 @@</span> <span class="p_context"> void rt_mutex_setprio(struct task_struct *p, int prio)</span>
 void set_user_nice(struct task_struct *p, long nice)
 {
 	int old_prio, delta, queued;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
 	if (task_nice(p) == nice || nice &lt; MIN_NICE || nice &gt; MAX_NICE)
<span class="p_chunk">@@ -3506,7 +3643,7 @@</span> <span class="p_context"> void set_user_nice(struct task_struct *p, long nice)</span>
 	 * We have to be careful, if called from sys_setpriority(),
 	 * the task might be in the middle of scheduling on another CPU.
 	 */
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	/*
 	 * The RT priorities are set via sched_setscheduler(), but we still
 	 * allow the &#39;normal&#39; nice value to be set - but as expected
<span class="p_chunk">@@ -3537,7 +3674,7 @@</span> <span class="p_context"> void set_user_nice(struct task_struct *p, long nice)</span>
 			resched_curr(rq);
 	}
 out_unlock:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 EXPORT_SYMBOL(set_user_nice);
 
<span class="p_chunk">@@ -3834,11 +3971,11 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 		      MAX_RT_PRIO - 1 - attr-&gt;sched_priority;
 	int retval, oldprio, oldpolicy = -1, queued, running;
 	int new_effective_prio, policy = attr-&gt;sched_policy;
<span class="p_del">-	unsigned long flags;</span>
 	const struct sched_class *prev_class;
<span class="p_del">-	struct rq *rq;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE;
<span class="p_add">+	struct rq *rq;</span>
 
 	/* may grab non-irq protected spin_locks */
 	BUG_ON(in_interrupt());
<span class="p_chunk">@@ -3933,13 +4070,13 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 	 * To be able to change p-&gt;policy safely, the appropriate
 	 * runqueue lock must be held.
 	 */
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * Changing the policy of the stop threads its a very bad idea
 	 */
 	if (p == rq-&gt;stop) {
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return -EINVAL;
 	}
 
<span class="p_chunk">@@ -3956,7 +4093,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 			goto change;
 
 		p-&gt;sched_reset_on_fork = reset_on_fork;
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return 0;
 	}
 change:
<span class="p_chunk">@@ -3970,7 +4107,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 		if (rt_bandwidth_enabled() &amp;&amp; rt_policy(policy) &amp;&amp;
 				task_group(p)-&gt;rt_bandwidth.rt_runtime == 0 &amp;&amp;
 				!task_group_is_autogroup(task_group(p))) {
<span class="p_del">-			task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+			task_rq_unlock(rq, p, &amp;rf);</span>
 			return -EPERM;
 		}
 #endif
<span class="p_chunk">@@ -3985,7 +4122,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 			 */
 			if (!cpumask_subset(span, &amp;p-&gt;cpus_allowed) ||
 			    rq-&gt;rd-&gt;dl_bw.bw == 0) {
<span class="p_del">-				task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+				task_rq_unlock(rq, p, &amp;rf);</span>
 				return -EPERM;
 			}
 		}
<span class="p_chunk">@@ -3995,7 +4132,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 	/* recheck policy now with rq lock held */
 	if (unlikely(oldpolicy != -1 &amp;&amp; oldpolicy != p-&gt;policy)) {
 		policy = oldpolicy = -1;
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		goto recheck;
 	}
 
<span class="p_chunk">@@ -4005,7 +4142,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 	 * is available.
 	 */
 	if ((dl_policy(policy) || dl_task(p)) &amp;&amp; dl_overflow(p, policy, attr)) {
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return -EBUSY;
 	}
 
<span class="p_chunk">@@ -4050,7 +4187,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct task_struct *p,</span>
 
 	check_class_changed(rq, p, prev_class, oldprio);
 	preempt_disable(); /* avoid rq from going away on us */
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	if (pi)
 		rt_mutex_adjust_pi(p);
<span class="p_chunk">@@ -4903,10 +5040,10 @@</span> <span class="p_context"> SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,</span>
 {
 	struct task_struct *p;
 	unsigned int time_slice;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct timespec t;</span>
 	struct rq *rq;
 	int retval;
<span class="p_del">-	struct timespec t;</span>
 
 	if (pid &lt; 0)
 		return -EINVAL;
<span class="p_chunk">@@ -4921,11 +5058,11 @@</span> <span class="p_context"> SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid,</span>
 	if (retval)
 		goto out_unlock;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	time_slice = 0;
 	if (p-&gt;sched_class-&gt;get_rr_interval)
 		time_slice = p-&gt;sched_class-&gt;get_rr_interval(rq, p);
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	rcu_read_unlock();
 	jiffies_to_timespec(time_slice, &amp;t);
<span class="p_chunk">@@ -5001,7 +5138,8 @@</span> <span class="p_context"> void show_state_filter(unsigned long state_filter)</span>
 	touch_all_softlockup_watchdogs();
 
 #ifdef CONFIG_SCHED_DEBUG
<span class="p_del">-	sysrq_sched_debug_show();</span>
<span class="p_add">+	if (!state_filter)</span>
<span class="p_add">+		sysrq_sched_debug_show();</span>
 #endif
 	rcu_read_unlock();
 	/*
<span class="p_chunk">@@ -5163,6 +5301,8 @@</span> <span class="p_context"> int task_can_attach(struct task_struct *p,</span>
 
 #ifdef CONFIG_SMP
 
<span class="p_add">+static bool sched_smp_initialized __read_mostly;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_NUMA_BALANCING
 /* Migrate current task p to target_cpu */
 int migrate_task_to(struct task_struct *p, int target_cpu)
<span class="p_chunk">@@ -5188,11 +5328,11 @@</span> <span class="p_context"> int migrate_task_to(struct task_struct *p, int target_cpu)</span>
  */
 void sched_setnuma(struct task_struct *p, int nid)
 {
<span class="p_del">-	struct rq *rq;</span>
<span class="p_del">-	unsigned long flags;</span>
 	bool queued, running;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	queued = task_on_rq_queued(p);
 	running = task_current(rq, p);
 
<span class="p_chunk">@@ -5207,7 +5347,7 @@</span> <span class="p_context"> void sched_setnuma(struct task_struct *p, int nid)</span>
 		p-&gt;sched_class-&gt;set_curr_task(rq);
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE);
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_chunk">@@ -5223,7 +5363,7 @@</span> <span class="p_context"> void idle_task_exit(void)</span>
 	BUG_ON(cpu_online(smp_processor_id()));
 
 	if (mm != &amp;init_mm) {
<span class="p_del">-		switch_mm(mm, &amp;init_mm, current);</span>
<span class="p_add">+		switch_mm_irqs_off(mm, &amp;init_mm, current);</span>
 		finish_arch_post_lock_switch();
 	}
 	mmdrop(mm);
<span class="p_chunk">@@ -5271,6 +5411,7 @@</span> <span class="p_context"> static void migrate_tasks(struct rq *dead_rq)</span>
 {
 	struct rq *rq = dead_rq;
 	struct task_struct *next, *stop = rq-&gt;stop;
<span class="p_add">+	struct pin_cookie cookie;</span>
 	int dest_cpu;
 
 	/*
<span class="p_chunk">@@ -5302,8 +5443,8 @@</span> <span class="p_context"> static void migrate_tasks(struct rq *dead_rq)</span>
 		/*
 		 * pick_next_task assumes pinned rq-&gt;lock.
 		 */
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-		next = pick_next_task(rq, &amp;fake_task);</span>
<span class="p_add">+		cookie = lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		next = pick_next_task(rq, &amp;fake_task, cookie);</span>
 		BUG_ON(!next);
 		next-&gt;sched_class-&gt;put_prev_task(rq, next);
 
<span class="p_chunk">@@ -5316,7 +5457,7 @@</span> <span class="p_context"> static void migrate_tasks(struct rq *dead_rq)</span>
 		 * because !cpu_active at this point, which means load-balance
 		 * will not interfere. Also, stop-machine.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		raw_spin_unlock(&amp;rq-&gt;lock);
 		raw_spin_lock(&amp;next-&gt;pi_lock);
 		raw_spin_lock(&amp;rq-&gt;lock);
<span class="p_chunk">@@ -5377,127 +5518,13 @@</span> <span class="p_context"> static void set_rq_offline(struct rq *rq)</span>
 	}
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * migration_call - callback that gets triggered when a CPU is added.</span>
<span class="p_del">- * Here we can start up the necessary migration thread for the new CPU.</span>
<span class="p_del">- */</span>
<span class="p_del">-static int</span>
<span class="p_del">-migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)</span>
<span class="p_add">+static void set_cpu_rq_start_time(unsigned int cpu)</span>
 {
<span class="p_del">-	int cpu = (long)hcpu;</span>
<span class="p_del">-	unsigned long flags;</span>
 	struct rq *rq = cpu_rq(cpu);
 
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_del">-</span>
<span class="p_del">-	case CPU_UP_PREPARE:</span>
<span class="p_del">-		rq-&gt;calc_load_update = calc_load_update;</span>
<span class="p_del">-		account_reset_rq(rq);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-</span>
<span class="p_del">-	case CPU_ONLINE:</span>
<span class="p_del">-		/* Update our root-domain */</span>
<span class="p_del">-		raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);</span>
<span class="p_del">-		if (rq-&gt;rd) {</span>
<span class="p_del">-			BUG_ON(!cpumask_test_cpu(cpu, rq-&gt;rd-&gt;span));</span>
<span class="p_del">-</span>
<span class="p_del">-			set_rq_online(rq);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_del">-	case CPU_DYING:</span>
<span class="p_del">-		sched_ttwu_pending();</span>
<span class="p_del">-		/* Update our root-domain */</span>
<span class="p_del">-		raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);</span>
<span class="p_del">-		if (rq-&gt;rd) {</span>
<span class="p_del">-			BUG_ON(!cpumask_test_cpu(cpu, rq-&gt;rd-&gt;span));</span>
<span class="p_del">-			set_rq_offline(rq);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		migrate_tasks(rq);</span>
<span class="p_del">-		BUG_ON(rq-&gt;nr_running != 1); /* the migration thread */</span>
<span class="p_del">-		raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-</span>
<span class="p_del">-	case CPU_DEAD:</span>
<span class="p_del">-		calc_load_migrate(rq);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	update_max_interval();</span>
<span class="p_del">-</span>
<span class="p_del">-	return NOTIFY_OK;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Register at high priority so that task migration (migrate_all_tasks)</span>
<span class="p_del">- * happens before everything else.  This has to be lower priority than</span>
<span class="p_del">- * the notifier in the perf_event subsystem, though.</span>
<span class="p_del">- */</span>
<span class="p_del">-static struct notifier_block migration_notifier = {</span>
<span class="p_del">-	.notifier_call = migration_call,</span>
<span class="p_del">-	.priority = CPU_PRI_MIGRATION,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static void set_cpu_rq_start_time(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-	struct rq *rq = cpu_rq(cpu);</span>
 	rq-&gt;age_stamp = sched_clock_cpu(cpu);
 }
 
<span class="p_del">-static int sched_cpu_active(struct notifier_block *nfb,</span>
<span class="p_del">-				      unsigned long action, void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = (long)hcpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_del">-	case CPU_STARTING:</span>
<span class="p_del">-		set_cpu_rq_start_time();</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-</span>
<span class="p_del">-	case CPU_DOWN_FAILED:</span>
<span class="p_del">-		set_cpu_active(cpu, true);</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int sched_cpu_inactive(struct notifier_block *nfb,</span>
<span class="p_del">-					unsigned long action, void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_del">-	case CPU_DOWN_PREPARE:</span>
<span class="p_del">-		set_cpu_active((long)hcpu, false);</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init migration_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	void *cpu = (void *)(long)smp_processor_id();</span>
<span class="p_del">-	int err;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Initialize migration for the boot CPU */</span>
<span class="p_del">-	err = migration_call(&amp;migration_notifier, CPU_UP_PREPARE, cpu);</span>
<span class="p_del">-	BUG_ON(err == NOTIFY_BAD);</span>
<span class="p_del">-	migration_call(&amp;migration_notifier, CPU_ONLINE, cpu);</span>
<span class="p_del">-	register_cpu_notifier(&amp;migration_notifier);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Register cpu active notifiers */</span>
<span class="p_del">-	cpu_notifier(sched_cpu_active, CPU_PRI_SCHED_ACTIVE);</span>
<span class="p_del">-	cpu_notifier(sched_cpu_inactive, CPU_PRI_SCHED_INACTIVE);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-early_initcall(migration_init);</span>
<span class="p_del">-</span>
 static cpumask_var_t sched_domains_tmpmask; /* sched_domains_mutex */
 
 #ifdef CONFIG_SCHED_DEBUG
<span class="p_chunk">@@ -6645,10 +6672,10 @@</span> <span class="p_context"> static void sched_init_numa(void)</span>
 	init_numa_topology_type();
 }
 
<span class="p_del">-static void sched_domains_numa_masks_set(int cpu)</span>
<span class="p_add">+static void sched_domains_numa_masks_set(unsigned int cpu)</span>
 {
<span class="p_del">-	int i, j;</span>
 	int node = cpu_to_node(cpu);
<span class="p_add">+	int i, j;</span>
 
 	for (i = 0; i &lt; sched_domains_numa_levels; i++) {
 		for (j = 0; j &lt; nr_node_ids; j++) {
<span class="p_chunk">@@ -6658,51 +6685,20 @@</span> <span class="p_context"> static void sched_domains_numa_masks_set(int cpu)</span>
 	}
 }
 
<span class="p_del">-static void sched_domains_numa_masks_clear(int cpu)</span>
<span class="p_add">+static void sched_domains_numa_masks_clear(unsigned int cpu)</span>
 {
 	int i, j;
<span class="p_add">+</span>
 	for (i = 0; i &lt; sched_domains_numa_levels; i++) {
 		for (j = 0; j &lt; nr_node_ids; j++)
 			cpumask_clear_cpu(cpu, sched_domains_numa_masks[i][j]);
 	}
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Update sched_domains_numa_masks[level][node] array when new cpus</span>
<span class="p_del">- * are onlined.</span>
<span class="p_del">- */</span>
<span class="p_del">-static int sched_domains_numa_masks_update(struct notifier_block *nfb,</span>
<span class="p_del">-					   unsigned long action,</span>
<span class="p_del">-					   void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = (long)hcpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_del">-	case CPU_ONLINE:</span>
<span class="p_del">-		sched_domains_numa_masks_set(cpu);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-</span>
<span class="p_del">-	case CPU_DEAD:</span>
<span class="p_del">-		sched_domains_numa_masks_clear(cpu);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return NOTIFY_OK;</span>
<span class="p_del">-}</span>
 #else
<span class="p_del">-static inline void sched_init_numa(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int sched_domains_numa_masks_update(struct notifier_block *nfb,</span>
<span class="p_del">-					   unsigned long action,</span>
<span class="p_del">-					   void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_add">+static inline void sched_init_numa(void) { }</span>
<span class="p_add">+static void sched_domains_numa_masks_set(unsigned int cpu) { }</span>
<span class="p_add">+static void sched_domains_numa_masks_clear(unsigned int cpu) { }</span>
 #endif /* CONFIG_NUMA */
 
 static int __sdt_alloc(const struct cpumask *cpu_map)
<span class="p_chunk">@@ -7092,13 +7088,9 @@</span> <span class="p_context"> static int num_cpus_frozen;	/* used to mark begin/end of suspend/resume */</span>
  * If we come here as part of a suspend/resume, don&#39;t touch cpusets because we
  * want to restore it back to its original state upon resume anyway.
  */
<span class="p_del">-static int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,</span>
<span class="p_del">-			     void *hcpu)</span>
<span class="p_add">+static void cpuset_cpu_active(void)</span>
 {
<span class="p_del">-	switch (action) {</span>
<span class="p_del">-	case CPU_ONLINE_FROZEN:</span>
<span class="p_del">-	case CPU_DOWN_FAILED_FROZEN:</span>
<span class="p_del">-</span>
<span class="p_add">+	if (cpuhp_tasks_frozen) {</span>
 		/*
 		 * num_cpus_frozen tracks how many CPUs are involved in suspend
 		 * resume sequence. As long as this is not the last online
<span class="p_chunk">@@ -7108,35 +7100,25 @@</span> <span class="p_context"> static int cpuset_cpu_active(struct notifier_block *nfb, unsigned long action,</span>
 		num_cpus_frozen--;
 		if (likely(num_cpus_frozen)) {
 			partition_sched_domains(1, NULL, NULL);
<span class="p_del">-			break;</span>
<span class="p_add">+			return;</span>
 		}
<span class="p_del">-</span>
 		/*
 		 * This is the last CPU online operation. So fall through and
 		 * restore the original sched domains by considering the
 		 * cpuset configurations.
 		 */
<span class="p_del">-</span>
<span class="p_del">-	case CPU_ONLINE:</span>
<span class="p_del">-		cpuset_update_active_cpus(true);</span>
<span class="p_del">-		break;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
 	}
<span class="p_del">-	return NOTIFY_OK;</span>
<span class="p_add">+	cpuset_update_active_cpus(true);</span>
 }
 
<span class="p_del">-static int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,</span>
<span class="p_del">-			       void *hcpu)</span>
<span class="p_add">+static int cpuset_cpu_inactive(unsigned int cpu)</span>
 {
 	unsigned long flags;
<span class="p_del">-	long cpu = (long)hcpu;</span>
 	struct dl_bw *dl_b;
 	bool overflow;
 	int cpus;
 
<span class="p_del">-	switch (action) {</span>
<span class="p_del">-	case CPU_DOWN_PREPARE:</span>
<span class="p_add">+	if (!cpuhp_tasks_frozen) {</span>
 		rcu_read_lock_sched();
 		dl_b = dl_bw_of(cpu);
 
<span class="p_chunk">@@ -7148,19 +7130,120 @@</span> <span class="p_context"> static int cpuset_cpu_inactive(struct notifier_block *nfb, unsigned long action,</span>
 		rcu_read_unlock_sched();
 
 		if (overflow)
<span class="p_del">-			return notifier_from_errno(-EBUSY);</span>
<span class="p_add">+			return -EBUSY;</span>
 		cpuset_update_active_cpus(false);
<span class="p_del">-		break;</span>
<span class="p_del">-	case CPU_DOWN_PREPARE_FROZEN:</span>
<span class="p_add">+	} else {</span>
 		num_cpus_frozen++;
 		partition_sched_domains(1, NULL, NULL);
<span class="p_del">-		break;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
 	}
<span class="p_del">-	return NOTIFY_OK;</span>
<span class="p_add">+	return 0;</span>
 }
 
<span class="p_add">+int sched_cpu_activate(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq = cpu_rq(cpu);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_cpu_active(cpu, true);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sched_smp_initialized) {</span>
<span class="p_add">+		sched_domains_numa_masks_set(cpu);</span>
<span class="p_add">+		cpuset_cpu_active();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Put the rq online, if not already. This happens:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 1) In the early boot process, because we build the real domains</span>
<span class="p_add">+	 *    after all cpus have been brought up.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 2) At runtime, if cpuset_cpu_active() fails to rebuild the</span>
<span class="p_add">+	 *    domains.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);</span>
<span class="p_add">+	if (rq-&gt;rd) {</span>
<span class="p_add">+		BUG_ON(!cpumask_test_cpu(cpu, rq-&gt;rd-&gt;span));</span>
<span class="p_add">+		set_rq_online(rq);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	update_max_interval();</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int sched_cpu_deactivate(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_cpu_active(cpu, false);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We&#39;ve cleared cpu_active_mask, wait for all preempt-disabled and RCU</span>
<span class="p_add">+	 * users of this state to go away such that all new such users will</span>
<span class="p_add">+	 * observe it.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For CONFIG_PREEMPT we have preemptible RCU and its sync_rcu() might</span>
<span class="p_add">+	 * not imply sync_sched(), so wait for both.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Do sync before park smpboot threads to take care the rcu boost case.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PREEMPT))</span>
<span class="p_add">+		synchronize_rcu_mult(call_rcu, call_rcu_sched);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		synchronize_rcu();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!sched_smp_initialized)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cpuset_cpu_inactive(cpu);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		set_cpu_active(cpu, true);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	sched_domains_numa_masks_clear(cpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void sched_rq_cpu_starting(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq = cpu_rq(cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	rq-&gt;calc_load_update = calc_load_update;</span>
<span class="p_add">+	account_reset_rq(rq);</span>
<span class="p_add">+	update_max_interval();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int sched_cpu_starting(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_cpu_rq_start_time(cpu);</span>
<span class="p_add">+	sched_rq_cpu_starting(cpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_add">+int sched_cpu_dying(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq = cpu_rq(cpu);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Handle pending wakeups and then migrate everything off */</span>
<span class="p_add">+	sched_ttwu_pending();</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);</span>
<span class="p_add">+	if (rq-&gt;rd) {</span>
<span class="p_add">+		BUG_ON(!cpumask_test_cpu(cpu, rq-&gt;rd-&gt;span));</span>
<span class="p_add">+		set_rq_offline(rq);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	migrate_tasks(rq);</span>
<span class="p_add">+	BUG_ON(rq-&gt;nr_running != 1);</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);</span>
<span class="p_add">+	calc_load_migrate(rq);</span>
<span class="p_add">+	update_max_interval();</span>
<span class="p_add">+	nohz_balance_exit_idle(cpu);</span>
<span class="p_add">+	hrtick_clear(rq);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void __init sched_init_smp(void)
 {
 	cpumask_var_t non_isolated_cpus;
<span class="p_chunk">@@ -7182,12 +7265,6 @@</span> <span class="p_context"> void __init sched_init_smp(void)</span>
 		cpumask_set_cpu(smp_processor_id(), non_isolated_cpus);
 	mutex_unlock(&amp;sched_domains_mutex);
 
<span class="p_del">-	hotcpu_notifier(sched_domains_numa_masks_update, CPU_PRI_SCHED_ACTIVE);</span>
<span class="p_del">-	hotcpu_notifier(cpuset_cpu_active, CPU_PRI_CPUSET_ACTIVE);</span>
<span class="p_del">-	hotcpu_notifier(cpuset_cpu_inactive, CPU_PRI_CPUSET_INACTIVE);</span>
<span class="p_del">-</span>
<span class="p_del">-	init_hrtick();</span>
<span class="p_del">-</span>
 	/* Move init over to a non-isolated CPU */
 	if (set_cpus_allowed_ptr(current, non_isolated_cpus) &lt; 0)
 		BUG();
<span class="p_chunk">@@ -7196,7 +7273,16 @@</span> <span class="p_context"> void __init sched_init_smp(void)</span>
 
 	init_sched_rt_class();
 	init_sched_dl_class();
<span class="p_add">+	sched_smp_initialized = true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init migration_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	sched_rq_cpu_starting(smp_processor_id());</span>
<span class="p_add">+	return 0;</span>
 }
<span class="p_add">+early_initcall(migration_init);</span>
<span class="p_add">+</span>
 #else
 void __init sched_init_smp(void)
 {
<span class="p_chunk">@@ -7331,8 +7417,6 @@</span> <span class="p_context"> void __init sched_init(void)</span>
 		for (j = 0; j &lt; CPU_LOAD_IDX_MAX; j++)
 			rq-&gt;cpu_load[j] = 0;
 
<span class="p_del">-		rq-&gt;last_load_update_tick = jiffies;</span>
<span class="p_del">-</span>
 #ifdef CONFIG_SMP
 		rq-&gt;sd = NULL;
 		rq-&gt;rd = NULL;
<span class="p_chunk">@@ -7351,12 +7435,13 @@</span> <span class="p_context"> void __init sched_init(void)</span>
 
 		rq_attach_root(rq, &amp;def_root_domain);
 #ifdef CONFIG_NO_HZ_COMMON
<span class="p_add">+		rq-&gt;last_load_update_tick = jiffies;</span>
 		rq-&gt;nohz_flags = 0;
 #endif
 #ifdef CONFIG_NO_HZ_FULL
 		rq-&gt;last_sched_tick = 0;
 #endif
<span class="p_del">-#endif</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
 		init_rq_hrtick(rq);
 		atomic_set(&amp;rq-&gt;nr_iowait, 0);
 	}
<span class="p_chunk">@@ -7394,7 +7479,7 @@</span> <span class="p_context"> void __init sched_init(void)</span>
 	if (cpu_isolated_map == NULL)
 		zalloc_cpumask_var(&amp;cpu_isolated_map, GFP_NOWAIT);
 	idle_thread_set_boot_cpu();
<span class="p_del">-	set_cpu_rq_start_time();</span>
<span class="p_add">+	set_cpu_rq_start_time(smp_processor_id());</span>
 #endif
 	init_sched_fair_class();
 
<span class="p_chunk">@@ -7639,10 +7724,10 @@</span> <span class="p_context"> void sched_move_task(struct task_struct *tsk)</span>
 {
 	struct task_group *tg;
 	int queued, running;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	rq = task_rq_lock(tsk, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(tsk, &amp;rf);</span>
 
 	running = task_current(rq, tsk);
 	queued = task_on_rq_queued(tsk);
<span class="p_chunk">@@ -7674,7 +7759,7 @@</span> <span class="p_context"> void sched_move_task(struct task_struct *tsk)</span>
 	if (queued)
 		enqueue_task(rq, tsk, ENQUEUE_RESTORE | ENQUEUE_MOVE);
 
<span class="p_del">-	task_rq_unlock(rq, tsk, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, tsk, &amp;rf);</span>
 }
 #endif /* CONFIG_CGROUP_SCHED */
 
<span class="p_chunk">@@ -7894,7 +7979,7 @@</span> <span class="p_context"> static int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk)</span>
 static int sched_rt_global_constraints(void)
 {
 	unsigned long flags;
<span class="p_del">-	int i, ret = 0;</span>
<span class="p_add">+	int i;</span>
 
 	raw_spin_lock_irqsave(&amp;def_rt_bandwidth.rt_runtime_lock, flags);
 	for_each_possible_cpu(i) {
<span class="p_chunk">@@ -7906,7 +7991,7 @@</span> <span class="p_context"> static int sched_rt_global_constraints(void)</span>
 	}
 	raw_spin_unlock_irqrestore(&amp;def_rt_bandwidth.rt_runtime_lock, flags);
 
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return 0;</span>
 }
 #endif /* CONFIG_RT_GROUP_SCHED */
 
<span class="p_header">diff --git a/kernel/sched/cpuacct.c b/kernel/sched/cpuacct.c</span>
<span class="p_header">index 4a811203c04a..41f85c4d0938 100644</span>
<span class="p_header">--- a/kernel/sched/cpuacct.c</span>
<span class="p_header">+++ b/kernel/sched/cpuacct.c</span>
<span class="p_chunk">@@ -25,11 +25,22 @@</span> <span class="p_context"> enum cpuacct_stat_index {</span>
 	CPUACCT_STAT_NSTATS,
 };
 
<span class="p_add">+enum cpuacct_usage_index {</span>
<span class="p_add">+	CPUACCT_USAGE_USER,	/* ... user mode */</span>
<span class="p_add">+	CPUACCT_USAGE_SYSTEM,	/* ... kernel mode */</span>
<span class="p_add">+</span>
<span class="p_add">+	CPUACCT_USAGE_NRUSAGE,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct cpuacct_usage {</span>
<span class="p_add">+	u64	usages[CPUACCT_USAGE_NRUSAGE];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /* track cpu usage of a group of tasks and its child groups */
 struct cpuacct {
 	struct cgroup_subsys_state css;
 	/* cpuusage holds pointer to a u64-type object on every cpu */
<span class="p_del">-	u64 __percpu *cpuusage;</span>
<span class="p_add">+	struct cpuacct_usage __percpu *cpuusage;</span>
 	struct kernel_cpustat __percpu *cpustat;
 };
 
<span class="p_chunk">@@ -49,7 +60,7 @@</span> <span class="p_context"> static inline struct cpuacct *parent_ca(struct cpuacct *ca)</span>
 	return css_ca(ca-&gt;css.parent);
 }
 
<span class="p_del">-static DEFINE_PER_CPU(u64, root_cpuacct_cpuusage);</span>
<span class="p_add">+static DEFINE_PER_CPU(struct cpuacct_usage, root_cpuacct_cpuusage);</span>
 static struct cpuacct root_cpuacct = {
 	.cpustat	= &amp;kernel_cpustat,
 	.cpuusage	= &amp;root_cpuacct_cpuusage,
<span class="p_chunk">@@ -68,7 +79,7 @@</span> <span class="p_context"> cpuacct_css_alloc(struct cgroup_subsys_state *parent_css)</span>
 	if (!ca)
 		goto out;
 
<span class="p_del">-	ca-&gt;cpuusage = alloc_percpu(u64);</span>
<span class="p_add">+	ca-&gt;cpuusage = alloc_percpu(struct cpuacct_usage);</span>
 	if (!ca-&gt;cpuusage)
 		goto out_free_ca;
 
<span class="p_chunk">@@ -96,20 +107,37 @@</span> <span class="p_context"> static void cpuacct_css_free(struct cgroup_subsys_state *css)</span>
 	kfree(ca);
 }
 
<span class="p_del">-static u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)</span>
<span class="p_add">+static u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu,</span>
<span class="p_add">+				 enum cpuacct_usage_index index)</span>
 {
<span class="p_del">-	u64 *cpuusage = per_cpu_ptr(ca-&gt;cpuusage, cpu);</span>
<span class="p_add">+	struct cpuacct_usage *cpuusage = per_cpu_ptr(ca-&gt;cpuusage, cpu);</span>
 	u64 data;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We allow index == CPUACCT_USAGE_NRUSAGE here to read</span>
<span class="p_add">+	 * the sum of suages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(index &gt; CPUACCT_USAGE_NRUSAGE);</span>
<span class="p_add">+</span>
 #ifndef CONFIG_64BIT
 	/*
 	 * Take rq-&gt;lock to make 64-bit read safe on 32-bit platforms.
 	 */
 	raw_spin_lock_irq(&amp;cpu_rq(cpu)-&gt;lock);
<span class="p_del">-	data = *cpuusage;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	if (index == CPUACCT_USAGE_NRUSAGE) {</span>
<span class="p_add">+		int i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		data = 0;</span>
<span class="p_add">+		for (i = 0; i &lt; CPUACCT_USAGE_NRUSAGE; i++)</span>
<span class="p_add">+			data += cpuusage-&gt;usages[i];</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		data = cpuusage-&gt;usages[index];</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_64BIT</span>
 	raw_spin_unlock_irq(&amp;cpu_rq(cpu)-&gt;lock);
<span class="p_del">-#else</span>
<span class="p_del">-	data = *cpuusage;</span>
 #endif
 
 	return data;
<span class="p_chunk">@@ -117,69 +145,103 @@</span> <span class="p_context"> static u64 cpuacct_cpuusage_read(struct cpuacct *ca, int cpu)</span>
 
 static void cpuacct_cpuusage_write(struct cpuacct *ca, int cpu, u64 val)
 {
<span class="p_del">-	u64 *cpuusage = per_cpu_ptr(ca-&gt;cpuusage, cpu);</span>
<span class="p_add">+	struct cpuacct_usage *cpuusage = per_cpu_ptr(ca-&gt;cpuusage, cpu);</span>
<span class="p_add">+	int i;</span>
 
 #ifndef CONFIG_64BIT
 	/*
 	 * Take rq-&gt;lock to make 64-bit write safe on 32-bit platforms.
 	 */
 	raw_spin_lock_irq(&amp;cpu_rq(cpu)-&gt;lock);
<span class="p_del">-	*cpuusage = val;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; CPUACCT_USAGE_NRUSAGE; i++)</span>
<span class="p_add">+		cpuusage-&gt;usages[i] = val;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_64BIT</span>
 	raw_spin_unlock_irq(&amp;cpu_rq(cpu)-&gt;lock);
<span class="p_del">-#else</span>
<span class="p_del">-	*cpuusage = val;</span>
 #endif
 }
 
 /* return total cpu usage (in nanoseconds) of a group */
<span class="p_del">-static u64 cpuusage_read(struct cgroup_subsys_state *css, struct cftype *cft)</span>
<span class="p_add">+static u64 __cpuusage_read(struct cgroup_subsys_state *css,</span>
<span class="p_add">+			   enum cpuacct_usage_index index)</span>
 {
 	struct cpuacct *ca = css_ca(css);
 	u64 totalcpuusage = 0;
 	int i;
 
<span class="p_del">-	for_each_present_cpu(i)</span>
<span class="p_del">-		totalcpuusage += cpuacct_cpuusage_read(ca, i);</span>
<span class="p_add">+	for_each_possible_cpu(i)</span>
<span class="p_add">+		totalcpuusage += cpuacct_cpuusage_read(ca, i, index);</span>
 
 	return totalcpuusage;
 }
 
<span class="p_add">+static u64 cpuusage_user_read(struct cgroup_subsys_state *css,</span>
<span class="p_add">+			      struct cftype *cft)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuusage_read(css, CPUACCT_USAGE_USER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static u64 cpuusage_sys_read(struct cgroup_subsys_state *css,</span>
<span class="p_add">+			     struct cftype *cft)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuusage_read(css, CPUACCT_USAGE_SYSTEM);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static u64 cpuusage_read(struct cgroup_subsys_state *css, struct cftype *cft)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuusage_read(css, CPUACCT_USAGE_NRUSAGE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int cpuusage_write(struct cgroup_subsys_state *css, struct cftype *cft,
 			  u64 val)
 {
 	struct cpuacct *ca = css_ca(css);
<span class="p_del">-	int err = 0;</span>
<span class="p_del">-	int i;</span>
<span class="p_add">+	int cpu;</span>
 
 	/*
 	 * Only allow &#39;0&#39; here to do a reset.
 	 */
<span class="p_del">-	if (val) {</span>
<span class="p_del">-		err = -EINVAL;</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (val)</span>
<span class="p_add">+		return -EINVAL;</span>
 
<span class="p_del">-	for_each_present_cpu(i)</span>
<span class="p_del">-		cpuacct_cpuusage_write(ca, i, 0);</span>
<span class="p_add">+	for_each_possible_cpu(cpu)</span>
<span class="p_add">+		cpuacct_cpuusage_write(ca, cpu, 0);</span>
 
<span class="p_del">-out:</span>
<span class="p_del">-	return err;</span>
<span class="p_add">+	return 0;</span>
 }
 
<span class="p_del">-static int cpuacct_percpu_seq_show(struct seq_file *m, void *V)</span>
<span class="p_add">+static int __cpuacct_percpu_seq_show(struct seq_file *m,</span>
<span class="p_add">+				     enum cpuacct_usage_index index)</span>
 {
 	struct cpuacct *ca = css_ca(seq_css(m));
 	u64 percpu;
 	int i;
 
<span class="p_del">-	for_each_present_cpu(i) {</span>
<span class="p_del">-		percpu = cpuacct_cpuusage_read(ca, i);</span>
<span class="p_add">+	for_each_possible_cpu(i) {</span>
<span class="p_add">+		percpu = cpuacct_cpuusage_read(ca, i, index);</span>
 		seq_printf(m, &quot;%llu &quot;, (unsigned long long) percpu);
 	}
 	seq_printf(m, &quot;\n&quot;);
 	return 0;
 }
 
<span class="p_add">+static int cpuacct_percpu_user_seq_show(struct seq_file *m, void *V)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuacct_percpu_seq_show(m, CPUACCT_USAGE_USER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuacct_percpu_sys_seq_show(struct seq_file *m, void *V)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuacct_percpu_seq_show(m, CPUACCT_USAGE_SYSTEM);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuacct_percpu_seq_show(struct seq_file *m, void *V)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuacct_percpu_seq_show(m, CPUACCT_USAGE_NRUSAGE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const char * const cpuacct_stat_desc[] = {
 	[CPUACCT_STAT_USER] = &quot;user&quot;,
 	[CPUACCT_STAT_SYSTEM] = &quot;system&quot;,
<span class="p_chunk">@@ -191,7 +253,7 @@</span> <span class="p_context"> static int cpuacct_stats_show(struct seq_file *sf, void *v)</span>
 	int cpu;
 	s64 val = 0;
 
<span class="p_del">-	for_each_online_cpu(cpu) {</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
 		struct kernel_cpustat *kcpustat = per_cpu_ptr(ca-&gt;cpustat, cpu);
 		val += kcpustat-&gt;cpustat[CPUTIME_USER];
 		val += kcpustat-&gt;cpustat[CPUTIME_NICE];
<span class="p_chunk">@@ -200,7 +262,7 @@</span> <span class="p_context"> static int cpuacct_stats_show(struct seq_file *sf, void *v)</span>
 	seq_printf(sf, &quot;%s %lld\n&quot;, cpuacct_stat_desc[CPUACCT_STAT_USER], val);
 
 	val = 0;
<span class="p_del">-	for_each_online_cpu(cpu) {</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
 		struct kernel_cpustat *kcpustat = per_cpu_ptr(ca-&gt;cpustat, cpu);
 		val += kcpustat-&gt;cpustat[CPUTIME_SYSTEM];
 		val += kcpustat-&gt;cpustat[CPUTIME_IRQ];
<span class="p_chunk">@@ -220,10 +282,26 @@</span> <span class="p_context"> static struct cftype files[] = {</span>
 		.write_u64 = cpuusage_write,
 	},
 	{
<span class="p_add">+		.name = &quot;usage_user&quot;,</span>
<span class="p_add">+		.read_u64 = cpuusage_user_read,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		.name = &quot;usage_sys&quot;,</span>
<span class="p_add">+		.read_u64 = cpuusage_sys_read,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
 		.name = &quot;usage_percpu&quot;,
 		.seq_show = cpuacct_percpu_seq_show,
 	},
 	{
<span class="p_add">+		.name = &quot;usage_percpu_user&quot;,</span>
<span class="p_add">+		.seq_show = cpuacct_percpu_user_seq_show,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		.name = &quot;usage_percpu_sys&quot;,</span>
<span class="p_add">+		.seq_show = cpuacct_percpu_sys_seq_show,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
 		.name = &quot;stat&quot;,
 		.seq_show = cpuacct_stats_show,
 	},
<span class="p_chunk">@@ -238,10 +316,17 @@</span> <span class="p_context"> static struct cftype files[] = {</span>
 void cpuacct_charge(struct task_struct *tsk, u64 cputime)
 {
 	struct cpuacct *ca;
<span class="p_add">+	int index = CPUACCT_USAGE_SYSTEM;</span>
<span class="p_add">+	struct pt_regs *regs = task_pt_regs(tsk);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (regs &amp;&amp; user_mode(regs))</span>
<span class="p_add">+		index = CPUACCT_USAGE_USER;</span>
 
 	rcu_read_lock();
<span class="p_add">+</span>
 	for (ca = task_ca(tsk); ca; ca = parent_ca(ca))
<span class="p_del">-		*this_cpu_ptr(ca-&gt;cpuusage) += cputime;</span>
<span class="p_add">+		this_cpu_ptr(ca-&gt;cpuusage)-&gt;usages[index] += cputime;</span>
<span class="p_add">+</span>
 	rcu_read_unlock();
 }
 
<span class="p_header">diff --git a/kernel/sched/cpudeadline.c b/kernel/sched/cpudeadline.c</span>
<span class="p_header">index 5a75b08cfd85..5be58820465c 100644</span>
<span class="p_header">--- a/kernel/sched/cpudeadline.c</span>
<span class="p_header">+++ b/kernel/sched/cpudeadline.c</span>
<span class="p_chunk">@@ -103,10 +103,10 @@</span> <span class="p_context"> int cpudl_find(struct cpudl *cp, struct task_struct *p,</span>
 	const struct sched_dl_entity *dl_se = &amp;p-&gt;dl;
 
 	if (later_mask &amp;&amp;
<span class="p_del">-	    cpumask_and(later_mask, cp-&gt;free_cpus, &amp;p-&gt;cpus_allowed)) {</span>
<span class="p_add">+	    cpumask_and(later_mask, cp-&gt;free_cpus, tsk_cpus_allowed(p))) {</span>
 		best_cpu = cpumask_any(later_mask);
 		goto out;
<span class="p_del">-	} else if (cpumask_test_cpu(cpudl_maximum(cp), &amp;p-&gt;cpus_allowed) &amp;&amp;</span>
<span class="p_add">+	} else if (cpumask_test_cpu(cpudl_maximum(cp), tsk_cpus_allowed(p)) &amp;&amp;</span>
 			dl_time_before(dl_se-&gt;deadline, cp-&gt;elements[0].dl)) {
 		best_cpu = cpudl_maximum(cp);
 		if (later_mask)
<span class="p_header">diff --git a/kernel/sched/cpupri.c b/kernel/sched/cpupri.c</span>
<span class="p_header">index 981fcd7dc394..11e9705bf937 100644</span>
<span class="p_header">--- a/kernel/sched/cpupri.c</span>
<span class="p_header">+++ b/kernel/sched/cpupri.c</span>
<span class="p_chunk">@@ -103,11 +103,11 @@</span> <span class="p_context"> int cpupri_find(struct cpupri *cp, struct task_struct *p,</span>
 		if (skip)
 			continue;
 
<span class="p_del">-		if (cpumask_any_and(&amp;p-&gt;cpus_allowed, vec-&gt;mask) &gt;= nr_cpu_ids)</span>
<span class="p_add">+		if (cpumask_any_and(tsk_cpus_allowed(p), vec-&gt;mask) &gt;= nr_cpu_ids)</span>
 			continue;
 
 		if (lowest_mask) {
<span class="p_del">-			cpumask_and(lowest_mask, &amp;p-&gt;cpus_allowed, vec-&gt;mask);</span>
<span class="p_add">+			cpumask_and(lowest_mask, tsk_cpus_allowed(p), vec-&gt;mask);</span>
 
 			/*
 			 * We have to ensure that we have at least one bit
<span class="p_header">diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c</span>
<span class="p_header">index 686ec8adf952..fcb7f0217ff4 100644</span>
<span class="p_header">--- a/kernel/sched/deadline.c</span>
<span class="p_header">+++ b/kernel/sched/deadline.c</span>
<span class="p_chunk">@@ -134,7 +134,7 @@</span> <span class="p_context"> static void inc_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)</span>
 {
 	struct task_struct *p = dl_task_of(dl_se);
 
<span class="p_del">-	if (p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		dl_rq-&gt;dl_nr_migratory++;
 
 	update_dl_migration(dl_rq);
<span class="p_chunk">@@ -144,7 +144,7 @@</span> <span class="p_context"> static void dec_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)</span>
 {
 	struct task_struct *p = dl_task_of(dl_se);
 
<span class="p_del">-	if (p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		dl_rq-&gt;dl_nr_migratory--;
 
 	update_dl_migration(dl_rq);
<span class="p_chunk">@@ -591,10 +591,10 @@</span> <span class="p_context"> static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)</span>
 						     struct sched_dl_entity,
 						     dl_timer);
 	struct task_struct *p = dl_task_of(dl_se);
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * The task might have changed its scheduling policy to something
<span class="p_chunk">@@ -670,14 +670,14 @@</span> <span class="p_context"> static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)</span>
 		 * Nothing relies on rq-&gt;lock after this, so its safe to drop
 		 * rq-&gt;lock.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 		push_dl_task(rq);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, rf.cookie);</span>
 	}
 #endif
 
 unlock:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	/*
 	 * This can free the task_struct, including this hrtimer, do not touch
<span class="p_chunk">@@ -717,10 +717,6 @@</span> <span class="p_context"> static void update_curr_dl(struct rq *rq)</span>
 	if (!dl_task(curr) || !on_dl_rq(dl_se))
 		return;
 
<span class="p_del">-	/* Kick cpufreq (see the comment in linux/cpufreq.h). */</span>
<span class="p_del">-	if (cpu_of(rq) == smp_processor_id())</span>
<span class="p_del">-		cpufreq_trigger_update(rq_clock(rq));</span>
<span class="p_del">-</span>
 	/*
 	 * Consumed budget is computed considering the time as
 	 * observed by schedulable tasks (excluding time spent
<span class="p_chunk">@@ -736,6 +732,10 @@</span> <span class="p_context"> static void update_curr_dl(struct rq *rq)</span>
 		return;
 	}
 
<span class="p_add">+	/* kick cpufreq (see the comment in linux/cpufreq.h). */</span>
<span class="p_add">+	if (cpu_of(rq) == smp_processor_id())</span>
<span class="p_add">+		cpufreq_trigger_update(rq_clock(rq));</span>
<span class="p_add">+</span>
 	schedstat_set(curr-&gt;se.statistics.exec_max,
 		      max(curr-&gt;se.statistics.exec_max, delta_exec));
 
<span class="p_chunk">@@ -966,7 +966,7 @@</span> <span class="p_context"> static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)</span>
 
 	enqueue_dl_entity(&amp;p-&gt;dl, pi_se, flags);
 
<span class="p_del">-	if (!task_current(rq, p) &amp;&amp; p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (!task_current(rq, p) &amp;&amp; tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		enqueue_pushable_dl_task(rq, p);
 }
 
<span class="p_chunk">@@ -1040,9 +1040,9 @@</span> <span class="p_context"> select_task_rq_dl(struct task_struct *p, int cpu, int sd_flag, int flags)</span>
 	 * try to make it stay here, it might be important.
 	 */
 	if (unlikely(dl_task(curr)) &amp;&amp;
<span class="p_del">-	    (curr-&gt;nr_cpus_allowed &lt; 2 ||</span>
<span class="p_add">+	    (tsk_nr_cpus_allowed(curr) &lt; 2 ||</span>
 	     !dl_entity_preempt(&amp;p-&gt;dl, &amp;curr-&gt;dl)) &amp;&amp;
<span class="p_del">-	    (p-&gt;nr_cpus_allowed &gt; 1)) {</span>
<span class="p_add">+	    (tsk_nr_cpus_allowed(p) &gt; 1)) {</span>
 		int target = find_later_rq(p);
 
 		if (target != -1 &amp;&amp;
<span class="p_chunk">@@ -1063,7 +1063,7 @@</span> <span class="p_context"> static void check_preempt_equal_dl(struct rq *rq, struct task_struct *p)</span>
 	 * Current can&#39;t be migrated, useless to reschedule,
 	 * let&#39;s hope p can move out.
 	 */
<span class="p_del">-	if (rq-&gt;curr-&gt;nr_cpus_allowed == 1 ||</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(rq-&gt;curr) == 1 ||</span>
 	    cpudl_find(&amp;rq-&gt;rd-&gt;cpudl, rq-&gt;curr, NULL) == -1)
 		return;
 
<span class="p_chunk">@@ -1071,7 +1071,7 @@</span> <span class="p_context"> static void check_preempt_equal_dl(struct rq *rq, struct task_struct *p)</span>
 	 * p is migratable, so let&#39;s not schedule it and
 	 * see if it is pushed or pulled somewhere else.
 	 */
<span class="p_del">-	if (p-&gt;nr_cpus_allowed != 1 &amp;&amp;</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) != 1 &amp;&amp;</span>
 	    cpudl_find(&amp;rq-&gt;rd-&gt;cpudl, p, NULL) != -1)
 		return;
 
<span class="p_chunk">@@ -1125,7 +1125,8 @@</span> <span class="p_context"> static struct sched_dl_entity *pick_next_dl_entity(struct rq *rq,</span>
 	return rb_entry(left, struct sched_dl_entity, rb_node);
 }
 
<span class="p_del">-struct task_struct *pick_next_task_dl(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+struct task_struct *</span>
<span class="p_add">+pick_next_task_dl(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	struct sched_dl_entity *dl_se;
 	struct task_struct *p;
<span class="p_chunk">@@ -1140,9 +1141,9 @@</span> <span class="p_context"> struct task_struct *pick_next_task_dl(struct rq *rq, struct task_struct *prev)</span>
 		 * disabled avoiding further scheduler activity on it and we&#39;re
 		 * being very careful to re-start the picking loop.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		pull_dl_task(rq);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, cookie);</span>
 		/*
 		 * pull_rt_task() can drop (and re-acquire) rq-&gt;lock; this
 		 * means a stop task can slip in, in which case we need to
<span class="p_chunk">@@ -1185,7 +1186,7 @@</span> <span class="p_context"> static void put_prev_task_dl(struct rq *rq, struct task_struct *p)</span>
 {
 	update_curr_dl(rq);
 
<span class="p_del">-	if (on_dl_rq(&amp;p-&gt;dl) &amp;&amp; p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (on_dl_rq(&amp;p-&gt;dl) &amp;&amp; tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		enqueue_pushable_dl_task(rq, p);
 }
 
<span class="p_chunk">@@ -1286,7 +1287,7 @@</span> <span class="p_context"> static int find_later_rq(struct task_struct *task)</span>
 	if (unlikely(!later_mask))
 		return -1;
 
<span class="p_del">-	if (task-&gt;nr_cpus_allowed == 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(task) == 1)</span>
 		return -1;
 
 	/*
<span class="p_chunk">@@ -1392,7 +1393,7 @@</span> <span class="p_context"> static struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq)</span>
 		if (double_lock_balance(rq, later_rq)) {
 			if (unlikely(task_rq(task) != rq ||
 				     !cpumask_test_cpu(later_rq-&gt;cpu,
<span class="p_del">-				                       &amp;task-&gt;cpus_allowed) ||</span>
<span class="p_add">+						       tsk_cpus_allowed(task)) ||</span>
 				     task_running(rq, task) ||
 				     !dl_task(task) ||
 				     !task_on_rq_queued(task))) {
<span class="p_chunk">@@ -1432,7 +1433,7 @@</span> <span class="p_context"> static struct task_struct *pick_next_pushable_dl_task(struct rq *rq)</span>
 
 	BUG_ON(rq-&gt;cpu != task_cpu(p));
 	BUG_ON(task_current(rq, p));
<span class="p_del">-	BUG_ON(p-&gt;nr_cpus_allowed &lt;= 1);</span>
<span class="p_add">+	BUG_ON(tsk_nr_cpus_allowed(p) &lt;= 1);</span>
 
 	BUG_ON(!task_on_rq_queued(p));
 	BUG_ON(!dl_task(p));
<span class="p_chunk">@@ -1471,7 +1472,7 @@</span> <span class="p_context"> static int push_dl_task(struct rq *rq)</span>
 	 */
 	if (dl_task(rq-&gt;curr) &amp;&amp;
 	    dl_time_before(next_task-&gt;dl.deadline, rq-&gt;curr-&gt;dl.deadline) &amp;&amp;
<span class="p_del">-	    rq-&gt;curr-&gt;nr_cpus_allowed &gt; 1) {</span>
<span class="p_add">+	    tsk_nr_cpus_allowed(rq-&gt;curr) &gt; 1) {</span>
 		resched_curr(rq);
 		return 0;
 	}
<span class="p_chunk">@@ -1618,9 +1619,9 @@</span> <span class="p_context"> static void task_woken_dl(struct rq *rq, struct task_struct *p)</span>
 {
 	if (!task_running(rq, p) &amp;&amp;
 	    !test_tsk_need_resched(rq-&gt;curr) &amp;&amp;
<span class="p_del">-	    p-&gt;nr_cpus_allowed &gt; 1 &amp;&amp;</span>
<span class="p_add">+	    tsk_nr_cpus_allowed(p) &gt; 1 &amp;&amp;</span>
 	    dl_task(rq-&gt;curr) &amp;&amp;
<span class="p_del">-	    (rq-&gt;curr-&gt;nr_cpus_allowed &lt; 2 ||</span>
<span class="p_add">+	    (tsk_nr_cpus_allowed(rq-&gt;curr) &lt; 2 ||</span>
 	     !dl_entity_preempt(&amp;p-&gt;dl, &amp;rq-&gt;curr-&gt;dl))) {
 		push_dl_tasks(rq);
 	}
<span class="p_chunk">@@ -1724,7 +1725,7 @@</span> <span class="p_context"> static void switched_to_dl(struct rq *rq, struct task_struct *p)</span>
 
 	if (task_on_rq_queued(p) &amp;&amp; rq-&gt;curr != p) {
 #ifdef CONFIG_SMP
<span class="p_del">-		if (p-&gt;nr_cpus_allowed &gt; 1 &amp;&amp; rq-&gt;dl.overloaded)</span>
<span class="p_add">+		if (tsk_nr_cpus_allowed(p) &gt; 1 &amp;&amp; rq-&gt;dl.overloaded)</span>
 			queue_push_tasks(rq);
 #else
 		if (dl_task(rq-&gt;curr))
<span class="p_header">diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c</span>
<span class="p_header">index 4fbc3bd5ff60..cf905f655ba1 100644</span>
<span class="p_header">--- a/kernel/sched/debug.c</span>
<span class="p_header">+++ b/kernel/sched/debug.c</span>
<span class="p_chunk">@@ -626,15 +626,16 @@</span> <span class="p_context"> do {									\</span>
 #undef P
 #undef PN
 
<span class="p_del">-#ifdef CONFIG_SCHEDSTATS</span>
<span class="p_del">-#define P(n) SEQ_printf(m, &quot;  .%-30s: %d\n&quot;, #n, rq-&gt;n);</span>
<span class="p_del">-#define P64(n) SEQ_printf(m, &quot;  .%-30s: %Ld\n&quot;, #n, rq-&gt;n);</span>
<span class="p_del">-</span>
 #ifdef CONFIG_SMP
<span class="p_add">+#define P64(n) SEQ_printf(m, &quot;  .%-30s: %Ld\n&quot;, #n, rq-&gt;n);</span>
 	P64(avg_idle);
 	P64(max_idle_balance_cost);
<span class="p_add">+#undef P64</span>
 #endif
 
<span class="p_add">+#ifdef CONFIG_SCHEDSTATS</span>
<span class="p_add">+#define P(n) SEQ_printf(m, &quot;  .%-30s: %d\n&quot;, #n, rq-&gt;n);</span>
<span class="p_add">+</span>
 	if (schedstat_enabled()) {
 		P(yld_count);
 		P(sched_count);
<span class="p_chunk">@@ -644,7 +645,6 @@</span> <span class="p_context"> do {									\</span>
 	}
 
 #undef P
<span class="p_del">-#undef P64</span>
 #endif
 	spin_lock_irqsave(&amp;sched_debug_lock, flags);
 	print_cfs_stats(m, cpu);
<span class="p_header">diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c</span>
<span class="p_header">index e7dd0ec169be..218f8e83db73 100644</span>
<span class="p_header">--- a/kernel/sched/fair.c</span>
<span class="p_header">+++ b/kernel/sched/fair.c</span>
<span class="p_chunk">@@ -204,7 +204,7 @@</span> <span class="p_context"> static void __update_inv_weight(struct load_weight *lw)</span>
  *   OR
  * (delta_exec * (weight * lw-&gt;inv_weight)) &gt;&gt; WMULT_SHIFT
  *
<span class="p_del">- * Either weight := NICE_0_LOAD and lw \e prio_to_wmult[], in which case</span>
<span class="p_add">+ * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case</span>
  * we&#39;re guaranteed shift stays positive because inv_weight is guaranteed to
  * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift &gt;= 22.
  *
<span class="p_chunk">@@ -682,17 +682,68 @@</span> <span class="p_context"> void init_entity_runnable_average(struct sched_entity *se)</span>
 	sa-&gt;period_contrib = 1023;
 	sa-&gt;load_avg = scale_load_down(se-&gt;load.weight);
 	sa-&gt;load_sum = sa-&gt;load_avg * LOAD_AVG_MAX;
<span class="p_del">-	sa-&gt;util_avg = scale_load_down(SCHED_LOAD_SCALE);</span>
<span class="p_del">-	sa-&gt;util_sum = sa-&gt;util_avg * LOAD_AVG_MAX;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point, util_avg won&#39;t be used in select_task_rq_fair anyway</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	sa-&gt;util_avg = 0;</span>
<span class="p_add">+	sa-&gt;util_sum = 0;</span>
 	/* when this task enqueue&#39;ed, it will contribute to its cfs_rq&#39;s load_avg */
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * With new tasks being created, their initial util_avgs are extrapolated</span>
<span class="p_add">+ * based on the cfs_rq&#39;s current util_avg:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   util_avg = cfs_rq-&gt;util_avg / (cfs_rq-&gt;load_avg + 1) * se.load.weight</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * However, in many cases, the above util_avg does not give a desired</span>
<span class="p_add">+ * value. Moreover, the sum of the util_avgs may be divergent, such</span>
<span class="p_add">+ * as when the series is a harmonic series.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * To solve this problem, we also cap the util_avg of successive tasks to</span>
<span class="p_add">+ * only 1/2 of the left utilization budget:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   util_avg_cap = (1024 - cfs_rq-&gt;avg.util_avg) / 2^n</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * where n denotes the nth task.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For example, a simplest series from the beginning would be like:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  task  util_avg: 512, 256, 128,  64,  32,   16,    8, ...</span>
<span class="p_add">+ * cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ...</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap)</span>
<span class="p_add">+ * if util_avg &gt; util_avg_cap.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void post_init_entity_util_avg(struct sched_entity *se)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cfs_rq *cfs_rq = cfs_rq_of(se);</span>
<span class="p_add">+	struct sched_avg *sa = &amp;se-&gt;avg;</span>
<span class="p_add">+	long cap = (long)(SCHED_CAPACITY_SCALE - cfs_rq-&gt;avg.util_avg) / 2;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cap &gt; 0) {</span>
<span class="p_add">+		if (cfs_rq-&gt;avg.util_avg != 0) {</span>
<span class="p_add">+			sa-&gt;util_avg  = cfs_rq-&gt;avg.util_avg * se-&gt;load.weight;</span>
<span class="p_add">+			sa-&gt;util_avg /= (cfs_rq-&gt;avg.load_avg + 1);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (sa-&gt;util_avg &gt; cap)</span>
<span class="p_add">+				sa-&gt;util_avg = cap;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			sa-&gt;util_avg = cap;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		sa-&gt;util_sum = sa-&gt;util_avg * LOAD_AVG_MAX;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq);
 static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq);
 #else
 void init_entity_runnable_average(struct sched_entity *se)
 {
 }
<span class="p_add">+void post_init_entity_util_avg(struct sched_entity *se)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 #endif
 
 /*
<span class="p_chunk">@@ -2437,10 +2488,12 @@</span> <span class="p_context"> account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se)</span>
 	update_load_sub(&amp;cfs_rq-&gt;load, se-&gt;load.weight);
 	if (!parent_entity(se))
 		update_load_sub(&amp;rq_of(cfs_rq)-&gt;load, se-&gt;load.weight);
<span class="p_add">+#ifdef CONFIG_SMP</span>
 	if (entity_is_task(se)) {
 		account_numa_dequeue(rq_of(cfs_rq), task_of(se));
 		list_del_init(&amp;se-&gt;group_node);
 	}
<span class="p_add">+#endif</span>
 	cfs_rq-&gt;nr_running--;
 }
 
<span class="p_chunk">@@ -2550,6 +2603,16 @@</span> <span class="p_context"> static const u32 runnable_avg_yN_sum[] = {</span>
 };
 
 /*
<span class="p_add">+ * Precomputed \Sum y^k { 1&lt;=k&lt;=n, where n%32=0). Values are rolled down to</span>
<span class="p_add">+ * lower integers. See Documentation/scheduler/sched-avg.txt how these</span>
<span class="p_add">+ * were generated:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static const u32 __accumulated_sum_N32[] = {</span>
<span class="p_add">+	    0, 23371, 35056, 40899, 43820, 45281,</span>
<span class="p_add">+	46011, 46376, 46559, 46650, 46696, 46719,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Approximate:
  *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period)
  */
<span class="p_chunk">@@ -2597,22 +2660,13 @@</span> <span class="p_context"> static u32 __compute_runnable_contrib(u64 n)</span>
 	else if (unlikely(n &gt;= LOAD_AVG_MAX_N))
 		return LOAD_AVG_MAX;
 
<span class="p_del">-	/* Compute \Sum k^n combining precomputed values for k^i, \Sum k^j */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		contrib /= 2; /* y^LOAD_AVG_PERIOD = 1/2 */</span>
<span class="p_del">-		contrib += runnable_avg_yN_sum[LOAD_AVG_PERIOD];</span>
<span class="p_del">-</span>
<span class="p_del">-		n -= LOAD_AVG_PERIOD;</span>
<span class="p_del">-	} while (n &gt; LOAD_AVG_PERIOD);</span>
<span class="p_del">-</span>
<span class="p_add">+	/* Since n &lt; LOAD_AVG_MAX_N, n/LOAD_AVG_PERIOD &lt; 11 */</span>
<span class="p_add">+	contrib = __accumulated_sum_N32[n/LOAD_AVG_PERIOD];</span>
<span class="p_add">+	n %= LOAD_AVG_PERIOD;</span>
 	contrib = decay_load(contrib, n);
 	return contrib + runnable_avg_yN_sum[n];
 }
 
<span class="p_del">-#if (SCHED_LOAD_SHIFT - SCHED_LOAD_RESOLUTION) != 10 || SCHED_CAPACITY_SHIFT != 10</span>
<span class="p_del">-#error &quot;load tracking assumes 2^10 as unit&quot;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #define cap_scale(v, s) ((v)*(s) &gt;&gt; SCHED_CAPACITY_SHIFT)
 
 /*
<span class="p_chunk">@@ -2821,23 +2875,54 @@</span> <span class="p_context"> static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {}</span>
 
 static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq);
 
<span class="p_add">+static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *rq = rq_of(cfs_rq);</span>
<span class="p_add">+	int cpu = cpu_of(rq);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu == smp_processor_id() &amp;&amp; &amp;rq-&gt;cfs == cfs_rq) {</span>
<span class="p_add">+		unsigned long max = rq-&gt;cpu_capacity_orig;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * There are a few boundary cases this might miss but it should</span>
<span class="p_add">+		 * get called often enough that that should (hopefully) not be</span>
<span class="p_add">+		 * a real problem -- added to that it only calls on the local</span>
<span class="p_add">+		 * CPU, so if we enqueue remotely we&#39;ll miss an update, but</span>
<span class="p_add">+		 * the next tick/schedule should update.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * It will not get called when we go idle, because the idle</span>
<span class="p_add">+		 * thread is a different class (!fair), nor will the utilization</span>
<span class="p_add">+		 * number include things like RT tasks.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * As is, the util number is not freq-invariant (we&#39;d have to</span>
<span class="p_add">+		 * implement arch_scale_freq_capacity() for that).</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See cpu_util().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cpufreq_update_util(rq_clock(rq),</span>
<span class="p_add">+				    min(cfs_rq-&gt;avg.util_avg, max), max);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Group cfs_rq&#39;s load_avg is used for task_h_load and update_cfs_share */
<span class="p_del">-static inline int update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)</span>
<span class="p_add">+static inline int</span>
<span class="p_add">+update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq, bool update_freq)</span>
 {
 	struct sched_avg *sa = &amp;cfs_rq-&gt;avg;
<span class="p_del">-	int decayed, removed = 0;</span>
<span class="p_add">+	int decayed, removed_load = 0, removed_util = 0;</span>
 
 	if (atomic_long_read(&amp;cfs_rq-&gt;removed_load_avg)) {
 		s64 r = atomic_long_xchg(&amp;cfs_rq-&gt;removed_load_avg, 0);
 		sa-&gt;load_avg = max_t(long, sa-&gt;load_avg - r, 0);
 		sa-&gt;load_sum = max_t(s64, sa-&gt;load_sum - r * LOAD_AVG_MAX, 0);
<span class="p_del">-		removed = 1;</span>
<span class="p_add">+		removed_load = 1;</span>
 	}
 
 	if (atomic_long_read(&amp;cfs_rq-&gt;removed_util_avg)) {
 		long r = atomic_long_xchg(&amp;cfs_rq-&gt;removed_util_avg, 0);
 		sa-&gt;util_avg = max_t(long, sa-&gt;util_avg - r, 0);
 		sa-&gt;util_sum = max_t(s32, sa-&gt;util_sum - r * LOAD_AVG_MAX, 0);
<span class="p_add">+		removed_util = 1;</span>
 	}
 
 	decayed = __update_load_avg(now, cpu_of(rq_of(cfs_rq)), sa,
<span class="p_chunk">@@ -2848,7 +2933,10 @@</span> <span class="p_context"> static inline int update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)</span>
 	cfs_rq-&gt;load_last_update_time_copy = sa-&gt;last_update_time;
 #endif
 
<span class="p_del">-	return decayed || removed;</span>
<span class="p_add">+	if (update_freq &amp;&amp; (decayed || removed_util))</span>
<span class="p_add">+		cfs_rq_util_change(cfs_rq);</span>
<span class="p_add">+</span>
<span class="p_add">+	return decayed || removed_load;</span>
 }
 
 /* Update task and its cfs_rq load average */
<span class="p_chunk">@@ -2867,31 +2955,8 @@</span> <span class="p_context"> static inline void update_load_avg(struct sched_entity *se, int update_tg)</span>
 			  se-&gt;on_rq * scale_load_down(se-&gt;load.weight),
 			  cfs_rq-&gt;curr == se, NULL);
 
<span class="p_del">-	if (update_cfs_rq_load_avg(now, cfs_rq) &amp;&amp; update_tg)</span>
<span class="p_add">+	if (update_cfs_rq_load_avg(now, cfs_rq, true) &amp;&amp; update_tg)</span>
 		update_tg_load_avg(cfs_rq, 0);
<span class="p_del">-</span>
<span class="p_del">-	if (cpu == smp_processor_id() &amp;&amp; &amp;rq-&gt;cfs == cfs_rq) {</span>
<span class="p_del">-		unsigned long max = rq-&gt;cpu_capacity_orig;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * There are a few boundary cases this might miss but it should</span>
<span class="p_del">-		 * get called often enough that that should (hopefully) not be</span>
<span class="p_del">-		 * a real problem -- added to that it only calls on the local</span>
<span class="p_del">-		 * CPU, so if we enqueue remotely we&#39;ll miss an update, but</span>
<span class="p_del">-		 * the next tick/schedule should update.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * It will not get called when we go idle, because the idle</span>
<span class="p_del">-		 * thread is a different class (!fair), nor will the utilization</span>
<span class="p_del">-		 * number include things like RT tasks.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * As is, the util number is not freq-invariant (we&#39;d have to</span>
<span class="p_del">-		 * implement arch_scale_freq_capacity() for that).</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * See cpu_util().</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		cpufreq_update_util(rq_clock(rq),</span>
<span class="p_del">-				    min(cfs_rq-&gt;avg.util_avg, max), max);</span>
<span class="p_del">-	}</span>
 }
 
 static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
<span class="p_chunk">@@ -2919,6 +2984,8 @@</span> <span class="p_context"> static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *s</span>
 	cfs_rq-&gt;avg.load_sum += se-&gt;avg.load_sum;
 	cfs_rq-&gt;avg.util_avg += se-&gt;avg.util_avg;
 	cfs_rq-&gt;avg.util_sum += se-&gt;avg.util_sum;
<span class="p_add">+</span>
<span class="p_add">+	cfs_rq_util_change(cfs_rq);</span>
 }
 
 static void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
<span class="p_chunk">@@ -2931,6 +2998,8 @@</span> <span class="p_context"> static void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *s</span>
 	cfs_rq-&gt;avg.load_sum = max_t(s64,  cfs_rq-&gt;avg.load_sum - se-&gt;avg.load_sum, 0);
 	cfs_rq-&gt;avg.util_avg = max_t(long, cfs_rq-&gt;avg.util_avg - se-&gt;avg.util_avg, 0);
 	cfs_rq-&gt;avg.util_sum = max_t(s32,  cfs_rq-&gt;avg.util_sum - se-&gt;avg.util_sum, 0);
<span class="p_add">+</span>
<span class="p_add">+	cfs_rq_util_change(cfs_rq);</span>
 }
 
 /* Add the load generated by se into cfs_rq&#39;s load average */
<span class="p_chunk">@@ -2948,7 +3017,7 @@</span> <span class="p_context"> enqueue_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)</span>
 			cfs_rq-&gt;curr == se, NULL);
 	}
 
<span class="p_del">-	decayed = update_cfs_rq_load_avg(now, cfs_rq);</span>
<span class="p_add">+	decayed = update_cfs_rq_load_avg(now, cfs_rq, !migrated);</span>
 
 	cfs_rq-&gt;runnable_load_avg += sa-&gt;load_avg;
 	cfs_rq-&gt;runnable_load_sum += sa-&gt;load_sum;
<span class="p_chunk">@@ -3185,20 +3254,61 @@</span> <span class="p_context"> static inline void check_schedstat_required(void)</span>
 #endif
 }
 
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * MIGRATION</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	dequeue</span>
<span class="p_add">+ *	  update_curr()</span>
<span class="p_add">+ *	    update_min_vruntime()</span>
<span class="p_add">+ *	  vruntime -= min_vruntime</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	enqueue</span>
<span class="p_add">+ *	  update_curr()</span>
<span class="p_add">+ *	    update_min_vruntime()</span>
<span class="p_add">+ *	  vruntime += min_vruntime</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * this way the vruntime transition between RQs is done when both</span>
<span class="p_add">+ * min_vruntime are up-to-date.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * WAKEUP (remote)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	-&gt;migrate_task_rq_fair() (p-&gt;state == TASK_WAKING)</span>
<span class="p_add">+ *	  vruntime -= min_vruntime</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	enqueue</span>
<span class="p_add">+ *	  update_curr()</span>
<span class="p_add">+ *	    update_min_vruntime()</span>
<span class="p_add">+ *	  vruntime += min_vruntime</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * this way we don&#39;t have the most up-to-date min_vruntime on the originating</span>
<span class="p_add">+ * CPU and an up-to-date min_vruntime on the destination CPU.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
 static void
 enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
<span class="p_add">+	bool renorm = !(flags &amp; ENQUEUE_WAKEUP) || (flags &amp; ENQUEUE_MIGRATED);</span>
<span class="p_add">+	bool curr = cfs_rq-&gt;curr == se;</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * Update the normalized vruntime before updating min_vruntime</span>
<span class="p_del">-	 * through calling update_curr().</span>
<span class="p_add">+	 * If we&#39;re the current task, we must renormalise before calling</span>
<span class="p_add">+	 * update_curr().</span>
 	 */
<span class="p_del">-	if (!(flags &amp; ENQUEUE_WAKEUP) || (flags &amp; ENQUEUE_WAKING))</span>
<span class="p_add">+	if (renorm &amp;&amp; curr)</span>
 		se-&gt;vruntime += cfs_rq-&gt;min_vruntime;
 
<span class="p_add">+	update_curr(cfs_rq);</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * Update run-time statistics of the &#39;current&#39;.</span>
<span class="p_add">+	 * Otherwise, renormalise after, such that we&#39;re placed at the current</span>
<span class="p_add">+	 * moment in time, instead of some random moment in the past. Being</span>
<span class="p_add">+	 * placed in the past could significantly boost this task to the</span>
<span class="p_add">+	 * fairness detriment of existing tasks.</span>
 	 */
<span class="p_del">-	update_curr(cfs_rq);</span>
<span class="p_add">+	if (renorm &amp;&amp; !curr)</span>
<span class="p_add">+		se-&gt;vruntime += cfs_rq-&gt;min_vruntime;</span>
<span class="p_add">+</span>
 	enqueue_entity_load_avg(cfs_rq, se);
 	account_entity_enqueue(cfs_rq, se);
 	update_cfs_shares(cfs_rq);
<span class="p_chunk">@@ -3214,7 +3324,7 @@</span> <span class="p_context"> enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)</span>
 		update_stats_enqueue(cfs_rq, se);
 		check_spread(cfs_rq, se);
 	}
<span class="p_del">-	if (se != cfs_rq-&gt;curr)</span>
<span class="p_add">+	if (!curr)</span>
 		__enqueue_entity(cfs_rq, se);
 	se-&gt;on_rq = 1;
 
<span class="p_chunk">@@ -4422,7 +4532,7 @@</span> <span class="p_context"> static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)</span>
 }
 
 #ifdef CONFIG_SMP
<span class="p_del">-</span>
<span class="p_add">+#ifdef CONFIG_NO_HZ_COMMON</span>
 /*
  * per rq &#39;load&#39; arrray crap; XXX kill this.
  */
<span class="p_chunk">@@ -4488,13 +4598,13 @@</span> <span class="p_context"> decay_load_missed(unsigned long load, unsigned long missed_updates, int idx)</span>
 	}
 	return load;
 }
<span class="p_add">+#endif /* CONFIG_NO_HZ_COMMON */</span>
 
 /**
<span class="p_del">- * __update_cpu_load - update the rq-&gt;cpu_load[] statistics</span>
<span class="p_add">+ * __cpu_load_update - update the rq-&gt;cpu_load[] statistics</span>
  * @this_rq: The rq to update statistics for
  * @this_load: The current load
  * @pending_updates: The number of missed updates
<span class="p_del">- * @active: !0 for NOHZ_FULL</span>
  *
  * Update rq-&gt;cpu_load[] statistics. This function is usually called every
  * scheduler tick (TICK_NSEC).
<span class="p_chunk">@@ -4523,12 +4633,12 @@</span> <span class="p_context"> decay_load_missed(unsigned long load, unsigned long missed_updates, int idx)</span>
  *   load[i]_n = (1 - 1/2^i)^n * load[i]_0
  *
  * see decay_load_misses(). For NOHZ_FULL we get to subtract and add the extra
<span class="p_del">- * term. See the @active paramter.</span>
<span class="p_add">+ * term.</span>
  */
<span class="p_del">-static void __update_cpu_load(struct rq *this_rq, unsigned long this_load,</span>
<span class="p_del">-			      unsigned long pending_updates, int active)</span>
<span class="p_add">+static void cpu_load_update(struct rq *this_rq, unsigned long this_load,</span>
<span class="p_add">+			    unsigned long pending_updates)</span>
 {
<span class="p_del">-	unsigned long tickless_load = active ? this_rq-&gt;cpu_load[0] : 0;</span>
<span class="p_add">+	unsigned long __maybe_unused tickless_load = this_rq-&gt;cpu_load[0];</span>
 	int i, scale;
 
 	this_rq-&gt;nr_load_updates++;
<span class="p_chunk">@@ -4541,6 +4651,7 @@</span> <span class="p_context"> static void __update_cpu_load(struct rq *this_rq, unsigned long this_load,</span>
 		/* scale is effectively 1 &lt;&lt; i now, and &gt;&gt; i divides by scale */
 
 		old_load = this_rq-&gt;cpu_load[i];
<span class="p_add">+#ifdef CONFIG_NO_HZ_COMMON</span>
 		old_load = decay_load_missed(old_load, pending_updates - 1, i);
 		if (tickless_load) {
 			old_load -= decay_load_missed(tickless_load, pending_updates - 1, i);
<span class="p_chunk">@@ -4551,6 +4662,7 @@</span> <span class="p_context"> static void __update_cpu_load(struct rq *this_rq, unsigned long this_load,</span>
 			 */
 			old_load += tickless_load;
 		}
<span class="p_add">+#endif</span>
 		new_load = this_load;
 		/*
 		 * Round up the averaging division if load is increasing. This
<span class="p_chunk">@@ -4573,10 +4685,23 @@</span> <span class="p_context"> static unsigned long weighted_cpuload(const int cpu)</span>
 }
 
 #ifdef CONFIG_NO_HZ_COMMON
<span class="p_del">-static void __update_cpu_load_nohz(struct rq *this_rq,</span>
<span class="p_del">-				   unsigned long curr_jiffies,</span>
<span class="p_del">-				   unsigned long load,</span>
<span class="p_del">-				   int active)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * There is no sane way to deal with nohz on smp when using jiffies because the</span>
<span class="p_add">+ * cpu doing the jiffies update might drift wrt the cpu doing the jiffy reading</span>
<span class="p_add">+ * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Therefore we need to avoid the delta approach from the regular tick when</span>
<span class="p_add">+ * possible since that would seriously skew the load calculation. This is why we</span>
<span class="p_add">+ * use cpu_load_update_periodic() for CPUs out of nohz. However we&#39;ll rely on</span>
<span class="p_add">+ * jiffies deltas for updates happening while in nohz mode (idle ticks, idle</span>
<span class="p_add">+ * loop exit, nohz_idle_balance, nohz full exit...)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This means we might still be one tick off for nohz periods.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static void cpu_load_update_nohz(struct rq *this_rq,</span>
<span class="p_add">+				 unsigned long curr_jiffies,</span>
<span class="p_add">+				 unsigned long load)</span>
 {
 	unsigned long pending_updates;
 
<span class="p_chunk">@@ -4588,28 +4713,15 @@</span> <span class="p_context"> static void __update_cpu_load_nohz(struct rq *this_rq,</span>
 		 * In the NOHZ_FULL case, we were non-idle, we should consider
 		 * its weighted load.
 		 */
<span class="p_del">-		__update_cpu_load(this_rq, load, pending_updates, active);</span>
<span class="p_add">+		cpu_load_update(this_rq, load, pending_updates);</span>
 	}
 }
 
 /*
<span class="p_del">- * There is no sane way to deal with nohz on smp when using jiffies because the</span>
<span class="p_del">- * cpu doing the jiffies update might drift wrt the cpu doing the jiffy reading</span>
<span class="p_del">- * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Therefore we cannot use the delta approach from the regular tick since that</span>
<span class="p_del">- * would seriously skew the load calculation. However we&#39;ll make do for those</span>
<span class="p_del">- * updates happening while idle (nohz_idle_balance) or coming out of idle</span>
<span class="p_del">- * (tick_nohz_idle_exit).</span>
<span class="p_del">- *</span>
<span class="p_del">- * This means we might still be one tick off for nohz periods.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Called from nohz_idle_balance() to update the load ratings before doing the
  * idle balance.
  */
<span class="p_del">-static void update_cpu_load_idle(struct rq *this_rq)</span>
<span class="p_add">+static void cpu_load_update_idle(struct rq *this_rq)</span>
 {
 	/*
 	 * bail if there&#39;s load or we&#39;re actually up-to-date.
<span class="p_chunk">@@ -4617,38 +4729,71 @@</span> <span class="p_context"> static void update_cpu_load_idle(struct rq *this_rq)</span>
 	if (weighted_cpuload(cpu_of(this_rq)))
 		return;
 
<span class="p_del">-	__update_cpu_load_nohz(this_rq, READ_ONCE(jiffies), 0, 0);</span>
<span class="p_add">+	cpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0);</span>
 }
 
 /*
<span class="p_del">- * Called from tick_nohz_idle_exit() -- try and fix up the ticks we missed.</span>
<span class="p_add">+ * Record CPU load on nohz entry so we know the tickless load to account</span>
<span class="p_add">+ * on nohz exit. cpu_load[0] happens then to be updated more frequently</span>
<span class="p_add">+ * than other cpu_load[idx] but it should be fine as cpu_load readers</span>
<span class="p_add">+ * shouldn&#39;t rely into synchronized cpu_load[*] updates.</span>
  */
<span class="p_del">-void update_cpu_load_nohz(int active)</span>
<span class="p_add">+void cpu_load_update_nohz_start(void)</span>
 {
 	struct rq *this_rq = this_rq();
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is all lockless but should be fine. If weighted_cpuload changes</span>
<span class="p_add">+	 * concurrently we&#39;ll exit nohz. And cpu_load write can race with</span>
<span class="p_add">+	 * cpu_load_update_idle() but both updater would be writing the same.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	this_rq-&gt;cpu_load[0] = weighted_cpuload(cpu_of(this_rq));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Account the tickless load in the end of a nohz frame.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpu_load_update_nohz_stop(void)</span>
<span class="p_add">+{</span>
 	unsigned long curr_jiffies = READ_ONCE(jiffies);
<span class="p_del">-	unsigned long load = active ? weighted_cpuload(cpu_of(this_rq)) : 0;</span>
<span class="p_add">+	struct rq *this_rq = this_rq();</span>
<span class="p_add">+	unsigned long load;</span>
 
 	if (curr_jiffies == this_rq-&gt;last_load_update_tick)
 		return;
 
<span class="p_add">+	load = weighted_cpuload(cpu_of(this_rq));</span>
 	raw_spin_lock(&amp;this_rq-&gt;lock);
<span class="p_del">-	__update_cpu_load_nohz(this_rq, curr_jiffies, load, active);</span>
<span class="p_add">+	update_rq_clock(this_rq);</span>
<span class="p_add">+	cpu_load_update_nohz(this_rq, curr_jiffies, load);</span>
 	raw_spin_unlock(&amp;this_rq-&gt;lock);
 }
<span class="p_del">-#endif /* CONFIG_NO_HZ */</span>
<span class="p_add">+#else /* !CONFIG_NO_HZ_COMMON */</span>
<span class="p_add">+static inline void cpu_load_update_nohz(struct rq *this_rq,</span>
<span class="p_add">+					unsigned long curr_jiffies,</span>
<span class="p_add">+					unsigned long load) { }</span>
<span class="p_add">+#endif /* CONFIG_NO_HZ_COMMON */</span>
<span class="p_add">+</span>
<span class="p_add">+static void cpu_load_update_periodic(struct rq *this_rq, unsigned long load)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_NO_HZ_COMMON</span>
<span class="p_add">+	/* See the mess around cpu_load_update_nohz(). */</span>
<span class="p_add">+	this_rq-&gt;last_load_update_tick = READ_ONCE(jiffies);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	cpu_load_update(this_rq, load, 1);</span>
<span class="p_add">+}</span>
 
 /*
  * Called from scheduler_tick()
  */
<span class="p_del">-void update_cpu_load_active(struct rq *this_rq)</span>
<span class="p_add">+void cpu_load_update_active(struct rq *this_rq)</span>
 {
 	unsigned long load = weighted_cpuload(cpu_of(this_rq));
<span class="p_del">-	/*</span>
<span class="p_del">-	 * See the mess around update_cpu_load_idle() / update_cpu_load_nohz().</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	this_rq-&gt;last_load_update_tick = jiffies;</span>
<span class="p_del">-	__update_cpu_load(this_rq, load, 1, 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tick_nohz_tick_stopped())</span>
<span class="p_add">+		cpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		cpu_load_update_periodic(this_rq, load);</span>
 }
 
 /*
<span class="p_chunk">@@ -4706,46 +4851,6 @@</span> <span class="p_context"> static unsigned long cpu_avg_load_per_task(int cpu)</span>
 	return 0;
 }
 
<span class="p_del">-static void record_wakee(struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Rough decay (wiping) for cost saving, don&#39;t worry</span>
<span class="p_del">-	 * about the boundary, really active task won&#39;t care</span>
<span class="p_del">-	 * about the loss.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (time_after(jiffies, current-&gt;wakee_flip_decay_ts + HZ)) {</span>
<span class="p_del">-		current-&gt;wakee_flips &gt;&gt;= 1;</span>
<span class="p_del">-		current-&gt;wakee_flip_decay_ts = jiffies;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (current-&gt;last_wakee != p) {</span>
<span class="p_del">-		current-&gt;last_wakee = p;</span>
<span class="p_del">-		current-&gt;wakee_flips++;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void task_waking_fair(struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct sched_entity *se = &amp;p-&gt;se;</span>
<span class="p_del">-	struct cfs_rq *cfs_rq = cfs_rq_of(se);</span>
<span class="p_del">-	u64 min_vruntime;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef CONFIG_64BIT</span>
<span class="p_del">-	u64 min_vruntime_copy;</span>
<span class="p_del">-</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		min_vruntime_copy = cfs_rq-&gt;min_vruntime_copy;</span>
<span class="p_del">-		smp_rmb();</span>
<span class="p_del">-		min_vruntime = cfs_rq-&gt;min_vruntime;</span>
<span class="p_del">-	} while (min_vruntime != min_vruntime_copy);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	min_vruntime = cfs_rq-&gt;min_vruntime;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	se-&gt;vruntime -= min_vruntime;</span>
<span class="p_del">-	record_wakee(p);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /*
  * effective_load() calculates the load change as seen from the root_task_group
<span class="p_chunk">@@ -4861,17 +4966,39 @@</span> <span class="p_context"> static long effective_load(struct task_group *tg, int cpu, long wl, long wg)</span>
 
 #endif
 
<span class="p_add">+static void record_wakee(struct task_struct *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Only decay a single time; tasks that have less then 1 wakeup per</span>
<span class="p_add">+	 * jiffy will not have built up many flips.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (time_after(jiffies, current-&gt;wakee_flip_decay_ts + HZ)) {</span>
<span class="p_add">+		current-&gt;wakee_flips &gt;&gt;= 1;</span>
<span class="p_add">+		current-&gt;wakee_flip_decay_ts = jiffies;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (current-&gt;last_wakee != p) {</span>
<span class="p_add">+		current-&gt;last_wakee = p;</span>
<span class="p_add">+		current-&gt;wakee_flips++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Detect M:N waker/wakee relationships via a switching-frequency heuristic.
<span class="p_add">+ *</span>
  * A waker of many should wake a different task than the one last awakened
<span class="p_del">- * at a frequency roughly N times higher than one of its wakees.  In order</span>
<span class="p_del">- * to determine whether we should let the load spread vs consolodating to</span>
<span class="p_del">- * shared cache, we look for a minimum &#39;flip&#39; frequency of llc_size in one</span>
<span class="p_del">- * partner, and a factor of lls_size higher frequency in the other.  With</span>
<span class="p_del">- * both conditions met, we can be relatively sure that the relationship is</span>
<span class="p_del">- * non-monogamous, with partner count exceeding socket size.  Waker/wakee</span>
<span class="p_del">- * being client/server, worker/dispatcher, interrupt source or whatever is</span>
<span class="p_del">- * irrelevant, spread criteria is apparent partner count exceeds socket size.</span>
<span class="p_add">+ * at a frequency roughly N times higher than one of its wakees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * In order to determine whether we should let the load spread vs consolidating</span>
<span class="p_add">+ * to shared cache, we look for a minimum &#39;flip&#39; frequency of llc_size in one</span>
<span class="p_add">+ * partner, and a factor of lls_size higher frequency in the other.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With both conditions met, we can be relatively sure that the relationship is</span>
<span class="p_add">+ * non-monogamous, with partner count exceeding socket size.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Waker/wakee being client/server, worker/dispatcher, interrupt source or</span>
<span class="p_add">+ * whatever is irrelevant, spread criteria is apparent partner count exceeds</span>
<span class="p_add">+ * socket size.</span>
  */
 static int wake_wide(struct task_struct *p)
 {
<span class="p_chunk">@@ -5176,8 +5303,10 @@</span> <span class="p_context"> select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_f</span>
 	int want_affine = 0;
 	int sync = wake_flags &amp; WF_SYNC;
 
<span class="p_del">-	if (sd_flag &amp; SD_BALANCE_WAKE)</span>
<span class="p_add">+	if (sd_flag &amp; SD_BALANCE_WAKE) {</span>
<span class="p_add">+		record_wakee(p);</span>
 		want_affine = !wake_wide(p) &amp;&amp; cpumask_test_cpu(cpu, tsk_cpus_allowed(p));
<span class="p_add">+	}</span>
 
 	rcu_read_lock();
 	for_each_domain(cpu, tmp) {
<span class="p_chunk">@@ -5257,6 +5386,32 @@</span> <span class="p_context"> select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_f</span>
 static void migrate_task_rq_fair(struct task_struct *p)
 {
 	/*
<span class="p_add">+	 * As blocked tasks retain absolute vruntime the migration needs to</span>
<span class="p_add">+	 * deal with this by subtracting the old and adding the new</span>
<span class="p_add">+	 * min_vruntime -- the latter is done by enqueue_entity() when placing</span>
<span class="p_add">+	 * the task on the new runqueue.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (p-&gt;state == TASK_WAKING) {</span>
<span class="p_add">+		struct sched_entity *se = &amp;p-&gt;se;</span>
<span class="p_add">+		struct cfs_rq *cfs_rq = cfs_rq_of(se);</span>
<span class="p_add">+		u64 min_vruntime;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_64BIT</span>
<span class="p_add">+		u64 min_vruntime_copy;</span>
<span class="p_add">+</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			min_vruntime_copy = cfs_rq-&gt;min_vruntime_copy;</span>
<span class="p_add">+			smp_rmb();</span>
<span class="p_add">+			min_vruntime = cfs_rq-&gt;min_vruntime;</span>
<span class="p_add">+		} while (min_vruntime != min_vruntime_copy);</span>
<span class="p_add">+#else</span>
<span class="p_add">+		min_vruntime = cfs_rq-&gt;min_vruntime;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+		se-&gt;vruntime -= min_vruntime;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * We are supposed to update the task to &quot;current&quot; time, then its up to date
 	 * and ready to go to new CPU/cfs_rq. But we have difficulty in getting
 	 * what current time is, so simply throw away the out-of-date time. This
<span class="p_chunk">@@ -5439,7 +5594,7 @@</span> <span class="p_context"> static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_</span>
 }
 
 static struct task_struct *
<span class="p_del">-pick_next_task_fair(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	struct cfs_rq *cfs_rq = &amp;rq-&gt;cfs;
 	struct sched_entity *se;
<span class="p_chunk">@@ -5552,9 +5707,9 @@</span> <span class="p_context"> pick_next_task_fair(struct rq *rq, struct task_struct *prev)</span>
 	 * further scheduler activity on it and we&#39;re being very careful to
 	 * re-start the picking loop.
 	 */
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 	new_tasks = idle_balance(rq);
<span class="p_del">-	lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_repin_lock(&amp;rq-&gt;lock, cookie);</span>
 	/*
 	 * Because idle_balance() releases (and re-acquires) rq-&gt;lock, it is
 	 * possible for any higher priority task to appear. In that case we
<span class="p_chunk">@@ -5653,7 +5808,7 @@</span> <span class="p_context"> static bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preemp</span>
  *   W_i,0 = \Sum_j w_i,j                                             (2)
  *
  * Where w_i,j is the weight of the j-th runnable task on cpu i. This weight
<span class="p_del">- * is derived from the nice value as per prio_to_weight[].</span>
<span class="p_add">+ * is derived from the nice value as per sched_prio_to_weight[].</span>
  *
  * The weight average is an exponential decay average of the instantaneous
  * weight:
<span class="p_chunk">@@ -6155,7 +6310,7 @@</span> <span class="p_context"> static void update_blocked_averages(int cpu)</span>
 		if (throttled_hierarchy(cfs_rq))
 			continue;
 
<span class="p_del">-		if (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq))</span>
<span class="p_add">+		if (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq, true))</span>
 			update_tg_load_avg(cfs_rq, 0);
 	}
 	raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);
<span class="p_chunk">@@ -6216,7 +6371,7 @@</span> <span class="p_context"> static inline void update_blocked_averages(int cpu)</span>
 
 	raw_spin_lock_irqsave(&amp;rq-&gt;lock, flags);
 	update_rq_clock(rq);
<span class="p_del">-	update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq);</span>
<span class="p_add">+	update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq, true);</span>
 	raw_spin_unlock_irqrestore(&amp;rq-&gt;lock, flags);
 }
 
<span class="p_chunk">@@ -6625,6 +6780,9 @@</span> <span class="p_context"> static bool update_sd_pick_busiest(struct lb_env *env,</span>
 	if (!(env-&gt;sd-&gt;flags &amp; SD_ASYM_PACKING))
 		return true;
 
<span class="p_add">+	/* No ASYM_PACKING if target cpu is already busy */</span>
<span class="p_add">+	if (env-&gt;idle == CPU_NOT_IDLE)</span>
<span class="p_add">+		return true;</span>
 	/*
 	 * ASYM_PACKING needs to move all the work to the lowest
 	 * numbered CPUs in the group, therefore mark all groups
<span class="p_chunk">@@ -6634,7 +6792,8 @@</span> <span class="p_context"> static bool update_sd_pick_busiest(struct lb_env *env,</span>
 		if (!sds-&gt;busiest)
 			return true;
 
<span class="p_del">-		if (group_first_cpu(sds-&gt;busiest) &gt; group_first_cpu(sg))</span>
<span class="p_add">+		/* Prefer to move from highest possible cpu&#39;s work */</span>
<span class="p_add">+		if (group_first_cpu(sds-&gt;busiest) &lt; group_first_cpu(sg))</span>
 			return true;
 	}
 
<span class="p_chunk">@@ -6780,6 +6939,9 @@</span> <span class="p_context"> static int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds)</span>
 	if (!(env-&gt;sd-&gt;flags &amp; SD_ASYM_PACKING))
 		return 0;
 
<span class="p_add">+	if (env-&gt;idle == CPU_NOT_IDLE)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	if (!sds-&gt;busiest)
 		return 0;
 
<span class="p_chunk">@@ -6888,9 +7050,10 @@</span> <span class="p_context"> static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s</span>
 	}
 
 	/*
<span class="p_del">-	 * In the presence of smp nice balancing, certain scenarios can have</span>
<span class="p_del">-	 * max load less than avg load(as we skip the groups at or below</span>
<span class="p_del">-	 * its cpu_capacity, while calculating max_load..)</span>
<span class="p_add">+	 * Avg load of busiest sg can be less and avg load of local sg can</span>
<span class="p_add">+	 * be greater than avg load across all sgs of sd because avg load</span>
<span class="p_add">+	 * factors in sg capacity and sgs with smaller group_type are</span>
<span class="p_add">+	 * skipped when updating the busiest sg:</span>
 	 */
 	if (busiest-&gt;avg_load &lt;= sds-&gt;avg_load ||
 	    local-&gt;avg_load &gt;= sds-&gt;avg_load) {
<span class="p_chunk">@@ -6903,11 +7066,12 @@</span> <span class="p_context"> static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s</span>
 	 */
 	if (busiest-&gt;group_type == group_overloaded &amp;&amp;
 	    local-&gt;group_type   == group_overloaded) {
<span class="p_del">-		load_above_capacity = busiest-&gt;sum_nr_running *</span>
<span class="p_del">-					SCHED_LOAD_SCALE;</span>
<span class="p_del">-		if (load_above_capacity &gt; busiest-&gt;group_capacity)</span>
<span class="p_add">+		load_above_capacity = busiest-&gt;sum_nr_running * SCHED_CAPACITY_SCALE;</span>
<span class="p_add">+		if (load_above_capacity &gt; busiest-&gt;group_capacity) {</span>
 			load_above_capacity -= busiest-&gt;group_capacity;
<span class="p_del">-		else</span>
<span class="p_add">+			load_above_capacity *= NICE_0_LOAD;</span>
<span class="p_add">+			load_above_capacity /= busiest-&gt;group_capacity;</span>
<span class="p_add">+		} else</span>
 			load_above_capacity = ~0UL;
 	}
 
<span class="p_chunk">@@ -6915,9 +7079,8 @@</span> <span class="p_context"> static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s</span>
 	 * We&#39;re trying to get all the cpus to the average_load, so we don&#39;t
 	 * want to push ourselves above the average load, nor do we wish to
 	 * reduce the max loaded cpu below the average load. At the same time,
<span class="p_del">-	 * we also don&#39;t want to reduce the group load below the group capacity</span>
<span class="p_del">-	 * (so that we can implement power-savings policies etc). Thus we look</span>
<span class="p_del">-	 * for the minimum possible imbalance.</span>
<span class="p_add">+	 * we also don&#39;t want to reduce the group load below the group</span>
<span class="p_add">+	 * capacity. Thus we look for the minimum possible imbalance.</span>
 	 */
 	max_pull = min(busiest-&gt;avg_load - sds-&gt;avg_load, load_above_capacity);
 
<span class="p_chunk">@@ -6941,10 +7104,7 @@</span> <span class="p_context"> static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s</span>
 
 /**
  * find_busiest_group - Returns the busiest group within the sched_domain
<span class="p_del">- * if there is an imbalance. If there isn&#39;t an imbalance, and</span>
<span class="p_del">- * the user has opted for power-savings, it returns a group whose</span>
<span class="p_del">- * CPUs can be put to idle by rebalancing those tasks elsewhere, if</span>
<span class="p_del">- * such a group exists.</span>
<span class="p_add">+ * if there is an imbalance.</span>
  *
  * Also calculates the amount of weighted load which should be moved
  * to restore balance.
<span class="p_chunk">@@ -6952,9 +7112,6 @@</span> <span class="p_context"> static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s</span>
  * @env: The load balancing environment.
  *
  * Return:	- The busiest group if imbalance exists.
<span class="p_del">- *		- If no imbalance and user has opted for power-savings balance,</span>
<span class="p_del">- *		   return the least loaded group whose CPUs can be</span>
<span class="p_del">- *		   put to idle by rebalancing its tasks onto our group.</span>
  */
 static struct sched_group *find_busiest_group(struct lb_env *env)
 {
<span class="p_chunk">@@ -6972,8 +7129,7 @@</span> <span class="p_context"> static struct sched_group *find_busiest_group(struct lb_env *env)</span>
 	busiest = &amp;sds.busiest_stat;
 
 	/* ASYM feature bypasses nice load balance check */
<span class="p_del">-	if ((env-&gt;idle == CPU_IDLE || env-&gt;idle == CPU_NEWLY_IDLE) &amp;&amp;</span>
<span class="p_del">-	    check_asym_packing(env, &amp;sds))</span>
<span class="p_add">+	if (check_asym_packing(env, &amp;sds))</span>
 		return sds.busiest;
 
 	/* There is no busy sibling group to pull tasks from */
<span class="p_chunk">@@ -7398,10 +7554,7 @@</span> <span class="p_context"> static int load_balance(int this_cpu, struct rq *this_rq,</span>
 					&amp;busiest-&gt;active_balance_work);
 			}
 
<span class="p_del">-			/*</span>
<span class="p_del">-			 * We&#39;ve kicked active balancing, reset the failure</span>
<span class="p_del">-			 * counter.</span>
<span class="p_del">-			 */</span>
<span class="p_add">+			/* We&#39;ve kicked active balancing, force task migration. */</span>
 			sd-&gt;nr_balance_failed = sd-&gt;cache_nice_tries+1;
 		}
 	} else
<span class="p_chunk">@@ -7636,10 +7789,13 @@</span> <span class="p_context"> static int active_load_balance_cpu_stop(void *data)</span>
 		schedstat_inc(sd, alb_count);
 
 		p = detach_one_task(&amp;env);
<span class="p_del">-		if (p)</span>
<span class="p_add">+		if (p) {</span>
 			schedstat_inc(sd, alb_pushed);
<span class="p_del">-		else</span>
<span class="p_add">+			/* Active balancing done, reset the failure counter. */</span>
<span class="p_add">+			sd-&gt;nr_balance_failed = 0;</span>
<span class="p_add">+		} else {</span>
 			schedstat_inc(sd, alb_failed);
<span class="p_add">+		}</span>
 	}
 	rcu_read_unlock();
 out_unlock:
<span class="p_chunk">@@ -7710,7 +7866,7 @@</span> <span class="p_context"> static void nohz_balancer_kick(void)</span>
 	return;
 }
 
<span class="p_del">-static inline void nohz_balance_exit_idle(int cpu)</span>
<span class="p_add">+void nohz_balance_exit_idle(unsigned int cpu)</span>
 {
 	if (unlikely(test_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu)))) {
 		/*
<span class="p_chunk">@@ -7783,18 +7939,6 @@</span> <span class="p_context"> void nohz_balance_enter_idle(int cpu)</span>
 	atomic_inc(&amp;nohz.nr_cpus);
 	set_bit(NOHZ_TICK_STOPPED, nohz_flags(cpu));
 }
<span class="p_del">-</span>
<span class="p_del">-static int sched_ilb_notifier(struct notifier_block *nfb,</span>
<span class="p_del">-					unsigned long action, void *hcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_del">-	case CPU_DYING:</span>
<span class="p_del">-		nohz_balance_exit_idle(smp_processor_id());</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return NOTIFY_DONE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
 #endif
 
 static DEFINE_SPINLOCK(balancing);
<span class="p_chunk">@@ -7956,7 +8100,7 @@</span> <span class="p_context"> static void nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)</span>
 		if (time_after_eq(jiffies, rq-&gt;next_balance)) {
 			raw_spin_lock_irq(&amp;rq-&gt;lock);
 			update_rq_clock(rq);
<span class="p_del">-			update_cpu_load_idle(rq);</span>
<span class="p_add">+			cpu_load_update_idle(rq);</span>
 			raw_spin_unlock_irq(&amp;rq-&gt;lock);
 			rebalance_domains(rq, CPU_IDLE);
 		}
<span class="p_chunk">@@ -8381,6 +8525,7 @@</span> <span class="p_context"> int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)</span>
 		init_cfs_rq(cfs_rq);
 		init_tg_cfs_entry(tg, cfs_rq, se, i, parent-&gt;se[i]);
 		init_entity_runnable_average(se);
<span class="p_add">+		post_init_entity_util_avg(se);</span>
 	}
 
 	return 1;
<span class="p_chunk">@@ -8537,7 +8682,6 @@</span> <span class="p_context"> const struct sched_class fair_sched_class = {</span>
 	.rq_online		= rq_online_fair,
 	.rq_offline		= rq_offline_fair,
 
<span class="p_del">-	.task_waking		= task_waking_fair,</span>
 	.task_dead		= task_dead_fair,
 	.set_cpus_allowed	= set_cpus_allowed_common,
 #endif
<span class="p_chunk">@@ -8599,7 +8743,6 @@</span> <span class="p_context"> __init void init_sched_fair_class(void)</span>
 #ifdef CONFIG_NO_HZ_COMMON
 	nohz.next_balance = jiffies;
 	zalloc_cpumask_var(&amp;nohz.idle_cpus_mask, GFP_NOWAIT);
<span class="p_del">-	cpu_notifier(sched_ilb_notifier, 0);</span>
 #endif
 #endif /* SMP */
 
<span class="p_header">diff --git a/kernel/sched/idle_task.c b/kernel/sched/idle_task.c</span>
<span class="p_header">index 47ce94931f1b..2ce5458bbe1d 100644</span>
<span class="p_header">--- a/kernel/sched/idle_task.c</span>
<span class="p_header">+++ b/kernel/sched/idle_task.c</span>
<span class="p_chunk">@@ -24,7 +24,7 @@</span> <span class="p_context"> static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int fl</span>
 }
 
 static struct task_struct *
<span class="p_del">-pick_next_task_idle(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	put_prev_task(rq, prev);
 
<span class="p_header">diff --git a/kernel/sched/loadavg.c b/kernel/sched/loadavg.c</span>
<span class="p_header">index ef7159012cf3..b0b93fd33af9 100644</span>
<span class="p_header">--- a/kernel/sched/loadavg.c</span>
<span class="p_header">+++ b/kernel/sched/loadavg.c</span>
<span class="p_chunk">@@ -99,10 +99,13 @@</span> <span class="p_context"> long calc_load_fold_active(struct rq *this_rq)</span>
 static unsigned long
 calc_load(unsigned long load, unsigned long exp, unsigned long active)
 {
<span class="p_del">-	load *= exp;</span>
<span class="p_del">-	load += active * (FIXED_1 - exp);</span>
<span class="p_del">-	load += 1UL &lt;&lt; (FSHIFT - 1);</span>
<span class="p_del">-	return load &gt;&gt; FSHIFT;</span>
<span class="p_add">+	unsigned long newload;</span>
<span class="p_add">+</span>
<span class="p_add">+	newload = load * exp + active * (FIXED_1 - exp);</span>
<span class="p_add">+	if (active &gt;= load)</span>
<span class="p_add">+		newload += FIXED_1-1;</span>
<span class="p_add">+</span>
<span class="p_add">+	return newload / FIXED_1;</span>
 }
 
 #ifdef CONFIG_NO_HZ_COMMON
<span class="p_header">diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c</span>
<span class="p_header">index ec4f538d4396..d5690b722691 100644</span>
<span class="p_header">--- a/kernel/sched/rt.c</span>
<span class="p_header">+++ b/kernel/sched/rt.c</span>
<span class="p_chunk">@@ -334,7 +334,7 @@</span> <span class="p_context"> static void inc_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)</span>
 	rt_rq = &amp;rq_of_rt_rq(rt_rq)-&gt;rt;
 
 	rt_rq-&gt;rt_nr_total++;
<span class="p_del">-	if (p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		rt_rq-&gt;rt_nr_migratory++;
 
 	update_rt_migration(rt_rq);
<span class="p_chunk">@@ -351,7 +351,7 @@</span> <span class="p_context"> static void dec_rt_migration(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)</span>
 	rt_rq = &amp;rq_of_rt_rq(rt_rq)-&gt;rt;
 
 	rt_rq-&gt;rt_nr_total--;
<span class="p_del">-	if (p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		rt_rq-&gt;rt_nr_migratory--;
 
 	update_rt_migration(rt_rq);
<span class="p_chunk">@@ -953,14 +953,14 @@</span> <span class="p_context"> static void update_curr_rt(struct rq *rq)</span>
 	if (curr-&gt;sched_class != &amp;rt_sched_class)
 		return;
 
<span class="p_del">-	/* Kick cpufreq (see the comment in linux/cpufreq.h). */</span>
<span class="p_del">-	if (cpu_of(rq) == smp_processor_id())</span>
<span class="p_del">-		cpufreq_trigger_update(rq_clock(rq));</span>
<span class="p_del">-</span>
 	delta_exec = rq_clock_task(rq) - curr-&gt;se.exec_start;
 	if (unlikely((s64)delta_exec &lt;= 0))
 		return;
 
<span class="p_add">+	/* Kick cpufreq (see the comment in linux/cpufreq.h). */</span>
<span class="p_add">+	if (cpu_of(rq) == smp_processor_id())</span>
<span class="p_add">+		cpufreq_trigger_update(rq_clock(rq));</span>
<span class="p_add">+</span>
 	schedstat_set(curr-&gt;se.statistics.exec_max,
 		      max(curr-&gt;se.statistics.exec_max, delta_exec));
 
<span class="p_chunk">@@ -1324,7 +1324,7 @@</span> <span class="p_context"> enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)</span>
 
 	enqueue_rt_entity(rt_se, flags);
 
<span class="p_del">-	if (!task_current(rq, p) &amp;&amp; p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (!task_current(rq, p) &amp;&amp; tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		enqueue_pushable_task(rq, p);
 }
 
<span class="p_chunk">@@ -1413,7 +1413,7 @@</span> <span class="p_context"> select_task_rq_rt(struct task_struct *p, int cpu, int sd_flag, int flags)</span>
 	 * will have to sort it out.
 	 */
 	if (curr &amp;&amp; unlikely(rt_task(curr)) &amp;&amp;
<span class="p_del">-	    (curr-&gt;nr_cpus_allowed &lt; 2 ||</span>
<span class="p_add">+	    (tsk_nr_cpus_allowed(curr) &lt; 2 ||</span>
 	     curr-&gt;prio &lt;= p-&gt;prio)) {
 		int target = find_lowest_rq(p);
 
<span class="p_chunk">@@ -1437,7 +1437,7 @@</span> <span class="p_context"> static void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)</span>
 	 * Current can&#39;t be migrated, useless to reschedule,
 	 * let&#39;s hope p can move out.
 	 */
<span class="p_del">-	if (rq-&gt;curr-&gt;nr_cpus_allowed == 1 ||</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(rq-&gt;curr) == 1 ||</span>
 	    !cpupri_find(&amp;rq-&gt;rd-&gt;cpupri, rq-&gt;curr, NULL))
 		return;
 
<span class="p_chunk">@@ -1445,7 +1445,7 @@</span> <span class="p_context"> static void check_preempt_equal_prio(struct rq *rq, struct task_struct *p)</span>
 	 * p is migratable, so let&#39;s not schedule it and
 	 * see if it is pushed or pulled somewhere else.
 	 */
<span class="p_del">-	if (p-&gt;nr_cpus_allowed != 1</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(p) != 1</span>
 	    &amp;&amp; cpupri_find(&amp;rq-&gt;rd-&gt;cpupri, p, NULL))
 		return;
 
<span class="p_chunk">@@ -1524,7 +1524,7 @@</span> <span class="p_context"> static struct task_struct *_pick_next_task_rt(struct rq *rq)</span>
 }
 
 static struct task_struct *
<span class="p_del">-pick_next_task_rt(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+pick_next_task_rt(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	struct task_struct *p;
 	struct rt_rq *rt_rq = &amp;rq-&gt;rt;
<span class="p_chunk">@@ -1536,9 +1536,9 @@</span> <span class="p_context"> pick_next_task_rt(struct rq *rq, struct task_struct *prev)</span>
 		 * disabled avoiding further scheduler activity on it and we&#39;re
 		 * being very careful to re-start the picking loop.
 		 */
<span class="p_del">-		lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_unpin_lock(&amp;rq-&gt;lock, cookie);</span>
 		pull_rt_task(rq);
<span class="p_del">-		lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+		lockdep_repin_lock(&amp;rq-&gt;lock, cookie);</span>
 		/*
 		 * pull_rt_task() can drop (and re-acquire) rq-&gt;lock; this
 		 * means a dl or stop task can slip in, in which case we need
<span class="p_chunk">@@ -1579,7 +1579,7 @@</span> <span class="p_context"> static void put_prev_task_rt(struct rq *rq, struct task_struct *p)</span>
 	 * The previous task needs to be made eligible for pushing
 	 * if it is still active
 	 */
<span class="p_del">-	if (on_rt_rq(&amp;p-&gt;rt) &amp;&amp; p-&gt;nr_cpus_allowed &gt; 1)</span>
<span class="p_add">+	if (on_rt_rq(&amp;p-&gt;rt) &amp;&amp; tsk_nr_cpus_allowed(p) &gt; 1)</span>
 		enqueue_pushable_task(rq, p);
 }
 
<span class="p_chunk">@@ -1629,7 +1629,7 @@</span> <span class="p_context"> static int find_lowest_rq(struct task_struct *task)</span>
 	if (unlikely(!lowest_mask))
 		return -1;
 
<span class="p_del">-	if (task-&gt;nr_cpus_allowed == 1)</span>
<span class="p_add">+	if (tsk_nr_cpus_allowed(task) == 1)</span>
 		return -1; /* No other targets possible */
 
 	if (!cpupri_find(&amp;task_rq(task)-&gt;rd-&gt;cpupri, task, lowest_mask))
<span class="p_chunk">@@ -1762,7 +1762,7 @@</span> <span class="p_context"> static struct task_struct *pick_next_pushable_task(struct rq *rq)</span>
 
 	BUG_ON(rq-&gt;cpu != task_cpu(p));
 	BUG_ON(task_current(rq, p));
<span class="p_del">-	BUG_ON(p-&gt;nr_cpus_allowed &lt;= 1);</span>
<span class="p_add">+	BUG_ON(tsk_nr_cpus_allowed(p) &lt;= 1);</span>
 
 	BUG_ON(!task_on_rq_queued(p));
 	BUG_ON(!rt_task(p));
<span class="p_chunk">@@ -2122,9 +2122,9 @@</span> <span class="p_context"> static void task_woken_rt(struct rq *rq, struct task_struct *p)</span>
 {
 	if (!task_running(rq, p) &amp;&amp;
 	    !test_tsk_need_resched(rq-&gt;curr) &amp;&amp;
<span class="p_del">-	    p-&gt;nr_cpus_allowed &gt; 1 &amp;&amp;</span>
<span class="p_add">+	    tsk_nr_cpus_allowed(p) &gt; 1 &amp;&amp;</span>
 	    (dl_task(rq-&gt;curr) || rt_task(rq-&gt;curr)) &amp;&amp;
<span class="p_del">-	    (rq-&gt;curr-&gt;nr_cpus_allowed &lt; 2 ||</span>
<span class="p_add">+	    (tsk_nr_cpus_allowed(rq-&gt;curr) &lt; 2 ||</span>
 	     rq-&gt;curr-&gt;prio &lt;= p-&gt;prio))
 		push_rt_tasks(rq);
 }
<span class="p_chunk">@@ -2197,7 +2197,7 @@</span> <span class="p_context"> static void switched_to_rt(struct rq *rq, struct task_struct *p)</span>
 	 */
 	if (task_on_rq_queued(p) &amp;&amp; rq-&gt;curr != p) {
 #ifdef CONFIG_SMP
<span class="p_del">-		if (p-&gt;nr_cpus_allowed &gt; 1 &amp;&amp; rq-&gt;rt.overloaded)</span>
<span class="p_add">+		if (tsk_nr_cpus_allowed(p) &gt; 1 &amp;&amp; rq-&gt;rt.overloaded)</span>
 			queue_push_tasks(rq);
 #else
 		if (p-&gt;prio &lt; rq-&gt;curr-&gt;prio)
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index ec2e8d23527e..e51145e76807 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -31,9 +31,9 @@</span> <span class="p_context"> extern void calc_global_load_tick(struct rq *this_rq);</span>
 extern long calc_load_fold_active(struct rq *this_rq);
 
 #ifdef CONFIG_SMP
<span class="p_del">-extern void update_cpu_load_active(struct rq *this_rq);</span>
<span class="p_add">+extern void cpu_load_update_active(struct rq *this_rq);</span>
 #else
<span class="p_del">-static inline void update_cpu_load_active(struct rq *this_rq) { }</span>
<span class="p_add">+static inline void cpu_load_update_active(struct rq *this_rq) { }</span>
 #endif
 
 /*
<span class="p_chunk">@@ -49,25 +49,32 @@</span> <span class="p_context"> static inline void update_cpu_load_active(struct rq *this_rq) { }</span>
  * and does not change the user-interface for setting shares/weights.
  *
  * We increase resolution only if we have enough bits to allow this increased
<span class="p_del">- * resolution (i.e. BITS_PER_LONG &gt; 32). The costs for increasing resolution</span>
<span class="p_del">- * when BITS_PER_LONG &lt;= 32 are pretty high and the returns do not justify the</span>
<span class="p_del">- * increased costs.</span>
<span class="p_add">+ * resolution (i.e. 64bit). The costs for increasing resolution when 32bit are</span>
<span class="p_add">+ * pretty high and the returns do not justify the increased costs.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Really only required when CONFIG_FAIR_GROUP_SCHED is also set, but to</span>
<span class="p_add">+ * increase coverage and consistency always enable it on 64bit platforms.</span>
  */
<span class="p_del">-#if 0 /* BITS_PER_LONG &gt; 32 -- currently broken: it increases power usage under light load  */</span>
<span class="p_del">-# define SCHED_LOAD_RESOLUTION	10</span>
<span class="p_del">-# define scale_load(w)		((w) &lt;&lt; SCHED_LOAD_RESOLUTION)</span>
<span class="p_del">-# define scale_load_down(w)	((w) &gt;&gt; SCHED_LOAD_RESOLUTION)</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)</span>
<span class="p_add">+# define scale_load(w)		((w) &lt;&lt; SCHED_FIXEDPOINT_SHIFT)</span>
<span class="p_add">+# define scale_load_down(w)	((w) &gt;&gt; SCHED_FIXEDPOINT_SHIFT)</span>
 #else
<span class="p_del">-# define SCHED_LOAD_RESOLUTION	0</span>
<span class="p_add">+# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)</span>
 # define scale_load(w)		(w)
 # define scale_load_down(w)	(w)
 #endif
 
<span class="p_del">-#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)</span>
<span class="p_del">-#define SCHED_LOAD_SCALE	(1L &lt;&lt; SCHED_LOAD_SHIFT)</span>
<span class="p_del">-</span>
<span class="p_del">-#define NICE_0_LOAD		SCHED_LOAD_SCALE</span>
<span class="p_del">-#define NICE_0_SHIFT		SCHED_LOAD_SHIFT</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Task weight (visible to users) and its load (invisible to users) have</span>
<span class="p_add">+ * independent resolution, but they should be well calibrated. We use</span>
<span class="p_add">+ * scale_load() and scale_load_down(w) to convert between them. The</span>
<span class="p_add">+ * following must be true:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  scale_load(sched_prio_to_weight[USER_PRIO(NICE_TO_PRIO(0))]) == NICE_0_LOAD</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define NICE_0_LOAD		(1L &lt;&lt; NICE_0_LOAD_SHIFT)</span>
 
 /*
  * Single value that decides SCHED_DEADLINE internal math precision.
<span class="p_chunk">@@ -585,11 +592,13 @@</span> <span class="p_context"> struct rq {</span>
 #endif
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
<span class="p_del">-	unsigned long last_load_update_tick;</span>
 #ifdef CONFIG_NO_HZ_COMMON
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	unsigned long last_load_update_tick;</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
 	u64 nohz_stamp;
 	unsigned long nohz_flags;
<span class="p_del">-#endif</span>
<span class="p_add">+#endif /* CONFIG_NO_HZ_COMMON */</span>
 #ifdef CONFIG_NO_HZ_FULL
 	unsigned long last_sched_tick;
 #endif
<span class="p_chunk">@@ -854,7 +863,7 @@</span> <span class="p_context"> DECLARE_PER_CPU(struct sched_domain *, sd_asym);</span>
 struct sched_group_capacity {
 	atomic_t ref;
 	/*
<span class="p_del">-	 * CPU capacity of this group, SCHED_LOAD_SCALE being max capacity</span>
<span class="p_add">+	 * CPU capacity of this group, SCHED_CAPACITY_SCALE being max capacity</span>
 	 * for a single CPU.
 	 */
 	unsigned int capacity;
<span class="p_chunk">@@ -1159,7 +1168,7 @@</span> <span class="p_context"> extern const u32 sched_prio_to_wmult[40];</span>
  *
  * ENQUEUE_HEAD      - place at front of runqueue (tail if not specified)
  * ENQUEUE_REPLENISH - CBS (replenish runtime and postpone deadline)
<span class="p_del">- * ENQUEUE_WAKING    - sched_class::task_waking was called</span>
<span class="p_add">+ * ENQUEUE_MIGRATED  - the task was migrated during wakeup</span>
  *
  */
 
<span class="p_chunk">@@ -1174,9 +1183,9 @@</span> <span class="p_context"> extern const u32 sched_prio_to_wmult[40];</span>
 #define ENQUEUE_HEAD		0x08
 #define ENQUEUE_REPLENISH	0x10
 #ifdef CONFIG_SMP
<span class="p_del">-#define ENQUEUE_WAKING		0x20</span>
<span class="p_add">+#define ENQUEUE_MIGRATED	0x20</span>
 #else
<span class="p_del">-#define ENQUEUE_WAKING		0x00</span>
<span class="p_add">+#define ENQUEUE_MIGRATED	0x00</span>
 #endif
 
 #define RETRY_TASK		((void *)-1UL)
<span class="p_chunk">@@ -1200,14 +1209,14 @@</span> <span class="p_context"> struct sched_class {</span>
 	 * tasks.
 	 */
 	struct task_struct * (*pick_next_task) (struct rq *rq,
<span class="p_del">-						struct task_struct *prev);</span>
<span class="p_add">+						struct task_struct *prev,</span>
<span class="p_add">+						struct pin_cookie cookie);</span>
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p);
 
<span class="p_del">-	void (*task_waking) (struct task_struct *task);</span>
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
<span class="p_chunk">@@ -1313,6 +1322,7 @@</span> <span class="p_context"> extern void init_dl_task_timer(struct sched_dl_entity *dl_se);</span>
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
<span class="p_add">+extern void post_init_entity_util_avg(struct sched_entity *se);</span>
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(struct rq *rq);
<span class="p_chunk">@@ -1448,86 +1458,32 @@</span> <span class="p_context"> static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }</span>
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
<span class="p_del">-/*</span>
<span class="p_del">- * __task_rq_lock - lock the rq @p resides on.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct rq *__task_rq_lock(struct task_struct *p)</span>
<span class="p_del">-	__acquires(rq-&gt;lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct rq *rq;</span>
<span class="p_del">-</span>
<span class="p_del">-	lockdep_assert_held(&amp;p-&gt;pi_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		rq = task_rq(p);</span>
<span class="p_del">-		raw_spin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-		if (likely(rq == task_rq(p) &amp;&amp; !task_on_rq_migrating(p))) {</span>
<span class="p_del">-			lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-			return rq;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		raw_spin_unlock(&amp;rq-&gt;lock);</span>
<span class="p_del">-</span>
<span class="p_del">-		while (unlikely(task_on_rq_migrating(p)))</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_add">+struct rq_flags {</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct pin_cookie cookie;</span>
<span class="p_add">+};</span>
 
<span class="p_del">-/*</span>
<span class="p_del">- * task_rq_lock - lock p-&gt;pi_lock and lock the rq @p resides on.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)</span>
<span class="p_add">+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
<span class="p_add">+	__acquires(rq-&gt;lock);</span>
<span class="p_add">+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
 	__acquires(p-&gt;pi_lock)
<span class="p_del">-	__acquires(rq-&gt;lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct rq *rq;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_del">-		rq = task_rq(p);</span>
<span class="p_del">-		raw_spin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 *	move_queued_task()		task_rq_lock()</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 *	ACQUIRE (rq-&gt;lock)</span>
<span class="p_del">-		 *	[S] -&gt;on_rq = MIGRATING		[L] rq = task_rq()</span>
<span class="p_del">-		 *	WMB (__set_task_cpu())		ACQUIRE (rq-&gt;lock);</span>
<span class="p_del">-		 *	[S] -&gt;cpu = new_cpu		[L] task_rq()</span>
<span class="p_del">-		 *					[L] -&gt;on_rq</span>
<span class="p_del">-		 *	RELEASE (rq-&gt;lock)</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * If we observe the old cpu in task_rq_lock, the acquire of</span>
<span class="p_del">-		 * the old rq-&gt;lock will fully serialize against the stores.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * If we observe the new cpu in task_rq_lock, the acquire will</span>
<span class="p_del">-		 * pair with the WMB to ensure we must then also see migrating.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (likely(rq == task_rq(p) &amp;&amp; !task_on_rq_migrating(p))) {</span>
<span class="p_del">-			lockdep_pin_lock(&amp;rq-&gt;lock);</span>
<span class="p_del">-			return rq;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		raw_spin_unlock(&amp;rq-&gt;lock);</span>
<span class="p_del">-		raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		while (unlikely(task_on_rq_migrating(p)))</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_add">+	__acquires(rq-&gt;lock);</span>
 
<span class="p_del">-static inline void __task_rq_unlock(struct rq *rq)</span>
<span class="p_add">+static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)</span>
 	__releases(rq-&gt;lock)
 {
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, rf-&gt;cookie);</span>
 	raw_spin_unlock(&amp;rq-&gt;lock);
 }
 
 static inline void
<span class="p_del">-task_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)</span>
<span class="p_add">+task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)</span>
 	__releases(rq-&gt;lock)
 	__releases(p-&gt;pi_lock)
 {
<span class="p_del">-	lockdep_unpin_lock(&amp;rq-&gt;lock);</span>
<span class="p_add">+	lockdep_unpin_lock(&amp;rq-&gt;lock, rf-&gt;cookie);</span>
 	raw_spin_unlock(&amp;rq-&gt;lock);
<span class="p_del">-	raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
 }
 
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -1743,6 +1699,10 @@</span> <span class="p_context"> enum rq_nohz_flag_bits {</span>
 };
 
 #define nohz_flags(cpu)	(&amp;cpu_rq(cpu)-&gt;nohz_flags)
<span class="p_add">+</span>
<span class="p_add">+extern void nohz_balance_exit_idle(unsigned int cpu);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void nohz_balance_exit_idle(unsigned int cpu) { }</span>
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
<span class="p_header">diff --git a/kernel/sched/stop_task.c b/kernel/sched/stop_task.c</span>
<span class="p_header">index cbc67da10954..604297a08b3a 100644</span>
<span class="p_header">--- a/kernel/sched/stop_task.c</span>
<span class="p_header">+++ b/kernel/sched/stop_task.c</span>
<span class="p_chunk">@@ -24,7 +24,7 @@</span> <span class="p_context"> check_preempt_curr_stop(struct rq *rq, struct task_struct *p, int flags)</span>
 }
 
 static struct task_struct *
<span class="p_del">-pick_next_task_stop(struct rq *rq, struct task_struct *prev)</span>
<span class="p_add">+pick_next_task_stop(struct rq *rq, struct task_struct *prev, struct pin_cookie cookie)</span>
 {
 	struct task_struct *stop = rq-&gt;stop;
 
<span class="p_header">diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c</span>
<span class="p_header">index 58e3310c9b21..31872bc53bc4 100644</span>
<span class="p_header">--- a/kernel/time/tick-sched.c</span>
<span class="p_header">+++ b/kernel/time/tick-sched.c</span>
<span class="p_chunk">@@ -776,6 +776,7 @@</span> <span class="p_context"> static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,</span>
 	if (!ts-&gt;tick_stopped) {
 		nohz_balance_enter_idle(cpu);
 		calc_load_enter_idle();
<span class="p_add">+		cpu_load_update_nohz_start();</span>
 
 		ts-&gt;last_tick = hrtimer_get_expires(&amp;ts-&gt;sched_timer);
 		ts-&gt;tick_stopped = 1;
<span class="p_chunk">@@ -802,11 +803,11 @@</span> <span class="p_context"> static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,</span>
 	return tick;
 }
 
<span class="p_del">-static void tick_nohz_restart_sched_tick(struct tick_sched *ts, ktime_t now, int active)</span>
<span class="p_add">+static void tick_nohz_restart_sched_tick(struct tick_sched *ts, ktime_t now)</span>
 {
 	/* Update jiffies first */
 	tick_do_update_jiffies64(now);
<span class="p_del">-	update_cpu_load_nohz(active);</span>
<span class="p_add">+	cpu_load_update_nohz_stop();</span>
 
 	calc_load_exit_idle();
 	touch_softlockup_watchdog_sched();
<span class="p_chunk">@@ -833,7 +834,7 @@</span> <span class="p_context"> static void tick_nohz_full_update_tick(struct tick_sched *ts)</span>
 	if (can_stop_full_tick(ts))
 		tick_nohz_stop_sched_tick(ts, ktime_get(), cpu);
 	else if (ts-&gt;tick_stopped)
<span class="p_del">-		tick_nohz_restart_sched_tick(ts, ktime_get(), 1);</span>
<span class="p_add">+		tick_nohz_restart_sched_tick(ts, ktime_get());</span>
 #endif
 }
 
<span class="p_chunk">@@ -1024,7 +1025,7 @@</span> <span class="p_context"> void tick_nohz_idle_exit(void)</span>
 		tick_nohz_stop_idle(ts, now);
 
 	if (ts-&gt;tick_stopped) {
<span class="p_del">-		tick_nohz_restart_sched_tick(ts, now, 0);</span>
<span class="p_add">+		tick_nohz_restart_sched_tick(ts, now);</span>
 		tick_nohz_account_idle_ticks(ts);
 	}
 
<span class="p_header">diff --git a/mm/mmu_context.c b/mm/mmu_context.c</span>
<span class="p_header">index f802c2d216a7..6f4d27c5bb32 100644</span>
<span class="p_header">--- a/mm/mmu_context.c</span>
<span class="p_header">+++ b/mm/mmu_context.c</span>
<span class="p_chunk">@@ -4,9 +4,9 @@</span> <span class="p_context"></span>
  */
 
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;linux/mmu_context.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_del">-#include &lt;linux/sched.h&gt;</span>
 
 #include &lt;asm/mmu_context.h&gt;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



