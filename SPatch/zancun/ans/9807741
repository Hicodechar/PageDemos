
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm: larger stack guard gap, between vmas - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm: larger stack guard gap, between vmas</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 24, 2017, 9:11 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.LSU.2.11.1706240115300.1232@eggly.anvils&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9807741/mbox/"
   >mbox</a>
|
   <a href="/patch/9807741/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9807741/">/patch/9807741/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	CB96760382 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 24 Jun 2017 09:12:10 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BA35B2873B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 24 Jun 2017 09:12:10 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AE17428793; Sat, 24 Jun 2017 09:12:10 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3AF7C2873B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 24 Jun 2017 09:12:08 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751332AbdFXJLu (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 24 Jun 2017 05:11:50 -0400
Received: from mail-pg0-f51.google.com ([74.125.83.51]:33751 &quot;EHLO
	mail-pg0-f51.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751266AbdFXJLr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 24 Jun 2017 05:11:47 -0400
Received: by mail-pg0-f51.google.com with SMTP id f127so30107336pgc.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sat, 24 Jun 2017 02:11:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=google.com; s=20161025;
	h=date:from:to:cc:subject:in-reply-to:message-id:references
	:user-agent:mime-version;
	bh=4ID6zze2E8E86ldtwCNiA9eBZ0ru2cBRuHk7oPAPnEE=;
	b=CB6BqegU/48EuIpjEYqN8ywlkf3UAUV10Q5l6fFfaKdhHhYDoAV94vEd3sUiHhaPSI
	D6+VnAqlocKfK9jNir9gYRhQ7PU3vWGAfZn+mOVkdOfyZsq4gq9qUipK/2buMyg/90qK
	ouHFMpGL88wG5p8zHcpT/izvbhuj0CoTZc4eWRAH9QvC17SVRP4i7bRKJsc6jrQaMjz0
	aD+Se/KEgpD38X0tF0cBac4yoh83QbxfMiKlupp/GOrTL8eEz0uO9D7pCTjS1zOKlMy5
	KjAV/rQOCRtU/vn+PyFoeLNktRqb8/OPD8DvUkkRH2HaizIs0mJFW/88m0u2kigWX7oP
	9YAg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:date:from:to:cc:subject:in-reply-to:message-id
	:references:user-agent:mime-version;
	bh=4ID6zze2E8E86ldtwCNiA9eBZ0ru2cBRuHk7oPAPnEE=;
	b=KXIgvgBzpdCNzf8F1S5EkyM4aRj3l2VkDzauIoj/yvP+FdgCUNyCQhsP7BB8GoO5VD
	SNu+/NUrAHS+t68wANihZwkRlCUSNCRCmeVRSy4OSuZfux7OVo5tt8RvAhzUjzNPXXpz
	ffHciHZpZZP9ivLlCqrhTsT7jsNJLi9CKTgge7SsGF4W3FBft0QizNKa9BNlLZ1/pzcv
	kRHOMcBuvjiPpjuL853mH3GzOMWlfAN1sHR3t4xPHJVj23Yt+LlGXXcxcxVoskSBbK/E
	YWXnZY9VH6ae9y42MuGnH1xUP8fpQOgYMA8rKk2VGj8EWRn6LLaSH5wFBWKlWm7LFkr1
	wWDw==
X-Gm-Message-State: AKS2vOyHYsU0v9YrdqwM7KCLWl8DQdnDnsCFcqlGFmftoPLMC5IZXHid
	a2gJRV3uUC064Tf2
X-Received: by 10.84.232.70 with SMTP id f6mr13290892pln.169.1498295505593; 
	Sat, 24 Jun 2017 02:11:45 -0700 (PDT)
Received: from eggly.attlocal.net
	(172-10-233-147.lightspeed.sntcca.sbcglobal.net. [172.10.233.147])
	by smtp.gmail.com with ESMTPSA id
	d2sm11975224pfb.49.2017.06.24.02.11.43
	(version=TLS1 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Sat, 24 Jun 2017 02:11:44 -0700 (PDT)
Date: Sat, 24 Jun 2017 02:11:36 -0700 (PDT)
From: Hugh Dickins &lt;hughd@google.com&gt;
X-X-Sender: hugh@eggly.anvils
To: Ben Hutchings &lt;ben@decadent.org.uk&gt;
cc: Hugh Dickins &lt;hughd@google.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	&quot;Jason A. Donenfeld&quot; &lt;Jason@zx2c4.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Tony Luck &lt;tony.luck@intel.com&gt;,
	&quot;James E.J. Bottomley&quot; &lt;jejb@parisc-linux.org&gt;,
	Helge Diller &lt;deller@gmx.de&gt;, James Hogan &lt;james.hogan@imgtec.com&gt;,
	Laura Abbott &lt;labbott@redhat.com&gt;, Willy Tarreau &lt;w@1wt.eu&gt;,
	Greg KH &lt;greg@kroah.com&gt;, stable &lt;stable@vger.kernel.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: Re: [PATCH] mm: larger stack guard gap, between vmas
In-Reply-To: &lt;alpine.LSU.2.11.1706222119230.23215@eggly.anvils&gt;
Message-ID: &lt;alpine.LSU.2.11.1706240115300.1232@eggly.anvils&gt;
References: &lt;alpine.LSU.2.11.1706190355140.2626@eggly.anvils&gt;
	&lt;20170622123045.GA2694@decadent.org.uk&gt;
	&lt;alpine.LSU.2.11.1706222119230.23215@eggly.anvils&gt;
User-Agent: Alpine 2.11 (LSU 23 2013-08-11)
MIME-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a> - June 24, 2017, 9:11 a.m.</div>
<pre class="content">
On Thu, 22 Jun 2017, Hugh Dickins wrote:
<span class="quote">&gt; On Thu, 22 Jun 2017, Ben Hutchings wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; &gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; &gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; &gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; &gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Both your speed and your stamina are much better than mine; and your</span>
<span class="quote">&gt; patch belies your Sturgeon&#39;s law signature.  I haven&#39;t got beyond the</span>
<span class="quote">&gt; architectures yet in my parallel attempt, and you do appear to be</span>
<span class="quote">&gt; doing everything right (but a local variable often welcome, yes).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m giving up for the night, will contine tomorrow.</span>
<span class="quote">&gt; The only discrepancy I notice so far is that I have</span>
<span class="quote">&gt; arch/alpha/kernel/osf_sys.c</span>
<span class="quote">&gt; arch/ia64/mm/hugetlbpage.c</span>
<span class="quote">&gt; arch/sparc/kernel/sys_sparc_32.c</span>
<span class="quote">&gt; in my list of changed files, but they&#39;re not in yours.</span>

And here&#39;s my attempt at a backport to 3.2.89, at last.
I know it builds and boots and runs on x86 64 and 32,
but that&#39;s about all that I&#39;ve tried.

If you diff against yours (I preferred not to send that diff,
because of the couple of rejects in yours against 3.2.89),
you&#39;ll find most of the difference is just noise from where
I used a variable, but you had not yet done so in yours.

But there are those three missing files, and there are a few
places where I have a little &quot;if (prev) {&quot; block at the head of
the loop after find_vma_prev(): I think those loops start off
wrongly without that.

I notice now that you don&#39;t use find_vma_prev() in your generic
(mm/mmap.c) arch_get_unmapped_area() and _topdown(): and now
that I reflect on it, I think you&#39;re perfectly correct to keep
those simple (especially given the inefficient implementation
of find_vma_prev() in 3.2 - only later was it changed to make
use of vm_prev), since both ia64 and parisc provide their own
arch_get_unmapped_area() in this release, and neither use the
_topdown().

Whereas I spent much too much time on adapting those generics
to vm_end_gap(), and pretty much gave up on the _topdown() -
it grieved me to end up calling find_vma_prev() each time
around the loop (where before it called find_vma() each time
around the loop), there is definitely better use to be made
of vm_prev there, but too hard to get right as I grew tired.

So please at least take a look through the diff from yours, I
think you&#39;ll find a few things to bring in, but a lot to ignore.

Hugh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 24, 2017, 6:29 p.m.</div>
<pre class="content">
On Sat, 2017-06-24 at 02:11 -0700, Hugh Dickins wrote:
<span class="quote">&gt; On Thu, 22 Jun 2017, Hugh Dickins wrote:</span>
<span class="quote">&gt; &gt; On Thu, 22 Jun 2017, Ben Hutchings wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Here&#39;s my attempt at a backport to 3.2.  This is only tested on</span>
<span class="quote">&gt; &gt; &gt; x86_64 and I think I should introduce local variables for</span>
<span class="quote">&gt; &gt; &gt; vma_start_gap() in a few places.  I had to cherry-pick commit</span>
<span class="quote">&gt; &gt; &gt; 09884964335e &quot;mm: do not grow the stack vma just because of an overrun</span>
<span class="quote">&gt; &gt; &gt; on preceding vma&quot; before this one (which was a clean cherry-pick).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Both your speed and your stamina are much better than mine; and your</span>
<span class="quote">&gt; &gt; patch belies your Sturgeon&#39;s law signature.  I haven&#39;t got beyond the</span>
<span class="quote">&gt; &gt; architectures yet in my parallel attempt, and you do appear to be</span>
<span class="quote">&gt; &gt; doing everything right (but a local variable often welcome, yes).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;m giving up for the night, will contine tomorrow.</span>
<span class="quote">&gt; &gt; The only discrepancy I notice so far is that I have</span>
<span class="quote">&gt; &gt; arch/alpha/kernel/osf_sys.c</span>
<span class="quote">&gt; &gt; arch/ia64/mm/hugetlbpage.c</span>
<span class="quote">&gt; &gt; arch/sparc/kernel/sys_sparc_32.c</span>
<span class="quote">&gt; &gt; in my list of changed files, but they&#39;re not in yours.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And here&#39;s my attempt at a backport to 3.2.89, at last.</span>
<span class="quote">&gt; I know it builds and boots and runs on x86 64 and 32,</span>
<span class="quote">&gt; but that&#39;s about all that I&#39;ve tried.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you diff against yours (I preferred not to send that diff,</span>
<span class="quote">&gt; because of the couple of rejects in yours against 3.2.89),</span>
<span class="quote">&gt; you&#39;ll find most of the difference is just noise from where</span>
<span class="quote">&gt; I used a variable, but you had not yet done so in yours.</span>

Thanks, this is much nicer.
<span class="quote">
&gt; But there are those three missing files, and there are a few</span>
<span class="quote">&gt; places where I have a little &quot;if (prev) {&quot; block at the head of</span>
<span class="quote">&gt; the loop after find_vma_prev(): I think those loops start off</span>
<span class="quote">&gt; wrongly without that.</span>

I also failed to update prev.

[...]
<span class="quote">&gt; So please at least take a look through the diff from yours, I</span>
<span class="quote">&gt; think you&#39;ll find a few things to bring in, but a lot to ignore.</span>

I think I&#39;ll take most of yours, thanks.

Ben.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff -purN 302s/arch/alpha/kernel/osf_sys.c 302h/arch/alpha/kernel/osf_sys.c</span>
<span class="p_header">--- 302s/arch/alpha/kernel/osf_sys.c	2011-10-24 00:10:05.000000000 -0700</span>
<span class="p_header">+++ 302h/arch/alpha/kernel/osf_sys.c	2017-06-22 18:16:22.425283525 -0700</span>
<span class="p_chunk">@@ -1147,7 +1147,7 @@</span> <span class="p_context"> arch_get_unmapped_area_1(unsigned long a</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (limit - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_vma(vma))</span>
 			return addr;
 		addr = vma-&gt;vm_end;
 		vma = vma-&gt;vm_next;
<span class="p_header">diff -purN 302s/arch/arm/mm/mmap.c 302h/arch/arm/mm/mmap.c</span>
<span class="p_header">--- 302s/arch/arm/mm/mmap.c	2012-01-04 15:55:44.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/arm/mm/mmap.c	2017-06-23 21:30:38.061880299 -0700</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_align = 0;
 	int aliasing = cache_is_vipt_aliasing();
 
<span class="p_chunk">@@ -62,7 +62,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -96,15 +96,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 		if (do_align)
 			addr = COLOUR_ALIGN(addr, pgoff);
<span class="p_header">diff -purN 302s/arch/frv/mm/elf-fdpic.c 302h/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">--- 302s/arch/frv/mm/elf-fdpic.c	2007-07-08 16:32:17.000000000 -0700</span>
<span class="p_header">+++ 302h/arch/frv/mm/elf-fdpic.c	2017-06-22 18:27:22.823308633 -0700</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 			for (; vma; vma = vma-&gt;vm_next) {
 				if (addr &gt; limit)
 					break;
<span class="p_del">-				if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+				if (addr + len &lt;= vm_start_gap(vma))</span>
 					goto success;
 				addr = vma-&gt;vm_end;
 			}
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		for (; vma; vma = vma-&gt;vm_next) {
 			if (addr &gt; limit)
 				break;
<span class="p_del">-			if (addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (addr + len &lt;= vm_start_gap(vma))</span>
 				goto success;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_header">diff -purN 302s/arch/ia64/kernel/sys_ia64.c 302h/arch/ia64/kernel/sys_ia64.c</span>
<span class="p_header">--- 302s/arch/ia64/kernel/sys_ia64.c	2010-02-24 10:52:17.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/ia64/kernel/sys_ia64.c	2017-06-23 21:31:37.581321626 -0700</span>
<span class="p_chunk">@@ -27,7 +27,8 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 	long map_shared = (flags &amp; MAP_SHARED);
 	unsigned long start_addr, align_mask = PAGE_SIZE - 1;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	if (len &gt; RGN_MAP_LIMIT)
 		return -ENOMEM;
<span class="p_chunk">@@ -58,7 +59,17 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
   full_search:
 	start_addr = addr = (addr + align_mask) &amp; ~align_mask;
 
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = (prev_end + align_mask) &amp; ~align_mask;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr || RGN_MAP_LIMIT - len &lt; REGION_OFFSET(addr)) {
 			if (start_addr != TASK_UNMAPPED_BASE) {
<span class="p_chunk">@@ -68,12 +79,11 @@</span> <span class="p_context"> arch_get_unmapped_area (struct file *fil</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma)) {</span>
 			/* Remember the address where we stopped this search:  */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		addr = (vma-&gt;vm_end + align_mask) &amp; ~align_mask;</span>
 	}
 }
 
<span class="p_header">diff -purN 302s/arch/ia64/mm/hugetlbpage.c 302h/arch/ia64/mm/hugetlbpage.c</span>
<span class="p_header">--- 302s/arch/ia64/mm/hugetlbpage.c	2011-03-14 18:20:32.000000000 -0700</span>
<span class="p_header">+++ 302h/arch/ia64/mm/hugetlbpage.c	2017-06-22 20:50:22.569517894 -0700</span>
<span class="p_chunk">@@ -171,9 +171,9 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (REGION_OFFSET(addr) + len &gt; RGN_MAP_LIMIT)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || (addr + len) &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || (addr + len) &lt;= vm_start_gap(vmm))</span>
 			return addr;
<span class="p_del">-		addr = ALIGN(vmm-&gt;vm_end, HPAGE_SIZE);</span>
<span class="p_add">+		addr = ALIGN(vm_end_gap(vmm), HPAGE_SIZE);</span>
 	}
 }
 
<span class="p_header">diff -purN 302s/arch/mips/mm/mmap.c 302h/arch/mips/mm/mmap.c</span>
<span class="p_header">--- 302s/arch/mips/mm/mmap.c	2011-10-24 00:10:05.000000000 -0700</span>
<span class="p_header">+++ 302h/arch/mips/mm/mmap.c	2017-06-22 20:34:16.758377572 -0700</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_color_align;
 
 	if (unlikely(len &gt; TASK_SIZE))
<span class="p_chunk">@@ -103,7 +104,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -118,7 +119,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 			if (TASK_SIZE - len &lt; addr)
 				return -ENOMEM;
<span class="p_del">-			if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+			if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 				return addr;
 			addr = vma-&gt;vm_end;
 			if (do_color_align)
<span class="p_chunk">@@ -145,7 +146,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 		/* make sure it can fit in the remaining address space */
 		if (likely(addr &gt; len)) {
 			vma = find_vma(mm, addr - len);
<span class="p_del">-			if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+			if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr - len;
 			}
<span class="p_chunk">@@ -165,20 +166,22 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 			 * return with success:
 			 */
 			vma = find_vma(mm, addr);
<span class="p_del">-			if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+			if (vma)</span>
<span class="p_add">+				vm_start = vm_start_gap(vma);</span>
<span class="p_add">+			if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 				/* cache the address as a hint for next time */
 				return mm-&gt;free_area_cache = addr;
 			}
 
 			/* remember the largest hole we saw so far */
<span class="p_del">-			if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-				mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+			if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+				mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 			/* try just below the current vma-&gt;vm_start */
<span class="p_del">-			addr = vma-&gt;vm_start - len;</span>
<span class="p_add">+			addr = vm_start - len;</span>
 			if (do_color_align)
 				addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-		} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 		/*
<span class="p_header">diff -purN 302s/arch/parisc/kernel/sys_parisc.c 302h/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">--- 302s/arch/parisc/kernel/sys_parisc.c	2017-06-20 16:22:15.561319552 -0700</span>
<span class="p_header">+++ 302h/arch/parisc/kernel/sys_parisc.c	2017-06-23 21:35:09.003338157 -0700</span>
<span class="p_chunk">@@ -35,17 +35,27 @@</span> <span class="p_context"></span>
 
 static unsigned long get_unshared_area(unsigned long addr, unsigned long len)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 
 	addr = PAGE_ALIGN(addr);
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
 	}
 }
 
<span class="p_chunk">@@ -70,22 +80,32 @@</span> <span class="p_context"> static int get_offset(struct address_spa</span>
 static unsigned long get_shared_area(struct address_space *mapping,
 		unsigned long addr, unsigned long len, unsigned long pgoff)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long prev_end;</span>
 	int offset = mapping ? get_offset(mapping) : 0;
 
 	offset = (offset + (pgoff &lt;&lt; PAGE_SHIFT)) &amp; 0x3FF000;
 
 	addr = DCACHE_ALIGN(addr - offset) + offset;
 
<span class="p_del">-	for (vma = find_vma(current-&gt;mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(current-&gt;mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+							vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = DCACHE_ALIGN(prev_end - offset) + offset;</span>
<span class="p_add">+				if (addr &lt; prev_end)	/* handle wraparound */</span>
<span class="p_add">+					return -ENOMEM;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start_gap(vma))</span>
 			return addr;
<span class="p_del">-		addr = DCACHE_ALIGN(vma-&gt;vm_end - offset) + offset;</span>
<span class="p_del">-		if (addr &lt; vma-&gt;vm_end) /* handle wraparound */</span>
<span class="p_del">-			return -ENOMEM;</span>
 	}
 }
 
<span class="p_header">diff -purN 302s/arch/powerpc/mm/slice.c 302h/arch/powerpc/mm/slice.c</span>
<span class="p_header">--- 302s/arch/powerpc/mm/slice.c	2012-01-04 15:55:44.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/powerpc/mm/slice.c	2017-06-23 21:36:04.038822093 -0700</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_chunk">@@ -227,7 +227,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_bot</span>
 					      int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr, addr;</span>
<span class="p_add">+	unsigned long start_addr, addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -256,7 +256,9 @@</span> <span class="p_context"> full_search:</span>
 				addr = _ALIGN_UP(addr + 1,  1ul &lt;&lt; SLICE_HIGH_SHIFT);
 			continue;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
<span class="p_chunk">@@ -264,8 +266,8 @@</span> <span class="p_context"> full_search:</span>
 				mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = vma-&gt;vm_end;
 	}
 
<span class="p_chunk">@@ -284,7 +286,7 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 					     int psize, int use_cache)
 {
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long addr;</span>
<span class="p_add">+	unsigned long addr, vm_start;</span>
 	struct slice_mask mask;
 	int pshift = max_t(int, mmu_psize_defs[psize].shift, PAGE_SHIFT);
 
<span class="p_chunk">@@ -336,7 +338,9 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || (addr + len) &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || (addr + len) &lt;= vm_start) {</span>
 			/* remember the address as a hint for next time */
 			if (use_cache)
 				mm-&gt;free_area_cache = addr;
<span class="p_chunk">@@ -344,11 +348,11 @@</span> <span class="p_context"> static unsigned long slice_find_area_top</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (use_cache &amp;&amp; (addr + mm-&gt;cached_hole_size) &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start;</span>
<span class="p_add">+		addr = vm_start;</span>
 	}
 
 	/*
<span class="p_header">diff -purN 302s/arch/sh/mm/mmap.c 302h/arch/sh/mm/mmap.c</span>
<span class="p_header">--- 302s/arch/sh/mm/mmap.c	2010-02-24 10:52:17.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/sh/mm/mmap.c	2017-06-23 21:36:50.758384088 -0700</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -106,15 +106,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_colour_align)
<span class="p_chunk">@@ -130,6 +132,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 	int do_colour_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -158,7 +161,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -179,7 +182,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -199,20 +202,22 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_add">+		addr = vm_start-len;</span>
 		if (do_colour_align)
 			addr = COLOUR_ALIGN_DOWN(addr, pgoff);
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_header">diff -purN 302s/arch/sparc/kernel/sys_sparc_32.c 302h/arch/sparc/kernel/sys_sparc_32.c</span>
<span class="p_header">--- 302s/arch/sparc/kernel/sys_sparc_32.c	2011-01-04 16:50:19.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/sparc/kernel/sys_sparc_32.c	2017-06-22 19:35:28.166491263 -0700</span>
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		}
 		if (TASK_SIZE - PAGE_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">diff -purN 302s/arch/sparc/kernel/sys_sparc_64.c 302h/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">--- 302s/arch/sparc/kernel/sys_sparc_64.c	2017-06-20 16:22:15.661318622 -0700</span>
<span class="p_header">+++ 302h/arch/sparc/kernel/sys_sparc_64.c	2017-06-23 21:38:31.169442960 -0700</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	int do_color_align;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,15 +181,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		if (do_color_align)
<span class="p_chunk">@@ -237,7 +239,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff -purN 302s/arch/sparc/mm/hugetlbpage.c 302h/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">--- 302s/arch/sparc/mm/hugetlbpage.c	2012-01-04 15:55:44.000000000 -0800</span>
<span class="p_header">+++ 302h/arch/sparc/mm/hugetlbpage.c	2017-06-23 21:39:56.800640620 -0700</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct * vma;
 	unsigned long task_size = TASK_SIZE;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (test_thread_flag(TIF_32BIT))
 		task_size = STACK_TOP32;
<span class="p_chunk">@@ -67,15 +67,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (likely(!vma || addr + len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = ALIGN(vma-&gt;vm_end, HPAGE_SIZE);
 	}
<span class="p_chunk">@@ -90,6 +92,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* This should only ever run for 32-bit processes.  */
 	BUG_ON(!test_thread_flag(TIF_32BIT));
<span class="p_chunk">@@ -106,7 +109,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 	/* make sure it can fit in the remaining address space */
 	if (likely(addr &gt; len)) {
 		vma = find_vma(mm, addr-len);
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (!vma || addr &lt;= vm_start_gap(vma)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 		}
<span class="p_chunk">@@ -124,18 +127,20 @@</span> <span class="p_context"> hugetlb_get_unmapped_area_topdown(struct</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (likely(!vma || addr+len &lt;= vma-&gt;vm_start)) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (likely(!vma || addr + len &lt;= vm_start)) {</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 		}
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start-len) &amp; HPAGE_MASK;</span>
<span class="p_del">-	} while (likely(len &lt; vma-&gt;vm_start));</span>
<span class="p_add">+		addr = (vm_start - len) &amp; HPAGE_MASK;</span>
<span class="p_add">+	} while (likely(len &lt; vm_start));</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -182,7 +187,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff -purN 302s/arch/tile/mm/hugetlbpage.c 302h/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">--- 302s/arch/tile/mm/hugetlbpage.c	2011-05-18 21:06:34.000000000 -0700</span>
<span class="p_header">+++ 302h/arch/tile/mm/hugetlbpage.c	2017-06-22 20:35:23.725762639 -0700</span>
<span class="p_chunk">@@ -159,7 +159,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 		start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -185,12 +185,14 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -204,6 +206,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -234,9 +237,10 @@</span> <span class="p_context"> try_again:</span>
 
 		/*
 		 * new region fits between prev_vma-&gt;vm_end and
<span class="p_del">-		 * vma-&gt;vm_start, use it:</span>
<span class="p_add">+		 * vm_start, use it:</span>
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 			    (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 			mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -251,13 +255,13 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
 
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -312,7 +316,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff -purN 302s/arch/x86/kernel/sys_x86_64.c 302h/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">--- 302s/arch/x86/kernel/sys_x86_64.c	2017-06-20 16:22:15.749317803 -0700</span>
<span class="p_header">+++ 302h/arch/x86/kernel/sys_x86_64.c	2017-06-22 20:36:04.897384626 -0700</span>
<span class="p_chunk">@@ -126,7 +126,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 	unsigned long begin, end;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (((flags &amp; MAP_32BIT) || test_thread_flag(TIF_IA32))
<span class="p_chunk">@@ -172,15 +172,17 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		addr = vma-&gt;vm_end;
 		addr = align_addr(addr, filp, 0);
<span class="p_chunk">@@ -196,6 +198,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	struct vm_area_struct *vma;
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start;</span>
 
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
<span class="p_chunk">@@ -213,7 +216,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -232,7 +235,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 						    ALIGN_TOPDOWN);
 
 		vma = find_vma(mm, tmp_addr);
<span class="p_del">-		if (!vma || tmp_addr + len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (!vma || tmp_addr + len &lt;= vm_start_gap(vma))</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = tmp_addr;
 	}
<span class="p_chunk">@@ -251,17 +254,19 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * return with success:
 		 */
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start)</span>
 			/* remember the address as a hint for next time */
 			return mm-&gt;free_area_cache = addr;
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-			mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (len &lt; vma-&gt;vm_start);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (len &lt; vm_start);</span>
 
 bottomup:
 	/*
<span class="p_header">diff -purN 302s/arch/x86/mm/hugetlbpage.c 302h/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">--- 302s/arch/x86/mm/hugetlbpage.c	2017-06-20 16:22:15.773317580 -0700</span>
<span class="p_header">+++ 302h/arch/x86/mm/hugetlbpage.c	2017-06-23 21:40:52.016123391 -0700</span>
<span class="p_chunk">@@ -277,7 +277,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct hstate *h = hstate_file(file);
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	unsigned long start_addr, vm_start;</span>
 
 	if (len &gt; mm-&gt;cached_hole_size) {
 	        start_addr = mm-&gt;free_area_cache;
<span class="p_chunk">@@ -303,12 +303,14 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		if (vma)</span>
<span class="p_add">+			vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (!vma || addr + len &lt;= vm_start) {</span>
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 		addr = ALIGN(vma-&gt;vm_end, huge_page_size(h));
 	}
 }
<span class="p_chunk">@@ -322,6 +324,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmappe</span>
 	struct vm_area_struct *vma, *prev_vma;
 	unsigned long base = mm-&gt;mmap_base, addr = addr0;
 	unsigned long largest_hole = mm-&gt;cached_hole_size;
<span class="p_add">+	unsigned long vm_start;</span>
 	int first_time = 1;
 
 	/* don&#39;t allow allocations above current base */
<span class="p_chunk">@@ -351,7 +354,8 @@</span> <span class="p_context"> try_again:</span>
 		 * new region fits between prev_vma-&gt;vm_end and
 		 * vma-&gt;vm_start, use it:
 		 */
<span class="p_del">-		if (addr + len &lt;= vma-&gt;vm_start &amp;&amp;</span>
<span class="p_add">+		vm_start = vm_start_gap(vma);</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp;</span>
 		            (!prev_vma || (addr &gt;= prev_vma-&gt;vm_end))) {
 			/* remember the address as a hint for next time */
 		        mm-&gt;cached_hole_size = largest_hole;
<span class="p_chunk">@@ -365,12 +369,12 @@</span> <span class="p_context"> try_again:</span>
 		}
 
 		/* remember the largest hole we saw so far */
<span class="p_del">-		if (addr + largest_hole &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        largest_hole = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + largest_hole &lt; vm_start)</span>
<span class="p_add">+			largest_hole = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = (vma-&gt;vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_del">-	} while (len &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+		addr = (vm_start - len) &amp; huge_page_mask(h);</span>
<span class="p_add">+	} while (len &lt;= vm_start);</span>
 
 fail:
 	/*
<span class="p_chunk">@@ -426,7 +430,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff -purN 302s/Documentation/kernel-parameters.txt 302h/Documentation/kernel-parameters.txt</span>
<span class="p_header">--- 302s/Documentation/kernel-parameters.txt	2017-06-20 16:22:15.389321153 -0700</span>
<span class="p_header">+++ 302h/Documentation/kernel-parameters.txt	2017-06-21 20:07:38.174763661 -0700</span>
<span class="p_chunk">@@ -2457,6 +2457,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">diff -purN 302s/fs/hugetlbfs/inode.c 302h/fs/hugetlbfs/inode.c</span>
<span class="p_header">--- 302s/fs/hugetlbfs/inode.c	2017-06-20 16:22:17.277303587 -0700</span>
<span class="p_header">+++ 302h/fs/hugetlbfs/inode.c	2017-06-21 20:07:38.174763661 -0700</span>
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff -purN 302s/fs/proc/task_mmu.c 302h/fs/proc/task_mmu.c</span>
<span class="p_header">--- 302s/fs/proc/task_mmu.c	2017-06-20 16:22:17.401302434 -0700</span>
<span class="p_header">+++ 302h/fs/proc/task_mmu.c	2017-06-21 20:07:38.174763661 -0700</span>
<span class="p_chunk">@@ -230,11 +230,7 @@</span> <span class="p_context"> static void show_map_vma(struct seq_file</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n&quot;,
 			start,
<span class="p_header">diff -purN 302s/include/linux/mm.h 302h/include/linux/mm.h</span>
<span class="p_header">--- 302s/include/linux/mm.h	2017-06-20 16:22:17.509301429 -0700</span>
<span class="p_header">+++ 302h/include/linux/mm.h	2017-06-22 17:47:33.388923303 -0700</span>
<span class="p_chunk">@@ -1015,34 +1015,6 @@</span> <span class="p_context"> int set_page_dirty(struct page *page);</span>
 int set_page_dirty_lock(struct page *page);
 int clear_page_dirty_for_io(struct page *page);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
 		unsigned long old_addr, struct vm_area_struct *new_vma,
 		unsigned long new_addr, unsigned long len);
<span class="p_chunk">@@ -1462,6 +1434,7 @@</span> <span class="p_context"> unsigned long ra_submit(struct file_ra_s</span>
 			struct address_space *mapping,
 			struct file *filp);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1490,6 +1463,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * fi</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">diff -purN 302s/mm/memory.c 302h/mm/memory.c</span>
<span class="p_header">--- 302s/mm/memory.c	2017-06-20 16:22:17.725299419 -0700</span>
<span class="p_header">+++ 302h/mm/memory.c	2017-06-21 20:07:38.178763623 -0700</span>
<span class="p_chunk">@@ -1605,12 +1605,6 @@</span> <span class="p_context"> no_page_table:</span>
 	return page;
 }
 
<span class="p_del">-static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return stack_guard_page_start(vma, addr) ||</span>
<span class="p_del">-	       stack_guard_page_end(vma, addr+PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * __get_user_pages() - pin user pages in memory
  * @tsk:	task_struct of target task
<span class="p_chunk">@@ -1761,11 +1755,6 @@</span> <span class="p_context"> int __get_user_pages(struct task_struct</span>
 				int ret;
 				unsigned int fault_flags = 0;
 
<span class="p_del">-				/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-				if (foll_flags &amp; FOLL_MLOCK) {</span>
<span class="p_del">-					if (stack_guard_page(vma, start))</span>
<span class="p_del">-						goto next_page;</span>
<span class="p_del">-				}</span>
 				if (foll_flags &amp; FOLL_WRITE)
 					fault_flags |= FAULT_FLAG_WRITE;
 				if (nonblocking)
<span class="p_chunk">@@ -3122,40 +3111,6 @@</span> <span class="p_context"> out_release:</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -3174,10 +3129,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_s</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">diff -purN 302s/mm/mmap.c 302h/mm/mmap.c</span>
<span class="p_header">--- 302s/mm/mmap.c	2017-06-20 16:22:17.733299345 -0700</span>
<span class="p_header">+++ 302h/mm/mmap.c	2017-06-23 21:42:54.430977017 -0700</span>
<span class="p_chunk">@@ -245,6 +245,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -289,7 +290,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -1368,8 +1370,8 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_del">-	unsigned long start_addr;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
<span class="p_add">+	unsigned long start_addr, vm_start, prev_end;</span>
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
 		return -ENOMEM;
<span class="p_chunk">@@ -1379,9 +1381,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 	if (len &gt; mm-&gt;cached_hole_size) {
<span class="p_chunk">@@ -1392,7 +1395,17 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 	}
 
 full_search:
<span class="p_del">-	for (vma = find_vma(mm, addr); ; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+	for (vma = find_vma_prev(mm, addr, &amp;prev); ; prev = vma,</span>
<span class="p_add">+						vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		if (prev) {</span>
<span class="p_add">+			prev_end = vm_end_gap(prev);</span>
<span class="p_add">+			if (addr &lt; prev_end) {</span>
<span class="p_add">+				addr = prev_end;</span>
<span class="p_add">+				/* If vma already violates gap, forget it */</span>
<span class="p_add">+				if (vma &amp;&amp; addr &gt; vma-&gt;vm_start)</span>
<span class="p_add">+					addr = vma-&gt;vm_start;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
 		/* At this point:  (!vma || addr &lt; vma-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr) {
 			/*
<span class="p_chunk">@@ -1407,16 +1420,16 @@</span> <span class="p_context"> full_search:</span>
 			}
 			return -ENOMEM;
 		}
<span class="p_del">-		if (!vma || addr + len &lt;= vma-&gt;vm_start) {</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : TASK_SIZE;</span>
<span class="p_add">+		if (addr + len &lt;= vm_start) {</span>
 			/*
 			 * Remember the place where we stopped the search:
 			 */
 			mm-&gt;free_area_cache = addr + len;
 			return addr;
 		}
<span class="p_del">-		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">-		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_del">-		addr = vma-&gt;vm_end;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 	}
 }
 #endif	
<span class="p_chunk">@@ -1442,9 +1455,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
<span class="p_add">+	unsigned long vm_start, prev_end;</span>
 	unsigned long low_limit = max(PAGE_SIZE, mmap_min_addr);
 
 	/* requested length too big for entire address space */
<span class="p_chunk">@@ -1457,9 +1471,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1474,8 +1489,9 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 	/* make sure it can fit in the remaining address space */
 	if (addr &gt;= low_limit + len) {
<span class="p_del">-		vma = find_vma(mm, addr-len);</span>
<span class="p_del">-		if (!vma || addr &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr-len, &amp;prev);</span>
<span class="p_add">+		if ((!vma || addr &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr-len &gt;= vm_end_gap(prev)))</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr-len);
 	}
<span class="p_chunk">@@ -1491,18 +1507,21 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		 * else if new region fits below vma-&gt;vm_start,
 		 * return with success:
 		 */
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_del">-		if (!vma || addr+len &lt;= vma-&gt;vm_start)</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
<span class="p_add">+		vm_start = vma ? vm_start_gap(vma) : mm-&gt;mmap_base;</span>
<span class="p_add">+		prev_end = prev ? vm_end_gap(prev) : low_limit;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (addr + len &lt;= vm_start &amp;&amp; addr &gt;= prev_end)</span>
 			/* remember the address as a hint for next time */
 			return (mm-&gt;free_area_cache = addr);
 
  		/* remember the largest hole we saw so far */
<span class="p_del">- 		if (addr + mm-&gt;cached_hole_size &lt; vma-&gt;vm_start)</span>
<span class="p_del">- 		        mm-&gt;cached_hole_size = vma-&gt;vm_start - addr;</span>
<span class="p_add">+		if (addr + mm-&gt;cached_hole_size &lt; vm_start)</span>
<span class="p_add">+			mm-&gt;cached_hole_size = vm_start - addr;</span>
 
 		/* try just below the current vma-&gt;vm_start */
<span class="p_del">-		addr = vma-&gt;vm_start-len;</span>
<span class="p_del">-	} while (vma-&gt;vm_start &gt;= low_limit + len);</span>
<span class="p_add">+		addr = vm_start - len;</span>
<span class="p_add">+	} while (vm_start &gt;= low_limit + len);</span>
 
 bottomup:
 	/*
<span class="p_chunk">@@ -1647,21 +1666,19 @@</span> <span class="p_context"> out:</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -1703,32 +1720,40 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_a</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	int error;</span>
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
<span class="p_add">+	int error = 0;</span>
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	/* Guard against wrapping around to address 0. */</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+	if (!address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &lt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
<span class="p_del">-	 * Also guard against wrapping around to address 0.</span>
 	 */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else {</span>
<span class="p_del">-		vma_unlock_anon_vma(vma);</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	error = 0;</span>
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &gt; vma-&gt;vm_end) {
<span class="p_chunk">@@ -1758,27 +1783,36 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We must make sure the anon_vma is allocated</span>
<span class="p_del">-	 * so that the anon_vma locking is not a noop.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	address &amp;= PAGE_MASK;
 	error = security_file_mmap(NULL, 0, 0, 0, address, 1);
 	if (error)
 		return error;
 
<span class="p_del">-	vma_lock_anon_vma(vma);</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
<span class="p_add">+	vma_lock_anon_vma(vma);</span>
 
 	/* Somebody else might have raced and expanded it already */
 	if (address &lt; vma-&gt;vm_start) {
<span class="p_chunk">@@ -1802,6 +1836,22 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_stru</span>
 	return error;
 }
 
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



