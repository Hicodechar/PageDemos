
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,07/11] x86: support dma_map_pfn() - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,07/11] x86: support dma_map_pfn()</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=320">Dan Williams</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 12, 2015, 4:30 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150512043006.11521.34823.stgit@dwillia2-desk3.amr.corp.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6385651/mbox/"
   >mbox</a>
|
   <a href="/patch/6385651/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6385651/">/patch/6385651/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id BE025BEEE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 04:33:07 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 43C6C20121
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 04:33:06 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 82AF9203B5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 May 2015 04:33:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752883AbbELEcz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 May 2015 00:32:55 -0400
Received: from mga01.intel.com ([192.55.52.88]:36911 &quot;EHLO mga01.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752791AbbELEcu (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 May 2015 00:32:50 -0400
Received: from orsmga001.jf.intel.com ([10.7.209.18])
	by fmsmga101.fm.intel.com with ESMTP; 11 May 2015 21:32:46 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.13,412,1427785200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;693442336&quot;
Received: from dwillia2-desk3.jf.intel.com (HELO
	dwillia2-desk3.amr.corp.intel.com) ([10.23.232.36])
	by orsmga001.jf.intel.com with ESMTP; 11 May 2015 21:32:47 -0700
Subject: [PATCH v3 07/11] x86: support dma_map_pfn()
From: Dan Williams &lt;dan.j.williams@intel.com&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-arch@vger.kernel.org, axboe@kernel.dk, riel@redhat.com,
	linux-nvdimm@lists.01.org, david@fromorbit.com, hch@lst.de,
	j.glisse@gmail.com, mgorman@suse.de, linux-fsdevel@vger.kernel.org,
	akpm@linux-foundation.org, mingo@kernel.org
Date: Tue, 12 May 2015 00:30:07 -0400
Message-ID: &lt;20150512043006.11521.34823.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
In-Reply-To: &lt;20150512042629.11521.70356.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
References: &lt;20150512042629.11521.70356.stgit@dwillia2-desk3.amr.corp.intel.com&gt;
User-Agent: StGit/0.17.1-8-g92dd
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - May 12, 2015, 4:30 a.m.</div>
<pre class="content">
Fix up x86 dma_map_ops to allow pfn-only mappings.

As long as a dma_map_sg() implementation uses the generic sg_phys()
helpers it can support scatterlists that use __pfn_t instead of struct
page.
<span class="signed-off-by">
Signed-off-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
---
 arch/x86/Kconfig                         |    5 +++++
 arch/x86/kernel/amd_gart_64.c            |   22 +++++++++++++++++-----
 arch/x86/kernel/pci-nommu.c              |   22 +++++++++++++++++-----
 arch/x86/kernel/pci-swiotlb.c            |    4 ++++
 arch/x86/pci/sta2x11-fixup.c             |    4 ++++
 arch/x86/xen/pci-swiotlb-xen.c           |    4 ++++
 drivers/iommu/amd_iommu.c                |   21 ++++++++++++++++-----
 drivers/iommu/intel-iommu.c              |   22 +++++++++++++++++-----
 drivers/xen/swiotlb-xen.c                |   29 +++++++++++++++++++----------
 include/asm-generic/dma-mapping-common.h |    4 ++--
 include/asm-generic/scatterlist.h        |    1 +
 include/linux/swiotlb.h                  |    4 ++++
 lib/swiotlb.c                            |   20 +++++++++++++++-----
 13 files changed, 125 insertions(+), 37 deletions(-)


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 226d5696e1d1..c626ffa5c01e 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -796,6 +796,7 @@</span> <span class="p_context"> config CALGARY_IOMMU</span>
 	bool &quot;IBM Calgary IOMMU support&quot;
 	select SWIOTLB
 	depends on X86_64 &amp;&amp; PCI
<span class="p_add">+	depends on !HAVE_DMA_PFN</span>
 	---help---
 	  Support for hardware IOMMUs in IBM&#39;s xSeries x366 and x460
 	  systems. Needed to run systems with more than 3GB of memory
<span class="p_chunk">@@ -1432,6 +1433,10 @@</span> <span class="p_context"> config X86_PMEM_LEGACY</span>
 
 	  Say Y if unsure.
 
<span class="p_add">+config X86_PMEM_DMA</span>
<span class="p_add">+	def_bool DEV_PFN</span>
<span class="p_add">+	select HAVE_DMA_PFN</span>
<span class="p_add">+</span>
 config HIGHPTE
 	bool &quot;Allocate 3rd-level pagetables from highmem&quot;
 	depends on HIGHMEM
<span class="p_header">diff --git a/arch/x86/kernel/amd_gart_64.c b/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_header">index 8e3842fc8bea..8fad83c8dfd2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/amd_gart_64.c</span>
<span class="p_chunk">@@ -239,13 +239,13 @@</span> <span class="p_context"> static dma_addr_t dma_map_area(struct device *dev, dma_addr_t phys_mem,</span>
 }
 
 /* Map a single area into the IOMMU */
<span class="p_del">-static dma_addr_t gart_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				unsigned long offset, size_t size,</span>
<span class="p_del">-				enum dma_data_direction dir,</span>
<span class="p_del">-				struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t gart_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+			       unsigned long offset, size_t size,</span>
<span class="p_add">+			       enum dma_data_direction dir,</span>
<span class="p_add">+			       struct dma_attrs *attrs)</span>
 {
 	unsigned long bus;
<span class="p_del">-	phys_addr_t paddr = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t paddr = __pfn_t_to_phys(pfn) + offset;</span>
 
 	if (!dev)
 		dev = &amp;x86_dma_fallback_dev;
<span class="p_chunk">@@ -259,6 +259,14 @@</span> <span class="p_context"> static dma_addr_t gart_map_page(struct device *dev, struct page *page,</span>
 	return bus;
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t gart_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return gart_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Free a DMA mapping.
  */
<span class="p_chunk">@@ -699,7 +707,11 @@</span> <span class="p_context"> static __init int init_amd_gatt(struct agp_kern_info *info)</span>
 static struct dma_map_ops gart_dma_ops = {
 	.map_sg				= gart_map_sg,
 	.unmap_sg			= gart_unmap_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn			= gart_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page			= gart_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page			= gart_unmap_page,
 	.alloc				= gart_alloc_coherent,
 	.free				= gart_free_coherent,
<span class="p_header">diff --git a/arch/x86/kernel/pci-nommu.c b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">index da15918d1c81..876dacfbabf6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-nommu.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-nommu.c</span>
<span class="p_chunk">@@ -25,12 +25,12 @@</span> <span class="p_context"> check_addr(char *name, struct device *hwdev, dma_addr_t bus, size_t size)</span>
 	return 1;
 }
 
<span class="p_del">-static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				 unsigned long offset, size_t size,</span>
<span class="p_del">-				 enum dma_data_direction dir,</span>
<span class="p_del">-				 struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t nommu_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				unsigned long offset, size_t size,</span>
<span class="p_add">+				enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	dma_addr_t bus = page_to_phys(page) + offset;</span>
<span class="p_add">+	dma_addr_t bus = __pfn_t_to_phys(pfn) + offset;</span>
 	WARN_ON(size == 0);
 	if (!check_addr(&quot;map_single&quot;, dev, bus, size))
 		return DMA_ERROR_CODE;
<span class="p_chunk">@@ -38,6 +38,14 @@</span> <span class="p_context"> static dma_addr_t nommu_map_page(struct device *dev, struct page *page,</span>
 	return bus;
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t nommu_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return nommu_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Map a set of buffers described by scatterlist in streaming
  * mode for DMA.  This is the scatter-gather version of the
  * above pci_map_single interface.  Here the scatter gather list
<span class="p_chunk">@@ -92,7 +100,11 @@</span> <span class="p_context"> struct dma_map_ops nommu_dma_ops = {</span>
 	.alloc			= dma_generic_alloc_coherent,
 	.free			= dma_generic_free_coherent,
 	.map_sg			= nommu_map_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn		= nommu_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page		= nommu_map_page,
<span class="p_add">+#endif</span>
 	.sync_single_for_device = nommu_sync_single_for_device,
 	.sync_sg_for_device	= nommu_sync_sg_for_device,
 	.is_phys		= 1,
<span class="p_header">diff --git a/arch/x86/kernel/pci-swiotlb.c b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">index 77dd0ad58be4..5351eb8c8f7f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-swiotlb.c</span>
<span class="p_chunk">@@ -48,7 +48,11 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = swiotlb_sync_sg_for_device,
 	.map_sg = swiotlb_map_sg_attrs,
 	.unmap_sg = swiotlb_unmap_sg_attrs,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = swiotlb_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = swiotlb_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = swiotlb_unmap_page,
 	.dma_supported = NULL,
 };
<span class="p_header">diff --git a/arch/x86/pci/sta2x11-fixup.c b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">index 5ceda85b8687..d1c6e3808bb5 100644</span>
<span class="p_header">--- a/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_header">+++ b/arch/x86/pci/sta2x11-fixup.c</span>
<span class="p_chunk">@@ -182,7 +182,11 @@</span> <span class="p_context"> static void *sta2x11_swiotlb_alloc_coherent(struct device *dev,</span>
 static struct dma_map_ops sta2x11_dma_ops = {
 	.alloc = sta2x11_swiotlb_alloc_coherent,
 	.free = x86_swiotlb_free_coherent,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = swiotlb_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = swiotlb_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = swiotlb_unmap_page,
 	.map_sg = swiotlb_map_sg_attrs,
 	.unmap_sg = swiotlb_unmap_sg_attrs,
<span class="p_header">diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c</span>
<span class="p_header">index 0e98e5d241d0..e65ea48d7aed 100644</span>
<span class="p_header">--- a/arch/x86/xen/pci-swiotlb-xen.c</span>
<span class="p_header">+++ b/arch/x86/xen/pci-swiotlb-xen.c</span>
<span class="p_chunk">@@ -28,7 +28,11 @@</span> <span class="p_context"> static struct dma_map_ops xen_swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = xen_swiotlb_sync_sg_for_device,
 	.map_sg = xen_swiotlb_map_sg_attrs,
 	.unmap_sg = xen_swiotlb_unmap_sg_attrs,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = xen_swiotlb_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = xen_swiotlb_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = xen_swiotlb_unmap_page,
 	.dma_supported = xen_swiotlb_dma_supported,
 };
<span class="p_header">diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="p_header">index e43d48956dea..ee8f70224b73 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu.c</span>
<span class="p_chunk">@@ -2754,16 +2754,15 @@</span> <span class="p_context"> static void __unmap_single(struct dma_ops_domain *dma_dom,</span>
 /*
  * The exported map_single function for dma_ops.
  */
<span class="p_del">-static dma_addr_t map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-			   unsigned long offset, size_t size,</span>
<span class="p_del">-			   enum dma_data_direction dir,</span>
<span class="p_del">-			   struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t map_pfn(struct device *dev, __pfn_t pfn, unsigned long offset,</span>
<span class="p_add">+		size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+		struct dma_attrs *attrs)</span>
 {
 	unsigned long flags;
 	struct protection_domain *domain;
 	dma_addr_t addr;
 	u64 dma_mask;
<span class="p_del">-	phys_addr_t paddr = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t paddr = __pfn_t_to_phys(pfn) + offset;</span>
 
 	INC_STATS_COUNTER(cnt_map_single);
 
<span class="p_chunk">@@ -2788,6 +2787,14 @@</span> <span class="p_context"> out:</span>
 	spin_unlock_irqrestore(&amp;domain-&gt;lock, flags);
 
 	return addr;
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __maybe_unused dma_addr_t map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+		unsigned long offset, size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+		struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return map_pfn(dev, page_to_pfn_t(page), offset, size, dir, attrs);</span>
 }
 
 /*
<span class="p_chunk">@@ -3062,7 +3069,11 @@</span> <span class="p_context"> static void __init prealloc_protection_domains(void)</span>
 static struct dma_map_ops amd_iommu_dma_ops = {
 	.alloc = alloc_coherent,
 	.free = free_coherent,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = unmap_page,
 	.map_sg = map_sg,
 	.unmap_sg = unmap_sg,
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 9b9ada71e0d3..6d9a0f85b827 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -3086,15 +3086,23 @@</span> <span class="p_context"> error:</span>
 	return 0;
 }
 
<span class="p_del">-static dma_addr_t intel_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				 unsigned long offset, size_t size,</span>
<span class="p_del">-				 enum dma_data_direction dir,</span>
<span class="p_del">-				 struct dma_attrs *attrs)</span>
<span class="p_add">+static dma_addr_t intel_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				unsigned long offset, size_t size,</span>
<span class="p_add">+				enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	return __intel_map_single(dev, page_to_phys(page) + offset, size,</span>
<span class="p_add">+	return __intel_map_single(dev, __pfn_t_to_phys(pfn) + offset, size,</span>
 				  dir, *dev-&gt;dma_mask);
 }
 
<span class="p_add">+static __maybe_unused dma_addr_t intel_map_page(struct device *dev,</span>
<span class="p_add">+		struct page *page, unsigned long offset, size_t size,</span>
<span class="p_add">+		enum dma_data_direction dir, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return intel_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void flush_unmaps(void)
 {
 	int i, j;
<span class="p_chunk">@@ -3380,7 +3388,11 @@</span> <span class="p_context"> struct dma_map_ops intel_dma_ops = {</span>
 	.free = intel_free_coherent,
 	.map_sg = intel_map_sg,
 	.unmap_sg = intel_unmap_sg,
<span class="p_add">+#ifdef CONFIG_HAVE_DMA_PFN</span>
<span class="p_add">+	.map_pfn = intel_map_pfn,</span>
<span class="p_add">+#else</span>
 	.map_page = intel_map_page,
<span class="p_add">+#endif</span>
 	.unmap_page = intel_unmap_page,
 	.mapping_error = intel_mapping_error,
 };
<span class="p_header">diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c</span>
<span class="p_header">index 810ad419e34c..bd29d09bbacc 100644</span>
<span class="p_header">--- a/drivers/xen/swiotlb-xen.c</span>
<span class="p_header">+++ b/drivers/xen/swiotlb-xen.c</span>
<span class="p_chunk">@@ -382,10 +382,10 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(xen_swiotlb_free_coherent);</span>
  * Once the device is given the dma address, the device owns this memory until
  * either xen_swiotlb_unmap_page or xen_swiotlb_dma_sync_single is performed.
  */
<span class="p_del">-dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				unsigned long offset, size_t size,</span>
<span class="p_del">-				enum dma_data_direction dir,</span>
<span class="p_del">-				struct dma_attrs *attrs)</span>
<span class="p_add">+dma_addr_t xen_swiotlb_map_pfn(struct device *dev, unsigned long pfn,</span>
<span class="p_add">+			       unsigned long offset, size_t size,</span>
<span class="p_add">+			       enum dma_data_direction dir,</span>
<span class="p_add">+			       struct dma_attrs *attrs)</span>
 {
 	phys_addr_t map, phys = page_to_phys(page) + offset;
 	dma_addr_t dev_addr = xen_phys_to_bus(phys);
<span class="p_chunk">@@ -429,6 +429,16 @@</span> <span class="p_context"> dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,</span>
 	}
 	return dev_addr;
 }
<span class="p_add">+EXPORT_SYMBOL_GPL(xen_swiotlb_map_pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+				unsigned long offset, size_t size,</span>
<span class="p_add">+				enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return xen_swiotlb_map_pfn(dev, page_to_pfn(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL_GPL(xen_swiotlb_map_page);
 
 /*
<span class="p_chunk">@@ -582,15 +592,14 @@</span> <span class="p_context"> xen_swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,</span>
 						attrs);
 			sg-&gt;dma_address = xen_phys_to_bus(map);
 		} else {
<span class="p_add">+			__pfn_t pfn = { .pfn = paddr &gt;&gt; PAGE_SHIFT };</span>
<span class="p_add">+			unsigned long offset = paddr &amp; ~PAGE_MASK;</span>
<span class="p_add">+</span>
 			/* we are not interested in the dma_addr returned by
 			 * xen_dma_map_page, only in the potential cache flushes executed
 			 * by the function. */
<span class="p_del">-			xen_dma_map_page(hwdev, pfn_to_page(paddr &gt;&gt; PAGE_SHIFT),</span>
<span class="p_del">-						dev_addr,</span>
<span class="p_del">-						paddr &amp; ~PAGE_MASK,</span>
<span class="p_del">-						sg-&gt;length,</span>
<span class="p_del">-						dir,</span>
<span class="p_del">-						attrs);</span>
<span class="p_add">+			xen_dma_map_pfn(hwdev, pfn, dev_addr, offset,</span>
<span class="p_add">+					sg-&gt;length, attrs);</span>
 			sg-&gt;dma_address = dev_addr;
 		}
 		sg_dma_len(sg) = sg-&gt;length;
<span class="p_header">diff --git a/include/asm-generic/dma-mapping-common.h b/include/asm-generic/dma-mapping-common.h</span>
<span class="p_header">index 7305efb1bac6..e031b079ce4e 100644</span>
<span class="p_header">--- a/include/asm-generic/dma-mapping-common.h</span>
<span class="p_header">+++ b/include/asm-generic/dma-mapping-common.h</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,</span>
 	kmemcheck_mark_initialized(ptr, size);
 	BUG_ON(!valid_dma_direction(dir));
 #ifdef CONFIG_HAVE_DMA_PFN
<span class="p_del">-	addr = ops-&gt;map_pfn(dev, page_to_pfn_typed(virt_to_page(ptr)),</span>
<span class="p_add">+	addr = ops-&gt;map_pfn(dev, page_to_pfn_t(virt_to_page(ptr)),</span>
 			     (unsigned long)ptr &amp; ~PAGE_MASK, size,
 			     dir, attrs);
 #else
<span class="p_chunk">@@ -99,7 +99,7 @@</span> <span class="p_context"> static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,</span>
 				      enum dma_data_direction dir)
 {
 	kmemcheck_mark_initialized(page_address(page) + offset, size);
<span class="p_del">-	return dma_map_pfn(dev, page_to_pfn_typed(page), offset, size, dir);</span>
<span class="p_add">+	return dma_map_pfn(dev, page_to_pfn_t(page), offset, size, dir);</span>
 }
 #else
 static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
<span class="p_header">diff --git a/include/asm-generic/scatterlist.h b/include/asm-generic/scatterlist.h</span>
<span class="p_header">index 959f51572a8e..ddc743513e55 100644</span>
<span class="p_header">--- a/include/asm-generic/scatterlist.h</span>
<span class="p_header">+++ b/include/asm-generic/scatterlist.h</span>
<span class="p_chunk">@@ -2,6 +2,7 @@</span> <span class="p_context"></span>
 #define __ASM_GENERIC_SCATTERLIST_H
 
 #include &lt;linux/types.h&gt;
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
 
 struct scatterlist {
 #ifdef CONFIG_DEBUG_SG
<span class="p_header">diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h</span>
<span class="p_header">index e7a018eaf3a2..5093fc8d2825 100644</span>
<span class="p_header">--- a/include/linux/swiotlb.h</span>
<span class="p_header">+++ b/include/linux/swiotlb.h</span>
<span class="p_chunk">@@ -66,6 +66,10 @@</span> <span class="p_context"> extern dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 				   unsigned long offset, size_t size,
 				   enum dma_data_direction dir,
 				   struct dma_attrs *attrs);
<span class="p_add">+extern dma_addr_t swiotlb_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+				  unsigned long offset, size_t size,</span>
<span class="p_add">+				  enum dma_data_direction dir,</span>
<span class="p_add">+				  struct dma_attrs *attrs);</span>
 extern void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 			       size_t size, enum dma_data_direction dir,
 			       struct dma_attrs *attrs);
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 4abda074ea45..4183d691bf2e 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -727,12 +727,12 @@</span> <span class="p_context"> swiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,</span>
  * Once the device is given the dma address, the device owns this memory until
  * either swiotlb_unmap_page or swiotlb_dma_sync_single is performed.
  */
<span class="p_del">-dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-			    unsigned long offset, size_t size,</span>
<span class="p_del">-			    enum dma_data_direction dir,</span>
<span class="p_del">-			    struct dma_attrs *attrs)</span>
<span class="p_add">+dma_addr_t swiotlb_map_pfn(struct device *dev, __pfn_t pfn,</span>
<span class="p_add">+			   unsigned long offset, size_t size,</span>
<span class="p_add">+			   enum dma_data_direction dir,</span>
<span class="p_add">+			   struct dma_attrs *attrs)</span>
 {
<span class="p_del">-	phys_addr_t map, phys = page_to_phys(page) + offset;</span>
<span class="p_add">+	phys_addr_t map, phys = __pfn_t_to_phys(pfn) + offset;</span>
 	dma_addr_t dev_addr = phys_to_dma(dev, phys);
 
 	BUG_ON(dir == DMA_NONE);
<span class="p_chunk">@@ -763,6 +763,16 @@</span> <span class="p_context"> dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
 
 	return dev_addr;
 }
<span class="p_add">+EXPORT_SYMBOL_GPL(swiotlb_map_pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+			    unsigned long offset, size_t size,</span>
<span class="p_add">+			    enum dma_data_direction dir,</span>
<span class="p_add">+			    struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swiotlb_map_pfn(dev, page_to_pfn_t(page), offset, size, dir,</span>
<span class="p_add">+			attrs);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL_GPL(swiotlb_map_page);
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



