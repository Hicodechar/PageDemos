
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[patches] Re: [PATCH v9 05/12] RISC-V: Atomic and Locking Code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [patches] Re: [PATCH v9 05/12] RISC-V: Atomic and Locking Code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 15, 2017, 7:48 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;mhng-6f69cfda-45a6-419d-93b4-ab53d4288660@palmer-si-x1c4&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10060111/mbox/"
   >mbox</a>
|
   <a href="/patch/10060111/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10060111/">/patch/10060111/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D0AB26023A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Nov 2017 19:48:16 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BC3BE2A006
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Nov 2017 19:48:16 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B07522A299; Wed, 15 Nov 2017 19:48:16 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C2D0D2A006
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 15 Nov 2017 19:48:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1759211AbdKOTsM (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 15 Nov 2017 14:48:12 -0500
Received: from mail-pg0-f68.google.com ([74.125.83.68]:49565 &quot;EHLO
	mail-pg0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756561AbdKOTsE (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 15 Nov 2017 14:48:04 -0500
Received: by mail-pg0-f68.google.com with SMTP id 70so6249670pgf.6
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 15 Nov 2017 11:48:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=date:subject:in-reply-to:cc:from:to:message-id:mime-version
	:content-transfer-encoding;
	bh=kTp3ILLurYwDpGgPmgr4NDdN86/qZE89YzLqucjWcv0=;
	b=OheV0MJ65/NHCewSKsbgxdKOJVoUh65rsfdt/xZa1a8hz2VVCH0BZp1JJJGZrxiV+Y
	9VTZ/WqFpCDdd+v8nioFxhPa7NYO7xSWGP6NoGssCtuSCAuuZ2fXVu2Dsu8Gnx4/AFBl
	SZA8N/hKkdMGerAdJ6AyU/fxK3WvMz4DTHhtnPnf3l3wj20iQ/wMAyZluu808FJNc71K
	yUVqnyAihu4anWbXr//rsJrYVS9uNjZmaRw5r3OYueY9PB3JxQ1LiLfkrk4QoEwFWIs7
	YBI3rGlRQmsb0HDw7egVm1BgnrleoYQfWY6FbtLq7hmvU4JridlU7BTqp8rCzgFDqgRi
	021g==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:date:subject:in-reply-to:cc:from:to:message-id
	:mime-version:content-transfer-encoding;
	bh=kTp3ILLurYwDpGgPmgr4NDdN86/qZE89YzLqucjWcv0=;
	b=SGjPa5Tr7CYiogyiI+Y/aoHrQ5bs3L+hRtEIiKtN8pbYJCiyLSXNaO0Gy5M3lHrp60
	1Dv0fxTsK5jGEN+QOtVm1GkVXRO7Eu4mwLBNLjHAE/P7Kt9ko2iB/Rl9hqRy9ux+05ld
	Tv5zJ7m9HgKvDLCo5BRG4eBo05At9beHEZzjCIw9ZjsCvbX9dsVBOi04j4IZaDV5BkCL
	6vU2kQCBuoW8Ntq8pD8Rp6DgzPrsfK0ODreWZFkQBuRReVOMA9qA/H97Kg3t4URjn6d4
	B9SOl+pKu0IWHMF1jzRdFXt2swU9Niv0Ew0ThxINYJ9b8JysDcvEzn2MTI8gEbxcWlXe
	+zXw==
X-Gm-Message-State: AJaThX7ROW5akDEZG95gysvVFHgMiQAsWBPuh7NLTc7m5bLQ0uB/bO+z
	PG5lQp8abPdndYNaK38drbDG2g==
X-Google-Smtp-Source: AGs4zMYSOPA8YMeajvU1Nl4oDugSg3op4E1Te48FFO4UxEKhLd7KKbMAH7Q8rq1hzdpdHdIsJr9J1Q==
X-Received: by 10.98.157.219 with SMTP id a88mr15942607pfk.126.1510775283830;
	Wed, 15 Nov 2017 11:48:03 -0800 (PST)
Received: from localhost ([12.206.222.5]) by smtp.gmail.com with ESMTPSA id
	b16sm43706582pfe.58.2017.11.15.11.48.02
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 15 Nov 2017 11:48:03 -0800 (PST)
Date: Wed, 15 Nov 2017 11:48:03 -0800 (PST)
X-Google-Original-Date: Wed, 15 Nov 2017 11:47:58 PST (-0800)
Subject: Re: [patches] Re: [PATCH v9 05/12] RISC-V: Atomic and Locking Code
In-Reply-To: &lt;20171115180600.GR19071@arm.com&gt;
CC: Daniel Lustig &lt;dlustig@nvidia.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Olof Johansson &lt;olof@lixom.net&gt;, linux-kernel@vger.kernel.org,
	patches@groups.riscv.org, peterz@infradead.org, boqun.feng@gmail.com
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: will.deacon@arm.com
Message-ID: &lt;mhng-6f69cfda-45a6-419d-93b4-ab53d4288660@palmer-si-x1c4&gt;
Mime-Version: 1.0 (MHng)
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - Nov. 15, 2017, 7:48 p.m.</div>
<pre class="content">
On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:
<span class="quote">&gt; Hi Palmer,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com wrote:</span>
<span class="quote">&gt;&gt; &gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i)</span>
<span class="quote">&gt;&gt; &gt;&gt;+ATOMIC_OPS(sub, add, +, -i)</span>
<span class="quote">&gt;&gt; &gt;&gt;+ATOMIC_OPS(and, and, &amp;,  i)</span>
<span class="quote">&gt;&gt; &gt;&gt;+ATOMIC_OPS( or,  or, |,  i)</span>
<span class="quote">&gt;&gt; &gt;&gt;+ATOMIC_OPS(xor, xor, ^,  i)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;What is the point in the c_op parameter to these things?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I guess there isn&#39;t one, it just lingered from a handful of refactorings.</span>
<span class="quote">&gt;&gt; It&#39;s used in some of the other functions if we need to do a C operation</span>
<span class="quote">&gt;&gt; after the atomic.  How does this look?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; commit 5db229491a205ad0e1aa18287e3b342176c62d30 (HEAD -&gt; staging-mm)</span>
<span class="quote">&gt;&gt; Author: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
<span class="quote">&gt;&gt; Date:   Tue Nov 14 11:35:37 2017 -0800</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    RISC-V: Remove c_op from ATOMIC_OP</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    This was an unused macro parameter.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;    Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You can do the same thing for FETCH_OP, I think.</span>

I agree.  I think I got them all this time.

commit be88c8a5f714569b56fffcc9f2d3bc673bdaf16b
Author: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Date:   Wed Nov 15 10:19:15 2017 -0800

    Remove c_op from more places


Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175025">Daniel Lustig</a> - Nov. 15, 2017, 11:59 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:</span>
<span class="quote">&gt;&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; &gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Hi Palmer,</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire) ATOMIC_OPS(add, add,</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;++,  i, .rl  , _release)</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since there</span>
<span class="quote">&gt; &gt;&gt; &gt;are interpretations where that isn&#39;t the case. Specifically:</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;// all variables zero at start of time</span>
<span class="quote">&gt; &gt;&gt; &gt;P0:</span>
<span class="quote">&gt; &gt;&gt; &gt;WRITE_ONCE(x) = 1;</span>
<span class="quote">&gt; &gt;&gt; &gt;atomic_add_return(y, 1);</span>
<span class="quote">&gt; &gt;&gt; &gt;WRITE_ONCE(z) = 1;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;P1:</span>
<span class="quote">&gt; &gt;&gt; &gt;READ_ONCE(z) // reads 1</span>
<span class="quote">&gt; &gt;&gt; &gt;smp_rmb();</span>
<span class="quote">&gt; &gt;&gt; &gt;READ_ONCE(x) // must not read 0</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification yet.</span>
<span class="quote">&gt; &gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should have</span>
<span class="quote">&gt; &gt;&gt; a better idea</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I know it&#39;s the case for our current processors, and I&#39;m pretty sure it&#39;s the</span>
<span class="quote">&gt; case for what&#39;s formally specified, but we&#39;ll have to wait for the spec in order</span>
<span class="quote">&gt; to prove it.</span>

I think Will is right.  In the current spec, using .aqrl converts an RCpc load
or store into an RCsc load or store, but the acquire(-RCsc) annotation still
only applies to the load part of the atomic, and the release(-RCsc) annotation
applies only to the store part of the atomic.

Why is that?  Picture an machine which implements AMOs using something that
looks more like an LR/SC under the covers, or one that uses cache line locking,
or anything else along those same lines.  In some such machines, there could be
a window between lock/reserve and unlock/store-conditional where other later
stores could squeeze into, and that would break Will&#39;s example among others.

It&#39;s likely the same reasoning that causes ARM to use a trailing dmb here,
rather than just using ldaxr/stlxr.  Is that right Will?  I know that&#39;s LL/SC
and this particular cases uses AMOADD, but it&#39;s the same principle.  Well, at
least according to how we have it in the current memory model draft.

Also, RISC-V currently prefers leading fence mappings, so I think the result
here, for atomic_add_return() for example, should be this:

fence rw,rw
amoadd.aq ...

Note that at this point, I think you could even elide the .rl.  If I&#39;m reading
it right it looks like the ARM mapping does this too (well, the reverse: ARM
elides the &quot;a&quot; in ldaxr due to the trailing dmb making it redundant).

Does that seem reasonable to you all?

Dan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - Nov. 16, 2017, 1:19 a.m.</div>
<pre class="content">
On Wed, Nov 15, 2017 at 11:59:44PM +0000, Daniel Lustig wrote:
<span class="quote">&gt; &gt; On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:</span>
<span class="quote">&gt; &gt;&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Hi Palmer,</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire) ATOMIC_OPS(add, add,</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;&gt;++,  i, .rl  , _release)</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since there</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;are interpretations where that isn&#39;t the case. Specifically:</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;// all variables zero at start of time</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;P0:</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;WRITE_ONCE(x) = 1;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;atomic_add_return(y, 1);</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;WRITE_ONCE(z) = 1;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;P1:</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;READ_ONCE(z) // reads 1</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;smp_rmb();</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;READ_ONCE(x) // must not read 0</span>
<span class="quote">&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification yet.</span>
<span class="quote">&gt; &gt; &gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should have</span>
<span class="quote">&gt; &gt; &gt;&gt; a better idea</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I know it&#39;s the case for our current processors, and I&#39;m pretty sure it&#39;s the</span>
<span class="quote">&gt; &gt; case for what&#39;s formally specified, but we&#39;ll have to wait for the spec in order</span>
<span class="quote">&gt; &gt; to prove it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think Will is right.  In the current spec, using .aqrl converts an RCpc load</span>
<span class="quote">&gt; or store into an RCsc load or store, but the acquire(-RCsc) annotation still</span>
<span class="quote">&gt; only applies to the load part of the atomic, and the release(-RCsc) annotation</span>
<span class="quote">&gt; applies only to the store part of the atomic.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why is that?  Picture an machine which implements AMOs using something that</span>
<span class="quote">&gt; looks more like an LR/SC under the covers, or one that uses cache line locking,</span>
<span class="quote">&gt; or anything else along those same lines.  In some such machines, there could be</span>
<span class="quote">&gt; a window between lock/reserve and unlock/store-conditional where other later</span>
<span class="quote">&gt; stores could squeeze into, and that would break Will&#39;s example among others.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s likely the same reasoning that causes ARM to use a trailing dmb here,</span>
<span class="quote">&gt; rather than just using ldaxr/stlxr.  Is that right Will?  I know that&#39;s LL/SC</span>
<span class="quote">&gt; and this particular cases uses AMOADD, but it&#39;s the same principle.  Well, at</span>
<span class="quote">&gt; least according to how we have it in the current memory model draft.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, RISC-V currently prefers leading fence mappings, so I think the result</span>
<span class="quote">&gt; here, for atomic_add_return() for example, should be this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; fence rw,rw</span>
<span class="quote">&gt; amoadd.aq ...</span>
<span class="quote">&gt; </span>

Hmm.. if atomic_add_return() is implemented like that, how about the
following case:

	{x=0, y=0}

	P1:
	
	r1 = atomic_add_return(&amp;x, 1); // r1 == 0, x will 1 afterwards
	WRITE_ONCE(y, 1);

	P2:

	r2 = READ_ONCE(y); // r2 = 1
	smp_rmb();
	r3 = atomic_read(&amp;x); // r3 = 0?

, could this result in r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0? Given you said .aq
only effects the load part of AMO, and I don&#39;t see anything here
preventing the reordering between store of y and the store part of the
AMO on P1.

Note: we don&#39;t allow (r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0) in above case for
linux kernel. Please see Documentation/atomic_t.txt:

&quot;Fully ordered primitives are ordered against everything prior and
everything subsequent. Therefore a fully ordered primitive is like
having an smp_mb() before and an smp_mb() after the primitive.&quot;

Regards,
Boqun
<span class="quote">
&gt; Note that at this point, I think you could even elide the .rl.  If I&#39;m reading</span>
<span class="quote">&gt; it right it looks like the ARM mapping does this too (well, the reverse: ARM</span>
<span class="quote">&gt; elides the &quot;a&quot; in ldaxr due to the trailing dmb making it redundant).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does that seem reasonable to you all?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Dan</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175025">Daniel Lustig</a> - Nov. 16, 2017, 1:31 a.m.</div>
<pre class="content">
<span class="quote">&gt; -----Original Message-----</span>
<span class="quote">&gt; From: Boqun Feng [mailto:boqun.feng@gmail.com]</span>
<span class="quote">&gt; Sent: Wednesday, November 15, 2017 5:19 PM</span>
<span class="quote">&gt; To: Daniel Lustig &lt;dlustig@nvidia.com&gt;</span>
<span class="quote">&gt; Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;; will.deacon@arm.com; Arnd</span>
<span class="quote">&gt; Bergmann &lt;arnd@arndb.de&gt;; Olof Johansson &lt;olof@lixom.net&gt;; linux-</span>
<span class="quote">&gt; kernel@vger.kernel.org; patches@groups.riscv.org; peterz@infradead.org</span>
<span class="quote">&gt; Subject: Re: [patches] Re: [PATCH v9 05/12] RISC-V: Atomic and Locking Code</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Wed, Nov 15, 2017 at 11:59:44PM +0000, Daniel Lustig wrote:</span>
<span class="quote">&gt; &gt; &gt; On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com</span>
<span class="quote">&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; &gt;&gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Hi Palmer,</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire) ATOMIC_OPS(add,</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;+add,</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;++,  i, .rl  , _release)</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;there are interpretations where that isn&#39;t the case. Specifically:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;// all variables zero at start of time</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;P0:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;WRITE_ONCE(x) = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;atomic_add_return(y, 1);</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;WRITE_ONCE(z) = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;P1:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;READ_ONCE(z) // reads 1</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;smp_rmb();</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;READ_ONCE(x) // must not read 0</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification</span>
<span class="quote">&gt; yet.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; have a better idea</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; I know it&#39;s the case for our current processors, and I&#39;m pretty sure</span>
<span class="quote">&gt; &gt; &gt; it&#39;s the case for what&#39;s formally specified, but we&#39;ll have to wait</span>
<span class="quote">&gt; &gt; &gt; for the spec in order to prove it.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think Will is right.  In the current spec, using .aqrl converts an</span>
<span class="quote">&gt; &gt; RCpc load or store into an RCsc load or store, but the acquire(-RCsc)</span>
<span class="quote">&gt; &gt; annotation still only applies to the load part of the atomic, and the</span>
<span class="quote">&gt; &gt; release(-RCsc) annotation applies only to the store part of the atomic.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Why is that?  Picture an machine which implements AMOs using something</span>
<span class="quote">&gt; &gt; that looks more like an LR/SC under the covers, or one that uses cache</span>
<span class="quote">&gt; &gt; line locking, or anything else along those same lines.  In some such</span>
<span class="quote">&gt; &gt; machines, there could be a window between lock/reserve and</span>
<span class="quote">&gt; &gt; unlock/store-conditional where other later stores could squeeze into, and</span>
<span class="quote">&gt; that would break Will&#39;s example among others.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It&#39;s likely the same reasoning that causes ARM to use a trailing dmb</span>
<span class="quote">&gt; &gt; here, rather than just using ldaxr/stlxr.  Is that right Will?  I know</span>
<span class="quote">&gt; &gt; that&#39;s LL/SC and this particular cases uses AMOADD, but it&#39;s the same</span>
<span class="quote">&gt; &gt; principle.  Well, at least according to how we have it in the current memory</span>
<span class="quote">&gt; model draft.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Also, RISC-V currently prefers leading fence mappings, so I think the</span>
<span class="quote">&gt; &gt; result here, for atomic_add_return() for example, should be this:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; fence rw,rw</span>
<span class="quote">&gt; &gt; amoadd.aq ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm.. if atomic_add_return() is implemented like that, how about the</span>
<span class="quote">&gt; following case:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	{x=0, y=0}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	P1:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	r1 = atomic_add_return(&amp;x, 1); // r1 == 0, x will 1 afterwards</span>
<span class="quote">&gt; 	WRITE_ONCE(y, 1);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	P2:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	r2 = READ_ONCE(y); // r2 = 1</span>
<span class="quote">&gt; 	smp_rmb();</span>
<span class="quote">&gt; 	r3 = atomic_read(&amp;x); // r3 = 0?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; , could this result in r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0? Given you said .aq only</span>
<span class="quote">&gt; effects the load part of AMO, and I don&#39;t see anything here preventing the</span>
<span class="quote">&gt; reordering between store of y and the store part of the AMO on P1.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note: we don&#39;t allow (r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0) in above case for linux</span>
<span class="quote">&gt; kernel. Please see Documentation/atomic_t.txt:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;Fully ordered primitives are ordered against everything prior and everything</span>
<span class="quote">&gt; subsequent. Therefore a fully ordered primitive is like having an smp_mb()</span>
<span class="quote">&gt; before and an smp_mb() after the primitive.&quot;</span>

Yes, you&#39;re right Boqun.  Good catch, and sorry for over-optimizing too quickly.

In that case, maybe we should just start out having a fence on both sides for
now, and then we&#39;ll discuss offline whether we want to change the model&#39;s
behavior here.

Dan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - Nov. 16, 2017, 1:52 a.m.</div>
<pre class="content">
On Thu, Nov 16, 2017 at 01:31:21AM +0000, Daniel Lustig wrote:
<span class="quote">&gt; &gt; -----Original Message-----</span>
<span class="quote">&gt; &gt; From: Boqun Feng [mailto:boqun.feng@gmail.com]</span>
<span class="quote">&gt; &gt; Sent: Wednesday, November 15, 2017 5:19 PM</span>
<span class="quote">&gt; &gt; To: Daniel Lustig &lt;dlustig@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;; will.deacon@arm.com; Arnd</span>
<span class="quote">&gt; &gt; Bergmann &lt;arnd@arndb.de&gt;; Olof Johansson &lt;olof@lixom.net&gt;; linux-</span>
<span class="quote">&gt; &gt; kernel@vger.kernel.org; patches@groups.riscv.org; peterz@infradead.org</span>
<span class="quote">&gt; &gt; Subject: Re: [patches] Re: [PATCH v9 05/12] RISC-V: Atomic and Locking Code</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Wed, Nov 15, 2017 at 11:59:44PM +0000, Daniel Lustig wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com</span>
<span class="quote">&gt; &gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; &gt;&gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Hi Palmer,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire) ATOMIC_OPS(add,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;&gt;+add,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;&gt;++,  i, .rl  , _release)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;there are interpretations where that isn&#39;t the case. Specifically:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;// all variables zero at start of time</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;P0:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;WRITE_ONCE(x) = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;atomic_add_return(y, 1);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;WRITE_ONCE(z) = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;P1:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;READ_ONCE(z) // reads 1</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;smp_rmb();</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; &gt;READ_ONCE(x) // must not read 0</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification</span>
<span class="quote">&gt; &gt; yet.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;&gt; have a better idea</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; I know it&#39;s the case for our current processors, and I&#39;m pretty sure</span>
<span class="quote">&gt; &gt; &gt; &gt; it&#39;s the case for what&#39;s formally specified, but we&#39;ll have to wait</span>
<span class="quote">&gt; &gt; &gt; &gt; for the spec in order to prove it.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; I think Will is right.  In the current spec, using .aqrl converts an</span>
<span class="quote">&gt; &gt; &gt; RCpc load or store into an RCsc load or store, but the acquire(-RCsc)</span>
<span class="quote">&gt; &gt; &gt; annotation still only applies to the load part of the atomic, and the</span>
<span class="quote">&gt; &gt; &gt; release(-RCsc) annotation applies only to the store part of the atomic.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Why is that?  Picture an machine which implements AMOs using something</span>
<span class="quote">&gt; &gt; &gt; that looks more like an LR/SC under the covers, or one that uses cache</span>
<span class="quote">&gt; &gt; &gt; line locking, or anything else along those same lines.  In some such</span>
<span class="quote">&gt; &gt; &gt; machines, there could be a window between lock/reserve and</span>
<span class="quote">&gt; &gt; &gt; unlock/store-conditional where other later stores could squeeze into, and</span>
<span class="quote">&gt; &gt; that would break Will&#39;s example among others.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; It&#39;s likely the same reasoning that causes ARM to use a trailing dmb</span>
<span class="quote">&gt; &gt; &gt; here, rather than just using ldaxr/stlxr.  Is that right Will?  I know</span>
<span class="quote">&gt; &gt; &gt; that&#39;s LL/SC and this particular cases uses AMOADD, but it&#39;s the same</span>
<span class="quote">&gt; &gt; &gt; principle.  Well, at least according to how we have it in the current memory</span>
<span class="quote">&gt; &gt; model draft.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Also, RISC-V currently prefers leading fence mappings, so I think the</span>
<span class="quote">&gt; &gt; &gt; result here, for atomic_add_return() for example, should be this:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; fence rw,rw</span>
<span class="quote">&gt; &gt; &gt; amoadd.aq ...</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hmm.. if atomic_add_return() is implemented like that, how about the</span>
<span class="quote">&gt; &gt; following case:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	{x=0, y=0}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	P1:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	r1 = atomic_add_return(&amp;x, 1); // r1 == 0, x will 1 afterwards</span>
<span class="quote">&gt; &gt; 	WRITE_ONCE(y, 1);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	P2:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	r2 = READ_ONCE(y); // r2 = 1</span>
<span class="quote">&gt; &gt; 	smp_rmb();</span>
<span class="quote">&gt; &gt; 	r3 = atomic_read(&amp;x); // r3 = 0?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; , could this result in r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0? Given you said .aq only</span>
<span class="quote">&gt; &gt; effects the load part of AMO, and I don&#39;t see anything here preventing the</span>
<span class="quote">&gt; &gt; reordering between store of y and the store part of the AMO on P1.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Note: we don&#39;t allow (r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0) in above case for linux</span>
<span class="quote">&gt; &gt; kernel. Please see Documentation/atomic_t.txt:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &quot;Fully ordered primitives are ordered against everything prior and everything</span>
<span class="quote">&gt; &gt; subsequent. Therefore a fully ordered primitive is like having an smp_mb()</span>
<span class="quote">&gt; &gt; before and an smp_mb() after the primitive.&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, you&#39;re right Boqun.  Good catch, and sorry for over-optimizing too quickly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In that case, maybe we should just start out having a fence on both sides for</span>

Actually, given your architecture is RCsc rather than RCpc, so I think
maybe you could follow the way that ARM uses(i.e. relaxed load + release
store + a full barrier). You can see the commit log of 8e86f0b409a4
(&quot;arm64: atomics: fix use of acquire + release for full barrier
semantics&quot;) for the reasoning:
	
	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8e86f0b409a44193f1587e87b69c5dcf8f65be67
<span class="quote">
&gt; now, and then we&#39;ll discuss offline whether we want to change the model&#39;s</span>
<span class="quote">&gt; behavior here.</span>
<span class="quote">&gt; </span>

Sounds great! Any estimation when we can see that(maybe a draft)?

Regards,
Boqun
<span class="quote">
&gt; Dan</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - Nov. 16, 2017, 2:08 a.m.</div>
<pre class="content">
On Wed, 15 Nov 2017 15:59:44 PST (-0800), Daniel Lustig wrote:
<span class="quote">&gt;&gt; On Wed, 15 Nov 2017 10:06:01 PST (-0800), will.deacon@arm.com wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Nov 14, 2017 at 12:30:59PM -0800, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt; On Tue, 24 Oct 2017 07:10:33 PDT (-0700), will.deacon@arm.com wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt;On Tue, Sep 26, 2017 at 06:56:31PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Hi Palmer,</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire) ATOMIC_OPS(add, add,</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt;++,  i, .rl  , _release)</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since there</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;are interpretations where that isn&#39;t the case. Specifically:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;// all variables zero at start of time</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;P0:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;WRITE_ONCE(x) = 1;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;atomic_add_return(y, 1);</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;WRITE_ONCE(z) = 1;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;P1:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;READ_ONCE(z) // reads 1</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;smp_rmb();</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;READ_ONCE(x) // must not read 0</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification yet.</span>
<span class="quote">&gt;&gt; &gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should have</span>
<span class="quote">&gt;&gt; &gt;&gt; a better idea</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I know it&#39;s the case for our current processors, and I&#39;m pretty sure it&#39;s the</span>
<span class="quote">&gt;&gt; case for what&#39;s formally specified, but we&#39;ll have to wait for the spec in order</span>
<span class="quote">&gt;&gt; to prove it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think Will is right.  In the current spec, using .aqrl converts an RCpc load</span>
<span class="quote">&gt; or store into an RCsc load or store, but the acquire(-RCsc) annotation still</span>
<span class="quote">&gt; only applies to the load part of the atomic, and the release(-RCsc) annotation</span>
<span class="quote">&gt; applies only to the store part of the atomic.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why is that?  Picture an machine which implements AMOs using something that</span>
<span class="quote">&gt; looks more like an LR/SC under the covers, or one that uses cache line locking,</span>
<span class="quote">&gt; or anything else along those same lines.  In some such machines, there could be</span>
<span class="quote">&gt; a window between lock/reserve and unlock/store-conditional where other later</span>
<span class="quote">&gt; stores could squeeze into, and that would break Will&#39;s example among others.</span>

I&#39;m not sure I understand this.  My brand new mental model here is that

  amoadd.w.aqrl Rout, Rinc, Raddr

is exactly the same as

  0:
    lr.w.aq Rout, Raddr
    add Rout, Rout, Rinc
    sc.w.rl Rtmp, Raddr
    bnez Rtmp, 0b

but I don&#39;t see how that allows the WRITE_ONCE(z) to appear before the 
WRITE_ONCE(x).  I get it appearing before the SC, but not the LR.  Am I 
misunderstanding acquire/release here?  My model for that is that that .aq 
doesn&#39;t let accesses from after (in program order) be observed before, while 
.rl doesn&#39;t let accesses from before move after.

Either way, I think that&#39;s still broken, as given the above sequence we could 
observe

  READ_ONCE(y); // -&gt; 0, the add hasn&#39;t happened
  smp_rmb();
  READ_ONCE(z); // -&gt; 1, the write has happened

which is bad.
<span class="quote">
&gt; It&#39;s likely the same reasoning that causes ARM to use a trailing dmb here,</span>
<span class="quote">&gt; rather than just using ldaxr/stlxr.  Is that right Will?  I know that&#39;s LL/SC</span>
<span class="quote">&gt; and this particular cases uses AMOADD, but it&#39;s the same principle.  Well, at</span>
<span class="quote">&gt; least according to how we have it in the current memory model draft.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also, RISC-V currently prefers leading fence mappings, so I think the result</span>
<span class="quote">&gt; here, for atomic_add_return() for example, should be this:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; fence rw,rw</span>
<span class="quote">&gt; amoadd.aq ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Note that at this point, I think you could even elide the .rl.  If I&#39;m reading</span>
<span class="quote">&gt; it right it looks like the ARM mapping does this too (well, the reverse: ARM</span>
<span class="quote">&gt; elides the &quot;a&quot; in ldaxr due to the trailing dmb making it redundant).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Does that seem reasonable to you all?</span>

Well, it&#39;s kind of a bummer on my end -- we&#39;re going to have to change a lot of 
stuff to add an extra fence, and it appears that .aqrl is pretty useless.  I&#39;m 
also not convinced this is even valid, it&#39;d map to

    fence rw,rw
  0:
    lr.w.aq Rout, Raddr
    add Rout, Rout, Rinc
    sc.w Rtmp, Raddr
    bnez Rtmp, 0b

which would still allow a subsequent store to be made visible before the 
&#39;sc.w&#39;, with or without a .rl.  Doesn&#39;t this mean we need

  fence rw,rw
  amo.w ...
  fence rw,rw

which seems pretty heavy-handed.  FWIW, this is exactly how our current 
hardware interprets amo.w.aqrl -- not really an argument for anything, we&#39;re 
just super conservative because there&#39;s no formal spec.

It looks like we&#39;re still safe for the _acquire and _release versions.  From 
Documentation/atomic_t.txt:

  {}_relaxed: unordered
  {}_acquire: the R of the RMW (or atomic_read) is an ACQUIRE
  {}_release: the W of the RMW (or atomic_set)  is a  RELEASE

So it&#39;s just the ordered ones that are broken.   I&#39;m assuming this means our 
cmpxchg is broken as well

  0:
    lr.w.aqrl Rold, Raddr
    bne Rcomp, Rold, 1f
    sc.w.aqrl Rtmp, Rnew, Raddr
    bnez Rtmp, 0b
  1:

because sc.w.aqrl would be the same as sc.w.rl, so it&#39;s just the same as the 
amoadd case above.

Hopefully I&#39;m just crazy here, I should probably get some sleep at some point 
:)...  I just scanned over some more mail that went by, let&#39;s just double-fence 
for now.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175025">Daniel Lustig</a> - Nov. 16, 2017, 6:40 a.m.</div>
<pre class="content">
<span class="quote">&gt; &gt; In that case, maybe we should just start out having a fence on both</span>
<span class="quote">&gt; &gt; sides for</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Actually, given your architecture is RCsc rather than RCpc, so I think maybe</span>
<span class="quote">&gt; you could follow the way that ARM uses(i.e. relaxed load + release store + a</span>
<span class="quote">&gt; full barrier). You can see the commit log of 8e86f0b409a4</span>
<span class="quote">&gt; (&quot;arm64: atomics: fix use of acquire + release for full barrier</span>
<span class="quote">&gt; semantics&quot;) for the reasoning:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/co</span>
<span class="quote">&gt; mmit/?id=8e86f0b409a44193f1587e87b69c5dcf8f65be67</span>

OK I&#39;m catching up now...and I have to say, that is clever!  Thanks for the
corrections.  It would definitely be good to avoid the double fence.  Let me
think this over a bit more.

One subtle point about RCpc vs. RCsc, though: see below.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Sounds great! Any estimation when we can see that(maybe a draft)?</span>

The latest should be November 28-30, coinciding with the next RISC-V workshop.
Possibly even sooner.  We just recently resolved a big debate that&#39;s been
holding us up for a while, and so now it&#39;s just a matter of me writing it all
up so it&#39;s understandable.

In the meantime, though, let me quickly and informally summarize some of the
highlights, in case it helps unblock any further review here:

- The model is now (other-)multi-copy atomic
- .aq and .rl alone mean RCpc
- .aqrl means RCsc
- .aq applies only to the read part of an RMW
- .rl applies only to the write part of an RMW
- Syntactic dependencies are now respected

I recognize this isn&#39;t enough detail to do it proper justice, but we&#39;ll get the
full model out as soon as we can.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Regards,</span>
<span class="quote">&gt; Boqun</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Dan</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Nov. 16, 2017, 10:25 a.m.</div>
<pre class="content">
Hi Daniel,

On Thu, Nov 16, 2017 at 06:40:46AM +0000, Daniel Lustig wrote:
<span class="quote">&gt; &gt; &gt; In that case, maybe we should just start out having a fence on both</span>
<span class="quote">&gt; &gt; &gt; sides for</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Actually, given your architecture is RCsc rather than RCpc, so I think maybe</span>
<span class="quote">&gt; &gt; you could follow the way that ARM uses(i.e. relaxed load + release store + a</span>
<span class="quote">&gt; &gt; full barrier). You can see the commit log of 8e86f0b409a4</span>
<span class="quote">&gt; &gt; (&quot;arm64: atomics: fix use of acquire + release for full barrier</span>
<span class="quote">&gt; &gt; semantics&quot;) for the reasoning:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/co</span>
<span class="quote">&gt; &gt; mmit/?id=8e86f0b409a44193f1587e87b69c5dcf8f65be67</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK I&#39;m catching up now...and I have to say, that is clever!  Thanks for the</span>
<span class="quote">&gt; corrections.  It would definitely be good to avoid the double fence.  Let me</span>
<span class="quote">&gt; think this over a bit more.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; One subtle point about RCpc vs. RCsc, though: see below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Sounds great! Any estimation when we can see that(maybe a draft)?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The latest should be November 28-30, coinciding with the next RISC-V workshop.</span>
<span class="quote">&gt; Possibly even sooner.  We just recently resolved a big debate that&#39;s been</span>
<span class="quote">&gt; holding us up for a while, and so now it&#39;s just a matter of me writing it all</span>
<span class="quote">&gt; up so it&#39;s understandable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the meantime, though, let me quickly and informally summarize some of the</span>
<span class="quote">&gt; highlights, in case it helps unblock any further review here:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - The model is now (other-)multi-copy atomic</span>
<span class="quote">&gt; - .aq and .rl alone mean RCpc</span>
<span class="quote">&gt; - .aqrl means RCsc</span>

That presents you with an interesting choice when implementing locks: do you
use .aqrl for lock and unlokc and elide smp_mb__after_spinlock, or do you use
.aq/.rl for lock/unlock respectively and emit a fence for
smp_mb__after_spinlock?
<span class="quote">
&gt; - .aq applies only to the read part of an RMW</span>
<span class="quote">&gt; - .rl applies only to the write part of an RMW</span>
<span class="quote">&gt; - Syntactic dependencies are now respected</span>

Thanks for the update, even this brief summary is better than the current
architecture document ;)
<span class="quote">
&gt; I recognize this isn&#39;t enough detail to do it proper justice, but we&#39;ll get the</span>
<span class="quote">&gt; full model out as soon as we can.</span>

Ok, I&#39;ll bite! Do you forbid LB?

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=175025">Daniel Lustig</a> - Nov. 16, 2017, 5:12 p.m.</div>
<pre class="content">
<span class="quote">&gt; From: Will Deacon [mailto:will.deacon@arm.com]</span>
<span class="quote">&gt; Hi Daniel,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Nov 16, 2017 at 06:40:46AM +0000, Daniel Lustig wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; In that case, maybe we should just start out having a fence on</span>
<span class="quote">&gt; &gt; &gt; &gt; both sides for</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Actually, given your architecture is RCsc rather than RCpc, so I</span>
<span class="quote">&gt; &gt; &gt; think maybe you could follow the way that ARM uses(i.e. relaxed load</span>
<span class="quote">&gt; &gt; &gt; + release store + a full barrier). You can see the commit log of</span>
<span class="quote">&gt; &gt; &gt; 8e86f0b409a4</span>
<span class="quote">&gt; &gt; &gt; (&quot;arm64: atomics: fix use of acquire + release for full barrier</span>
<span class="quote">&gt; &gt; &gt; semantics&quot;) for the reasoning:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/c</span>
<span class="quote">&gt; &gt; &gt; o</span>
<span class="quote">&gt; &gt; &gt; mmit/?id=8e86f0b409a44193f1587e87b69c5dcf8f65be67</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; OK I&#39;m catching up now...and I have to say, that is clever!  Thanks</span>
<span class="quote">&gt; &gt; for the corrections.  It would definitely be good to avoid the double</span>
<span class="quote">&gt; &gt; fence.  Let me think this over a bit more.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; One subtle point about RCpc vs. RCsc, though: see below.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Sounds great! Any estimation when we can see that(maybe a draft)?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The latest should be November 28-30, coinciding with the next RISC-V</span>
<span class="quote">&gt; workshop.</span>
<span class="quote">&gt; &gt; Possibly even sooner.  We just recently resolved a big debate that&#39;s</span>
<span class="quote">&gt; &gt; been holding us up for a while, and so now it&#39;s just a matter of me</span>
<span class="quote">&gt; &gt; writing it all up so it&#39;s understandable.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; In the meantime, though, let me quickly and informally summarize some</span>
<span class="quote">&gt; &gt; of the highlights, in case it helps unblock any further review here:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; - The model is now (other-)multi-copy atomic</span>
<span class="quote">&gt; &gt; - .aq and .rl alone mean RCpc</span>
<span class="quote">&gt; &gt; - .aqrl means RCsc</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That presents you with an interesting choice when implementing locks: do</span>
<span class="quote">&gt; you use .aqrl for lock and unlokc and elide smp_mb__after_spinlock, or do</span>
<span class="quote">&gt; you use .aq/.rl for lock/unlock respectively and emit a fence for</span>
<span class="quote">&gt; smp_mb__after_spinlock?</span>

Good question, and I should probably do my homework before weighing in strongly
either way.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; - .aq applies only to the read part of an RMW</span>
<span class="quote">&gt; &gt; - .rl applies only to the write part of an RMW</span>
<span class="quote">&gt; &gt; - Syntactic dependencies are now respected</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for the update, even this brief summary is better than the current</span>
<span class="quote">&gt; architecture document ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I recognize this isn&#39;t enough detail to do it proper justice, but</span>
<span class="quote">&gt; &gt; we&#39;ll get the full model out as soon as we can.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, I&#39;ll bite! Do you forbid LB?</span>

This was definitely one of the topics we debated.  In the end, we chose to
allow LB but forbid LB+deps or stronger.  CPUs may not produce LB all that
often, but we didn&#39;t see a super strong need to rule it out either.
Out-of-thin-air is covered now that we enforce dependencies.  Also, GPUs do
produce LB, and that&#39;s relevant to our (NVIDIA&#39;s) use case at least.

I&#39;m happy to keep answering other questions too.

Dan
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
<span class="p_header">index 77a7fc55daab..1982c865ba59 100644</span>
<span class="p_header">--- a/arch/riscv/include/asm/atomic.h</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -83,7 +83,7 @@</span> <span class="p_context"> ATOMIC_OPS(xor, xor,  i)</span>
  * There&#39;s two flavors of these: the arithmatic ops have both fetch and return
  * versions, while the logical ops only have fetch versions.
  */
<span class="p_del">-#define ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, asm_op, I, asm_or, c_or, asm_type, c_type, prefix)				\</span>
 static __always_inline c_type atomic##prefix##_fetch_##op##c_or(c_type i, atomic##prefix##_t *v)	\
 {													\
 	register c_type ret;										\
<span class="p_chunk">@@ -103,13 +103,13 @@</span> <span class="p_context"> static __always_inline c_type atomic##prefix##_##op##_return##c_or(c_type i, ato</span>

 #ifdef CONFIG_GENERIC_ATOMIC64
 #define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I, asm_or, c_or, w,  int,   )	\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )
 #else
 #define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I, asm_or, c_or, w,  int,   )	\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, d, long, 64)	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I, asm_or, c_or, d, long, 64)	\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)
 #endif

<span class="p_chunk">@@ -126,28 +126,28 @@</span> <span class="p_context"> ATOMIC_OPS(sub, add, +, -i, .aqrl,         )</span>
 #undef ATOMIC_OPS

 #ifdef CONFIG_GENERIC_ATOMIC64
<span class="p_del">-#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_del">-        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, I, asm_or, c_or, w,  int,   )</span>
 #else
<span class="p_del">-#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_del">-        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )		\</span>
<span class="p_del">-        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, I, asm_or, c_or, d, long, 64)</span>
 #endif

<span class="p_del">-ATOMIC_OPS(and, and, &amp;,  i,      , _relaxed)</span>
<span class="p_del">-ATOMIC_OPS(and, and, &amp;,  i, .aq  , _acquire)</span>
<span class="p_del">-ATOMIC_OPS(and, and, &amp;,  i, .rl  , _release)</span>
<span class="p_del">-ATOMIC_OPS(and, and, &amp;,  i, .aqrl,         )</span>
<span class="p_add">+ATOMIC_OPS(and, and, i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(and, and, i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(and, and, i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(and, and, i, .aqrl,         )</span>

<span class="p_del">-ATOMIC_OPS( or,  or, |,  i,      , _relaxed)</span>
<span class="p_del">-ATOMIC_OPS( or,  or, |,  i, .aq  , _acquire)</span>
<span class="p_del">-ATOMIC_OPS( or,  or, |,  i, .rl  , _release)</span>
<span class="p_del">-ATOMIC_OPS( or,  or, |,  i, .aqrl,         )</span>
<span class="p_add">+ATOMIC_OPS( or,  or, i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, i, .aqrl,         )</span>

<span class="p_del">-ATOMIC_OPS(xor, xor, ^,  i,      , _relaxed)</span>
<span class="p_del">-ATOMIC_OPS(xor, xor, ^,  i, .aq  , _acquire)</span>
<span class="p_del">-ATOMIC_OPS(xor, xor, ^,  i, .rl  , _release)</span>
<span class="p_del">-ATOMIC_OPS(xor, xor, ^,  i, .aqrl,         )</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, i, .aqrl,         )</span>

 #undef ATOMIC_OPS

<span class="p_chunk">@@ -182,13 +182,13 @@</span> <span class="p_context"> ATOMIC_OPS(add_negative, add,  &lt;, 0)</span>
 #undef ATOMIC_OP
 #undef ATOMIC_OPS

<span class="p_del">-#define ATOMIC_OP(op, func_op, c_op, I, c_type, prefix)				\</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, I, c_type, prefix)				\</span>
 static __always_inline void atomic##prefix##_##op(atomic##prefix##_t *v)	\
 {										\
 	atomic##prefix##_##func_op(I, v);					\
 }

<span class="p_del">-#define ATOMIC_FETCH_OP(op, func_op, c_op, I, c_type, prefix)				\</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, func_op, I, c_type, prefix)					\</span>
 static __always_inline c_type atomic##prefix##_fetch_##op(atomic##prefix##_t *v)	\
 {											\
 	return atomic##prefix##_fetch_##func_op(I, v);					\
<span class="p_chunk">@@ -202,16 +202,16 @@</span> <span class="p_context"> static __always_inline c_type atomic##prefix##_##op##_return(atomic##prefix##_t</span>

 #ifdef CONFIG_GENERIC_ATOMIC64
 #define ATOMIC_OPS(op, asm_op, c_op, I)						\
<span class="p_del">-        ATOMIC_OP       (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op,       I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I,  int,   )				\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )
 #else
 #define ATOMIC_OPS(op, asm_op, c_op, I)						\
<span class="p_del">-        ATOMIC_OP       (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op,       I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I,  int,   )				\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )				\
<span class="p_del">-        ATOMIC_OP       (op, asm_op, c_op, I, long, 64)				\</span>
<span class="p_del">-        ATOMIC_FETCH_OP (op, asm_op, c_op, I, long, 64)				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op,       I, long, 64)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op,       I, long, 64)				\</span>
         ATOMIC_OP_RETURN(op, asm_op, c_op, I, long, 64)
 #endif

&gt;
&gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i,      , _relaxed)
&gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire)
&gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .rl  , _release)
&gt;&gt; &gt;&gt;+ATOMIC_OPS(add, add, +,  i, .aqrl,         )
&gt;&gt; &gt;
&gt;&gt; &gt;Have you checked that .aqrl is equivalent to &quot;ordered&quot;, since there are
&gt;&gt; &gt;interpretations where that isn&#39;t the case. Specifically:
&gt;&gt; &gt;
&gt;&gt; &gt;// all variables zero at start of time
&gt;&gt; &gt;P0:
&gt;&gt; &gt;WRITE_ONCE(x) = 1;
&gt;&gt; &gt;atomic_add_return(y, 1);
&gt;&gt; &gt;WRITE_ONCE(z) = 1;
&gt;&gt; &gt;
&gt;&gt; &gt;P1:
&gt;&gt; &gt;READ_ONCE(z) // reads 1
&gt;&gt; &gt;smp_rmb();
&gt;&gt; &gt;READ_ONCE(x) // must not read 0
&gt;&gt;
&gt;&gt; I haven&#39;t.  We don&#39;t quite have a formal memory model specification yet.
&gt;&gt; I&#39;ve added Daniel Lustig, who is creating that model.  He should have a
&gt;&gt; better idea
&gt;
&gt; Thanks. You really do need to ensure that, as it&#39;s heavily relied upon.

I know it&#39;s the case for our current processors, and I&#39;m pretty sure it&#39;s the 
case for what&#39;s formally specified, but we&#39;ll have to wait for the spec in 
order to prove it.

&gt;&gt; &gt;&gt;+static inline void arch_read_lock(arch_rwlock_t *lock)
&gt;&gt; &gt;&gt;+{
&gt;&gt; &gt;&gt;+	int tmp;
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+	__asm__ __volatile__(
&gt;&gt; &gt;&gt;+		&quot;1:	lr.w	%1, %0\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	bltz	%1, 1b\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	addi	%1, %1, 1\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	sc.w.aq	%1, %1, %0\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	bnez	%1, 1b\n&quot;
&gt;&gt; &gt;&gt;+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)
&gt;&gt; &gt;&gt;+		:: &quot;memory&quot;);
&gt;&gt; &gt;&gt;+}
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+static inline void arch_write_lock(arch_rwlock_t *lock)
&gt;&gt; &gt;&gt;+{
&gt;&gt; &gt;&gt;+	int tmp;
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+	__asm__ __volatile__(
&gt;&gt; &gt;&gt;+		&quot;1:	lr.w	%1, %0\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	bnez	%1, 1b\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	li	%1, -1\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	sc.w.aq	%1, %1, %0\n&quot;
&gt;&gt; &gt;&gt;+		&quot;	bnez	%1, 1b\n&quot;
&gt;&gt; &gt;&gt;+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)
&gt;&gt; &gt;&gt;+		:: &quot;memory&quot;);
&gt;&gt; &gt;&gt;+}
&gt;&gt; &gt;
&gt;&gt; &gt;I think you have the same starvation issues as we have on arm64 here. I
&gt;&gt; &gt;strongly recommend moving over to qrwlock :)
&gt;
&gt; I still strongly recommend this!

I agree.  I think we can replace both of these locks with the generic versions 
<span class="p_del">-- they&#39;re a lot better than ours.  It&#39;s on the TODO list.</span>

&gt;
&gt;&gt; &gt;
&gt;&gt; &gt;&gt;+#ifndef _ASM_RISCV_TLBFLUSH_H
&gt;&gt; &gt;&gt;+#define _ASM_RISCV_TLBFLUSH_H
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+#ifdef CONFIG_MMU
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+/* Flush entire local TLB */
&gt;&gt; &gt;&gt;+static inline void local_flush_tlb_all(void)
&gt;&gt; &gt;&gt;+{
&gt;&gt; &gt;&gt;+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);
&gt;&gt; &gt;&gt;+}
&gt;&gt; &gt;&gt;+
&gt;&gt; &gt;&gt;+/* Flush one page from local TLB */
&gt;&gt; &gt;&gt;+static inline void local_flush_tlb_page(unsigned long addr)
&gt;&gt; &gt;&gt;+{
&gt;&gt; &gt;&gt;+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);
&gt;&gt; &gt;&gt;+}
&gt;&gt; &gt;
&gt;&gt; &gt;Is this serialised against prior updates to the page table (set_pte) and
&gt;&gt; &gt;also against subsequent instruction fetch?
&gt;&gt;
&gt;&gt; This is a store -&gt; (load, store) fence.  The ordering is between stores that
&gt;&gt; touch paging data structures and the implicit loads that come from any
&gt;&gt; memory access when paging is enabled.  As far as I can tell, it does not
&gt;&gt; enforce any instruction fetch ordering constraints.
&gt;
&gt; That sounds pretty suspicious to me. You need to ensure that speculative
&gt; instruction fetch doesn&#39;t fetch instructions from the old mapping. Also,
&gt; store -&gt; (load, store) fences don&#39;t generally order the page table walk,
&gt; which is what you need to order here.

It&#39;s the specific type of store -&gt; (load, store) fence that orders page table 
walking.  &quot;sfence.vma&quot; orders between the page table walker and the store 
buffer on a single core, as opposed to &quot;fence w,rw&quot; which would order between 
the memory stages of two different cores.

I&#39;ll have to check with a memory model guy for sure, but I don&#39;t think 
&quot;sfence.vma&quot; enforces anything WRT the instruction stream.  If that&#39;s the case, 
this should do it

commit 4fa4374b3c73fc0022e51e648e8c9285829f6155
Author: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Date:   Wed Nov 15 10:28:16 2017 -0800

    fence.i on TLB flushes

<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_header">index 5ee4ae370b5e..cda08a9734c5 100644</span>
<span class="p_header">--- a/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -17,16 +17,28 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_MMU
 
<span class="p_del">-/* Flush entire local TLB */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Flush entire local TLB.  We need the fence.i to ensure newly mapped pages</span>
<span class="p_add">+ * are fetched correctly.</span>
<span class="p_add">+ */</span>
 static inline void local_flush_tlb_all(void)
 {
<span class="p_del">-	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;sfence.vma\n\t&quot;</span>
<span class="p_add">+		&quot;fence.i&quot;</span>
<span class="p_add">+		: : : &quot;memory&quot;</span>
<span class="p_add">+	);</span>
 }
 
 /* Flush one page from local TLB */
 static inline void local_flush_tlb_page(unsigned long addr)
 {
<span class="p_del">-	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;sfence.vma %0\n\t&quot;</span>
<span class="p_add">+		&quot;fence.i&quot;</span>
<span class="p_add">+		: : &quot;r&quot; (addr)</span>
<span class="p_add">+		: &quot;memory&quot;</span>
<span class="p_add">+	);</span>
 }
 
 #ifndef CONFIG_SMP

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



