
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm, oom: allow oom reaper to race with exit_mmap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm, oom: allow oom reaper to race with exit_mmap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 25, 2017, 6:26 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170725182619.GQ29716@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9862693/mbox/"
   >mbox</a>
|
   <a href="/patch/9862693/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9862693/">/patch/9862693/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2749C6038C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 18:26:29 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 216EF2679B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 18:26:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 1574B284AF; Tue, 25 Jul 2017 18:26:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 653B82679B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 18:26:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751344AbdGYS0Z (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 25 Jul 2017 14:26:25 -0400
Received: from mx1.redhat.com ([209.132.183.28]:58632 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750768AbdGYS0Y (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 25 Jul 2017 14:26:24 -0400
Received: from smtp.corp.redhat.com
	(int-mx06.intmail.prod.int.phx2.redhat.com [10.5.11.16])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 42F8A7F3F8;
	Tue, 25 Jul 2017 18:26:24 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 42F8A7F3F8
Authentication-Results: ext-mx01.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx01.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=aarcange@redhat.com
Received: from mail.random (ovpn-116-29.ams2.redhat.com [10.36.116.29])
	by smtp.corp.redhat.com (Postfix) with ESMTPS id 9143865E94;
	Tue, 25 Jul 2017 18:26:20 +0000 (UTC)
Date: Tue, 25 Jul 2017 20:26:19 +0200
From: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
To: Michal Hocko &lt;mhocko@kernel.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;, linux-mm@kvack.org,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: Re: [PATCH] mm, oom: allow oom reaper to race with exit_mmap
Message-ID: &lt;20170725182619.GQ29716@redhat.com&gt;
References: &lt;20170724072332.31903-1-mhocko@kernel.org&gt;
	&lt;20170725152639.GP29716@redhat.com&gt;
	&lt;20170725154514.GN26723@dhcp22.suse.cz&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20170725154514.GN26723@dhcp22.suse.cz&gt;
User-Agent: Mutt/1.8.3 (2017-05-23)
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.16
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.25]);
	Tue, 25 Jul 2017 18:26:24 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 25, 2017, 6:26 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 05:45:14PM +0200, Michal Hocko wrote:
<span class="quote">&gt; That problem is real though as reported by David.</span>

I&#39;m not against fixing it, I just think it&#39;s not a major concern, and
the solution doesn&#39;t seem optimal as measured by Kirill.

I&#39;m just skeptical it&#39;s the best to solve that tiny race, 99.9% of the
time such down_write is unnecessary.
<span class="quote">
&gt; it is not only about exit_mmap. __mmput calls into exit_aio and that can</span>
<span class="quote">&gt; wait for completion and there is no way to guarantee this will finish in</span>
<span class="quote">&gt; finite time.</span>

exit_aio blocking is actually the only good point for wanting this
concurrency where exit_mmap-&gt;unmap_vmas and
oom_reap_task-&gt;unmap_page_range have to run concurrently on the same
mm.

exit_mmap would have no issue, if there was enough time in the
lifetime CPU to allocate the memory, sure the memory will also be
freed in finite amount of time by exit_mmap.

In fact you mentioned multiple OOM in the NUMA case, exit_mmap may not
solve that, so depending on the runtime it may have been better not to
wait all memory of the process to be freed before moving to the next
task, but only a couple of seconds before the OOM reaper moves to a
new candidate. Again this is only a tradeoff between solving the OOM
faster vs risk of false positives OOM.

If it wasn&#39;t because of exit_aio (which may have to wait I/O
completion), changing the OOM reaper to return &quot;false&quot; if
mmget_not_zero returns zero and MMF_OOM_SKIP is not set yet, would
have been enough (and depending on the runtime it may have solved OOM
faster in NUMA) and there would be absolutely no need to run OOM
reaper and exit_mmap concurrently on the same mm. However there&#39;s such
exit_aio..

Raw I/O mempools never require memory allocations, although aio if it
involves a filesystem to complete may run into filesystem or buffering
locks which are known to loop forever or depend on other tasks stuck
in kernel allocations, so I didn&#39;t go down that chain too long.

So the simplest is to use a similar trick to what ksm_exit uses, this
is untested just to show the idea it may require further adjustment as
the bit isn&#39;t used only for the test_and_set_bit locking, but I didn&#39;t
see much issues with other set_bit/test_bit.

From f414244480fdc1f771b3148feb3fac77ec813e4c Mon Sep 17 00:00:00 2001
<span class="from">From: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
Date: Tue, 25 Jul 2017 20:02:27 +0200
Subject: [PATCH 1/1] mm: oom: let oom_reap_task and exit_mmap to run
 concurrently

This is purely required because exit_aio() may block and exit_mmap() may
never start, if the oom_reap_task cannot start running on a mm with
mm_users == 0.

At the same time if the OOM reaper doesn&#39;t wait at all for the memory
of the current OOM candidate to be freed by exit_mmap-&gt;unmap_vmas, it
would generate a spurious OOM kill.

If it wasn&#39;t because of the exit_aio it would be enough to change the
oom_reap_task in the case it finds mm_users == 0, to wait for a
timeout or to wait for __mmput to set MMF_OOM_SKIP itself, but it&#39;s
not just exit_mmap the problem here so the concurrency of exit_mmap
and oom_reap_task is apparently warranted.

It&#39;s a non standard runtime, exit_mmap runs without mmap_sem, and
oom_reap_task runs with the mmap_sem for reading as usual (kind of
MADV_DONTNEED).

The race between the two is solved with pair of test_and_set_bit and a
dummy down_write/up_write cycle on the same lines of the ksm_exit
method.

If the OOM reaper sets the bit first, exit_mmap will wait it to finish
in down_write (before taking down mm structures that would make the
oom_reap_task fail with use after free).

If exit_mmap comes first, oom reaper will skip the mm as it&#39;s already
all freed.
<span class="signed-off-by">
Signed-off-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
---
 kernel/fork.c |  1 -
 mm/mmap.c     |  5 +++++
 mm/oom_kill.c | 15 ++++++++++++---
 3 files changed, 17 insertions(+), 4 deletions(-)


This will perform the same as then the set_bit in __mmput can be
removed and test_and_set_bit doesn&#39;t cost more (at least on x86, on
other archs it requires an smp_mb too, but it&#39;s marginal difference),
still an atomic op that is.
<span class="quote">
&gt; &gt; 4) how is it safe to overwrite a VM_FAULT_RETRY that returns without</span>
<span class="quote">&gt; &gt;    mmap_sem and then the arch code will release the mmap_sem despite</span>
<span class="quote">&gt; &gt;    it was already released by handle_mm_fault? Anonymous memory faults</span>
<span class="quote">&gt; &gt;    aren&#39;t common to return VM_FAULT_RETRY but an userfault</span>
<span class="quote">&gt; &gt;    can. Shouldn&#39;t there be a block that prevents overwriting if</span>
<span class="quote">&gt; &gt;    VM_FAULT_RETRY is set below? (not only VM_FAULT_ERROR)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (unlikely((current-&gt;flags &amp; PF_KTHREAD) &amp;&amp; !(ret &amp; VM_FAULT_ERROR)</span>
<span class="quote">&gt; &gt; 				&amp;&amp; test_bit(MMF_UNSTABLE, &amp;vma-&gt;vm_mm-&gt;flags)))</span>
<span class="quote">&gt; &gt; 		ret = VM_FAULT_SIGBUS;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand what you mean and how this is related to the</span>
<span class="quote">&gt; patch?</span>

It&#39;s not related to the patch but it involves the OOM reaper as it
only happens when MMF_UNSTABLE is set which is set only by the OOM
reaper. I was simply reading the OOM reaper code and following up what
MMF_UNSTABLE does and it ringed a bell.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 26, 2017, 5:45 a.m.</div>
<pre class="content">
On Tue 25-07-17 20:26:19, Andrea Arcangeli wrote:
<span class="quote">&gt; On Tue, Jul 25, 2017 at 05:45:14PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; That problem is real though as reported by David.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not against fixing it, I just think it&#39;s not a major concern, and</span>
<span class="quote">&gt; the solution doesn&#39;t seem optimal as measured by Kirill.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m just skeptical it&#39;s the best to solve that tiny race, 99.9% of the</span>
<span class="quote">&gt; time such down_write is unnecessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; it is not only about exit_mmap. __mmput calls into exit_aio and that can</span>
<span class="quote">&gt; &gt; wait for completion and there is no way to guarantee this will finish in</span>
<span class="quote">&gt; &gt; finite time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; exit_aio blocking is actually the only good point for wanting this</span>
<span class="quote">&gt; concurrency where exit_mmap-&gt;unmap_vmas and</span>
<span class="quote">&gt; oom_reap_task-&gt;unmap_page_range have to run concurrently on the same</span>
<span class="quote">&gt; mm.</span>

Yes, exit_aio is the only blocking call I know of currently. But I would
like this to be as robust as possible and so I do not want to rely on
the current implementation. This can change in future and I can
guarantee that nobody will think about the oom path when adding
something to the final __mmput path.
<span class="quote">
&gt; exit_mmap would have no issue, if there was enough time in the</span>
<span class="quote">&gt; lifetime CPU to allocate the memory, sure the memory will also be</span>
<span class="quote">&gt; freed in finite amount of time by exit_mmap.</span>

I am not sure I understand. Say that any call prior to unmap_vmas blocks
on a lock which is held by another call path which cannot proceed with
the allocation...
<span class="quote"> 
&gt; In fact you mentioned multiple OOM in the NUMA case, exit_mmap may not</span>
<span class="quote">&gt; solve that, so depending on the runtime it may have been better not to</span>
<span class="quote">&gt; wait all memory of the process to be freed before moving to the next</span>
<span class="quote">&gt; task, but only a couple of seconds before the OOM reaper moves to a</span>
<span class="quote">&gt; new candidate. Again this is only a tradeoff between solving the OOM</span>
<span class="quote">&gt; faster vs risk of false positives OOM.</span>

I really do not want to rely on any timing. This just too fragile. Once
we have killed a task then we shouldn&#39;t pick another victim until it
passed exit_mmap or the oom_reaper did its job. Otherwise we just risk
false positives while we have already disrupted the workload.
<span class="quote"> 
&gt; If it wasn&#39;t because of exit_aio (which may have to wait I/O</span>
<span class="quote">&gt; completion), changing the OOM reaper to return &quot;false&quot; if</span>
<span class="quote">&gt; mmget_not_zero returns zero and MMF_OOM_SKIP is not set yet, would</span>
<span class="quote">&gt; have been enough (and depending on the runtime it may have solved OOM</span>
<span class="quote">&gt; faster in NUMA) and there would be absolutely no need to run OOM</span>
<span class="quote">&gt; reaper and exit_mmap concurrently on the same mm. However there&#39;s such</span>
<span class="quote">&gt; exit_aio..</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Raw I/O mempools never require memory allocations, although aio if it</span>
<span class="quote">&gt; involves a filesystem to complete may run into filesystem or buffering</span>
<span class="quote">&gt; locks which are known to loop forever or depend on other tasks stuck</span>
<span class="quote">&gt; in kernel allocations, so I didn&#39;t go down that chain too long.</span>

Exactly. We simply cannot assume anything here because veryfying this
basically impossible.
 
[...]
<span class="quote">&gt; diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="quote">&gt; index f19efcf75418..615133762b99 100644</span>
<span class="quote">&gt; --- a/mm/mmap.c</span>
<span class="quote">&gt; +++ b/mm/mmap.c</span>
<span class="quote">&gt; @@ -2993,6 +2993,11 @@ void exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt;  	/* Use -1 here to ensure all VMAs in the mm are unmapped */</span>
<span class="quote">&gt;  	unmap_vmas(&amp;tlb, vma, 0, -1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (test_and_set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags)) {</span>
<span class="quote">&gt; +		/* wait the OOM reaper to stop working on this mm */</span>
<span class="quote">&gt; +		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; index 9e8b4f030c1c..2a7000995784 100644</span>
<span class="quote">&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; @@ -471,6 +471,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	struct mmu_gather tlb;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	bool ret = true;</span>
<span class="quote">&gt; +	bool mmgot = true;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * We have to make sure to not race with the victim exit path</span>
<span class="quote">&gt; @@ -500,9 +501,16 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	 * and delayed __mmput doesn&#39;t matter that much</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (!mmget_not_zero(mm)) {</span>
<span class="quote">&gt; -		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		trace_skip_task_reaping(tsk-&gt;pid);</span>
<span class="quote">&gt; -		goto unlock_oom;</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * MMF_OOM_SKIP is set by exit_mmap when the OOM</span>
<span class="quote">&gt; +		 * reaper can&#39;t work on the mm anymore.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (test_and_set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags)) {</span>
<span class="quote">&gt; +			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +			goto unlock_oom;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		mmgot = false;</span>
<span class="quote">&gt;  	}</span>

This will work more or less the same to what we have currently.

[victim]		[oom reaper]				[oom killer]
do_exit			__oom_reap_task_mm
  mmput
    __mmput
			  mmget_not_zero
			    test_and_set_bit(MMF_OOM_SKIP)
			    					oom_evaluate_task
								   # select next victim 
			  # reap the mm
      unmap_vmas

so we can select a next victim while the current one is still not
completely torn down.
<span class="quote">
&gt; &gt; &gt; 4) how is it safe to overwrite a VM_FAULT_RETRY that returns without</span>
<span class="quote">&gt; &gt; &gt;    mmap_sem and then the arch code will release the mmap_sem despite</span>
<span class="quote">&gt; &gt; &gt;    it was already released by handle_mm_fault? Anonymous memory faults</span>
<span class="quote">&gt; &gt; &gt;    aren&#39;t common to return VM_FAULT_RETRY but an userfault</span>
<span class="quote">&gt; &gt; &gt;    can. Shouldn&#39;t there be a block that prevents overwriting if</span>
<span class="quote">&gt; &gt; &gt;    VM_FAULT_RETRY is set below? (not only VM_FAULT_ERROR)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 	if (unlikely((current-&gt;flags &amp; PF_KTHREAD) &amp;&amp; !(ret &amp; VM_FAULT_ERROR)</span>
<span class="quote">&gt; &gt; &gt; 				&amp;&amp; test_bit(MMF_UNSTABLE, &amp;vma-&gt;vm_mm-&gt;flags)))</span>
<span class="quote">&gt; &gt; &gt; 		ret = VM_FAULT_SIGBUS;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I am not sure I understand what you mean and how this is related to the</span>
<span class="quote">&gt; &gt; patch?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s not related to the patch but it involves the OOM reaper as it</span>
<span class="quote">&gt; only happens when MMF_UNSTABLE is set which is set only by the OOM</span>
<span class="quote">&gt; reaper. I was simply reading the OOM reaper code and following up what</span>
<span class="quote">&gt; MMF_UNSTABLE does and it ringed a bell.</span>

I hope 3f70dc38cec2 (&quot;mm: make sure that kthreads will not refault oom
reaped memory&quot;) will clarify this code. If not please start a new thread
so that we do not conflate different things together.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 26, 2017, 4:39 p.m.</div>
<pre class="content">
On Wed, Jul 26, 2017 at 07:45:33AM +0200, Michal Hocko wrote:
<span class="quote">&gt; Yes, exit_aio is the only blocking call I know of currently. But I would</span>
<span class="quote">&gt; like this to be as robust as possible and so I do not want to rely on</span>
<span class="quote">&gt; the current implementation. This can change in future and I can</span>
<span class="quote">&gt; guarantee that nobody will think about the oom path when adding</span>
<span class="quote">&gt; something to the final __mmput path.</span>

I think ksm_exit may block too waiting for allocations, the generic
idea is those calls before exit_mmap can cause a problem yes.
<span class="quote">
&gt; &gt; exit_mmap would have no issue, if there was enough time in the</span>
<span class="quote">&gt; &gt; lifetime CPU to allocate the memory, sure the memory will also be</span>
<span class="quote">&gt; &gt; freed in finite amount of time by exit_mmap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure I understand. Say that any call prior to unmap_vmas blocks</span>
<span class="quote">&gt; on a lock which is held by another call path which cannot proceed with</span>
<span class="quote">&gt; the allocation...</span>

What I meant was, if three was no prior call to exit_mmap-&gt;unmap_vmas.
<span class="quote">
&gt; I really do not want to rely on any timing. This just too fragile. Once</span>
<span class="quote">&gt; we have killed a task then we shouldn&#39;t pick another victim until it</span>
<span class="quote">&gt; passed exit_mmap or the oom_reaper did its job. Otherwise we just risk</span>
<span class="quote">&gt; false positives while we have already disrupted the workload.</span>

On smaller systems lack or parallelism in OOM killing surely isn&#39;t a
problem.
<span class="quote">
&gt; This will work more or less the same to what we have currently.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [victim]		[oom reaper]				[oom killer]</span>
<span class="quote">&gt; do_exit			__oom_reap_task_mm</span>
<span class="quote">&gt;   mmput</span>
<span class="quote">&gt;     __mmput</span>
<span class="quote">&gt; 			  mmget_not_zero</span>
<span class="quote">&gt; 			    test_and_set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; 			    					oom_evaluate_task</span>
<span class="quote">&gt; 								   # select next victim </span>
<span class="quote">&gt; 			  # reap the mm</span>
<span class="quote">&gt;       unmap_vmas</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; so we can select a next victim while the current one is still not</span>
<span class="quote">&gt; completely torn down.</span>

How does oom_evaluate_task possibly run at the same time of
test_and_set_bit in __oom_reap_task_mm considering both are running
under the oom_lock? It&#39;s hard to see how what you describe above could
materialize as second and third column cannot run in parallel because
of the oom_lock.

I don&#39;t think there was any issue, but then you pointed out the
locking on signal-&gt;oom_mm that is protected by the task_lock vs
current-&gt;mm NULL check, so I can replace in my patch the
test_and_set_bit with set_bit on one side and the oom_mm task_lock
protected locking on the other side. This way I can put back a set_bit
in the __mmput fast path (instead of test_and_set_bit) and it&#39;s even
more efficient. With such a change, I&#39;ll also stop depending on the
oom_lock to prevent second and third column to run in parallel.

I still didn&#39;t remove the oom_lock outright that seems orthogonal
change unrelated to this issue but now you could remove it as far as
the above is concerned.
<span class="quote">
&gt; I hope 3f70dc38cec2 (&quot;mm: make sure that kthreads will not refault oom</span>
<span class="quote">&gt; reaped memory&quot;) will clarify this code. If not please start a new thread</span>
<span class="quote">&gt; so that we do not conflate different things together.</span>

I&#39;ll look into that, thanks.
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 27, 2017, 6:32 a.m.</div>
<pre class="content">
On Wed 26-07-17 18:39:28, Andrea Arcangeli wrote:
<span class="quote">&gt; On Wed, Jul 26, 2017 at 07:45:33AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; Yes, exit_aio is the only blocking call I know of currently. But I would</span>
<span class="quote">&gt; &gt; like this to be as robust as possible and so I do not want to rely on</span>
<span class="quote">&gt; &gt; the current implementation. This can change in future and I can</span>
<span class="quote">&gt; &gt; guarantee that nobody will think about the oom path when adding</span>
<span class="quote">&gt; &gt; something to the final __mmput path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think ksm_exit may block too waiting for allocations, the generic</span>
<span class="quote">&gt; idea is those calls before exit_mmap can cause a problem yes.</span>

I thought that ksm used __GFP_NORETRY but haven&#39;t checked too deeply.
Anyway I guess we agree that enabling oom_reaper to race with the final
__mmput is desirable?

[...]
<span class="quote">&gt; &gt; This will work more or less the same to what we have currently.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [victim]		[oom reaper]				[oom killer]</span>
<span class="quote">&gt; &gt; do_exit			__oom_reap_task_mm</span>
<span class="quote">&gt; &gt;   mmput</span>
<span class="quote">&gt; &gt;     __mmput</span>
<span class="quote">&gt; &gt; 			  mmget_not_zero</span>
<span class="quote">&gt; &gt; 			    test_and_set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; &gt; 			    					oom_evaluate_task</span>
<span class="quote">&gt; &gt; 								   # select next victim </span>
<span class="quote">&gt; &gt; 			  # reap the mm</span>
<span class="quote">&gt; &gt;       unmap_vmas</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; so we can select a next victim while the current one is still not</span>
<span class="quote">&gt; &gt; completely torn down.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How does oom_evaluate_task possibly run at the same time of</span>
<span class="quote">&gt; test_and_set_bit in __oom_reap_task_mm considering both are running</span>
<span class="quote">&gt; under the oom_lock?</span>

You are absolutely right. This race is impossible. It was just me
assuming we are going to get rid of the oom_lock because I have that
idea in the back of my head and I would really like to get rid of
it. Global locks are nasty and I would prefer dropping it if we can.

[...]
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 9ec98b0c4675..ed412d85a596 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -910,7 +910,6 @@</span> <span class="p_context"> static inline void __mmput(struct mm_struct *mm)</span>
 	}
 	if (mm-&gt;binfmt)
 		module_put(mm-&gt;binfmt-&gt;module);
<span class="p_del">-	set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags);</span>
 	mmdrop(mm);
 }
 
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index f19efcf75418..615133762b99 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2993,6 +2993,11 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
<span class="p_add">+	if (test_and_set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags)) {</span>
<span class="p_add">+		/* wait the OOM reaper to stop working on this mm */</span>
<span class="p_add">+		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	}</span>
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f030c1c..2a7000995784 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -471,6 +471,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	bool ret = true;
<span class="p_add">+	bool mmgot = true;</span>
 
 	/*
 	 * We have to make sure to not race with the victim exit path
<span class="p_chunk">@@ -500,9 +501,16 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * and delayed __mmput doesn&#39;t matter that much
 	 */
 	if (!mmget_not_zero(mm)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * MMF_OOM_SKIP is set by exit_mmap when the OOM</span>
<span class="p_add">+		 * reaper can&#39;t work on the mm anymore.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (test_and_set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags)) {</span>
<span class="p_add">+			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			goto unlock_oom;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		mmgot = false;</span>
 	}
 
 	trace_start_task_reaping(tsk-&gt;pid);
<span class="p_chunk">@@ -547,7 +555,8 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * different context because we shouldn&#39;t risk we get stuck there and
 	 * put the oom_reaper out of the way.
 	 */
<span class="p_del">-	mmput_async(mm);</span>
<span class="p_add">+	if (mmgot)</span>
<span class="p_add">+		mmput_async(mm);</span>
 	trace_finish_task_reaping(tsk-&gt;pid);
 unlock_oom:
 	mutex_unlock(&amp;oom_lock);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



