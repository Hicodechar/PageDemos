
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v7,11/24] mm: Cache some VMA fields in the vm_fault structure - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v7,11/24] mm: Cache some VMA fields in the vm_fault structure</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 6, 2018, 4:49 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1517935810-31177-12-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10203417/mbox/"
   >mbox</a>
|
   <a href="/patch/10203417/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10203417/">/patch/10203417/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BE93560247 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Feb 2018 16:52:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A09EB28BE0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Feb 2018 16:52:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 94F1228D0C; Tue,  6 Feb 2018 16:52:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D02D028BE0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Feb 2018 16:52:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753029AbeBFQwP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 6 Feb 2018 11:52:15 -0500
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:41276 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1752890AbeBFQur (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 6 Feb 2018 11:50:47 -0500
Received: from pps.filterd (m0098416.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.22/8.16.0.22) with SMTP id
	w16Gn5V3122615
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 6 Feb 2018 11:50:46 -0500
Received: from e06smtp10.uk.ibm.com (e06smtp10.uk.ibm.com [195.75.94.106])
	by mx0b-001b2d01.pphosted.com with ESMTP id 2fye1ypvj3-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 06 Feb 2018 11:50:45 -0500
Received: from localhost
	by e06smtp10.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Tue, 6 Feb 2018 16:50:43 -0000
Received: from b06cxnps3075.portsmouth.uk.ibm.com (9.149.109.195)
	by e06smtp10.uk.ibm.com (192.168.101.140) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Tue, 6 Feb 2018 16:50:35 -0000
Received: from d06av25.portsmouth.uk.ibm.com (d06av25.portsmouth.uk.ibm.com
	[9.149.105.61])
	by b06cxnps3075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id w16GoYE266060334; Tue, 6 Feb 2018 16:50:34 GMT
Received: from d06av25.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 15FC811C054;
	Tue,  6 Feb 2018 16:43:58 +0000 (GMT)
Received: from d06av25.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 1844A11C05C;
	Tue,  6 Feb 2018 16:43:57 +0000 (GMT)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.101.4.33])
	by d06av25.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Tue,  6 Feb 2018 16:43:57 +0000 (GMT)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: paulmck@linux.vnet.ibm.com, peterz@infradead.org,
	akpm@linux-foundation.org, kirill@shutemov.name,
	ak@linux.intel.com, mhocko@kernel.org, dave@stgolabs.net,
	jack@suse.cz, Matthew Wilcox &lt;willy@infradead.org&gt;,
	benh@kernel.crashing.org, mpe@ellerman.id.au, paulus@samba.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, hpa@zytor.com,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Alexei Starovoitov &lt;alexei.starovoitov@gmail.com&gt;,
	kemi.wang@intel.com, sergey.senozhatsky.work@gmail.com,
	Daniel Jordan &lt;daniel.m.jordan@oracle.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	haren@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	npiggin@gmail.com, bsingharora@gmail.com,
	Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org
Subject: [PATCH v7 11/24] mm: Cache some VMA fields in the vm_fault structure
Date: Tue,  6 Feb 2018 17:49:57 +0100
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1517935810-31177-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
References: &lt;1517935810-31177-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 18020616-0040-0000-0000-0000040CFC94
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 18020616-0041-0000-0000-00002610AB97
Message-Id: &lt;1517935810-31177-12-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2018-02-06_07:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	priorityscore=1501
	malwarescore=0 suspectscore=2 phishscore=0 bulkscore=0 spamscore=0
	clxscore=1015 lowpriorityscore=0 impostorscore=0 adultscore=0
	classifier=spam adjust=0 reason=mlx scancount=1
	engine=8.0.1-1709140000
	definitions=main-1802060212
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Feb. 6, 2018, 4:49 p.m.</div>
<pre class="content">
When handling speculative page fault, the vma-&gt;vm_flags and
vma-&gt;vm_page_prot fields are read once the page table lock is released. So
there is no more guarantee that these fields would not change in our back.
They will be saved in the vm_fault structure before the VMA is checked for
changes.

This patch also set the fields in hugetlb_no_page() and
__collapse_huge_page_swapin even if it is not need for the callee.
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 include/linux/mm.h |  6 ++++++
 mm/hugetlb.c       |  2 ++
 mm/khugepaged.c    |  2 ++
 mm/memory.c        | 38 ++++++++++++++++++++------------------
 4 files changed, 30 insertions(+), 18 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index bb771afa59a2..c034f478b73d 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -361,6 +361,12 @@</span> <span class="p_context"> struct vm_fault {</span>
 					 * page table to avoid allocation from
 					 * atomic context.
 					 */
<span class="p_add">+	/*</span>
<span class="p_add">+	 * These entries are required when handling speculative page fault.</span>
<span class="p_add">+	 * This way the page handling is done using consistent field values.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned long vma_flags;</span>
<span class="p_add">+	pgprot_t vma_page_prot;</span>
 };
 
 /* page entry size for vm-&gt;huge_fault() */
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 7c204e3d132b..22a818c7a6de 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3716,6 +3716,8 @@</span> <span class="p_context"> static int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 				.vma = vma,
 				.address = address,
 				.flags = flags,
<span class="p_add">+				.vma_flags = vma-&gt;vm_flags,</span>
<span class="p_add">+				.vma_page_prot = vma-&gt;vm_page_prot,</span>
 				/*
 				 * Hard to debug if it ends up being
 				 * used by a callee that assumes
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index 32314e9e48dd..a946d5306160 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -882,6 +882,8 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 		.flags = FAULT_FLAG_ALLOW_RETRY,
 		.pmd = pmd,
 		.pgoff = linear_page_index(vma, address),
<span class="p_add">+		.vma_flags = vma-&gt;vm_flags,</span>
<span class="p_add">+		.vma_page_prot = vma-&gt;vm_page_prot,</span>
 	};
 
 	/* we only decide to swapin, if there is enough young ptes */
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 9661b1302645..1d6b9f91f2a6 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2609,7 +2609,7 @@</span> <span class="p_context"> static int wp_page_copy(struct vm_fault *vmf)</span>
 		 * Don&#39;t let another task, with possibly unlocked vma,
 		 * keep the mlocked page.
 		 */
<span class="p_del">-		if (page_copied &amp;&amp; (vma-&gt;vm_flags &amp; VM_LOCKED)) {</span>
<span class="p_add">+		if (page_copied &amp;&amp; (vmf-&gt;vma_flags &amp; VM_LOCKED)) {</span>
 			lock_page(old_page);	/* LRU manipulation */
 			if (PageMlocked(old_page))
 				munlock_vma_page(old_page);
<span class="p_chunk">@@ -2643,7 +2643,7 @@</span> <span class="p_context"> static int wp_page_copy(struct vm_fault *vmf)</span>
  */
 int finish_mkwrite_fault(struct vm_fault *vmf)
 {
<span class="p_del">-	WARN_ON_ONCE(!(vmf-&gt;vma-&gt;vm_flags &amp; VM_SHARED));</span>
<span class="p_add">+	WARN_ON_ONCE(!(vmf-&gt;vma_flags &amp; VM_SHARED));</span>
 	if (!pte_map_lock(vmf))
 		return VM_FAULT_RETRY;
 	/*
<span class="p_chunk">@@ -2745,7 +2745,7 @@</span> <span class="p_context"> static int do_wp_page(struct vm_fault *vmf)</span>
 		 * We should not cow pages in a shared writeable mapping.
 		 * Just mark the pages writable and/or call ops-&gt;pfn_mkwrite.
 		 */
<span class="p_del">-		if ((vma-&gt;vm_flags &amp; (VM_WRITE|VM_SHARED)) ==</span>
<span class="p_add">+		if ((vmf-&gt;vma_flags &amp; (VM_WRITE|VM_SHARED)) ==</span>
 				     (VM_WRITE|VM_SHARED))
 			return wp_pfn_shared(vmf);
 
<span class="p_chunk">@@ -2792,7 +2792,7 @@</span> <span class="p_context"> static int do_wp_page(struct vm_fault *vmf)</span>
 			return VM_FAULT_WRITE;
 		}
 		unlock_page(vmf-&gt;page);
<span class="p_del">-	} else if (unlikely((vma-&gt;vm_flags &amp; (VM_WRITE|VM_SHARED)) ==</span>
<span class="p_add">+	} else if (unlikely((vmf-&gt;vma_flags &amp; (VM_WRITE|VM_SHARED)) ==</span>
 					(VM_WRITE|VM_SHARED))) {
 		return wp_page_shared(vmf);
 	}
<span class="p_chunk">@@ -3079,7 +3079,7 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 
 	inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);
 	dec_mm_counter_fast(vma-&gt;vm_mm, MM_SWAPENTS);
<span class="p_del">-	pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="p_add">+	pte = mk_pte(page, vmf-&gt;vma_page_prot);</span>
 	if ((vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp; reuse_swap_page(page, NULL)) {
 		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
 		vmf-&gt;flags &amp;= ~FAULT_FLAG_WRITE;
<span class="p_chunk">@@ -3105,7 +3105,7 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 
 	swap_free(entry);
 	if (mem_cgroup_swap_full(page) ||
<span class="p_del">-	    (vma-&gt;vm_flags &amp; VM_LOCKED) || PageMlocked(page))</span>
<span class="p_add">+	    (vmf-&gt;vma_flags &amp; VM_LOCKED) || PageMlocked(page))</span>
 		try_to_free_swap(page);
 	unlock_page(page);
 	if (page != swapcache &amp;&amp; swapcache) {
<span class="p_chunk">@@ -3162,7 +3162,7 @@</span> <span class="p_context"> static int do_anonymous_page(struct vm_fault *vmf)</span>
 	pte_t entry;
 
 	/* File mapping without -&gt;vm_ops ? */
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_SHARED)</span>
<span class="p_add">+	if (vmf-&gt;vma_flags &amp; VM_SHARED)</span>
 		return VM_FAULT_SIGBUS;
 
 	/*
<span class="p_chunk">@@ -3186,7 +3186,7 @@</span> <span class="p_context"> static int do_anonymous_page(struct vm_fault *vmf)</span>
 	if (!(vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp;
 			!mm_forbids_zeropage(vma-&gt;vm_mm)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf-&gt;address),
<span class="p_del">-						vma-&gt;vm_page_prot));</span>
<span class="p_add">+						vmf-&gt;vma_page_prot));</span>
 		if (!pte_map_lock(vmf))
 			return VM_FAULT_RETRY;
 		if (!pte_none(*vmf-&gt;pte))
<span class="p_chunk">@@ -3219,8 +3219,8 @@</span> <span class="p_context"> static int do_anonymous_page(struct vm_fault *vmf)</span>
 	 */
 	__SetPageUptodate(page);
 
<span class="p_del">-	entry = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_WRITE)</span>
<span class="p_add">+	entry = mk_pte(page, vmf-&gt;vma_page_prot);</span>
<span class="p_add">+	if (vmf-&gt;vma_flags &amp; VM_WRITE)</span>
 		entry = pte_mkwrite(pte_mkdirty(entry));
 
 	if (!pte_map_lock(vmf)) {
<span class="p_chunk">@@ -3416,7 +3416,7 @@</span> <span class="p_context"> static int do_set_pmd(struct vm_fault *vmf, struct page *page)</span>
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++)
 		flush_icache_page(vma, page + i);
 
<span class="p_del">-	entry = mk_huge_pmd(page, vma-&gt;vm_page_prot);</span>
<span class="p_add">+	entry = mk_huge_pmd(page, vmf-&gt;vma_page_prot);</span>
 	if (write)
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 
<span class="p_chunk">@@ -3490,11 +3490,11 @@</span> <span class="p_context"> int alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,</span>
 		return VM_FAULT_NOPAGE;
 
 	flush_icache_page(vma, page);
<span class="p_del">-	entry = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="p_add">+	entry = mk_pte(page, vmf-&gt;vma_page_prot);</span>
 	if (write)
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	/* copy-on-write page */
<span class="p_del">-	if (write &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="p_add">+	if (write &amp;&amp; !(vmf-&gt;vma_flags &amp; VM_SHARED)) {</span>
 		inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);
 		page_add_new_anon_rmap(page, vma, vmf-&gt;address, false);
 		mem_cgroup_commit_charge(page, memcg, false, false);
<span class="p_chunk">@@ -3533,7 +3533,7 @@</span> <span class="p_context"> int finish_fault(struct vm_fault *vmf)</span>
 
 	/* Did we COW the page? */
 	if ((vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp;
<span class="p_del">-	    !(vmf-&gt;vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+	    !(vmf-&gt;vma_flags &amp; VM_SHARED))</span>
 		page = vmf-&gt;cow_page;
 	else
 		page = vmf-&gt;page;
<span class="p_chunk">@@ -3787,7 +3787,7 @@</span> <span class="p_context"> static int do_fault(struct vm_fault *vmf)</span>
 		ret = VM_FAULT_SIGBUS;
 	else if (!(vmf-&gt;flags &amp; FAULT_FLAG_WRITE))
 		ret = do_read_fault(vmf);
<span class="p_del">-	else if (!(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+	else if (!(vmf-&gt;vma_flags &amp; VM_SHARED))</span>
 		ret = do_cow_fault(vmf);
 	else
 		ret = do_shared_fault(vmf);
<span class="p_chunk">@@ -3844,7 +3844,7 @@</span> <span class="p_context"> static int do_numa_page(struct vm_fault *vmf)</span>
 	 * accessible ptes, some can allow access by kernel mode.
 	 */
 	pte = ptep_modify_prot_start(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte);
<span class="p_del">-	pte = pte_modify(pte, vma-&gt;vm_page_prot);</span>
<span class="p_add">+	pte = pte_modify(pte, vmf-&gt;vma_page_prot);</span>
 	pte = pte_mkyoung(pte);
 	if (was_writable)
 		pte = pte_mkwrite(pte);
<span class="p_chunk">@@ -3878,7 +3878,7 @@</span> <span class="p_context"> static int do_numa_page(struct vm_fault *vmf)</span>
 	 * Flag if the page is shared between multiple address spaces. This
 	 * is later used when determining whether to group tasks together
 	 */
<span class="p_del">-	if (page_mapcount(page) &gt; 1 &amp;&amp; (vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+	if (page_mapcount(page) &gt; 1 &amp;&amp; (vmf-&gt;vma_flags &amp; VM_SHARED))</span>
 		flags |= TNF_SHARED;
 
 	last_cpupid = page_cpupid_last(page);
<span class="p_chunk">@@ -3923,7 +3923,7 @@</span> <span class="p_context"> static inline int wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)</span>
 		return vmf-&gt;vma-&gt;vm_ops-&gt;huge_fault(vmf, PE_SIZE_PMD);
 
 	/* COW handled on pte level: split pmd */
<span class="p_del">-	VM_BUG_ON_VMA(vmf-&gt;vma-&gt;vm_flags &amp; VM_SHARED, vmf-&gt;vma);</span>
<span class="p_add">+	VM_BUG_ON_VMA(vmf-&gt;vma_flags &amp; VM_SHARED, vmf-&gt;vma);</span>
 	__split_huge_pmd(vmf-&gt;vma, vmf-&gt;pmd, vmf-&gt;address, false, NULL);
 
 	return VM_FAULT_FALLBACK;
<span class="p_chunk">@@ -4070,6 +4070,8 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		.flags = flags,
 		.pgoff = linear_page_index(vma, address),
 		.gfp_mask = __get_fault_gfp_mask(vma),
<span class="p_add">+		.vma_flags = vma-&gt;vm_flags,</span>
<span class="p_add">+		.vma_page_prot = vma-&gt;vm_page_prot,</span>
 	};
 	unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;
 	struct mm_struct *mm = vma-&gt;vm_mm;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



