
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC/PATCHv2,v2,2/4] dma-mapping: Add dma_remap() APIs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC/PATCHv2,v2,2/4] dma-mapping: Add dma_remap() APIs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=157651">Stephen Boyd</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 20, 2016, 1:04 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1461114269-13718-3-git-send-email-stephen.boyd@linaro.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8884211/mbox/"
   >mbox</a>
|
   <a href="/patch/8884211/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8884211/">/patch/8884211/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id A523FBF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 20 Apr 2016 01:05:40 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id DF844201BC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 20 Apr 2016 01:05:39 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0AFE4201F2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 20 Apr 2016 01:05:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753410AbcDTBEk (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 19 Apr 2016 21:04:40 -0400
Received: from mail-pa0-f41.google.com ([209.85.220.41]:34183 &quot;EHLO
	mail-pa0-f41.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750859AbcDTBEh (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 19 Apr 2016 21:04:37 -0400
Received: by mail-pa0-f41.google.com with SMTP id r5so10794460pag.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 19 Apr 2016 18:04:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=linaro.org; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=zOqML3R6UmsTUrTEr/gcWZpd3I8Gfqh7ovHS9hZ0GGI=;
	b=cDCq4MmAVdqaf2C+vfRJsAJQXRkaz/bUfBJV2sO1RrldRoOiN5KxsEc/dHFeOEzpHb
	5Sp9bni8W9+jXBkRflCRV49blRzTVisCu7gfiPXcxkOs7HVg6MbwS5gs7m5+blQFqnp1
	knh47VkSF/a8hWcCvEW1x9DyBrYUXdNhdx6aE=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=zOqML3R6UmsTUrTEr/gcWZpd3I8Gfqh7ovHS9hZ0GGI=;
	b=d9Mb+FXORN/npqG32wO3CsX451YG8byZqnbgLPCBeMjJf8YXvITcWcsoDAOwFv6rP1
	cp7Sr2hGnU7UVlXeeC05mCsYqGnIVPgrstF37RbeoEEjpcYKNBqnkfvPWBDCq63pn9jA
	00bEWbC2+hnXT7HXIq1Bz0rZyyzy022OCos1oZ7a8rNibmT83al/z5nJQ/pmmV8eFRI7
	8fDsMDsoEs0HZCB2RYta4PRlDpEisMl9MUra0YLtakfe04sWy/Tji9inDWSP01Ue2wmv
	A78UH16pmEdE9Ej0Cpn/CfnkL8EDwyv+jdIxJouD0FJI/dX4cSIyd054CFA4Dlvvj8iv
	kDFw==
X-Gm-Message-State: AOPr4FX/a1FiK14dxAaNtQFAJQcpx01vhRrgam0w+yN9hyHOpuCDTLrO3mJx46lfyS03RaeX
X-Received: by 10.66.156.232 with SMTP id wh8mr8367996pab.153.1461114276831; 
	Tue, 19 Apr 2016 18:04:36 -0700 (PDT)
Received: from localhost.localdomain (pat_11.qualcomm.com. [192.35.156.11])
	by smtp.gmail.com with ESMTPSA id
	ba9sm93738120pab.24.2016.04.19.18.04.35
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Tue, 19 Apr 2016 18:04:36 -0700 (PDT)
From: Stephen Boyd &lt;stephen.boyd@linaro.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Laura Abbott &lt;lauraa@codeaurora.org&gt;,
	linux-arm@lists.infradead.org, Robin Murphy &lt;robin.murphy@arm.com&gt;,
	Laura Abbott &lt;labbott@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Marek Szyprowski &lt;m.szyprowski@samsung.com&gt;,
	Mimi Zohar &lt;zohar@linux.vnet.ibm.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mark Brown &lt;broonie@kernel.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, Ming Lei &lt;ming.lei@canonical.com&gt;
Subject: [RFC/PATCHv2 v2 2/4] dma-mapping: Add dma_remap() APIs
Date: Tue, 19 Apr 2016 18:04:27 -0700
Message-Id: &lt;1461114269-13718-3-git-send-email-stephen.boyd@linaro.org&gt;
X-Mailer: git-send-email 2.8.0.rc4
In-Reply-To: &lt;1461114269-13718-1-git-send-email-stephen.boyd@linaro.org&gt;
References: &lt;1461114269-13718-1-git-send-email-stephen.boyd@linaro.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.0 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI, RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=157651">Stephen Boyd</a> - April 20, 2016, 1:04 a.m.</div>
<pre class="content">
<span class="from">From: Laura Abbott &lt;lauraa@codeaurora.org&gt;</span>

Some systems are memory constrained but they need to load very
large firmwares. The firmware subsystem allows drivers to request
this firmware be loaded from the filesystem, but this requires
that the entire firmware be loaded into kernel memory first
before it&#39;s provided to the driver. This can lead to a situation
where we map the firmware twice, once to load the firmware into
kernel memory and once to copy the firmware into the final
resting place.

This design creates needless memory pressure and delays loading
because we have to copy from kernel memory to somewhere else.
Let&#39;s add a couple DMA APIs that allow us to map DMA buffers into
the CPU&#39;s address space in arbitrary sizes. With this API, we can
allocate a DMA buffer with DMA_ATTR_NO_KERNEL_MAPPING and move a
small mapping window across our large DMA buffer to load the
firmware directly into buffer.
<span class="signed-off-by">
Signed-off-by: Laura Abbott &lt;lauraa@codeaurora.org&gt;</span>
[stephen.boyd@linaro.org: Add dma_attrs and offset to API, use
dma_common_contiguous_remap() instead of ioremap_page_range(),
support dma_remap() even when DMA_ATTR_NO_KERNEL_MAPPING isn&#39;t
specified, rewrite commit text]
<span class="signed-off-by">Signed-off-by: Stephen Boyd &lt;stephen.boyd@linaro.org&gt;</span>
---
 arch/arm64/mm/dma-mapping.c | 39 +++++++++++++++++++++++++++++++++++++++
 include/linux/dma-mapping.h | 35 +++++++++++++++++++++++++++++++++++
 2 files changed, 74 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - April 21, 2016, 10:35 a.m.</div>
<pre class="content">
Hi Stephen,

On Tue, Apr 19, 2016 at 06:04:27PM -0700, Stephen Boyd wrote:
<span class="quote">&gt; From: Laura Abbott &lt;lauraa@codeaurora.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Some systems are memory constrained but they need to load very</span>
<span class="quote">&gt; large firmwares. The firmware subsystem allows drivers to request</span>
<span class="quote">&gt; this firmware be loaded from the filesystem, but this requires</span>
<span class="quote">&gt; that the entire firmware be loaded into kernel memory first</span>
<span class="quote">&gt; before it&#39;s provided to the driver. This can lead to a situation</span>
<span class="quote">&gt; where we map the firmware twice, once to load the firmware into</span>
<span class="quote">&gt; kernel memory and once to copy the firmware into the final</span>
<span class="quote">&gt; resting place.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This design creates needless memory pressure and delays loading</span>
<span class="quote">&gt; because we have to copy from kernel memory to somewhere else.</span>
<span class="quote">&gt; Let&#39;s add a couple DMA APIs that allow us to map DMA buffers into</span>
<span class="quote">&gt; the CPU&#39;s address space in arbitrary sizes. With this API, we can</span>
<span class="quote">&gt; allocate a DMA buffer with DMA_ATTR_NO_KERNEL_MAPPING and move a</span>
<span class="quote">&gt; small mapping window across our large DMA buffer to load the</span>
<span class="quote">&gt; firmware directly into buffer.</span>

The first two patches in this series don&#39;t make sense to me. I don&#39;t
understand what the memory pressure is: physical or virtual? Because
they don&#39;t seem to address the former (the DMA buffer is allocated in
full) while the latter doesn&#39;t need any addressing at all on arm64, we
have plenty of VA space.

Why do you even need the coherent DMA API? Can you use the streaming API
(map_sg etc.) with a separately allocated buffer?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - April 27, 2016, 3:25 p.m.</div>
<pre class="content">
On Fri, Apr 22, 2016 at 05:35:16PM -0700, Stephen Boyd wrote:
<span class="quote">&gt; Quoting Catalin Marinas (2016-04-21 03:35:12)</span>
<span class="quote">&gt; &gt; On Tue, Apr 19, 2016 at 06:04:27PM -0700, Stephen Boyd wrote:</span>
<span class="quote">&gt; &gt; &gt; From: Laura Abbott &lt;lauraa@codeaurora.org&gt;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Some systems are memory constrained but they need to load very</span>
<span class="quote">&gt; &gt; &gt; large firmwares. The firmware subsystem allows drivers to request</span>
<span class="quote">&gt; &gt; &gt; this firmware be loaded from the filesystem, but this requires</span>
<span class="quote">&gt; &gt; &gt; that the entire firmware be loaded into kernel memory first</span>
<span class="quote">&gt; &gt; &gt; before it&#39;s provided to the driver. This can lead to a situation</span>
<span class="quote">&gt; &gt; &gt; where we map the firmware twice, once to load the firmware into</span>
<span class="quote">&gt; &gt; &gt; kernel memory and once to copy the firmware into the final</span>
<span class="quote">&gt; &gt; &gt; resting place.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This design creates needless memory pressure and delays loading</span>
<span class="quote">&gt; &gt; &gt; because we have to copy from kernel memory to somewhere else.</span>
<span class="quote">&gt; &gt; &gt; Let&#39;s add a couple DMA APIs that allow us to map DMA buffers into</span>
<span class="quote">&gt; &gt; &gt; the CPU&#39;s address space in arbitrary sizes. With this API, we can</span>
<span class="quote">&gt; &gt; &gt; allocate a DMA buffer with DMA_ATTR_NO_KERNEL_MAPPING and move a</span>
<span class="quote">&gt; &gt; &gt; small mapping window across our large DMA buffer to load the</span>
<span class="quote">&gt; &gt; &gt; firmware directly into buffer.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The first two patches in this series don&#39;t make sense to me. I don&#39;t</span>
<span class="quote">&gt; &gt; understand what the memory pressure is: physical or virtual? Because</span>
<span class="quote">&gt; &gt; they don&#39;t seem to address the former (the DMA buffer is allocated in</span>
<span class="quote">&gt; &gt; full) while the latter doesn&#39;t need any addressing at all on arm64, we</span>
<span class="quote">&gt; &gt; have plenty of VA space.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Why do you even need the coherent DMA API? Can you use the streaming API</span>
<span class="quote">&gt; &gt; (map_sg etc.) with a separately allocated buffer?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm I guess I need to add in the patches that show how this is used on</span>
<span class="quote">&gt; top of &quot;no-map&quot; DT reserved memory regions. There are some more patches</span>
<span class="quote">&gt; that allow us to assigned reserved memory regions with the &quot;no-map&quot;</span>
<span class="quote">&gt; attribute to devices and then allocate from those regions using the</span>
<span class="quote">&gt; coherent DMA APIs. In the downstream kernel it&#39;s called a removed dma</span>
<span class="quote">&gt; pool[1].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the plan is to wire that all up so that the device can have a</span>
<span class="quote">&gt; reserved chunk of memory for the firmware that doesn&#39;t exist in the</span>
<span class="quote">&gt; kernel&#39;s linear memory mappings. Once we have allocated the region, we</span>
<span class="quote">&gt; can map it into the kernel&#39;s view of memory for a short time so that we</span>
<span class="quote">&gt; can load the firmware into it (dma_remap part). Once that&#39;s over, we</span>
<span class="quote">&gt; want to destroy the mapping so that we 1) don&#39;t use any of the kernel&#39;s</span>
<span class="quote">&gt; virtual memory space (dma_unremap part) to back the buffer and 2) so</span>
<span class="quote">&gt; that the secure world can protect the memory from the non-secure world.</span>

Does the firmware already know about such memory? If yes, I presume the
kernel would have to be told about it and won&#39;t try to map it in the
linear mapping.

At this point, wouldn&#39;t a combination of:

	dma_declare_coherent_memory()
	dma_alloc_from_coherent()
	dma_release_from_coherent()
	dma_release_declared_memory()

work? The removed_alloc() implementation in the link you posted doesn&#39;t
seem far from dma_alloc_from_coherent(). The releasing of the declared
memory above would unmap the memory, so there are no VA mappings left.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - April 27, 2016, 6:16 p.m.</div>
<pre class="content">
On 04/27/2016 08:25 AM, Catalin Marinas wrote:
<span class="quote">&gt; On Fri, Apr 22, 2016 at 05:35:16PM -0700, Stephen Boyd wrote:</span>
<span class="quote">&gt;&gt; Quoting Catalin Marinas (2016-04-21 03:35:12)</span>
<span class="quote">&gt;&gt;&gt; On Tue, Apr 19, 2016 at 06:04:27PM -0700, Stephen Boyd wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; From: Laura Abbott &lt;lauraa@codeaurora.org&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Some systems are memory constrained but they need to load very</span>
<span class="quote">&gt;&gt;&gt;&gt; large firmwares. The firmware subsystem allows drivers to request</span>
<span class="quote">&gt;&gt;&gt;&gt; this firmware be loaded from the filesystem, but this requires</span>
<span class="quote">&gt;&gt;&gt;&gt; that the entire firmware be loaded into kernel memory first</span>
<span class="quote">&gt;&gt;&gt;&gt; before it&#39;s provided to the driver. This can lead to a situation</span>
<span class="quote">&gt;&gt;&gt;&gt; where we map the firmware twice, once to load the firmware into</span>
<span class="quote">&gt;&gt;&gt;&gt; kernel memory and once to copy the firmware into the final</span>
<span class="quote">&gt;&gt;&gt;&gt; resting place.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This design creates needless memory pressure and delays loading</span>
<span class="quote">&gt;&gt;&gt;&gt; because we have to copy from kernel memory to somewhere else.</span>
<span class="quote">&gt;&gt;&gt;&gt; Let&#39;s add a couple DMA APIs that allow us to map DMA buffers into</span>
<span class="quote">&gt;&gt;&gt;&gt; the CPU&#39;s address space in arbitrary sizes. With this API, we can</span>
<span class="quote">&gt;&gt;&gt;&gt; allocate a DMA buffer with DMA_ATTR_NO_KERNEL_MAPPING and move a</span>
<span class="quote">&gt;&gt;&gt;&gt; small mapping window across our large DMA buffer to load the</span>
<span class="quote">&gt;&gt;&gt;&gt; firmware directly into buffer.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The first two patches in this series don&#39;t make sense to me. I don&#39;t</span>
<span class="quote">&gt;&gt;&gt; understand what the memory pressure is: physical or virtual? Because</span>
<span class="quote">&gt;&gt;&gt; they don&#39;t seem to address the former (the DMA buffer is allocated in</span>
<span class="quote">&gt;&gt;&gt; full) while the latter doesn&#39;t need any addressing at all on arm64, we</span>
<span class="quote">&gt;&gt;&gt; have plenty of VA space.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Why do you even need the coherent DMA API? Can you use the streaming API</span>
<span class="quote">&gt;&gt;&gt; (map_sg etc.) with a separately allocated buffer?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hmm I guess I need to add in the patches that show how this is used on</span>
<span class="quote">&gt;&gt; top of &quot;no-map&quot; DT reserved memory regions. There are some more patches</span>
<span class="quote">&gt;&gt; that allow us to assigned reserved memory regions with the &quot;no-map&quot;</span>
<span class="quote">&gt;&gt; attribute to devices and then allocate from those regions using the</span>
<span class="quote">&gt;&gt; coherent DMA APIs. In the downstream kernel it&#39;s called a removed dma</span>
<span class="quote">&gt;&gt; pool[1].</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So the plan is to wire that all up so that the device can have a</span>
<span class="quote">&gt;&gt; reserved chunk of memory for the firmware that doesn&#39;t exist in the</span>
<span class="quote">&gt;&gt; kernel&#39;s linear memory mappings. Once we have allocated the region, we</span>
<span class="quote">&gt;&gt; can map it into the kernel&#39;s view of memory for a short time so that we</span>
<span class="quote">&gt;&gt; can load the firmware into it (dma_remap part). Once that&#39;s over, we</span>
<span class="quote">&gt;&gt; want to destroy the mapping so that we 1) don&#39;t use any of the kernel&#39;s</span>
<span class="quote">&gt;&gt; virtual memory space (dma_unremap part) to back the buffer and 2) so</span>
<span class="quote">&gt;&gt; that the secure world can protect the memory from the non-secure world.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Does the firmware already know about such memory? If yes, I presume the</span>
<span class="quote">&gt; kernel would have to be told about it and won&#39;t try to map it in the</span>
<span class="quote">&gt; linear mapping.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; At this point, wouldn&#39;t a combination of:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	dma_declare_coherent_memory()</span>
<span class="quote">&gt; 	dma_alloc_from_coherent()</span>
<span class="quote">&gt; 	dma_release_from_coherent()</span>
<span class="quote">&gt; 	dma_release_declared_memory()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; work? The removed_alloc() implementation in the link you posted doesn&#39;t</span>
<span class="quote">&gt; seem far from dma_alloc_from_coherent(). The releasing of the declared</span>
<span class="quote">&gt; memory above would unmap the memory, so there are no VA mappings left.</span>
<span class="quote">&gt;</span>

The removed alloc was specifically written as a fork of the coherent
pool. This was a choice for ease of out of tree maintenance. The better
choice here would be to fold those features back into dma-coherent.c
if needed.

Thanks,
Laura
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index 9686e722a047..c5816cfc155f 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -345,6 +345,43 @@</span> <span class="p_context"> static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
 	return ret;
 }
 
<span class="p_add">+static void *arm64_dma_remap(struct device *dev, void *cpu_addr,</span>
<span class="p_add">+			     dma_addr_t handle, size_t size,</span>
<span class="p_add">+			     unsigned long offset, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = phys_to_page(dma_to_phys(dev, handle) + offset);</span>
<span class="p_add">+	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_add">+	pgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL, coherent);</span>
<span class="p_add">+	void *ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs)) {</span>
<span class="p_add">+		offset &amp;= ~PAGE_MASK;</span>
<span class="p_add">+		size = PAGE_ALIGN(size + offset);</span>
<span class="p_add">+</span>
<span class="p_add">+		ptr = dma_common_contiguous_remap(page, size, VM_USERMAP, prot,</span>
<span class="p_add">+						  NULL);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		ptr = cpu_addr;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!ptr)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ptr + offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void arm64_dma_unremap(struct device *dev, void *cpu_addr,</span>
<span class="p_add">+			      size_t size, unsigned long offset,</span>
<span class="p_add">+			      struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	offset &amp;= ~PAGE_MASK;</span>
<span class="p_add">+	cpu_addr -= offset;</span>
<span class="p_add">+</span>
<span class="p_add">+	vunmap(cpu_addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct dma_map_ops swiotlb_dma_ops = {
 	.alloc = __dma_alloc,
 	.free = __dma_free,
<span class="p_chunk">@@ -360,6 +397,8 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
 	.dma_supported = swiotlb_dma_supported,
 	.mapping_error = swiotlb_dma_mapping_error,
<span class="p_add">+	.remap = arm64_dma_remap,</span>
<span class="p_add">+	.unremap = arm64_dma_unremap,</span>
 };
 
 static int __init atomic_pool_init(void)
<span class="p_header">diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h</span>
<span class="p_header">index 9ea9aba28049..737c38a6151d 100644</span>
<span class="p_header">--- a/include/linux/dma-mapping.h</span>
<span class="p_header">+++ b/include/linux/dma-mapping.h</span>
<span class="p_chunk">@@ -64,6 +64,12 @@</span> <span class="p_context"> struct dma_map_ops {</span>
 	int (*mapping_error)(struct device *dev, dma_addr_t dma_addr);
 	int (*dma_supported)(struct device *dev, u64 mask);
 	int (*set_dma_mask)(struct device *dev, u64 mask);
<span class="p_add">+	void *(*remap)(struct device *dev, void *cpu_addr, dma_addr_t handle,</span>
<span class="p_add">+			size_t size, unsigned long offset,</span>
<span class="p_add">+			struct dma_attrs *attrs);</span>
<span class="p_add">+	void (*unremap)(struct device *dev, void *cpu_addr,</span>
<span class="p_add">+			size_t size, unsigned long offset,</span>
<span class="p_add">+			struct dma_attrs *attrs);</span>
 #ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK
 	u64 (*get_required_mask)(struct device *dev);
 #endif
<span class="p_chunk">@@ -467,6 +473,35 @@</span> <span class="p_context"> static inline int dma_set_mask(struct device *dev, u64 mask)</span>
 }
 #endif
 
<span class="p_add">+static inline void *dma_remap(struct device *dev, void *cpu_addr,</span>
<span class="p_add">+		dma_addr_t dma_handle, size_t size, unsigned long offset,</span>
<span class="p_add">+		struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct dma_map_ops *ops = get_dma_ops(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!ops)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	if (!ops-&gt;remap)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ops-&gt;remap(dev, cpu_addr, dma_handle, size, offset, attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void dma_unremap(struct device *dev, void *remapped_addr,</span>
<span class="p_add">+				size_t size, unsigned long offset,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct dma_map_ops *ops = get_dma_ops(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!ops)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (!ops-&gt;unremap)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ops-&gt;unremap(dev, remapped_addr, size, offset, attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline u64 dma_get_mask(struct device *dev)
 {
 	if (dev &amp;&amp; dev-&gt;dma_mask &amp;&amp; *dev-&gt;dma_mask)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



