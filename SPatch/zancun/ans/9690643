
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,4/4] Change mmap_sem to range lock - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,4/4] Change mmap_sem to range lock</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 20, 2017, 2:28 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1492698500-24219-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9690643/mbox/"
   >mbox</a>
|
   <a href="/patch/9690643/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9690643/">/patch/9690643/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	722BA600C8 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 14:28:54 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5723B28484
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 14:28:54 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 47C0A28487; Thu, 20 Apr 2017 14:28:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 71A4928484
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Apr 2017 14:28:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S946380AbdDTO2l (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 20 Apr 2017 10:28:41 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:59273 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S946339AbdDTO2c (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 20 Apr 2017 10:28:32 -0400
Received: from pps.filterd (m0098416.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v3KENvgF133988
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 20 Apr 2017 10:28:31 -0400
Received: from e06smtp11.uk.ibm.com (e06smtp11.uk.ibm.com [195.75.94.107])
	by mx0b-001b2d01.pphosted.com with ESMTP id 29xur4j0gv-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 20 Apr 2017 10:28:29 -0400
Received: from localhost
	by e06smtp11.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Thu, 20 Apr 2017 15:28:25 +0100
Received: from b06cxnps3075.portsmouth.uk.ibm.com (9.149.109.195)
	by e06smtp11.uk.ibm.com (192.168.101.141) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Thu, 20 Apr 2017 15:28:22 +0100
Received: from d06av24.portsmouth.uk.ibm.com (d06av24.portsmouth.uk.ibm.com
	[9.149.105.60])
	by b06cxnps3075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v3KESLV0590272; Thu, 20 Apr 2017 14:28:21 GMT
Received: from d06av24.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 9F32242042;
	Thu, 20 Apr 2017 15:27:20 +0100 (BST)
Received: from d06av24.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 02F1A42074;
	Thu, 20 Apr 2017 15:27:20 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.101.4.33])
	by d06av24.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Thu, 20 Apr 2017 15:27:19 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: linux-mm@kvack.org
Cc: Davidlohr Bueso &lt;dave@stgolabs.net&gt;, akpm@linux-foundation.org,
	Jan Kara &lt;jack@suse.cz&gt;, &quot;Kirill A . Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	haren@linux.vnet.ibm.com, aneesh.kumar@linux.vnet.ibm.com,
	khandual@linux.vnet.ibm.com, Paul.McKenney@us.ibm.com,
	linux-kernel@vger.kernel.org
Subject: [RFC 4/4] Change mmap_sem to range lock
Date: Thu, 20 Apr 2017 16:28:20 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
References: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17042014-0040-0000-0000-0000038D4B60
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17042014-0041-0000-0000-00001FC6F3EF
Message-Id: &lt;1492698500-24219-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-04-20_13:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=4
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1704200115
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - April 20, 2017, 2:28 p.m.</div>
<pre class="content">
[resent this patch which seems to have not reached the mailing lists]

Change the mmap_sem to a range lock to allow finer grain locking on
the memory layout of a task.

This patch rename mmap_sem into mmap_rw_tree to avoid confusion and
replace any locking (read or write) by complete range locking.  So
there is no functional change except in the way the underlying locking
is achieved.

Currently, this patch only supports x86 and PowerPc architectures,
furthermore it should break the build of any others.
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/kernel/vdso.c                         |  8 ++--
 arch/powerpc/kvm/book3s_64_mmu_hv.c                |  6 ++-
 arch/powerpc/kvm/book3s_64_mmu_radix.c             |  6 ++-
 arch/powerpc/kvm/book3s_64_vio.c                   |  6 ++-
 arch/powerpc/kvm/book3s_hv.c                       |  8 ++--
 arch/powerpc/kvm/e500_mmu_host.c                   |  7 +++-
 arch/powerpc/mm/copro_fault.c                      |  6 ++-
 arch/powerpc/mm/fault.c                            | 12 +++---
 arch/powerpc/mm/mmu_context_iommu.c                |  6 ++-
 arch/powerpc/mm/subpage-prot.c                     | 16 +++++---
 arch/powerpc/oprofile/cell/spu_task_sync.c         |  8 ++--
 arch/powerpc/platforms/cell/spufs/file.c           |  4 +-
 arch/x86/entry/vdso/vma.c                          | 14 ++++---
 arch/x86/kernel/tboot.c                            |  2 +-
 arch/x86/kernel/vm86_32.c                          |  6 ++-
 arch/x86/mm/fault.c                                | 39 +++++++++++--------
 arch/x86/mm/mpx.c                                  | 18 ++++++---
 drivers/android/binder.c                           |  8 ++--
 drivers/firmware/efi/arm-runtime.c                 |  2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c             |  9 +++--
 drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c            |  8 ++--
 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c             |  8 ++--
 drivers/gpu/drm/amd/amdkfd/kfd_events.c            |  6 ++-
 drivers/gpu/drm/amd/amdkfd/kfd_process.c           |  6 ++-
 drivers/gpu/drm/etnaviv/etnaviv_gem.c              |  6 ++-
 drivers/gpu/drm/i915/i915_gem.c                    |  6 ++-
 drivers/gpu/drm/i915/i915_gem_userptr.c            | 12 ++++--
 drivers/gpu/drm/radeon/radeon_cs.c                 |  9 +++--
 drivers/gpu/drm/radeon/radeon_gem.c                |  8 ++--
 drivers/gpu/drm/radeon/radeon_mn.c                 |  8 ++--
 drivers/gpu/drm/ttm/ttm_bo_vm.c                    |  6 ++-
 drivers/gpu/drm/via/via_dmablit.c                  |  6 ++-
 drivers/infiniband/core/umem.c                     | 20 ++++++----
 drivers/infiniband/core/umem_odp.c                 |  6 ++-
 drivers/infiniband/hw/hfi1/user_pages.c            | 18 ++++++---
 drivers/infiniband/hw/mlx4/main.c                  |  6 ++-
 drivers/infiniband/hw/mlx5/main.c                  |  6 ++-
 drivers/infiniband/hw/qib/qib_user_pages.c         | 16 +++++---
 drivers/infiniband/hw/usnic/usnic_uiom.c           | 20 ++++++----
 drivers/iommu/amd_iommu_v2.c                       |  8 ++--
 drivers/iommu/intel-svm.c                          |  6 ++-
 drivers/media/v4l2-core/videobuf-core.c            |  9 +++--
 drivers/media/v4l2-core/videobuf-dma-contig.c      |  6 ++-
 drivers/media/v4l2-core/videobuf-dma-sg.c          |  6 ++-
 drivers/misc/cxl/fault.c                           |  6 ++-
 drivers/misc/mic/scif/scif_rma.c                   | 17 +++++---
 drivers/oprofile/buffer_sync.c                     | 14 ++++---
 drivers/staging/lustre/lustre/llite/llite_mmap.c   |  4 +-
 drivers/staging/lustre/lustre/llite/vvp_io.c       |  6 ++-
 .../interface/vchiq_arm/vchiq_2835_arm.c           |  7 +++-
 .../vc04_services/interface/vchiq_arm/vchiq_arm.c  |  6 ++-
 drivers/vfio/vfio_iommu_spapr_tce.c                | 13 +++++--
 drivers/vfio/vfio_iommu_type1.c                    | 24 +++++++-----
 drivers/virt/fsl_hypervisor.c                      |  6 ++-
 drivers/xen/gntdev.c                               |  6 ++-
 drivers/xen/privcmd.c                              | 14 ++++---
 fs/aio.c                                           |  7 +++-
 fs/coredump.c                                      |  6 ++-
 fs/exec.c                                          | 24 ++++++++----
 fs/proc/base.c                                     | 38 +++++++++++-------
 fs/proc/internal.h                                 |  1 +
 fs/proc/task_mmu.c                                 | 30 +++++++++------
 fs/proc/task_nommu.c                               | 27 ++++++++-----
 fs/userfaultfd.c                                   | 24 +++++++-----
 include/linux/mm_types.h                           |  3 +-
 ipc/shm.c                                          | 13 +++++--
 kernel/acct.c                                      |  6 ++-
 kernel/events/core.c                               |  6 ++-
 kernel/events/uprobes.c                            | 24 ++++++++----
 kernel/exit.c                                      | 10 +++--
 kernel/fork.c                                      | 21 ++++++----
 kernel/futex.c                                     |  8 ++--
 kernel/sched/fair.c                                |  7 ++--
 kernel/sys.c                                       | 31 ++++++++++-----
 kernel/trace/trace_output.c                        |  6 ++-
 mm/filemap.c                                       |  4 +-
 mm/frame_vector.c                                  |  9 +++--
 mm/gup.c                                           | 22 ++++++-----
 mm/init-mm.c                                       |  2 +-
 mm/khugepaged.c                                    | 44 +++++++++++++--------
 mm/ksm.c                                           | 45 ++++++++++++++--------
 mm/madvise.c                                       | 28 ++++++++------
 mm/memcontrol.c                                    | 14 ++++---
 mm/memory.c                                        | 21 +++++++---
 mm/mempolicy.c                                     | 30 +++++++++------
 mm/migrate.c                                       | 12 ++++--
 mm/mincore.c                                       |  6 ++-
 mm/mlock.c                                         | 25 ++++++++----
 mm/mmap.c                                          | 43 ++++++++++++++-------
 mm/mmu_notifier.c                                  |  6 ++-
 mm/mprotect.c                                      | 19 ++++++---
 mm/mremap.c                                        |  6 ++-
 mm/msync.c                                         | 10 +++--
 mm/nommu.c                                         | 31 ++++++++++-----
 mm/oom_kill.c                                      |  9 +++--
 mm/process_vm_access.c                             |  8 ++--
 mm/shmem.c                                         |  3 +-
 mm/swapfile.c                                      |  8 ++--
 mm/userfaultfd.c                                   | 19 +++++----
 mm/util.c                                          | 15 ++++++--
 virt/kvm/async_pf.c                                |  8 ++--
 virt/kvm/kvm_main.c                                | 21 ++++++----
 102 files changed, 838 insertions(+), 452 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - April 20, 2017, 2:37 p.m.</div>
<pre class="content">
On Thu, Apr 20, 2017 at 04:28:20PM +0200, Laurent Dufour wrote:
<span class="quote">&gt; [resent this patch which seems to have not reached the mailing lists]</span>

Probably because its too big at ~180k ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - April 20, 2017, 2:42 p.m.</div>
<pre class="content">
On 20/04/2017 16:37, Peter Zijlstra wrote:
<span class="quote">&gt; On Thu, Apr 20, 2017 at 04:28:20PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; [resent this patch which seems to have not reached the mailing lists]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Probably because its too big at ~180k ?</span>

Probably, but this time it has reached linux-mm ... at least.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - April 20, 2017, 11:36 p.m.</div>
<pre class="content">
Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt; writes:
<span class="quote">
&gt; [resent this patch which seems to have not reached the mailing lists]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Change the mmap_sem to a range lock to allow finer grain locking on</span>
<span class="quote">&gt; the memory layout of a task.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch rename mmap_sem into mmap_rw_tree to avoid confusion and</span>
<span class="quote">&gt; replace any locking (read or write) by complete range locking.  So</span>
<span class="quote">&gt; there is no functional change except in the way the underlying locking</span>
<span class="quote">&gt; is achieved.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Currently, this patch only supports x86 and PowerPc architectures,</span>
<span class="quote">&gt; furthermore it should break the build of any others.</span>

Thanks for working on this.

However as commented before I think the first step to make progress here
is a description of everything mmap_sem protects.

Surely the init full case could be done shorter with some wrapper
that combines the init_full and lock operation?

Then it would be likely a simple search&#39;n&#39;replace to move the
whole tree in one atomic step to the new wrappers.
Initially they could be just defined to use rwsems too to
not change anything at all.

It would be a good idea to merge such a patch as quickly
as possible beause it will be a nightmare to maintain
longer term.

Then you could add a config to use a range lock through
the wrappers.

Then after that you could add real ranges step by step,
after doing the proper analysis.

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - April 24, 2017, 3:47 p.m.</div>
<pre class="content">
On 21/04/2017 01:36, Andi Kleen wrote:
<span class="quote">&gt; Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; [resent this patch which seems to have not reached the mailing lists]</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Change the mmap_sem to a range lock to allow finer grain locking on</span>
<span class="quote">&gt;&gt; the memory layout of a task.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch rename mmap_sem into mmap_rw_tree to avoid confusion and</span>
<span class="quote">&gt;&gt; replace any locking (read or write) by complete range locking.  So</span>
<span class="quote">&gt;&gt; there is no functional change except in the way the underlying locking</span>
<span class="quote">&gt;&gt; is achieved.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Currently, this patch only supports x86 and PowerPc architectures,</span>
<span class="quote">&gt;&gt; furthermore it should break the build of any others.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for working on this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; However as commented before I think the first step to make progress here</span>
<span class="quote">&gt; is a description of everything mmap_sem protects.</span>

Hi Andy,

I looked for the write mmap_sem locking in x86 and ppc64 architectures,
here is what I found:

mmap_sem protects
 vdso mapping
 VMA layout changes
 VMA cache
 Page protection/layout
 Changes to mmu notifier chain
 mmap_sem is used to serialize khugepaged&#39;s access
 mmap_sem is used to serialize ksm&#39;s access
 protection keys (pkey_alloc()...)

Calls to
 get_unmap_area()
 do_mmap()
 do_mmap_pgoff()
 do_munmap()
 get_user_pages()
 put_page()
 set_page_dirty_lock()
 find_vma()
 find_vma_intersection()
 alloc_empty_pages()
 insert_vm_struct()
 get_mm_rss()
 uprobe_consumer-&gt;filter() (currently only uprobe_perf_filter())
 _install_special_mapping()
 pmdp_collapse_flush()
 do_swap_page()
 do_brk()
 __split_vma()
 mremap_to()
 vma_to_resize()
 vma_adjust()

MM fields
   pinned_vm
   stack_vm
   total_vm
   locked_vm
   start_stack
   start_code
   end_code
   start_data
   start_brk
   bd_addr
   mm_users
   core_state
   context.vdso_*
   def_flags
   mmu_notifier_mm

VMA fields
    vm_private_data
    vm_flags
    vm_page_prot
    vm_file
    vm_pgoff
    vm_policy


Userfaultfd has not been looked in details yet.
dup_mmap() locks the oldmm in write mode when copying it, is it necessary ?
<span class="quote">
&gt; Surely the init full case could be done shorter with some wrapper</span>
<span class="quote">&gt; that combines the init_full and lock operation?</span>

Yes that doable, I wrote this like that, because the range should be
initialized based on the on going operation, so having an explicit init
operation is making this more explicit.
<span class="quote">
&gt; Then it would be likely a simple search&#39;n&#39;replace to move the</span>
<span class="quote">&gt; whole tree in one atomic step to the new wrappers.</span>
<span class="quote">&gt; Initially they could be just defined to use rwsems too to</span>
<span class="quote">&gt; not change anything at all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be a good idea to merge such a patch as quickly</span>
<span class="quote">&gt; as possible beause it will be a nightmare to maintain</span>
<span class="quote">&gt; longer term.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then you could add a config to use a range lock through</span>
<span class="quote">&gt; the wrappers.</span>

I agree, I should try a way to make that patch activated through a
CONFIG_value, but there is a the additional range value that make it
more complex to achieve. I&#39;ll try to figure out a way to do that.
<span class="quote">
&gt; Then after that you could add real ranges step by step,</span>
<span class="quote">&gt; after doing the proper analysis.</span>

That&#39;s the biggest part of the job.
I&#39;m also wondering if a dedicated lock/sem should be introduced to
protect the VMA cache and the VMA list, since the range itself will not
protect against change while walking the VMA list.

Please advise.

Cheers,
Laurent.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/kernel/vdso.c b/arch/powerpc/kernel/vdso.c</span>
<span class="p_header">index 22b01a3962f0..3805c643de8c 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/vdso.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/vdso.c</span>
<span class="p_chunk">@@ -154,6 +154,7 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	struct page **vdso_pagelist;
 	unsigned long vdso_pages;
 	unsigned long vdso_base;
<span class="p_add">+	struct range_rwlock range;</span>
 	int rc;
 
 	if (!vdso_ready)
<span class="p_chunk">@@ -196,7 +197,8 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	 * and end up putting it elsewhere.
 	 * Add enough to the size so that the result can be aligned.
 	 */
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 	vdso_base = get_unmapped_area(NULL, vdso_base,
 				      (vdso_pages &lt;&lt; PAGE_SHIFT) +
<span class="p_chunk">@@ -236,11 +238,11 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 		goto fail_mmapsem;
 	}
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return 0;
 
  fail_mmapsem:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return rc;
 }
 
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index 710e491206ed..31026e3e11ec 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -485,6 +485,7 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	struct vm_area_struct *vma;
 	unsigned long rcbits;
 	long mmio_update;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (kvm_is_radix(kvm))
 		return kvmppc_book3s_radix_page_fault(run, vcpu, ea, dsisr);
<span class="p_chunk">@@ -568,7 +569,8 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	npages = get_user_pages_fast(hva, 1, writing, pages);
 	if (npages &lt; 1) {
 		/* Check if it&#39;s an I/O mapping */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; vma-&gt;vm_start &lt;= hva &amp;&amp; hva + psize &lt;= vma-&gt;vm_end &amp;&amp;
 		    (vma-&gt;vm_flags &amp; VM_PFNMAP)) {
<span class="p_chunk">@@ -578,7 +580,7 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 			is_ci = pte_ci(__pte((pgprot_val(vma-&gt;vm_page_prot))));
 			write_ok = vma-&gt;vm_flags &amp; VM_WRITE;
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (!pfn)
 			goto out_put;
 	} else {
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_radix.c b/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_header">index f6b3e67c5762..85c8a66bd45c 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_radix.c</span>
<span class="p_chunk">@@ -305,6 +305,7 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	pte_t pte, *ptep;
 	unsigned long pgflags;
 	unsigned int shift, level;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Check for unusual errors */
 	if (dsisr &amp; DSISR_UNSUPP_MMU) {
<span class="p_chunk">@@ -394,7 +395,8 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 	npages = get_user_pages_fast(hva, 1, writing, pages);
 	if (npages &lt; 1) {
 		/* Check if it&#39;s an I/O mapping */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; vma-&gt;vm_start &lt;= hva &amp;&amp; hva &lt; vma-&gt;vm_end &amp;&amp;
 		    (vma-&gt;vm_flags &amp; VM_PFNMAP)) {
<span class="p_chunk">@@ -402,7 +404,7 @@</span> <span class="p_context"> int kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 				((hva - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 			pgflags = pgprot_val(vma-&gt;vm_page_prot);
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (!pfn)
 			return -EFAULT;
 	} else {
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_vio.c b/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_header">index 3e26cd4979f9..3199d072ddd3 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_vio.c</span>
<span class="p_chunk">@@ -57,11 +57,13 @@</span> <span class="p_context"> static unsigned long kvmppc_stt_pages(unsigned long tce_pages)</span>
 static long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)
 {
 	long ret = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!current || !current-&gt;mm)
 		return ret; /* process exited */
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (inc) {
 		unsigned long locked, lock_limit;
<span class="p_chunk">@@ -86,7 +88,7 @@</span> <span class="p_context"> static long kvmppc_account_memlimit(unsigned long stt_pages, bool inc)</span>
 			rlimit(RLIMIT_MEMLOCK),
 			ret ? &quot; - exceeded&quot; : &quot;&quot;);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">index 1ec86d9e2a82..998b800b1ea8 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_chunk">@@ -3192,6 +3192,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	unsigned long lpcr = 0, senc;
 	unsigned long psize, porder;
 	int srcu_idx;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	mutex_lock(&amp;kvm-&gt;lock);
 	if (kvm-&gt;arch.hpte_setup_done)
<span class="p_chunk">@@ -3228,7 +3229,8 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 
 	/* Look up the VMA for the start of this memory slot */
 	hva = memslot-&gt;userspace_addr;
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, hva);
 	if (!vma || vma-&gt;vm_start &gt; hva || (vma-&gt;vm_flags &amp; VM_IO))
 		goto up_out;
<span class="p_chunk">@@ -3236,7 +3238,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	psize = vma_kernel_pagesize(vma);
 	porder = __ilog2(psize);
 
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/* We can handle 4k, 64k or 16M pages in the VRMA */
 	err = -EINVAL;
<span class="p_chunk">@@ -3270,7 +3272,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 	return err;
 
  up_out:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	goto out_srcu;
 }
 
<span class="p_header">diff --git a/arch/powerpc/kvm/e500_mmu_host.c b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">index 0fda4230f6c0..e50456aaf86c 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/e500_mmu_host.c</span>
<span class="p_chunk">@@ -357,7 +357,10 @@</span> <span class="p_context"> static inline int kvmppc_e500_shadow_map(struct kvmppc_vcpu_e500 *vcpu_e500,</span>
 
 	if (tlbsel == 1) {
 		struct vm_area_struct *vma;
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		vma = find_vma(current-&gt;mm, hva);
 		if (vma &amp;&amp; hva &gt;= vma-&gt;vm_start &amp;&amp;
<span class="p_chunk">@@ -443,7 +446,7 @@</span> <span class="p_context"> static inline int kvmppc_e500_shadow_map(struct kvmppc_vcpu_e500 *vcpu_e500,</span>
 			tsize = max(BOOK3E_PAGESZ_4K, tsize &amp; ~1);
 		}
 
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	if (likely(!pfnmap)) {
<span class="p_header">diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">index 81fbf79d2e97..386e9b614f4c 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 		unsigned long dsisr, unsigned *flt)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long is_write;
 	int ret;
 
<span class="p_chunk">@@ -46,7 +47,8 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	if (mm-&gt;pgd == NULL)
 		return -EFAULT;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = -EFAULT;
 	vma = find_vma(mm, ea);
 	if (!vma)
<span class="p_chunk">@@ -95,7 +97,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 		current-&gt;min_flt++;
 
 out_unlock:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL_GPL(copro_handle_mm_fault);
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index 20f470486177..9cd547e97f65 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -208,6 +208,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
  	int is_exec = trap == 0x400;
 	int fault;
 	int rc = 0, store_update_sp = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 #if !(defined(CONFIG_4xx) || defined(CONFIG_BOOKE))
 	/*
<span class="p_chunk">@@ -308,12 +309,13 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * source.  If this is invalid we can skip the address space check,
 	 * thus avoiding the deadlock.
 	 */
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX finer grain required here */</span>
<span class="p_add">+	if (!range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		if (!user_mode(regs) &amp;&amp; !search_exception_tables(regs-&gt;nip))
 			goto bad_area_nosemaphore;
 
 retry:
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	} else {
 		/*
 		 * The above down_read_trylock() might have succeeded in
<span class="p_chunk">@@ -446,7 +448,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags, NULL);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;range);</span>
 
 	/*
 	 * Handle the retry right now, the mmap_sem has been released in that
<span class="p_chunk">@@ -466,7 +468,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 		}
 		/* We will enter mm_fault_error() below */
 	} else
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (unlikely(fault &amp; (VM_FAULT_RETRY|VM_FAULT_ERROR))) {
 		if (fault &amp; VM_FAULT_SIGSEGV)
<span class="p_chunk">@@ -505,7 +507,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	goto bail;
 
 bad_area:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 bad_area_nosemaphore:
 	/* User mode accesses cause a SIGSEGV */
<span class="p_header">diff --git a/arch/powerpc/mm/mmu_context_iommu.c b/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_header">index 497130c5c742..c9c89b6f559a 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/mmu_context_iommu.c</span>
<span class="p_chunk">@@ -36,11 +36,13 @@</span> <span class="p_context"> static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,</span>
 		unsigned long npages, bool incr)
 {
 	long ret = 0, locked, lock_limit;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!npages)
 		return 0;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (incr) {
 		locked = mm-&gt;locked_vm + npages;
<span class="p_chunk">@@ -61,7 +63,7 @@</span> <span class="p_context"> static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,</span>
 			npages &lt;&lt; PAGE_SHIFT,
 			mm-&gt;locked_vm &lt;&lt; PAGE_SHIFT,
 			rlimit(RLIMIT_MEMLOCK));
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/arch/powerpc/mm/subpage-prot.c b/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_header">index 94210940112f..3eeb81767581 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/subpage-prot.c</span>
<span class="p_chunk">@@ -98,8 +98,10 @@</span> <span class="p_context"> static void subpage_prot_clear(unsigned long addr, unsigned long len)</span>
 	unsigned long i;
 	size_t nw;
 	unsigned long next, limit;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	limit = addr + len;
 	if (limit &gt; spt-&gt;maxaddr)
 		limit = spt-&gt;maxaddr;
<span class="p_chunk">@@ -127,7 +129,7 @@</span> <span class="p_context"> static void subpage_prot_clear(unsigned long addr, unsigned long len)</span>
 		/* now flush any existing HPTEs for the range */
 		hpte_flush_range(mm, addr, nw);
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_chunk">@@ -194,6 +196,7 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 	size_t nw;
 	unsigned long next, limit;
 	int err;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Check parameters */
 	if ((addr &amp; ~PAGE_MASK) || (len &amp; ~PAGE_MASK) ||
<span class="p_chunk">@@ -212,7 +215,8 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 	if (!access_ok(VERIFY_READ, map, (len &gt;&gt; PAGE_SHIFT) * sizeof(u32)))
 		return -EFAULT;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	subpage_mark_vma_nohuge(mm, addr, len);
 	for (limit = addr + len; addr &lt; limit; addr = next) {
 		next = pmd_addr_end(addr, limit);
<span class="p_chunk">@@ -247,11 +251,11 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 		if (addr + (nw &lt;&lt; PAGE_SHIFT) &gt; next)
 			nw = (next - addr) &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (__copy_from_user(spp, map, nw * sizeof(u32)))
 			return -EFAULT;
 		map += nw;
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		/* now flush any existing HPTEs for the range */
 		hpte_flush_range(mm, addr, nw);
<span class="p_chunk">@@ -260,6 +264,6 @@</span> <span class="p_context"> long sys_subpage_prot(unsigned long addr, unsigned long len, u32 __user *map)</span>
 		spt-&gt;maxaddr = limit;
 	err = 0;
  out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return err;
 }
<span class="p_header">diff --git a/arch/powerpc/oprofile/cell/spu_task_sync.c b/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_header">index 44d67b167e0b..70d8ea31940a 100644</span>
<span class="p_header">--- a/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_header">+++ b/arch/powerpc/oprofile/cell/spu_task_sync.c</span>
<span class="p_chunk">@@ -325,6 +325,7 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 	struct vm_area_struct *vma;
 	struct file *exe_file;
 	struct mm_struct *mm = spu-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!mm)
 		goto out;
<span class="p_chunk">@@ -336,7 +337,8 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 		fput(exe_file);
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		if (vma-&gt;vm_start &gt; spu_ref || vma-&gt;vm_end &lt;= spu_ref)
 			continue;
<span class="p_chunk">@@ -353,13 +355,13 @@</span> <span class="p_context"> get_exec_dcookie_and_offset(struct spu *spu, unsigned int *offsetp,</span>
 	*spu_bin_dcookie = fast_get_dcookie(&amp;vma-&gt;vm_file-&gt;f_path);
 	pr_debug(&quot;got dcookie for %pD\n&quot;, vma-&gt;vm_file);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 out:
 	return app_cookie;
 
 fail_no_image_cookie:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	printk(KERN_ERR &quot;SPU_PROF: &quot;
 		&quot;%s, line %d: Cannot find dcookie for SPU binary\n&quot;,
<span class="p_header">diff --git a/arch/powerpc/platforms/cell/spufs/file.c b/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_header">index ae2f740a82f1..87d2bcf59f46 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/cell/spufs/file.c</span>
<span class="p_chunk">@@ -347,11 +347,11 @@</span> <span class="p_context"> static int spufs_ps_fault(struct vm_fault *vmf,</span>
 		goto refault;
 
 	if (ctx-&gt;state == SPU_STATE_SAVED) {
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, vmf-&gt;lockrange);</span>
 		spu_context_nospu_trace(spufs_ps_fault__sleep, ctx);
 		ret = spufs_wait(ctx-&gt;run_wq, ctx-&gt;state == SPU_STATE_RUNNABLE);
 		spu_context_trace(spufs_ps_fault__wake, ctx, ctx-&gt;spu);
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, vmf-&gt;lockrange);</span>
 	} else {
 		area = ctx-&gt;spu-&gt;problem_phys + ps_offs;
 		vm_insert_pfn(vmf-&gt;vma, vmf-&gt;address, (area + offset) &gt;&gt; PAGE_SHIFT);
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 226ca70dc6bd..f8093b7ce2c1 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -148,10 +148,12 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long text_start;
 	int ret = 0;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	addr = get_unmapped_area(NULL, addr,
<span class="p_chunk">@@ -194,7 +196,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 	}
 
 up_fail:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -255,8 +257,10 @@</span> <span class="p_context"> int map_vdso_once(const struct vdso_image *image, unsigned long addr)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	/*
 	 * Check if we have already mapped vdso blob - fail to prevent
 	 * abusing from userspace install_speciall_mapping, which may
<span class="p_chunk">@@ -267,11 +271,11 @@</span> <span class="p_context"> int map_vdso_once(const struct vdso_image *image, unsigned long addr)</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		if (vma_is_special_mapping(vma, &amp;vdso_mapping) ||
 				vma_is_special_mapping(vma, &amp;vvar_mapping)) {
<span class="p_del">-			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			return -EEXIST;
 		}
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return map_vdso(image, addr);
 }
<span class="p_header">diff --git a/arch/x86/kernel/tboot.c b/arch/x86/kernel/tboot.c</span>
<span class="p_header">index b868fa1b812b..340364fa8b21 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tboot.c</span>
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> static struct mm_struct tboot_mm = {</span>
 	.pgd            = swapper_pg_dir,
 	.mm_users       = ATOMIC_INIT(2),
 	.mm_count       = ATOMIC_INIT(1),
<span class="p_del">-	.mmap_sem       = __RWSEM_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+	.mmap_rw_tree   = __RANGE_RWLOCK_TREE_INITIALIZER(init_mm.mmap_rw_tree),</span>
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.mmlist         = LIST_HEAD_INIT(init_mm.mmlist),
 };
<span class="p_header">diff --git a/arch/x86/kernel/vm86_32.c b/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">index 23ee89ce59a9..541f4e5515e5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vm86_32.c</span>
<span class="p_chunk">@@ -162,6 +162,7 @@</span> <span class="p_context"> void save_v86_state(struct kernel_vm86_regs *regs, int retval)</span>
 static void mark_screen_rdonly(struct mm_struct *mm)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	spinlock_t *ptl;
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_chunk">@@ -169,7 +170,8 @@</span> <span class="p_context"> static void mark_screen_rdonly(struct mm_struct *mm)</span>
 	pte_t *pte;
 	int i;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	pgd = pgd_offset(mm, 0xA0000);
 	if (pgd_none_or_clear_bad(pgd))
 		goto out;
<span class="p_chunk">@@ -192,7 +194,7 @@</span> <span class="p_context"> static void mark_screen_rdonly(struct mm_struct *mm)</span>
 	}
 	pte_unmap_unlock(pte, ptl);
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	flush_tlb();
 }
 
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index d81cd399544a..b7abcc782792 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -922,7 +922,8 @@</span> <span class="p_context"> bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,</span>
 
 static void
 __bad_area(struct pt_regs *regs, unsigned long error_code,
<span class="p_del">-	   unsigned long address,  struct vm_area_struct *vma, int si_code)</span>
<span class="p_add">+	   unsigned long address,  struct vm_area_struct *vma, int si_code,</span>
<span class="p_add">+	   struct range_rwlock *range)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 
<span class="p_chunk">@@ -930,15 +931,16 @@</span> <span class="p_context"> __bad_area(struct pt_regs *regs, unsigned long error_code,</span>
 	 * Something tried to access memory that isn&#39;t in our memory map..
 	 * Fix it, but check if it&#39;s kernel or user first..
 	 */
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 
 	__bad_area_nosemaphore(regs, error_code, address, vma, si_code);
 }
 
 static noinline void
<span class="p_del">-bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)</span>
<span class="p_add">+bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address,</span>
<span class="p_add">+	 struct range_rwlock *range)</span>
 {
<span class="p_del">-	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR);</span>
<span class="p_add">+	__bad_area(regs, error_code, address, NULL, SEGV_MAPERR, range);</span>
 }
 
 static inline bool bad_area_access_from_pkeys(unsigned long error_code,
<span class="p_chunk">@@ -960,7 +962,8 @@</span> <span class="p_context"> static inline bool bad_area_access_from_pkeys(unsigned long error_code,</span>
 
 static noinline void
 bad_area_access_error(struct pt_regs *regs, unsigned long error_code,
<span class="p_del">-		      unsigned long address, struct vm_area_struct *vma)</span>
<span class="p_add">+		      unsigned long address, struct vm_area_struct *vma,</span>
<span class="p_add">+		      struct range_rwlock *range)</span>
 {
 	/*
 	 * This OSPKE check is not strictly necessary at runtime.
<span class="p_chunk">@@ -968,9 +971,9 @@</span> <span class="p_context"> bad_area_access_error(struct pt_regs *regs, unsigned long error_code,</span>
 	 * if pkeys are compiled out.
 	 */
 	if (bad_area_access_from_pkeys(error_code, vma))
<span class="p_del">-		__bad_area(regs, error_code, address, vma, SEGV_PKUERR);</span>
<span class="p_add">+		__bad_area(regs, error_code, address, vma, SEGV_PKUERR, range);</span>
 	else
<span class="p_del">-		__bad_area(regs, error_code, address, vma, SEGV_ACCERR);</span>
<span class="p_add">+		__bad_area(regs, error_code, address, vma, SEGV_ACCERR, range);</span>
 }
 
 static void
<span class="p_chunk">@@ -1218,6 +1221,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	int fault, major = 0;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
<span class="p_chunk">@@ -1230,7 +1234,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 */
 	if (kmemcheck_active(regs))
 		kmemcheck_hide(regs);
<span class="p_del">-	prefetchw(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	prefetchw(&amp;mm-&gt;mmap_rw_tree);</span>
 
 	if (unlikely(kmmio_fault(regs, address)))
 		return;
<span class="p_chunk">@@ -1333,14 +1337,15 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * validate the source. If this is invalid we can skip the address
 	 * space check, thus avoiding the deadlock:
 	 */
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (unlikely(!range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range))) {</span>
 		if ((error_code &amp; PF_USER) == 0 &amp;&amp;
 		    !search_exception_tables(regs-&gt;ip)) {
 			bad_area_nosemaphore(regs, error_code, address, NULL);
 			return;
 		}
 retry:
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	} else {
 		/*
 		 * The above down_read_trylock() might have succeeded in
<span class="p_chunk">@@ -1352,13 +1357,13 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 
 	vma = find_vma(mm, address);
 	if (unlikely(!vma)) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 	if (likely(vma-&gt;vm_start &lt;= address))
 		goto good_area;
 	if (unlikely(!(vma-&gt;vm_flags &amp; VM_GROWSDOWN))) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 	if (error_code &amp; PF_USER) {
<span class="p_chunk">@@ -1369,12 +1374,12 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 		 * 32 pointers and then decrements %sp by 65535.)
 		 */
 		if (unlikely(address + 65536 + 32 * sizeof(unsigned long) &lt; regs-&gt;sp)) {
<span class="p_del">-			bad_area(regs, error_code, address);</span>
<span class="p_add">+			bad_area(regs, error_code, address, &amp;range);</span>
 			return;
 		}
 	}
 	if (unlikely(expand_stack(vma, address))) {
<span class="p_del">-		bad_area(regs, error_code, address);</span>
<span class="p_add">+		bad_area(regs, error_code, address, &amp;range);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -1384,7 +1389,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 */
 good_area:
 	if (unlikely(access_error(error_code, vma))) {
<span class="p_del">-		bad_area_access_error(regs, error_code, address, vma);</span>
<span class="p_add">+		bad_area_access_error(regs, error_code, address, vma, &amp;range);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -1394,7 +1399,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if
 	 * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags, NULL);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;range);</span>
 	major |= fault &amp; VM_FAULT_MAJOR;
 
 	/*
<span class="p_chunk">@@ -1420,7 +1425,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 		return;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (unlikely(fault &amp; VM_FAULT_ERROR)) {
 		mm_fault_error(regs, error_code, address, vma, fault);
 		return;
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index 864a47193b6c..65c032d23c5b 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -44,16 +44,18 @@</span> <span class="p_context"> static inline unsigned long mpx_bt_size_bytes(struct mm_struct *mm)</span>
 static unsigned long mpx_mmap(unsigned long len)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long addr, populate;
 
 	/* Only bounds table can be allocated here */
 	if (len != mpx_bt_size_bytes(mm))
 		return -EINVAL;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	addr = do_mmap(NULL, 0, len, PROT_READ | PROT_WRITE,
 		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (populate)
 		mm_populate(addr, populate);
 
<span class="p_chunk">@@ -340,6 +342,7 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 {
 	void __user *bd_base = MPX_INVALID_BOUNDS_DIR;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret = 0;
 
 	/*
<span class="p_chunk">@@ -354,25 +357,28 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 * unmap path; we can just use mm-&gt;context.bd_addr instead.
 	 */
 	bd_base = mpx_get_bounds_dir();
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
 int mpx_disable_management(void)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!cpu_feature_enabled(X86_FEATURE_MPX))
 		return -ENXIO;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mm-&gt;context.bd_addr = MPX_INVALID_BOUNDS_DIR;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/drivers/android/binder.c b/drivers/android/binder.c</span>
<span class="p_header">index aae4d8d4be36..425d70d49ef6 100644</span>
<span class="p_header">--- a/drivers/android/binder.c</span>
<span class="p_header">+++ b/drivers/android/binder.c</span>
<span class="p_chunk">@@ -581,6 +581,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 	unsigned long user_page_addr;
 	struct page **page;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	binder_debug(BINDER_DEBUG_BUFFER_ALLOC,
 		     &quot;%d: %s pages %p-%p\n&quot;, proc-&gt;pid,
<span class="p_chunk">@@ -597,7 +598,8 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 		mm = get_task_mm(proc-&gt;tsk);
 
 	if (mm) {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = proc-&gt;vma;
 		if (vma &amp;&amp; mm != proc-&gt;vma_vm_mm) {
 			pr_err(&quot;%d: vma mm and task mm mismatch\n&quot;,
<span class="p_chunk">@@ -647,7 +649,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 		/* vm_insert_page does not seem to increment the refcount */
 	}
 	if (mm) {
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		mmput(mm);
 	}
 	return 0;
<span class="p_chunk">@@ -669,7 +671,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 	}
 err_no_vma:
 	if (mm) {
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		mmput(mm);
 	}
 	return -ENOMEM;
<span class="p_header">diff --git a/drivers/firmware/efi/arm-runtime.c b/drivers/firmware/efi/arm-runtime.c</span>
<span class="p_header">index 974c5a31a005..e36999eeea95 100644</span>
<span class="p_header">--- a/drivers/firmware/efi/arm-runtime.c</span>
<span class="p_header">+++ b/drivers/firmware/efi/arm-runtime.c</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"> static struct mm_struct efi_mm = {</span>
 	.mm_rb			= RB_ROOT,
 	.mm_users		= ATOMIC_INIT(2),
 	.mm_count		= ATOMIC_INIT(1),
<span class="p_del">-	.mmap_sem		= __RWSEM_INITIALIZER(efi_mm.mmap_sem),</span>
<span class="p_add">+	.mmap_rw_tree		= __RANGE_RWLOCK_TREE_INITIALIZER(efi_mm.mmap_rw_tree),</span>
 	.page_table_lock	= __SPIN_LOCK_UNLOCKED(efi_mm.page_table_lock),
 	.mmlist			= LIST_HEAD_INIT(efi_mm.mmlist),
 };
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_header">index 99424cb8020b..c6dd32bb7f4d 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_cs.c</span>
<span class="p_chunk">@@ -509,6 +509,7 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 	struct amdgpu_fpriv *fpriv = p-&gt;filp-&gt;driver_priv;
 	struct amdgpu_bo_list_entry *e;
 	struct list_head duplicates;
<span class="p_add">+	struct range_rwlock range;</span>
 	bool need_mmap_lock = false;
 	unsigned i, tries = 10;
 	int r;
<span class="p_chunk">@@ -528,8 +529,10 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 	if (p-&gt;uf_entry.robj)
 		list_add(&amp;p-&gt;uf_entry.tv.head, &amp;p-&gt;validated);
 
<span class="p_del">-	if (need_mmap_lock)</span>
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (need_mmap_lock) {</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+	}</span>
 
 	while (1) {
 		struct list_head need_pages;
<span class="p_chunk">@@ -686,7 +689,7 @@</span> <span class="p_context"> static int amdgpu_cs_parser_bos(struct amdgpu_cs_parser *p,</span>
 error_free_pages:
 
 	if (need_mmap_lock)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (p-&gt;bo_list) {
 		for (i = p-&gt;bo_list-&gt;first_userptr;
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_header">index 106cf83c2e6b..20af4a036de5 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_gem.c</span>
<span class="p_chunk">@@ -269,6 +269,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	struct drm_amdgpu_gem_userptr *args = data;
 	struct drm_gem_object *gobj;
 	struct amdgpu_bo *bo;
<span class="p_add">+	struct range_rwlock range;</span>
 	uint32_t handle;
 	int r;
 
<span class="p_chunk">@@ -309,7 +310,8 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	}
 
 	if (args-&gt;flags &amp; AMDGPU_GEM_USERPTR_VALIDATE) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		r = amdgpu_ttm_tt_get_user_pages(bo-&gt;tbo.ttm,
 						 bo-&gt;tbo.ttm-&gt;pages);
<span class="p_chunk">@@ -326,7 +328,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 		if (r)
 			goto free_pages;
 
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	r = drm_gem_handle_create(filp, gobj, &amp;handle);
<span class="p_chunk">@@ -342,7 +344,7 @@</span> <span class="p_context"> int amdgpu_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	release_pages(bo-&gt;tbo.ttm-&gt;pages, bo-&gt;tbo.ttm-&gt;num_pages, false);
 
 unlock_mmap_sem:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 release_object:
 	drm_gem_object_unreference_unlocked(gobj);
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">index 7ea3cacf9f9f..dd6d1a620655 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_chunk">@@ -229,10 +229,12 @@</span> <span class="p_context"> static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct amdgpu_mn *rmn;
<span class="p_add">+	struct range_rwlock range;</span>
 	int r;
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	mutex_lock(&amp;adev-&gt;mn_lock);
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		mutex_unlock(&amp;adev-&gt;mn_lock);
 		return ERR_PTR(-EINTR);
 	}
<span class="p_chunk">@@ -260,13 +262,13 @@</span> <span class="p_context"> static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)</span>
 	hash_add(adev-&gt;mn_hash, &amp;rmn-&gt;node, (unsigned long)mm);
 
 release_locks:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mutex_unlock(&amp;adev-&gt;mn_lock);
 
 	return rmn;
 
 free_rmn:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mutex_unlock(&amp;adev-&gt;mn_lock);
 	kfree(rmn);
 
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_header">index d1ce83d73a87..1dad7d24e706 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c</span>
<span class="p_chunk">@@ -897,6 +897,7 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 {
 	struct kfd_hsa_memory_exception_data memory_exception_data;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/*
 	 * Because we are called from arbitrary context (workqueue) as opposed
<span class="p_chunk">@@ -910,7 +911,8 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 
 	memset(&amp;memory_exception_data, 0, sizeof(memory_exception_data));
 
<span class="p_del">-	down_read(&amp;p-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;p-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(p-&gt;mm, address);
 
 	memory_exception_data.gpu_id = dev-&gt;id;
<span class="p_chunk">@@ -937,7 +939,7 @@</span> <span class="p_context"> void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,</span>
 		}
 	}
 
<span class="p_del">-	up_read(&amp;p-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;p-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	mutex_lock(&amp;p-&gt;event_mutex);
 
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_header">index 84d1ffd1eef9..d43b57417041 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c</span>
<span class="p_chunk">@@ -78,6 +78,7 @@</span> <span class="p_context"> void kfd_process_destroy_wq(void)</span>
 struct kfd_process *kfd_create_process(const struct task_struct *thread)
 {
 	struct kfd_process *process;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	BUG_ON(!kfd_process_wq);
 
<span class="p_chunk">@@ -89,7 +90,8 @@</span> <span class="p_context"> struct kfd_process *kfd_create_process(const struct task_struct *thread)</span>
 		return ERR_PTR(-EINVAL);
 
 	/* Take mmap_sem because we call __mmu_notifier_register inside */
<span class="p_del">-	down_write(&amp;thread-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;thread-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * take kfd processes mutex before starting of process creation
<span class="p_chunk">@@ -108,7 +110,7 @@</span> <span class="p_context"> struct kfd_process *kfd_create_process(const struct task_struct *thread)</span>
 
 	mutex_unlock(&amp;kfd_processes_mutex);
 
<span class="p_del">-	up_write(&amp;thread-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;thread-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return process;
 }
<span class="p_header">diff --git a/drivers/gpu/drm/etnaviv/etnaviv_gem.c b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">index 75ca18aaa34e..20d7de5b9eb7 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_chunk">@@ -745,6 +745,7 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 {
 	int ret = 0, pinned, npages = etnaviv_obj-&gt;base.size &gt;&gt; PAGE_SHIFT;
 	struct page **pvec;
<span class="p_add">+	struct range_rwlock range;</span>
 	uintptr_t ptr;
 	unsigned int flags = 0;
 
<span class="p_chunk">@@ -758,7 +759,8 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 	pinned = 0;
 	ptr = etnaviv_obj-&gt;userptr.ptr;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	while (pinned &lt; npages) {
 		ret = get_user_pages_remote(task, mm, ptr, npages - pinned,
 					    flags, pvec + pinned, NULL, NULL,
<span class="p_chunk">@@ -769,7 +771,7 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 		ptr += ret * PAGE_SIZE;
 		pinned += ret;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (ret &lt; 0) {
 		release_pages(pvec, pinned, 0);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index fe531f904062..33a9c9c1ae9a 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -1673,8 +1673,10 @@</span> <span class="p_context"> i915_gem_mmap_ioctl(struct drm_device *dev, void *data,</span>
 	if (args-&gt;flags &amp; I915_MMAP_WC) {
 		struct mm_struct *mm = current-&gt;mm;
 		struct vm_area_struct *vma;
<span class="p_add">+		struct range_rwlock range;</span>
 
<span class="p_del">-		if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 			i915_gem_object_put(obj);
 			return -EINTR;
 		}
<span class="p_chunk">@@ -1684,7 +1686,7 @@</span> <span class="p_context"> i915_gem_mmap_ioctl(struct drm_device *dev, void *data,</span>
 				pgprot_writecombine(vm_get_page_prot(vma-&gt;vm_flags));
 		else
 			addr = -ENOMEM;
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		/* This may race, but that&#39;s ok, it only gets set */
 		WRITE_ONCE(obj-&gt;frontbuffer_ggtt_origin, ORIGIN_CPU);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 1f8e8eecb6df..b2a676450aa5 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -203,12 +203,14 @@</span> <span class="p_context"> static struct i915_mmu_notifier *</span>
 i915_mmu_notifier_find(struct i915_mm_struct *mm)
 {
 	struct i915_mmu_notifier *mn = mm-&gt;mn;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	mn = mm-&gt;mn;
 	if (mn)
 		return mn;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mutex_lock(&amp;mm-&gt;i915-&gt;mm_lock);
 	if ((mn = mm-&gt;mn) == NULL) {
 		mn = i915_mmu_notifier_create(mm-&gt;mm);
<span class="p_chunk">@@ -216,7 +218,7 @@</span> <span class="p_context"> i915_mmu_notifier_find(struct i915_mm_struct *mm)</span>
 			mm-&gt;mn = mn;
 	}
 	mutex_unlock(&amp;mm-&gt;i915-&gt;mm_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return mn;
 }
<span class="p_chunk">@@ -501,6 +503,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 
 	pvec = drm_malloc_gfp(npages, sizeof(struct page *), GFP_TEMPORARY);
 	if (pvec != NULL) {
<span class="p_add">+		struct range_rwlock range;</span>
 		struct mm_struct *mm = obj-&gt;userptr.mm-&gt;mm;
 		unsigned int flags = 0;
 
<span class="p_chunk">@@ -509,7 +512,8 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 
 		ret = -EFAULT;
 		if (mmget_not_zero(mm)) {
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+			range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			while (pinned &lt; npages) {
 				ret = get_user_pages_remote
 					(work-&gt;task, mm,
<span class="p_chunk">@@ -522,7 +526,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 
 				pinned += ret;
 			}
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			mmput(mm);
 		}
 	}
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_cs.c b/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_header">index a8442f7196d6..b5b02f6a81b4 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_cs.c</span>
<span class="p_chunk">@@ -76,6 +76,7 @@</span> <span class="p_context"> static int radeon_cs_parser_relocs(struct radeon_cs_parser *p)</span>
 {
 	struct radeon_cs_chunk *chunk;
 	struct radeon_cs_buckets buckets;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned i;
 	bool need_mmap_lock = false;
 	int r;
<span class="p_chunk">@@ -176,13 +177,15 @@</span> <span class="p_context"> static int radeon_cs_parser_relocs(struct radeon_cs_parser *p)</span>
 	if (p-&gt;cs_flags &amp; RADEON_CS_USE_VM)
 		p-&gt;vm_bos = radeon_vm_get_bos(p-&gt;rdev, p-&gt;ib.vm,
 					      &amp;p-&gt;validated);
<span class="p_del">-	if (need_mmap_lock)</span>
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (need_mmap_lock) {</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+	}</span>
 
 	r = radeon_bo_list_validate(p-&gt;rdev, &amp;p-&gt;ticket, &amp;p-&gt;validated, p-&gt;ring);
 
 	if (need_mmap_lock)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return r;
 }
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_gem.c b/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_header">index 96683f5b2b1b..0c21cb398ff8 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_gem.c</span>
<span class="p_chunk">@@ -285,6 +285,7 @@</span> <span class="p_context"> int radeon_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	struct drm_radeon_gem_userptr *args = data;
 	struct drm_gem_object *gobj;
 	struct radeon_bo *bo;
<span class="p_add">+	struct range_rwlock range;</span>
 	uint32_t handle;
 	int r;
 
<span class="p_chunk">@@ -331,17 +332,18 @@</span> <span class="p_context"> int radeon_gem_userptr_ioctl(struct drm_device *dev, void *data,</span>
 	}
 
 	if (args-&gt;flags &amp; RADEON_GEM_USERPTR_VALIDATE) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		r = radeon_bo_reserve(bo, true);
 		if (r) {
<span class="p_del">-			up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			goto release_object;
 		}
 
 		radeon_ttm_placement_from_domain(bo, RADEON_GEM_DOMAIN_GTT);
 		r = ttm_bo_validate(&amp;bo-&gt;tbo, &amp;bo-&gt;placement, true, false);
 		radeon_bo_unreserve(bo);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (r)
 			goto release_object;
 	}
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">index 896f2cf51e4e..d2f600325511 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_chunk">@@ -184,9 +184,11 @@</span> <span class="p_context"> static struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct radeon_mn *rmn;
<span class="p_add">+	struct range_rwlock range;</span>
 	int r;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return ERR_PTR(-EINTR);
 
 	mutex_lock(&amp;rdev-&gt;mn_lock);
<span class="p_chunk">@@ -215,13 +217,13 @@</span> <span class="p_context"> static struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)</span>
 
 release_locks:
 	mutex_unlock(&amp;rdev-&gt;mn_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return rmn;
 
 free_rmn:
 	mutex_unlock(&amp;rdev-&gt;mn_lock);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	kfree(rmn);
 
 	return ERR_PTR(r);
<span class="p_header">diff --git a/drivers/gpu/drm/ttm/ttm_bo_vm.c b/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_header">index 35ffb3754feb..4690ab0eae75 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/ttm/ttm_bo_vm.c</span>
<span class="p_chunk">@@ -66,7 +66,8 @@</span> <span class="p_context"> static int ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,</span>
 			goto out_unlock;
 
 		ttm_bo_reference(bo);
<span class="p_del">-		up_read(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+				  vmf-&gt;lockrange);</span>
 		(void) dma_fence_wait(bo-&gt;moving, true);
 		ttm_bo_unreserve(bo);
 		ttm_bo_unref(&amp;bo);
<span class="p_chunk">@@ -124,7 +125,8 @@</span> <span class="p_context"> static int ttm_bo_vm_fault(struct vm_fault *vmf)</span>
 		if (vmf-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY) {
 			if (!(vmf-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)) {
 				ttm_bo_reference(bo);
<span class="p_del">-				up_read(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_read_unlock(&amp;vmf-&gt;vma-&gt;vm_mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+						  vmf-&gt;lockrange);</span>
 				(void) ttm_bo_wait_unreserved(bo);
 				ttm_bo_unref(&amp;bo);
 			}
<span class="p_header">diff --git a/drivers/gpu/drm/via/via_dmablit.c b/drivers/gpu/drm/via/via_dmablit.c</span>
<span class="p_header">index 1a3ad769f8c8..88d05d6a6eaf 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/via/via_dmablit.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/via/via_dmablit.c</span>
<span class="p_chunk">@@ -230,6 +230,7 @@</span> <span class="p_context"> via_fire_dmablit(struct drm_device *dev, drm_via_sg_info_t *vsg, int engine)</span>
 static int
 via_lock_all_dma_pages(drm_via_sg_info_t *vsg,  drm_via_dmablit_t *xfer)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 	unsigned long first_pfn = VIA_PFN(xfer-&gt;mem_addr);
 	vsg-&gt;num_pages = VIA_PFN(xfer-&gt;mem_addr + (xfer-&gt;num_lines * xfer-&gt;mem_stride - 1)) -
<span class="p_chunk">@@ -238,13 +239,14 @@</span> <span class="p_context"> via_lock_all_dma_pages(drm_via_sg_info_t *vsg,  drm_via_dmablit_t *xfer)</span>
 	vsg-&gt;pages = vzalloc(sizeof(struct page *) * vsg-&gt;num_pages);
 	if (NULL == vsg-&gt;pages)
 		return -ENOMEM;
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = get_user_pages((unsigned long)xfer-&gt;mem_addr,
 			     vsg-&gt;num_pages,
 			     (vsg-&gt;direction == DMA_FROM_DEVICE) ? FOLL_WRITE : 0,
 			     vsg-&gt;pages, NULL);
 
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (ret != vsg-&gt;num_pages) {
 		if (ret &lt; 0)
 			return ret;
<span class="p_header">diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c</span>
<span class="p_header">index 0fe3bfb6839d..b88addd18858 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem.c</span>
<span class="p_chunk">@@ -86,6 +86,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 	struct ib_umem *umem;
 	struct page **page_list;
 	struct vm_area_struct **vma_list;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long locked;
 	unsigned long lock_limit;
 	unsigned long cur_base;
<span class="p_chunk">@@ -163,7 +164,8 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 
 	npages = ib_umem_num_pages(umem);
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	locked     = npages + current-&gt;mm-&gt;pinned_vm;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -236,7 +238,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 	} else
 		current-&gt;mm-&gt;pinned_vm = locked;
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (vma_list)
 		free_page((unsigned long) vma_list);
 	free_page((unsigned long) page_list);
<span class="p_chunk">@@ -248,10 +250,12 @@</span> <span class="p_context"> EXPORT_SYMBOL(ib_umem_get);</span>
 static void ib_umem_account(struct work_struct *work)
 {
 	struct ib_umem *umem = container_of(work, struct ib_umem, work);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;umem-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	umem-&gt;mm-&gt;pinned_vm -= umem-&gt;diff;
<span class="p_del">-	up_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;umem-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(umem-&gt;mm);
 	kfree(umem);
 }
<span class="p_chunk">@@ -265,6 +269,7 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 	struct ib_ucontext *context = umem-&gt;context;
 	struct mm_struct *mm;
 	struct task_struct *task;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long diff;
 
 	if (umem-&gt;odp_data) {
<span class="p_chunk">@@ -293,8 +298,9 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 	 * up here and not be able to take the mmap_sem.  In that case
 	 * we defer the vm_locked accounting to the system workqueue.
 	 */
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (context-&gt;closing) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!range_write_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 			INIT_WORK(&amp;umem-&gt;work, ib_umem_account);
 			umem-&gt;mm   = mm;
 			umem-&gt;diff = diff;
<span class="p_chunk">@@ -303,10 +309,10 @@</span> <span class="p_context"> void ib_umem_release(struct ib_umem *umem)</span>
 			return;
 		}
 	} else
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	mm-&gt;pinned_vm -= diff;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 out:
 	kfree(umem);
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index 0ac3c739a986..590ad2d3ab35 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -595,6 +595,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 	struct task_struct *owning_process  = NULL;
 	struct mm_struct   *owning_mm       = NULL;
 	struct page       **local_page_list = NULL;
<span class="p_add">+	struct range_rwlock range;</span>
 	u64 off;
 	int j, k, ret = 0, start_idx, npages = 0;
 	u64 base_virt_addr;
<span class="p_chunk">@@ -639,7 +640,8 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 			min_t(size_t, ALIGN(bcnt, PAGE_SIZE) / PAGE_SIZE,
 			      PAGE_SIZE / sizeof(struct page *));
 
<span class="p_del">-		down_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 		/*
 		 * Note: this might result in redundent page getting. We can
 		 * avoid this by checking dma_list to be 0 before calling
<span class="p_chunk">@@ -650,7 +652,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
 				flags, local_page_list, NULL, NULL, NULL);
<span class="p_del">-		up_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		if (npages &lt; 0)
 			break;
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_header">index 68295a12b771..86fe4147f991 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/user_pages.c</span>
<span class="p_chunk">@@ -71,6 +71,7 @@</span> <span class="p_context"> MODULE_PARM_DESC(cache_size, &quot;Send and receive side cache size limit (in MB)&quot;);</span>
 bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,
 			u32 nlocked, u32 npages)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long ulimit = rlimit(RLIMIT_MEMLOCK), pinned, cache_limit,
 		size = (cache_size * (1UL &lt;&lt; 20)); /* convert to bytes */
 	unsigned usr_ctxts = dd-&gt;num_rcv_contexts - dd-&gt;first_user_ctxt;
<span class="p_chunk">@@ -90,9 +91,10 @@</span> <span class="p_context"> bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,</span>
 	/* Convert to number of pages */
 	size = DIV_ROUND_UP(size, PAGE_SIZE);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	pinned = mm-&gt;pinned_vm;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/* First, check the absolute limit against all pinned pages. */
 	if (pinned + npages &gt;= ulimit &amp;&amp; !can_lock)
<span class="p_chunk">@@ -104,15 +106,17 @@</span> <span class="p_context"> bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,</span>
 int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t npages,
 			    bool writable, struct page **pages)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
 	ret = get_user_pages_fast(vaddr, npages, writable, pages);
 	if (ret &lt; 0)
 		return ret;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mm-&gt;pinned_vm += ret;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -120,6 +124,7 @@</span> <span class="p_context"> int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np</span>
 void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 			     size_t npages, bool dirty)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	size_t i;
 
 	for (i = 0; i &lt; npages; i++) {
<span class="p_chunk">@@ -129,8 +134,9 @@</span> <span class="p_context"> void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,</span>
 	}
 
 	if (mm) { /* during close after signal, mm can be NULL */
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		mm-&gt;pinned_vm -= npages;
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx4/main.c b/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_header">index fba94df28cf1..a540f45add11 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx4/main.c</span>
<span class="p_chunk">@@ -1142,6 +1142,7 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);
 	struct task_struct *owning_process  = NULL;
 	struct mm_struct   *owning_mm       = NULL;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	owning_process = get_pid_task(ibcontext-&gt;tgid, PIDTYPE_PID);
 	if (!owning_process)
<span class="p_chunk">@@ -1173,7 +1174,8 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	/* need to protect from a race on closing the vma as part of
 	 * mlx4_ib_vma_close().
 	 */
<span class="p_del">-	down_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (i = 0; i &lt; HW_BAR_COUNT; i++) {
 		vma = context-&gt;hw_bar_info[i].vma;
 		if (!vma)
<span class="p_chunk">@@ -1191,7 +1193,7 @@</span> <span class="p_context"> static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 		context-&gt;hw_bar_info[i].vma-&gt;vm_ops = NULL;
 	}
 
<span class="p_del">-	up_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(owning_mm);
 	put_task_struct(owning_process);
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">index 4dc0a8785fe0..215935c6ae4e 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_chunk">@@ -1449,6 +1449,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	struct mlx5_ib_ucontext *context = to_mucontext(ibcontext);
 	struct task_struct *owning_process  = NULL;
 	struct mm_struct   *owning_mm       = NULL;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	owning_process = get_pid_task(ibcontext-&gt;tgid, PIDTYPE_PID);
 	if (!owning_process)
<span class="p_chunk">@@ -1478,7 +1479,8 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	/* need to protect from a race on closing the vma as part of
 	 * mlx5_ib_vma_close.
 	 */
<span class="p_del">-	down_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 	list_for_each_entry_safe(vma_private, n, &amp;context-&gt;vma_private_list,
 				 list) {
 		vma = vma_private-&gt;vma;
<span class="p_chunk">@@ -1492,7 +1494,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 		list_del(&amp;vma_private-&gt;list);
 		kfree(vma_private);
 	}
<span class="p_del">-	up_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;owning_mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(owning_mm);
 	put_task_struct(owning_process);
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">index c1cf13f2722a..4b15be6f22b8 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_chunk">@@ -133,26 +133,32 @@</span> <span class="p_context"> dma_addr_t qib_map_page(struct pci_dev *hwdev, struct page *page,</span>
 int qib_get_user_pages(unsigned long start_page, size_t num_pages,
 		       struct page **p)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	ret = __qib_get_user_pages(start_page, num_pages, p);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
 
 void qib_release_user_pages(struct page **p, size_t num_pages)
 {
<span class="p_del">-	if (current-&gt;mm) /* during close after signal, mm can be NULL */</span>
<span class="p_del">-		down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (current-&gt;mm) { /* during close after signal, mm can be NULL */</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+	}</span>
 
 	__qib_release_user_pages(p, num_pages, 1);
 
 	if (current-&gt;mm) {
 		current-&gt;mm-&gt;pinned_vm -= num_pages;
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 }
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">index 1591d0e78bfa..c99f032d490b 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_chunk">@@ -55,12 +55,14 @@</span> <span class="p_context"> static struct workqueue_struct *usnic_uiom_wq;</span>
 
 static void usnic_uiom_reg_account(struct work_struct *work)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	struct usnic_uiom_reg *umem = container_of(work,
 						struct usnic_uiom_reg, work);
 
<span class="p_del">-	down_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;umem-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	umem-&gt;mm-&gt;locked_vm -= umem-&gt;diff;
<span class="p_del">-	up_write(&amp;umem-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;umem-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(umem-&gt;mm);
 	kfree(umem);
 }
<span class="p_chunk">@@ -103,6 +105,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 	struct page **page_list;
 	struct scatterlist *sg;
 	struct usnic_uiom_chunk *chunk;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long locked;
 	unsigned long lock_limit;
 	unsigned long cur_base;
<span class="p_chunk">@@ -125,7 +128,8 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 
 	npages = PAGE_ALIGN(size + (addr &amp; ~PAGE_MASK)) &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	locked = npages + current-&gt;mm-&gt;locked_vm;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -188,7 +192,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 	else
 		current-&gt;mm-&gt;locked_vm = locked;
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	free_page((unsigned long) page_list);
 	return ret;
 }
<span class="p_chunk">@@ -423,6 +427,7 @@</span> <span class="p_context"> struct usnic_uiom_reg *usnic_uiom_reg_get(struct usnic_uiom_pd *pd,</span>
 void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)
 {
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long diff;
 
 	__usnic_uiom_reg_release(uiomr-&gt;pd, uiomr, 1);
<span class="p_chunk">@@ -443,8 +448,9 @@</span> <span class="p_context"> void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)</span>
 	 * up here and not be able to take the mmap_sem.  In that case
 	 * we defer the vm_locked accounting to the system workqueue.
 	 */
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (closing) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!range_write_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 			INIT_WORK(&amp;uiomr-&gt;work, usnic_uiom_reg_account);
 			uiomr-&gt;mm = mm;
 			uiomr-&gt;diff = diff;
<span class="p_chunk">@@ -453,10 +459,10 @@</span> <span class="p_context"> void usnic_uiom_reg_release(struct usnic_uiom_reg *uiomr, int closing)</span>
 			return;
 		}
 	} else
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	current-&gt;mm-&gt;locked_vm -= diff;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 	kfree(uiomr);
 }
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">index 063343909b0d..e2a8e154adeb 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_chunk">@@ -518,6 +518,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	int ret = VM_FAULT_ERROR;
 	unsigned int flags = 0;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	u64 address;
 
 	mm = fault-&gt;state-&gt;mm;
<span class="p_chunk">@@ -529,7 +530,8 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 		flags |= FAULT_FLAG_WRITE;
 	flags |= FAULT_FLAG_REMOTE;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_extend_vma(mm, address);
 	if (!vma || address &lt; vma-&gt;vm_start)
 		/* failed to get a vma in the right range */
<span class="p_chunk">@@ -539,9 +541,9 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	if (access_error(vma, fault))
 		goto out;
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, flags, NULL);</span>
 out:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (ret &amp; VM_FAULT_ERROR)
 		/* failed to service fault */
<span class="p_header">diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c</span>
<span class="p_header">index 4ba770b9cfbb..fc67b4ce00da 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-svm.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-svm.c</span>
<span class="p_chunk">@@ -542,6 +542,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 		struct vm_area_struct *vma;
 		struct page_req_dsc *req;
 		struct qi_desc resp;
<span class="p_add">+		struct range_rwlock range;</span>
 		int ret, result;
 		u64 address;
 
<span class="p_chunk">@@ -582,7 +583,8 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 		/* If the mm is already defunct, don&#39;t handle faults. */
 		if (!mmget_not_zero(svm-&gt;mm))
 			goto bad_req;
<span class="p_del">-		down_read(&amp;svm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;svm-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_extend_vma(svm-&gt;mm, address);
 		if (!vma || address &lt; vma-&gt;vm_start)
 			goto invalid;
<span class="p_chunk">@@ -597,7 +599,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 
 		result = QI_RESP_SUCCESS;
 	invalid:
<span class="p_del">-		up_read(&amp;svm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;svm-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		mmput(svm-&gt;mm);
 	bad_req:
 		/* Accounting for major/minor faults? */
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-core.c b/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_header">index 1dbf6f7785bb..9a26d0a5d7d3 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-core.c</span>
<span class="p_chunk">@@ -530,14 +530,17 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(videobuf_querybuf);</span>
 int videobuf_qbuf(struct videobuf_queue *q, struct v4l2_buffer *b)
 {
 	struct videobuf_buffer *buf;
<span class="p_add">+	struct range_rwlock range;</span>
 	enum v4l2_field field;
 	unsigned long flags = 0;
 	int retval;
 
 	MAGIC_CHECK(q-&gt;int_ops-&gt;magic, MAGIC_QTYPE_OPS);
 
<span class="p_del">-	if (b-&gt;memory == V4L2_MEMORY_MMAP)</span>
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (b-&gt;memory == V4L2_MEMORY_MMAP) {</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+	}</span>
 
 	videobuf_queue_lock(q);
 	retval = -EBUSY;
<span class="p_chunk">@@ -624,7 +627,7 @@</span> <span class="p_context"> int videobuf_qbuf(struct videobuf_queue *q, struct v4l2_buffer *b)</span>
 	videobuf_queue_unlock(q);
 
 	if (b-&gt;memory == V4L2_MEMORY_MMAP)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return retval;
 }
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-contig.c b/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_header">index e02353e340dd..e74cf7f1119c 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-contig.c</span>
<span class="p_chunk">@@ -162,6 +162,7 @@</span> <span class="p_context"> static int videobuf_dma_contig_user_get(struct videobuf_dma_contig_memory *mem,</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	unsigned long prev_pfn, this_pfn;
 	unsigned long pages_done, user_address;
 	unsigned int offset;
<span class="p_chunk">@@ -171,7 +172,8 @@</span> <span class="p_context"> static int videobuf_dma_contig_user_get(struct videobuf_dma_contig_memory *mem,</span>
 	mem-&gt;size = PAGE_ALIGN(vb-&gt;size + offset);
 	ret = -EINVAL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	vma = find_vma(mm, vb-&gt;baddr);
 	if (!vma)
<span class="p_chunk">@@ -203,7 +205,7 @@</span> <span class="p_context"> static int videobuf_dma_contig_user_get(struct videobuf_dma_contig_memory *mem,</span>
 	}
 
 out_up:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">index b789070047df..785b69a5d52b 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_chunk">@@ -199,11 +199,13 @@</span> <span class="p_context"> static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,</span>
 static int videobuf_dma_init_user(struct videobuf_dmabuf *dma, int direction,
 			   unsigned long data, unsigned long size)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = videobuf_dma_init_user_locked(dma, direction, data, size);
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/drivers/misc/cxl/fault.c b/drivers/misc/cxl/fault.c</span>
<span class="p_header">index 2fa015c05561..45cefdf30656 100644</span>
<span class="p_header">--- a/drivers/misc/cxl/fault.c</span>
<span class="p_header">+++ b/drivers/misc/cxl/fault.c</span>
<span class="p_chunk">@@ -338,6 +338,7 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 	struct vm_area_struct *vma;
 	int rc;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	mm = get_mem_context(ctx);
 	if (mm == NULL) {
<span class="p_chunk">@@ -346,7 +347,8 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 		return;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		for (ea = vma-&gt;vm_start; ea &lt; vma-&gt;vm_end;
 				ea = next_segment(ea, slb.vsid)) {
<span class="p_chunk">@@ -361,7 +363,7 @@</span> <span class="p_context"> static void cxl_prefault_vma(struct cxl_context *ctx)</span>
 			last_esid = slb.esid;
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	mmput(mm);
 }
<span class="p_header">diff --git a/drivers/misc/mic/scif/scif_rma.c b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">index 30e3c524216d..6376e70efdb4 100644</span>
<span class="p_header">--- a/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">+++ b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_chunk">@@ -275,19 +275,22 @@</span> <span class="p_context"> static inline int</span>
 __scif_dec_pinned_vm_lock(struct mm_struct *mm,
 			  int nr_pages, bool try_lock)
 {
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
 	if (!mm || !nr_pages || !scif_ulimit_check)
 		return 0;
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (try_lock) {
<span class="p_del">-		if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (!range_write_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 			dev_err(scif_info.mdev.this_device,
 				&quot;%s %d err\n&quot;, __func__, __LINE__);
 			return -1;
 		}
 	} else {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 	mm-&gt;pinned_vm -= nr_pages;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1333,6 +1336,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 	int prot = *out_prot;
 	int ulimit = 0;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Unsupported flags */
 	if (map_flags &amp; ~(SCIF_MAP_KERNEL | SCIF_MAP_ULIMIT))
<span class="p_chunk">@@ -1386,11 +1390,12 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 		prot |= SCIF_PROT_WRITE;
 retry:
 		mm = current-&gt;mm;
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (ulimit) {
 			err = __scif_check_inc_pinned_vm(mm, nr_pages);
 			if (err) {
<span class="p_del">-				up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 				pinned_pages-&gt;nr_pages = 0;
 				goto error_unmap;
 			}
<span class="p_chunk">@@ -1402,7 +1407,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 				(prot &amp; SCIF_PROT_WRITE) ? FOLL_WRITE : 0,
 				pinned_pages-&gt;pages,
 				NULL, NULL);
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (nr_pages != pinned_pages-&gt;nr_pages) {
 			if (try_upgrade) {
 				if (ulimit)
<span class="p_header">diff --git a/drivers/oprofile/buffer_sync.c b/drivers/oprofile/buffer_sync.c</span>
<span class="p_header">index ac27f3d3fbb4..c0ce1baf7975 100644</span>
<span class="p_header">--- a/drivers/oprofile/buffer_sync.c</span>
<span class="p_header">+++ b/drivers/oprofile/buffer_sync.c</span>
<span class="p_chunk">@@ -90,12 +90,14 @@</span> <span class="p_context"> munmap_notify(struct notifier_block *self, unsigned long val, void *data)</span>
 	unsigned long addr = (unsigned long)data;
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *mpnt;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	mpnt = find_vma(mm, addr);
 	if (mpnt &amp;&amp; mpnt-&gt;vm_file &amp;&amp; (mpnt-&gt;vm_flags &amp; VM_EXEC)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		/* To avoid latency problems, we only process the current CPU,
 		 * hoping that most samples for the task are on this CPU
 		 */
<span class="p_chunk">@@ -103,7 +105,7 @@</span> <span class="p_context"> munmap_notify(struct notifier_block *self, unsigned long val, void *data)</span>
 		return 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -255,8 +257,10 @@</span> <span class="p_context"> lookup_dcookie(struct mm_struct *mm, unsigned long addr, off_t *offset)</span>
 {
 	unsigned long cookie = NO_COOKIE;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (vma = find_vma(mm, addr); vma; vma = vma-&gt;vm_next) {
 
 		if (addr &lt; vma-&gt;vm_start || addr &gt;= vma-&gt;vm_end)
<span class="p_chunk">@@ -276,7 +280,7 @@</span> <span class="p_context"> lookup_dcookie(struct mm_struct *mm, unsigned long addr, off_t *offset)</span>
 
 	if (!vma)
 		cookie = INVALID_COOKIE;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return cookie;
 }
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/llite/llite_mmap.c b/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_header">index 896196c74cd2..4a4fb38661c8 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/llite/llite_mmap.c</span>
<span class="p_chunk">@@ -61,9 +61,11 @@</span> <span class="p_context"> struct vm_area_struct *our_vma(struct mm_struct *mm, unsigned long addr,</span>
 			       size_t count)
 {
 	struct vm_area_struct *vma, *ret = NULL;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* mmap_sem must have been held by caller. */
<span class="p_del">-	LASSERT(!down_write_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	LASSERT(!range_write_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range));</span>
 
 	for (vma = find_vma(mm, addr);
 	    vma &amp;&amp; vma-&gt;vm_start &lt; (addr + count); vma = vma-&gt;vm_next) {
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/llite/vvp_io.c b/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_header">index 4c57755e06e7..508c02953946 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/llite/vvp_io.c</span>
<span class="p_chunk">@@ -376,6 +376,7 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 	int		 result = 0;
 	struct iov_iter i;
 	struct iovec iov;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	LASSERT(io-&gt;ci_type == CIT_READ || io-&gt;ci_type == CIT_WRITE);
 
<span class="p_chunk">@@ -395,7 +396,8 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 		count += addr &amp; (~PAGE_MASK);
 		addr &amp;= PAGE_MASK;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		while ((vma = our_vma(mm, addr, count)) != NULL) {
 			struct inode *inode = file_inode(vma-&gt;vm_file);
 			int flags = CEF_MUST;
<span class="p_chunk">@@ -436,7 +438,7 @@</span> <span class="p_context"> static int vvp_mmap_locks(const struct lu_env *env,</span>
 			count -= vma-&gt;vm_end - addr;
 			addr = vma-&gt;vm_end;
 		}
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (result &lt; 0)
 			break;
 	}
<span class="p_header">diff --git a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_header">index 3aeffcb9c87e..90f8fb91bdef 100644</span>
<span class="p_header">--- a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_header">+++ b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_2835_arm.c</span>
<span class="p_chunk">@@ -467,14 +467,17 @@</span> <span class="p_context"> create_pagelist(char __user *buf, size_t count, unsigned short type,</span>
 		}
 		/* do not try and release vmalloc pages */
 	} else {
<span class="p_del">-		down_read(&amp;task-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;task-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		actual_pages = get_user_pages(
 				          (unsigned long)buf &amp; ~(PAGE_SIZE - 1),
 					  num_pages,
 					  (type == PAGELIST_READ) ? FOLL_WRITE : 0,
 					  pages,
 					  NULL /*vmas */);
<span class="p_del">-		up_read(&amp;task-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;task-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		if (actual_pages != num_pages) {
 			vchiq_log_info(vchiq_arm_log_level,
<span class="p_header">diff --git a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_header">index 8a0d214f6e9b..24fd9317f220 100644</span>
<span class="p_header">--- a/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_header">+++ b/drivers/staging/vc04_services/interface/vchiq_arm/vchiq_arm.c</span>
<span class="p_chunk">@@ -1560,6 +1560,7 @@</span> <span class="p_context"> dump_phys_mem(void *virt_addr, u32 num_bytes)</span>
 	struct page   *page;
 	struct page  **pages;
 	u8            *kmapped_virt_ptr;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Align virtAddr and endVirtAddr to 16 byte boundaries. */
 
<span class="p_chunk">@@ -1580,14 +1581,15 @@</span> <span class="p_context"> dump_phys_mem(void *virt_addr, u32 num_bytes)</span>
 		return;
 	}
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	rc = get_user_pages(
 		(unsigned long)virt_addr, /* start */
 		num_pages,                /* len */
 		0,                        /* gup_flags */
 		pages,                    /* pages (array of page pointers) */
 		NULL);                    /* vmas */
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	prev_idx = -1;
 	page = NULL;
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_spapr_tce.c b/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_header">index cf3de91fbfe7..69ca50c6d67c 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_spapr_tce.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"> static void tce_iommu_detach_group(void *iommu_data,</span>
 static long try_increment_locked_vm(struct mm_struct *mm, long npages)
 {
 	long ret = 0, locked, lock_limit;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (WARN_ON_ONCE(!mm))
 		return -EPERM;
<span class="p_chunk">@@ -44,7 +45,8 @@</span> <span class="p_context"> static long try_increment_locked_vm(struct mm_struct *mm, long npages)</span>
 	if (!npages)
 		return 0;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	locked = mm-&gt;locked_vm + npages;
 	lock_limit = rlimit(RLIMIT_MEMLOCK) &gt;&gt; PAGE_SHIFT;
 	if (locked &gt; lock_limit &amp;&amp; !capable(CAP_IPC_LOCK))
<span class="p_chunk">@@ -58,17 +60,20 @@</span> <span class="p_context"> static long try_increment_locked_vm(struct mm_struct *mm, long npages)</span>
 			rlimit(RLIMIT_MEMLOCK),
 			ret ? &quot; - exceeded&quot; : &quot;&quot;);
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
 
 static void decrement_locked_vm(struct mm_struct *mm, long npages)
 {
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
 	if (!mm || !npages)
 		return;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (WARN_ON_ONCE(npages &gt; mm-&gt;locked_vm))
 		npages = mm-&gt;locked_vm;
 	mm-&gt;locked_vm -= npages;
<span class="p_chunk">@@ -76,7 +81,7 @@</span> <span class="p_context"> static void decrement_locked_vm(struct mm_struct *mm, long npages)</span>
 			npages &lt;&lt; PAGE_SHIFT,
 			mm-&gt;locked_vm &lt;&lt; PAGE_SHIFT,
 			rlimit(RLIMIT_MEMLOCK));
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 /*
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index 32d2633092a3..984cc1e71977 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -257,11 +257,13 @@</span> <span class="p_context"> static void vfio_lock_acct_bg(struct work_struct *work)</span>
 {
 	struct vwork *vwork = container_of(work, struct vwork, work);
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	mm = vwork-&gt;mm;
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mm-&gt;locked_vm += vwork-&gt;npage;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 	kfree(vwork);
 }
<span class="p_chunk">@@ -270,6 +272,7 @@</span> <span class="p_context"> static void vfio_lock_acct(struct task_struct *task, long npage)</span>
 {
 	struct vwork *vwork;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	bool is_current;
 
 	if (!npage)
<span class="p_chunk">@@ -281,9 +284,10 @@</span> <span class="p_context"> static void vfio_lock_acct(struct task_struct *task, long npage)</span>
 	if (!mm)
 		return; /* process exited */
 
<span class="p_del">-	if (down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		mm-&gt;locked_vm += npage;
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (!is_current)
 			mmput(mm);
 		return;
<span class="p_chunk">@@ -361,8 +365,10 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 {
 	struct page *page[1];
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (mm == current-&gt;mm) {
 		ret = get_user_pages_fast(vaddr, 1, !!(prot &amp; IOMMU_WRITE),
 					  page);
<span class="p_chunk">@@ -372,10 +378,10 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 		if (prot &amp; IOMMU_WRITE)
 			flags |= FOLL_WRITE;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		ret = get_user_pages_remote(NULL, mm, vaddr, 1, flags, page,
<span class="p_del">-					    NULL, NULL);</span>
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+					    NULL, NULL, NULL);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	if (ret == 1) {
<span class="p_chunk">@@ -383,7 +389,7 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 		return 0;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	vma = find_vma_intersection(mm, vaddr, vaddr + 1);
 
<span class="p_chunk">@@ -393,7 +399,7 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 			ret = 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/drivers/virt/fsl_hypervisor.c b/drivers/virt/fsl_hypervisor.c</span>
<span class="p_header">index 150ce2abf6c8..544bcc67e357 100644</span>
<span class="p_header">--- a/drivers/virt/fsl_hypervisor.c</span>
<span class="p_header">+++ b/drivers/virt/fsl_hypervisor.c</span>
<span class="p_chunk">@@ -146,6 +146,7 @@</span> <span class="p_context"> static long ioctl_stop(struct fsl_hv_ioctl_stop __user *p)</span>
  */
 static long ioctl_memcpy(struct fsl_hv_ioctl_memcpy __user *p)
 {
<span class="p_add">+	struct range_rwlock range;</span>
 	struct fsl_hv_ioctl_memcpy param;
 
 	struct page **pages = NULL;
<span class="p_chunk">@@ -243,11 +244,12 @@</span> <span class="p_context"> static long ioctl_memcpy(struct fsl_hv_ioctl_memcpy __user *p)</span>
 	sg_list = PTR_ALIGN(sg_list_unaligned, sizeof(struct fh_sg_list));
 
 	/* Get the physical addresses of the source buffer */
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	num_pinned = get_user_pages(param.local_vaddr - lb_offset,
 		num_pages, (param.source == -1) ? 0 : FOLL_WRITE,
 		pages, NULL);
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (num_pinned != num_pages) {
 		/* get_user_pages() failed */
<span class="p_header">diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c</span>
<span class="p_header">index f3bf8f4e2d6c..89ff9b6f6f89 100644</span>
<span class="p_header">--- a/drivers/xen/gntdev.c</span>
<span class="p_header">+++ b/drivers/xen/gntdev.c</span>
<span class="p_chunk">@@ -657,13 +657,15 @@</span> <span class="p_context"> static long gntdev_ioctl_get_offset_for_vaddr(struct gntdev_priv *priv,</span>
 	struct ioctl_gntdev_get_offset_for_vaddr op;
 	struct vm_area_struct *vma;
 	struct grant_map *map;
<span class="p_add">+	struct range_rwlock range;</span>
 	int rv = -EINVAL;
 
 	if (copy_from_user(&amp;op, u, sizeof(op)) != 0)
 		return -EFAULT;
 	pr_debug(&quot;priv %p, offset for vaddr %lx\n&quot;, priv, (unsigned long)op.vaddr);
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, op.vaddr);
 	if (!vma || vma-&gt;vm_ops != &amp;gntdev_vmops)
 		goto out_unlock;
<span class="p_chunk">@@ -677,7 +679,7 @@</span> <span class="p_context"> static long gntdev_ioctl_get_offset_for_vaddr(struct gntdev_priv *priv,</span>
 	rv = 0;
 
  out_unlock:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (rv == 0 &amp;&amp; copy_to_user(u, &amp;op, sizeof(op)) != 0)
 		return -EFAULT;
<span class="p_header">diff --git a/drivers/xen/privcmd.c b/drivers/xen/privcmd.c</span>
<span class="p_header">index 7a92a5e1d40c..a35de7996f1d 100644</span>
<span class="p_header">--- a/drivers/xen/privcmd.c</span>
<span class="p_header">+++ b/drivers/xen/privcmd.c</span>
<span class="p_chunk">@@ -260,6 +260,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 	int rc;
 	LIST_HEAD(pagelist);
 	struct mmap_gfn_state state;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* We only support privcmd_ioctl_mmap_batch for auto translated. */
 	if (xen_feature(XENFEAT_auto_translated_physmap))
<span class="p_chunk">@@ -279,7 +280,8 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 	if (rc || list_empty(&amp;pagelist))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	{
 		struct page *page = list_first_entry(&amp;pagelist,
<span class="p_chunk">@@ -304,7 +306,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap(struct file *file, void __user *udata)</span>
 
 
 out_up:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 out:
 	free_page_list(&amp;pagelist);
<span class="p_chunk">@@ -454,6 +456,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	unsigned long nr_pages;
 	LIST_HEAD(pagelist);
 	struct mmap_batch_state state;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	switch (version) {
 	case 1:
<span class="p_chunk">@@ -500,7 +503,8 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 		}
 	}
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	vma = find_vma(mm, m.addr);
 	if (!vma ||
<span class="p_chunk">@@ -556,7 +560,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	BUG_ON(traverse_pages_block(m.num, sizeof(xen_pfn_t),
 				    &amp;pagelist, mmap_batch_fn, &amp;state));
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (state.global_error) {
 		/* Write back errors in second pass. */
<span class="p_chunk">@@ -577,7 +581,7 @@</span> <span class="p_context"> static long privcmd_ioctl_mmap_batch(</span>
 	return ret;
 
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	goto out;
 }
 
<span class="p_header">diff --git a/fs/aio.c b/fs/aio.c</span>
<span class="p_header">index f52d925ee259..0858bf26c6c5 100644</span>
<span class="p_header">--- a/fs/aio.c</span>
<span class="p_header">+++ b/fs/aio.c</span>
<span class="p_chunk">@@ -450,6 +450,9 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	int nr_pages;
 	int i;
 	struct file *file;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	/* Compensate for the ring buffer&#39;s head/tail overlap entry */
 	nr_events += 2;	/* 1 is required, 2 for good luck */
<span class="p_chunk">@@ -504,7 +507,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	ctx-&gt;mmap_size = nr_pages * PAGE_SIZE;
 	pr_debug(&quot;attempting mmap of %lu bytes\n&quot;, ctx-&gt;mmap_size);
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		ctx-&gt;mmap_size = 0;
 		aio_free_ring(ctx);
 		return -EINTR;
<span class="p_chunk">@@ -513,7 +516,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx)</span>
 	ctx-&gt;mmap_base = do_mmap_pgoff(ctx-&gt;aio_ring_file, 0, ctx-&gt;mmap_size,
 				       PROT_READ | PROT_WRITE,
 				       MAP_SHARED, 0, &amp;unused, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (IS_ERR((void *)ctx-&gt;mmap_base)) {
 		ctx-&gt;mmap_size = 0;
 		aio_free_ring(ctx);
<span class="p_header">diff --git a/fs/coredump.c b/fs/coredump.c</span>
<span class="p_header">index 592683711c64..1aac9bc29b03 100644</span>
<span class="p_header">--- a/fs/coredump.c</span>
<span class="p_header">+++ b/fs/coredump.c</span>
<span class="p_chunk">@@ -411,17 +411,19 @@</span> <span class="p_context"> static int coredump_wait(int exit_code, struct core_state *core_state)</span>
 	struct task_struct *tsk = current;
 	struct mm_struct *mm = tsk-&gt;mm;
 	int core_waiters = -EBUSY;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	init_completion(&amp;core_state-&gt;startup);
 	core_state-&gt;dumper.task = tsk;
 	core_state-&gt;dumper.next = NULL;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	if (!mm-&gt;core_state)
 		core_waiters = zap_threads(tsk, mm, core_state, exit_code);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	if (core_waiters &gt; 0) {
 		struct core_thread *ptr;
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 49a3a19816f0..0a75d1cd1946 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -268,12 +268,14 @@</span> <span class="p_context"> static int __bprm_mm_init(struct linux_binprm *bprm)</span>
 	int err;
 	struct vm_area_struct *vma = NULL;
 	struct mm_struct *mm = bprm-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	bprm-&gt;vma = vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!vma)
 		return -ENOMEM;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		err = -EINTR;
 		goto err_free;
 	}
<span class="p_chunk">@@ -298,11 +300,11 @@</span> <span class="p_context"> static int __bprm_mm_init(struct linux_binprm *bprm)</span>
 
 	mm-&gt;stack_vm = mm-&gt;total_vm = 1;
 	arch_bprm_mm_init(mm, vma);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	bprm-&gt;p = vma-&gt;vm_end - sizeof(void *);
 	return 0;
 err:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 err_free:
 	bprm-&gt;vma = NULL;
 	kmem_cache_free(vm_area_cachep, vma);
<span class="p_chunk">@@ -673,6 +675,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 	unsigned long stack_size;
 	unsigned long stack_expand;
 	unsigned long rlim_stack;
<span class="p_add">+	struct range_rwlock range;</span>
 
 #ifdef CONFIG_STACK_GROWSUP
 	/* Limit stack size */
<span class="p_chunk">@@ -710,7 +713,8 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 		bprm-&gt;loader -= stack_shift;
 	bprm-&gt;exec -= stack_shift;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	vm_flags = VM_STACK_FLAGS;
<span class="p_chunk">@@ -767,7 +771,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 		ret = -EFAULT;
 
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL(setup_arg_pages);
<span class="p_chunk">@@ -1001,6 +1005,9 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 {
 	struct task_struct *tsk;
 	struct mm_struct *old_mm, *active_mm;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	/* Notify parent that we&#39;re no longer interested in the old VM */
 	tsk = current;
<span class="p_chunk">@@ -1015,9 +1022,10 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 		 * through with the exec.  We must hold mmap_sem around
 		 * checking core_state and changing tsk-&gt;mm.
 		 */
<span class="p_del">-		down_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+		range_read_lock(&amp;old_mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (unlikely(old_mm-&gt;core_state)) {
<span class="p_del">-			up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;old_mm-&gt;mmap_rw_tree, &amp;range);</span>
 			return -EINTR;
 		}
 	}
<span class="p_chunk">@@ -1030,7 +1038,7 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 	vmacache_flush(tsk);
 	task_unlock(tsk);
 	if (old_mm) {
<span class="p_del">-		up_read(&amp;old_mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;old_mm-&gt;mmap_rw_tree, &amp;range);</span>
 		BUG_ON(active_mm != old_mm);
 		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);
 		mm_update_next_owner(old_mm);
<span class="p_header">diff --git a/fs/proc/base.c b/fs/proc/base.c</span>
<span class="p_header">index c87b6b9a8a76..9e252fe49aa4 100644</span>
<span class="p_header">--- a/fs/proc/base.c</span>
<span class="p_header">+++ b/fs/proc/base.c</span>
<span class="p_chunk">@@ -216,7 +216,9 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 	unsigned long p;
 	char c;
 	ssize_t rv;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	BUG_ON(*pos &lt; 0);
 
 	tsk = get_proc_task(file_inode(file));
<span class="p_chunk">@@ -238,12 +240,12 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 		goto out_mmput;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	arg_start = mm-&gt;arg_start;
 	arg_end = mm-&gt;arg_end;
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	BUG_ON(arg_start &gt; arg_end);
 	BUG_ON(env_start &gt; env_end);
<span class="p_chunk">@@ -916,7 +918,9 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 	int ret = 0;
 	struct mm_struct *mm = file-&gt;private_data;
 	unsigned long env_start, env_end;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	/* Ensure the process spawned far enough to have an environment. */
 	if (!mm || !mm-&gt;env_end)
 		return 0;
<span class="p_chunk">@@ -929,10 +933,10 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 	if (!mmget_not_zero(mm))
 		goto free;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	while (count &gt; 0) {
 		size_t this_len, max_len;
<span class="p_chunk">@@ -1880,6 +1884,7 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 	struct task_struct *task;
 	struct inode *inode;
 	int status = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (flags &amp; LOOKUP_RCU)
 		return -ECHILD;
<span class="p_chunk">@@ -1894,9 +1899,10 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 		goto out;
 
 	if (!dname_to_vma_addr(dentry, &amp;vm_start, &amp;vm_end)) {
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		exact_vma_exists = !!find_exact_vma(mm, vm_start, vm_end);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	mmput(mm);
<span class="p_chunk">@@ -1927,6 +1933,7 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 	struct task_struct *task;
 	struct mm_struct *mm;
 	int rc;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	rc = -ENOENT;
 	task = get_proc_task(d_inode(dentry));
<span class="p_chunk">@@ -1943,14 +1950,15 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 		goto out_mmput;
 
 	rc = -ENOENT;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_exact_vma(mm, vm_start, vm_end);
 	if (vma &amp;&amp; vma-&gt;vm_file) {
 		*path = vma-&gt;vm_file-&gt;f_path;
 		path_get(path);
 		rc = 0;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 out_mmput:
 	mmput(mm);
<span class="p_chunk">@@ -2023,6 +2031,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	struct task_struct *task;
 	int result;
 	struct mm_struct *mm;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	result = -ENOENT;
 	task = get_proc_task(dir);
<span class="p_chunk">@@ -2041,7 +2050,8 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	if (!mm)
 		goto out_put_task;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_exact_vma(mm, vm_start, vm_end);
 	if (!vma)
 		goto out_no_vma;
<span class="p_chunk">@@ -2051,7 +2061,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 				(void *)(unsigned long)vma-&gt;vm_file-&gt;f_mode);
 
 out_no_vma:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 out_put_task:
 	put_task_struct(task);
<span class="p_chunk">@@ -2076,7 +2086,9 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	struct map_files_info info;
 	struct map_files_info *p;
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	ret = -ENOENT;
 	task = get_proc_task(file_inode(file));
 	if (!task)
<span class="p_chunk">@@ -2093,7 +2105,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	mm = get_task_mm(task);
 	if (!mm)
 		goto out_put_task;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	nr_files = 0;
 
<span class="p_chunk">@@ -2120,7 +2132,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 			ret = -ENOMEM;
 			if (fa)
 				flex_array_free(fa);
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			mmput(mm);
 			goto out_put_task;
 		}
<span class="p_chunk">@@ -2139,7 +2151,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 				BUG();
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	for (i = 0; i &lt; nr_files; i++) {
 		p = flex_array_get(fa, i);
<span class="p_header">diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="p_header">index c5ae09b6c726..c2991bfa9a6c 100644</span>
<span class="p_header">--- a/fs/proc/internal.h</span>
<span class="p_header">+++ b/fs/proc/internal.h</span>
<span class="p_chunk">@@ -279,6 +279,7 @@</span> <span class="p_context"> struct proc_maps_private {</span>
 #ifdef CONFIG_NUMA
 	struct mempolicy *task_mempolicy;
 #endif
<span class="p_add">+	struct range_rwlock range;</span>
 };
 
 struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 312578089544..3fceea238474 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -133,7 +133,7 @@</span> <span class="p_context"> static void vma_stop(struct proc_maps_private *priv)</span>
 	struct mm_struct *mm = priv-&gt;mm;
 
 	release_task_mempolicy(priv);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;priv-&gt;range);</span>
 	mmput(mm);
 }
 
<span class="p_chunk">@@ -171,7 +171,8 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *ppos)</span>
 	if (!mm || !mmget_not_zero(mm))
 		return NULL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;priv-&gt;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;priv-&gt;range);</span>
 	hold_task_mempolicy(priv);
 	priv-&gt;tail_vma = get_gate_vma(mm);
 
<span class="p_chunk">@@ -1009,7 +1010,9 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	enum clear_refs_types type;
 	int itype;
 	int rv;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	memset(buffer, 0, sizeof(buffer));
 	if (count &gt; sizeof(buffer) - 1)
 		count = sizeof(buffer) - 1;
<span class="p_chunk">@@ -1038,7 +1041,8 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		};
 
 		if (type == CLEAR_REFS_MM_HIWATER_RSS) {
<span class="p_del">-			if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+			if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+							   &amp;range)) {</span>
 				count = -EINTR;
 				goto out_mm;
 			}
<span class="p_chunk">@@ -1048,17 +1052,18 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 			 * resident set size to this mm&#39;s current rss value.
 			 */
 			reset_mm_hiwater_rss(mm);
<span class="p_del">-			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			goto out_mm;
 		}
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY) {
 			for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 				if (!(vma-&gt;vm_flags &amp; VM_SOFTDIRTY))
 					continue;
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-				if (down_write_killable(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+				range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+				if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+								   &amp;range)) {</span>
 					count = -EINTR;
 					goto out_mm;
 				}
<span class="p_chunk">@@ -1066,7 +1071,8 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 					vma-&gt;vm_flags &amp;= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
 				}
<span class="p_del">-				downgrade_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_downgrade_write(&amp;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+						      &amp;range);</span>
 				break;
 			}
 			mmu_notifier_invalidate_range_start(mm, 0, -1);
<span class="p_chunk">@@ -1075,7 +1081,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY)
 			mmu_notifier_invalidate_range_end(mm, 0, -1);
 		flush_tlb_mm(mm);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 out_mm:
 		mmput(mm);
 	}
<span class="p_chunk">@@ -1359,6 +1365,7 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 	unsigned long start_vaddr;
 	unsigned long end_vaddr;
 	int ret = 0, copied = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!mm || !mmget_not_zero(mm))
 		goto out;
<span class="p_chunk">@@ -1414,9 +1421,10 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 		/* overflow ? */
 		if (end &lt; start_vaddr || end &gt; end_vaddr)
 			end = end_vaddr;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		start_vaddr = end;
 
 		len = min(count, PM_ENTRY_BYTES * pm.pos);
<span class="p_header">diff --git a/fs/proc/task_nommu.c b/fs/proc/task_nommu.c</span>
<span class="p_header">index 23266694db11..50fa34b6a4bb 100644</span>
<span class="p_header">--- a/fs/proc/task_nommu.c</span>
<span class="p_header">+++ b/fs/proc/task_nommu.c</span>
<span class="p_chunk">@@ -23,8 +23,10 @@</span> <span class="p_context"> void task_mem(struct seq_file *m, struct mm_struct *mm)</span>
 	struct vm_region *region;
 	struct rb_node *p;
 	unsigned long bytes = 0, sbytes = 0, slack = 0, size;
<span class="p_del">-        </span>
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 
<span class="p_chunk">@@ -76,7 +78,7 @@</span> <span class="p_context"> void task_mem(struct seq_file *m, struct mm_struct *mm)</span>
 		&quot;Shared:\t%8lu bytes\n&quot;,
 		bytes, slack, sbytes);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 unsigned long task_vsize(struct mm_struct *mm)
<span class="p_chunk">@@ -84,13 +86,15 @@</span> <span class="p_context"> unsigned long task_vsize(struct mm_struct *mm)</span>
 	struct vm_area_struct *vma;
 	struct rb_node *p;
 	unsigned long vsize = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 		vsize += vma-&gt;vm_end - vma-&gt;vm_start;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return vsize;
 }
 
<span class="p_chunk">@@ -102,8 +106,10 @@</span> <span class="p_context"> unsigned long task_statm(struct mm_struct *mm,</span>
 	struct vm_region *region;
 	struct rb_node *p;
 	unsigned long size = kobjsize(mm);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p)) {
 		vma = rb_entry(p, struct vm_area_struct, vm_rb);
 		size += kobjsize(vma);
<span class="p_chunk">@@ -118,7 +124,7 @@</span> <span class="p_context"> unsigned long task_statm(struct mm_struct *mm,</span>
 		&gt;&gt; PAGE_SHIFT;
 	*data = (PAGE_ALIGN(mm-&gt;start_stack) - (mm-&gt;start_data &amp; PAGE_MASK))
 		&gt;&gt; PAGE_SHIFT;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	size &gt;&gt;= PAGE_SHIFT;
 	size += *text + *data;
 	*resident = size;
<span class="p_chunk">@@ -224,13 +230,14 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *pos)</span>
 	if (!mm || !mmget_not_zero(mm))
 		return NULL;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;priv-&gt;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;priv-&gt;range);</span>
 	/* start from the Nth VMA */
 	for (p = rb_first(&amp;mm-&gt;mm_rb); p; p = rb_next(p))
 		if (n-- == 0)
 			return p;
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;priv-&gt;range);</span>
 	mmput(mm);
 	return NULL;
 }
<span class="p_chunk">@@ -240,7 +247,7 @@</span> <span class="p_context"> static void m_stop(struct seq_file *m, void *_vml)</span>
 	struct proc_maps_private *priv = m-&gt;private;
 
 	if (!IS_ERR_OR_NULL(_vml)) {
<span class="p_del">-		up_read(&amp;priv-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;priv-&gt;mm-&gt;mmap_rw_tree, &amp;priv-&gt;range);</span>
 		mmput(priv-&gt;mm);
 	}
 	if (priv-&gt;task) {
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 5752b3b65638..9852b4711d29 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -431,7 +431,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_fault *vmf, unsigned long reason)</span>
 	else
 		must_wait = userfaultfd_huge_must_wait(ctx, vmf-&gt;address,
 						       vmf-&gt;flags, reason);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, vmf-&gt;lockrange);</span>
 
 	if (likely(must_wait &amp;&amp; !ACCESS_ONCE(ctx-&gt;released) &amp;&amp;
 		   (return_to_userland ? !signal_pending(current) :
<span class="p_chunk">@@ -485,7 +485,7 @@</span> <span class="p_context"> int handle_userfault(struct vm_fault *vmf, unsigned long reason)</span>
 			 * and there&#39;s no need to retake the mmap_sem
 			 * in such case.
 			 */
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_lock(&amp;mm-&gt;mmap_rw_tree, vmf-&gt;lockrange);</span>
 			ret = VM_FAULT_NOPAGE;
 		}
 	}
<span class="p_chunk">@@ -704,7 +704,7 @@</span> <span class="p_context"> bool userfaultfd_remove(struct vm_area_struct *vma,</span>
 		return true;
 
 	userfaultfd_ctx_get(ctx);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 
 	msg_init(&amp;ewq.msg);
 
<span class="p_chunk">@@ -783,7 +783,9 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 	/* len == 0 means wake all */
 	struct userfaultfd_wake_range range = { .len = 0, };
 	unsigned long new_flags;
<span class="p_add">+	struct range_rwlock lockrange;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;lockrange);</span>
 	ACCESS_ONCE(ctx-&gt;released) = true;
 
 	if (!mmget_not_zero(mm))
<span class="p_chunk">@@ -797,7 +799,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 	 * it&#39;s critical that released is set to true (above), before
 	 * taking the mmap_sem for writing.
 	 */
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;lockrange);</span>
 	prev = NULL;
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		cond_resched();
<span class="p_chunk">@@ -820,7 +822,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 		vma-&gt;vm_flags = new_flags;
 		vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 	}
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;lockrange);</span>
 	mmput(mm);
 wakeup:
 	/*
<span class="p_chunk">@@ -1180,6 +1182,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	bool non_anon_pages;
 	unsigned long start, end, vma_end;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	user_uffdio_register = (struct uffdio_register __user *) arg;
 
<span class="p_chunk">@@ -1219,7 +1222,8 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 	if (!mmget_not_zero(mm))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma_prev(mm, start, &amp;prev);
 	if (!vma)
 		goto out_unlock;
<span class="p_chunk">@@ -1347,7 +1351,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 		vma = vma-&gt;vm_next;
 	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 	if (!ret) {
 		/*
<span class="p_chunk">@@ -1375,6 +1379,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	unsigned long start, end, vma_end;
 	const void __user *buf = (void __user *)arg;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	ret = -EFAULT;
 	if (copy_from_user(&amp;uffdio_unregister, buf, sizeof(uffdio_unregister)))
<span class="p_chunk">@@ -1392,7 +1397,8 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 	if (!mmget_not_zero(mm))
 		goto out;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma_prev(mm, start, &amp;prev);
 	if (!vma)
 		goto out_unlock;
<span class="p_chunk">@@ -1505,7 +1511,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 		vma = vma-&gt;vm_next;
 	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);
 out_unlock:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	mmput(mm);
 out:
 	return ret;
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index f60f45fe226f..fcea774be217 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/spinlock.h&gt;
 #include &lt;linux/rbtree.h&gt;
 #include &lt;linux/rwsem.h&gt;
<span class="p_add">+#include &lt;linux/range_rwlock.h&gt;</span>
 #include &lt;linux/completion.h&gt;
 #include &lt;linux/cpumask.h&gt;
 #include &lt;linux/uprobes.h&gt;
<span class="p_chunk">@@ -398,7 +399,7 @@</span> <span class="p_context"> struct mm_struct {</span>
 	int map_count;				/* number of VMAs */
 
 	spinlock_t page_table_lock;		/* Protects page tables and some counters */
<span class="p_del">-	struct rw_semaphore mmap_sem;</span>
<span class="p_add">+	struct range_rwlock_tree mmap_rw_tree;	/* formerly mmap_sem */</span>
 
 	struct list_head mmlist;		/* List of maybe swapped mm&#39;s.	These are globally strung
 						 * together off init_mm.mmlist, and are protected
<span class="p_header">diff --git a/ipc/shm.c b/ipc/shm.c</span>
<span class="p_header">index 481d2a9c298a..f1a885962ac8 100644</span>
<span class="p_header">--- a/ipc/shm.c</span>
<span class="p_header">+++ b/ipc/shm.c</span>
<span class="p_chunk">@@ -1107,6 +1107,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	struct path path;
 	fmode_t f_mode;
 	unsigned long populate = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	err = -EINVAL;
 	if (shmid &lt; 0)
<span class="p_chunk">@@ -1213,7 +1214,9 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	if (err)
 		goto out_fput;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+					   &amp;range)) {</span>
 		err = -EINTR;
 		goto out_fput;
 	}
<span class="p_chunk">@@ -1233,7 +1236,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	if (IS_ERR_VALUE(addr))
 		err = (long)addr;
 invalid:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (populate)
 		mm_populate(addr, populate);
 
<span class="p_chunk">@@ -1284,11 +1287,13 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 	struct file *file;
 	struct vm_area_struct *next;
 #endif
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (addr &amp; ~PAGE_MASK)
 		return retval;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	/*
<span class="p_chunk">@@ -1376,7 +1381,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 
 #endif
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return retval;
 }
 
<span class="p_header">diff --git a/kernel/acct.c b/kernel/acct.c</span>
<span class="p_header">index 5b1284370367..928c1e75c025 100644</span>
<span class="p_header">--- a/kernel/acct.c</span>
<span class="p_header">+++ b/kernel/acct.c</span>
<span class="p_chunk">@@ -534,17 +534,19 @@</span> <span class="p_context"> void acct_collect(long exitcode, int group_dead)</span>
 	struct pacct_struct *pacct = &amp;current-&gt;signal-&gt;pacct;
 	u64 utime, stime;
 	unsigned long vsize = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (group_dead &amp;&amp; current-&gt;mm) {
 		struct vm_area_struct *vma;
 
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = current-&gt;mm-&gt;mmap;
 		while (vma) {
 			vsize += vma-&gt;vm_end - vma-&gt;vm_start;
 			vma = vma-&gt;vm_next;
 		}
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	spin_lock_irq(&amp;current-&gt;sighand-&gt;siglock);
<span class="p_header">diff --git a/kernel/events/core.c b/kernel/events/core.c</span>
<span class="p_header">index ff01cba86f43..4ecf3d5c783b 100644</span>
<span class="p_header">--- a/kernel/events/core.c</span>
<span class="p_header">+++ b/kernel/events/core.c</span>
<span class="p_chunk">@@ -8091,7 +8091,9 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	struct mm_struct *mm = NULL;
 	unsigned int count = 0;
 	unsigned long flags;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	/*
 	 * We may observe TASK_TOMBSTONE, which means that the event tear-down
 	 * will stop on the parent&#39;s child_mutex that our caller is also holding
<span class="p_chunk">@@ -8106,7 +8108,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	if (!mm)
 		goto restart;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	raw_spin_lock_irqsave(&amp;ifh-&gt;lock, flags);
 	list_for_each_entry(filter, &amp;ifh-&gt;list, entry) {
<span class="p_chunk">@@ -8126,7 +8128,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	event-&gt;addr_filters_gen++;
 	raw_spin_unlock_irqrestore(&amp;ifh-&gt;lock, flags);
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	mmput(mm);
 
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index dc2e5f7a8bb8..cbfbd020459e 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -806,11 +806,13 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 	while (info) {
 		struct mm_struct *mm = info-&gt;mm;
 		struct vm_area_struct *vma;
<span class="p_add">+		struct range_rwlock range;</span>
 
 		if (err &amp;&amp; is_register)
 			goto free;
 
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma(mm, info-&gt;vaddr);
 		if (!vma || !valid_vma(vma, is_register) ||
 		    file_inode(vma-&gt;vm_file) != uprobe-&gt;inode)
<span class="p_chunk">@@ -832,7 +834,7 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 		}
 
  unlock:
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
  free:
 		mmput(mm);
 		info = free_map_info(info);
<span class="p_chunk">@@ -971,9 +973,11 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(uprobe_unregister);</span>
 static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	int err = 0;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		unsigned long vaddr;
 		loff_t offset;
<span class="p_chunk">@@ -990,7 +994,7 @@</span> <span class="p_context"> static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
 		vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);
 		err |= remove_breakpoint(uprobe, mm, vaddr);
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return err;
 }
<span class="p_chunk">@@ -1138,9 +1142,11 @@</span> <span class="p_context"> void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned lon</span>
 static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	if (mm-&gt;uprobes_state.xol_area) {
<span class="p_chunk">@@ -1170,7 +1176,7 @@</span> <span class="p_context"> static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)</span>
 	smp_wmb();	/* pairs with get_xol_area() */
 	mm-&gt;uprobes_state.xol_area = area;
  fail:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1736,8 +1742,10 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct uprobe *uprobe = NULL;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(mm, bp_vaddr);
 	if (vma &amp;&amp; vma-&gt;vm_start &lt;= bp_vaddr) {
 		if (valid_vma(vma, false)) {
<span class="p_chunk">@@ -1755,7 +1763,7 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 
 	if (!uprobe &amp;&amp; test_and_clear_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags))
 		mmf_recalc_uprobes(mm);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return uprobe;
 }
<span class="p_header">diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="p_header">index 516acdb0e0ec..f59702a99289 100644</span>
<span class="p_header">--- a/kernel/exit.c</span>
<span class="p_header">+++ b/kernel/exit.c</span>
<span class="p_chunk">@@ -508,6 +508,7 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct core_state *core_state;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	mm_release(current, mm);
 	if (!mm)
<span class="p_chunk">@@ -520,12 +521,13 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 	 * will increment -&gt;nr_threads for each thread in the
 	 * group with -&gt;mm != NULL.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	core_state = mm-&gt;core_state;
 	if (core_state) {
 		struct core_thread self;
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		self.task = current;
 		self.next = xchg(&amp;core_state-&gt;dumper.next, &amp;self);
<span class="p_chunk">@@ -543,14 +545,14 @@</span> <span class="p_context"> static void exit_mm(void)</span>
 			freezable_schedule();
 		}
 		__set_current_state(TASK_RUNNING);
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 	mmgrab(mm);
 	BUG_ON(mm != current-&gt;active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(current);
 	current-&gt;mm = NULL;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
 	mm_update_next_owner(mm);
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 6c463c80e93d..478e85cc3e5c 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -573,9 +573,13 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	int retval;
 	unsigned long charge;
 	LIST_HEAD(uf);
<span class="p_add">+	struct range_rwlock range, oldrange;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;oldrange);</span>
 
 	uprobe_start_dup_mmap();
<span class="p_del">-	if (down_write_killable(&amp;oldmm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;oldmm-&gt;mmap_rw_tree, &amp;oldrange)) {</span>
 		retval = -EINTR;
 		goto fail_uprobe_end;
 	}
<span class="p_chunk">@@ -584,7 +588,7 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */
<span class="p_del">-	down_write_nested(&amp;mm-&gt;mmap_sem, SINGLE_DEPTH_NESTING);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/* No ordering required: file already has been exposed. */
 	RCU_INIT_POINTER(mm-&gt;exe_file, get_mm_exe_file(oldmm));
<span class="p_chunk">@@ -688,9 +692,9 @@</span> <span class="p_context"> static __latent_entropy int dup_mmap(struct mm_struct *mm,</span>
 	arch_dup_mmap(oldmm, mm);
 	retval = 0;
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	flush_tlb_mm(oldmm);
<span class="p_del">-	up_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;oldmm-&gt;mmap_rw_tree, &amp;oldrange);</span>
 	dup_userfaultfd_complete(&amp;uf);
 fail_uprobe_end:
 	uprobe_end_dup_mmap();
<span class="p_chunk">@@ -720,9 +724,12 @@</span> <span class="p_context"> static inline void mm_free_pgd(struct mm_struct *mm)</span>
 #else
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
<span class="p_del">-	down_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	struct range_rwlock oldrange;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;oldrange);</span>
<span class="p_add">+	range_write_lock(&amp;oldmm-&gt;mmap_rw_tree, &amp;oldrange);</span>
 	RCU_INIT_POINTER(mm-&gt;exe_file, get_mm_exe_file(oldmm));
<span class="p_del">-	up_write(&amp;oldmm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;oldmm-&gt;mmap_rw_tree, &amp;oldrange);</span>
 	return 0;
 }
 #define mm_alloc_pgd(mm)	(0)
<span class="p_chunk">@@ -771,7 +778,7 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,</span>
 	mm-&gt;vmacache_seqnum = 0;
 	atomic_set(&amp;mm-&gt;mm_users, 1);
 	atomic_set(&amp;mm-&gt;mm_count, 1);
<span class="p_del">-	init_rwsem(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_tree_init(&amp;mm-&gt;mmap_rw_tree);</span>
 	INIT_LIST_HEAD(&amp;mm-&gt;mmlist);
 	mm-&gt;core_state = NULL;
 	atomic_long_set(&amp;mm-&gt;nr_ptes, 0);
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 4dd1bba09831..0ac51c925fa8 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -723,12 +723,14 @@</span> <span class="p_context"> static inline void put_futex_key(union futex_key *key)</span>
 static int fault_in_user_writeable(u32 __user *uaddr)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX finer grain required here */</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = fixup_user_fault(current, mm, (unsigned long)uaddr,
<span class="p_del">-			       FAULT_FLAG_WRITE, NULL, NULL);</span>
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			       FAULT_FLAG_WRITE, NULL, &amp;range);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret &lt; 0 ? ret : 0;
 }
<span class="p_header">diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c</span>
<span class="p_header">index dea138964b91..a8e4243e8510 100644</span>
<span class="p_header">--- a/kernel/sched/fair.c</span>
<span class="p_header">+++ b/kernel/sched/fair.c</span>
<span class="p_chunk">@@ -2425,6 +2425,7 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 	unsigned long start, end;
 	unsigned long nr_pte_updates = 0;
 	long pages, virtpages;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
 
<span class="p_chunk">@@ -2474,8 +2475,8 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 	if (!pages)
 		return;
 
<span class="p_del">-</span>
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(mm, start);
 	if (!vma) {
 		reset_ptenuma_scan(p);
<span class="p_chunk">@@ -2542,7 +2543,7 @@</span> <span class="p_context"> void task_numa_work(struct callback_head *work)</span>
 		mm-&gt;numa_scan_offset = start;
 	else
 		reset_ptenuma_scan(p);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * Make sure tasks use at least 32x as much time to run other code
<span class="p_header">diff --git a/kernel/sys.c b/kernel/sys.c</span>
<span class="p_header">index 7ff6d1b10cec..4d449281575d 100644</span>
<span class="p_header">--- a/kernel/sys.c</span>
<span class="p_header">+++ b/kernel/sys.c</span>
<span class="p_chunk">@@ -1663,6 +1663,9 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	struct file *old_exe, *exe_file;
 	struct inode *inode;
 	int err;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	exe = fdget(fd);
 	if (!exe.file)
<span class="p_chunk">@@ -1691,7 +1694,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	if (exe_file) {
 		struct vm_area_struct *vma;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 			if (!vma-&gt;vm_file)
 				continue;
<span class="p_chunk">@@ -1700,7 +1703,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 				goto exit_err;
 		}
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		fput(exe_file);
 	}
 
<span class="p_chunk">@@ -1714,7 +1717,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
 	fdput(exe);
 	return err;
 exit_err:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	fput(exe_file);
 	goto exit;
 }
<span class="p_chunk">@@ -1821,6 +1824,9 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 	unsigned long user_auxv[AT_VECTOR_SIZE];
 	struct mm_struct *mm = current-&gt;mm;
 	int error;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	BUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm-&gt;saved_auxv));
 	BUILD_BUG_ON(sizeof(struct prctl_mm_map) &gt; 256);
<span class="p_chunk">@@ -1857,7 +1863,7 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 			return error;
 	}
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * We don&#39;t validate if these members are pointing to
<span class="p_chunk">@@ -1894,7 +1900,7 @@</span> <span class="p_context"> static int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data</span>
 	if (prctl_map.auxv_size)
 		memcpy(mm-&gt;saved_auxv, user_auxv, sizeof(user_auxv));
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return 0;
 }
 #endif /* CONFIG_CHECKPOINT_RESTORE */
<span class="p_chunk">@@ -1936,6 +1942,9 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 	struct prctl_mm_map prctl_map;
 	struct vm_area_struct *vma;
 	int error;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	if (arg5 || (arg4 &amp;&amp; (opt != PR_SET_MM_AUXV &amp;&amp;
 			      opt != PR_SET_MM_MAP &amp;&amp;
<span class="p_chunk">@@ -1961,7 +1970,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 
 	error = -EINVAL;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(mm, addr);
 
 	prctl_map.start_code	= mm-&gt;start_code;
<span class="p_chunk">@@ -2054,7 +2063,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 
 	error = 0;
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -2094,6 +2103,9 @@</span> <span class="p_context"> SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,</span>
 	struct task_struct *me = current;
 	unsigned char comm[sizeof(me-&gt;comm)];
 	long error;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	error = security_task_prctl(option, arg2, arg3, arg4, arg5);
 	if (error != -ENOSYS)
<span class="p_chunk">@@ -2266,13 +2278,14 @@</span> <span class="p_context"> SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,</span>
 	case PR_SET_THP_DISABLE:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
<span class="p_del">-		if (down_write_killable(&amp;me-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (range_write_lock_interruptible(&amp;me-&gt;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+						   &amp;range))</span>
 			return -EINTR;
 		if (arg2)
 			me-&gt;mm-&gt;def_flags |= VM_NOHUGEPAGE;
 		else
 			me-&gt;mm-&gt;def_flags &amp;= ~VM_NOHUGEPAGE;
<span class="p_del">-		up_write(&amp;me-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;me-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		break;
 	case PR_MPX_ENABLE_MANAGEMENT:
 		if (arg2 || arg3 || arg4 || arg5)
<span class="p_header">diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c</span>
<span class="p_header">index 02a4aeb22c47..ad8b11c47855 100644</span>
<span class="p_header">--- a/kernel/trace/trace_output.c</span>
<span class="p_header">+++ b/kernel/trace/trace_output.c</span>
<span class="p_chunk">@@ -380,6 +380,7 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 	struct file *file = NULL;
 	unsigned long vmstart = 0;
 	int ret = 1;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (s-&gt;full)
 		return 0;
<span class="p_chunk">@@ -387,7 +388,8 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 	if (mm) {
 		const struct vm_area_struct *vma;
 
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma(mm, ip);
 		if (vma) {
 			file = vma-&gt;vm_file;
<span class="p_chunk">@@ -399,7 +401,7 @@</span> <span class="p_context"> static int seq_print_user_ip(struct trace_seq *s, struct mm_struct *mm,</span>
 				trace_seq_printf(s, &quot;[+0x%lx]&quot;,
 						 ip - vmstart);
 		}
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 	if (ret &amp;&amp; ((sym_flags &amp; TRACE_ITER_SYM_ADDR) || !file))
 		trace_seq_printf(s, &quot; &lt;&quot; IP_FMT &quot;&gt;&quot;, ip);
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 3a5945f2fd3c..50518a2f8dde 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -1063,7 +1063,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 		if (flags &amp; FAULT_FLAG_RETRY_NOWAIT)
 			return 0;
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		if (flags &amp; FAULT_FLAG_KILLABLE)
 			wait_on_page_locked_killable(page);
 		else
<span class="p_chunk">@@ -1075,7 +1075,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 
 			ret = __lock_page_killable(page);
 			if (ret) {
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 				return 0;
 			}
 		} else
<span class="p_header">diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="p_header">index 579d1cbe039c..ea0014a513a0 100644</span>
<span class="p_header">--- a/mm/frame_vector.c</span>
<span class="p_header">+++ b/mm/frame_vector.c</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret = 0;
 	int err;
 	int locked;
<span class="p_chunk">@@ -45,7 +46,8 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	if (WARN_ON_ONCE(nr_frames &gt; vec-&gt;nr_allocated))
 		nr_frames = vec-&gt;nr_allocated;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	locked = 1;
 	vma = find_vma_intersection(mm, start, start + 1);
 	if (!vma) {
<span class="p_chunk">@@ -56,7 +58,8 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 		vec-&gt;got_ref = true;
 		vec-&gt;is_pfns = false;
 		ret = get_user_pages_locked(start, nr_frames,
<span class="p_del">-			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked, NULL);</span>
<span class="p_add">+			gup_flags, (struct page **)(vec-&gt;ptrs),</span>
<span class="p_add">+			&amp;locked, &amp;range);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -85,7 +88,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	} while (vma &amp;&amp; vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP));
 out:
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (!ret)
 		ret = -EFAULT;
 	if (ret &gt; 0)
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index ad83cfa38649..218cc0b8c032 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -735,7 +735,7 @@</span> <span class="p_context"> int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
 	}
 
 	if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		if (!(fault_flags &amp; FAULT_FLAG_TRIED)) {
 			*unlocked = true;
 			fault_flags &amp;= ~FAULT_FLAG_ALLOW_RETRY;
<span class="p_chunk">@@ -819,7 +819,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 */
 		*locked = 1;
 		lock_dropped = true;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
 				       pages, NULL, NULL, range);
 		if (ret != 1) {
<span class="p_chunk">@@ -840,7 +840,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 * We must let the caller know we temporarily dropped the lock
 		 * and so the critical section protected by it was lost.
 		 */
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		*locked = 0;
 	}
 	return pages_done;
<span class="p_chunk">@@ -892,12 +892,14 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 {
 	long ret;
 	int locked = 1;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = __get_user_pages_locked(tsk, mm, start, nr_pages, pages, NULL,
<span class="p_del">-				      &amp;locked, false, NULL, gup_flags);</span>
<span class="p_add">+				      &amp;locked, false, &amp;range, gup_flags);</span>
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1081,6 +1083,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 	struct vm_area_struct *vma = NULL;
 	int locked = 0;
 	long ret = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	VM_BUG_ON(start &amp; ~PAGE_MASK);
 	VM_BUG_ON(len != PAGE_ALIGN(len));
<span class="p_chunk">@@ -1093,7 +1096,8 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 */
 		if (!locked) {
 			locked = 1;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+			range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			vma = find_vma(mm, nstart);
 		} else if (nstart &gt;= vma-&gt;vm_end)
 			vma = vma-&gt;vm_next;
<span class="p_chunk">@@ -1114,7 +1118,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 * if the vma was already munlocked.
 		 */
 		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked,
<span class="p_del">-					      NULL);</span>
<span class="p_add">+					      &amp;range);</span>
 		if (ret &lt; 0) {
 			if (ignore_errors) {
 				ret = 0;
<span class="p_chunk">@@ -1126,7 +1130,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		ret = 0;
 	}
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;	/* 0 or negative error code */
 }
 
<span class="p_header">diff --git a/mm/init-mm.c b/mm/init-mm.c</span>
<span class="p_header">index 975e49f00f34..7d980728ba26 100644</span>
<span class="p_header">--- a/mm/init-mm.c</span>
<span class="p_header">+++ b/mm/init-mm.c</span>
<span class="p_chunk">@@ -19,7 +19,7 @@</span> <span class="p_context"> struct mm_struct init_mm = {</span>
 	.pgd		= swapper_pg_dir,
 	.mm_users	= ATOMIC_INIT(2),
 	.mm_count	= ATOMIC_INIT(1),
<span class="p_del">-	.mmap_sem	= __RWSEM_INITIALIZER(init_mm.mmap_sem),</span>
<span class="p_add">+	.mmap_rw_tree	= __RANGE_RWLOCK_TREE_INITIALIZER(init_mm.mmap_rw_tree),</span>
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
 	.user_ns	= &amp;init_user_ns,
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index d2b2a06f7853..2e40c4449166 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -453,6 +453,9 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 {
 	struct mm_slot *mm_slot;
 	int free = 0;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	spin_lock(&amp;khugepaged_mm_lock);
 	mm_slot = get_mm_slot(mm);
<span class="p_chunk">@@ -476,8 +479,8 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 		 * khugepaged has finished working on the pagetables
 		 * under the mmap_sem.
 		 */
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 }
 
<span class="p_chunk">@@ -904,7 +907,7 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 
 		/* do_swap_page returns VM_FAULT_RETRY with released mmap_sem */
 		if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_lock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 			if (hugepage_vma_revalidate(mm, address, &amp;vmf.vma)) {
 				/* vma is no longer available, don&#39;t continue to swapin */
 				trace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, 0);
<span class="p_chunk">@@ -956,7 +959,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * sync compaction, and we do not need to hold the mmap_sem during
 	 * that. We will recheck the vma after taking it again in write mode.
 	 */
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
 	if (!new_page) {
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
<span class="p_chunk">@@ -968,11 +971,11 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -980,7 +983,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (!pmd) {
 		result = SCAN_PMD_NULL;
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -992,17 +995,17 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced,
 					 range)) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 	/*
 	 * Prevent all access to pagetables with the exception of
 	 * gup_fast later handled by the ptep_clear_flush and the VM
 	 * handled by the anon_vma lock + PG_lock.
 	 */
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result)
 		goto out;
<span class="p_chunk">@@ -1085,7 +1088,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	khugepaged_pages_collapsed++;
 	result = SCAN_SUCCEED;
 out_up_write:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, range);</span>
 out_nolock:
 	trace_mm_collapse_huge_page(mm, isolated, result);
 	return;
<span class="p_chunk">@@ -1249,6 +1252,9 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 	struct vm_area_struct *vma;
 	unsigned long addr;
 	pmd_t *pmd, _pmd;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX finer grain doable here */</span>
 
 	i_mmap_lock_write(mapping);
 	vma_interval_tree_foreach(vma, &amp;mapping-&gt;i_mmap, pgoff, pgoff) {
<span class="p_chunk">@@ -1269,12 +1275,12 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 		 * re-fault. Not ideal, but it&#39;s more important to not disturb
 		 * the system too much.
 		 */
<span class="p_del">-		if (down_write_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (range_write_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 			spinlock_t *ptl = pmd_lock(vma-&gt;vm_mm, pmd);
 			/* assume page table is clear */
 			_pmd = pmdp_collapse_flush(vma, addr, pmd);
 			spin_unlock(ptl);
<span class="p_del">-			up_write(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_write_unlock(&amp;vma-&gt;vm_mm-&gt;mmap_rw_tree, &amp;range);</span>
 			atomic_long_dec(&amp;vma-&gt;vm_mm-&gt;nr_ptes);
 			pte_free(vma-&gt;vm_mm, pmd_pgtable(_pmd));
 		}
<span class="p_chunk">@@ -1664,6 +1670,9 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int progress = 0;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX is finer grain doable here ? */</span>
 
 	VM_BUG_ON(!pages);
 	VM_BUG_ON(NR_CPUS != 1 &amp;&amp; !spin_is_locked(&amp;khugepaged_mm_lock));
<span class="p_chunk">@@ -1679,7 +1688,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 	spin_unlock(&amp;khugepaged_mm_lock);
 
 	mm = mm_slot-&gt;mm;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (unlikely(khugepaged_test_exit(mm)))
 		vma = NULL;
 	else
<span class="p_chunk">@@ -1725,7 +1734,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 				if (!shmem_huge_enabled(vma))
 					goto skip;
 				file = get_file(vma-&gt;vm_file);
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 				ret = 1;
 				khugepaged_scan_shmem(mm, file-&gt;f_mapping,
 						pgoff, hpage);
<span class="p_chunk">@@ -1733,7 +1742,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 			} else {
 				ret = khugepaged_scan_pmd(mm, vma,
 						khugepaged_scan.address,
<span class="p_del">-						hpage, NULL);</span>
<span class="p_add">+						hpage, &amp;range);</span>
 			}
 			/* move to next address */
 			khugepaged_scan.address += HPAGE_PMD_SIZE;
<span class="p_chunk">@@ -1746,7 +1755,8 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 		}
 	}
 breakouterloop:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem); /* exit_mmap will destroy ptes after this */</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+			  &amp;range); /* exit_mmap will destroy ptes after this */</span>
 breakouterloop_mmap_sem:
 
 	spin_lock(&amp;khugepaged_mm_lock);
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index c419f53912ba..f4e16b6f960e 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -447,18 +447,20 @@</span> <span class="p_context"> static void break_cow(struct rmap_item *rmap_item)</span>
 	struct mm_struct *mm = rmap_item-&gt;mm;
 	unsigned long addr = rmap_item-&gt;address;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	/*
 	 * It is not an accident that whenever we want to break COW
 	 * to undo, we also need to drop a reference to the anon_vma.
 	 */
 	put_anon_vma(rmap_item-&gt;anon_vma);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_mergeable_vma(mm, addr);
 	if (vma)
 		break_ksm(vma, addr);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 static struct page *get_mergeable_page(struct rmap_item *rmap_item)
<span class="p_chunk">@@ -467,8 +469,10 @@</span> <span class="p_context"> static struct page *get_mergeable_page(struct rmap_item *rmap_item)</span>
 	unsigned long addr = rmap_item-&gt;address;
 	struct vm_area_struct *vma;
 	struct page *page;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_mergeable_vma(mm, addr);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -484,7 +488,7 @@</span> <span class="p_context"> static struct page *get_mergeable_page(struct rmap_item *rmap_item)</span>
 out:
 		page = NULL;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return page;
 }
 
<span class="p_chunk">@@ -775,7 +779,9 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int err = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	spin_lock(&amp;ksm_mmlist_lock);
 	ksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,
 						struct mm_slot, mm_list);
<span class="p_chunk">@@ -784,7 +790,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	for (mm_slot = ksm_scan.mm_slot;
 			mm_slot != &amp;ksm_mm_head; mm_slot = ksm_scan.mm_slot) {
 		mm = mm_slot-&gt;mm;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 			if (ksm_test_exit(mm))
 				break;
<span class="p_chunk">@@ -797,7 +803,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 		}
 
 		remove_trailing_rmap_items(mm_slot, &amp;mm_slot-&gt;rmap_list);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		spin_lock(&amp;ksm_mmlist_lock);
 		ksm_scan.mm_slot = list_entry(mm_slot-&gt;mm_list.next,
<span class="p_chunk">@@ -820,7 +826,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	return 0;
 
 error:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	spin_lock(&amp;ksm_mmlist_lock);
 	ksm_scan.mm_slot = &amp;ksm_mm_head;
 	spin_unlock(&amp;ksm_mmlist_lock);
<span class="p_chunk">@@ -1088,8 +1094,11 @@</span> <span class="p_context"> static int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,</span>
 	struct mm_struct *mm = rmap_item-&gt;mm;
 	struct vm_area_struct *vma;
 	int err = -EFAULT;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX finer grain required here */</span>
<span class="p_add">+</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_mergeable_vma(mm, rmap_item-&gt;address);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -1105,7 +1114,7 @@</span> <span class="p_context"> static int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,</span>
 	rmap_item-&gt;anon_vma = vma-&gt;anon_vma;
 	get_anon_vma(vma-&gt;anon_vma);
 out:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1579,6 +1588,9 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 	struct vm_area_struct *vma;
 	struct rmap_item *rmap_item;
 	int nid;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);	/* XXX finer grain required here */</span>
 
 	if (list_empty(&amp;ksm_mm_head.mm_list))
 		return NULL;
<span class="p_chunk">@@ -1635,7 +1647,7 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 	}
 
 	mm = slot-&gt;mm;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (ksm_test_exit(mm))
 		vma = NULL;
 	else
<span class="p_chunk">@@ -1669,7 +1681,7 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 					ksm_scan.address += PAGE_SIZE;
 				} else
 					put_page(*page);
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 				return rmap_item;
 			}
 			put_page(*page);
<span class="p_chunk">@@ -1707,10 +1719,10 @@</span> <span class="p_context"> static struct rmap_item *scan_get_next_rmap_item(struct page **page)</span>
 
 		free_mm_slot(slot);
 		clear_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		mmdrop(mm);
 	} else {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		/*
 		 * up_read(&amp;mm-&gt;mmap_sem) first because after
 		 * spin_unlock(&amp;ksm_mmlist_lock) run, the &quot;mm&quot; may
<span class="p_chunk">@@ -1869,6 +1881,9 @@</span> <span class="p_context"> void __ksm_exit(struct mm_struct *mm)</span>
 {
 	struct mm_slot *mm_slot;
 	int easy_to_free = 0;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	/*
 	 * This process is exiting: if it&#39;s straightforward (as is the
<span class="p_chunk">@@ -1898,8 +1913,8 @@</span> <span class="p_context"> void __ksm_exit(struct mm_struct *mm)</span>
 		clear_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags);
 		mmdrop(mm);
 	} else if (mm_slot) {
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 }
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 7eb62e3995ca..05442bb8b9c3 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -517,7 +517,7 @@</span> <span class="p_context"> static long madvise_dontneed(struct vm_area_struct *vma,</span>
 	if (!userfaultfd_remove(vma, start, end, range)) {
 		*prev = NULL; /* mmap_sem has been dropped, prev is stale */
 
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, range);</span>
 		vma = find_vma(current-&gt;mm, start);
 		if (!vma)
 			return -ENOMEM;
<span class="p_chunk">@@ -560,8 +560,9 @@</span> <span class="p_context"> static long madvise_dontneed(struct vm_area_struct *vma,</span>
  * This is effectively punching a hole into the middle of a file.
  */
 static long madvise_remove(struct vm_area_struct *vma,
<span class="p_del">-				struct vm_area_struct **prev,</span>
<span class="p_del">-				unsigned long start, unsigned long end)</span>
<span class="p_add">+			   struct vm_area_struct **prev,</span>
<span class="p_add">+			   unsigned long start, unsigned long end,</span>
<span class="p_add">+			   struct range_rwlock *range)</span>
 {
 	loff_t offset;
 	int error;
<span class="p_chunk">@@ -591,15 +592,15 @@</span> <span class="p_context"> static long madvise_remove(struct vm_area_struct *vma,</span>
 	 * mmap_sem.
 	 */
 	get_file(f);
<span class="p_del">-	if (userfaultfd_remove(vma, start, end, NULL) {</span>
<span class="p_add">+	if (userfaultfd_remove(vma, start, end, range)) {</span>
 		/* mmap_sem was not released by userfaultfd_remove() */
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, range);</span>
 	}
 	error = vfs_fallocate(f,
 				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
 				offset, end - start);
 	fput(f);
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -649,7 +650,7 @@</span> <span class="p_context"> madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 {
 	switch (behavior) {
 	case MADV_REMOVE:
<span class="p_del">-		return madvise_remove(vma, prev, start, end);</span>
<span class="p_add">+		return madvise_remove(vma, prev, start, end, range);</span>
 	case MADV_WILLNEED:
 		return madvise_willneed(vma, prev, start, end);
 	case MADV_FREE:
<span class="p_chunk">@@ -762,6 +763,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 	int write;
 	size_t len;
 	struct blk_plug plug;
<span class="p_add">+	struct range_rwlock range;</span>
 
 #ifdef CONFIG_MEMORY_FAILURE
 	if (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)
<span class="p_chunk">@@ -786,12 +788,14 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 	if (end == start)
 		return error;
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	write = madvise_need_mmap_write(behavior);
 	if (write) {
<span class="p_del">-		if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+						   &amp;range))</span>
 			return -EINTR;
 	} else {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	/*
<span class="p_chunk">@@ -824,7 +828,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 			tmp = end;
 
 		/* Here vma-&gt;vm_start &lt;= start &lt; tmp &lt;= (end|vma-&gt;vm_end). */
<span class="p_del">-		error = madvise_vma(vma, &amp;prev, start, tmp, behavior, NULL);</span>
<span class="p_add">+		error = madvise_vma(vma, &amp;prev, start, tmp, behavior, &amp;range);</span>
 		if (error)
 			goto out;
 		start = tmp;
<span class="p_chunk">@@ -841,9 +845,9 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 out:
 	blk_finish_plug(&amp;plug);
 	if (write)
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	else
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return error;
 }
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 2bd7541d7c11..cfa5e6623d4e 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4687,15 +4687,17 @@</span> <span class="p_context"> static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
 static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)
 {
 	unsigned long precharge;
<span class="p_del">-</span>
 	struct mm_walk mem_cgroup_count_precharge_walk = {
 		.pmd_entry = mem_cgroup_count_precharge_pte_range,
 		.mm = mm,
 	};
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	walk_page_range(0, mm-&gt;highest_vm_end,
 			&amp;mem_cgroup_count_precharge_walk);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	precharge = mc.precharge;
 	mc.precharge = 0;
<span class="p_chunk">@@ -4956,7 +4958,9 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 		.pmd_entry = mem_cgroup_move_charge_pte_range,
 		.mm = mc.mm,
 	};
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	lru_add_drain_all();
 	/*
 	 * Signal lock_page_memcg() to take the memcg&#39;s move_lock
<span class="p_chunk">@@ -4966,7 +4970,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	atomic_inc(&amp;mc.from-&gt;moving_account);
 	synchronize_rcu();
 retry:
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mc.mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	if (unlikely(!range_read_trylock(&amp;mc.mm-&gt;mmap_rw_tree, &amp;range))) {</span>
 		/*
 		 * Someone who are holding the mmap_sem might be waiting in
 		 * waitq. So we cancel all extra charges, wake up all waiters,
<span class="p_chunk">@@ -4984,7 +4988,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	 */
 	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk);
 
<span class="p_del">-	up_read(&amp;mc.mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mc.mm-&gt;mmap_rw_tree, &amp;range);</span>
 	atomic_dec(&amp;mc.from-&gt;moving_account);
 }
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 9adb7d4396bf..45fad119fc21 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1630,12 +1630,16 @@</span> <span class="p_context"> static int insert_page(struct vm_area_struct *vma, unsigned long addr,</span>
 int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
 			struct page *page)
 {
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+</span>
 	if (addr &lt; vma-&gt;vm_start || addr &gt;= vma-&gt;vm_end)
 		return -EFAULT;
 	if (!page_count(page))
 		return -EINVAL;
 	if (!(vma-&gt;vm_flags &amp; VM_MIXEDMAP)) {
<span class="p_del">-		BUG_ON(down_read_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem));</span>
<span class="p_add">+		BUG_ON(range_read_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_rw_tree, &amp;range));</span>
 		BUG_ON(vma-&gt;vm_flags &amp; VM_PFNMAP);
 		vma-&gt;vm_flags |= VM_MIXEDMAP;
 	}
<span class="p_chunk">@@ -4160,8 +4164,10 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags &amp; FOLL_WRITE;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
<span class="p_chunk">@@ -4169,7 +4175,8 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		struct page *page = NULL;
 
 		ret = get_user_pages_remote(tsk, mm, addr, 1,
<span class="p_del">-					    gup_flags, &amp;page, &amp;vma, NULL, NULL);</span>
<span class="p_add">+					    gup_flags, &amp;page, &amp;vma, NULL,</span>
<span class="p_add">+					    NULL /* mm range lock untouched */);</span>
 		if (ret &lt;= 0) {
 #ifndef CONFIG_HAVE_IOREMAP_PROT
 			break;
<span class="p_chunk">@@ -4210,7 +4217,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		buf += bytes;
 		addr += bytes;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return buf - old_buf;
 }
<span class="p_chunk">@@ -4261,6 +4268,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/*
 	 * Do not print if we are in atomic
<span class="p_chunk">@@ -4269,7 +4277,8 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 	if (preempt_count())
 		return;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(mm, ip);
 	if (vma &amp;&amp; vma-&gt;vm_file) {
 		struct file *f = vma-&gt;vm_file;
<span class="p_chunk">@@ -4286,7 +4295,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 			free_page((unsigned long)buf);
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 #if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 0658c7240e54..69e3b5bb9406 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -445,11 +445,13 @@</span> <span class="p_context"> void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new,</span>
 void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)
 		mpol_rebind_policy(vma-&gt;vm_policy, new, MPOL_REBIND_ONCE);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 static const struct mempolicy_operations mpol_ops[MPOL_MAX] = {
<span class="p_chunk">@@ -871,6 +873,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma = NULL;
 	struct mempolicy *pol = current-&gt;mempolicy;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (flags &amp;
 		~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))
<span class="p_chunk">@@ -892,10 +895,11 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 		 * vma/shared policy at addr is NULL.  We
 		 * want to return MPOL_DEFAULT in this case.
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma_intersection(mm, addr, addr+1);
 		if (!vma) {
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			return -EFAULT;
 		}
 		if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;get_policy)
<span class="p_chunk">@@ -932,7 +936,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 	}
 
 	if (vma) {
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = NULL;
 	}
 
<span class="p_chunk">@@ -950,7 +954,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
  out:
 	mpol_cond_put(pol);
 	if (vma)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1028,12 +1032,14 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 	int busy = 0;
 	int err;
 	nodemask_t tmp;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	err = migrate_prep();
 	if (err)
 		return err;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * Find a &#39;source&#39; bit set in &#39;tmp&#39; whose corresponding &#39;dest&#39;
<span class="p_chunk">@@ -1114,7 +1120,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 		if (err &lt; 0)
 			break;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (err &lt; 0)
 		return err;
 	return busy;
<span class="p_chunk">@@ -1178,7 +1184,9 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	unsigned long end;
 	int err;
 	LIST_HEAD(pagelist);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	if (flags &amp; ~(unsigned long)MPOL_MF_VALID)
 		return -EINVAL;
 	if ((flags &amp; MPOL_MF_MOVE_ALL) &amp;&amp; !capable(CAP_SYS_NICE))
<span class="p_chunk">@@ -1225,12 +1233,12 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	{
 		NODEMASK_SCRATCH(scratch);
 		if (scratch) {
<span class="p_del">-			down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			task_lock(current);
 			err = mpol_set_nodemask(new, nmask, scratch);
 			task_unlock(current);
 			if (err)
<span class="p_del">-				up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		} else
 			err = -ENOMEM;
 		NODEMASK_SCRATCH_FREE(scratch);
<span class="p_chunk">@@ -1259,7 +1267,7 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	} else
 		putback_movable_pages(&amp;pagelist);
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
  mpol_out:
 	mpol_put(new);
 	return err;
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index ed97c2c14fa8..0ee1409c1723 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1405,8 +1405,10 @@</span> <span class="p_context"> static int do_move_page_to_node_array(struct mm_struct *mm,</span>
 	int err;
 	struct page_to_node *pp;
 	LIST_HEAD(pagelist);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * Build a list of pages to migrate
<span class="p_chunk">@@ -1477,7 +1479,7 @@</span> <span class="p_context"> static int do_move_page_to_node_array(struct mm_struct *mm,</span>
 			putback_movable_pages(&amp;pagelist);
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1575,8 +1577,10 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 				const void __user **pages, int *status)
 {
 	unsigned long i;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	for (i = 0; i &lt; nr_pages; i++) {
 		unsigned long addr = (unsigned long)(*pages);
<span class="p_chunk">@@ -1603,7 +1607,7 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 		status++;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="p_header">index c5687c45c326..3d455eb4fa35 100644</span>
<span class="p_header">--- a/mm/mincore.c</span>
<span class="p_header">+++ b/mm/mincore.c</span>
<span class="p_chunk">@@ -226,6 +226,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 	long retval;
 	unsigned long pages;
 	unsigned char *tmp;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Check the start address: needs to be page-aligned.. */
 	if (start &amp; ~PAGE_MASK)
<span class="p_chunk">@@ -252,9 +253,10 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 		 * Do at most PAGE_SIZE entries per iteration, due to
 		 * the temporary buffer size.
 		 */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 		if (retval &lt;= 0)
 			break;
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index 0dd9ca18e19e..92028e885ba1 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -668,6 +668,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	unsigned long locked;
 	unsigned long lock_limit;
 	int error = -ENOMEM;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (!can_do_mlock())
 		return -EPERM;
<span class="p_chunk">@@ -681,7 +682,8 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	lock_limit &gt;&gt;= PAGE_SHIFT;
 	locked = len &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	locked += current-&gt;mm-&gt;locked_vm;
<span class="p_chunk">@@ -700,7 +702,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 	if ((locked &lt;= lock_limit) || capable(CAP_IPC_LOCK))
 		error = apply_vma_lock_flags(start, len, flags);
 
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (error)
 		return error;
 
<span class="p_chunk">@@ -731,14 +733,16 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)</span>
 SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)
 {
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	len = PAGE_ALIGN(len + (offset_in_page(start)));
 	start &amp;= PAGE_MASK;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 	ret = apply_vma_lock_flags(start, len, 0);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -793,6 +797,9 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 {
 	unsigned long lock_limit;
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))
 		return -EINVAL;
<span class="p_chunk">@@ -806,14 +813,14 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 	lock_limit = rlimit(RLIMIT_MEMLOCK);
 	lock_limit &gt;&gt;= PAGE_SHIFT;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	ret = -ENOMEM;
 	if (!(flags &amp; MCL_CURRENT) || (current-&gt;mm-&gt;total_vm &lt;= lock_limit) ||
 	    capable(CAP_IPC_LOCK))
 		ret = apply_mlockall_flags(flags);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (!ret &amp;&amp; (flags &amp; MCL_CURRENT))
 		mm_populate(0, TASK_SIZE);
 
<span class="p_chunk">@@ -823,11 +830,13 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 SYSCALL_DEFINE0(munlockall)
 {
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 	ret = apply_mlockall_flags(0);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 4df13e633e92..e31d2bb6a245 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -186,8 +186,11 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long min_brk;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* we should do better here */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 #ifdef CONFIG_COMPAT_BRK
<span class="p_chunk">@@ -239,7 +242,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 set_brk:
 	mm-&gt;brk = brk;
 	populate = newbrk &gt; oldbrk &amp;&amp; (mm-&gt;def_flags &amp; VM_LOCKED) != 0;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate)
 		mm_populate(oldbrk, newbrk - oldbrk);
<span class="p_chunk">@@ -247,7 +250,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 
 out:
 	retval = mm-&gt;brk;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -2681,12 +2684,15 @@</span> <span class="p_context"> int vm_munmap(unsigned long start, size_t len)</span>
 	int ret;
 	struct mm_struct *mm = current-&gt;mm;
 	LIST_HEAD(uf);
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	ret = do_munmap(mm, start, len, &amp;uf);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	return ret;
 }
<span class="p_chunk">@@ -2711,6 +2717,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	unsigned long populate = 0;
 	unsigned long ret = -EINVAL;
 	struct file *file;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	pr_warn_once(&quot;%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.txt.\n&quot;,
 		     current-&gt;comm, current-&gt;pid);
<span class="p_chunk">@@ -2727,7 +2734,8 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	if (pgoff + (size &gt;&gt; PAGE_SHIFT) &lt; pgoff)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	vma = find_vma(mm, start);
<span class="p_chunk">@@ -2790,7 +2798,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 			prot, flags, pgoff, &amp;populate, NULL);
 	fput(file);
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (populate)
 		mm_populate(ret, populate);
 	if (!IS_ERR_VALUE(ret))
<span class="p_chunk">@@ -2801,9 +2809,12 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 static inline void verify_mm_writelocked(struct mm_struct *mm)
 {
 #ifdef CONFIG_DEBUG_VM
<span class="p_del">-	if (unlikely(down_read_trylock(&amp;mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (unlikely(range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range))) {</span>
 		WARN_ON(1);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 #endif
 }
<span class="p_chunk">@@ -2910,13 +2921,15 @@</span> <span class="p_context"> int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
 	int ret;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXXX apply finer grain lock */</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	ret = do_brk_flags(addr, len, flags, &amp;uf);
 	populate = ((mm-&gt;def_flags &amp; VM_LOCKED) != 0);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate &amp;&amp; !ret)
 		mm_populate(addr, len);
<span class="p_chunk">@@ -3359,8 +3372,10 @@</span> <span class="p_context"> int mm_take_all_locks(struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	BUG_ON(down_read_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	BUG_ON(range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range));</span>
 
 	mutex_lock(&amp;mm_all_locks_mutex);
 
<span class="p_chunk">@@ -3439,8 +3454,10 @@</span> <span class="p_context"> void mm_drop_all_locks(struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	BUG_ON(down_read_trylock(&amp;mm-&gt;mmap_sem));</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	BUG_ON(range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range));</span>
 	BUG_ON(!mutex_is_locked(&amp;mm_all_locks_mutex));
 
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_header">diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="p_header">index a7652acd2ab9..1a61278116dc 100644</span>
<span class="p_header">--- a/mm/mmu_notifier.c</span>
<span class="p_header">+++ b/mm/mmu_notifier.c</span>
<span class="p_chunk">@@ -249,7 +249,9 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 {
 	struct mmu_notifier_mm *mmu_notifier_mm;
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_users) &lt;= 0);
 
 	/*
<span class="p_chunk">@@ -264,7 +266,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 		goto out;
 
 	if (take_mmap_sem)
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = mm_take_all_locks(mm);
 	if (unlikely(ret))
 		goto out_clean;
<span class="p_chunk">@@ -293,7 +295,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 	mm_drop_all_locks(mm);
 out_clean:
 	if (take_mmap_sem)
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	kfree(mmu_notifier_mm);
 out:
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_users) &lt;= 0);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index fef798619b06..fbafcc30b252 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -383,6 +383,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 	const int grows = prot &amp; (PROT_GROWSDOWN|PROT_GROWSUP);
 	const bool rier = (current-&gt;personality &amp; READ_IMPLIES_EXEC) &amp;&amp;
 				(prot &amp; PROT_READ);
<span class="p_add">+	struct range_rwlock range;</span>
 
 	prot &amp;= ~(PROT_GROWSDOWN|PROT_GROWSUP);
 	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can&#39;t be both */
<span class="p_chunk">@@ -401,7 +402,8 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 
 	reqprot = prot;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	/*
<span class="p_chunk">@@ -491,7 +493,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 		prot = reqprot;
 	}
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -513,6 +515,9 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 {
 	int pkey;
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	/* No flags supported yet. */
 	if (flags)
<span class="p_chunk">@@ -521,7 +526,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	if (init_val &amp; ~PKEY_ACCESS_MASK)
 		return -EINVAL;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	pkey = mm_pkey_alloc(current-&gt;mm);
 
 	ret = -ENOSPC;
<span class="p_chunk">@@ -535,17 +540,19 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	}
 	ret = pkey;
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
 SYSCALL_DEFINE1(pkey_free, int, pkey)
 {
 	int ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = mm_pkey_free(current-&gt;mm, pkey);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * We could provie warnings or errors if any VMA still
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index cd8a1b199ef9..0565fa644da7 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -515,6 +515,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	bool locked = false;
 	struct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;
 	LIST_HEAD(uf_unmap);
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (flags &amp; ~(MREMAP_FIXED | MREMAP_MAYMOVE))
 		return ret;
<span class="p_chunk">@@ -536,7 +537,8 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	if (!new_len)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX should be finer grain */</span>
<span class="p_add">+	if (range_write_lock_interruptible(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range))</span>
 		return -EINTR;
 
 	if (flags &amp; MREMAP_FIXED) {
<span class="p_chunk">@@ -618,7 +620,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		vm_unacct_memory(charged);
 		locked = 0;
 	}
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (locked &amp;&amp; new_len &gt; old_len)
 		mm_populate(new_addr + old_len, new_len - old_len);
 	mremap_userfaultfd_complete(&amp;uf, addr, new_addr, old_len);
<span class="p_header">diff --git a/mm/msync.c b/mm/msync.c</span>
<span class="p_header">index 24e612fefa04..4e2554b2bc55 100644</span>
<span class="p_header">--- a/mm/msync.c</span>
<span class="p_header">+++ b/mm/msync.c</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	struct vm_area_struct *vma;
 	int unmapped_error = 0;
 	int error = -EINVAL;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	if (flags &amp; ~(MS_ASYNC | MS_INVALIDATE | MS_SYNC))
 		goto out;
<span class="p_chunk">@@ -54,7 +55,8 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	 * If the interval [start,end) covers some unmapped address ranges,
 	 * just ignore them, but return -ENOMEM at the end.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(mm, start);
 	for (;;) {
 		struct file *file;
<span class="p_chunk">@@ -85,12 +87,12 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		if ((flags &amp; MS_SYNC) &amp;&amp; file &amp;&amp;
 				(vma-&gt;vm_flags &amp; VM_SHARED)) {
 			get_file(file);
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			error = vfs_fsync_range(file, fstart, fend, 1);
 			fput(file);
 			if (error || start &gt;= end)
 				goto out;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 			vma = find_vma(mm, start);
 		} else {
 			if (start &gt;= end) {
<span class="p_chunk">@@ -101,7 +103,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		}
 	}
 out_unlock:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 out:
 	return error ? : unmapped_error;
 }
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index 2d131b97a851..c75c0a2ac835 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -183,10 +183,13 @@</span> <span class="p_context"> static long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 			unsigned int gup_flags)
 {
 	long ret;
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,
 				NULL, NULL);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -245,12 +248,14 @@</span> <span class="p_context"> void *vmalloc_user(unsigned long size)</span>
 			PAGE_KERNEL);
 	if (ret) {
 		struct vm_area_struct *vma;
<span class="p_add">+		struct range_rwlock range;</span>
 
<span class="p_del">-		down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		vma = find_vma(current-&gt;mm, (unsigned long)ret);
 		if (vma)
 			vma-&gt;vm_flags |= VM_USERMAP;
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	}
 
 	return ret;
<span class="p_chunk">@@ -1642,11 +1647,13 @@</span> <span class="p_context"> EXPORT_SYMBOL(do_munmap);</span>
 int vm_munmap(unsigned long addr, size_t len)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	int ret;
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = do_munmap(mm, addr, len, NULL);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 EXPORT_SYMBOL(vm_munmap);
<span class="p_chunk">@@ -1732,10 +1739,12 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		unsigned long, new_addr)
 {
 	unsigned long ret;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_write_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	ret = do_mremap(addr, old_len, new_len, flags, new_addr);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_write_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1814,9 +1823,11 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		unsigned long addr, void *buf, int len, unsigned int gup_flags)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	struct range_rwlock range;</span>
 	int write = gup_flags &amp; FOLL_WRITE;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/* the access must start within one of the target process&#39;s mappings */
 	vma = find_vma(mm, addr);
<span class="p_chunk">@@ -1838,7 +1849,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		len = 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return len;
 }
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index d083714a2bb9..c3a57e85ff22 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -471,6 +471,9 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	bool ret = true;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	/*
 	 * We have to make sure to not race with the victim exit path
<span class="p_chunk">@@ -488,7 +491,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 */
 	mutex_lock(&amp;oom_lock);
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		ret = false;
 		goto unlock_oom;
 	}
<span class="p_chunk">@@ -499,7 +502,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * and delayed __mmput doesn&#39;t matter that much
 	 */
 	if (!mmget_not_zero(mm)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		goto unlock_oom;
 	}
 
<span class="p_chunk">@@ -536,7 +539,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * Drop our reference but make sure the mmput slow path is called from a
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index fb4f2b96d488..d7d175c38500 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -90,6 +90,7 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 	unsigned long max_pages_per_loop = PVM_MAX_KMALLOC_PAGES
 		/ sizeof(struct pages *);
 	unsigned int flags = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* Work out address and page range required */
 	if (len == 0)
<span class="p_chunk">@@ -109,12 +110,13 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 		 * access remotely because task/mm might not
 		 * current/current-&gt;mm
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		pages = get_user_pages_remote(task, mm, pa, pages, flags,
 					      process_pages, NULL, &amp;locked,
<span class="p_del">-					      NULL);</span>
<span class="p_add">+					      &amp;range);</span>
 		if (locked)
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		if (pages &lt;= 0)
 			return -EFAULT;
 
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index e67d6ba4e98e..c0f873cdd0f1 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -1951,7 +1951,8 @@</span> <span class="p_context"> static int shmem_fault(struct vm_fault *vmf)</span>
 			if ((vmf-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY) &amp;&amp;
 			   !(vmf-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)) {
 				/* It&#39;s polite to up mmap_sem if we can */
<span class="p_del">-				up_read(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+				range_read_unlock(&amp;vma-&gt;vm_mm-&gt;mmap_rw_tree,</span>
<span class="p_add">+						  vmf-&gt;lockrange);</span>
 				ret = VM_FAULT_RETRY;
 			}
 
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 178130880b90..a1c09332cb22 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1592,15 +1592,17 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 {
 	struct vm_area_struct *vma;
 	int ret = 0;
<span class="p_add">+	struct range_rwlock range;</span>
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	if (!range_read_trylock(&amp;mm-&gt;mmap_rw_tree, &amp;range)) {</span>
 		/*
 		 * Activate page so shrink_inactive_list is unlikely to unmap
 		 * its ptes while lock is dropped, so swapoff can make progress.
 		 */
 		activate_page(page);
 		unlock_page(page);
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		lock_page(page);
 	}
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_chunk">@@ -1608,7 +1610,7 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 			break;
 		cond_resched();
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return (ret &lt; 0)? ret: 0;
 }
 
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index 923a1ef22bc2..bf5b00b92c55 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -179,7 +179,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	 * feature is not supported.
 	 */
 	if (zeropage) {
<span class="p_del">-		up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;dst_mm-&gt;mmap_rw_tree, range);</span>
 		return -EINVAL;
 	}
 
<span class="p_chunk">@@ -277,7 +277,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 		cond_resched();
 
 		if (unlikely(err == -EFAULT)) {
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;dst_mm-&gt;mmap_rw_tree, range);</span>
 			BUG_ON(!page);
 
 			err = copy_huge_page_from_user(page,
<span class="p_chunk">@@ -287,7 +287,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 				err = -EFAULT;
 				goto out;
 			}
<span class="p_del">-			down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_lock(&amp;dst_mm-&gt;mmap_rw_tree, range);</span>
 
 			dst_vma = NULL;
 			goto retry;
<span class="p_chunk">@@ -307,7 +307,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;dst_mm-&gt;mmap_rw_tree, range);</span>
 out:
 	if (page) {
 		/*
<span class="p_chunk">@@ -385,6 +385,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	unsigned long src_addr, dst_addr;
 	long copied;
 	struct page *page;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/*
 	 * Sanitize the command parameters:
<span class="p_chunk">@@ -400,8 +401,9 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	dst_addr = dst_start;
 	copied = 0;
 	page = NULL;
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 retry:
<span class="p_del">-	down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;dst_mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	/*
 	 * Make sure the vma is not shared, that the dst range is
<span class="p_chunk">@@ -441,7 +443,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	 */
 	if (is_vm_hugetlb_page(dst_vma))
 		return  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,
<span class="p_del">-					       src_start, len, zeropage, NULL);</span>
<span class="p_add">+					       src_start, len, zeropage,</span>
<span class="p_add">+					       &amp;range);</span>
 
 	if (!vma_is_anonymous(dst_vma) &amp;&amp; !vma_is_shmem(dst_vma))
 		goto out_unlock;
<span class="p_chunk">@@ -510,7 +513,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 		if (unlikely(err == -EFAULT)) {
 			void *page_kaddr;
 
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			range_read_unlock(&amp;dst_mm-&gt;mmap_rw_tree, &amp;range);</span>
 			BUG_ON(!page);
 
 			page_kaddr = kmap(page);
<span class="p_chunk">@@ -539,7 +542,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;dst_mm-&gt;mmap_rw_tree, &amp;range);</span>
 out:
 	if (page)
 		put_page(page);
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index 656dc5e37a87..df0ea6f6f1ff 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -301,14 +301,17 @@</span> <span class="p_context"> unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long populate;
 	LIST_HEAD(uf);
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
 
 	ret = security_mmap_file(file, prot, flag);
 	if (!ret) {
<span class="p_del">-		if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (range_write_lock_interruptible(&amp;mm-&gt;mmap_rw_tree, &amp;range))</span>
 			return -EINTR;
 		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
 				    &amp;populate, &amp;uf);
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_write_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		userfaultfd_unmap_complete(mm, &amp;uf);
 		if (populate)
 			mm_populate(ret, populate);
<span class="p_chunk">@@ -614,17 +617,21 @@</span> <span class="p_context"> int get_cmdline(struct task_struct *task, char *buffer, int buflen)</span>
 	unsigned int len;
 	struct mm_struct *mm = get_task_mm(task);
 	unsigned long arg_start, arg_end, env_start, env_end;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+</span>
 	if (!mm)
 		goto out;
 	if (!mm-&gt;arg_end)
 		goto out_mm;	/* Shh! No looking before we&#39;re done */
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	arg_start = mm-&gt;arg_start;
 	arg_end = mm-&gt;arg_end;
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	len = arg_end - arg_start;
 
<span class="p_header">diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c</span>
<span class="p_header">index bb298a200cd3..733b604a3471 100644</span>
<span class="p_header">--- a/virt/kvm/async_pf.c</span>
<span class="p_header">+++ b/virt/kvm/async_pf.c</span>
<span class="p_chunk">@@ -74,6 +74,7 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	struct kvm_async_pf *apf =
 		container_of(work, struct kvm_async_pf, work);
 	struct mm_struct *mm = apf-&gt;mm;
<span class="p_add">+	struct range_rwlock range;</span>
 	struct kvm_vcpu *vcpu = apf-&gt;vcpu;
 	unsigned long addr = apf-&gt;addr;
 	gva_t gva = apf-&gt;gva;
<span class="p_chunk">@@ -86,11 +87,12 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	 * mm and might be done in another context, so we must
 	 * access remotely.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	get_user_pages_remote(NULL, mm, addr, 1, FOLL_WRITE, NULL, NULL,
<span class="p_del">-			&amp;locked);</span>
<span class="p_add">+			      &amp;locked, &amp;range);</span>
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	kvm_async_page_present_sync(vcpu, apf);
 
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 43b8a01ac131..519f0f16d623 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -1252,6 +1252,7 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long addr, size;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	size = PAGE_SIZE;
 
<span class="p_chunk">@@ -1259,7 +1260,8 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 	if (kvm_is_error_hva(addr))
 		return PAGE_SIZE;
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	vma = find_vma(current-&gt;mm, addr);
 	if (!vma)
 		goto out;
<span class="p_chunk">@@ -1267,7 +1269,7 @@</span> <span class="p_context"> unsigned long kvm_host_page_size(struct kvm *kvm, gfn_t gfn)</span>
 	size = vma_kernel_pagesize(vma);
 
 out:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 
 	return size;
 }
<span class="p_chunk">@@ -1407,6 +1409,9 @@</span> <span class="p_context"> static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,</span>
 {
 	struct page *page[1];
 	int npages = 0;
<span class="p_add">+	struct range_rwlock range;</span>
<span class="p_add">+</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range); /* XXX finer grain required here */</span>
 
 	might_sleep();
 
<span class="p_chunk">@@ -1414,9 +1419,9 @@</span> <span class="p_context"> static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,</span>
 		*writable = write_fault;
 
 	if (async) {
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 		npages = get_user_page_nowait(addr, write_fault, page);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	} else {
 		unsigned int flags = FOLL_HWPOISON;
 
<span class="p_chunk">@@ -1523,6 +1528,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn = 0;
 	int npages, r;
<span class="p_add">+	struct range_rwlock range;</span>
 
 	/* we can do it either atomically or asynchronously, not both */
 	BUG_ON(atomic &amp;&amp; async);
<span class="p_chunk">@@ -1537,7 +1543,8 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	if (npages == 1)
 		return pfn;
 
<span class="p_del">-	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_rwlock_init_full(&amp;range);</span>
<span class="p_add">+	range_read_lock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	if (npages == -EHWPOISON ||
 	      (!async &amp;&amp; check_user_page_hwpoison(addr))) {
 		pfn = KVM_PFN_ERR_HWPOISON;
<span class="p_chunk">@@ -1551,7 +1558,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 		pfn = KVM_PFN_ERR_FAULT;
 	else if (vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP)) {
 		r = hva_to_pfn_remapped(vma, addr, async, write_fault, &amp;pfn,
<span class="p_del">-					NULL);</span>
<span class="p_add">+					&amp;range);</span>
 		if (r == -EAGAIN)
 			goto retry;
 		if (r &lt; 0)
<span class="p_chunk">@@ -1562,7 +1569,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 		pfn = KVM_PFN_ERR_FAULT;
 	}
 exit:
<span class="p_del">-	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	range_read_unlock(&amp;current-&gt;mm-&gt;mmap_rw_tree, &amp;range);</span>
 	return pfn;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



