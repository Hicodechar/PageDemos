
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm, oom: allow oom reaper to race with exit_mmap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm, oom: allow oom reaper to race with exit_mmap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 10, 2017, 8:16 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170810081632.31265-1-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9892985/mbox/"
   >mbox</a>
|
   <a href="/patch/9892985/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9892985/">/patch/9892985/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	62C0760236 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 08:17:37 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5442728AD7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 08:17:37 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 48C4A28AD5; Thu, 10 Aug 2017 08:17:37 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7ED1F28AD7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 10 Aug 2017 08:17:36 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751731AbdHJIQm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 10 Aug 2017 04:16:42 -0400
Received: from mail-wm0-f67.google.com ([74.125.82.67]:38779 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750868AbdHJIQj (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 10 Aug 2017 04:16:39 -0400
Received: by mail-wm0-f67.google.com with SMTP id y206so2095899wmd.5
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 10 Aug 2017 01:16:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id;
	bh=9ufPgqX5KdGP7nHz1ljLdOWcTuUeC0IEgF/iCZiVLC8=;
	b=Bh0ik2Bszry2A04R7p2FRZZerahc6WD7ikRoA3LCftjApBhaSTk5dVnsxcKH5ClR8l
	7f/Y21hAkMl2hq1Lqcirl5TwbvaOrIgAZGR2VUq8F60/utb+hKeV8N5KrJkx5NeoG6Rt
	OqD9ZSwleShE9+fEA8mVnPrYey2FzpE+uyXPxE+qfaI8bW0b98b0Z7+ENNP+j4BBzftJ
	hklX8xYgnB888vPB/szsXdbsofGyViXL7WPp6ozOsukOgDUztTsKXBCdvSk59SFPwGYi
	q5M3MsXsGcT1u3DiQLiw84NB9R2I3R2As+argUrOI+y7Qw3PMOp0NV7i6STKWIqb558t
	jV2A==
X-Gm-Message-State: AHYfb5h7UvtzLR7UCyYTGS/zIJ/EeTvrfNkjQIDTcyxfTJDzUr9TaBCN
	N6r6FCvgJhsiGA==
X-Received: by 10.28.238.218 with SMTP id j87mr6480585wmi.141.1502352998267; 
	Thu, 10 Aug 2017 01:16:38 -0700 (PDT)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	b130sm5118868wme.28.2017.08.10.01.16.37
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 10 Aug 2017 01:16:37 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Oleg Nesterov &lt;oleg@redhat.com&gt;,
	Andrea Argangeli &lt;andrea@kernel.org&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;, &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH] mm, oom: allow oom reaper to race with exit_mmap
Date: Thu, 10 Aug 2017 10:16:32 +0200
Message-Id: &lt;20170810081632.31265-1-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.13.2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 10, 2017, 8:16 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

David has noticed that the oom killer might kill additional tasks while
the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks
the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down
to 0. The race is as follows

oom_reap_task				do_exit
					  exit_mm
  __oom_reap_task_mm
					    mmput
					      __mmput
    mmget_not_zero # fails
    						exit_mmap # frees memory
  set_bit(MMF_OOM_SKIP)

The victim is still visible to the OOM killer until it is unhashed.

Currently we try to reduce a risk of this race by taking oom_lock
and wait for out_of_memory sleep while holding the lock to give the
victim some time to exit. This is quite suboptimal approach because
there is no guarantee the victim (especially a large one) will manage
to unmap its address space and free enough memory to the particular oom
domain which needs a memory (e.g. a specific NUMA node).

Fix this problem by allowing __oom_reap_task_mm and __mmput path to
race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed
to run in parallel with other unmappers (hence the mmap_sem for read).

The only tricky part is to exclude page tables tear down and all
operations which modify the address space in the __mmput path. exit_mmap
doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing
really forbids us to use mmap_sem for write, though. In fact we are
already relying on this lock earlier in the __mmput path to synchronize
with ksm and khugepaged.

Take the exclusive mmap_sem when calling free_pgtables and destroying
vmas to sync with __oom_reap_task_mm which take the lock for read. All
other operations can safely race with the parallel unmap.

Changes v1
- bail on null mm-&gt;mmap early as per David Rientjes
- take exclusive mmap_sem in exit_mmap only for oom victims to reduce
  the lock overhead

Reported-by: David Rientjes &lt;rientjes@google.com&gt;
Fixes: 26db62f179d1 (&quot;oom: keep mm of the killed task available&quot;)
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
Hi,
the previous version of the patch has been posted here [1]. The original
patch has taken mmap_sem in exit_mmap unconditionally but Kirill was
worried this could have a performance impact (we should exercise the
fast path most of the time because nobody should be holding lock at that
stage). An artificial testcase [2] has shown ~3% difference but numbers
are quite noisy [3] so it is effect is not all that clear. Anyway I have
made the lock conditional for oom victims.

Andrea has proposed and alternative solution [4] which should be
equivalent functionally similar to {ksm,khugepaged}_exit. I have to
confess I really don&#39;t like that approach but I can live with it if
that is a preferred way (to be honest I would like to drop the empty
down_write();up_write() from the other two callers as well). In fact I
have asked Andrea to post his patch [5] but that hasn&#39;t happened. I do
not think we should wait much longer and finally merge some fix. 

[1] http://lkml.kernel.org/r/20170724072332.31903-1-mhocko@kernel.org
[2] http://lkml.kernel.org/r/20170725142626.GJ26723@dhcp22.suse.cz
[3] http://lkml.kernel.org/r/20170725160359.GO26723@dhcp22.suse.cz
[4] http://lkml.kernel.org/r/20170726162912.GA29716@redhat.com
[5] http://lkml.kernel.org/r/20170728062345.GA2274@dhcp22.suse.cz

 mm/mmap.c     | 16 ++++++++++++++++
 mm/oom_kill.c | 47 ++++++++---------------------------------------
 2 files changed, 24 insertions(+), 39 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Aug. 10, 2017, 6:05 p.m.</div>
<pre class="content">
On Thu, Aug 10, 2017 at 10:16:32AM +0200, Michal Hocko wrote:
<span class="quote">&gt; Andrea has proposed and alternative solution [4] which should be</span>
<span class="quote">&gt; equivalent functionally similar to {ksm,khugepaged}_exit. I have to</span>
<span class="quote">&gt; confess I really don&#39;t like that approach but I can live with it if</span>
<span class="quote">&gt; that is a preferred way (to be honest I would like to drop the empty</span>

Well you added two branches, when only one is necessary. It&#39;s more or
less like preferring a rwsem when a mutex is enough, because you&#39;re
more used to use rwsems.
<span class="quote">
&gt; down_write();up_write() from the other two callers as well). In fact I</span>
<span class="quote">&gt; have asked Andrea to post his patch [5] but that hasn&#39;t happened. I do</span>
<span class="quote">&gt; not think we should wait much longer and finally merge some fix. </span>

It&#39;s posted in [4] already below I didn&#39;t think it was necessary to
resend it. The only other improvement I can think of is an unlikely
around tsk_is_oom_victim() in exit_mmap, but your patch below would
need it too, and two of them.
<span class="quote">
&gt; [1] http://lkml.kernel.org/r/20170724072332.31903-1-mhocko@kernel.org</span>
<span class="quote">&gt; [2] http://lkml.kernel.org/r/20170725142626.GJ26723@dhcp22.suse.cz</span>
<span class="quote">&gt; [3] http://lkml.kernel.org/r/20170725160359.GO26723@dhcp22.suse.cz</span>
<span class="quote">&gt; [4] http://lkml.kernel.org/r/20170726162912.GA29716@redhat.com</span>
<span class="quote">&gt; [5] http://lkml.kernel.org/r/20170728062345.GA2274@dhcp22.suse.cz</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	if (tsk_is_oom_victim(current)) {</span>
<span class="quote">&gt; +		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +		locked = true;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -3005,7 +3018,10 @@ void exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt;  			nr_accounted += vma_pages(vma);</span>
<span class="quote">&gt;  		vma = remove_vma(vma);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +	mm-&gt;mmap = NULL;</span>
<span class="quote">&gt;  	vm_unacct_memory(nr_accounted);</span>
<span class="quote">&gt; +	if (locked)</span>
<span class="quote">&gt; +		up_write(&amp;mm-&gt;mmap_sem);</span>

I wouldn&#39;t normally repost to add an unlikely when I&#39;m not sure if it
gets merged, but if it gets merged I would immediately tell to Andrew
about the microoptimization being missing there so he can fold it
later.

Before reposting about the unlikely I thought we should agree which
version to merge: [4] or the above double branch (for no good as far
as I tangibly can tell).

I think down_write;up_write is the correct thing to do here because
holding the lock for any additional instruction has zero benefits, and
if it has zero benefits it only adds up confusion and makes the code
partly senseless, and that ultimately hurts the reader when it tries
to understand why you&#39;re holding the lock for so long when it&#39;s not
needed.

I just read other code yesterday for another bug about the rss going
off by one in some older kernel, that calls add_mm_rss_vec(mm, rss);
where rss is on the stack and mm-&gt;rss_stat is mm global, under the PT
lock, and again I had to ask myself why is it done there, and if the
PT lock could possibly help. My evaluation was no, it&#39;s just done in
the wrong place, but then I&#39;m not 100% sure because there&#39;s a chance I
misread something very subtle.

	add_mm_rss_vec(mm, rss);
	arch_leave_lazy_mmu_mode();

	/* Do the actual TLB flush before dropping ptl */
	if (force_flush)
		tlb_flush_mmu_tlbonly(tlb);
	pte_unmap_unlock(start_pte, ptl);

The tlb flushing doesn&#39;t seem to check the stats either, so why is
add_mm_rss_vec isn&#39;t called after pte_unmap_unlock?

And yes it looks offtopic (and there&#39;s no bug in the rss accounting, I
was just reading around it just in case) but it&#39;s not, it&#39;s precisely
the kind of issue I have with your patch because it&#39;ll introduce
another case like above that I can&#39;t explain why it&#39;s done under a
lock when it doesn&#39;t need it, and it&#39;s hard to guess it was just your
dislike for down_read;up_write that made you choose to hold the lock
for no good reason arbitrarily a bit longer.

Thanks,
Andrea
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 24e9261bdcc0..822e8860b9d2 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/userfaultfd_k.h&gt;
 #include &lt;linux/moduleparam.h&gt;
 #include &lt;linux/pkeys.h&gt;
<span class="p_add">+#include &lt;linux/oom.h&gt;</span>
 
 #include &lt;linux/uaccess.h&gt;
 #include &lt;asm/cacheflush.h&gt;
<span class="p_chunk">@@ -2967,6 +2968,7 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	unsigned long nr_accounted = 0;
<span class="p_add">+	bool locked = false;</span>
 
 	/* mm&#39;s last user has gone, and its about to be pulled down */
 	mmu_notifier_release(mm);
<span class="p_chunk">@@ -2993,6 +2995,17 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * oom reaper might race with exit_mmap so make sure we won&#39;t free</span>
<span class="p_add">+	 * page tables or unmap VMAs under its feet</span>
<span class="p_add">+	 * Please note that mark_oom_victim is always called under task_lock</span>
<span class="p_add">+	 * with tsk-&gt;mm != NULL checked on !current tasks which synchronizes</span>
<span class="p_add">+	 * with exit_mm and so we cannot race here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (tsk_is_oom_victim(current)) {</span>
<span class="p_add">+		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		locked = true;</span>
<span class="p_add">+	}</span>
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 
<span class="p_chunk">@@ -3005,7 +3018,10 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
<span class="p_add">+	mm-&gt;mmap = NULL;</span>
 	vm_unacct_memory(nr_accounted);
<span class="p_add">+	if (locked)</span>
<span class="p_add">+		up_write(&amp;mm-&gt;mmap_sem);</span>
 }
 
 /* Insert vm structure into process list sorted by address
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f030c1c..b1c96e1910f2 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -470,40 +470,15 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
<span class="p_del">-	bool ret = true;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We have to make sure to not race with the victim exit path</span>
<span class="p_del">-	 * and cause premature new oom victim selection:</span>
<span class="p_del">-	 * __oom_reap_task_mm		exit_mm</span>
<span class="p_del">-	 *   mmget_not_zero</span>
<span class="p_del">-	 *				  mmput</span>
<span class="p_del">-	 *				    atomic_dec_and_test</span>
<span class="p_del">-	 *				  exit_oom_victim</span>
<span class="p_del">-	 *				[...]</span>
<span class="p_del">-	 *				out_of_memory</span>
<span class="p_del">-	 *				  select_bad_process</span>
<span class="p_del">-	 *				    # no TIF_MEMDIE task selects new victim</span>
<span class="p_del">-	 *  unmap_page_range # frees some memory</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mutex_lock(&amp;oom_lock);</span>
 
 	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {
<span class="p_del">-		ret = false;</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		return false;</span>
 	}
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * increase mm_users only after we know we will reap something so</span>
<span class="p_del">-	 * that the mmput_async is called only when we have reaped something</span>
<span class="p_del">-	 * and delayed __mmput doesn&#39;t matter that much</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!mmget_not_zero(mm)) {</span>
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		trace_skip_task_reaping(tsk-&gt;pid);</span>
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/* There is nothing to reap so bail out without signs in the log */</span>
<span class="p_add">+	if (!mm-&gt;mmap)</span>
<span class="p_add">+		goto unlock;</span>
 
 	trace_start_task_reaping(tsk-&gt;pid);
 
<span class="p_chunk">@@ -540,18 +515,12 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Drop our reference but make sure the mmput slow path is called from a</span>
<span class="p_del">-	 * different context because we shouldn&#39;t risk we get stuck there and</span>
<span class="p_del">-	 * put the oom_reaper out of the way.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mmput_async(mm);</span>
 	trace_finish_task_reaping(tsk-&gt;pid);
<span class="p_del">-unlock_oom:</span>
<span class="p_del">-	mutex_unlock(&amp;oom_lock);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
 }
 
 #define MAX_OOM_REAP_RETRIES 10

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



