
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[15/24] x86/mm: Allow flushing for future ASID switches - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [15/24] x86/mm: Allow flushing for future ASID switches</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 28, 2017, 4:39 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171128163908.e3gj6zgq6kcbdlxe@hirez.programming.kicks-ass.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10080763/mbox/"
   >mbox</a>
|
   <a href="/patch/10080763/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10080763/">/patch/10080763/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6281A60234 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 28 Nov 2017 16:39:25 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 63356294DB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 28 Nov 2017 16:39:25 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 58411294DC; Tue, 28 Nov 2017 16:39:25 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3FFBF294DD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 28 Nov 2017 16:39:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754030AbdK1QjW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 28 Nov 2017 11:39:22 -0500
Received: from merlin.infradead.org ([205.233.59.134]:55808 &quot;EHLO
	merlin.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753212AbdK1QjU (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 28 Nov 2017 11:39:20 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=merlin.20170209;
	h=In-Reply-To:Content-Type:MIME-Version:
	References:Message-ID:Subject:Cc:To:From:Date:Sender:Reply-To:
	Content-Transfer-Encoding:Content-ID:Content-Description:Resent-Date:
	Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:
	List-Help:List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=EZ3HEap/lctMwpetrqhwNGYQ+jR3lyWBRA8harhEJ3s=;
	b=E2Q9zJyPtfOs0belbb/y5WyPX
	tu5svxVKXcpBJt5g07d1B4AC1LMAypwYvnmnpfQULP6JtBMcpBaGr2l6YUMRvsW9TGy6ZOUP6Wi2z
	WtJQywi/rAXlUlaDyOQY7p+c8GAOyv1dd6Z5+Jy5TMA1Y/7VHE9N1z6/6KOQBxGz9wDT5+UujUHO3
	X9xgGjcDMq8BkJjimVyeOUU0zygZvL/RYFQmLBWXYARj42cGLm9zAS+Ph2svxvihkaB8H2Nk1Y+/8
	yutU85aRfCM1vl4E8hvzgFsY9Q7AQ/0zlJTNLSokxKYizl9pT3E3xnPZk2kxXNdYwsKjGDtQ1gVUg
	Mx3RwzhMQ==;
Received: from j217100.upc-j.chello.nl ([24.132.217.100]
	helo=hirez.programming.kicks-ass.net)
	by merlin.infradead.org with esmtpsa (Exim 4.87 #1 (Red Hat Linux))
	id 1eJiuc-0006aI-NQ; Tue, 28 Nov 2017 16:39:11 +0000
Received: by hirez.programming.kicks-ass.net (Postfix, from userid 1000)
	id A5C0720321DB8; Tue, 28 Nov 2017 17:39:08 +0100 (CET)
Date: Tue, 28 Nov 2017 17:39:08 +0100
From: Peter Zijlstra &lt;peterz@infradead.org&gt;
To: Andy Lutomirski &lt;luto@amacapital.net&gt;
Cc: Ingo Molnar &lt;mingo@kernel.org&gt;,
	&quot;linux-kernel@vger.kernel.org&quot; &lt;linux-kernel@vger.kernel.org&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Subject: Re: [PATCH 15/24] x86/mm: Allow flushing for future ASID switches
Message-ID: &lt;20171128163908.e3gj6zgq6kcbdlxe@hirez.programming.kicks-ass.net&gt;
References: &lt;20171127104923.14378-1-mingo@kernel.org&gt;
	&lt;20171127104923.14378-16-mingo@kernel.org&gt;
	&lt;CALCETrUFfpDLEP78K9V4GsbHWS5=M6k8ndv0p+R0ud0=xxbaMg@mail.gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;CALCETrUFfpDLEP78K9V4GsbHWS5=M6k8ndv0p+R0ud0=xxbaMg@mail.gmail.com&gt;
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 28, 2017, 4:39 p.m.</div>
<pre class="content">
On Mon, Nov 27, 2017 at 09:16:19PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; We need to split up __flush_tlb_one() into __flush_tlb_one() and</span>
<span class="quote">&gt; __flush_tlb_one_kernel().  We&#39;ve gotten away with having a single</span>
<span class="quote">&gt; function for both this long because we&#39;ve never had PCID on and</span>
<span class="quote">&gt; nonglobal kernel mappings around.  So we&#39;re busted starting with</span>
<span class="quote">&gt; &quot;x86/mm/kaiser: Disable global pages by default with KAISER&quot;, which</span>
<span class="quote">&gt; means that we have a potential corruption issue affecting anyone who</span>
<span class="quote">&gt; tries to bisect the series.</span>

Didn&#39;t do that..
<span class="quote">
&gt; Then we need to make the kernel variant do something sane (presumably</span>
<span class="quote">&gt; just call __flush_tlb_all if we have PCID &amp;&amp; !PGE).  And, for the user</span>
<span class="quote">&gt; variant, we need a straightforward, clean, efficient way to mark a</span>
<span class="quote">&gt; given address space on a given CPU as needing a usermode PCID flush</span>
<span class="quote">&gt; when its usermode tables are next loaded.  This patch isn&#39;t it.</span>

Did give this a try, mostly also to get PCID + !INVPCID working for my
IVB.

The below is fairly ugly but it does boot and build a kernel so its not
immensely broken.

---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 28, 2017, 4:48 p.m.</div>
<pre class="content">
On Tue, Nov 28, 2017 at 05:39:08PM +0100, Peter Zijlstra wrote:
<span class="quote">
&gt; @@ -220,7 +221,21 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;  .macro SWITCH_TO_USER_CR3 scratch_reg:req</span>
<span class="quote">&gt;  	STATIC_JUMP_IF_FALSE .Lend_\@, kaiser_enabled_key, def=1</span>
<span class="quote">&gt;  	mov	%cr3, \scratch_reg</span>
<span class="quote">&gt; -	ADJUST_USER_CR3 \scratch_reg</span>
<span class="quote">&gt; +	push	\scratch_reg</span>
<span class="quote">&gt; +	andq	$(0x7FF), \scratch_reg</span>
<span class="quote">&gt; +	bt	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>
<span class="quote">&gt; +	jnc	.Lnoflush_\@</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	btr	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>
<span class="quote">&gt; +	pop	\scratch_reg</span>
<span class="quote">&gt; +	jmp	.Ldo_\@</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +.Lnoflush_\@:</span>
<span class="quote">&gt; +	pop	\scratch_reg</span>
<span class="quote">&gt; +	ALTERNATIVE &quot;&quot;, &quot;bts $63, \scratch_reg&quot;, X86_FEATURE_PCID</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +.Ldo_\@:</span>
<span class="quote">&gt; +	orq     $(KAISER_SWITCH_MASK), \scratch_reg</span>
<span class="quote">&gt;  	mov	\scratch_reg, %cr3</span>
<span class="quote">&gt;  .Lend_\@:</span>
<span class="quote">&gt;  .endm</span>

Ah, I suppose I should also deal with RESTORE_CR3...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Nov. 28, 2017, 6:13 p.m.</div>
<pre class="content">
Thanks for looking at this, Peter.  I&#39;ve been resisting doing this for a
bit and it&#39;s an embarrassingly small amount of code.

On 11/28/2017 08:39 AM, Peter Zijlstra wrote:
<span class="quote">&gt; @@ -220,7 +221,21 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;  .macro SWITCH_TO_USER_CR3 scratch_reg:req</span>
<span class="quote">&gt;  	STATIC_JUMP_IF_FALSE .Lend_\@, kaiser_enabled_key, def=1</span>
<span class="quote">&gt;  	mov	%cr3, \scratch_reg</span>
<span class="quote">&gt; -	ADJUST_USER_CR3 \scratch_reg</span>
<span class="quote">&gt; +	push	\scratch_reg</span>

Do we have a good stack in all the spots that we need to do this?  It
may have changed with the trampoline stack, but I&#39;m 100% sure that it
wasn&#39;t so in the recent past.

Let me see if I&#39;m reading the assembly right.

Load the kernel&#39;s ASID from CR3 into \scratch_reg:
<span class="quote">
&gt; +	andq	$(0x7FF), \scratch_reg</span>

See if that ASID needs a flush by checking its bit in __asid_flush.
Store value of the bit in CF:
<span class="quote">
&gt; +	bt	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>

Jump if CF bit is clear:
<span class="quote">
&gt; +	jnc	.Lnoflush_\@</span>

Clear the ASID bit from __asid_flush since we are about to do the flush:
<span class="quote">
&gt; +	btr	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>

Restore CR3 back to what it was:
<span class="quote">
&gt; +	pop	\scratch_reg</span>

Jump past the code that sets the no-flush bit (63), forcing a flush:
<span class="quote">
&gt; +	jmp	.Ldo_\@</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +.Lnoflush_\@:</span>
<span class="quote">&gt; +	pop	\scratch_reg</span>
<span class="quote">&gt; +	ALTERNATIVE &quot;&quot;, &quot;bts $63, \scratch_reg&quot;, X86_FEATURE_PCID</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +.Ldo_\@:</span>
<span class="quote">&gt; +	orq     $(KAISER_SWITCH_MASK), \scratch_reg</span>
<span class="quote">&gt;  	mov	\scratch_reg, %cr3</span>
<span class="quote">&gt;  .Lend_\@:</span>
<span class="quote">&gt;  .endm</span>
<span class="quote">


&gt; diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; index 27eb7e8c5e84..1fb137da4c9f 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; @@ -9,6 +9,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/cpufeature.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/special_insns.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/smp.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/kaiser.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
<span class="quote">&gt;  			     unsigned long type)</span>
<span class="quote">&gt; @@ -347,9 +348,33 @@ static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void initialize_tlbstate_and_flush(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +DECLARE_PER_CPU(unsigned long, __asid_flush);</span>

Could we spare enough space to make this something like
user_asid_flush_pending_mask?

It took me a minute to realize that it was a mask.  Also, since we only
have 6 asids, should we bit a bit more stingy with the type?
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * Given an asid, flush the corresponding KAISER user ASID.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void flush_user_asid(u16 asid)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* There is no user ASID if KAISER is off */</span>
<span class="quote">&gt; +	if (!IS_ENABLED(CONFIG_KAISER))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We only have a single ASID if PCID is off and the CR3</span>
<span class="quote">&gt; +	 * write will have flushed it.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!kaiser_enabled)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	__set_bit(kern_asid(asid), this_cpu_ptr(&amp;__asid_flush));</span>
<span class="quote">&gt; +}</span>

We flush_user_asid() and thus set bits in __asid_flush in two cases:

1. When we flush the TLB explicitly
2. When we re-use an ASID for a new mm

It took me a minute to realize that mixing these is still OK, even if
the mm associated with the ASID changes.  It&#39;s because once the ASID is
stale, it doesn&#39;t matter *why* it is stale.  Just that the next guy who
*uses* it needs to do the flush.  You can do 1,000 tlb flushes, a
context switch, a tlb flush and another context switch, but if you only
go out to userspace once, you only need 1 ASID flush.  That fits
perfectly with this bit that gets set a bunch of times and only cleared
once at exit to userspace.

IOW, this all seems sane, but it took me a few minutes of staring at it
to come to that conclusion.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 28, 2017, 7:05 p.m.</div>
<pre class="content">
On Tue, Nov 28, 2017 at 10:13:30AM -0800, Dave Hansen wrote:
<span class="quote">&gt; Thanks for looking at this, Peter.  I&#39;ve been resisting doing this for a</span>
<span class="quote">&gt; bit and it&#39;s an embarrassingly small amount of code.</span>

Right, well, its not complete yet, and it might be complete crap :-)
<span class="quote">
&gt; On 11/28/2017 08:39 AM, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; @@ -220,7 +221,21 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt; &gt;  .macro SWITCH_TO_USER_CR3 scratch_reg:req</span>
<span class="quote">&gt; &gt;  	STATIC_JUMP_IF_FALSE .Lend_\@, kaiser_enabled_key, def=1</span>
<span class="quote">&gt; &gt;  	mov	%cr3, \scratch_reg</span>
<span class="quote">&gt; &gt; -	ADJUST_USER_CR3 \scratch_reg</span>
<span class="quote">&gt; &gt; +	push	\scratch_reg</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we have a good stack in all the spots that we need to do this?  It</span>
<span class="quote">&gt; may have changed with the trampoline stack, but I&#39;m 100% sure that it</span>
<span class="quote">&gt; wasn&#39;t so in the recent past.</span>

Dunno really. I figured I&#39;d give it a go and see what happens. So far
the machine still works. But I was hoping Andy would have an opinion on
this.
<span class="quote">
&gt; Let me see if I&#39;m reading the assembly right.</span>

Yep, seems you can read asm :-)
<span class="quote">

&gt; &gt; +DECLARE_PER_CPU(unsigned long, __asid_flush);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could we spare enough space to make this something like</span>
<span class="quote">&gt; user_asid_flush_pending_mask?</span>

Yeah, if I can get it all working we&#39;ll bikeshed on a name ;-)
<span class="quote">
&gt; It took me a minute to realize that it was a mask.  Also, since we only</span>
<span class="quote">&gt; have 6 asids, should we bit a bit more stingy with the type?</span>

I picked unsigned long because our bitops (__set_bit in this case, use
it), and I know we&#39;re LE and could simply use a shorter type, but meh.
<span class="quote">
&gt; It took me a minute to realize that mixing these is still OK, even if</span>
<span class="quote">&gt; the mm associated with the ASID changes.  It&#39;s because once the ASID is</span>
<span class="quote">&gt; stale, it doesn&#39;t matter *why* it is stale.  Just that the next guy who</span>
<span class="quote">&gt; *uses* it needs to do the flush.  You can do 1,000 tlb flushes, a</span>
<span class="quote">&gt; context switch, a tlb flush and another context switch, but if you only</span>
<span class="quote">&gt; go out to userspace once, you only need 1 ASID flush.  That fits</span>
<span class="quote">&gt; perfectly with this bit that gets set a bunch of times and only cleared</span>
<span class="quote">&gt; once at exit to userspace.</span>

Just so.

I&#39;m now staring at the RESTORE_CR3 stuff, and that appears to be called
in the NMI handling where the stack is not to be used (if I read it
right), so that&#39;s going to be a little more tricky.

Let me prod at that..
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 28, 2017, 7:36 p.m.</div>
<pre class="content">
On Tue, Nov 28, 2017 at 08:05:05PM +0100, Peter Zijlstra wrote:
<span class="quote">&gt; I&#39;m now staring at the RESTORE_CR3 stuff, and that appears to be called</span>
<span class="quote">&gt; in the NMI handling where the stack is not to be used (if I read it</span>
<span class="quote">&gt; right), so that&#39;s going to be a little more tricky.</span>

As I just mentioned on IRC; I just realized that RESTORE_CR3 is always
flushing. So what I just wrote is effectively an optimization that
allows a nonflush.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Nov. 28, 2017, 8:34 p.m.</div>
<pre class="content">
On Tue, Nov 28, 2017 at 11:05 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Tue, Nov 28, 2017 at 10:13:30AM -0800, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt; Thanks for looking at this, Peter.  I&#39;ve been resisting doing this for a</span>
<span class="quote">&gt;&gt; bit and it&#39;s an embarrassingly small amount of code.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right, well, its not complete yet, and it might be complete crap :-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On 11/28/2017 08:39 AM, Peter Zijlstra wrote:</span>
<span class="quote">&gt;&gt; &gt; @@ -220,7 +221,21 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;&gt; &gt;  .macro SWITCH_TO_USER_CR3 scratch_reg:req</span>
<span class="quote">&gt;&gt; &gt;     STATIC_JUMP_IF_FALSE .Lend_\@, kaiser_enabled_key, def=1</span>
<span class="quote">&gt;&gt; &gt;     mov     %cr3, \scratch_reg</span>
<span class="quote">&gt;&gt; &gt; -   ADJUST_USER_CR3 \scratch_reg</span>
<span class="quote">&gt;&gt; &gt; +   push    \scratch_reg</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Do we have a good stack in all the spots that we need to do this?  It</span>
<span class="quote">&gt;&gt; may have changed with the trampoline stack, but I&#39;m 100% sure that it</span>
<span class="quote">&gt;&gt; wasn&#39;t so in the recent past.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Dunno really. I figured I&#39;d give it a go and see what happens. So far</span>
<span class="quote">&gt; the machine still works. But I was hoping Andy would have an opinion on</span>
<span class="quote">&gt; this.</span>

I thought we had a stack in all these places even before the
trampoline.  There was an issue with *entry*, but I think exit has
always been okay.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; Let me see if I&#39;m reading the assembly right.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yep, seems you can read asm :-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; +DECLARE_PER_CPU(unsigned long, __asid_flush);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Could we spare enough space to make this something like</span>
<span class="quote">&gt;&gt; user_asid_flush_pending_mask?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, if I can get it all working we&#39;ll bikeshed on a name ;-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; It took me a minute to realize that it was a mask.  Also, since we only</span>
<span class="quote">&gt;&gt; have 6 asids, should we bit a bit more stingy with the type?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I picked unsigned long because our bitops (__set_bit in this case, use</span>
<span class="quote">&gt; it), and I know we&#39;re LE and could simply use a shorter type, but meh.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; It took me a minute to realize that mixing these is still OK, even if</span>
<span class="quote">&gt;&gt; the mm associated with the ASID changes.  It&#39;s because once the ASID is</span>
<span class="quote">&gt;&gt; stale, it doesn&#39;t matter *why* it is stale.  Just that the next guy who</span>
<span class="quote">&gt;&gt; *uses* it needs to do the flush.  You can do 1,000 tlb flushes, a</span>
<span class="quote">&gt;&gt; context switch, a tlb flush and another context switch, but if you only</span>
<span class="quote">&gt;&gt; go out to userspace once, you only need 1 ASID flush.  That fits</span>
<span class="quote">&gt;&gt; perfectly with this bit that gets set a bunch of times and only cleared</span>
<span class="quote">&gt;&gt; once at exit to userspace.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Just so.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m now staring at the RESTORE_CR3 stuff, and that appears to be called</span>
<span class="quote">&gt; in the NMI handling where the stack is not to be used (if I read it</span>
<span class="quote">&gt; right), so that&#39;s going to be a little more tricky.</span>

I think it should be fine.  A very old version of the patches had that
problem, but, in -tip, the nmi RESTORE_CR3 is in the fancy
recursion-protected region, and the stack is okay.  The idea is that
we&#39;re already on the old (possibly user) CR3 before we do the crazy
recursion-checking bits.  But that&#39;s fine, since all that&#39;s accessed
there is the IST stack, and that&#39;s in the cpu_entry_area and thus safe
regardless of CR3.

Side question: on extremely quick read, you&#39;re doing bt then btr.  Why
not just do a single btr and be done with it?  Are you trying to avoid
getting exclusive access to the cacheline when not needed?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Nov. 28, 2017, 8:39 p.m.</div>
<pre class="content">
On Tue, Nov 28, 2017 at 12:34:17PM -0800, Andy Lutomirski wrote:
<span class="quote">&gt; Side question: on extremely quick read, you&#39;re doing bt then btr.  Why</span>
<span class="quote">&gt; not just do a single btr and be done with it?  Are you trying to avoid</span>
<span class="quote">&gt; getting exclusive access to the cacheline when not needed?</span>

Yes, avoids the M in the common !flush case.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 07fa7fdd7b68..d7f1be6ccc97 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -4,6 +4,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeatures.h&gt;
 #include &lt;asm/page_types.h&gt;
 #include &lt;asm/pgtable_types.h&gt;
<span class="p_add">+#include &lt;asm/percpu.h&gt;</span>
 
 /*
 
<span class="p_chunk">@@ -220,7 +221,21 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 .macro SWITCH_TO_USER_CR3 scratch_reg:req
 	STATIC_JUMP_IF_FALSE .Lend_\@, kaiser_enabled_key, def=1
 	mov	%cr3, \scratch_reg
<span class="p_del">-	ADJUST_USER_CR3 \scratch_reg</span>
<span class="p_add">+	push	\scratch_reg</span>
<span class="p_add">+	andq	$(0x7FF), \scratch_reg</span>
<span class="p_add">+	bt	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	btr	\scratch_reg, PER_CPU_VAR(__asid_flush)</span>
<span class="p_add">+	pop	\scratch_reg</span>
<span class="p_add">+	jmp	.Ldo_\@</span>
<span class="p_add">+</span>
<span class="p_add">+.Lnoflush_\@:</span>
<span class="p_add">+	pop	\scratch_reg</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;bts $63, \scratch_reg&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+</span>
<span class="p_add">+.Ldo_\@:</span>
<span class="p_add">+	orq     $(KAISER_SWITCH_MASK), \scratch_reg</span>
 	mov	\scratch_reg, %cr3
 .Lend_\@:
 .endm
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 27eb7e8c5e84..1fb137da4c9f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeature.h&gt;
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/smp.h&gt;
<span class="p_add">+#include &lt;asm/kaiser.h&gt;</span>
 
 static inline void __invpcid(unsigned long pcid, unsigned long addr,
 			     unsigned long type)
<span class="p_chunk">@@ -347,9 +348,33 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 
 extern void initialize_tlbstate_and_flush(void);
 
<span class="p_add">+DECLARE_PER_CPU(unsigned long, __asid_flush);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Given an asid, flush the corresponding KAISER user ASID.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void flush_user_asid(u16 asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* There is no user ASID if KAISER is off */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_KAISER))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We only have a single ASID if PCID is off and the CR3</span>
<span class="p_add">+	 * write will have flushed it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!kaiser_enabled)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__set_bit(kern_asid(asid), this_cpu_ptr(&amp;__asid_flush));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void __native_flush_tlb(void)
 {
 	if (!cpu_feature_enabled(X86_FEATURE_INVPCID)) {
<span class="p_add">+#if 0</span>
 		/*
 		 * native_write_cr3() only clears the current PCID if
 		 * CR4 has X86_CR4_PCIDE set.  In other words, this does
<span class="p_chunk">@@ -358,9 +383,10 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 		 * With KAISER and PCIDs, the means that we did not
 		 * flush the user PCID.  Warn if it gets called.
 		 */
<span class="p_del">-		if (IS_ENABLED(CONFIG_KAISER))</span>
<span class="p_del">-			WARN_ON_ONCE(this_cpu_read(cpu_tlbstate.cr4) &amp;</span>
<span class="p_del">-				     X86_CR4_PCIDE);</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_KAISER) &amp;&amp; kaiser_enabled)</span>
<span class="p_add">+			WARN_ON_ONCE(this_cpu_read(cpu_tlbstate.cr4) &amp; X86_CR4_PCIDE);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		flush_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
 		/*
 		 * If current-&gt;mm == NULL then we borrow a mm
 		 * which may change during a task switch and
<span class="p_chunk">@@ -435,6 +461,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb_single(unsigned long addr)</span>
 	 * early.
 	 */
 	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE)) {
<span class="p_add">+		flush_user_asid(loaded_mm_asid);</span>
<span class="p_add">+</span>
 		asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 72f115178d14..2dcd01615772 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -218,11 +218,13 @@</span> <span class="p_context"> static void setup_pcid(void)</span>
 		 * INVPCID.  Just avoid using PCIDs at all if we
 		 * have KAISER and do not have INVPCID.
 		 */
<span class="p_add">+#if 0</span>
 		if (!IS_ENABLED(CONFIG_X86_GLOBAL_PAGES) &amp;&amp;
<span class="p_del">-		    !boot_cpu_has(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+		    kaiser_enabled &amp;&amp; !boot_cpu_has(X86_FEATURE_INVPCID)) {</span>
 			setup_clear_cpu_cap(X86_FEATURE_PCID);
 			return;
 		}
<span class="p_add">+#endif</span>
 		/*
 		 * This can&#39;t be cr4_set_bits_and_update_boot() --
 		 * the trampoline code can&#39;t handle CR4.PCIDE and
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index f75b6eb47a6d..4ed1d0dfd54f 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -100,55 +100,14 @@</span> <span class="p_context"> static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
 	*need_flush = true;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Given a kernel asid, flush the corresponding KAISER</span>
<span class="p_del">- * user ASID.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void flush_user_asid(pgd_t *pgd, u16 kern_asid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* There is no user ASID if KAISER is off */</span>
<span class="p_del">-	if (!IS_ENABLED(CONFIG_KAISER))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We only have a single ASID if PCID is off and the CR3</span>
<span class="p_del">-	 * write will have flushed it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * With PCIDs enabled, write_cr3() only flushes TLB</span>
<span class="p_del">-	 * entries for the current (kernel) ASID.  This leaves</span>
<span class="p_del">-	 * old TLB entries for the user ASID in place and we must</span>
<span class="p_del">-	 * flush that context separately.  We can theoretically</span>
<span class="p_del">-	 * delay doing this until we actually load up the</span>
<span class="p_del">-	 * userspace CR3, but do it here for simplicity.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (cpu_feature_enabled(X86_FEATURE_INVPCID)) {</span>
<span class="p_del">-		invpcid_flush_single_context(user_asid(kern_asid));</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * On systems with PCIDs, but no INVPCID, the only</span>
<span class="p_del">-		 * way to flush a PCID is a CR3 write.  Note that</span>
<span class="p_del">-		 * we use the kernel page tables with the *user*</span>
<span class="p_del">-		 * ASID here.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		unsigned long user_asid_flush_cr3;</span>
<span class="p_del">-		user_asid_flush_cr3 = build_cr3(pgd, user_asid(kern_asid));</span>
<span class="p_del">-		write_cr3(user_asid_flush_cr3);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We do not use PCIDs with KAISER unless we also</span>
<span class="p_del">-		 * have INVPCID.  Getting here is unexpected.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		WARN_ON_ONCE(1);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_add">+__visible DEFINE_PER_CPU(unsigned long, __asid_flush);</span>
 
 static void load_new_mm_cr3(pgd_t *pgdir, u16 new_asid, bool need_flush)
 {
 	unsigned long new_mm_cr3;
 
 	if (need_flush) {
<span class="p_del">-		flush_user_asid(pgdir, new_asid);</span>
<span class="p_add">+		flush_user_asid(new_asid);</span>
 		new_mm_cr3 = build_cr3(pgdir, new_asid);
 	} else {
 		new_mm_cr3 = build_cr3_noflush(pgdir, new_asid);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



