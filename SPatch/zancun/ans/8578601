
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] cpu hotplug updates for 4.6 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] cpu hotplug updates for 4.6</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 14, 2016, 11:31 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.11.1603141225170.3657@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8578601/mbox/"
   >mbox</a>
|
   <a href="/patch/8578601/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8578601/">/patch/8578601/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id CE8419F294
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 11:33:02 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id EBF2E2041F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 11:32:58 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id CFE552041D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 11:32:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934193AbcCNLcv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 14 Mar 2016 07:32:51 -0400
Received: from www.linutronix.de ([62.245.132.108]:46149 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752020AbcCNLcn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 14 Mar 2016 07:32:43 -0400
Received: from localhost ([127.0.0.1]) by Galois.linutronix.de with esmtps
	(TLS1.0:DHE_RSA_AES_256_CBC_SHA1:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1afQjo-0005xM-6G; Mon, 14 Mar 2016 12:32:40 +0100
Date: Mon, 14 Mar 2016 12:31:15 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Subject: [GIT pull] cpu hotplug updates for 4.6
Message-ID: &lt;alpine.DEB.2.11.1603141225170.3657@nanos&gt;
User-Agent: Alpine 2.11 (DEB 23 2013-08-11)
MIME-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - March 14, 2016, 11:31 a.m.</div>
<pre class="content">
Linus,

please pull the latest smp-hotplug-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git smp-hotplug-for-linus

This is the first part of the ongoing cpu hotplug rework:

  - Initial implementation of the state machine

  - Runs all online and prepare down callbacks on the plugged cpu and not on
    some random processor

  - Replaces busy loop waiting with completions

  - Adds tracepoints so the states can be followed

Further information is here:

 http://lkml.kernel.org/r/20160226164321.657646833@linutronix.de

Thanks,

	tglx

------------------&gt;
Thomas Gleixner (23):
      cpu/hotplug: Restructure FROZEN state handling
      cpu/hotplug: Restructure cpu_up code
      cpu/hotplug: Split out cpu down functions
      cpu/hotplug: Add tracepoints
      cpu/hotplug: Convert to a state machine for the control processor
      cpu/hotplug: Convert the hotplugged cpu work to a state machine
      cpu/hotplug: Hand in target state to _cpu_up/down
      cpu/hotplug: Add sysfs state interface
      cpu/hotplug: Make target state writeable
      cpu/hotplug: Implement setup/removal interface
      cpu/hotplug: Move scheduler cpu_online notifier to hotplug core
      cpu/hotplug: Unpark smpboot threads from the state machine
      cpu/hotplug: Split out the state walk into functions
      cpu/hotplug: Create hotplug threads
      cpu/hotplug: Move online calls to hotplugged cpu
      arch/hotplug: Call into idle with a proper state
      cpu/hotplug: Let upcoming cpu bring itself fully up
      cpu/hotplug: Make wait for dead cpu completion based
      rcu: Make CPU_DYING_IDLE an explicit call
      cpu/hotplug: Plug death reporting race
      cpu/hotplug: Remove redundant state check
      cpu/hotplug: Fix smpboot thread ordering
      cpu/hotplug: Document states better


 arch/alpha/kernel/smp.c         |    2 +-
 arch/arc/kernel/smp.c           |    2 +-
 arch/arm/kernel/smp.c           |    2 +-
 arch/arm64/kernel/smp.c         |    2 +-
 arch/blackfin/mach-common/smp.c |    2 +-
 arch/hexagon/kernel/smp.c       |    2 +-
 arch/ia64/kernel/smpboot.c      |    2 +-
 arch/m32r/kernel/smpboot.c      |    2 +-
 arch/metag/kernel/smp.c         |    2 +-
 arch/mips/kernel/smp.c          |    2 +-
 arch/mn10300/kernel/smp.c       |    2 +-
 arch/parisc/kernel/smp.c        |    2 +-
 arch/powerpc/kernel/smp.c       |    2 +-
 arch/s390/kernel/smp.c          |    2 +-
 arch/sh/kernel/smp.c            |    2 +-
 arch/sparc/kernel/smp_32.c      |    2 +-
 arch/sparc/kernel/smp_64.c      |    2 +-
 arch/tile/kernel/smpboot.c      |    2 +-
 arch/x86/kernel/smpboot.c       |    2 +-
 arch/x86/xen/smp.c              |    2 +-
 arch/xtensa/kernel/smp.c        |    2 +-
 include/linux/cpu.h             |   27 +-
 include/linux/cpuhotplug.h      |   93 ++++
 include/linux/notifier.h        |    2 +
 include/linux/rcupdate.h        |    4 +-
 include/trace/events/cpuhp.h    |   66 +++
 init/main.c                     |   16 +-
 kernel/cpu.c                    | 1162 ++++++++++++++++++++++++++++++++++-----
 kernel/rcu/tree.c               |   70 +--
 kernel/sched/core.c             |   10 -
 kernel/sched/idle.c             |    9 +-
 kernel/smp.c                    |    1 +
 kernel/smpboot.c                |    6 +-
 kernel/smpboot.h                |    6 +-
 lib/Kconfig.debug               |   13 +
 35 files changed, 1291 insertions(+), 236 deletions(-)
 create mode 100644 include/linux/cpuhotplug.h
 create mode 100644 include/trace/events/cpuhp.h
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/alpha/kernel/smp.c b/arch/alpha/kernel/smp.c</span>
<span class="p_header">index 2f24447fef92..46bf263c3153 100644</span>
<span class="p_header">--- a/arch/alpha/kernel/smp.c</span>
<span class="p_header">+++ b/arch/alpha/kernel/smp.c</span>
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> smp_callin(void)</span>
 	      cpuid, current, current-&gt;active_mm));
 
 	preempt_disable();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 /* Wait until hwrpb-&gt;txrdy is clear for cpu.  Return -1 on timeout.  */
<span class="p_header">diff --git a/arch/arc/kernel/smp.c b/arch/arc/kernel/smp.c</span>
<span class="p_header">index 424e937da5c8..4cb3add77c75 100644</span>
<span class="p_header">--- a/arch/arc/kernel/smp.c</span>
<span class="p_header">+++ b/arch/arc/kernel/smp.c</span>
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> void start_kernel_secondary(void)</span>
 
 	local_irq_enable();
 	preempt_disable();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c</span>
<span class="p_header">index 37312f6749f3..baee70267f29 100644</span>
<span class="p_header">--- a/arch/arm/kernel/smp.c</span>
<span class="p_header">+++ b/arch/arm/kernel/smp.c</span>
<span class="p_chunk">@@ -409,7 +409,7 @@</span> <span class="p_context"> asmlinkage void secondary_start_kernel(void)</span>
 	/*
 	 * OK, it&#39;s off to the idle thread for us
 	 */
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)
<span class="p_header">diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c</span>
<span class="p_header">index b1adc51b2c2e..460765799c64 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/smp.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/smp.c</span>
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> asmlinkage void secondary_start_kernel(void)</span>
 	/*
 	 * OK, it&#39;s off to the idle thread for us
 	 */
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
<span class="p_header">diff --git a/arch/blackfin/mach-common/smp.c b/arch/blackfin/mach-common/smp.c</span>
<span class="p_header">index 0030e21cfceb..23c4ef5f8bdc 100644</span>
<span class="p_header">--- a/arch/blackfin/mach-common/smp.c</span>
<span class="p_header">+++ b/arch/blackfin/mach-common/smp.c</span>
<span class="p_chunk">@@ -333,7 +333,7 @@</span> <span class="p_context"> void secondary_start_kernel(void)</span>
 
 	/* We are done with local CPU inits, unblock the boot CPU. */
 	set_cpu_online(cpu, true);
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 void __init smp_prepare_boot_cpu(void)
<span class="p_header">diff --git a/arch/hexagon/kernel/smp.c b/arch/hexagon/kernel/smp.c</span>
<span class="p_header">index ff759f26b96a..983bae7d2665 100644</span>
<span class="p_header">--- a/arch/hexagon/kernel/smp.c</span>
<span class="p_header">+++ b/arch/hexagon/kernel/smp.c</span>
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> void start_secondary(void)</span>
 
 	local_irq_enable();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 
<span class="p_header">diff --git a/arch/ia64/kernel/smpboot.c b/arch/ia64/kernel/smpboot.c</span>
<span class="p_header">index 0e76fad27975..74fe317477e6 100644</span>
<span class="p_header">--- a/arch/ia64/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/smpboot.c</span>
<span class="p_chunk">@@ -454,7 +454,7 @@</span> <span class="p_context"> start_secondary (void *unused)</span>
 	preempt_disable();
 	smp_callin();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/m32r/kernel/smpboot.c b/arch/m32r/kernel/smpboot.c</span>
<span class="p_header">index a468467542f4..f98d2f6519d6 100644</span>
<span class="p_header">--- a/arch/m32r/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/m32r/kernel/smpboot.c</span>
<span class="p_chunk">@@ -432,7 +432,7 @@</span> <span class="p_context"> int __init start_secondary(void *unused)</span>
 	 */
 	local_flush_tlb_all();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/metag/kernel/smp.c b/arch/metag/kernel/smp.c</span>
<span class="p_header">index c3c6f0864881..bad13232de51 100644</span>
<span class="p_header">--- a/arch/metag/kernel/smp.c</span>
<span class="p_header">+++ b/arch/metag/kernel/smp.c</span>
<span class="p_chunk">@@ -396,7 +396,7 @@</span> <span class="p_context"> asmlinkage void secondary_start_kernel(void)</span>
 	/*
 	 * OK, it&#39;s off to the idle thread for us
 	 */
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)
<span class="p_header">diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c</span>
<span class="p_header">index bd4385a8e6e8..f2112a8ddf15 100644</span>
<span class="p_header">--- a/arch/mips/kernel/smp.c</span>
<span class="p_header">+++ b/arch/mips/kernel/smp.c</span>
<span class="p_chunk">@@ -191,7 +191,7 @@</span> <span class="p_context"> asmlinkage void start_secondary(void)</span>
 	WARN_ON_ONCE(!irqs_disabled());
 	mp_ops-&gt;smp_finish();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 static void stop_this_cpu(void *dummy)
<span class="p_header">diff --git a/arch/mn10300/kernel/smp.c b/arch/mn10300/kernel/smp.c</span>
<span class="p_header">index f984193718b1..426173c4b0b9 100644</span>
<span class="p_header">--- a/arch/mn10300/kernel/smp.c</span>
<span class="p_header">+++ b/arch/mn10300/kernel/smp.c</span>
<span class="p_chunk">@@ -675,7 +675,7 @@</span> <span class="p_context"> int __init start_secondary(void *unused)</span>
 #ifdef CONFIG_GENERIC_CLOCKEVENTS
 	init_clockevents();
 #endif
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/parisc/kernel/smp.c b/arch/parisc/kernel/smp.c</span>
<span class="p_header">index 52e85973a283..c2a9cc55a62f 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/smp.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/smp.c</span>
<span class="p_chunk">@@ -305,7 +305,7 @@</span> <span class="p_context"> void __init smp_callin(void)</span>
 
 	local_irq_enable();  /* Interrupts have been off until now */
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 
 	/* NOTREACHED */
 	panic(&quot;smp_callin() AAAAaaaaahhhh....\n&quot;);
<span class="p_header">diff --git a/arch/powerpc/kernel/smp.c b/arch/powerpc/kernel/smp.c</span>
<span class="p_header">index ec9ec2058d2d..cc13d4c83291 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/smp.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/smp.c</span>
<span class="p_chunk">@@ -727,7 +727,7 @@</span> <span class="p_context"> void start_secondary(void *unused)</span>
 
 	local_irq_enable();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 
 	BUG();
 }
<span class="p_header">diff --git a/arch/s390/kernel/smp.c b/arch/s390/kernel/smp.c</span>
<span class="p_header">index 3c65a8eae34d..40a6b4f9c36c 100644</span>
<span class="p_header">--- a/arch/s390/kernel/smp.c</span>
<span class="p_header">+++ b/arch/s390/kernel/smp.c</span>
<span class="p_chunk">@@ -798,7 +798,7 @@</span> <span class="p_context"> static void smp_start_secondary(void *cpuvoid)</span>
 	set_cpu_online(smp_processor_id(), true);
 	inc_irq_stat(CPU_RST);
 	local_irq_enable();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 /* Upping and downing of CPUs */
<span class="p_header">diff --git a/arch/sh/kernel/smp.c b/arch/sh/kernel/smp.c</span>
<span class="p_header">index de6be008fc01..13f633add29a 100644</span>
<span class="p_header">--- a/arch/sh/kernel/smp.c</span>
<span class="p_header">+++ b/arch/sh/kernel/smp.c</span>
<span class="p_chunk">@@ -203,7 +203,7 @@</span> <span class="p_context"> asmlinkage void start_secondary(void)</span>
 	set_cpu_online(cpu, true);
 	per_cpu(cpu_state, cpu) = CPU_ONLINE;
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 extern struct {
<span class="p_header">diff --git a/arch/sparc/kernel/smp_32.c b/arch/sparc/kernel/smp_32.c</span>
<span class="p_header">index b3a5d81b20f0..fb30e7c6a5b1 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/smp_32.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/smp_32.c</span>
<span class="p_chunk">@@ -364,7 +364,7 @@</span> <span class="p_context"> static void sparc_start_secondary(void *arg)</span>
 	local_irq_enable();
 
 	wmb();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 
 	/* We should never reach here! */
 	BUG();
<span class="p_header">diff --git a/arch/sparc/kernel/smp_64.c b/arch/sparc/kernel/smp_64.c</span>
<span class="p_header">index 19cd08d18672..8a6151a628ce 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/smp_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/smp_64.c</span>
<span class="p_chunk">@@ -134,7 +134,7 @@</span> <span class="p_context"> void smp_callin(void)</span>
 
 	local_irq_enable();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 void cpu_panic(void)
<span class="p_header">diff --git a/arch/tile/kernel/smpboot.c b/arch/tile/kernel/smpboot.c</span>
<span class="p_header">index 20d52a98e171..6c0abaacec33 100644</span>
<span class="p_header">--- a/arch/tile/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/tile/kernel/smpboot.c</span>
<span class="p_chunk">@@ -208,7 +208,7 @@</span> <span class="p_context"> void online_secondary(void)</span>
 	/* Set up tile-timer clock-event device on this cpu */
 	setup_tile_timer();
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 int __cpu_up(unsigned int cpu, struct task_struct *tidle)
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 24d57f77b3c1..293b22a7ab02 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -248,7 +248,7 @@</span> <span class="p_context"> static void notrace start_secondary(void *unused)</span>
 	x86_cpuinit.setup_percpu_clockev();
 
 	wmb();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 void __init smp_store_boot_cpu_info(void)
<span class="p_header">diff --git a/arch/x86/xen/smp.c b/arch/x86/xen/smp.c</span>
<span class="p_header">index 3f4ebf0261f2..3c6d17fd423a 100644</span>
<span class="p_header">--- a/arch/x86/xen/smp.c</span>
<span class="p_header">+++ b/arch/x86/xen/smp.c</span>
<span class="p_chunk">@@ -112,7 +112,7 @@</span> <span class="p_context"> asmlinkage __visible void cpu_bringup_and_idle(int cpu)</span>
 		xen_pvh_secondary_vcpu_init(cpu);
 #endif
 	cpu_bringup();
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 static void xen_smp_intr_free(unsigned int cpu)
<span class="p_header">diff --git a/arch/xtensa/kernel/smp.c b/arch/xtensa/kernel/smp.c</span>
<span class="p_header">index 4d02e38514f5..fc4ad21a5ed4 100644</span>
<span class="p_header">--- a/arch/xtensa/kernel/smp.c</span>
<span class="p_header">+++ b/arch/xtensa/kernel/smp.c</span>
<span class="p_chunk">@@ -157,7 +157,7 @@</span> <span class="p_context"> void secondary_start_kernel(void)</span>
 
 	complete(&amp;cpu_running);
 
<span class="p_del">-	cpu_startup_entry(CPUHP_ONLINE);</span>
<span class="p_add">+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);</span>
 }
 
 static void mx_cpu_start(void *p)
<span class="p_header">diff --git a/include/linux/cpu.h b/include/linux/cpu.h</span>
<span class="p_header">index d2ca8c38f9c4..f9b1fab4388a 100644</span>
<span class="p_header">--- a/include/linux/cpu.h</span>
<span class="p_header">+++ b/include/linux/cpu.h</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/node.h&gt;
 #include &lt;linux/compiler.h&gt;
 #include &lt;linux/cpumask.h&gt;
<span class="p_add">+#include &lt;linux/cpuhotplug.h&gt;</span>
 
 struct device;
 struct device_node;
<span class="p_chunk">@@ -27,6 +28,9 @@</span> <span class="p_context"> struct cpu {</span>
 	struct device dev;
 };
 
<span class="p_add">+extern void boot_cpu_init(void);</span>
<span class="p_add">+extern void boot_cpu_state_init(void);</span>
<span class="p_add">+</span>
 extern int register_cpu(struct cpu *cpu, int num);
 extern struct device *get_cpu_device(unsigned cpu);
 extern bool cpu_is_hotpluggable(unsigned cpu);
<span class="p_chunk">@@ -74,7 +78,7 @@</span> <span class="p_context"> enum {</span>
 	/* migration should happen before other stuff but after perf */
 	CPU_PRI_PERF		= 20,
 	CPU_PRI_MIGRATION	= 10,
<span class="p_del">-	CPU_PRI_SMPBOOT		= 9,</span>
<span class="p_add">+</span>
 	/* bring up workqueues before normal notifiers and down after */
 	CPU_PRI_WORKQUEUE_UP	= 5,
 	CPU_PRI_WORKQUEUE_DOWN	= -5,
<span class="p_chunk">@@ -97,9 +101,7 @@</span> <span class="p_context"> enum {</span>
 					* Called on the new cpu, just before
 					* enabling interrupts. Must not sleep,
 					* must not fail */
<span class="p_del">-#define CPU_DYING_IDLE		0x000B /* CPU (unsigned)v dying, reached</span>
<span class="p_del">-					* idle loop. */</span>
<span class="p_del">-#define CPU_BROKEN		0x000C /* CPU (unsigned)v did not die properly,</span>
<span class="p_add">+#define CPU_BROKEN		0x000B /* CPU (unsigned)v did not die properly,</span>
 					* perhaps due to preemption. */
 
 /* Used for CPU hotplug events occurring while tasks are frozen due to a suspend
<span class="p_chunk">@@ -118,6 +120,7 @@</span> <span class="p_context"> enum {</span>
 
 
 #ifdef CONFIG_SMP
<span class="p_add">+extern bool cpuhp_tasks_frozen;</span>
 /* Need to know about CPUs going up/down? */
 #if defined(CONFIG_HOTPLUG_CPU) || !defined(MODULE)
 #define cpu_notifier(fn, pri) {					\
<span class="p_chunk">@@ -167,7 +170,6 @@</span> <span class="p_context"> static inline void __unregister_cpu_notifier(struct notifier_block *nb)</span>
 }
 #endif
 
<span class="p_del">-void smpboot_thread_init(void);</span>
 int cpu_up(unsigned int cpu);
 void notify_cpu_starting(unsigned int cpu);
 extern void cpu_maps_update_begin(void);
<span class="p_chunk">@@ -177,6 +179,7 @@</span> <span class="p_context"> extern void cpu_maps_update_done(void);</span>
 #define cpu_notifier_register_done	cpu_maps_update_done
 
 #else	/* CONFIG_SMP */
<span class="p_add">+#define cpuhp_tasks_frozen	0</span>
 
 #define cpu_notifier(fn, pri)	do { (void)(fn); } while (0)
 #define __cpu_notifier(fn, pri)	do { (void)(fn); } while (0)
<span class="p_chunk">@@ -215,10 +218,6 @@</span> <span class="p_context"> static inline void cpu_notifier_register_done(void)</span>
 {
 }
 
<span class="p_del">-static inline void smpboot_thread_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* CONFIG_SMP */
 extern struct bus_type cpu_subsys;
 
<span class="p_chunk">@@ -265,11 +264,6 @@</span> <span class="p_context"> static inline int disable_nonboot_cpus(void) { return 0; }</span>
 static inline void enable_nonboot_cpus(void) {}
 #endif /* !CONFIG_PM_SLEEP_SMP */
 
<span class="p_del">-enum cpuhp_state {</span>
<span class="p_del">-	CPUHP_OFFLINE,</span>
<span class="p_del">-	CPUHP_ONLINE,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 void cpu_startup_entry(enum cpuhp_state state);
 
 void cpu_idle_poll_ctrl(bool enable);
<span class="p_chunk">@@ -280,14 +274,15 @@</span> <span class="p_context"> void arch_cpu_idle_enter(void);</span>
 void arch_cpu_idle_exit(void);
 void arch_cpu_idle_dead(void);
 
<span class="p_del">-DECLARE_PER_CPU(bool, cpu_dead_idle);</span>
<span class="p_del">-</span>
 int cpu_report_state(int cpu);
 int cpu_check_up_prepare(int cpu);
 void cpu_set_state_online(int cpu);
 #ifdef CONFIG_HOTPLUG_CPU
 bool cpu_wait_death(unsigned int cpu, int seconds);
 bool cpu_report_death(void);
<span class="p_add">+void cpuhp_report_idle_dead(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void cpuhp_report_idle_dead(void) { }</span>
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 
 #endif /* _LINUX_CPU_H_ */
<span class="p_header">diff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5d68e15e46b7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/cpuhotplug.h</span>
<span class="p_chunk">@@ -0,0 +1,93 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef __CPUHOTPLUG_H</span>
<span class="p_add">+#define __CPUHOTPLUG_H</span>
<span class="p_add">+</span>
<span class="p_add">+enum cpuhp_state {</span>
<span class="p_add">+	CPUHP_OFFLINE,</span>
<span class="p_add">+	CPUHP_CREATE_THREADS,</span>
<span class="p_add">+	CPUHP_NOTIFY_PREPARE,</span>
<span class="p_add">+	CPUHP_BRINGUP_CPU,</span>
<span class="p_add">+	CPUHP_AP_IDLE_DEAD,</span>
<span class="p_add">+	CPUHP_AP_OFFLINE,</span>
<span class="p_add">+	CPUHP_AP_NOTIFY_STARTING,</span>
<span class="p_add">+	CPUHP_AP_ONLINE,</span>
<span class="p_add">+	CPUHP_TEARDOWN_CPU,</span>
<span class="p_add">+	CPUHP_AP_ONLINE_IDLE,</span>
<span class="p_add">+	CPUHP_AP_SMPBOOT_THREADS,</span>
<span class="p_add">+	CPUHP_AP_NOTIFY_ONLINE,</span>
<span class="p_add">+	CPUHP_AP_ONLINE_DYN,</span>
<span class="p_add">+	CPUHP_AP_ONLINE_DYN_END		= CPUHP_AP_ONLINE_DYN + 30,</span>
<span class="p_add">+	CPUHP_ONLINE,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int __cpuhp_setup_state(enum cpuhp_state state,	const char *name, bool invoke,</span>
<span class="p_add">+			int (*startup)(unsigned int cpu),</span>
<span class="p_add">+			int (*teardown)(unsigned int cpu));</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_setup_state - Setup hotplug state callbacks with calling the callbacks</span>
<span class="p_add">+ * @state:	The state for which the calls are installed</span>
<span class="p_add">+ * @name:	Name of the callback (will be used in debug output)</span>
<span class="p_add">+ * @startup:	startup callback function</span>
<span class="p_add">+ * @teardown:	teardown callback function</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Installs the callback functions and invokes the startup callback on</span>
<span class="p_add">+ * the present cpus which have already reached the @state.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int cpuhp_setup_state(enum cpuhp_state state,</span>
<span class="p_add">+				    const char *name,</span>
<span class="p_add">+				    int (*startup)(unsigned int cpu),</span>
<span class="p_add">+				    int (*teardown)(unsigned int cpu))</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuhp_setup_state(state, name, true, startup, teardown);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_setup_state_nocalls - Setup hotplug state callbacks without calling the</span>
<span class="p_add">+ *			       callbacks</span>
<span class="p_add">+ * @state:	The state for which the calls are installed</span>
<span class="p_add">+ * @name:	Name of the callback.</span>
<span class="p_add">+ * @startup:	startup callback function</span>
<span class="p_add">+ * @teardown:	teardown callback function</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Same as @cpuhp_setup_state except that no calls are executed are invoked</span>
<span class="p_add">+ * during installation of this callback. NOP if SMP=n or HOTPLUG_CPU=n.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int cpuhp_setup_state_nocalls(enum cpuhp_state state,</span>
<span class="p_add">+					    const char *name,</span>
<span class="p_add">+					    int (*startup)(unsigned int cpu),</span>
<span class="p_add">+					    int (*teardown)(unsigned int cpu))</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __cpuhp_setup_state(state, name, false, startup, teardown);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __cpuhp_remove_state(enum cpuhp_state state, bool invoke);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_remove_state - Remove hotplug state callbacks and invoke the teardown</span>
<span class="p_add">+ * @state:	The state for which the calls are removed</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Removes the callback functions and invokes the teardown callback on</span>
<span class="p_add">+ * the present cpus which have already reached the @state.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void cpuhp_remove_state(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__cpuhp_remove_state(state, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_remove_state_nocalls - Remove hotplug state callbacks without invoking</span>
<span class="p_add">+ *				teardown</span>
<span class="p_add">+ * @state:	The state for which the calls are removed</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void cpuhp_remove_state_nocalls(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__cpuhp_remove_state(state, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+void cpuhp_online_idle(enum cpuhp_state state);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void cpuhp_online_idle(enum cpuhp_state state) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/notifier.h b/include/linux/notifier.h</span>
<span class="p_header">index d14a4c362465..4149868de4e6 100644</span>
<span class="p_header">--- a/include/linux/notifier.h</span>
<span class="p_header">+++ b/include/linux/notifier.h</span>
<span class="p_chunk">@@ -47,6 +47,8 @@</span> <span class="p_context"></span>
  * runtime initialization.
  */
 
<span class="p_add">+struct notifier_block;</span>
<span class="p_add">+</span>
 typedef	int (*notifier_fn_t)(struct notifier_block *nb,
 			unsigned long action, void *data);
 
<span class="p_header">diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h</span>
<span class="p_header">index 14e6f47ee16f..fc46fe3ea259 100644</span>
<span class="p_header">--- a/include/linux/rcupdate.h</span>
<span class="p_header">+++ b/include/linux/rcupdate.h</span>
<span class="p_chunk">@@ -332,9 +332,7 @@</span> <span class="p_context"> void rcu_init(void);</span>
 void rcu_sched_qs(void);
 void rcu_bh_qs(void);
 void rcu_check_callbacks(int user);
<span class="p_del">-struct notifier_block;</span>
<span class="p_del">-int rcu_cpu_notify(struct notifier_block *self,</span>
<span class="p_del">-		   unsigned long action, void *hcpu);</span>
<span class="p_add">+void rcu_report_dead(unsigned int cpu);</span>
 
 #ifndef CONFIG_TINY_RCU
 void rcu_end_inkernel_boot(void);
<span class="p_header">diff --git a/include/trace/events/cpuhp.h b/include/trace/events/cpuhp.h</span>
new file mode 100644
<span class="p_header">index 000000000000..a72bd93ec7e5</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/trace/events/cpuhp.h</span>
<span class="p_chunk">@@ -0,0 +1,66 @@</span> <span class="p_context"></span>
<span class="p_add">+#undef TRACE_SYSTEM</span>
<span class="p_add">+#define TRACE_SYSTEM cpuhp</span>
<span class="p_add">+</span>
<span class="p_add">+#if !defined(_TRACE_CPUHP_H) || defined(TRACE_HEADER_MULTI_READ)</span>
<span class="p_add">+#define _TRACE_CPUHP_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/tracepoint.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+TRACE_EVENT(cpuhp_enter,</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_PROTO(unsigned int cpu,</span>
<span class="p_add">+		 int target,</span>
<span class="p_add">+		 int idx,</span>
<span class="p_add">+		 int (*fun)(unsigned int)),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_ARGS(cpu, target, idx, fun),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_STRUCT__entry(</span>
<span class="p_add">+		__field( unsigned int,	cpu		)</span>
<span class="p_add">+		__field( int,		target		)</span>
<span class="p_add">+		__field( int,		idx		)</span>
<span class="p_add">+		__field( void *,	fun		)</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_fast_assign(</span>
<span class="p_add">+		__entry-&gt;cpu	= cpu;</span>
<span class="p_add">+		__entry-&gt;target	= target;</span>
<span class="p_add">+		__entry-&gt;idx	= idx;</span>
<span class="p_add">+		__entry-&gt;fun	= fun;</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_printk(&quot;cpu: %04u target: %3d step: %3d (%pf)&quot;,</span>
<span class="p_add">+		  __entry-&gt;cpu, __entry-&gt;target, __entry-&gt;idx, __entry-&gt;fun)</span>
<span class="p_add">+);</span>
<span class="p_add">+</span>
<span class="p_add">+TRACE_EVENT(cpuhp_exit,</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_PROTO(unsigned int cpu,</span>
<span class="p_add">+		 int state,</span>
<span class="p_add">+		 int idx,</span>
<span class="p_add">+		 int ret),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_ARGS(cpu, state, idx, ret),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_STRUCT__entry(</span>
<span class="p_add">+		__field( unsigned int,	cpu		)</span>
<span class="p_add">+		__field( int,		state		)</span>
<span class="p_add">+		__field( int,		idx		)</span>
<span class="p_add">+		__field( int,		ret		)</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_fast_assign(</span>
<span class="p_add">+		__entry-&gt;cpu	= cpu;</span>
<span class="p_add">+		__entry-&gt;state	= state;</span>
<span class="p_add">+		__entry-&gt;idx	= idx;</span>
<span class="p_add">+		__entry-&gt;ret	= ret;</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_printk(&quot; cpu: %04u  state: %3d step: %3d ret: %d&quot;,</span>
<span class="p_add">+		  __entry-&gt;cpu, __entry-&gt;state, __entry-&gt;idx,  __entry-&gt;ret)</span>
<span class="p_add">+);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* This part must be outside protection */</span>
<span class="p_add">+#include &lt;trace/define_trace.h&gt;</span>
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 58c9e374704b..55563fd36be3 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -388,7 +388,6 @@</span> <span class="p_context"> static noinline void __init_refok rest_init(void)</span>
 	int pid;
 
 	rcu_scheduler_starting();
<span class="p_del">-	smpboot_thread_init();</span>
 	/*
 	 * We need to spawn init first so that it obtains pid 1, however
 	 * the init task will end up wanting to create kthreads, which, if
<span class="p_chunk">@@ -452,20 +451,6 @@</span> <span class="p_context"> void __init parse_early_param(void)</span>
 	done = 1;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- *	Activate the first processor.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init boot_cpu_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-	/* Mark the boot cpu &quot;present&quot;, &quot;online&quot; etc for SMP and UP case */</span>
<span class="p_del">-	set_cpu_online(cpu, true);</span>
<span class="p_del">-	set_cpu_active(cpu, true);</span>
<span class="p_del">-	set_cpu_present(cpu, true);</span>
<span class="p_del">-	set_cpu_possible(cpu, true);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 void __init __weak smp_setup_processor_id(void)
 {
 }
<span class="p_chunk">@@ -530,6 +515,7 @@</span> <span class="p_context"> asmlinkage __visible void __init start_kernel(void)</span>
 	setup_command_line(command_line);
 	setup_nr_cpu_ids();
 	setup_per_cpu_areas();
<span class="p_add">+	boot_cpu_state_init();</span>
 	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */
 
 	build_all_zonelists(NULL, NULL);
<span class="p_header">diff --git a/kernel/cpu.c b/kernel/cpu.c</span>
<span class="p_header">index 5b9d39633ce9..6ea42e8da861 100644</span>
<span class="p_header">--- a/kernel/cpu.c</span>
<span class="p_header">+++ b/kernel/cpu.c</span>
<span class="p_chunk">@@ -22,13 +22,88 @@</span> <span class="p_context"></span>
 #include &lt;linux/lockdep.h&gt;
 #include &lt;linux/tick.h&gt;
 #include &lt;linux/irq.h&gt;
<span class="p_add">+#include &lt;linux/smpboot.h&gt;</span>
<span class="p_add">+</span>
 #include &lt;trace/events/power.h&gt;
<span class="p_add">+#define CREATE_TRACE_POINTS</span>
<span class="p_add">+#include &lt;trace/events/cpuhp.h&gt;</span>
 
 #include &quot;smpboot.h&quot;
 
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_cpu_state - Per cpu hotplug state storage</span>
<span class="p_add">+ * @state:	The current cpu state</span>
<span class="p_add">+ * @target:	The target state</span>
<span class="p_add">+ * @thread:	Pointer to the hotplug thread</span>
<span class="p_add">+ * @should_run:	Thread should execute</span>
<span class="p_add">+ * @cb_stat:	The state for a single callback (install/uninstall)</span>
<span class="p_add">+ * @cb:		Single callback function (install/uninstall)</span>
<span class="p_add">+ * @result:	Result of the operation</span>
<span class="p_add">+ * @done:	Signal completion to the issuer of the task</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpuhp_cpu_state {</span>
<span class="p_add">+	enum cpuhp_state	state;</span>
<span class="p_add">+	enum cpuhp_state	target;</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	struct task_struct	*thread;</span>
<span class="p_add">+	bool			should_run;</span>
<span class="p_add">+	enum cpuhp_state	cb_state;</span>
<span class="p_add">+	int			(*cb)(unsigned int cpu);</span>
<span class="p_add">+	int			result;</span>
<span class="p_add">+	struct completion	done;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_step - Hotplug state machine step</span>
<span class="p_add">+ * @name:	Name of the step</span>
<span class="p_add">+ * @startup:	Startup function of the step</span>
<span class="p_add">+ * @teardown:	Teardown function of the step</span>
<span class="p_add">+ * @skip_onerr:	Do not invoke the functions on error rollback</span>
<span class="p_add">+ *		Will go away once the notifiers	are gone</span>
<span class="p_add">+ * @cant_stop:	Bringup/teardown can&#39;t be stopped at this step</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpuhp_step {</span>
<span class="p_add">+	const char	*name;</span>
<span class="p_add">+	int		(*startup)(unsigned int cpu);</span>
<span class="p_add">+	int		(*teardown)(unsigned int cpu);</span>
<span class="p_add">+	bool		skip_onerr;</span>
<span class="p_add">+	bool		cant_stop;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_MUTEX(cpuhp_state_mutex);</span>
<span class="p_add">+static struct cpuhp_step cpuhp_bp_states[];</span>
<span class="p_add">+static struct cpuhp_step cpuhp_ap_states[];</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * cpuhp_invoke_callback _ Invoke the callbacks for a given state</span>
<span class="p_add">+ * @cpu:	The cpu for which the callback should be invoked</span>
<span class="p_add">+ * @step:	The step in the state machine</span>
<span class="p_add">+ * @cb:		The callback function to invoke</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Called from cpu hotplug and from the state register machinery</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state step,</span>
<span class="p_add">+				 int (*cb)(unsigned int))</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cb) {</span>
<span class="p_add">+		trace_cpuhp_enter(cpu, st-&gt;target, step, cb);</span>
<span class="p_add">+		ret = cb(cpu);</span>
<span class="p_add">+		trace_cpuhp_exit(cpu, st-&gt;state, step, ret);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_SMP
 /* Serializes the updates to cpu_online_mask, cpu_present_mask */
 static DEFINE_MUTEX(cpu_add_remove_lock);
<span class="p_add">+bool cpuhp_tasks_frozen;</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);</span>
 
 /*
  * The following two APIs (cpu_maps_update_begin/done) must be used when
<span class="p_chunk">@@ -207,31 +282,281 @@</span> <span class="p_context"> int __register_cpu_notifier(struct notifier_block *nb)</span>
 	return raw_notifier_chain_register(&amp;cpu_chain, nb);
 }
 
<span class="p_del">-static int __cpu_notify(unsigned long val, void *v, int nr_to_call,</span>
<span class="p_add">+static int __cpu_notify(unsigned long val, unsigned int cpu, int nr_to_call,</span>
 			int *nr_calls)
 {
<span class="p_add">+	unsigned long mod = cpuhp_tasks_frozen ? CPU_TASKS_FROZEN : 0;</span>
<span class="p_add">+	void *hcpu = (void *)(long)cpu;</span>
<span class="p_add">+</span>
 	int ret;
 
<span class="p_del">-	ret = __raw_notifier_call_chain(&amp;cpu_chain, val, v, nr_to_call,</span>
<span class="p_add">+	ret = __raw_notifier_call_chain(&amp;cpu_chain, val | mod, hcpu, nr_to_call,</span>
 					nr_calls);
 
 	return notifier_to_errno(ret);
 }
 
<span class="p_del">-static int cpu_notify(unsigned long val, void *v)</span>
<span class="p_add">+static int cpu_notify(unsigned long val, unsigned int cpu)</span>
 {
<span class="p_del">-	return __cpu_notify(val, v, -1, NULL);</span>
<span class="p_add">+	return __cpu_notify(val, cpu, -1, NULL);</span>
 }
 
<span class="p_del">-#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_add">+/* Notifier wrappers for transitioning to state machine */</span>
<span class="p_add">+static int notify_prepare(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nr_calls = 0;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = __cpu_notify(CPU_UP_PREPARE, cpu, -1, &amp;nr_calls);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		nr_calls--;</span>
<span class="p_add">+		printk(KERN_WARNING &quot;%s: attempt to bring up CPU %u failed\n&quot;,</span>
<span class="p_add">+				__func__, cpu);</span>
<span class="p_add">+		__cpu_notify(CPU_UP_CANCELED, cpu, nr_calls, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int notify_online(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_notify(CPU_ONLINE, cpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int notify_starting(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_notify(CPU_STARTING, cpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int bringup_wait_for_ap(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	wait_for_completion(&amp;st-&gt;done);</span>
<span class="p_add">+	return st-&gt;result;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int bringup_cpu(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct task_struct *idle = idle_thread_get(cpu);</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Arch-specific enabling code. */</span>
<span class="p_add">+	ret = __cpu_up(cpu, idle);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		cpu_notify(CPU_UP_CANCELED, cpu);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = bringup_wait_for_ap(cpu);</span>
<span class="p_add">+	BUG_ON(!cpu_online(cpu));</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Hotplug state machine related functions</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void undo_cpu_down(unsigned int cpu, struct cpuhp_cpu_state *st,</span>
<span class="p_add">+			  struct cpuhp_step *steps)</span>
<span class="p_add">+{</span>
<span class="p_add">+	for (st-&gt;state++; st-&gt;state &lt; st-&gt;target; st-&gt;state++) {</span>
<span class="p_add">+		struct cpuhp_step *step = steps + st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!step-&gt;skip_onerr)</span>
<span class="p_add">+			cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;startup);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,</span>
<span class="p_add">+				struct cpuhp_step *steps, enum cpuhp_state target)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum cpuhp_state prev_state = st-&gt;state;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; st-&gt;state &gt; target; st-&gt;state--) {</span>
<span class="p_add">+		struct cpuhp_step *step = steps + st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;teardown);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			st-&gt;target = prev_state;</span>
<span class="p_add">+			undo_cpu_down(cpu, st, steps);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void undo_cpu_up(unsigned int cpu, struct cpuhp_cpu_state *st,</span>
<span class="p_add">+			struct cpuhp_step *steps)</span>
<span class="p_add">+{</span>
<span class="p_add">+	for (st-&gt;state--; st-&gt;state &gt; st-&gt;target; st-&gt;state--) {</span>
<span class="p_add">+		struct cpuhp_step *step = steps + st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!step-&gt;skip_onerr)</span>
<span class="p_add">+			cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;teardown);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,</span>
<span class="p_add">+			      struct cpuhp_step *steps, enum cpuhp_state target)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum cpuhp_state prev_state = st-&gt;state;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (st-&gt;state &lt; target) {</span>
<span class="p_add">+		struct cpuhp_step *step;</span>
<span class="p_add">+</span>
<span class="p_add">+		st-&gt;state++;</span>
<span class="p_add">+		step = steps + st-&gt;state;</span>
<span class="p_add">+		ret = cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;startup);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			st-&gt;target = prev_state;</span>
<span class="p_add">+			undo_cpu_up(cpu, st, steps);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The cpu hotplug threads manage the bringup and teardown of the cpus</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void cpuhp_create(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	init_completion(&amp;st-&gt;done);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuhp_should_run(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = this_cpu_ptr(&amp;cpuhp_state);</span>
<span class="p_add">+</span>
<span class="p_add">+	return st-&gt;should_run;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Execute the teardown callbacks. Used to be CPU_DOWN_PREPARE */</span>
<span class="p_add">+static int cpuhp_ap_offline(unsigned int cpu, struct cpuhp_cpu_state *st)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum cpuhp_state target = max((int)st-&gt;target, CPUHP_TEARDOWN_CPU);</span>
<span class="p_add">+</span>
<span class="p_add">+	return cpuhp_down_callbacks(cpu, st, cpuhp_ap_states, target);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Execute the online startup callbacks. Used to be CPU_ONLINE */</span>
<span class="p_add">+static int cpuhp_ap_online(unsigned int cpu, struct cpuhp_cpu_state *st)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cpuhp_up_callbacks(cpu, st, cpuhp_ap_states, st-&gt;target);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke</span>
<span class="p_add">+ * callbacks when a state gets [un]installed at runtime.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void cpuhp_thread_fun(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = this_cpu_ptr(&amp;cpuhp_state);</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Paired with the mb() in cpuhp_kick_ap_work and</span>
<span class="p_add">+	 * cpuhp_invoke_ap_callback, so the work set is consistent visible.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_mb();</span>
<span class="p_add">+	if (!st-&gt;should_run)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	st-&gt;should_run = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Single callback invocation for [un]install ? */</span>
<span class="p_add">+	if (st-&gt;cb) {</span>
<span class="p_add">+		if (st-&gt;cb_state &lt; CPUHP_AP_ONLINE) {</span>
<span class="p_add">+			local_irq_disable();</span>
<span class="p_add">+			ret = cpuhp_invoke_callback(cpu, st-&gt;cb_state, st-&gt;cb);</span>
<span class="p_add">+			local_irq_enable();</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			ret = cpuhp_invoke_callback(cpu, st-&gt;cb_state, st-&gt;cb);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Cannot happen .... */</span>
<span class="p_add">+		BUG_ON(st-&gt;state &lt; CPUHP_AP_ONLINE_IDLE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Regular hotplug work */</span>
<span class="p_add">+		if (st-&gt;state &lt; st-&gt;target)</span>
<span class="p_add">+			ret = cpuhp_ap_online(cpu, st);</span>
<span class="p_add">+		else if (st-&gt;state &gt; st-&gt;target)</span>
<span class="p_add">+			ret = cpuhp_ap_offline(cpu, st);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	st-&gt;result = ret;</span>
<span class="p_add">+	complete(&amp;st-&gt;done);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-static void cpu_notify_nofail(unsigned long val, void *v)</span>
<span class="p_add">+/* Invoke a single callback on a remote cpu */</span>
<span class="p_add">+static int cpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state,</span>
<span class="p_add">+				    int (*cb)(unsigned int))</span>
 {
<span class="p_del">-	BUG_ON(cpu_notify(val, v));</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_online(cpu))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	st-&gt;cb_state = state;</span>
<span class="p_add">+	st-&gt;cb = cb;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure the above stores are visible before should_run becomes</span>
<span class="p_add">+	 * true. Paired with the mb() above in cpuhp_thread_fun()</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_mb();</span>
<span class="p_add">+	st-&gt;should_run = true;</span>
<span class="p_add">+	wake_up_process(st-&gt;thread);</span>
<span class="p_add">+	wait_for_completion(&amp;st-&gt;done);</span>
<span class="p_add">+	return st-&gt;result;</span>
 }
<span class="p_add">+</span>
<span class="p_add">+/* Regular hotplug invocation of the AP hotplug thread */</span>
<span class="p_add">+static void __cpuhp_kick_ap_work(struct cpuhp_cpu_state *st)</span>
<span class="p_add">+{</span>
<span class="p_add">+	st-&gt;result = 0;</span>
<span class="p_add">+	st-&gt;cb = NULL;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure the above stores are visible before should_run becomes</span>
<span class="p_add">+	 * true. Paired with the mb() above in cpuhp_thread_fun()</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_mb();</span>
<span class="p_add">+	st-&gt;should_run = true;</span>
<span class="p_add">+	wake_up_process(st-&gt;thread);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuhp_kick_ap_work(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+	enum cpuhp_state state = st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_cpuhp_enter(cpu, st-&gt;target, state, cpuhp_kick_ap_work);</span>
<span class="p_add">+	__cpuhp_kick_ap_work(st);</span>
<span class="p_add">+	wait_for_completion(&amp;st-&gt;done);</span>
<span class="p_add">+	trace_cpuhp_exit(cpu, st-&gt;state, state, st-&gt;result);</span>
<span class="p_add">+	return st-&gt;result;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct smp_hotplug_thread cpuhp_threads = {</span>
<span class="p_add">+	.store			= &amp;cpuhp_state.thread,</span>
<span class="p_add">+	.create			= &amp;cpuhp_create,</span>
<span class="p_add">+	.thread_should_run	= cpuhp_should_run,</span>
<span class="p_add">+	.thread_fn		= cpuhp_thread_fun,</span>
<span class="p_add">+	.thread_comm		= &quot;cpuhp/%u&quot;,</span>
<span class="p_add">+	.selfparking		= true,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+void __init cpuhp_threads_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(smpboot_register_percpu_thread(&amp;cpuhp_threads));</span>
<span class="p_add">+	kthread_unpark(this_cpu_read(cpuhp_state.thread));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HOTPLUG_CPU</span>
 EXPORT_SYMBOL(register_cpu_notifier);
 EXPORT_SYMBOL(__register_cpu_notifier);
<span class="p_del">-</span>
 void unregister_cpu_notifier(struct notifier_block *nb)
 {
 	cpu_maps_update_begin();
<span class="p_chunk">@@ -311,57 +636,60 @@</span> <span class="p_context"> static inline void check_for_tasks(int dead_cpu)</span>
 	read_unlock(&amp;tasklist_lock);
 }
 
<span class="p_del">-struct take_cpu_down_param {</span>
<span class="p_del">-	unsigned long mod;</span>
<span class="p_del">-	void *hcpu;</span>
<span class="p_del">-};</span>
<span class="p_add">+static void cpu_notify_nofail(unsigned long val, unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(cpu_notify(val, cpu));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int notify_down_prepare(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err, nr_calls = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	err = __cpu_notify(CPU_DOWN_PREPARE, cpu, -1, &amp;nr_calls);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		nr_calls--;</span>
<span class="p_add">+		__cpu_notify(CPU_DOWN_FAILED, cpu, nr_calls, NULL);</span>
<span class="p_add">+		pr_warn(&quot;%s: attempt to take down CPU %u failed\n&quot;,</span>
<span class="p_add">+				__func__, cpu);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int notify_dying(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_notify(CPU_DYING, cpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
 /* Take this CPU down. */
 static int take_cpu_down(void *_param)
 {
<span class="p_del">-	struct take_cpu_down_param *param = _param;</span>
<span class="p_del">-	int err;</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = this_cpu_ptr(&amp;cpuhp_state);</span>
<span class="p_add">+	enum cpuhp_state target = max((int)st-&gt;target, CPUHP_AP_OFFLINE);</span>
<span class="p_add">+	int err, cpu = smp_processor_id();</span>
 
 	/* Ensure this CPU doesn&#39;t handle any more interrupts. */
 	err = __cpu_disable();
 	if (err &lt; 0)
 		return err;
 
<span class="p_del">-	cpu_notify(CPU_DYING | param-&gt;mod, param-&gt;hcpu);</span>
<span class="p_add">+	/* Invoke the former CPU_DYING callbacks */</span>
<span class="p_add">+	for (; st-&gt;state &gt; target; st-&gt;state--) {</span>
<span class="p_add">+		struct cpuhp_step *step = cpuhp_ap_states + st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;teardown);</span>
<span class="p_add">+	}</span>
 	/* Give up timekeeping duties */
 	tick_handover_do_timer();
 	/* Park the stopper thread */
<span class="p_del">-	stop_machine_park((long)param-&gt;hcpu);</span>
<span class="p_add">+	stop_machine_park(cpu);</span>
 	return 0;
 }
 
<span class="p_del">-/* Requires cpu_add_remove_lock to be held */</span>
<span class="p_del">-static int _cpu_down(unsigned int cpu, int tasks_frozen)</span>
<span class="p_add">+static int takedown_cpu(unsigned int cpu)</span>
 {
<span class="p_del">-	int err, nr_calls = 0;</span>
<span class="p_del">-	void *hcpu = (void *)(long)cpu;</span>
<span class="p_del">-	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;</span>
<span class="p_del">-	struct take_cpu_down_param tcd_param = {</span>
<span class="p_del">-		.mod = mod,</span>
<span class="p_del">-		.hcpu = hcpu,</span>
<span class="p_del">-	};</span>
<span class="p_del">-</span>
<span class="p_del">-	if (num_online_cpus() == 1)</span>
<span class="p_del">-		return -EBUSY;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!cpu_online(cpu))</span>
<span class="p_del">-		return -EINVAL;</span>
<span class="p_del">-</span>
<span class="p_del">-	cpu_hotplug_begin();</span>
<span class="p_del">-</span>
<span class="p_del">-	err = __cpu_notify(CPU_DOWN_PREPARE | mod, hcpu, -1, &amp;nr_calls);</span>
<span class="p_del">-	if (err) {</span>
<span class="p_del">-		nr_calls--;</span>
<span class="p_del">-		__cpu_notify(CPU_DOWN_FAILED | mod, hcpu, nr_calls, NULL);</span>
<span class="p_del">-		pr_warn(&quot;%s: attempt to take down CPU %u failed\n&quot;,</span>
<span class="p_del">-			__func__, cpu);</span>
<span class="p_del">-		goto out_release;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+	int err;</span>
 
 	/*
 	 * By now we&#39;ve cleared cpu_active_mask, wait for all preempt-disabled
<span class="p_chunk">@@ -378,6 +706,8 @@</span> <span class="p_context"> static int _cpu_down(unsigned int cpu, int tasks_frozen)</span>
 	else
 		synchronize_rcu();
 
<span class="p_add">+	/* Park the smpboot threads */</span>
<span class="p_add">+	kthread_park(per_cpu_ptr(&amp;cpuhp_state, cpu)-&gt;thread);</span>
 	smpboot_park_threads(cpu);
 
 	/*
<span class="p_chunk">@@ -389,12 +719,12 @@</span> <span class="p_context"> static int _cpu_down(unsigned int cpu, int tasks_frozen)</span>
 	/*
 	 * So now all preempt/rcu users must observe !cpu_active().
 	 */
<span class="p_del">-	err = stop_machine(take_cpu_down, &amp;tcd_param, cpumask_of(cpu));</span>
<span class="p_add">+	err = stop_machine(take_cpu_down, NULL, cpumask_of(cpu));</span>
 	if (err) {
 		/* CPU didn&#39;t die: tell everyone.  Can&#39;t complain. */
<span class="p_del">-		cpu_notify_nofail(CPU_DOWN_FAILED | mod, hcpu);</span>
<span class="p_add">+		cpu_notify_nofail(CPU_DOWN_FAILED, cpu);</span>
 		irq_unlock_sparse();
<span class="p_del">-		goto out_release;</span>
<span class="p_add">+		return err;</span>
 	}
 	BUG_ON(cpu_online(cpu));
 
<span class="p_chunk">@@ -405,10 +735,8 @@</span> <span class="p_context"> static int _cpu_down(unsigned int cpu, int tasks_frozen)</span>
 	 *
 	 * Wait for the stop thread to go away.
 	 */
<span class="p_del">-	while (!per_cpu(cpu_dead_idle, cpu))</span>
<span class="p_del">-		cpu_relax();</span>
<span class="p_del">-	smp_mb(); /* Read from cpu_dead_idle before __cpu_die(). */</span>
<span class="p_del">-	per_cpu(cpu_dead_idle, cpu) = false;</span>
<span class="p_add">+	wait_for_completion(&amp;st-&gt;done);</span>
<span class="p_add">+	BUG_ON(st-&gt;state != CPUHP_AP_IDLE_DEAD);</span>
 
 	/* Interrupts are moved away from the dying cpu, reenable alloc/free */
 	irq_unlock_sparse();
<span class="p_chunk">@@ -417,20 +745,104 @@</span> <span class="p_context"> static int _cpu_down(unsigned int cpu, int tasks_frozen)</span>
 	/* This actually kills the CPU. */
 	__cpu_die(cpu);
 
<span class="p_del">-	/* CPU is completely dead: tell everyone.  Too late to complain. */</span>
 	tick_cleanup_dead_cpu(cpu);
<span class="p_del">-	cpu_notify_nofail(CPU_DEAD | mod, hcpu);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
<span class="p_add">+static int notify_dead(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_notify_nofail(CPU_DEAD, cpu);</span>
 	check_for_tasks(cpu);
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-out_release:</span>
<span class="p_add">+static void cpuhp_complete_idle_dead(void *arg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = arg;</span>
<span class="p_add">+</span>
<span class="p_add">+	complete(&amp;st-&gt;done);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpuhp_report_idle_dead(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = this_cpu_ptr(&amp;cpuhp_state);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(st-&gt;state != CPUHP_AP_OFFLINE);</span>
<span class="p_add">+	rcu_report_dead(smp_processor_id());</span>
<span class="p_add">+	st-&gt;state = CPUHP_AP_IDLE_DEAD;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We cannot call complete after rcu_report_dead() so we delegate it</span>
<span class="p_add">+	 * to an online cpu.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_call_function_single(cpumask_first(cpu_online_mask),</span>
<span class="p_add">+				 cpuhp_complete_idle_dead, st, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define notify_down_prepare	NULL</span>
<span class="p_add">+#define takedown_cpu		NULL</span>
<span class="p_add">+#define notify_dead		NULL</span>
<span class="p_add">+#define notify_dying		NULL</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Requires cpu_add_remove_lock to be held */</span>
<span class="p_add">+static int __ref _cpu_down(unsigned int cpu, int tasks_frozen,</span>
<span class="p_add">+			   enum cpuhp_state target)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+	int prev_state, ret = 0;</span>
<span class="p_add">+	bool hasdied = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (num_online_cpus() == 1)</span>
<span class="p_add">+		return -EBUSY;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpu_present(cpu))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu_hotplug_begin();</span>
<span class="p_add">+</span>
<span class="p_add">+	cpuhp_tasks_frozen = tasks_frozen;</span>
<span class="p_add">+</span>
<span class="p_add">+	prev_state = st-&gt;state;</span>
<span class="p_add">+	st-&gt;target = target;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the current CPU state is in the range of the AP hotplug thread,</span>
<span class="p_add">+	 * then we need to kick the thread.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (st-&gt;state &gt; CPUHP_TEARDOWN_CPU) {</span>
<span class="p_add">+		ret = cpuhp_kick_ap_work(cpu);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The AP side has done the error rollback already. Just</span>
<span class="p_add">+		 * return the error code..</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We might have stopped still in the range of the AP hotplug</span>
<span class="p_add">+		 * thread. Nothing to do anymore.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (st-&gt;state &gt; CPUHP_TEARDOWN_CPU)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need</span>
<span class="p_add">+	 * to do the further cleanups.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ret = cpuhp_down_callbacks(cpu, st, cpuhp_bp_states, target);</span>
<span class="p_add">+</span>
<span class="p_add">+	hasdied = prev_state != st-&gt;state &amp;&amp; st-&gt;state == CPUHP_OFFLINE;</span>
<span class="p_add">+out:</span>
 	cpu_hotplug_done();
<span class="p_del">-	if (!err)</span>
<span class="p_del">-		cpu_notify_nofail(CPU_POST_DEAD | mod, hcpu);</span>
<span class="p_del">-	return err;</span>
<span class="p_add">+	/* This post dead nonsense must die */</span>
<span class="p_add">+	if (!ret &amp;&amp; hasdied)</span>
<span class="p_add">+		cpu_notify_nofail(CPU_POST_DEAD, cpu);</span>
<span class="p_add">+	return ret;</span>
 }
 
<span class="p_del">-int cpu_down(unsigned int cpu)</span>
<span class="p_add">+static int do_cpu_down(unsigned int cpu, enum cpuhp_state target)</span>
 {
 	int err;
 
<span class="p_chunk">@@ -441,100 +853,131 @@</span> <span class="p_context"> int cpu_down(unsigned int cpu)</span>
 		goto out;
 	}
 
<span class="p_del">-	err = _cpu_down(cpu, 0);</span>
<span class="p_add">+	err = _cpu_down(cpu, 0, target);</span>
 
 out:
 	cpu_maps_update_done();
 	return err;
 }
<span class="p_add">+int cpu_down(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return do_cpu_down(cpu, CPUHP_OFFLINE);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL(cpu_down);
 #endif /*CONFIG_HOTPLUG_CPU*/
 
<span class="p_del">-/*</span>
<span class="p_del">- * Unpark per-CPU smpboot kthreads at CPU-online time.</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * notify_cpu_starting(cpu) - call the CPU_STARTING notifiers</span>
<span class="p_add">+ * @cpu: cpu that just started</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function calls the cpu_chain notifiers with CPU_STARTING.</span>
<span class="p_add">+ * It must be called by the arch code on the new cpu, before the new cpu</span>
<span class="p_add">+ * enables interrupts and before the &quot;boot&quot; cpu returns from __cpu_up().</span>
  */
<span class="p_del">-static int smpboot_thread_call(struct notifier_block *nfb,</span>
<span class="p_del">-			       unsigned long action, void *hcpu)</span>
<span class="p_add">+void notify_cpu_starting(unsigned int cpu)</span>
 {
<span class="p_del">-	int cpu = (long)hcpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (action &amp; ~CPU_TASKS_FROZEN) {</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+	enum cpuhp_state target = min((int)st-&gt;target, CPUHP_AP_ONLINE);</span>
 
<span class="p_del">-	case CPU_DOWN_FAILED:</span>
<span class="p_del">-	case CPU_ONLINE:</span>
<span class="p_del">-		smpboot_unpark_threads(cpu);</span>
<span class="p_del">-		break;</span>
<span class="p_add">+	while (st-&gt;state &lt; target) {</span>
<span class="p_add">+		struct cpuhp_step *step;</span>
 
<span class="p_del">-	default:</span>
<span class="p_del">-		break;</span>
<span class="p_add">+		st-&gt;state++;</span>
<span class="p_add">+		step = cpuhp_ap_states + st-&gt;state;</span>
<span class="p_add">+		cpuhp_invoke_callback(cpu, st-&gt;state, step-&gt;startup);</span>
 	}
<span class="p_del">-</span>
<span class="p_del">-	return NOTIFY_OK;</span>
 }
 
<span class="p_del">-static struct notifier_block smpboot_thread_notifier = {</span>
<span class="p_del">-	.notifier_call = smpboot_thread_call,</span>
<span class="p_del">-	.priority = CPU_PRI_SMPBOOT,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-void smpboot_thread_init(void)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called from the idle task. We need to set active here, so we can kick off</span>
<span class="p_add">+ * the stopper thread and unpark the smpboot threads. If the target state is</span>
<span class="p_add">+ * beyond CPUHP_AP_ONLINE_IDLE we kick cpuhp thread and let it bring up the</span>
<span class="p_add">+ * cpu further.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpuhp_online_idle(enum cpuhp_state state)</span>
 {
<span class="p_del">-	register_cpu_notifier(&amp;smpboot_thread_notifier);</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = this_cpu_ptr(&amp;cpuhp_state);</span>
<span class="p_add">+	unsigned int cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Happens for the boot cpu */</span>
<span class="p_add">+	if (state != CPUHP_AP_ONLINE_IDLE)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	st-&gt;state = CPUHP_AP_ONLINE_IDLE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* The cpu is marked online, set it active now */</span>
<span class="p_add">+	set_cpu_active(cpu, true);</span>
<span class="p_add">+	/* Unpark the stopper thread and the hotplug thread of this cpu */</span>
<span class="p_add">+	stop_machine_unpark(cpu);</span>
<span class="p_add">+	kthread_unpark(st-&gt;thread);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Should we go further up ? */</span>
<span class="p_add">+	if (st-&gt;target &gt; CPUHP_AP_ONLINE_IDLE)</span>
<span class="p_add">+		__cpuhp_kick_ap_work(st);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		complete(&amp;st-&gt;done);</span>
 }
 
 /* Requires cpu_add_remove_lock to be held */
<span class="p_del">-static int _cpu_up(unsigned int cpu, int tasks_frozen)</span>
<span class="p_add">+static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)</span>
 {
<span class="p_del">-	int ret, nr_calls = 0;</span>
<span class="p_del">-	void *hcpu = (void *)(long)cpu;</span>
<span class="p_del">-	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
 	struct task_struct *idle;
<span class="p_add">+	int ret = 0;</span>
 
 	cpu_hotplug_begin();
 
<span class="p_del">-	if (cpu_online(cpu) || !cpu_present(cpu)) {</span>
<span class="p_add">+	if (!cpu_present(cpu)) {</span>
 		ret = -EINVAL;
 		goto out;
 	}
 
<span class="p_del">-	idle = idle_thread_get(cpu);</span>
<span class="p_del">-	if (IS_ERR(idle)) {</span>
<span class="p_del">-		ret = PTR_ERR(idle);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = smpboot_create_threads(cpu);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The caller of do_cpu_up might have raced with another</span>
<span class="p_add">+	 * caller. Ignore it for now.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (st-&gt;state &gt;= target)</span>
 		goto out;
 
<span class="p_del">-	ret = __cpu_notify(CPU_UP_PREPARE | mod, hcpu, -1, &amp;nr_calls);</span>
<span class="p_del">-	if (ret) {</span>
<span class="p_del">-		nr_calls--;</span>
<span class="p_del">-		pr_warn(&quot;%s: attempt to bring up CPU %u failed\n&quot;,</span>
<span class="p_del">-			__func__, cpu);</span>
<span class="p_del">-		goto out_notify;</span>
<span class="p_add">+	if (st-&gt;state == CPUHP_OFFLINE) {</span>
<span class="p_add">+		/* Let it fail before we try to bring the cpu up */</span>
<span class="p_add">+		idle = idle_thread_get(cpu);</span>
<span class="p_add">+		if (IS_ERR(idle)) {</span>
<span class="p_add">+			ret = PTR_ERR(idle);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
 	}
 
<span class="p_del">-	/* Arch-specific enabling code. */</span>
<span class="p_del">-	ret = __cpu_up(cpu, idle);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (ret != 0)</span>
<span class="p_del">-		goto out_notify;</span>
<span class="p_del">-	BUG_ON(!cpu_online(cpu));</span>
<span class="p_add">+	cpuhp_tasks_frozen = tasks_frozen;</span>
 
<span class="p_del">-	/* Now call notifier in preparation. */</span>
<span class="p_del">-	cpu_notify(CPU_ONLINE | mod, hcpu);</span>
<span class="p_add">+	st-&gt;target = target;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the current CPU state is in the range of the AP hotplug thread,</span>
<span class="p_add">+	 * then we need to kick the thread once more.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (st-&gt;state &gt; CPUHP_BRINGUP_CPU) {</span>
<span class="p_add">+		ret = cpuhp_kick_ap_work(cpu);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The AP side has done the error rollback already. Just</span>
<span class="p_add">+		 * return the error code..</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-out_notify:</span>
<span class="p_del">-	if (ret != 0)</span>
<span class="p_del">-		__cpu_notify(CPU_UP_CANCELED | mod, hcpu, nr_calls, NULL);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Try to reach the target state. We max out on the BP at</span>
<span class="p_add">+	 * CPUHP_BRINGUP_CPU. After that the AP hotplug thread is</span>
<span class="p_add">+	 * responsible for bringing it up to the target state.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	target = min((int)target, CPUHP_BRINGUP_CPU);</span>
<span class="p_add">+	ret = cpuhp_up_callbacks(cpu, st, cpuhp_bp_states, target);</span>
 out:
 	cpu_hotplug_done();
<span class="p_del">-</span>
 	return ret;
 }
 
<span class="p_del">-int cpu_up(unsigned int cpu)</span>
<span class="p_add">+static int do_cpu_up(unsigned int cpu, enum cpuhp_state target)</span>
 {
 	int err = 0;
 
<span class="p_chunk">@@ -558,12 +1001,16 @@</span> <span class="p_context"> int cpu_up(unsigned int cpu)</span>
 		goto out;
 	}
 
<span class="p_del">-	err = _cpu_up(cpu, 0);</span>
<span class="p_del">-</span>
<span class="p_add">+	err = _cpu_up(cpu, 0, target);</span>
 out:
 	cpu_maps_update_done();
 	return err;
 }
<span class="p_add">+</span>
<span class="p_add">+int cpu_up(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return do_cpu_up(cpu, CPUHP_ONLINE);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL_GPL(cpu_up);
 
 #ifdef CONFIG_PM_SLEEP_SMP
<span class="p_chunk">@@ -586,7 +1033,7 @@</span> <span class="p_context"> int disable_nonboot_cpus(void)</span>
 		if (cpu == first_cpu)
 			continue;
 		trace_suspend_resume(TPS(&quot;CPU_OFF&quot;), cpu, true);
<span class="p_del">-		error = _cpu_down(cpu, 1);</span>
<span class="p_add">+		error = _cpu_down(cpu, 1, CPUHP_OFFLINE);</span>
 		trace_suspend_resume(TPS(&quot;CPU_OFF&quot;), cpu, false);
 		if (!error)
 			cpumask_set_cpu(cpu, frozen_cpus);
<span class="p_chunk">@@ -636,7 +1083,7 @@</span> <span class="p_context"> void enable_nonboot_cpus(void)</span>
 
 	for_each_cpu(cpu, frozen_cpus) {
 		trace_suspend_resume(TPS(&quot;CPU_ON&quot;), cpu, true);
<span class="p_del">-		error = _cpu_up(cpu, 1);</span>
<span class="p_add">+		error = _cpu_up(cpu, 1, CPUHP_ONLINE);</span>
 		trace_suspend_resume(TPS(&quot;CPU_ON&quot;), cpu, false);
 		if (!error) {
 			pr_info(&quot;CPU%d is up\n&quot;, cpu);
<span class="p_chunk">@@ -709,26 +1156,463 @@</span> <span class="p_context"> core_initcall(cpu_hotplug_pm_sync_init);</span>
 
 #endif /* CONFIG_PM_SLEEP_SMP */
 
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Boot processor state steps */</span>
<span class="p_add">+static struct cpuhp_step cpuhp_bp_states[] = {</span>
<span class="p_add">+	[CPUHP_OFFLINE] = {</span>
<span class="p_add">+		.name			= &quot;offline&quot;,</span>
<span class="p_add">+		.startup		= NULL,</span>
<span class="p_add">+		.teardown		= NULL,</span>
<span class="p_add">+	},</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	[CPUHP_CREATE_THREADS]= {</span>
<span class="p_add">+		.name			= &quot;threads:create&quot;,</span>
<span class="p_add">+		.startup		= smpboot_create_threads,</span>
<span class="p_add">+		.teardown		= NULL,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Preparatory and dead notifiers. Will be replaced once the notifiers</span>
<span class="p_add">+	 * are converted to states.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	[CPUHP_NOTIFY_PREPARE] = {</span>
<span class="p_add">+		.name			= &quot;notify:prepare&quot;,</span>
<span class="p_add">+		.startup		= notify_prepare,</span>
<span class="p_add">+		.teardown		= notify_dead,</span>
<span class="p_add">+		.skip_onerr		= true,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* Kicks the plugged cpu into life */</span>
<span class="p_add">+	[CPUHP_BRINGUP_CPU] = {</span>
<span class="p_add">+		.name			= &quot;cpu:bringup&quot;,</span>
<span class="p_add">+		.startup		= bringup_cpu,</span>
<span class="p_add">+		.teardown		= NULL,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Handled on controll processor until the plugged processor manages</span>
<span class="p_add">+	 * this itself.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	[CPUHP_TEARDOWN_CPU] = {</span>
<span class="p_add">+		.name			= &quot;cpu:teardown&quot;,</span>
<span class="p_add">+		.startup		= NULL,</span>
<span class="p_add">+		.teardown		= takedown_cpu,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Application processor state steps */</span>
<span class="p_add">+static struct cpuhp_step cpuhp_ap_states[] = {</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	/* Final state before CPU kills itself */</span>
<span class="p_add">+	[CPUHP_AP_IDLE_DEAD] = {</span>
<span class="p_add">+		.name			= &quot;idle:dead&quot;,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Last state before CPU enters the idle loop to die. Transient state</span>
<span class="p_add">+	 * for synchronization.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	[CPUHP_AP_OFFLINE] = {</span>
<span class="p_add">+		.name			= &quot;ap:offline&quot;,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Low level startup/teardown notifiers. Run with interrupts</span>
<span class="p_add">+	 * disabled. Will be removed once the notifiers are converted to</span>
<span class="p_add">+	 * states.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	[CPUHP_AP_NOTIFY_STARTING] = {</span>
<span class="p_add">+		.name			= &quot;notify:starting&quot;,</span>
<span class="p_add">+		.startup		= notify_starting,</span>
<span class="p_add">+		.teardown		= notify_dying,</span>
<span class="p_add">+		.skip_onerr		= true,</span>
<span class="p_add">+		.cant_stop		= true,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* Entry state on starting. Interrupts enabled from here on. Transient</span>
<span class="p_add">+	 * state for synchronsization */</span>
<span class="p_add">+	[CPUHP_AP_ONLINE] = {</span>
<span class="p_add">+		.name			= &quot;ap:online&quot;,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/* Handle smpboot threads park/unpark */</span>
<span class="p_add">+	[CPUHP_AP_SMPBOOT_THREADS] = {</span>
<span class="p_add">+		.name			= &quot;smpboot:threads&quot;,</span>
<span class="p_add">+		.startup		= smpboot_unpark_threads,</span>
<span class="p_add">+		.teardown		= NULL,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Online/down_prepare notifiers. Will be removed once the notifiers</span>
<span class="p_add">+	 * are converted to states.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	[CPUHP_AP_NOTIFY_ONLINE] = {</span>
<span class="p_add">+		.name			= &quot;notify:online&quot;,</span>
<span class="p_add">+		.startup		= notify_online,</span>
<span class="p_add">+		.teardown		= notify_down_prepare,</span>
<span class="p_add">+	},</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The dynamically registered state space is here</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* CPU is fully up and running. */</span>
<span class="p_add">+	[CPUHP_ONLINE] = {</span>
<span class="p_add">+		.name			= &quot;online&quot;,</span>
<span class="p_add">+		.startup		= NULL,</span>
<span class="p_add">+		.teardown		= NULL,</span>
<span class="p_add">+	},</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Sanity check for callbacks */</span>
<span class="p_add">+static int cpuhp_cb_check(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (state &lt;= CPUHP_OFFLINE || state &gt;= CPUHP_ONLINE)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool cpuhp_is_ap_state(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The extra check for CPUHP_TEARDOWN_CPU is only for documentation</span>
<span class="p_add">+	 * purposes as that state is handled explicitely in cpu_down.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return state &gt; CPUHP_BRINGUP_CPU &amp;&amp; state != CPUHP_TEARDOWN_CPU;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_step *sp;</span>
<span class="p_add">+</span>
<span class="p_add">+	sp = cpuhp_is_ap_state(state) ? cpuhp_ap_states : cpuhp_bp_states;</span>
<span class="p_add">+	return sp + state;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void cpuhp_store_callbacks(enum cpuhp_state state,</span>
<span class="p_add">+				  const char *name,</span>
<span class="p_add">+				  int (*startup)(unsigned int cpu),</span>
<span class="p_add">+				  int (*teardown)(unsigned int cpu))</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* (Un)Install the callbacks for further cpu hotplug operations */</span>
<span class="p_add">+	struct cpuhp_step *sp;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	sp = cpuhp_get_step(state);</span>
<span class="p_add">+	sp-&gt;startup = startup;</span>
<span class="p_add">+	sp-&gt;teardown = teardown;</span>
<span class="p_add">+	sp-&gt;name = name;</span>
<span class="p_add">+	mutex_unlock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *cpuhp_get_teardown_cb(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cpuhp_get_step(state)-&gt;teardown;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Call the startup/teardown function for a step either on the AP or</span>
<span class="p_add">+ * on the current CPU.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpuhp_issue_call(int cpu, enum cpuhp_state state,</span>
<span class="p_add">+			    int (*cb)(unsigned int), bool bringup)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cb)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The non AP bound callbacks can fail on bringup. On teardown</span>
<span class="p_add">+	 * e.g. module removal we crash for now.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+	if (cpuhp_is_ap_state(state))</span>
<span class="p_add">+		ret = cpuhp_invoke_ap_callback(cpu, state, cb);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		ret = cpuhp_invoke_callback(cpu, state, cb);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	ret = cpuhp_invoke_callback(cpu, state, cb);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	BUG_ON(ret &amp;&amp; !bringup);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called from __cpuhp_setup_state on a recoverable failure.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: The teardown callbacks for rollback are not allowed to fail!</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,</span>
<span class="p_add">+				   int (*teardown)(unsigned int cpu))</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!teardown)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Roll back the already executed steps on the other cpus */</span>
<span class="p_add">+	for_each_present_cpu(cpu) {</span>
<span class="p_add">+		struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+		int cpustate = st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (cpu &gt;= failedcpu)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Did we invoke the startup call on that cpu ? */</span>
<span class="p_add">+		if (cpustate &gt;= state)</span>
<span class="p_add">+			cpuhp_issue_call(cpu, state, teardown, false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns a free for dynamic slot assignment of the Online state. The states</span>
<span class="p_add">+ * are protected by the cpuhp_slot_states mutex and an empty slot is identified</span>
<span class="p_add">+ * by having no name assigned.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpuhp_reserve_state(enum cpuhp_state state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum cpuhp_state i;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	for (i = CPUHP_AP_ONLINE_DYN; i &lt;= CPUHP_AP_ONLINE_DYN_END; i++) {</span>
<span class="p_add">+		if (cpuhp_ap_states[i].name)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		cpuhp_ap_states[i].name = &quot;Reserved&quot;;</span>
<span class="p_add">+		mutex_unlock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+		return i;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	mutex_unlock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	WARN(1, &quot;No more dynamic states available for CPU hotplug\n&quot;);</span>
<span class="p_add">+	return -ENOSPC;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
<span class="p_del">- * notify_cpu_starting(cpu) - call the CPU_STARTING notifiers</span>
<span class="p_del">- * @cpu: cpu that just started</span>
<span class="p_add">+ * __cpuhp_setup_state - Setup the callbacks for an hotplug machine state</span>
<span class="p_add">+ * @state:	The state to setup</span>
<span class="p_add">+ * @invoke:	If true, the startup function is invoked for cpus where</span>
<span class="p_add">+ *		cpu state &gt;= @state</span>
<span class="p_add">+ * @startup:	startup callback function</span>
<span class="p_add">+ * @teardown:	teardown callback function</span>
  *
<span class="p_del">- * This function calls the cpu_chain notifiers with CPU_STARTING.</span>
<span class="p_del">- * It must be called by the arch code on the new cpu, before the new cpu</span>
<span class="p_del">- * enables interrupts and before the &quot;boot&quot; cpu returns from __cpu_up().</span>
<span class="p_add">+ * Returns 0 if successful, otherwise a proper error code</span>
  */
<span class="p_del">-void notify_cpu_starting(unsigned int cpu)</span>
<span class="p_add">+int __cpuhp_setup_state(enum cpuhp_state state,</span>
<span class="p_add">+			const char *name, bool invoke,</span>
<span class="p_add">+			int (*startup)(unsigned int cpu),</span>
<span class="p_add">+			int (*teardown)(unsigned int cpu))</span>
 {
<span class="p_del">-	unsigned long val = CPU_STARTING;</span>
<span class="p_add">+	int cpu, ret = 0;</span>
<span class="p_add">+	int dyn_state = 0;</span>
 
<span class="p_del">-#ifdef CONFIG_PM_SLEEP_SMP</span>
<span class="p_del">-	if (frozen_cpus != NULL &amp;&amp; cpumask_test_cpu(cpu, frozen_cpus))</span>
<span class="p_del">-		val = CPU_STARTING_FROZEN;</span>
<span class="p_del">-#endif /* CONFIG_PM_SLEEP_SMP */</span>
<span class="p_del">-	cpu_notify(val, (void *)(long)cpu);</span>
<span class="p_add">+	if (cpuhp_cb_check(state) || !name)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	get_online_cpus();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* currently assignments for the ONLINE state are possible */</span>
<span class="p_add">+	if (state == CPUHP_AP_ONLINE_DYN) {</span>
<span class="p_add">+		dyn_state = 1;</span>
<span class="p_add">+		ret = cpuhp_reserve_state(state);</span>
<span class="p_add">+		if (ret &lt; 0)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		state = ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	cpuhp_store_callbacks(state, name, startup, teardown);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!invoke || !startup)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Try to call the startup callback for each present cpu</span>
<span class="p_add">+	 * depending on the hotplug state of the cpu.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for_each_present_cpu(cpu) {</span>
<span class="p_add">+		struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+		int cpustate = st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (cpustate &lt; state)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = cpuhp_issue_call(cpu, state, startup, true);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			cpuhp_rollback_install(cpu, state, teardown);</span>
<span class="p_add">+			cpuhp_store_callbacks(state, NULL, NULL, NULL);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	put_online_cpus();</span>
<span class="p_add">+	if (!ret &amp;&amp; dyn_state)</span>
<span class="p_add">+		return state;</span>
<span class="p_add">+	return ret;</span>
 }
<span class="p_add">+EXPORT_SYMBOL(__cpuhp_setup_state);</span>
 
<span class="p_del">-#endif /* CONFIG_SMP */</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __cpuhp_remove_state - Remove the callbacks for an hotplug machine state</span>
<span class="p_add">+ * @state:	The state to remove</span>
<span class="p_add">+ * @invoke:	If true, the teardown function is invoked for cpus where</span>
<span class="p_add">+ *		cpu state &gt;= @state</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The teardown callback is currently not allowed to fail. Think</span>
<span class="p_add">+ * about module removal!</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __cpuhp_remove_state(enum cpuhp_state state, bool invoke)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int (*teardown)(unsigned int cpu) = cpuhp_get_teardown_cb(state);</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(cpuhp_cb_check(state));</span>
<span class="p_add">+</span>
<span class="p_add">+	get_online_cpus();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!invoke || !teardown)</span>
<span class="p_add">+		goto remove;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Call the teardown callback for each present cpu depending</span>
<span class="p_add">+	 * on the hotplug state of the cpu. This function is not</span>
<span class="p_add">+	 * allowed to fail currently!</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for_each_present_cpu(cpu) {</span>
<span class="p_add">+		struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, cpu);</span>
<span class="p_add">+		int cpustate = st-&gt;state;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (cpustate &gt;= state)</span>
<span class="p_add">+			cpuhp_issue_call(cpu, state, teardown, false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+remove:</span>
<span class="p_add">+	cpuhp_store_callbacks(state, NULL, NULL, NULL);</span>
<span class="p_add">+	put_online_cpus();</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__cpuhp_remove_state);</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_SYSFS) &amp;&amp; defined(CONFIG_HOTPLUG_CPU)</span>
<span class="p_add">+static ssize_t show_cpuhp_state(struct device *dev,</span>
<span class="p_add">+				struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, dev-&gt;id);</span>
<span class="p_add">+</span>
<span class="p_add">+	return sprintf(buf, &quot;%d\n&quot;, st-&gt;state);</span>
<span class="p_add">+}</span>
<span class="p_add">+static DEVICE_ATTR(state, 0444, show_cpuhp_state, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t write_cpuhp_target(struct device *dev,</span>
<span class="p_add">+				  struct device_attribute *attr,</span>
<span class="p_add">+				  const char *buf, size_t count)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, dev-&gt;id);</span>
<span class="p_add">+	struct cpuhp_step *sp;</span>
<span class="p_add">+	int target, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = kstrtoint(buf, 10, &amp;target);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL</span>
<span class="p_add">+	if (target &lt; CPUHP_OFFLINE || target &gt; CPUHP_ONLINE)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	if (target != CPUHP_OFFLINE &amp;&amp; target != CPUHP_ONLINE)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = lock_device_hotplug_sysfs();</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	sp = cpuhp_get_step(target);</span>
<span class="p_add">+	ret = !sp-&gt;name || sp-&gt;cant_stop ? -EINVAL : 0;</span>
<span class="p_add">+	mutex_unlock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (st-&gt;state &lt; target)</span>
<span class="p_add">+		ret = do_cpu_up(dev-&gt;id, target);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		ret = do_cpu_down(dev-&gt;id, target);</span>
<span class="p_add">+</span>
<span class="p_add">+	unlock_device_hotplug();</span>
<span class="p_add">+	return ret ? ret : count;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t show_cpuhp_target(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpuhp_cpu_state *st = per_cpu_ptr(&amp;cpuhp_state, dev-&gt;id);</span>
<span class="p_add">+</span>
<span class="p_add">+	return sprintf(buf, &quot;%d\n&quot;, st-&gt;target);</span>
<span class="p_add">+}</span>
<span class="p_add">+static DEVICE_ATTR(target, 0644, show_cpuhp_target, write_cpuhp_target);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *cpuhp_cpu_attrs[] = {</span>
<span class="p_add">+	&amp;dev_attr_state.attr,</span>
<span class="p_add">+	&amp;dev_attr_target.attr,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute_group cpuhp_cpu_attr_group = {</span>
<span class="p_add">+	.attrs = cpuhp_cpu_attrs,</span>
<span class="p_add">+	.name = &quot;hotplug&quot;,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t show_cpuhp_states(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ssize_t cur, res = 0;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	for (i = CPUHP_OFFLINE; i &lt;= CPUHP_ONLINE; i++) {</span>
<span class="p_add">+		struct cpuhp_step *sp = cpuhp_get_step(i);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (sp-&gt;name) {</span>
<span class="p_add">+			cur = sprintf(buf, &quot;%3d: %s\n&quot;, i, sp-&gt;name);</span>
<span class="p_add">+			buf += cur;</span>
<span class="p_add">+			res += cur;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	mutex_unlock(&amp;cpuhp_state_mutex);</span>
<span class="p_add">+	return res;</span>
<span class="p_add">+}</span>
<span class="p_add">+static DEVICE_ATTR(states, 0444, show_cpuhp_states, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *cpuhp_cpu_root_attrs[] = {</span>
<span class="p_add">+	&amp;dev_attr_states.attr,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute_group cpuhp_cpu_root_attr_group = {</span>
<span class="p_add">+	.attrs = cpuhp_cpu_root_attrs,</span>
<span class="p_add">+	.name = &quot;hotplug&quot;,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cpuhp_sysfs_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = sysfs_create_group(&amp;cpu_subsys.dev_root-&gt;kobj,</span>
<span class="p_add">+				 &amp;cpuhp_cpu_root_attr_group);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_possible_cpu(cpu) {</span>
<span class="p_add">+		struct device *dev = get_cpu_device(cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!dev)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		ret = sysfs_create_group(&amp;dev-&gt;kobj, &amp;cpuhp_cpu_attr_group);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+device_initcall(cpuhp_sysfs_init);</span>
<span class="p_add">+#endif</span>
 
 /*
  * cpu_bit_bitmap[] is a special, &quot;compressed&quot; data structure that
<span class="p_chunk">@@ -789,3 +1673,25 @@</span> <span class="p_context"> void init_cpu_online(const struct cpumask *src)</span>
 {
 	cpumask_copy(&amp;__cpu_online_mask, src);
 }
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Activate the first processor.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init boot_cpu_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Mark the boot cpu &quot;present&quot;, &quot;online&quot; etc for SMP and UP case */</span>
<span class="p_add">+	set_cpu_online(cpu, true);</span>
<span class="p_add">+	set_cpu_active(cpu, true);</span>
<span class="p_add">+	set_cpu_present(cpu, true);</span>
<span class="p_add">+	set_cpu_possible(cpu, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Must be called _AFTER_ setting up the per_cpu areas</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init boot_cpu_state_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	per_cpu_ptr(&amp;cpuhp_state, smp_processor_id())-&gt;state = CPUHP_ONLINE;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c</span>
<span class="p_header">index e41dd4131f7a..85b41341272e 100644</span>
<span class="p_header">--- a/kernel/rcu/tree.c</span>
<span class="p_header">+++ b/kernel/rcu/tree.c</span>
<span class="p_chunk">@@ -2607,28 +2607,6 @@</span> <span class="p_context"> static void rcu_cleanup_dead_rnp(struct rcu_node *rnp_leaf)</span>
 }
 
 /*
<span class="p_del">- * The CPU is exiting the idle loop into the arch_cpu_idle_dead()</span>
<span class="p_del">- * function.  We now remove it from the rcu_node tree&#39;s -&gt;qsmaskinit</span>
<span class="p_del">- * bit masks.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void rcu_cleanup_dying_idle_cpu(int cpu, struct rcu_state *rsp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	unsigned long mask;</span>
<span class="p_del">-	struct rcu_data *rdp = per_cpu_ptr(rsp-&gt;rda, cpu);</span>
<span class="p_del">-	struct rcu_node *rnp = rdp-&gt;mynode;  /* Outgoing CPU&#39;s rdp &amp; rnp. */</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!IS_ENABLED(CONFIG_HOTPLUG_CPU))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Remove outgoing CPU from mask in the leaf rcu_node structure. */</span>
<span class="p_del">-	mask = rdp-&gt;grpmask;</span>
<span class="p_del">-	raw_spin_lock_irqsave_rcu_node(rnp, flags); /* Enforce GP memory-order guarantee. */</span>
<span class="p_del">-	rnp-&gt;qsmaskinitnext &amp;= ~mask;</span>
<span class="p_del">-	raw_spin_unlock_irqrestore(&amp;rnp-&gt;lock, flags);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * The CPU has been completely removed, and some other CPU is reporting
  * this fact from process context.  Do the remainder of the cleanup,
  * including orphaning the outgoing CPU&#39;s RCU callbacks, and also
<span class="p_chunk">@@ -4247,6 +4225,43 @@</span> <span class="p_context"> static void rcu_prepare_cpu(int cpu)</span>
 		rcu_init_percpu_data(cpu, rsp);
 }
 
<span class="p_add">+#ifdef CONFIG_HOTPLUG_CPU</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The CPU is exiting the idle loop into the arch_cpu_idle_dead()</span>
<span class="p_add">+ * function.  We now remove it from the rcu_node tree&#39;s -&gt;qsmaskinit</span>
<span class="p_add">+ * bit masks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void rcu_cleanup_dying_idle_cpu(int cpu, struct rcu_state *rsp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned long mask;</span>
<span class="p_add">+	struct rcu_data *rdp = per_cpu_ptr(rsp-&gt;rda, cpu);</span>
<span class="p_add">+	struct rcu_node *rnp = rdp-&gt;mynode;  /* Outgoing CPU&#39;s rdp &amp; rnp. */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_HOTPLUG_CPU))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Remove outgoing CPU from mask in the leaf rcu_node structure. */</span>
<span class="p_add">+	mask = rdp-&gt;grpmask;</span>
<span class="p_add">+	raw_spin_lock_irqsave_rcu_node(rnp, flags); /* Enforce GP memory-order guarantee. */</span>
<span class="p_add">+	rnp-&gt;qsmaskinitnext &amp;= ~mask;</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;rnp-&gt;lock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void rcu_report_dead(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rcu_state *rsp;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* QS for any half-done expedited RCU-sched GP. */</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	rcu_report_exp_rdp(&amp;rcu_sched_state,</span>
<span class="p_add">+			   this_cpu_ptr(rcu_sched_state.rda), true);</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+	for_each_rcu_flavor(rsp)</span>
<span class="p_add">+		rcu_cleanup_dying_idle_cpu(cpu, rsp);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Handle CPU online/offline notification events.
  */
<span class="p_chunk">@@ -4278,17 +4293,6 @@</span> <span class="p_context"> int rcu_cpu_notify(struct notifier_block *self,</span>
 		for_each_rcu_flavor(rsp)
 			rcu_cleanup_dying_cpu(rsp);
 		break;
<span class="p_del">-	case CPU_DYING_IDLE:</span>
<span class="p_del">-		/* QS for any half-done expedited RCU-sched GP. */</span>
<span class="p_del">-		preempt_disable();</span>
<span class="p_del">-		rcu_report_exp_rdp(&amp;rcu_sched_state,</span>
<span class="p_del">-				   this_cpu_ptr(rcu_sched_state.rda), true);</span>
<span class="p_del">-		preempt_enable();</span>
<span class="p_del">-</span>
<span class="p_del">-		for_each_rcu_flavor(rsp) {</span>
<span class="p_del">-			rcu_cleanup_dying_idle_cpu(cpu, rsp);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		break;</span>
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN:
 	case CPU_UP_CANCELED:
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 9503d590e5ef..626646396ca0 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -5692,16 +5692,6 @@</span> <span class="p_context"> static int sched_cpu_active(struct notifier_block *nfb,</span>
 		set_cpu_rq_start_time();
 		return NOTIFY_OK;
 
<span class="p_del">-	case CPU_ONLINE:</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * At this point a starting CPU has marked itself as online via</span>
<span class="p_del">-		 * set_cpu_online(). But it might not yet have marked itself</span>
<span class="p_del">-		 * as active, which is essential from here on.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		set_cpu_active(cpu, true);</span>
<span class="p_del">-		stop_machine_unpark(cpu);</span>
<span class="p_del">-		return NOTIFY_OK;</span>
<span class="p_del">-</span>
 	case CPU_DOWN_FAILED:
 		set_cpu_active(cpu, true);
 		return NOTIFY_OK;
<span class="p_header">diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c</span>
<span class="p_header">index 544a7133cbd1..bd12c6c714ec 100644</span>
<span class="p_header">--- a/kernel/sched/idle.c</span>
<span class="p_header">+++ b/kernel/sched/idle.c</span>
<span class="p_chunk">@@ -4,6 +4,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/cpu.h&gt;
 #include &lt;linux/cpuidle.h&gt;
<span class="p_add">+#include &lt;linux/cpuhotplug.h&gt;</span>
 #include &lt;linux/tick.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/stackprotector.h&gt;
<span class="p_chunk">@@ -193,8 +194,6 @@</span> <span class="p_context"> exit_idle:</span>
 	rcu_idle_exit();
 }
 
<span class="p_del">-DEFINE_PER_CPU(bool, cpu_dead_idle);</span>
<span class="p_del">-</span>
 /*
  * Generic idle loop implementation
  *
<span class="p_chunk">@@ -221,10 +220,7 @@</span> <span class="p_context"> static void cpu_idle_loop(void)</span>
 			rmb();
 
 			if (cpu_is_offline(smp_processor_id())) {
<span class="p_del">-				rcu_cpu_notify(NULL, CPU_DYING_IDLE,</span>
<span class="p_del">-					       (void *)(long)smp_processor_id());</span>
<span class="p_del">-				smp_mb(); /* all activity before dead. */</span>
<span class="p_del">-				this_cpu_write(cpu_dead_idle, true);</span>
<span class="p_add">+				cpuhp_report_idle_dead();</span>
 				arch_cpu_idle_dead();
 			}
 
<span class="p_chunk">@@ -291,5 +287,6 @@</span> <span class="p_context"> void cpu_startup_entry(enum cpuhp_state state)</span>
 	boot_init_stack_canary();
 #endif
 	arch_cpu_idle_prepare();
<span class="p_add">+	cpuhp_online_idle(state);</span>
 	cpu_idle_loop();
 }
<span class="p_header">diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="p_header">index d903c02223af..822ffb1ada3f 100644</span>
<span class="p_header">--- a/kernel/smp.c</span>
<span class="p_header">+++ b/kernel/smp.c</span>
<span class="p_chunk">@@ -569,6 +569,7 @@</span> <span class="p_context"> void __init smp_init(void)</span>
 	unsigned int cpu;
 
 	idle_threads_init();
<span class="p_add">+	cpuhp_threads_init();</span>
 
 	/* FIXME: This should be done in userspace --RR */
 	for_each_present_cpu(cpu) {
<span class="p_header">diff --git a/kernel/smpboot.c b/kernel/smpboot.c</span>
<span class="p_header">index d264f59bff56..13bc43d1fb22 100644</span>
<span class="p_header">--- a/kernel/smpboot.c</span>
<span class="p_header">+++ b/kernel/smpboot.c</span>
<span class="p_chunk">@@ -226,7 +226,7 @@</span> <span class="p_context"> static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cp</span>
 		kthread_unpark(tsk);
 }
 
<span class="p_del">-void smpboot_unpark_threads(unsigned int cpu)</span>
<span class="p_add">+int smpboot_unpark_threads(unsigned int cpu)</span>
 {
 	struct smp_hotplug_thread *cur;
 
<span class="p_chunk">@@ -235,6 +235,7 @@</span> <span class="p_context"> void smpboot_unpark_threads(unsigned int cpu)</span>
 		if (cpumask_test_cpu(cpu, cur-&gt;cpumask))
 			smpboot_unpark_thread(cur, cpu);
 	mutex_unlock(&amp;smpboot_threads_lock);
<span class="p_add">+	return 0;</span>
 }
 
 static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
<span class="p_chunk">@@ -245,7 +246,7 @@</span> <span class="p_context"> static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)</span>
 		kthread_park(tsk);
 }
 
<span class="p_del">-void smpboot_park_threads(unsigned int cpu)</span>
<span class="p_add">+int smpboot_park_threads(unsigned int cpu)</span>
 {
 	struct smp_hotplug_thread *cur;
 
<span class="p_chunk">@@ -253,6 +254,7 @@</span> <span class="p_context"> void smpboot_park_threads(unsigned int cpu)</span>
 	list_for_each_entry_reverse(cur, &amp;hotplug_threads, list)
 		smpboot_park_thread(cur, cpu);
 	mutex_unlock(&amp;smpboot_threads_lock);
<span class="p_add">+	return 0;</span>
 }
 
 static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
<span class="p_header">diff --git a/kernel/smpboot.h b/kernel/smpboot.h</span>
<span class="p_header">index 72415a0eb955..485b81cfab34 100644</span>
<span class="p_header">--- a/kernel/smpboot.h</span>
<span class="p_header">+++ b/kernel/smpboot.h</span>
<span class="p_chunk">@@ -14,7 +14,9 @@</span> <span class="p_context"> static inline void idle_threads_init(void) { }</span>
 #endif
 
 int smpboot_create_threads(unsigned int cpu);
<span class="p_del">-void smpboot_park_threads(unsigned int cpu);</span>
<span class="p_del">-void smpboot_unpark_threads(unsigned int cpu);</span>
<span class="p_add">+int smpboot_park_threads(unsigned int cpu);</span>
<span class="p_add">+int smpboot_unpark_threads(unsigned int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+void __init cpuhp_threads_init(void);</span>
 
 #endif
<span class="p_header">diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug</span>
<span class="p_header">index 8bfd1aca7a3d..f28f7fad452f 100644</span>
<span class="p_header">--- a/lib/Kconfig.debug</span>
<span class="p_header">+++ b/lib/Kconfig.debug</span>
<span class="p_chunk">@@ -1442,6 +1442,19 @@</span> <span class="p_context"> config DEBUG_BLOCK_EXT_DEVT</span>
 
 	  Say N if you are unsure.
 
<span class="p_add">+config CPU_HOTPLUG_STATE_CONTROL</span>
<span class="p_add">+	bool &quot;Enable CPU hotplug state control&quot;</span>
<span class="p_add">+	depends on DEBUG_KERNEL</span>
<span class="p_add">+	depends on HOTPLUG_CPU</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Allows to write steps between &quot;offline&quot; and &quot;online&quot; to the CPUs</span>
<span class="p_add">+	  sysfs target file so states can be stepped granular. This is a debug</span>
<span class="p_add">+	  option for now as the hotplug machinery cannot be stopped and</span>
<span class="p_add">+	  restarted at arbitrary points yet.</span>
<span class="p_add">+</span>
<span class="p_add">+	  Say N if your are unsure.</span>
<span class="p_add">+</span>
 config NOTIFIER_ERROR_INJECTION
 	tristate &quot;Notifier error injection&quot;
 	depends on DEBUG_KERNEL

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



