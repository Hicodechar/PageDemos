
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,08/14] mm: thp: enable thp migration in generic path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,08/14] mm: thp: enable thp migration in generic path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 5, 2017, 4:12 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170205161252.85004-9-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9556157/mbox/"
   >mbox</a>
|
   <a href="/patch/9556157/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9556157/">/patch/9556157/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F1BB360236 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:15:49 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E2B6526419
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:15:49 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D757826785; Sun,  5 Feb 2017 16:15:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B13EB26419
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun,  5 Feb 2017 16:15:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752481AbdBEQPe (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 5 Feb 2017 11:15:34 -0500
Received: from out1-smtp.messagingengine.com ([66.111.4.25]:48417 &quot;EHLO
	out1-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752174AbdBEQOf (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 5 Feb 2017 11:14:35 -0500
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by mailout.nyi.internal (Postfix) with ESMTP id 38BC5206D0;
	Sun,  5 Feb 2017 11:14:34 -0500 (EST)
Received: from frontend2 ([10.202.2.161])
	by compute3.internal (MEProxy); Sun, 05 Feb 2017 11:14:34 -0500
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-me-sender:x-me-sender:x-sasl-enc:x-sasl-enc; s=mesmtp; bh=BJZ
	uZJvXfR6pU+KNaEN34cl5w9A=; b=yAS+2evCTsZm7c9QqaQhEezaPif2ElLD6/o
	rGGdyRhwYWwF0ao1fzC2Rihu8B61ThiLdtZpywgbTE0c+DUwkMrOWty8p5zP9FLm
	dg18SXcEQc1IhL6GwQW3Yyi7dyzNZsMb5LQeHhE9l2eBARggUQI0UMGl0QH3Dy4L
	61APQ+5w=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-me-sender:x-me-sender:x-sasl-enc
	:x-sasl-enc; s=smtpout; bh=BJZuZJvXfR6pU+KNaEN34cl5w9A=; b=qLjeS
	KPVwr2gQgCZuW82iymS+Utk2udY8BdbyLQQySRKYc9VrAHPHU+lJHzQzO09R2SBl
	nC4NCd3Ub/Eik8o110omGnW+LpM4m7nO4bGPaXaTkyPTu1lu0DVng4lbvpSRQaoh
	blE7OGd+TyRawqDPdMKySK2hw9w/vcszO3UDaU=
X-ME-Sender: &lt;xms:ak-XWEzz7skuCgAEMp1rpXqvDr3VOqmcBvKlTpXfM-vr8zc9vLZXyA&gt;
X-Sasl-enc: SnljLmGRXQgkDXtogpYiRrOB3tIehqCikxSRmGOsxz8j 1486311273
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id CCAB12443E;
	Sun,  5 Feb 2017 11:14:33 -0500 (EST)
From: Zi Yan &lt;zi.yan@sent.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	kirill.shutemov@linux.intel.com
Cc: akpm@linux-foundation.org, minchan@kernel.org, vbabka@suse.cz,
	mgorman@techsingularity.net, n-horiguchi@ah.jp.nec.com,
	khandual@linux.vnet.ibm.com, zi.yan@cs.rutgers.edu
Subject: [PATCH v3 08/14] mm: thp: enable thp migration in generic path
Date: Sun,  5 Feb 2017 11:12:46 -0500
Message-Id: &lt;20170205161252.85004-9-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170205161252.85004-1-zi.yan@sent.com&gt;
References: &lt;20170205161252.85004-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - Feb. 5, 2017, 4:12 p.m.</div>
<pre class="content">
<span class="from">From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

This patch adds thp migration&#39;s core code, including conversions
between a PMD entry and a swap entry, setting PMD migration entry,
removing PMD migration entry, and waiting on PMD migration entries.

This patch makes it possible to support thp migration.
If you fail to allocate a destination page as a thp, you just split
the source thp as we do now, and then enter the normal page migration.
If you succeed to allocate destination thp, you enter thp migration.
Subsequent patches actually enable thp migration for each caller of
page migration by allowing its get_new_page() callback to
allocate thps.

ChangeLog v1 -&gt; v2:
- support pte-mapped thp, doubly-mapped thp
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

ChangeLog v2 -&gt; v3:
- use page_vma_mapped_walk()
<span class="signed-off-by">
Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
---
 arch/x86/include/asm/pgtable_64.h |   2 +
 include/linux/swapops.h           |  70 +++++++++++++++++-
 mm/huge_memory.c                  | 151 ++++++++++++++++++++++++++++++++++----
 mm/migrate.c                      |  29 +++++++-
 mm/page_vma_mapped.c              |  13 +++-
 mm/pgtable-generic.c              |   3 +-
 mm/rmap.c                         |  14 +++-
 7 files changed, 259 insertions(+), 23 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Feb. 9, 2017, 9:15 a.m.</div>
<pre class="content">
On Sun, Feb 05, 2017 at 11:12:46AM -0500, Zi Yan wrote:
<span class="quote">&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; allocate thps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h |   2 +</span>
<span class="quote">&gt;  include/linux/swapops.h           |  70 +++++++++++++++++-</span>
<span class="quote">&gt;  mm/huge_memory.c                  | 151 ++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  mm/migrate.c                      |  29 +++++++-</span>
<span class="quote">&gt;  mm/page_vma_mapped.c              |  13 +++-</span>
<span class="quote">&gt;  mm/pgtable-generic.c              |   3 +-</span>
<span class="quote">&gt;  mm/rmap.c                         |  14 +++-</span>
<span class="quote">&gt;  7 files changed, 259 insertions(+), 23 deletions(-)</span>
<span class="quote">&gt; </span>
...
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 6893c47428b6..fd54bbdc16cf 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1613,20 +1613,51 @@ int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">
&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>

This block is exactly equal to the one in else block below,
So you can factor out into some function.
<span class="quote">
&gt;  		} else {</span>
<span class="quote">&gt; -			if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; -				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">
&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">
&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; +			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -2634,3 +2665,97 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		pmd_t pmdswp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>

Don&#39;t you have to put mmu_notifier_invalidate_range_* outside this if block?

Thanks,
Naoya Horiguchi
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		if (pmd_dirty(pmdval))</span>
<span class="quote">&gt; +			set_page_dirty(page);</span>
<span class="quote">&gt; +		entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="quote">&gt; +		pmdswp = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +		set_pmd_at(mm, address, pvmw-&gt;pmd, pmdswp);</span>
<span class="quote">&gt; +		page_remove_rmap(page, true);</span>
<span class="quote">&gt; +		put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pteval;</span>
<span class="quote">&gt; +		struct page *subpage = page - page_to_pfn(page) + pte_pfn(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pteval = ptep_clear_flush(vma, address, pvmw-&gt;pte);</span>
<span class="quote">&gt; +		if (pte_dirty(pteval))</span>
<span class="quote">&gt; +			set_page_dirty(subpage);</span>
<span class="quote">&gt; +		entry = make_migration_entry(subpage, pte_write(pteval));</span>
<span class="quote">&gt; +		swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, pvmw-&gt;pte, swp_pte);</span>
<span class="quote">&gt; +		page_remove_rmap(subpage, false);</span>
<span class="quote">&gt; +		put_page(subpage);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* PMD-mapped THP  */</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; +		unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; +		pmd_t pmde;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="quote">&gt; +		pmdp_huge_clear_flush_notify(vma, mmun_start, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="quote">&gt; +		flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; +			mlock_vma_page(new);</span>
<span class="quote">&gt; +		update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +		pte_t *ptep = pvmw-&gt;pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt; +		if (pte_swp_soft_dirty(*pvmw-&gt;pte))</span>
<span class="quote">&gt; +			pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pte = maybe_mkwrite(pte, vma);</span>
<span class="quote">&gt; +		flush_dcache_page(new);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, ptep, pte);</span>
<span class="quote">&gt; +		if (PageAnon(new))</span>
<span class="quote">&gt; +			page_add_anon_rmap(new, vma, address, false);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			page_add_file_rmap(new, false);</span>
<span class="quote">&gt; +		update_mmu_cache(vma, address, ptep);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 95e8580dc902..84181a3668c6 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -214,6 +214,12 @@ static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;  			linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt; +		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		get_page(new);</span>
<span class="quote">&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>
<span class="quote">&gt; @@ -327,6 +333,27 @@ void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	__migration_entry_wait(mm, pte, ptl);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (!is_pmd_migration_entry(*pmd))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="quote">&gt; +	if (!get_page_unless_zero(page))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	wait_on_page_locked(page);</span>
<span class="quote">&gt; +	put_page(page);</span>
<span class="quote">&gt; +	return;</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_BLOCK</span>
<span class="quote">&gt;  /* Returns true if all buffers are successfully locked */</span>
<span class="quote">&gt;  static bool buffer_migrate_lock_buffers(struct buffer_head *head,</span>
<span class="quote">&gt; @@ -1085,7 +1112,7 @@ static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (unlikely(PageTransHuge(page))) {</span>
<span class="quote">&gt; +	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
<span class="quote">&gt;  		lock_page(page);</span>
<span class="quote">&gt;  		rc = split_huge_page(page);</span>
<span class="quote">&gt;  		unlock_page(page);</span>
<span class="quote">&gt; diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; index a23001a22c15..0ed3aee62d50 100644</span>
<span class="quote">&gt; --- a/mm/page_vma_mapped.c</span>
<span class="quote">&gt; +++ b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; @@ -137,16 +137,23 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
<span class="quote">&gt;  	if (!pud_present(*pud))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;  		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);</span>
<span class="quote">&gt; -		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="quote">&gt; -			return not_found(pvmw);</span>
<span class="quote">&gt;  		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {</span>
<span class="quote">&gt;  			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			if (pmd_page(*pvmw-&gt;pmd) != page)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			return true;</span>
<span class="quote">&gt; +		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="quote">&gt; +				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				if (migration_entry_to_page(entry) != page)</span>
<span class="quote">&gt; +					return not_found(pvmw);</span>
<span class="quote">&gt; +				return true;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			return not_found(pvmw);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			/* THP pmd was split under us: handle on pte level */</span>
<span class="quote">&gt;  			spin_unlock(pvmw-&gt;ptl);</span>
<span class="quote">&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt; index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="quote">&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt; @@ -118,7 +118,8 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="quote">&gt; +	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="quote">&gt; +		  !pmd_devmap(*pmdp));</span>
<span class="quote">&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;  	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 16789b936e3a..b33216668fa4 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1304,6 +1304,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	struct rmap_private *rp = arg;</span>
<span class="quote">&gt;  	enum ttu_flags flags = rp-&gt;flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt;  		return SWAP_AGAIN;</span>
<span class="quote">&gt; @@ -1314,12 +1315,19 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (page_vma_mapped_walk(&amp;pvmw)) {</span>
<span class="quote">&gt; +		/* THP migration */</span>
<span class="quote">&gt; +		if (flags &amp; TTU_MIGRATION) {</span>
<span class="quote">&gt; +			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		/* Unexpected PMD-mapped THP */</span>
<span class="quote">&gt; +		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);</span>
<span class="quote">&gt;  		address = pvmw.address;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		/* Unexpected PMD-mapped THP? */</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If the page is mlock()d, we cannot swap it out.</span>
<span class="quote">&gt;  		 * If it&#39;s recently referenced (perhaps page_referenced</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123671">Zi Yan</a> - Feb. 9, 2017, 3:17 p.m.</div>
<pre class="content">
On 9 Feb 2017, at 3:15, Naoya Horiguchi wrote:
<span class="quote">
&gt; On Sun, Feb 05, 2017 at 11:12:46AM -0500, Zi Yan wrote:</span>
<span class="quote">&gt;&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt;&gt; allocate thps.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/pgtable_64.h |   2 +</span>
<span class="quote">&gt;&gt;  include/linux/swapops.h           |  70 +++++++++++++++++-</span>
<span class="quote">&gt;&gt;  mm/huge_memory.c                  | 151 ++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;&gt;  mm/migrate.c                      |  29 +++++++-</span>
<span class="quote">&gt;&gt;  mm/page_vma_mapped.c              |  13 +++-</span>
<span class="quote">&gt;&gt;  mm/pgtable-generic.c              |   3 +-</span>
<span class="quote">&gt;&gt;  mm/rmap.c                         |  14 +++-</span>
<span class="quote">&gt;&gt;  7 files changed, 259 insertions(+), 23 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; index 6893c47428b6..fd54bbdc16cf 100644</span>
<span class="quote">&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; @@ -1613,20 +1613,51 @@ int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;  	} else {</span>
<span class="quote">&gt;&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt;&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt;&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt;&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +		struct page *page;</span>
<span class="quote">&gt;&gt; +		int migration = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt;&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt;&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt;&gt; +								      pmd);</span>
<span class="quote">&gt;&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt;&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +			} else {</span>
<span class="quote">&gt;&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt;&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This block is exactly equal to the one in else block below,</span>
<span class="quote">&gt; So you can factor out into some function.</span>

Of course, I will do that.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;  		} else {</span>
<span class="quote">&gt;&gt; -			if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt;&gt; -				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt;&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt;&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt;&gt; +								      pmd);</span>
<span class="quote">&gt;&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt;&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +			} else {</span>
<span class="quote">&gt;&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt;&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt;&gt; +			migration = 1;</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;  		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt; @@ -2634,3 +2665,97 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt;&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt;&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt;&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt;&gt; +		pmd_t pmdswp;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt;&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Don&#39;t you have to put mmu_notifier_invalidate_range_* outside this if block?</span>

I think I need to add mmu_notifier_invalidate_page() in else block.

Because Kirill&#39;s page_vma_mapped_walk() iterates each PMD or PTE.
In set_pmd_migration_etnry(), if the page is PMD-mapped, it will be called once
with PMD, then mmu_notifier_invalidate_range_* can be used. On the other hand,
if the page is PTE-mapped, the function will be called 1~512 times depending
on how many PTEs are present. mmu_notifier_invalidate_range_* is not suitable.
mmu_notifier_invalidate_page() on the corresponding subpage should work.



--
Best Regards
Yan Zi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Feb. 9, 2017, 11:04 p.m.</div>
<pre class="content">
On Thu, Feb 09, 2017 at 09:17:01AM -0600, Zi Yan wrote:
<span class="quote">&gt; On 9 Feb 2017, at 3:15, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Sun, Feb 05, 2017 at 11:12:46AM -0500, Zi Yan wrote:</span>
<span class="quote">&gt; &gt;&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; &gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; &gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; &gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; &gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; &gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; &gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; &gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; &gt;&gt; allocate thps.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; &gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; &gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/include/asm/pgtable_64.h |   2 +</span>
<span class="quote">&gt; &gt;&gt;  include/linux/swapops.h           |  70 +++++++++++++++++-</span>
<span class="quote">&gt; &gt;&gt;  mm/huge_memory.c                  | 151 ++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt; &gt;&gt;  mm/migrate.c                      |  29 +++++++-</span>
<span class="quote">&gt; &gt;&gt;  mm/page_vma_mapped.c              |  13 +++-</span>
<span class="quote">&gt; &gt;&gt;  mm/pgtable-generic.c              |   3 +-</span>
<span class="quote">&gt; &gt;&gt;  mm/rmap.c                         |  14 +++-</span>
<span class="quote">&gt; &gt;&gt;  7 files changed, 259 insertions(+), 23 deletions(-)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; index 6893c47428b6..fd54bbdc16cf 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; @@ -1613,20 +1613,51 @@ int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt;  		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt;&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;&gt;  	} else {</span>
<span class="quote">&gt; &gt;&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt;&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt;&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt;&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; &gt;&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; &gt;&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; &gt;&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt;&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +		struct page *page;</span>
<span class="quote">&gt; &gt;&gt; +		int migration = 0;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; &gt;&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt;&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt;&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; &gt;&gt; +								      pmd);</span>
<span class="quote">&gt; &gt;&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; &gt;&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; &gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +			} else {</span>
<span class="quote">&gt; &gt;&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; &gt;&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; &gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; &gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +			}</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This block is exactly equal to the one in else block below,</span>
<span class="quote">&gt; &gt; So you can factor out into some function.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Of course, I will do that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;  		} else {</span>
<span class="quote">&gt; &gt;&gt; -			if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; &gt;&gt; -				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; &gt;&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt;&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; &gt;&gt; +								      pmd);</span>
<span class="quote">&gt; &gt;&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; &gt;&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; &gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +			} else {</span>
<span class="quote">&gt; &gt;&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; &gt;&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; &gt;&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; &gt;&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt;&gt; +			}</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; &gt;&gt; +			migration = 1;</span>
<span class="quote">&gt; &gt;&gt;  		}</span>
<span class="quote">&gt; &gt;&gt;  		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;&gt;  	}</span>
<span class="quote">&gt; &gt;&gt; @@ -2634,3 +2665,97 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt; &gt;&gt;  }</span>
<span class="quote">&gt; &gt;&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt; &gt;&gt;  #endif</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; &gt;&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; &gt;&gt; +		struct page *page)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; &gt;&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; &gt;&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt; &gt;&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; &gt;&gt; +		pmd_t pmdswp;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt; &gt;&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Don&#39;t you have to put mmu_notifier_invalidate_range_* outside this if block?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think I need to add mmu_notifier_invalidate_page() in else block.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because Kirill&#39;s page_vma_mapped_walk() iterates each PMD or PTE.</span>
<span class="quote">&gt; In set_pmd_migration_etnry(), if the page is PMD-mapped, it will be called once</span>
<span class="quote">&gt; with PMD, then mmu_notifier_invalidate_range_* can be used. On the other hand,</span>
<span class="quote">&gt; if the page is PTE-mapped, the function will be called 1~512 times depending</span>
<span class="quote">&gt; on how many PTEs are present. mmu_notifier_invalidate_range_* is not suitable.</span>
<span class="quote">&gt; mmu_notifier_invalidate_page() on the corresponding subpage should work.</span>
<span class="quote">&gt; </span>

Ah right, mmu_notifier_invalidate_page() is better for PTE-mapped thp.

Thanks,
Naoya
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123671">Zi Yan</a> - Feb. 14, 2017, 8:13 p.m.</div>
<pre class="content">
Hi Kirill,

I just wonder if you have time to take a look at this
patch, since it is based on your page_vma_mapped_walk()
function and I also changed your page_vma_mapped_walk()
code to beware of pmd_migration_entry.

Thanks.


On 5 Feb 2017, at 10:12, Zi Yan wrote:
<span class="quote">
&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; allocate thps.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h |   2 +</span>
<span class="quote">&gt;  include/linux/swapops.h           |  70 +++++++++++++++++-</span>
<span class="quote">&gt;  mm/huge_memory.c                  | 151 ++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  mm/migrate.c                      |  29 +++++++-</span>
<span class="quote">&gt;  mm/page_vma_mapped.c              |  13 +++-</span>
<span class="quote">&gt;  mm/pgtable-generic.c              |   3 +-</span>
<span class="quote">&gt;  mm/rmap.c                         |  14 +++-</span>
<span class="quote">&gt;  7 files changed, 259 insertions(+), 23 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index 768eccc85553..0277f7755f3a 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -182,7 +182,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 5c3a5f3e7eec..6625bea13869 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt;  	return p;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -163,6 +164,71 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 6893c47428b6..fd54bbdc16cf 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1613,20 +1613,51 @@ int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; -			if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; -				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; +					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; +			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -2634,3 +2665,97 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		pmd_t pmdswp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		if (pmd_dirty(pmdval))</span>
<span class="quote">&gt; +			set_page_dirty(page);</span>
<span class="quote">&gt; +		entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="quote">&gt; +		pmdswp = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +		set_pmd_at(mm, address, pvmw-&gt;pmd, pmdswp);</span>
<span class="quote">&gt; +		page_remove_rmap(page, true);</span>
<span class="quote">&gt; +		put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pteval;</span>
<span class="quote">&gt; +		struct page *subpage = page - page_to_pfn(page) + pte_pfn(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pteval = ptep_clear_flush(vma, address, pvmw-&gt;pte);</span>
<span class="quote">&gt; +		if (pte_dirty(pteval))</span>
<span class="quote">&gt; +			set_page_dirty(subpage);</span>
<span class="quote">&gt; +		entry = make_migration_entry(subpage, pte_write(pteval));</span>
<span class="quote">&gt; +		swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, pvmw-&gt;pte, swp_pte);</span>
<span class="quote">&gt; +		page_remove_rmap(subpage, false);</span>
<span class="quote">&gt; +		put_page(subpage);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* PMD-mapped THP  */</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; +		unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; +		pmd_t pmde;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="quote">&gt; +		pmdp_huge_clear_flush_notify(vma, mmun_start, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="quote">&gt; +		flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; +			mlock_vma_page(new);</span>
<span class="quote">&gt; +		update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +		pte_t *ptep = pvmw-&gt;pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt; +		if (pte_swp_soft_dirty(*pvmw-&gt;pte))</span>
<span class="quote">&gt; +			pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pte = maybe_mkwrite(pte, vma);</span>
<span class="quote">&gt; +		flush_dcache_page(new);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, ptep, pte);</span>
<span class="quote">&gt; +		if (PageAnon(new))</span>
<span class="quote">&gt; +			page_add_anon_rmap(new, vma, address, false);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			page_add_file_rmap(new, false);</span>
<span class="quote">&gt; +		update_mmu_cache(vma, address, ptep);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 95e8580dc902..84181a3668c6 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -214,6 +214,12 @@ static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;  			linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt; +		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		get_page(new);</span>
<span class="quote">&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>
<span class="quote">&gt; @@ -327,6 +333,27 @@ void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	__migration_entry_wait(mm, pte, ptl);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (!is_pmd_migration_entry(*pmd))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="quote">&gt; +	if (!get_page_unless_zero(page))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	wait_on_page_locked(page);</span>
<span class="quote">&gt; +	put_page(page);</span>
<span class="quote">&gt; +	return;</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_BLOCK</span>
<span class="quote">&gt;  /* Returns true if all buffers are successfully locked */</span>
<span class="quote">&gt;  static bool buffer_migrate_lock_buffers(struct buffer_head *head,</span>
<span class="quote">&gt; @@ -1085,7 +1112,7 @@ static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -	if (unlikely(PageTransHuge(page))) {</span>
<span class="quote">&gt; +	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
<span class="quote">&gt;  		lock_page(page);</span>
<span class="quote">&gt;  		rc = split_huge_page(page);</span>
<span class="quote">&gt;  		unlock_page(page);</span>
<span class="quote">&gt; diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; index a23001a22c15..0ed3aee62d50 100644</span>
<span class="quote">&gt; --- a/mm/page_vma_mapped.c</span>
<span class="quote">&gt; +++ b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; @@ -137,16 +137,23 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
<span class="quote">&gt;  	if (!pud_present(*pud))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;  		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);</span>
<span class="quote">&gt; -		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="quote">&gt; -			return not_found(pvmw);</span>
<span class="quote">&gt;  		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {</span>
<span class="quote">&gt;  			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			if (pmd_page(*pvmw-&gt;pmd) != page)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			return true;</span>
<span class="quote">&gt; +		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="quote">&gt; +				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				if (migration_entry_to_page(entry) != page)</span>
<span class="quote">&gt; +					return not_found(pvmw);</span>
<span class="quote">&gt; +				return true;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			return not_found(pvmw);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			/* THP pmd was split under us: handle on pte level */</span>
<span class="quote">&gt;  			spin_unlock(pvmw-&gt;ptl);</span>
<span class="quote">&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt; index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="quote">&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt; @@ -118,7 +118,8 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="quote">&gt; +	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="quote">&gt; +		  !pmd_devmap(*pmdp));</span>
<span class="quote">&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;  	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 16789b936e3a..b33216668fa4 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1304,6 +1304,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	struct rmap_private *rp = arg;</span>
<span class="quote">&gt;  	enum ttu_flags flags = rp-&gt;flags;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt;  		return SWAP_AGAIN;</span>
<span class="quote">&gt; @@ -1314,12 +1315,19 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	while (page_vma_mapped_walk(&amp;pvmw)) {</span>
<span class="quote">&gt; +		/* THP migration */</span>
<span class="quote">&gt; +		if (flags &amp; TTU_MIGRATION) {</span>
<span class="quote">&gt; +			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		/* Unexpected PMD-mapped THP */</span>
<span class="quote">&gt; +		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);</span>
<span class="quote">&gt;  		address = pvmw.address;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -		/* Unexpected PMD-mapped THP? */</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If the page is mlock()d, we cannot swap it out.</span>
<span class="quote">&gt;  		 * If it&#39;s recently referenced (perhaps page_referenced</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>


--
Best Regards
Yan Zi
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 768eccc85553..0277f7755f3a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -182,7 +182,9 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \
 					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
<span class="p_add">+#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
<span class="p_add">+#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
 
 extern int kern_addr_valid(unsigned long addr);
 extern void cleanup_highmap(void);
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3e7eec..6625bea13869 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -103,7 +103,8 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_del">-	BUG_ON(!PageLocked(page));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="p_add">+</span>
 	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,
 			page_to_pfn(page));
 }
<span class="p_chunk">@@ -126,7 +127,7 @@</span> <span class="p_context"> static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
 	 * Any use of migration entries may only occur while the
 	 * corresponding page is locked
 	 */
<span class="p_del">-	BUG_ON(!PageLocked(p));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(p)));</span>
 	return p;
 }
 
<span class="p_chunk">@@ -163,6 +164,71 @@</span> <span class="p_context"> static inline int is_write_migration_entry(swp_entry_t entry)</span>
 
 #endif
 
<span class="p_add">+struct page_vma_mapped_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="p_add">+	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="p_add">+	return __swp_entry_to_pmd(arch_entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return (pmd_t){ 0 };</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MEMORY_FAILURE
 
 extern atomic_long_t num_poisoned_pages __read_mostly;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 6893c47428b6..fd54bbdc16cf 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1613,20 +1613,51 @@</span> <span class="p_context"> int __zap_huge_pmd_locked(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);
 		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_del">-		if (PageAnon(page)) {</span>
<span class="p_del">-			pgtable_t pgtable;</span>
<span class="p_del">-			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_del">-			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_del">-			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			if (PageAnon(page)) {</span>
<span class="p_add">+				pgtable_t pgtable;</span>
<span class="p_add">+</span>
<span class="p_add">+				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="p_add">+								      pmd);</span>
<span class="p_add">+				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				if (arch_needs_pgtable_deposit())</span>
<span class="p_add">+					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			}</span>
 		} else {
<span class="p_del">-			if (arch_needs_pgtable_deposit())</span>
<span class="p_del">-				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+			if (PageAnon(page)) {</span>
<span class="p_add">+				pgtable_t pgtable;</span>
<span class="p_add">+</span>
<span class="p_add">+				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="p_add">+								      pmd);</span>
<span class="p_add">+				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				if (arch_needs_pgtable_deposit())</span>
<span class="p_add">+					zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="p_add">+			migration = 1;</span>
 		}
 		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);
 	}
<span class="p_chunk">@@ -2634,3 +2665,97 @@</span> <span class="p_context"> static int __init split_huge_pages_debugfs(void)</span>
 }
 late_initcall(split_huge_pages_debugfs);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	pmd_t pmdval;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="p_add">+		pmd_t pmdswp;</span>
<span class="p_add">+</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="p_add">+				address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+		if (pmd_dirty(pmdval))</span>
<span class="p_add">+			set_page_dirty(page);</span>
<span class="p_add">+		entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="p_add">+		pmdswp = swp_entry_to_pmd(entry);</span>
<span class="p_add">+		set_pmd_at(mm, address, pvmw-&gt;pmd, pmdswp);</span>
<span class="p_add">+		page_remove_rmap(page, true);</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="p_add">+				address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	} else { /* pte-mapped thp */</span>
<span class="p_add">+		pte_t pteval;</span>
<span class="p_add">+		struct page *subpage = page - page_to_pfn(page) + pte_pfn(*pvmw-&gt;pte);</span>
<span class="p_add">+		pte_t swp_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		pteval = ptep_clear_flush(vma, address, pvmw-&gt;pte);</span>
<span class="p_add">+		if (pte_dirty(pteval))</span>
<span class="p_add">+			set_page_dirty(subpage);</span>
<span class="p_add">+		entry = make_migration_entry(subpage, pte_write(pteval));</span>
<span class="p_add">+		swp_pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+		set_pte_at(mm, address, pvmw-&gt;pte, swp_pte);</span>
<span class="p_add">+		page_remove_rmap(subpage, false);</span>
<span class="p_add">+		put_page(subpage);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PMD-mapped THP  */</span>
<span class="p_add">+	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="p_add">+		unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+		unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="p_add">+		pmd_t pmde;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+		get_page(new);</span>
<span class="p_add">+		pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="p_add">+		if (is_write_migration_entry(entry))</span>
<span class="p_add">+			pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+		page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="p_add">+		pmdp_huge_clear_flush_notify(vma, mmun_start, pvmw-&gt;pmd);</span>
<span class="p_add">+		set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="p_add">+		flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+			mlock_vma_page(new);</span>
<span class="p_add">+		update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	} else { /* pte-mapped thp */</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+		pte_t *ptep = pvmw-&gt;pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="p_add">+		get_page(new);</span>
<span class="p_add">+		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="p_add">+		if (pte_swp_soft_dirty(*pvmw-&gt;pte))</span>
<span class="p_add">+			pte = pte_mksoft_dirty(pte);</span>
<span class="p_add">+		if (is_write_migration_entry(entry))</span>
<span class="p_add">+			pte = maybe_mkwrite(pte, vma);</span>
<span class="p_add">+		flush_dcache_page(new);</span>
<span class="p_add">+		set_pte_at(mm, address, ptep, pte);</span>
<span class="p_add">+		if (PageAnon(new))</span>
<span class="p_add">+			page_add_anon_rmap(new, vma, address, false);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			page_add_file_rmap(new, false);</span>
<span class="p_add">+		update_mmu_cache(vma, address, ptep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 95e8580dc902..84181a3668c6 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -214,6 +214,12 @@</span> <span class="p_context"> static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
 		new = page - pvmw.page-&gt;index +
 			linear_page_index(vma, pvmw.address);
 
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="p_add">+			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		get_page(new);
 		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));
 		if (pte_swp_soft_dirty(*pvmw.pte))
<span class="p_chunk">@@ -327,6 +333,27 @@</span> <span class="p_context"> void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
 	__migration_entry_wait(mm, pte, ptl);
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pmd_lock(mm, pmd);</span>
<span class="p_add">+	if (!is_pmd_migration_entry(*pmd))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="p_add">+	if (!get_page_unless_zero(page))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	wait_on_page_locked(page);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLOCK
 /* Returns true if all buffers are successfully locked */
 static bool buffer_migrate_lock_buffers(struct buffer_head *head,
<span class="p_chunk">@@ -1085,7 +1112,7 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 		goto out;
 	}
 
<span class="p_del">-	if (unlikely(PageTransHuge(page))) {</span>
<span class="p_add">+	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
 		lock_page(page);
 		rc = split_huge_page(page);
 		unlock_page(page);
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index a23001a22c15..0ed3aee62d50 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -137,16 +137,23 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	if (!pud_present(*pud))
 		return false;
 	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);
<span class="p_del">-	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
 		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);
<span class="p_del">-		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="p_del">-			return not_found(pvmw);</span>
 		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {
 			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)
 				return not_found(pvmw);
 			if (pmd_page(*pvmw-&gt;pmd) != page)
 				return not_found(pvmw);
 			return true;
<span class="p_add">+		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="p_add">+				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+				if (migration_entry_to_page(entry) != page)</span>
<span class="p_add">+					return not_found(pvmw);</span>
<span class="p_add">+				return true;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			return not_found(pvmw);</span>
 		} else {
 			/* THP pmd was split under us: handle on pte level */
 			spin_unlock(pvmw-&gt;ptl);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -118,7 +118,8 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 {
 	pmd_t pmd;
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="p_add">+	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="p_add">+		  !pmd_devmap(*pmdp));</span>
 	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return pmd;
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 16789b936e3a..b33216668fa4 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1304,6 +1304,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	struct rmap_private *rp = arg;
 	enum ttu_flags flags = rp-&gt;flags;
 
<span class="p_add">+</span>
 	/* munlock has nothing to gain from examining un-locked vmas */
 	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))
 		return SWAP_AGAIN;
<span class="p_chunk">@@ -1314,12 +1315,19 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	}
 
 	while (page_vma_mapped_walk(&amp;pvmw)) {
<span class="p_add">+		/* THP migration */</span>
<span class="p_add">+		if (flags &amp; TTU_MIGRATION) {</span>
<span class="p_add">+			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="p_add">+				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* Unexpected PMD-mapped THP */</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="p_add">+</span>
 		subpage = page - page_to_pfn(page) + pte_pfn(*pvmw.pte);
 		address = pvmw.address;
 
<span class="p_del">-		/* Unexpected PMD-mapped THP? */</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!pvmw.pte, page);</span>
<span class="p_del">-</span>
 		/*
 		 * If the page is mlock()d, we cannot swap it out.
 		 * If it&#39;s recently referenced (perhaps page_referenced

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



