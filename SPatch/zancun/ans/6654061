
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] scheduler changes for v4.2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] scheduler changes for v4.2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2015, 8:08 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150622080839.GA5773@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6654061/mbox/"
   >mbox</a>
|
   <a href="/patch/6654061/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6654061/">/patch/6654061/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id B9BEAC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 08:09:58 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 497CE20636
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 08:09:44 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 31BBC20624
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 08:09:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756173AbbFVIJK (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 22 Jun 2015 04:09:10 -0400
Received: from mail-wi0-f178.google.com ([209.85.212.178]:36936 &quot;EHLO
	mail-wi0-f178.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933168AbbFVIIq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 22 Jun 2015 04:08:46 -0400
Received: by wicgi11 with SMTP id gi11so67025130wic.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 22 Jun 2015 01:08:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version:content-type
	:content-disposition:content-transfer-encoding:user-agent;
	bh=ph0LzVLH9GRPUPpK6PGtC1twEiXTPAGdp4Vqy8dfS4c=;
	b=lBtYBkhFGdD0KuZ6yE2hJh5HC+arl2lUz1/6lIvYRBnsjPbTqNQ3NdLrXrW1wMmXtl
	zD22Mc4BbWClSJH2XAg0o7XjaaIlbhR8Mndn7AsEay8WfHtjbfcc9A+Ql7Kt/b9MX/Hb
	FDjk04eHHGNwp1kuCLIW9AbP6o6ndj/m1UCY3PKU+0EliuHaapfDs0+UMVjJouhA1CrD
	q83lIeNfIncLQL8Ngt4y2wYklfBqMSSzWbKzDVx+nS2gDqLI9FfDql1mZp6aAaA5e846
	us2ck3UgkhwN5KLUpbq8kEr6MhpbmgtZexktuhU9GBasgRzbLMeHpqXyWjZyQ4r/BEzT
	gDaw==
X-Received: by 10.180.84.194 with SMTP id b2mr28684041wiz.36.1434960523949; 
	Mon, 22 Jun 2015 01:08:43 -0700 (PDT)
Received: from gmail.com (54033495.catv.pool.telekom.hu. [84.3.52.149])
	by mx.google.com with ESMTPSA id
	a19sm15982572wiv.2.2015.06.22.01.08.41
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 22 Jun 2015 01:08:42 -0700 (PDT)
Date: Mon, 22 Jun 2015 10:08:39 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] scheduler changes for v4.2
Message-ID: &lt;20150622080839.GA5773@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.2 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=ham version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 22, 2015, 8:08 a.m.</div>
<pre class="content">
Linus,

Please pull the latest sched-core-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git sched-core-for-linus

   # HEAD: 6fab54101923044712baee429ff573f03b99fc47 sched/deadline: Remove needless parameter in dl_runtime_exceeded()

The main changes are:

  - lockless wakeup support for futexes and IPC message queues.
    (Davidlohr Bueso, Peter Zijlstra)

  - Replace spinlocks with atomics in thread_group_cputimer(), to improve 
    scalability (Jason Low)

  - NUMA balancing improvements (Rik van Riel)

  - SCHED_DEADLINE improvements (Wanpeng Li)

  - clean up and reorganize preemption helpers (Frederic Weisbecker)

  - decouple page fault disabling machinery from the preemption counter, to 
    improve debuggability and robustness. (David Hildenbrand)

  - SCHED_DEADLINE documentation updates (Luca Abeni)

  - topology CPU masks cleanups. (Bartosz Golaszewski)

  - /proc/sched_debug improvements. (Srikar Dronamraju)

 Thanks,

	Ingo

------------------&gt;
Bartosz Golaszewski (9):
      sched/topology: Rename topology_thread_cpumask() to topology_sibling_cpumask()
      Documentation: Update cputopology.txt
      coretemp: Replace cpu_sibling_mask() with topology_sibling_cpumask()
      powernow-k8: Replace cpu_core_mask() with topology_core_cpumask()
      p4-clockmod: Replace cpu_sibling_mask() with topology_sibling_cpumask()
      acpi-cpufreq: Replace cpu_**_mask() with topology_**_cpumask()
      speedstep-ich: Replace cpu_sibling_mask() with topology_sibling_cpumask()
      x86: Replace cpu_**_mask() with topology_**_cpumask()
      x86: Remove cpu_sibling_mask() and cpu_core_mask()

Ben Segall (1):
      sched/fair: Prevent throttling in early pick_next_task_fair()

David Hildenbrand (15):
      sched/preempt, mm/fault: Count pagefault_disable() levels in pagefault_disabled
      sched/preempt, mm/fault: Trigger might_sleep() in might_fault() with disabled pagefaults
      mm/uaccess, mm/fault: Clarify that uaccess may only sleep if pagefaults are enabled
      sched/preempt, mm/kmap: Explicitly disable/enable preemption in kmap_atomic_*
      sched/preempt, mm/kmap, MIPS: Disable preemption in kmap_coherent() explicitly
      mm/fault, arch: Use pagefault_disable() to check for disabled pagefaults in the handler
      mm/fault, drm/i915: Use pagefault_disabled() to check for disabled pagefaults
      sched/preempt, futex: Disable preemption in UP futex_atomic_op_inuser() explicitly
      sched/preempt, futex: Disable preemption in UP futex_atomic_cmpxchg_inatomic() explicitly
      sched/preempt, arm/futex: Disable preemption in UP futex_atomic_cmpxchg_inatomic() explicitly
      sched/preempt, arm/futex: Disable preemption in UP futex_atomic_op_inuser() explicitly
      sched/preempt, futex: Update comments to clarify that preemption doesn&#39;t have to be disabled
      sched/preempt, powerpc: Disable preemption in enable_kernel_altivec() explicitly
      sched/preempt, MIPS: Properly lock access to the FPU
      sched/preempt, mm/fault: Decouple preemption from the page fault logic

Davidlohr Bueso (2):
      futex: Implement lockless wakeups
      ipc/mqueue: Implement lockless pipelined wakeups

Frederic Weisbecker (9):
      sched/preempt: Merge preempt_mask.h into preempt.h
      sched/preempt: Rearrange a few symbols after headers merge
      sched/preempt: Rename PREEMPT_CHECK_OFFSET to PREEMPT_DISABLE_OFFSET
      sched/preempt: Optimize preemption operations on __schedule() callers
      sched/preempt: Fix out of date comment
      sched/preempt: Remove PREEMPT_ACTIVE unmasking off in_atomic()
      sched: Make preempt_schedule_context() function-tracing safe
      preempt: Use preempt_schedule_context() as the official tracing preemption point
      preempt: Reorganize the notrace definitions a bit

Huang Rui (1):
      sched/x86: Drop repeated word from mwait_idle() comment

Jason Low (6):
      sched, timer: Convert usages of ACCESS_ONCE() in the scheduler to READ_ONCE()/WRITE_ONCE()
      sched/numa: Document usages of mm-&gt;numa_scan_seq
      sched, timer: Replace spinlocks with atomics in thread_group_cputimer(), to improve scalability
      sched, timer: Provide an atomic &#39;struct task_cputime&#39; data structure
      sched, timer: Use the atomic task_cputime in thread_group_cputimer
      sched, timer: Fix documentation for &#39;struct thread_group_cputimer&#39;

Luca Abeni (8):
      sched/dl/Documentation: Switch to American English
      sched/dl/Documentation: Fix typos
      sched/dl/Documentation: Use consistent naming
      sched/dl/Documentation: Clarify indexing notation
      sched/dl/Documentation: Add some notes on EDF schedulability
      sched/dl/Documentation: Add some references
      sched/dl/Documentation: Clarify the relationship between tasks&#39; deadlines and absolute scheduling deadlines
      sched/dl/Documentation: Split Section 3

Mathieu Desnoyers (1):
      sched/preempt: Fix preempt notifiers documentation about hlist_del() within unsafe iteration

Nicholas Mc Guire (2):
      sched/core: Remove unnecessary down/up conversion
      sched: Fix function declaration return type mismatch

Nikolay Borisov (1):
      sched: Remove redundant #ifdef

Palmer Dabbelt (3):
      signals, sched: Change all uses of JOBCTL_* from &#39;int&#39; to &#39;long&#39;
      sched/wait: Change wait_on_bit*() to take an unsigned long *, not a void *
      signals, ptrace, sched: Fix a misaligned load inside ptrace_attach()

Paul Gortmaker (1):
      sched/core: Remove __cpuinit section tag that crept back in

Peter Zijlstra (6):
      sched: Move the loadavg code to a more obvious location
      sched: Implement lockless wake-queues
      sched/wait: Introduce TASK_NOLOAD and TASK_IDLE
      mm/fault, um: Fix compile error
      sched/stop_machine: Fix deadlock between multiple stop_two_cpus()
      sched/preempt: Add static_key() to preempt_notifiers

Rik van Riel (3):
      sched/numa: Reduce conflict between fbq_classify_rq() and migration
      Revert 095bebf61a46 (&quot;sched/numa: Do not move past the balance point if unbalanced&quot;)
      sched/numa: Only consider less busy nodes as numa balancing destinations

Srikar Dronamraju (3):
      sched/debug: Properly format runnable tasks in /proc/sched_debug
      sched/debug: Replace vruntime with wait_sum in /proc/sched_debug
      sched/debug: Add sum_sleep_runtime to /proc/&lt;pid&gt;/sched

Tobias Klauser (1):
      sched/autogroup: Remove unnecessary #ifdef guards

Wanpeng Li (5):
      sched/deadline: Optimize pull_dl_task()
      sched/deadline: Make init_sched_dl_class() __init
      sched/deadline: Reduce rq lock contention by eliminating locking of non-feasible target
      sched/deadline: Drop duplicate init_sched_dl_class() declaration
      sched: Remove superfluous resetting of the p-&gt;dl_throttled flag

Zhiqiang Zhang (2):
      sched/dl/Documentation: Correct the definition of density as C_i/min{D_i,P_i}
      sched/deadline: Remove needless parameter in dl_runtime_exceeded()


 Documentation/cputopology.txt                      |  37 +-
 Documentation/scheduler/sched-deadline.txt         | 184 ++++++++--
 arch/alpha/mm/fault.c                              |   5 +-
 arch/arc/include/asm/futex.h                       |  10 +-
 arch/arc/mm/fault.c                                |   2 +-
 arch/arm/include/asm/futex.h                       |  13 +-
 arch/arm/include/asm/topology.h                    |   2 +-
 arch/arm/mm/fault.c                                |   2 +-
 arch/arm/mm/highmem.c                              |   3 +
 arch/arm64/include/asm/futex.h                     |   4 +-
 arch/arm64/include/asm/topology.h                  |   2 +-
 arch/arm64/mm/fault.c                              |   2 +-
 arch/avr32/include/asm/uaccess.h                   |  12 +-
 arch/avr32/mm/fault.c                              |   4 +-
 arch/cris/mm/fault.c                               |   6 +-
 arch/frv/mm/fault.c                                |   4 +-
 arch/frv/mm/highmem.c                              |   2 +
 arch/hexagon/include/asm/uaccess.h                 |   3 +-
 arch/ia64/include/asm/topology.h                   |   2 +-
 arch/ia64/mm/fault.c                               |   4 +-
 arch/m32r/include/asm/uaccess.h                    |  30 +-
 arch/m32r/mm/fault.c                               |   8 +-
 arch/m68k/include/asm/irqflags.h                   |   3 -
 arch/m68k/mm/fault.c                               |   4 +-
 arch/metag/mm/fault.c                              |   2 +-
 arch/metag/mm/highmem.c                            |   4 +-
 arch/microblaze/include/asm/uaccess.h              |   6 +-
 arch/microblaze/mm/fault.c                         |   8 +-
 arch/microblaze/mm/highmem.c                       |   4 +-
 arch/mips/include/asm/topology.h                   |   2 +-
 arch/mips/include/asm/uaccess.h                    |  45 ++-
 arch/mips/kernel/signal-common.h                   |   9 +-
 arch/mips/mm/fault.c                               |   4 +-
 arch/mips/mm/highmem.c                             |   5 +-
 arch/mips/mm/init.c                                |   2 +
 arch/mn10300/include/asm/highmem.h                 |   3 +
 arch/mn10300/mm/fault.c                            |   4 +-
 arch/nios2/mm/fault.c                              |   2 +-
 arch/parisc/include/asm/cacheflush.h               |   2 +
 arch/parisc/kernel/traps.c                         |   4 +-
 arch/parisc/mm/fault.c                             |   4 +-
 arch/powerpc/include/asm/topology.h                |   2 +-
 arch/powerpc/lib/vmx-helper.c                      |  11 +-
 arch/powerpc/mm/fault.c                            |   9 +-
 arch/powerpc/mm/highmem.c                          |   4 +-
 arch/powerpc/mm/tlb_nohash.c                       |   2 +-
 arch/s390/include/asm/topology.h                   |   3 +-
 arch/s390/include/asm/uaccess.h                    |  15 +-
 arch/s390/mm/fault.c                               |   2 +-
 arch/score/include/asm/uaccess.h                   |  15 +-
 arch/score/mm/fault.c                              |   3 +-
 arch/sh/mm/fault.c                                 |   5 +-
 arch/sparc/include/asm/topology_64.h               |   2 +-
 arch/sparc/mm/fault_32.c                           |   4 +-
 arch/sparc/mm/fault_64.c                           |   4 +-
 arch/sparc/mm/highmem.c                            |   4 +-
 arch/sparc/mm/init_64.c                            |   2 +-
 arch/tile/include/asm/topology.h                   |   2 +-
 arch/tile/include/asm/uaccess.h                    |  18 +-
 arch/tile/mm/fault.c                               |   4 +-
 arch/tile/mm/highmem.c                             |   3 +-
 arch/um/kernel/trap.c                              |   5 +-
 arch/unicore32/mm/fault.c                          |   2 +-
 arch/x86/include/asm/preempt.h                     |   8 +-
 arch/x86/include/asm/smp.h                         |  10 -
 arch/x86/include/asm/topology.h                    |   2 +-
 arch/x86/include/asm/uaccess.h                     |  15 +-
 arch/x86/include/asm/uaccess_32.h                  |   6 +-
 arch/x86/kernel/cpu/perf_event_intel.c             |   6 +-
 arch/x86/kernel/cpu/proc.c                         |   3 +-
 arch/x86/kernel/i386_ksyms_32.c                    |   4 +-
 arch/x86/kernel/process.c                          |   7 +-
 arch/x86/kernel/smpboot.c                          |  42 +--
 arch/x86/kernel/tsc_sync.c                         |   2 +-
 arch/x86/kernel/x8664_ksyms_64.c                   |   4 +-
 arch/x86/lib/thunk_32.S                            |   4 +-
 arch/x86/lib/thunk_64.S                            |   4 +-
 arch/x86/lib/usercopy_32.c                         |   6 +-
 arch/x86/mm/fault.c                                |   5 +-
 arch/x86/mm/highmem_32.c                           |   3 +-
 arch/x86/mm/iomap_32.c                             |   2 +
 arch/xtensa/mm/fault.c                             |   4 +-
 arch/xtensa/mm/highmem.c                           |   2 +
 block/blk-mq-cpumap.c                              |   2 +-
 drivers/acpi/acpi_pad.c                            |   2 +-
 drivers/base/topology.c                            |   2 +-
 drivers/cpufreq/acpi-cpufreq.c                     |   5 +-
 drivers/cpufreq/p4-clockmod.c                      |   2 +-
 drivers/cpufreq/powernow-k8.c                      |  13 +-
 drivers/cpufreq/speedstep-ich.c                    |   2 +-
 drivers/crypto/vmx/aes.c                           |   8 +-
 drivers/crypto/vmx/aes_cbc.c                       |   6 +
 drivers/crypto/vmx/ghash.c                         |   8 +
 drivers/gpu/drm/i915/i915_gem_execbuffer.c         |   3 +-
 drivers/hwmon/coretemp.c                           |   3 +-
 drivers/net/ethernet/sfc/efx.c                     |   2 +-
 .../staging/lustre/lustre/libcfs/linux/linux-cpu.c |   2 +-
 drivers/staging/lustre/lustre/ptlrpc/service.c     |   4 +-
 include/asm-generic/futex.h                        |   7 +-
 include/asm-generic/preempt.h                      |   7 +-
 include/linux/bottom_half.h                        |   1 -
 include/linux/hardirq.h                            |   2 +-
 include/linux/highmem.h                            |   2 +
 include/linux/init_task.h                          |   5 +-
 include/linux/io-mapping.h                         |   2 +
 include/linux/kernel.h                             |   3 +-
 include/linux/lglock.h                             |   5 +
 include/linux/preempt.h                            | 159 +++++++--
 include/linux/preempt_mask.h                       | 117 -------
 include/linux/sched.h                              | 118 +++++--
 include/linux/topology.h                           |   6 +-
 include/linux/uaccess.h                            |  48 ++-
 include/linux/wait.h                               |  17 +-
 include/trace/events/sched.h                       |   3 +-
 ipc/mqueue.c                                       |  54 +--
 kernel/fork.c                                      |   8 +-
 kernel/futex.c                                     |  33 +-
 kernel/locking/lglock.c                            |  22 ++
 kernel/sched/Makefile                              |   2 +-
 kernel/sched/auto_group.c                          |   6 +-
 kernel/sched/auto_group.h                          |   2 +-
 kernel/sched/core.c                                | 136 +++++---
 kernel/sched/cputime.c                             |   2 +-
 kernel/sched/deadline.c                            |  51 ++-
 kernel/sched/debug.c                               |  11 +-
 kernel/sched/fair.c                                | 372 ++++++++++++++++-----
 kernel/sched/{proc.c =&gt; loadavg.c}                 | 236 ++-----------
 kernel/sched/rt.c                                  |   2 +-
 kernel/sched/sched.h                               |  11 +-
 kernel/sched/stats.h                               |  15 +-
 kernel/sched/wait.c                                |   4 +-
 kernel/signal.c                                    |   6 +-
 kernel/stop_machine.c                              |  42 +--
 kernel/time/posix-cpu-timers.c                     |  87 +++--
 lib/cpu_rmap.c                                     |   2 +-
 lib/radix-tree.c                                   |   2 +-
 lib/strnlen_user.c                                 |   6 +-
 mm/memory.c                                        |  18 +-
 138 files changed, 1442 insertions(+), 972 deletions(-)
 delete mode 100644 include/linux/preempt_mask.h
 rename kernel/sched/{proc.c =&gt; loadavg.c} (62%)

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/cputopology.txt b/Documentation/cputopology.txt</span>
<span class="p_header">index 0aad6deb2d96..12b1b25b4da9 100644</span>
<span class="p_header">--- a/Documentation/cputopology.txt</span>
<span class="p_header">+++ b/Documentation/cputopology.txt</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 
 Export CPU topology info via sysfs. Items (attributes) are similar
<span class="p_del">-to /proc/cpuinfo.</span>
<span class="p_add">+to /proc/cpuinfo output of some architectures:</span>
 
 1) /sys/devices/system/cpu/cpuX/topology/physical_package_id:
 
<span class="p_chunk">@@ -23,20 +23,35 @@</span> <span class="p_context"> to /proc/cpuinfo.</span>
 4) /sys/devices/system/cpu/cpuX/topology/thread_siblings:
 
 	internal kernel map of cpuX&#39;s hardware threads within the same
<span class="p_del">-	core as cpuX</span>
<span class="p_add">+	core as cpuX.</span>
 
<span class="p_del">-5) /sys/devices/system/cpu/cpuX/topology/core_siblings:</span>
<span class="p_add">+5) /sys/devices/system/cpu/cpuX/topology/thread_siblings_list:</span>
<span class="p_add">+</span>
<span class="p_add">+	human-readable list of cpuX&#39;s hardware threads within the same</span>
<span class="p_add">+	core as cpuX.</span>
<span class="p_add">+</span>
<span class="p_add">+6) /sys/devices/system/cpu/cpuX/topology/core_siblings:</span>
 
 	internal kernel map of cpuX&#39;s hardware threads within the same
 	physical_package_id.
 
<span class="p_del">-6) /sys/devices/system/cpu/cpuX/topology/book_siblings:</span>
<span class="p_add">+7) /sys/devices/system/cpu/cpuX/topology/core_siblings_list:</span>
<span class="p_add">+</span>
<span class="p_add">+	human-readable list of cpuX&#39;s hardware threads within the same</span>
<span class="p_add">+	physical_package_id.</span>
<span class="p_add">+</span>
<span class="p_add">+8) /sys/devices/system/cpu/cpuX/topology/book_siblings:</span>
 
 	internal kernel map of cpuX&#39;s hardware threads within the same
 	book_id.
 
<span class="p_add">+9) /sys/devices/system/cpu/cpuX/topology/book_siblings_list:</span>
<span class="p_add">+</span>
<span class="p_add">+	human-readable list of cpuX&#39;s hardware threads within the same</span>
<span class="p_add">+	book_id.</span>
<span class="p_add">+</span>
 To implement it in an architecture-neutral way, a new source file,
<span class="p_del">-drivers/base/topology.c, is to export the 4 or 6 attributes. The two book</span>
<span class="p_add">+drivers/base/topology.c, is to export the 6 or 9 attributes. The three book</span>
 related sysfs files will only be created if CONFIG_SCHED_BOOK is selected.
 
 For an architecture to support this feature, it must define some of
<span class="p_chunk">@@ -44,20 +59,22 @@</span> <span class="p_context"> For an architecture to support this feature, it must define some of</span>
 #define topology_physical_package_id(cpu)
 #define topology_core_id(cpu)
 #define topology_book_id(cpu)
<span class="p_del">-#define topology_thread_cpumask(cpu)</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)</span>
 #define topology_core_cpumask(cpu)
 #define topology_book_cpumask(cpu)
 
<span class="p_del">-The type of **_id is int.</span>
<span class="p_del">-The type of siblings is (const) struct cpumask *.</span>
<span class="p_add">+The type of **_id macros is int.</span>
<span class="p_add">+The type of **_cpumask macros is (const) struct cpumask *. The latter</span>
<span class="p_add">+correspond with appropriate **_siblings sysfs attributes (except for</span>
<span class="p_add">+topology_sibling_cpumask() which corresponds with thread_siblings).</span>
 
 To be consistent on all architectures, include/linux/topology.h
 provides default definitions for any of the above macros that are
 not defined by include/asm-XXX/topology.h:
 1) physical_package_id: -1
 2) core_id: 0
<span class="p_del">-3) thread_siblings: just the given CPU</span>
<span class="p_del">-4) core_siblings: just the given CPU</span>
<span class="p_add">+3) sibling_cpumask: just the given CPU</span>
<span class="p_add">+4) core_cpumask: just the given CPU</span>
 
 For architectures that don&#39;t support books (CONFIG_SCHED_BOOK) there are no
 default definitions for topology_book_id() and topology_book_cpumask().
<span class="p_header">diff --git a/Documentation/scheduler/sched-deadline.txt b/Documentation/scheduler/sched-deadline.txt</span>
<span class="p_header">index 21461a0441c1..e114513a2731 100644</span>
<span class="p_header">--- a/Documentation/scheduler/sched-deadline.txt</span>
<span class="p_header">+++ b/Documentation/scheduler/sched-deadline.txt</span>
<span class="p_chunk">@@ -8,6 +8,10 @@</span> <span class="p_context"> CONTENTS</span>
  1. Overview
  2. Scheduling algorithm
  3. Scheduling Real-Time Tasks
<span class="p_add">+   3.1 Definitions</span>
<span class="p_add">+   3.2 Schedulability Analysis for Uniprocessor Systems</span>
<span class="p_add">+   3.3 Schedulability Analysis for Multiprocessor Systems</span>
<span class="p_add">+   3.4 Relationship with SCHED_DEADLINE Parameters</span>
  4. Bandwidth management
    4.1 System-wide settings
    4.2 Task interface
<span class="p_chunk">@@ -43,7 +47,7 @@</span> <span class="p_context"> CONTENTS</span>
  &quot;deadline&quot;, to schedule tasks. A SCHED_DEADLINE task should receive
  &quot;runtime&quot; microseconds of execution time every &quot;period&quot; microseconds, and
  these &quot;runtime&quot; microseconds are available within &quot;deadline&quot; microseconds
<span class="p_del">- from the beginning of the period.  In order to implement this behaviour,</span>
<span class="p_add">+ from the beginning of the period.  In order to implement this behavior,</span>
  every time the task wakes up, the scheduler computes a &quot;scheduling deadline&quot;
  consistent with the guarantee (using the CBS[2,3] algorithm). Tasks are then
  scheduled using EDF[1] on these scheduling deadlines (the task with the
<span class="p_chunk">@@ -52,7 +56,7 @@</span> <span class="p_context"> CONTENTS</span>
  &quot;admission control&quot; strategy (see Section &quot;4. Bandwidth management&quot;) is used
  (clearly, if the system is overloaded this guarantee cannot be respected).
 
<span class="p_del">- Summing up, the CBS[2,3] algorithms assigns scheduling deadlines to tasks so</span>
<span class="p_add">+ Summing up, the CBS[2,3] algorithm assigns scheduling deadlines to tasks so</span>
  that each task runs for at most its runtime every period, avoiding any
  interference between different tasks (bandwidth isolation), while the EDF[1]
  algorithm selects the task with the earliest scheduling deadline as the one
<span class="p_chunk">@@ -63,7 +67,7 @@</span> <span class="p_context"> CONTENTS</span>
  In more details, the CBS algorithm assigns scheduling deadlines to
  tasks in the following way:
 
<span class="p_del">-  - Each SCHED_DEADLINE task is characterised by the &quot;runtime&quot;,</span>
<span class="p_add">+  - Each SCHED_DEADLINE task is characterized by the &quot;runtime&quot;,</span>
     &quot;deadline&quot;, and &quot;period&quot; parameters;
 
   - The state of the task is described by a &quot;scheduling deadline&quot;, and
<span class="p_chunk">@@ -78,7 +82,7 @@</span> <span class="p_context"> CONTENTS</span>
 
     then, if the scheduling deadline is smaller than the current time, or
     this condition is verified, the scheduling deadline and the
<span class="p_del">-    remaining runtime are re-initialised as</span>
<span class="p_add">+    remaining runtime are re-initialized as</span>
 
          scheduling deadline = current time + deadline
          remaining runtime = runtime
<span class="p_chunk">@@ -126,31 +130,37 @@</span> <span class="p_context"> CONTENTS</span>
  suited for periodic or sporadic real-time tasks that need guarantees on their
  timing behavior, e.g., multimedia, streaming, control applications, etc.
 
<span class="p_add">+3.1 Definitions</span>
<span class="p_add">+------------------------</span>
<span class="p_add">+</span>
  A typical real-time task is composed of a repetition of computation phases
  (task instances, or jobs) which are activated on a periodic or sporadic
  fashion.
<span class="p_del">- Each job J_j (where J_j is the j^th job of the task) is characterised by an</span>
<span class="p_add">+ Each job J_j (where J_j is the j^th job of the task) is characterized by an</span>
  arrival time r_j (the time when the job starts), an amount of computation
  time c_j needed to finish the job, and a job absolute deadline d_j, which
  is the time within which the job should be finished. The maximum execution
<span class="p_del">- time max_j{c_j} is called &quot;Worst Case Execution Time&quot; (WCET) for the task.</span>
<span class="p_add">+ time max{c_j} is called &quot;Worst Case Execution Time&quot; (WCET) for the task.</span>
  A real-time task can be periodic with period P if r_{j+1} = r_j + P, or
  sporadic with minimum inter-arrival time P is r_{j+1} &gt;= r_j + P. Finally,
  d_j = r_j + D, where D is the task&#39;s relative deadline.
<span class="p_del">- The utilisation of a real-time task is defined as the ratio between its</span>
<span class="p_add">+ Summing up, a real-time task can be described as</span>
<span class="p_add">+	Task = (WCET, D, P)</span>
<span class="p_add">+</span>
<span class="p_add">+ The utilization of a real-time task is defined as the ratio between its</span>
  WCET and its period (or minimum inter-arrival time), and represents
  the fraction of CPU time needed to execute the task.
 
<span class="p_del">- If the total utilisation sum_i(WCET_i/P_i) is larger than M (with M equal</span>
<span class="p_add">+ If the total utilization U=sum(WCET_i/P_i) is larger than M (with M equal</span>
  to the number of CPUs), then the scheduler is unable to respect all the
  deadlines.
<span class="p_del">- Note that total utilisation is defined as the sum of the utilisations</span>
<span class="p_add">+ Note that total utilization is defined as the sum of the utilizations</span>
  WCET_i/P_i over all the real-time tasks in the system. When considering
  multiple real-time tasks, the parameters of the i-th task are indicated
  with the &quot;_i&quot; suffix.
<span class="p_del">- Moreover, if the total utilisation is larger than M, then we risk starving</span>
<span class="p_add">+ Moreover, if the total utilization is larger than M, then we risk starving</span>
  non- real-time tasks by real-time tasks.
<span class="p_del">- If, instead, the total utilisation is smaller than M, then non real-time</span>
<span class="p_add">+ If, instead, the total utilization is smaller than M, then non real-time</span>
  tasks will not be starved and the system might be able to respect all the
  deadlines.
  As a matter of fact, in this case it is possible to provide an upper bound
<span class="p_chunk">@@ -159,38 +169,119 @@</span> <span class="p_context"> CONTENTS</span>
  More precisely, it can be proven that using a global EDF scheduler the
  maximum tardiness of each task is smaller or equal than
 	((M ? 1) · WCET_max ? WCET_min)/(M ? (M ? 2) · U_max) + WCET_max
<span class="p_del">- where WCET_max = max_i{WCET_i} is the maximum WCET, WCET_min=min_i{WCET_i}</span>
<span class="p_del">- is the minimum WCET, and U_max = max_i{WCET_i/P_i} is the maximum utilisation.</span>
<span class="p_add">+ where WCET_max = max{WCET_i} is the maximum WCET, WCET_min=min{WCET_i}</span>
<span class="p_add">+ is the minimum WCET, and U_max = max{WCET_i/P_i} is the maximum</span>
<span class="p_add">+ utilization[12].</span>
<span class="p_add">+</span>
<span class="p_add">+3.2 Schedulability Analysis for Uniprocessor Systems</span>
<span class="p_add">+------------------------</span>
 
  If M=1 (uniprocessor system), or in case of partitioned scheduling (each
  real-time task is statically assigned to one and only one CPU), it is
  possible to formally check if all the deadlines are respected.
  If D_i = P_i for all tasks, then EDF is able to respect all the deadlines
<span class="p_del">- of all the tasks executing on a CPU if and only if the total utilisation</span>
<span class="p_add">+ of all the tasks executing on a CPU if and only if the total utilization</span>
  of the tasks running on such a CPU is smaller or equal than 1.
  If D_i != P_i for some task, then it is possible to define the density of
<span class="p_del">- a task as C_i/min{D_i,T_i}, and EDF is able to respect all the deadlines</span>
<span class="p_del">- of all the tasks running on a CPU if the sum sum_i C_i/min{D_i,T_i} of the</span>
<span class="p_del">- densities of the tasks running on such a CPU is smaller or equal than 1</span>
<span class="p_del">- (notice that this condition is only sufficient, and not necessary).</span>
<span class="p_add">+ a task as WCET_i/min{D_i,P_i}, and EDF is able to respect all the deadlines</span>
<span class="p_add">+ of all the tasks running on a CPU if the sum of the densities of the tasks</span>
<span class="p_add">+ running on such a CPU is smaller or equal than 1:</span>
<span class="p_add">+	sum(WCET_i / min{D_i, P_i}) &lt;= 1</span>
<span class="p_add">+ It is important to notice that this condition is only sufficient, and not</span>
<span class="p_add">+ necessary: there are task sets that are schedulable, but do not respect the</span>
<span class="p_add">+ condition. For example, consider the task set {Task_1,Task_2} composed by</span>
<span class="p_add">+ Task_1=(50ms,50ms,100ms) and Task_2=(10ms,100ms,100ms).</span>
<span class="p_add">+ EDF is clearly able to schedule the two tasks without missing any deadline</span>
<span class="p_add">+ (Task_1 is scheduled as soon as it is released, and finishes just in time</span>
<span class="p_add">+ to respect its deadline; Task_2 is scheduled immediately after Task_1, hence</span>
<span class="p_add">+ its response time cannot be larger than 50ms + 10ms = 60ms) even if</span>
<span class="p_add">+	50 / min{50,100} + 10 / min{100, 100} = 50 / 50 + 10 / 100 = 1.1</span>
<span class="p_add">+ Of course it is possible to test the exact schedulability of tasks with</span>
<span class="p_add">+ D_i != P_i (checking a condition that is both sufficient and necessary),</span>
<span class="p_add">+ but this cannot be done by comparing the total utilization or density with</span>
<span class="p_add">+ a constant. Instead, the so called &quot;processor demand&quot; approach can be used,</span>
<span class="p_add">+ computing the total amount of CPU time h(t) needed by all the tasks to</span>
<span class="p_add">+ respect all of their deadlines in a time interval of size t, and comparing</span>
<span class="p_add">+ such a time with the interval size t. If h(t) is smaller than t (that is,</span>
<span class="p_add">+ the amount of time needed by the tasks in a time interval of size t is</span>
<span class="p_add">+ smaller than the size of the interval) for all the possible values of t, then</span>
<span class="p_add">+ EDF is able to schedule the tasks respecting all of their deadlines. Since</span>
<span class="p_add">+ performing this check for all possible values of t is impossible, it has been</span>
<span class="p_add">+ proven[4,5,6] that it is sufficient to perform the test for values of t</span>
<span class="p_add">+ between 0 and a maximum value L. The cited papers contain all of the</span>
<span class="p_add">+ mathematical details and explain how to compute h(t) and L.</span>
<span class="p_add">+ In any case, this kind of analysis is too complex as well as too</span>
<span class="p_add">+ time-consuming to be performed on-line. Hence, as explained in Section</span>
<span class="p_add">+ 4 Linux uses an admission test based on the tasks&#39; utilizations.</span>
<span class="p_add">+</span>
<span class="p_add">+3.3 Schedulability Analysis for Multiprocessor Systems</span>
<span class="p_add">+------------------------</span>
 
  On multiprocessor systems with global EDF scheduling (non partitioned
  systems), a sufficient test for schedulability can not be based on the
<span class="p_del">- utilisations (it can be shown that task sets with utilisations slightly</span>
<span class="p_del">- larger than 1 can miss deadlines regardless of the number of CPUs M).</span>
<span class="p_del">- However, as previously stated, enforcing that the total utilisation is smaller</span>
<span class="p_del">- than M is enough to guarantee that non real-time tasks are not starved and</span>
<span class="p_del">- that the tardiness of real-time tasks has an upper bound.</span>
<span class="p_add">+ utilizations or densities: it can be shown that even if D_i = P_i task</span>
<span class="p_add">+ sets with utilizations slightly larger than 1 can miss deadlines regardless</span>
<span class="p_add">+ of the number of CPUs.</span>
<span class="p_add">+</span>
<span class="p_add">+ Consider a set {Task_1,...Task_{M+1}} of M+1 tasks on a system with M</span>
<span class="p_add">+ CPUs, with the first task Task_1=(P,P,P) having period, relative deadline</span>
<span class="p_add">+ and WCET equal to P. The remaining M tasks Task_i=(e,P-1,P-1) have an</span>
<span class="p_add">+ arbitrarily small worst case execution time (indicated as &quot;e&quot; here) and a</span>
<span class="p_add">+ period smaller than the one of the first task. Hence, if all the tasks</span>
<span class="p_add">+ activate at the same time t, global EDF schedules these M tasks first</span>
<span class="p_add">+ (because their absolute deadlines are equal to t + P - 1, hence they are</span>
<span class="p_add">+ smaller than the absolute deadline of Task_1, which is t + P). As a</span>
<span class="p_add">+ result, Task_1 can be scheduled only at time t + e, and will finish at</span>
<span class="p_add">+ time t + e + P, after its absolute deadline. The total utilization of the</span>
<span class="p_add">+ task set is U = M · e / (P - 1) + P / P = M · e / (P - 1) + 1, and for small</span>
<span class="p_add">+ values of e this can become very close to 1. This is known as &quot;Dhall&#39;s</span>
<span class="p_add">+ effect&quot;[7]. Note: the example in the original paper by Dhall has been</span>
<span class="p_add">+ slightly simplified here (for example, Dhall more correctly computed</span>
<span class="p_add">+ lim_{e-&gt;0}U).</span>
<span class="p_add">+</span>
<span class="p_add">+ More complex schedulability tests for global EDF have been developed in</span>
<span class="p_add">+ real-time literature[8,9], but they are not based on a simple comparison</span>
<span class="p_add">+ between total utilization (or density) and a fixed constant. If all tasks</span>
<span class="p_add">+ have D_i = P_i, a sufficient schedulability condition can be expressed in</span>
<span class="p_add">+ a simple way:</span>
<span class="p_add">+	sum(WCET_i / P_i) &lt;= M - (M - 1) · U_max</span>
<span class="p_add">+ where U_max = max{WCET_i / P_i}[10]. Notice that for U_max = 1,</span>
<span class="p_add">+ M - (M - 1) · U_max becomes M - M + 1 = 1 and this schedulability condition</span>
<span class="p_add">+ just confirms the Dhall&#39;s effect. A more complete survey of the literature</span>
<span class="p_add">+ about schedulability tests for multi-processor real-time scheduling can be</span>
<span class="p_add">+ found in [11].</span>
<span class="p_add">+</span>
<span class="p_add">+ As seen, enforcing that the total utilization is smaller than M does not</span>
<span class="p_add">+ guarantee that global EDF schedules the tasks without missing any deadline</span>
<span class="p_add">+ (in other words, global EDF is not an optimal scheduling algorithm). However,</span>
<span class="p_add">+ a total utilization smaller than M is enough to guarantee that non real-time</span>
<span class="p_add">+ tasks are not starved and that the tardiness of real-time tasks has an upper</span>
<span class="p_add">+ bound[12] (as previously noted). Different bounds on the maximum tardiness</span>
<span class="p_add">+ experienced by real-time tasks have been developed in various papers[13,14],</span>
<span class="p_add">+ but the theoretical result that is important for SCHED_DEADLINE is that if</span>
<span class="p_add">+ the total utilization is smaller or equal than M then the response times of</span>
<span class="p_add">+ the tasks are limited.</span>
<span class="p_add">+</span>
<span class="p_add">+3.4 Relationship with SCHED_DEADLINE Parameters</span>
<span class="p_add">+------------------------</span>
 
<span class="p_del">- SCHED_DEADLINE can be used to schedule real-time tasks guaranteeing that</span>
<span class="p_del">- the jobs&#39; deadlines of a task are respected. In order to do this, a task</span>
<span class="p_del">- must be scheduled by setting:</span>
<span class="p_add">+ Finally, it is important to understand the relationship between the</span>
<span class="p_add">+ SCHED_DEADLINE scheduling parameters described in Section 2 (runtime,</span>
<span class="p_add">+ deadline and period) and the real-time task parameters (WCET, D, P)</span>
<span class="p_add">+ described in this section. Note that the tasks&#39; temporal constraints are</span>
<span class="p_add">+ represented by its absolute deadlines d_j = r_j + D described above, while</span>
<span class="p_add">+ SCHED_DEADLINE schedules the tasks according to scheduling deadlines (see</span>
<span class="p_add">+ Section 2).</span>
<span class="p_add">+ If an admission test is used to guarantee that the scheduling deadlines</span>
<span class="p_add">+ are respected, then SCHED_DEADLINE can be used to schedule real-time tasks</span>
<span class="p_add">+ guaranteeing that all the jobs&#39; deadlines of a task are respected.</span>
<span class="p_add">+ In order to do this, a task must be scheduled by setting:</span>
 
   - runtime &gt;= WCET
   - deadline = D
   - period &lt;= P
 
<span class="p_del">- IOW, if runtime &gt;= WCET and if period is &gt;= P, then the scheduling deadlines</span>
<span class="p_add">+ IOW, if runtime &gt;= WCET and if period is &lt;= P, then the scheduling deadlines</span>
  and the absolute deadlines (d_j) coincide, so a proper admission control
  allows to respect the jobs&#39; absolute deadlines for this task (this is what is
  called &quot;hard schedulability property&quot; and is an extension of Lemma 1 of [2]).
<span class="p_chunk">@@ -206,6 +297,39 @@</span> <span class="p_context"> CONTENTS</span>
       Symposium, 1998. http://retis.sssup.it/~giorgio/paps/1998/rtss98-cbs.pdf
   3 - L. Abeni. Server Mechanisms for Multimedia Applications. ReTiS Lab
       Technical Report. http://disi.unitn.it/~abeni/tr-98-01.pdf
<span class="p_add">+  4 - J. Y. Leung and M.L. Merril. A Note on Preemptive Scheduling of</span>
<span class="p_add">+      Periodic, Real-Time Tasks. Information Processing Letters, vol. 11,</span>
<span class="p_add">+      no. 3, pp. 115-118, 1980.</span>
<span class="p_add">+  5 - S. K. Baruah, A. K. Mok and L. E. Rosier. Preemptively Scheduling</span>
<span class="p_add">+      Hard-Real-Time Sporadic Tasks on One Processor. Proceedings of the</span>
<span class="p_add">+      11th IEEE Real-time Systems Symposium, 1990.</span>
<span class="p_add">+  6 - S. K. Baruah, L. E. Rosier and R. R. Howell. Algorithms and Complexity</span>
<span class="p_add">+      Concerning the Preemptive Scheduling of Periodic Real-Time tasks on</span>
<span class="p_add">+      One Processor. Real-Time Systems Journal, vol. 4, no. 2, pp 301-324,</span>
<span class="p_add">+      1990.</span>
<span class="p_add">+  7 - S. J. Dhall and C. L. Liu. On a real-time scheduling problem. Operations</span>
<span class="p_add">+      research, vol. 26, no. 1, pp 127-140, 1978.</span>
<span class="p_add">+  8 - T. Baker. Multiprocessor EDF and Deadline Monotonic Schedulability</span>
<span class="p_add">+      Analysis. Proceedings of the 24th IEEE Real-Time Systems Symposium, 2003.</span>
<span class="p_add">+  9 - T. Baker. An Analysis of EDF Schedulability on a Multiprocessor.</span>
<span class="p_add">+      IEEE Transactions on Parallel and Distributed Systems, vol. 16, no. 8,</span>
<span class="p_add">+      pp 760-768, 2005.</span>
<span class="p_add">+  10 - J. Goossens, S. Funk and S. Baruah, Priority-Driven Scheduling of</span>
<span class="p_add">+       Periodic Task Systems on Multiprocessors. Real-Time Systems Journal,</span>
<span class="p_add">+       vol. 25, no. 2–3, pp. 187–205, 2003.</span>
<span class="p_add">+  11 - R. Davis and A. Burns. A Survey of Hard Real-Time Scheduling for</span>
<span class="p_add">+       Multiprocessor Systems. ACM Computing Surveys, vol. 43, no. 4, 2011.</span>
<span class="p_add">+       http://www-users.cs.york.ac.uk/~robdavis/papers/MPSurveyv5.0.pdf</span>
<span class="p_add">+  12 - U. C. Devi and J. H. Anderson. Tardiness Bounds under Global EDF</span>
<span class="p_add">+       Scheduling on a Multiprocessor. Real-Time Systems Journal, vol. 32,</span>
<span class="p_add">+       no. 2, pp 133-189, 2008.</span>
<span class="p_add">+  13 - P. Valente and G. Lipari. An Upper Bound to the Lateness of Soft</span>
<span class="p_add">+       Real-Time Tasks Scheduled by EDF on Multiprocessors. Proceedings of</span>
<span class="p_add">+       the 26th IEEE Real-Time Systems Symposium, 2005.</span>
<span class="p_add">+  14 - J. Erickson, U. Devi and S. Baruah. Improved tardiness bounds for</span>
<span class="p_add">+       Global EDF. Proceedings of the 22nd Euromicro Conference on</span>
<span class="p_add">+       Real-Time Systems, 2010.</span>
<span class="p_add">+</span>
 
 4. Bandwidth management
 =======================
<span class="p_chunk">@@ -218,10 +342,10 @@</span> <span class="p_context"> CONTENTS</span>
  no guarantee can be given on the actual scheduling of the -deadline tasks.
 
  As already stated in Section 3, a necessary condition to be respected to
<span class="p_del">- correctly schedule a set of real-time tasks is that the total utilisation</span>
<span class="p_add">+ correctly schedule a set of real-time tasks is that the total utilization</span>
  is smaller than M. When talking about -deadline tasks, this requires that
  the sum of the ratio between runtime and period for all tasks is smaller
<span class="p_del">- than M. Notice that the ratio runtime/period is equivalent to the utilisation</span>
<span class="p_add">+ than M. Notice that the ratio runtime/period is equivalent to the utilization</span>
  of a &quot;traditional&quot; real-time task, and is also often referred to as
  &quot;bandwidth&quot;.
  The interface used to control the CPU bandwidth that can be allocated
<span class="p_chunk">@@ -251,7 +375,7 @@</span> <span class="p_context"> CONTENTS</span>
  The system wide settings are configured under the /proc virtual file system.
 
  For now the -rt knobs are used for -deadline admission control and the
<span class="p_del">- -deadline runtime is accounted against the -rt runtime. We realise that this</span>
<span class="p_add">+ -deadline runtime is accounted against the -rt runtime. We realize that this</span>
  isn&#39;t entirely desirable; however, it is better to have a small interface for
  now, and be able to change it easily later. The ideal situation (see 5.) is to
  run -rt tasks from a -deadline server; in which case the -rt bandwidth is a
<span class="p_header">diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c</span>
<span class="p_header">index 9d0ac091a52a..4a905bd667e2 100644</span>
<span class="p_header">--- a/arch/alpha/mm/fault.c</span>
<span class="p_header">+++ b/arch/alpha/mm/fault.c</span>
<span class="p_chunk">@@ -23,8 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/smp.h&gt;
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_del">-</span>
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 extern void die_if_kernel(char *,struct pt_regs *,long, unsigned long *);
 
<span class="p_chunk">@@ -107,7 +106,7 @@</span> <span class="p_context"> do_page_fault(unsigned long address, unsigned long mmcsr,</span>
 
 	/* If we&#39;re in an interrupt context, or have no user context,
 	   we must not take the fault.  */
<span class="p_del">-	if (!mm || in_atomic())</span>
<span class="p_add">+	if (!mm || faulthandler_disabled())</span>
 		goto no_context;
 
 #ifdef CONFIG_ALPHA_LARGE_VMALLOC
<span class="p_header">diff --git a/arch/arc/include/asm/futex.h b/arch/arc/include/asm/futex.h</span>
<span class="p_header">index 4dc64ddebece..05b5aaf5b0f9 100644</span>
<span class="p_header">--- a/arch/arc/include/asm/futex.h</span>
<span class="p_header">+++ b/arch/arc/include/asm/futex.h</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> static inline int futex_atomic_op_inuser(int encoded_op, u32 __user *uaddr)</span>
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(int)))
 		return -EFAULT;
 
<span class="p_del">-	pagefault_disable();	/* implies preempt_disable() */</span>
<span class="p_add">+	pagefault_disable();</span>
 
 	switch (op) {
 	case FUTEX_OP_SET:
<span class="p_chunk">@@ -75,7 +75,7 @@</span> <span class="p_context"> static inline int futex_atomic_op_inuser(int encoded_op, u32 __user *uaddr)</span>
 		ret = -ENOSYS;
 	}
 
<span class="p_del">-	pagefault_enable();	/* subsumes preempt_enable() */</span>
<span class="p_add">+	pagefault_enable();</span>
 
 	if (!ret) {
 		switch (cmp) {
<span class="p_chunk">@@ -104,7 +104,7 @@</span> <span class="p_context"> static inline int futex_atomic_op_inuser(int encoded_op, u32 __user *uaddr)</span>
 	return ret;
 }
 
<span class="p_del">-/* Compare-xchg with preemption disabled.</span>
<span class="p_add">+/* Compare-xchg with pagefaults disabled.</span>
  *  Notes:
  *      -Best-Effort: Exchg happens only if compare succeeds.
  *          If compare fails, returns; leaving retry/looping to upper layers
<span class="p_chunk">@@ -121,7 +121,7 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr, u32 oldval,</span>
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(int)))
 		return -EFAULT;
 
<span class="p_del">-	pagefault_disable();	/* implies preempt_disable() */</span>
<span class="p_add">+	pagefault_disable();</span>
 
 	/* TBD : can use llock/scond */
 	__asm__ __volatile__(
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr, u32 oldval,</span>
 	: &quot;r&quot;(oldval), &quot;r&quot;(newval), &quot;r&quot;(uaddr), &quot;ir&quot;(-EFAULT)
 	: &quot;cc&quot;, &quot;memory&quot;);
 
<span class="p_del">-	pagefault_enable();	/* subsumes preempt_enable() */</span>
<span class="p_add">+	pagefault_enable();</span>
 
 	*uval = val;
 	return val;
<span class="p_header">diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c</span>
<span class="p_header">index 6a2e006cbcce..d948e4e9d89c 100644</span>
<span class="p_header">--- a/arch/arc/mm/fault.c</span>
<span class="p_header">+++ b/arch/arc/mm/fault.c</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> void do_page_fault(unsigned long address, struct pt_regs *regs)</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/arm/include/asm/futex.h b/arch/arm/include/asm/futex.h</span>
<span class="p_header">index 4e78065a16aa..5eed82809d82 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/futex.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/futex.h</span>
<span class="p_chunk">@@ -93,6 +93,7 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,</span>
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
 		return -EFAULT;
 
<span class="p_add">+	preempt_disable();</span>
 	__asm__ __volatile__(&quot;@futex_atomic_cmpxchg_inatomic\n&quot;
 	&quot;1:	&quot; TUSER(ldr) &quot;	%1, [%4]\n&quot;
 	&quot;	teq	%1, %2\n&quot;
<span class="p_chunk">@@ -104,6 +105,8 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,</span>
 	: &quot;cc&quot;, &quot;memory&quot;);
 
 	*uval = val;
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -124,7 +127,10 @@</span> <span class="p_context"> futex_atomic_op_inuser (int encoded_op, u32 __user *uaddr)</span>
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
 		return -EFAULT;
 
<span class="p_del">-	pagefault_disable();	/* implies preempt_disable() */</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	pagefault_disable();</span>
 
 	switch (op) {
 	case FUTEX_OP_SET:
<span class="p_chunk">@@ -146,7 +152,10 @@</span> <span class="p_context"> futex_atomic_op_inuser (int encoded_op, u32 __user *uaddr)</span>
 		ret = -ENOSYS;
 	}
 
<span class="p_del">-	pagefault_enable();	/* subsumes preempt_enable() */</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+#endif</span>
 
 	if (!ret) {
 		switch (cmp) {
<span class="p_header">diff --git a/arch/arm/include/asm/topology.h b/arch/arm/include/asm/topology.h</span>
<span class="p_header">index 2fe85fff5cca..370f7a732900 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/topology.h</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> extern struct cputopo_arm cpu_topology[NR_CPUS];</span>
 #define topology_physical_package_id(cpu)	(cpu_topology[cpu].socket_id)
 #define topology_core_id(cpu)		(cpu_topology[cpu].core_id)
 #define topology_core_cpumask(cpu)	(&amp;cpu_topology[cpu].core_sibling)
<span class="p_del">-#define topology_thread_cpumask(cpu)	(&amp;cpu_topology[cpu].thread_sibling)</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)	(&amp;cpu_topology[cpu].thread_sibling)</span>
 
 void init_cpu_topology(void);
 void store_cpu_topology(unsigned int cpuid);
<span class="p_header">diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c</span>
<span class="p_header">index 6333d9c17875..0d629b8f973f 100644</span>
<span class="p_header">--- a/arch/arm/mm/fault.c</span>
<span class="p_header">+++ b/arch/arm/mm/fault.c</span>
<span class="p_chunk">@@ -276,7 +276,7 @@</span> <span class="p_context"> do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/arm/mm/highmem.c b/arch/arm/mm/highmem.c</span>
<span class="p_header">index b98895d9fe57..ee8dfa793989 100644</span>
<span class="p_header">--- a/arch/arm/mm/highmem.c</span>
<span class="p_header">+++ b/arch/arm/mm/highmem.c</span>
<span class="p_chunk">@@ -59,6 +59,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	void *kmap;
 	int type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -121,6 +122,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 		kunmap_high(pte_page(pkmap_page_table[PKMAP_NR(vaddr)]));
 	}
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_chunk">@@ -130,6 +132,7 @@</span> <span class="p_context"> void *kmap_atomic_pfn(unsigned long pfn)</span>
 	int idx, type;
 	struct page *page = pfn_to_page(pfn);
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_header">diff --git a/arch/arm64/include/asm/futex.h b/arch/arm64/include/asm/futex.h</span>
<span class="p_header">index 5f750dc96e0f..74069b3bd919 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/futex.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/futex.h</span>
<span class="p_chunk">@@ -58,7 +58,7 @@</span> <span class="p_context"> futex_atomic_op_inuser (int encoded_op, u32 __user *uaddr)</span>
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
 		return -EFAULT;
 
<span class="p_del">-	pagefault_disable();	/* implies preempt_disable() */</span>
<span class="p_add">+	pagefault_disable();</span>
 
 	switch (op) {
 	case FUTEX_OP_SET:
<span class="p_chunk">@@ -85,7 +85,7 @@</span> <span class="p_context"> futex_atomic_op_inuser (int encoded_op, u32 __user *uaddr)</span>
 		ret = -ENOSYS;
 	}
 
<span class="p_del">-	pagefault_enable();	/* subsumes preempt_enable() */</span>
<span class="p_add">+	pagefault_enable();</span>
 
 	if (!ret) {
 		switch (cmp) {
<span class="p_header">diff --git a/arch/arm64/include/asm/topology.h b/arch/arm64/include/asm/topology.h</span>
<span class="p_header">index 7ebcd31ce51c..225ec3524fbf 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/topology.h</span>
<span class="p_chunk">@@ -18,7 +18,7 @@</span> <span class="p_context"> extern struct cpu_topology cpu_topology[NR_CPUS];</span>
 #define topology_physical_package_id(cpu)	(cpu_topology[cpu].cluster_id)
 #define topology_core_id(cpu)		(cpu_topology[cpu].core_id)
 #define topology_core_cpumask(cpu)	(&amp;cpu_topology[cpu].core_sibling)
<span class="p_del">-#define topology_thread_cpumask(cpu)	(&amp;cpu_topology[cpu].thread_sibling)</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)	(&amp;cpu_topology[cpu].thread_sibling)</span>
 
 void init_cpu_topology(void);
 void store_cpu_topology(unsigned int cpuid);
<span class="p_header">diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c</span>
<span class="p_header">index 96da13167d4a..0948d327d013 100644</span>
<span class="p_header">--- a/arch/arm64/mm/fault.c</span>
<span class="p_header">+++ b/arch/arm64/mm/fault.c</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,</span>
 	 * If we&#39;re in an interrupt or have no user context, we must not take
 	 * the fault.
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/avr32/include/asm/uaccess.h b/arch/avr32/include/asm/uaccess.h</span>
<span class="p_header">index a46f7cf3e1ea..68cf638faf48 100644</span>
<span class="p_header">--- a/arch/avr32/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/avr32/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -97,7 +97,8 @@</span> <span class="p_context"> static inline __kernel_size_t __copy_from_user(void *to,</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -116,7 +117,8 @@</span> <span class="p_context"> static inline __kernel_size_t __copy_from_user(void *to,</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -136,7 +138,8 @@</span> <span class="p_context"> static inline __kernel_size_t __copy_from_user(void *to,</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -158,7 +161,8 @@</span> <span class="p_context"> static inline __kernel_size_t __copy_from_user(void *to,</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_header">diff --git a/arch/avr32/mm/fault.c b/arch/avr32/mm/fault.c</span>
<span class="p_header">index d223a8b57c1e..c03533937a9f 100644</span>
<span class="p_header">--- a/arch/avr32/mm/fault.c</span>
<span class="p_header">+++ b/arch/avr32/mm/fault.c</span>
<span class="p_chunk">@@ -14,11 +14,11 @@</span> <span class="p_context"></span>
 #include &lt;linux/pagemap.h&gt;
 #include &lt;linux/kdebug.h&gt;
 #include &lt;linux/kprobes.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/sysreg.h&gt;
 #include &lt;asm/tlb.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 
 #ifdef CONFIG_KPROBES
 static inline int notify_page_fault(struct pt_regs *regs, int trap)
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(unsigned long ecr, struct pt_regs *regs)</span>
 	 * If we&#39;re in an interrupt or have no user context, we must
 	 * not take the fault...
 	 */
<span class="p_del">-	if (in_atomic() || !mm || regs-&gt;sr &amp; SYSREG_BIT(GM))</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm || regs-&gt;sr &amp; SYSREG_BIT(GM))</span>
 		goto no_context;
 
 	local_irq_enable();
<span class="p_header">diff --git a/arch/cris/mm/fault.c b/arch/cris/mm/fault.c</span>
<span class="p_header">index 83f12f2ed9e3..3066d40a6db1 100644</span>
<span class="p_header">--- a/arch/cris/mm/fault.c</span>
<span class="p_header">+++ b/arch/cris/mm/fault.c</span>
<span class="p_chunk">@@ -8,7 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/wait.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 #include &lt;arch/system.h&gt;
 
 extern int find_fixup_code(struct pt_regs *);
<span class="p_chunk">@@ -109,11 +109,11 @@</span> <span class="p_context"> do_page_fault(unsigned long address, struct pt_regs *regs,</span>
 	info.si_code = SEGV_MAPERR;
 
 	/*
<span class="p_del">-	 * If we&#39;re in an interrupt or &quot;atomic&quot; operation or have no</span>
<span class="p_add">+	 * If we&#39;re in an interrupt, have pagefaults disabled or have no</span>
 	 * user context, we must not take the fault.
 	 */
 
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/frv/mm/fault.c b/arch/frv/mm/fault.c</span>
<span class="p_header">index ec4917ddf678..61d99767fe16 100644</span>
<span class="p_header">--- a/arch/frv/mm/fault.c</span>
<span class="p_header">+++ b/arch/frv/mm/fault.c</span>
<span class="p_chunk">@@ -19,9 +19,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/kernel.h&gt;
 #include &lt;linux/ptrace.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/pgtable.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/gdb-stub.h&gt;
 
 /*****************************************************************************/
<span class="p_chunk">@@ -78,7 +78,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(__frame))
<span class="p_header">diff --git a/arch/frv/mm/highmem.c b/arch/frv/mm/highmem.c</span>
<span class="p_header">index bed9a9bd3c10..785344bbdc07 100644</span>
<span class="p_header">--- a/arch/frv/mm/highmem.c</span>
<span class="p_header">+++ b/arch/frv/mm/highmem.c</span>
<span class="p_chunk">@@ -42,6 +42,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	unsigned long paddr;
 	int type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	type = kmap_atomic_idx_push();
 	paddr = page_to_phys(page);
<span class="p_chunk">@@ -85,5 +86,6 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 	}
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
<span class="p_header">diff --git a/arch/hexagon/include/asm/uaccess.h b/arch/hexagon/include/asm/uaccess.h</span>
<span class="p_header">index e4127e4d6a5b..f000a382bc7f 100644</span>
<span class="p_header">--- a/arch/hexagon/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/hexagon/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -36,7 +36,8 @@</span> <span class="p_context"></span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_header">diff --git a/arch/ia64/include/asm/topology.h b/arch/ia64/include/asm/topology.h</span>
<span class="p_header">index 6437ca21f61b..3ad8f6988363 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/topology.h</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> void build_cpu_to_node_map(void);</span>
 #define topology_physical_package_id(cpu)	(cpu_data(cpu)-&gt;socket_id)
 #define topology_core_id(cpu)			(cpu_data(cpu)-&gt;core_id)
 #define topology_core_cpumask(cpu)		(&amp;cpu_core_map[cpu])
<span class="p_del">-#define topology_thread_cpumask(cpu)		(&amp;per_cpu(cpu_sibling_map, cpu))</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)		(&amp;per_cpu(cpu_sibling_map, cpu))</span>
 #endif
 
 extern void arch_fix_phys_package_id(int num, u32 slot);
<span class="p_header">diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c</span>
<span class="p_header">index ba5ba7accd0d..70b40d1205a6 100644</span>
<span class="p_header">--- a/arch/ia64/mm/fault.c</span>
<span class="p_header">+++ b/arch/ia64/mm/fault.c</span>
<span class="p_chunk">@@ -11,10 +11,10 @@</span> <span class="p_context"></span>
 #include &lt;linux/kprobes.h&gt;
 #include &lt;linux/kdebug.h&gt;
 #include &lt;linux/prefetch.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/processor.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 
 extern int die(char *, struct pt_regs *, long);
 
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re</span>
 	/*
 	 * If we&#39;re in an interrupt or have no user context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 #ifdef CONFIG_VIRTUAL_MEM_MAP
<span class="p_header">diff --git a/arch/m32r/include/asm/uaccess.h b/arch/m32r/include/asm/uaccess.h</span>
<span class="p_header">index 71adff209405..cac7014daef3 100644</span>
<span class="p_header">--- a/arch/m32r/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/m32r/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -91,7 +91,8 @@</span> <span class="p_context"> static inline void set_fs(mm_segment_t s)</span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_chunk">@@ -155,7 +156,8 @@</span> <span class="p_context"> extern int fixup_exception(struct pt_regs *regs);</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -175,7 +177,8 @@</span> <span class="p_context"> extern int fixup_exception(struct pt_regs *regs);</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -194,7 +197,8 @@</span> <span class="p_context"> extern int fixup_exception(struct pt_regs *regs);</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -274,7 +278,8 @@</span> <span class="p_context"> do {									\</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -568,7 +573,8 @@</span> <span class="p_context"> unsigned long __generic_copy_from_user(void *, const void __user *, unsigned lon</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -588,7 +594,8 @@</span> <span class="p_context"> unsigned long __generic_copy_from_user(void *, const void __user *, unsigned lon</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.
  *
<span class="p_chunk">@@ -606,7 +613,8 @@</span> <span class="p_context"> unsigned long __generic_copy_from_user(void *, const void __user *, unsigned lon</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -626,7 +634,8 @@</span> <span class="p_context"> unsigned long __generic_copy_from_user(void *, const void __user *, unsigned lon</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.
  *
<span class="p_chunk">@@ -677,7 +686,8 @@</span> <span class="p_context"> unsigned long clear_user(void __user *mem, unsigned long len);</span>
  * strlen_user: - Get the size of a string in user space.
  * @str: The string to measure.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_header">diff --git a/arch/m32r/mm/fault.c b/arch/m32r/mm/fault.c</span>
<span class="p_header">index e3d4d4890104..8f9875b7933d 100644</span>
<span class="p_header">--- a/arch/m32r/mm/fault.c</span>
<span class="p_header">+++ b/arch/m32r/mm/fault.c</span>
<span class="p_chunk">@@ -24,9 +24,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/vt_kern.h&gt;		/* For unblank_screen() */
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/m32r.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/hardirq.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/tlbflush.h&gt;
<span class="p_chunk">@@ -111,10 +111,10 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	mm = tsk-&gt;mm;
 
 	/*
<span class="p_del">-	 * If we&#39;re in an interrupt or have no user context or are running in an</span>
<span class="p_del">-	 * atomic region then we must not take the fault..</span>
<span class="p_add">+	 * If we&#39;re in an interrupt or have no user context or have pagefaults</span>
<span class="p_add">+	 * disabled then we must not take the fault.</span>
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto bad_area_nosemaphore;
 
 	if (error_code &amp; ACE_USERMODE)
<span class="p_header">diff --git a/arch/m68k/include/asm/irqflags.h b/arch/m68k/include/asm/irqflags.h</span>
<span class="p_header">index a823cd73dc09..b5941818346f 100644</span>
<span class="p_header">--- a/arch/m68k/include/asm/irqflags.h</span>
<span class="p_header">+++ b/arch/m68k/include/asm/irqflags.h</span>
<span class="p_chunk">@@ -2,9 +2,6 @@</span> <span class="p_context"></span>
 #define _M68K_IRQFLAGS_H
 
 #include &lt;linux/types.h&gt;
<span class="p_del">-#ifdef CONFIG_MMU</span>
<span class="p_del">-#include &lt;linux/preempt_mask.h&gt;</span>
<span class="p_del">-#endif</span>
 #include &lt;linux/preempt.h&gt;
 #include &lt;asm/thread_info.h&gt;
 #include &lt;asm/entry.h&gt;
<span class="p_header">diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c</span>
<span class="p_header">index b2f04aee46ec..6a94cdd0c830 100644</span>
<span class="p_header">--- a/arch/m68k/mm/fault.c</span>
<span class="p_header">+++ b/arch/m68k/mm/fault.c</span>
<span class="p_chunk">@@ -10,10 +10,10 @@</span> <span class="p_context"></span>
 #include &lt;linux/ptrace.h&gt;
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/traps.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/pgalloc.h&gt;
 
 extern void die_if_kernel(char *, struct pt_regs *, long);
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/metag/mm/fault.c b/arch/metag/mm/fault.c</span>
<span class="p_header">index 2de5dc695a87..f57edca63609 100644</span>
<span class="p_header">--- a/arch/metag/mm/fault.c</span>
<span class="p_header">+++ b/arch/metag/mm/fault.c</span>
<span class="p_chunk">@@ -105,7 +105,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 
 	mm = tsk-&gt;mm;
 
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/metag/mm/highmem.c b/arch/metag/mm/highmem.c</span>
<span class="p_header">index d71f621a2c0b..807f1b1c4e65 100644</span>
<span class="p_header">--- a/arch/metag/mm/highmem.c</span>
<span class="p_header">+++ b/arch/metag/mm/highmem.c</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	unsigned long vaddr;
 	int type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -82,6 +82,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 	}
 
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_chunk">@@ -95,6 +96,7 @@</span> <span class="p_context"> void *kmap_atomic_pfn(unsigned long pfn)</span>
 	unsigned long vaddr;
 	int type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	type = kmap_atomic_idx_push();
<span class="p_header">diff --git a/arch/microblaze/include/asm/uaccess.h b/arch/microblaze/include/asm/uaccess.h</span>
<span class="p_header">index 62942fd12672..331b0d35f89c 100644</span>
<span class="p_header">--- a/arch/microblaze/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/microblaze/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -178,7 +178,8 @@</span> <span class="p_context"> extern long __user_bad(void);</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -290,7 +291,8 @@</span> <span class="p_context"> extern long __user_bad(void);</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_header">diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c</span>
<span class="p_header">index d46a5ebb7570..177dfc003643 100644</span>
<span class="p_header">--- a/arch/microblaze/mm/fault.c</span>
<span class="p_header">+++ b/arch/microblaze/mm/fault.c</span>
<span class="p_chunk">@@ -107,14 +107,14 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	if ((error_code &amp; 0x13) == 0x13 || (error_code &amp; 0x11) == 0x11)
 		is_write = 0;
 
<span class="p_del">-	if (unlikely(in_atomic() || !mm)) {</span>
<span class="p_add">+	if (unlikely(faulthandler_disabled() || !mm)) {</span>
 		if (kernel_mode(regs))
 			goto bad_area_nosemaphore;
 
<span class="p_del">-		/* in_atomic() in user mode is really bad,</span>
<span class="p_add">+		/* faulthandler_disabled() in user mode is really bad,</span>
 		   as is current-&gt;mm == NULL. */
<span class="p_del">-		pr_emerg(&quot;Page fault in user mode with in_atomic(), mm = %p\n&quot;,</span>
<span class="p_del">-									mm);</span>
<span class="p_add">+		pr_emerg(&quot;Page fault in user mode with faulthandler_disabled(), mm = %p\n&quot;,</span>
<span class="p_add">+			 mm);</span>
 		pr_emerg(&quot;r15 = %lx  MSR = %lx\n&quot;,
 		       regs-&gt;r15, regs-&gt;msr);
 		die(&quot;Weird page fault&quot;, regs, SIGSEGV);
<span class="p_header">diff --git a/arch/microblaze/mm/highmem.c b/arch/microblaze/mm/highmem.c</span>
<span class="p_header">index 5a92576fad92..2fcc5a52d84d 100644</span>
<span class="p_header">--- a/arch/microblaze/mm/highmem.c</span>
<span class="p_header">+++ b/arch/microblaze/mm/highmem.c</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> void *kmap_atomic_prot(struct page *page, pgprot_t prot)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -63,6 +63,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	if (vaddr &lt; __fix_to_virt(FIX_KMAP_END)) {
 		pagefault_enable();
<span class="p_add">+		preempt_enable();</span>
 		return;
 	}
 
<span class="p_chunk">@@ -84,5 +85,6 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 #endif
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
<span class="p_header">diff --git a/arch/mips/include/asm/topology.h b/arch/mips/include/asm/topology.h</span>
<span class="p_header">index 3e307ec2afba..7afda4150a59 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/topology.h</span>
<span class="p_chunk">@@ -15,7 +15,7 @@</span> <span class="p_context"></span>
 #define topology_physical_package_id(cpu)	(cpu_data[cpu].package)
 #define topology_core_id(cpu)			(cpu_data[cpu].core)
 #define topology_core_cpumask(cpu)		(&amp;cpu_core_map[cpu])
<span class="p_del">-#define topology_thread_cpumask(cpu)		(&amp;cpu_sibling_map[cpu])</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)		(&amp;cpu_sibling_map[cpu])</span>
 #endif
 
 #endif /* __ASM_TOPOLOGY_H */
<span class="p_header">diff --git a/arch/mips/include/asm/uaccess.h b/arch/mips/include/asm/uaccess.h</span>
<span class="p_header">index bf8b32450ef6..9722357d2854 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -103,7 +103,8 @@</span> <span class="p_context"> extern u64 __ua_limit;</span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_chunk">@@ -138,7 +139,8 @@</span> <span class="p_context"> extern u64 __ua_limit;</span>
  * @x:	 Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -157,7 +159,8 @@</span> <span class="p_context"> extern u64 __ua_limit;</span>
  * @x:	 Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -177,7 +180,8 @@</span> <span class="p_context"> extern u64 __ua_limit;</span>
  * @x:	 Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -199,7 +203,8 @@</span> <span class="p_context"> extern u64 __ua_limit;</span>
  * @x:	 Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -498,7 +503,8 @@</span> <span class="p_context"> extern void __put_user_unknown(void);</span>
  * @x:	 Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -517,7 +523,8 @@</span> <span class="p_context"> extern void __put_user_unknown(void);</span>
  * @x:	 Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -537,7 +544,8 @@</span> <span class="p_context"> extern void __put_user_unknown(void);</span>
  * @x:	 Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -559,7 +567,8 @@</span> <span class="p_context"> extern void __put_user_unknown(void);</span>
  * @x:	 Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -815,7 +824,8 @@</span> <span class="p_context"> extern size_t __copy_user(void *__to, const void *__from, size_t __n);</span>
  * @from: Source address, in kernel space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -888,7 +898,8 @@</span> <span class="p_context"> extern size_t __copy_user_inatomic(void *__to, const void *__from, size_t __n);</span>
  * @from: Source address, in kernel space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.
  *
<span class="p_chunk">@@ -1075,7 +1086,8 @@</span> <span class="p_context"> extern size_t __copy_in_user_eva(void *__to, const void *__from, size_t __n);</span>
  * @from: Source address, in user space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -1107,7 +1119,8 @@</span> <span class="p_context"> extern size_t __copy_in_user_eva(void *__to, const void *__from, size_t __n);</span>
  * @from: Source address, in user space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.
  *
<span class="p_chunk">@@ -1329,7 +1342,8 @@</span> <span class="p_context"> strncpy_from_user(char *__to, const char __user *__from, long __len)</span>
  * strlen_user: - Get the size of a string in user space.
  * @str: The string to measure.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_chunk">@@ -1398,7 +1412,8 @@</span> <span class="p_context"> static inline long __strnlen_user(const char __user *s, long n)</span>
  * strnlen_user: - Get the size of a string in user space.
  * @str: The string to measure.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_header">diff --git a/arch/mips/kernel/signal-common.h b/arch/mips/kernel/signal-common.h</span>
<span class="p_header">index 06805e09bcd3..0b85f827cd18 100644</span>
<span class="p_header">--- a/arch/mips/kernel/signal-common.h</span>
<span class="p_header">+++ b/arch/mips/kernel/signal-common.h</span>
<span class="p_chunk">@@ -28,12 +28,7 @@</span> <span class="p_context"> extern void __user *get_sigframe(struct ksignal *ksig, struct pt_regs *regs,</span>
 extern int fpcsr_pending(unsigned int __user *fpcsr);
 
 /* Make sure we will not lose FPU ownership */
<span class="p_del">-#ifdef CONFIG_PREEMPT</span>
<span class="p_del">-#define lock_fpu_owner()	preempt_disable()</span>
<span class="p_del">-#define unlock_fpu_owner()	preempt_enable()</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define lock_fpu_owner()	pagefault_disable()</span>
<span class="p_del">-#define unlock_fpu_owner()	pagefault_enable()</span>
<span class="p_del">-#endif</span>
<span class="p_add">+#define lock_fpu_owner()	({ preempt_disable(); pagefault_disable(); })</span>
<span class="p_add">+#define unlock_fpu_owner()	({ pagefault_enable(); preempt_enable(); })</span>
 
 #endif	/* __SIGNAL_COMMON_H */
<span class="p_header">diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c</span>
<span class="p_header">index 7ff8637e530d..36c0f26fac6b 100644</span>
<span class="p_header">--- a/arch/mips/mm/fault.c</span>
<span class="p_header">+++ b/arch/mips/mm/fault.c</span>
<span class="p_chunk">@@ -21,10 +21,10 @@</span> <span class="p_context"></span>
 #include &lt;linux/module.h&gt;
 #include &lt;linux/kprobes.h&gt;
 #include &lt;linux/perf_event.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/branch.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/highmem.h&gt;		/* For VMALLOC_END */
 #include &lt;linux/kdebug.h&gt;
<span class="p_chunk">@@ -94,7 +94,7 @@</span> <span class="p_context"> static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto bad_area_nosemaphore;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/mips/mm/highmem.c b/arch/mips/mm/highmem.c</span>
<span class="p_header">index da815d295239..11661cbc11a8 100644</span>
<span class="p_header">--- a/arch/mips/mm/highmem.c</span>
<span class="p_header">+++ b/arch/mips/mm/highmem.c</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -72,6 +72,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	if (vaddr &lt; FIXADDR_START) { // FIXME
 		pagefault_enable();
<span class="p_add">+		preempt_enable();</span>
 		return;
 	}
 
<span class="p_chunk">@@ -92,6 +93,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 #endif
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_chunk">@@ -104,6 +106,7 @@</span> <span class="p_context"> void *kmap_atomic_pfn(unsigned long pfn)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	type = kmap_atomic_idx_push();
<span class="p_header">diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c</span>
<span class="p_header">index faa5c9822ecc..198a3147dd7d 100644</span>
<span class="p_header">--- a/arch/mips/mm/init.c</span>
<span class="p_header">+++ b/arch/mips/mm/init.c</span>
<span class="p_chunk">@@ -90,6 +90,7 @@</span> <span class="p_context"> static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)</span>
 
 	BUG_ON(Page_dcache_dirty(page));
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	idx = (addr &gt;&gt; PAGE_SHIFT) &amp; (FIX_N_COLOURS - 1);
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
<span class="p_chunk">@@ -152,6 +153,7 @@</span> <span class="p_context"> void kunmap_coherent(void)</span>
 	write_c0_entryhi(old_ctx);
 	local_irq_restore(flags);
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 
 void copy_user_highpage(struct page *to, struct page *from,
<span class="p_header">diff --git a/arch/mn10300/include/asm/highmem.h b/arch/mn10300/include/asm/highmem.h</span>
<span class="p_header">index 2fbbe4d920aa..1ddea5afba09 100644</span>
<span class="p_header">--- a/arch/mn10300/include/asm/highmem.h</span>
<span class="p_header">+++ b/arch/mn10300/include/asm/highmem.h</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"> static inline void *kmap_atomic(struct page *page)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (page &lt; highmem_start_page)
 		return page_address(page);
<span class="p_chunk">@@ -98,6 +99,7 @@</span> <span class="p_context"> static inline void __kunmap_atomic(unsigned long vaddr)</span>
 
 	if (vaddr &lt; FIXADDR_START) { /* FIXME */
 		pagefault_enable();
<span class="p_add">+		preempt_enable();</span>
 		return;
 	}
 
<span class="p_chunk">@@ -122,6 +124,7 @@</span> <span class="p_context"> static inline void __kunmap_atomic(unsigned long vaddr)</span>
 
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 #endif /* __KERNEL__ */
 
<span class="p_header">diff --git a/arch/mn10300/mm/fault.c b/arch/mn10300/mm/fault.c</span>
<span class="p_header">index 0c2cc5d39c8e..4a1d181ed32f 100644</span>
<span class="p_header">--- a/arch/mn10300/mm/fault.c</span>
<span class="p_header">+++ b/arch/mn10300/mm/fault.c</span>
<span class="p_chunk">@@ -23,8 +23,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/init.h&gt;
 #include &lt;linux/vt_kern.h&gt;		/* For unblank_screen() */
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/hardirq.h&gt;
 #include &lt;asm/cpu-regs.h&gt;
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if ((fault_code &amp; MMUFCR_xFC_ACCESS) == MMUFCR_xFC_ACCESS_USR)
<span class="p_header">diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c</span>
<span class="p_header">index 0c9b6afe69e9..b51878b0c6b8 100644</span>
<span class="p_header">--- a/arch/nios2/mm/fault.c</span>
<span class="p_header">+++ b/arch/nios2/mm/fault.c</span>
<span class="p_chunk">@@ -77,7 +77,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto bad_area_nosemaphore;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/parisc/include/asm/cacheflush.h b/arch/parisc/include/asm/cacheflush.h</span>
<span class="p_header">index de65f66ea64e..ec2df4bab302 100644</span>
<span class="p_header">--- a/arch/parisc/include/asm/cacheflush.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -142,6 +142,7 @@</span> <span class="p_context"> static inline void kunmap(struct page *page)</span>
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	return page_address(page);
 }
<span class="p_chunk">@@ -150,6 +151,7 @@</span> <span class="p_context"> static inline void __kunmap_atomic(void *addr)</span>
 {
 	flush_kernel_dcache_page_addr(addr);
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
<span class="p_header">diff --git a/arch/parisc/kernel/traps.c b/arch/parisc/kernel/traps.c</span>
<span class="p_header">index 47ee620d15d2..6548fd1d2e62 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/traps.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/traps.c</span>
<span class="p_chunk">@@ -26,9 +26,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/console.h&gt;
 #include &lt;linux/bug.h&gt;
 #include &lt;linux/ratelimit.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/assembly.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/io.h&gt;
 #include &lt;asm/irq.h&gt;
 #include &lt;asm/traps.h&gt;
<span class="p_chunk">@@ -800,7 +800,7 @@</span> <span class="p_context"> void notrace handle_interruption(int code, struct pt_regs *regs)</span>
 	     * unless pagefault_disable() was called before.
 	     */
 
<span class="p_del">-	    if (fault_space == 0 &amp;&amp; !in_atomic())</span>
<span class="p_add">+	    if (fault_space == 0 &amp;&amp; !faulthandler_disabled())</span>
 	    {
 		pdc_chassis_send_status(PDC_CHASSIS_DIRECT_PANIC);
 		parisc_terminate(&quot;Kernel Fault&quot;, regs, code, fault_address);
<span class="p_header">diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c</span>
<span class="p_header">index e5120e653240..15503adddf4f 100644</span>
<span class="p_header">--- a/arch/parisc/mm/fault.c</span>
<span class="p_header">+++ b/arch/parisc/mm/fault.c</span>
<span class="p_chunk">@@ -15,8 +15,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/traps.h&gt;
 
 /* Various important other fields */
<span class="p_chunk">@@ -207,7 +207,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long code,</span>
 	int fault;
 	unsigned int flags;
 
<span class="p_del">-	if (in_atomic())</span>
<span class="p_add">+	if (pagefault_disabled())</span>
 		goto no_context;
 
 	tsk = current;
<span class="p_header">diff --git a/arch/powerpc/include/asm/topology.h b/arch/powerpc/include/asm/topology.h</span>
<span class="p_header">index 5f1048eaa5b6..8b3b46b7b0f2 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/topology.h</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> static inline int prrn_is_enabled(void)</span>
 #include &lt;asm/smp.h&gt;
 
 #define topology_physical_package_id(cpu)	(cpu_to_chip_id(cpu))
<span class="p_del">-#define topology_thread_cpumask(cpu)	(per_cpu(cpu_sibling_map, cpu))</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)	(per_cpu(cpu_sibling_map, cpu))</span>
 #define topology_core_cpumask(cpu)	(per_cpu(cpu_core_map, cpu))
 #define topology_core_id(cpu)		(cpu_to_core_id(cpu))
 #endif
<span class="p_header">diff --git a/arch/powerpc/lib/vmx-helper.c b/arch/powerpc/lib/vmx-helper.c</span>
<span class="p_header">index 3cf529ceec5b..ac93a3bd2730 100644</span>
<span class="p_header">--- a/arch/powerpc/lib/vmx-helper.c</span>
<span class="p_header">+++ b/arch/powerpc/lib/vmx-helper.c</span>
<span class="p_chunk">@@ -27,11 +27,11 @@</span> <span class="p_context"> int enter_vmx_usercopy(void)</span>
 	if (in_interrupt())
 		return 0;
 
<span class="p_del">-	/* This acts as preempt_disable() as well and will make</span>
<span class="p_del">-	 * enable_kernel_altivec(). We need to disable page faults</span>
<span class="p_del">-	 * as they can call schedule and thus make us lose the VMX</span>
<span class="p_del">-	 * context. So on page faults, we just fail which will cause</span>
<span class="p_del">-	 * a fallback to the normal non-vmx copy.</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We need to disable page faults as they can call schedule and</span>
<span class="p_add">+	 * thus make us lose the VMX context. So on page faults, we just</span>
<span class="p_add">+	 * fail which will cause a fallback to the normal non-vmx copy.</span>
 	 */
 	pagefault_disable();
 
<span class="p_chunk">@@ -47,6 +47,7 @@</span> <span class="p_context"> int enter_vmx_usercopy(void)</span>
 int exit_vmx_usercopy(void)
 {
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index b396868d2aa7..6d535973b200 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -33,13 +33,13 @@</span> <span class="p_context"></span>
 #include &lt;linux/ratelimit.h&gt;
 #include &lt;linux/context_tracking.h&gt;
 #include &lt;linux/hugetlb.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/firmware.h&gt;
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/mmu.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/siginfo.h&gt;
 #include &lt;asm/debug.h&gt;
<span class="p_chunk">@@ -272,15 +272,16 @@</span> <span class="p_context"> int __kprobes do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	if (!arch_irq_disabled_regs(regs))
 		local_irq_enable();
 
<span class="p_del">-	if (in_atomic() || mm == NULL) {</span>
<span class="p_add">+	if (faulthandler_disabled() || mm == NULL) {</span>
 		if (!user_mode(regs)) {
 			rc = SIGSEGV;
 			goto bail;
 		}
<span class="p_del">-		/* in_atomic() in user mode is really bad,</span>
<span class="p_add">+		/* faulthandler_disabled() in user mode is really bad,</span>
 		   as is current-&gt;mm == NULL. */
 		printk(KERN_EMERG &quot;Page fault in user mode with &quot;
<span class="p_del">-		       &quot;in_atomic() = %d mm = %p\n&quot;, in_atomic(), mm);</span>
<span class="p_add">+		       &quot;faulthandler_disabled() = %d mm = %p\n&quot;,</span>
<span class="p_add">+		       faulthandler_disabled(), mm);</span>
 		printk(KERN_EMERG &quot;NIP = %lx  MSR = %lx\n&quot;,
 		       regs-&gt;nip, regs-&gt;msr);
 		die(&quot;Weird page fault&quot;, regs, SIGSEGV);
<span class="p_header">diff --git a/arch/powerpc/mm/highmem.c b/arch/powerpc/mm/highmem.c</span>
<span class="p_header">index e7450bdbe83a..e292c8a60952 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/highmem.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/highmem.c</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"> void *kmap_atomic_prot(struct page *page, pgprot_t prot)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -59,6 +59,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	if (vaddr &lt; __fix_to_virt(FIX_KMAP_END)) {
 		pagefault_enable();
<span class="p_add">+		preempt_enable();</span>
 		return;
 	}
 
<span class="p_chunk">@@ -82,5 +83,6 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
<span class="p_header">diff --git a/arch/powerpc/mm/tlb_nohash.c b/arch/powerpc/mm/tlb_nohash.c</span>
<span class="p_header">index cbd3d069897f..723a099f6be3 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/tlb_nohash.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/tlb_nohash.c</span>
<span class="p_chunk">@@ -217,7 +217,7 @@</span> <span class="p_context"> static DEFINE_RAW_SPINLOCK(tlbivax_lock);</span>
 static int mm_is_core_local(struct mm_struct *mm)
 {
 	return cpumask_subset(mm_cpumask(mm),
<span class="p_del">-			      topology_thread_cpumask(smp_processor_id()));</span>
<span class="p_add">+			      topology_sibling_cpumask(smp_processor_id()));</span>
 }
 
 struct tlb_flush_param {
<span class="p_header">diff --git a/arch/s390/include/asm/topology.h b/arch/s390/include/asm/topology.h</span>
<span class="p_header">index b1453a2ae1ca..4990f6c66288 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/topology.h</span>
<span class="p_chunk">@@ -22,7 +22,8 @@</span> <span class="p_context"> DECLARE_PER_CPU(struct cpu_topology_s390, cpu_topology);</span>
 
 #define topology_physical_package_id(cpu) (per_cpu(cpu_topology, cpu).socket_id)
 #define topology_thread_id(cpu)		  (per_cpu(cpu_topology, cpu).thread_id)
<span class="p_del">-#define topology_thread_cpumask(cpu)	  (&amp;per_cpu(cpu_topology, cpu).thread_mask)</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu) \</span>
<span class="p_add">+		(&amp;per_cpu(cpu_topology, cpu).thread_mask)</span>
 #define topology_core_id(cpu)		  (per_cpu(cpu_topology, cpu).core_id)
 #define topology_core_cpumask(cpu)	  (&amp;per_cpu(cpu_topology, cpu).core_mask)
 #define topology_book_id(cpu)		  (per_cpu(cpu_topology, cpu).book_id)
<span class="p_header">diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h</span>
<span class="p_header">index d64a7a62164f..9dd4cc47ddc7 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -98,7 +98,8 @@</span> <span class="p_context"> static inline unsigned long extable_fixup(const struct exception_table_entry *x)</span>
  * @from: Source address, in user space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -118,7 +119,8 @@</span> <span class="p_context"> unsigned long __must_check __copy_from_user(void *to, const void __user *from,</span>
  * @from: Source address, in kernel space.
  * @n:	  Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.	This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -264,7 +266,8 @@</span> <span class="p_context"> int __get_user_bad(void) __attribute__((noreturn));</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.
  *
<span class="p_chunk">@@ -290,7 +293,8 @@</span> <span class="p_context"> __compiletime_warning(&quot;copy_from_user() buffer size is not provably correct&quot;)</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.
  *
<span class="p_chunk">@@ -348,7 +352,8 @@</span> <span class="p_context"> static inline unsigned long strnlen_user(const char __user *src, unsigned long n</span>
  * strlen_user: - Get the size of a string in user space.
  * @str: The string to measure.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_header">diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c</span>
<span class="p_header">index 76515bcea2f1..4c8f5d7f9c23 100644</span>
<span class="p_header">--- a/arch/s390/mm/fault.c</span>
<span class="p_header">+++ b/arch/s390/mm/fault.c</span>
<span class="p_chunk">@@ -399,7 +399,7 @@</span> <span class="p_context"> static inline int do_exception(struct pt_regs *regs, int access)</span>
 	 * user context.
 	 */
 	fault = VM_FAULT_BADCONTEXT;
<span class="p_del">-	if (unlikely(!user_space_fault(regs) || in_atomic() || !mm))</span>
<span class="p_add">+	if (unlikely(!user_space_fault(regs) || faulthandler_disabled() || !mm))</span>
 		goto out;
 
 	address = trans_exc_code &amp; __FAIL_ADDR_MASK;
<span class="p_header">diff --git a/arch/score/include/asm/uaccess.h b/arch/score/include/asm/uaccess.h</span>
<span class="p_header">index ab66ddde777b..20a3591225cc 100644</span>
<span class="p_header">--- a/arch/score/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/score/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -36,7 +36,8 @@</span> <span class="p_context"></span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_chunk">@@ -61,7 +62,8 @@</span> <span class="p_context"></span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -79,7 +81,8 @@</span> <span class="p_context"></span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -98,7 +101,8 @@</span> <span class="p_context"></span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -119,7 +123,8 @@</span> <span class="p_context"></span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_header">diff --git a/arch/score/mm/fault.c b/arch/score/mm/fault.c</span>
<span class="p_header">index 6860beb2a280..37a6c2e0e969 100644</span>
<span class="p_header">--- a/arch/score/mm/fault.c</span>
<span class="p_header">+++ b/arch/score/mm/fault.c</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/string.h&gt;
 #include &lt;linux/types.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 /*
  * This routine handles page faults.  It determines the address,
<span class="p_chunk">@@ -73,7 +74,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	* If we&#39;re in an interrupt or have no user
 	* context, we must not take the fault..
 	*/
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (pagefault_disabled() || !mm)</span>
 		goto bad_area_nosemaphore;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c</span>
<span class="p_header">index a58fec9b55e0..79d8276377d1 100644</span>
<span class="p_header">--- a/arch/sh/mm/fault.c</span>
<span class="p_header">+++ b/arch/sh/mm/fault.c</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kprobes.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/kdebug.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 #include &lt;asm/io_trapped.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/tlbflush.h&gt;
<span class="p_chunk">@@ -438,9 +439,9 @@</span> <span class="p_context"> asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,</span>
 
 	/*
 	 * If we&#39;re in an interrupt, have no user context or are running
<span class="p_del">-	 * in an atomic region then we must not take the fault:</span>
<span class="p_add">+	 * with pagefaults disabled then we must not take the fault:</span>
 	 */
<span class="p_del">-	if (unlikely(in_atomic() || !mm)) {</span>
<span class="p_add">+	if (unlikely(faulthandler_disabled() || !mm)) {</span>
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
 	}
<span class="p_header">diff --git a/arch/sparc/include/asm/topology_64.h b/arch/sparc/include/asm/topology_64.h</span>
<span class="p_header">index d1761df5cca6..01d17046225a 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/topology_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/topology_64.h</span>
<span class="p_chunk">@@ -41,7 +41,7 @@</span> <span class="p_context"> static inline int pcibus_to_node(struct pci_bus *pbus)</span>
 #define topology_physical_package_id(cpu)	(cpu_data(cpu).proc_id)
 #define topology_core_id(cpu)			(cpu_data(cpu).core_id)
 #define topology_core_cpumask(cpu)		(&amp;cpu_core_sib_map[cpu])
<span class="p_del">-#define topology_thread_cpumask(cpu)		(&amp;per_cpu(cpu_sibling_map, cpu))</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)		(&amp;per_cpu(cpu_sibling_map, cpu))</span>
 #endif /* CONFIG_SMP */
 
 extern cpumask_t cpu_core_map[NR_CPUS];
<span class="p_header">diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c</span>
<span class="p_header">index 70d817154fe8..c399e7b3b035 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_32.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_32.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/interrupt.h&gt;
 #include &lt;linux/kdebug.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_chunk">@@ -29,7 +30,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/smp.h&gt;
 #include &lt;asm/traps.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 
 #include &quot;mm_32.h&quot;
 
<span class="p_chunk">@@ -196,7 +196,7 @@</span> <span class="p_context"> asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (pagefault_disabled() || !mm)</span>
 		goto no_context;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
<span class="p_header">diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c</span>
<span class="p_header">index 479823249429..e9268ea1a68d 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_64.c</span>
<span class="p_chunk">@@ -22,12 +22,12 @@</span> <span class="p_context"></span>
 #include &lt;linux/kdebug.h&gt;
 #include &lt;linux/percpu.h&gt;
 #include &lt;linux/context_tracking.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/openprom.h&gt;
 #include &lt;asm/oplib.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/asi.h&gt;
 #include &lt;asm/lsu.h&gt;
 #include &lt;asm/sections.h&gt;
<span class="p_chunk">@@ -330,7 +330,7 @@</span> <span class="p_context"> asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto intr_or_no_mm;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
<span class="p_header">diff --git a/arch/sparc/mm/highmem.c b/arch/sparc/mm/highmem.c</span>
<span class="p_header">index 449f864f0cef..a454ec5ff07a 100644</span>
<span class="p_header">--- a/arch/sparc/mm/highmem.c</span>
<span class="p_header">+++ b/arch/sparc/mm/highmem.c</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	unsigned long vaddr;
 	long idx, type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -91,6 +91,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	if (vaddr &lt; FIXADDR_START) { // FIXME
 		pagefault_enable();
<span class="p_add">+		preempt_enable();</span>
 		return;
 	}
 
<span class="p_chunk">@@ -126,5 +127,6 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 
 	kmap_atomic_idx_pop();
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index 559cb744112c..c5d08b89a96c 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -2738,7 +2738,7 @@</span> <span class="p_context"> void hugetlb_setup(struct pt_regs *regs)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct tsb_config *tp;
 
<span class="p_del">-	if (in_atomic() || !mm) {</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm) {</span>
 		const struct exception_table_entry *entry;
 
 		entry = search_exception_tables(regs-&gt;tpc);
<span class="p_header">diff --git a/arch/tile/include/asm/topology.h b/arch/tile/include/asm/topology.h</span>
<span class="p_header">index 938311844233..76b0d0ebb244 100644</span>
<span class="p_header">--- a/arch/tile/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/tile/include/asm/topology.h</span>
<span class="p_chunk">@@ -55,7 +55,7 @@</span> <span class="p_context"> static inline const struct cpumask *cpumask_of_node(int node)</span>
 #define topology_physical_package_id(cpu)       ((void)(cpu), 0)
 #define topology_core_id(cpu)                   (cpu)
 #define topology_core_cpumask(cpu)              ((void)(cpu), cpu_online_mask)
<span class="p_del">-#define topology_thread_cpumask(cpu)            cpumask_of(cpu)</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)           cpumask_of(cpu)</span>
 #endif
 
 #endif /* _ASM_TILE_TOPOLOGY_H */
<span class="p_header">diff --git a/arch/tile/include/asm/uaccess.h b/arch/tile/include/asm/uaccess.h</span>
<span class="p_header">index f41cb53cf645..a33276bf5ca1 100644</span>
<span class="p_header">--- a/arch/tile/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/tile/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -78,7 +78,8 @@</span> <span class="p_context"> int __range_ok(unsigned long addr, unsigned long size);</span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_chunk">@@ -192,7 +193,8 @@</span> <span class="p_context"> extern int __get_user_bad(void)</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -274,7 +276,8 @@</span> <span class="p_context"> extern int __put_user_bad(void)</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -330,7 +333,8 @@</span> <span class="p_context"> extern int __put_user_bad(void)</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -366,7 +370,8 @@</span> <span class="p_context"> copy_to_user(void __user *to, const void *from, unsigned long n)</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -437,7 +442,8 @@</span> <span class="p_context"> static inline unsigned long __must_check copy_from_user(void *to,</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to user space.  Caller must check
  * the specified blocks with access_ok() before calling this function.
<span class="p_header">diff --git a/arch/tile/mm/fault.c b/arch/tile/mm/fault.c</span>
<span class="p_header">index e83cc999da02..3f4f58d34a92 100644</span>
<span class="p_header">--- a/arch/tile/mm/fault.c</span>
<span class="p_header">+++ b/arch/tile/mm/fault.c</span>
<span class="p_chunk">@@ -354,9 +354,9 @@</span> <span class="p_context"> static int handle_page_fault(struct pt_regs *regs,</span>
 
 	/*
 	 * If we&#39;re in an interrupt, have no user context or are running in an
<span class="p_del">-	 * atomic region then we must not take the fault.</span>
<span class="p_add">+	 * region with pagefaults disabled then we must not take the fault.</span>
 	 */
<span class="p_del">-	if (in_atomic() || !mm) {</span>
<span class="p_add">+	if (pagefault_disabled() || !mm) {</span>
 		vma = NULL;  /* happy compiler */
 		goto bad_area_nosemaphore;
 	}
<span class="p_header">diff --git a/arch/tile/mm/highmem.c b/arch/tile/mm/highmem.c</span>
<span class="p_header">index 6aa2f2625447..fcd545014e79 100644</span>
<span class="p_header">--- a/arch/tile/mm/highmem.c</span>
<span class="p_header">+++ b/arch/tile/mm/highmem.c</span>
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> void *kmap_atomic_prot(struct page *page, pgprot_t prot)</span>
 	int idx, type;
 	pte_t *pte;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	/* Avoid icache flushes by disallowing atomic executable mappings. */
<span class="p_chunk">@@ -259,6 +259,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 	}
 
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_header">diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c</span>
<span class="p_header">index 8e4daf44e980..47ff9b7f3e5d 100644</span>
<span class="p_header">--- a/arch/um/kernel/trap.c</span>
<span class="p_header">+++ b/arch/um/kernel/trap.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/hardirq.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 #include &lt;asm/current.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/tlbflush.h&gt;
<span class="p_chunk">@@ -35,10 +36,10 @@</span> <span class="p_context"> int handle_page_fault(unsigned long address, unsigned long ip,</span>
 	*code_out = SEGV_MAPERR;
 
 	/*
<span class="p_del">-	 * If the fault was during atomic operation, don&#39;t take the fault, just</span>
<span class="p_add">+	 * If the fault was with pagefaults disabled, don&#39;t take the fault, just</span>
 	 * fail.
 	 */
<span class="p_del">-	if (in_atomic())</span>
<span class="p_add">+	if (faulthandler_disabled())</span>
 		goto out_nosemaphore;
 
 	if (is_user)
<span class="p_header">diff --git a/arch/unicore32/mm/fault.c b/arch/unicore32/mm/fault.c</span>
<span class="p_header">index 0dc922dba915..afccef5529cc 100644</span>
<span class="p_header">--- a/arch/unicore32/mm/fault.c</span>
<span class="p_header">+++ b/arch/unicore32/mm/fault.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> static int do_pf(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 	 * If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm)</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm)</span>
 		goto no_context;
 
 	if (user_mode(regs))
<span class="p_header">diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h</span>
<span class="p_header">index 8f3271842533..dca71714f860 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/preempt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/preempt.h</span>
<span class="p_chunk">@@ -99,11 +99,9 @@</span> <span class="p_context"> static __always_inline bool should_resched(void)</span>
   extern asmlinkage void ___preempt_schedule(void);
 # define __preempt_schedule() asm (&quot;call ___preempt_schedule&quot;)
   extern asmlinkage void preempt_schedule(void);
<span class="p_del">-# ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-    extern asmlinkage void ___preempt_schedule_context(void);</span>
<span class="p_del">-#   define __preempt_schedule_context() asm (&quot;call ___preempt_schedule_context&quot;)</span>
<span class="p_del">-    extern asmlinkage void preempt_schedule_context(void);</span>
<span class="p_del">-# endif</span>
<span class="p_add">+  extern asmlinkage void ___preempt_schedule_notrace(void);</span>
<span class="p_add">+# define __preempt_schedule_notrace() asm (&quot;call ___preempt_schedule_notrace&quot;)</span>
<span class="p_add">+  extern asmlinkage void preempt_schedule_notrace(void);</span>
 #endif
 
 #endif /* __ASM_PREEMPT_H */
<span class="p_header">diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h</span>
<span class="p_header">index 17a8dced12da..222a6a3ca2b5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/smp.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/smp.h</span>
<span class="p_chunk">@@ -37,16 +37,6 @@</span> <span class="p_context"> DECLARE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_llc_shared_map);</span>
 DECLARE_PER_CPU_READ_MOSTLY(u16, cpu_llc_id);
 DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
 
<span class="p_del">-static inline struct cpumask *cpu_sibling_mask(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return per_cpu(cpu_sibling_map, cpu);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline struct cpumask *cpu_core_mask(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return per_cpu(cpu_core_map, cpu);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline struct cpumask *cpu_llc_shared_mask(int cpu)
 {
 	return per_cpu(cpu_llc_shared_map, cpu);
<span class="p_header">diff --git a/arch/x86/include/asm/topology.h b/arch/x86/include/asm/topology.h</span>
<span class="p_header">index 0e8f04f2c26f..5a77593fdace 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/topology.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/topology.h</span>
<span class="p_chunk">@@ -124,7 +124,7 @@</span> <span class="p_context"> extern const struct cpumask *cpu_coregroup_mask(int cpu);</span>
 
 #ifdef ENABLE_TOPO_DEFINES
 #define topology_core_cpumask(cpu)		(per_cpu(cpu_core_map, cpu))
<span class="p_del">-#define topology_thread_cpumask(cpu)		(per_cpu(cpu_sibling_map, cpu))</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)		(per_cpu(cpu_sibling_map, cpu))</span>
 #endif
 
 static inline void arch_fix_phys_package_id(int num, u32 slot)
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">index ace9dec050b1..a8df874f3e88 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -74,7 +74,8 @@</span> <span class="p_context"> static inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, un</span>
  * @addr: User space pointer to start of block to check
  * @size: Size of block to check
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Checks if a pointer to a block of memory in user space is valid.
  *
<span class="p_chunk">@@ -145,7 +146,8 @@</span> <span class="p_context"> __typeof__(__builtin_choose_expr(sizeof(x) &gt; sizeof(0UL), 0ULL, 0UL))</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -240,7 +242,8 @@</span> <span class="p_context"> extern void __put_user_8(void);</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -455,7 +458,8 @@</span> <span class="p_context"> struct __large_struct { unsigned long buf[100]; };</span>
  * @x:   Variable to store result.
  * @ptr: Source address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple variable from user space to kernel
  * space.  It supports simple types like char and int, but not larger
<span class="p_chunk">@@ -479,7 +483,8 @@</span> <span class="p_context"> struct __large_struct { unsigned long buf[100]; };</span>
  * @x:   Value to copy to user space.
  * @ptr: Destination address, in user space.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * This macro copies a single simple value from kernel space to user
  * space.  It supports simple types like char and int, but not larger
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess_32.h b/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_header">index 3c03a5de64d3..7c8ad3451988 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess_32.h</span>
<span class="p_chunk">@@ -70,7 +70,8 @@</span> <span class="p_context"> __copy_to_user_inatomic(void __user *to, const void *from, unsigned long n)</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_chunk">@@ -117,7 +118,8 @@</span> <span class="p_context"> __copy_from_user_inatomic(void *to, const void __user *from, unsigned long n)</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event_intel.c b/arch/x86/kernel/cpu/perf_event_intel.c</span>
<span class="p_header">index 3998131d1a68..324817735771 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event_intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event_intel.c</span>
<span class="p_chunk">@@ -2621,7 +2621,7 @@</span> <span class="p_context"> static void intel_pmu_cpu_starting(int cpu)</span>
 	if (!(x86_pmu.flags &amp; PMU_FL_NO_HT_SHARING)) {
 		void **onln = &amp;cpuc-&gt;kfree_on_online[X86_PERF_KFREE_SHARED];
 
<span class="p_del">-		for_each_cpu(i, topology_thread_cpumask(cpu)) {</span>
<span class="p_add">+		for_each_cpu(i, topology_sibling_cpumask(cpu)) {</span>
 			struct intel_shared_regs *pc;
 
 			pc = per_cpu(cpu_hw_events, i).shared_regs;
<span class="p_chunk">@@ -2641,7 +2641,7 @@</span> <span class="p_context"> static void intel_pmu_cpu_starting(int cpu)</span>
 	if (x86_pmu.flags &amp; PMU_FL_EXCL_CNTRS) {
 		int h = x86_pmu.num_counters &gt;&gt; 1;
 
<span class="p_del">-		for_each_cpu(i, topology_thread_cpumask(cpu)) {</span>
<span class="p_add">+		for_each_cpu(i, topology_sibling_cpumask(cpu)) {</span>
 			struct intel_excl_cntrs *c;
 
 			c = per_cpu(cpu_hw_events, i).excl_cntrs;
<span class="p_chunk">@@ -3403,7 +3403,7 @@</span> <span class="p_context"> static __init int fixup_ht_bug(void)</span>
 	if (!(x86_pmu.flags &amp; PMU_FL_EXCL_ENABLED))
 		return 0;
 
<span class="p_del">-	w = cpumask_weight(topology_thread_cpumask(cpu));</span>
<span class="p_add">+	w = cpumask_weight(topology_sibling_cpumask(cpu));</span>
 	if (w &gt; 1) {
 		pr_info(&quot;PMU erratum BJ122, BV98, HSD29 worked around, HT is on\n&quot;);
 		return 0;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">index e7d8c7608471..18ca99f2798b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_chunk">@@ -12,7 +12,8 @@</span> <span class="p_context"> static void show_cpuinfo_core(struct seq_file *m, struct cpuinfo_x86 *c,</span>
 {
 #ifdef CONFIG_SMP
 	seq_printf(m, &quot;physical id\t: %d\n&quot;, c-&gt;phys_proc_id);
<span class="p_del">-	seq_printf(m, &quot;siblings\t: %d\n&quot;, cpumask_weight(cpu_core_mask(cpu)));</span>
<span class="p_add">+	seq_printf(m, &quot;siblings\t: %d\n&quot;,</span>
<span class="p_add">+		   cpumask_weight(topology_core_cpumask(cpu)));</span>
 	seq_printf(m, &quot;core id\t\t: %d\n&quot;, c-&gt;cpu_core_id);
 	seq_printf(m, &quot;cpu cores\t: %d\n&quot;, c-&gt;booted_cores);
 	seq_printf(m, &quot;apicid\t\t: %d\n&quot;, c-&gt;apicid);
<span class="p_header">diff --git a/arch/x86/kernel/i386_ksyms_32.c b/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_header">index 05fd74f537d6..64341aa485ae 100644</span>
<span class="p_header">--- a/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_chunk">@@ -40,7 +40,5 @@</span> <span class="p_context"> EXPORT_SYMBOL(empty_zero_page);</span>
 
 #ifdef CONFIG_PREEMPT
 EXPORT_SYMBOL(___preempt_schedule);
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-EXPORT_SYMBOL(___preempt_schedule_context);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+EXPORT_SYMBOL(___preempt_schedule_notrace);</span>
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index 6e338e3b1dc0..c648139d68d7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -445,11 +445,10 @@</span> <span class="p_context"> static int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)</span>
 }
 
 /*
<span class="p_del">- * MONITOR/MWAIT with no hints, used for default default C1 state.</span>
<span class="p_del">- * This invokes MWAIT with interrutps enabled and no flags,</span>
<span class="p_del">- * which is backwards compatible with the original MWAIT implementation.</span>
<span class="p_add">+ * MONITOR/MWAIT with no hints, used for default C1 state. This invokes MWAIT</span>
<span class="p_add">+ * with interrupts enabled and no flags, which is backwards compatible with the</span>
<span class="p_add">+ * original MWAIT implementation.</span>
  */
<span class="p_del">-</span>
 static void mwait_idle(void)
 {
 	if (!current_set_polling_and_test()) {
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 50e547eac8cd..0e8209619455 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -314,10 +314,10 @@</span> <span class="p_context"> topology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)</span>
 		cpu1, name, cpu2, cpu_to_node(cpu1), cpu_to_node(cpu2));
 }
 
<span class="p_del">-#define link_mask(_m, c1, c2)						\</span>
<span class="p_add">+#define link_mask(mfunc, c1, c2)					\</span>
 do {									\
<span class="p_del">-	cpumask_set_cpu((c1), cpu_##_m##_mask(c2));			\</span>
<span class="p_del">-	cpumask_set_cpu((c2), cpu_##_m##_mask(c1));			\</span>
<span class="p_add">+	cpumask_set_cpu((c1), mfunc(c2));				\</span>
<span class="p_add">+	cpumask_set_cpu((c2), mfunc(c1));				\</span>
 } while (0)
 
 static bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)
<span class="p_chunk">@@ -398,9 +398,9 @@</span> <span class="p_context"> void set_cpu_sibling_map(int cpu)</span>
 	cpumask_set_cpu(cpu, cpu_sibling_setup_mask);
 
 	if (!has_mp) {
<span class="p_del">-		cpumask_set_cpu(cpu, cpu_sibling_mask(cpu));</span>
<span class="p_add">+		cpumask_set_cpu(cpu, topology_sibling_cpumask(cpu));</span>
 		cpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));
<span class="p_del">-		cpumask_set_cpu(cpu, cpu_core_mask(cpu));</span>
<span class="p_add">+		cpumask_set_cpu(cpu, topology_core_cpumask(cpu));</span>
 		c-&gt;booted_cores = 1;
 		return;
 	}
<span class="p_chunk">@@ -409,32 +409,34 @@</span> <span class="p_context"> void set_cpu_sibling_map(int cpu)</span>
 		o = &amp;cpu_data(i);
 
 		if ((i == cpu) || (has_smt &amp;&amp; match_smt(c, o)))
<span class="p_del">-			link_mask(sibling, cpu, i);</span>
<span class="p_add">+			link_mask(topology_sibling_cpumask, cpu, i);</span>
 
 		if ((i == cpu) || (has_mp &amp;&amp; match_llc(c, o)))
<span class="p_del">-			link_mask(llc_shared, cpu, i);</span>
<span class="p_add">+			link_mask(cpu_llc_shared_mask, cpu, i);</span>
 
 	}
 
 	/*
 	 * This needs a separate iteration over the cpus because we rely on all
<span class="p_del">-	 * cpu_sibling_mask links to be set-up.</span>
<span class="p_add">+	 * topology_sibling_cpumask links to be set-up.</span>
 	 */
 	for_each_cpu(i, cpu_sibling_setup_mask) {
 		o = &amp;cpu_data(i);
 
 		if ((i == cpu) || (has_mp &amp;&amp; match_die(c, o))) {
<span class="p_del">-			link_mask(core, cpu, i);</span>
<span class="p_add">+			link_mask(topology_core_cpumask, cpu, i);</span>
 
 			/*
 			 *  Does this new cpu bringup a new core?
 			 */
<span class="p_del">-			if (cpumask_weight(cpu_sibling_mask(cpu)) == 1) {</span>
<span class="p_add">+			if (cpumask_weight(</span>
<span class="p_add">+			    topology_sibling_cpumask(cpu)) == 1) {</span>
 				/*
 				 * for each core in package, increment
 				 * the booted_cores for this new cpu
 				 */
<span class="p_del">-				if (cpumask_first(cpu_sibling_mask(i)) == i)</span>
<span class="p_add">+				if (cpumask_first(</span>
<span class="p_add">+				    topology_sibling_cpumask(i)) == i)</span>
 					c-&gt;booted_cores++;
 				/*
 				 * increment the core count for all
<span class="p_chunk">@@ -1009,8 +1011,8 @@</span> <span class="p_context"> static __init void disable_smp(void)</span>
 		physid_set_mask_of_physid(boot_cpu_physical_apicid, &amp;phys_cpu_present_map);
 	else
 		physid_set_mask_of_physid(0, &amp;phys_cpu_present_map);
<span class="p_del">-	cpumask_set_cpu(0, cpu_sibling_mask(0));</span>
<span class="p_del">-	cpumask_set_cpu(0, cpu_core_mask(0));</span>
<span class="p_add">+	cpumask_set_cpu(0, topology_sibling_cpumask(0));</span>
<span class="p_add">+	cpumask_set_cpu(0, topology_core_cpumask(0));</span>
 }
 
 enum {
<span class="p_chunk">@@ -1293,22 +1295,22 @@</span> <span class="p_context"> static void remove_siblinginfo(int cpu)</span>
 	int sibling;
 	struct cpuinfo_x86 *c = &amp;cpu_data(cpu);
 
<span class="p_del">-	for_each_cpu(sibling, cpu_core_mask(cpu)) {</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, cpu_core_mask(sibling));</span>
<span class="p_add">+	for_each_cpu(sibling, topology_core_cpumask(cpu)) {</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, topology_core_cpumask(sibling));</span>
 		/*/
 		 * last thread sibling in this cpu core going down
 		 */
<span class="p_del">-		if (cpumask_weight(cpu_sibling_mask(cpu)) == 1)</span>
<span class="p_add">+		if (cpumask_weight(topology_sibling_cpumask(cpu)) == 1)</span>
 			cpu_data(sibling).booted_cores--;
 	}
 
<span class="p_del">-	for_each_cpu(sibling, cpu_sibling_mask(cpu))</span>
<span class="p_del">-		cpumask_clear_cpu(cpu, cpu_sibling_mask(sibling));</span>
<span class="p_add">+	for_each_cpu(sibling, topology_sibling_cpumask(cpu))</span>
<span class="p_add">+		cpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));</span>
 	for_each_cpu(sibling, cpu_llc_shared_mask(cpu))
 		cpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));
 	cpumask_clear(cpu_llc_shared_mask(cpu));
<span class="p_del">-	cpumask_clear(cpu_sibling_mask(cpu));</span>
<span class="p_del">-	cpumask_clear(cpu_core_mask(cpu));</span>
<span class="p_add">+	cpumask_clear(topology_sibling_cpumask(cpu));</span>
<span class="p_add">+	cpumask_clear(topology_core_cpumask(cpu));</span>
 	c-&gt;phys_proc_id = 0;
 	c-&gt;cpu_core_id = 0;
 	cpumask_clear_cpu(cpu, cpu_sibling_setup_mask);
<span class="p_header">diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c</span>
<span class="p_header">index 26488487bc61..dd8d0791dfb5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tsc_sync.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tsc_sync.c</span>
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> static void check_tsc_warp(unsigned int timeout)</span>
  */
 static inline unsigned int loop_timeout(int cpu)
 {
<span class="p_del">-	return (cpumask_weight(cpu_core_mask(cpu)) &gt; 1) ? 2 : 20;</span>
<span class="p_add">+	return (cpumask_weight(topology_core_cpumask(cpu)) &gt; 1) ? 2 : 20;</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/kernel/x8664_ksyms_64.c b/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_header">index 37d8fa4438f0..a0695be19864 100644</span>
<span class="p_header">--- a/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_chunk">@@ -75,7 +75,5 @@</span> <span class="p_context"> EXPORT_SYMBOL(native_load_gs_index);</span>
 
 #ifdef CONFIG_PREEMPT
 EXPORT_SYMBOL(___preempt_schedule);
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-EXPORT_SYMBOL(___preempt_schedule_context);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+EXPORT_SYMBOL(___preempt_schedule_notrace);</span>
 #endif
<span class="p_header">diff --git a/arch/x86/lib/thunk_32.S b/arch/x86/lib/thunk_32.S</span>
<span class="p_header">index 5eb715087b80..e407941d0488 100644</span>
<span class="p_header">--- a/arch/x86/lib/thunk_32.S</span>
<span class="p_header">+++ b/arch/x86/lib/thunk_32.S</span>
<span class="p_chunk">@@ -38,8 +38,6 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_PREEMPT
 	THUNK ___preempt_schedule, preempt_schedule
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-	THUNK ___preempt_schedule_context, preempt_schedule_context</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	THUNK ___preempt_schedule_notrace, preempt_schedule_notrace</span>
 #endif
 
<span class="p_header">diff --git a/arch/x86/lib/thunk_64.S b/arch/x86/lib/thunk_64.S</span>
<span class="p_header">index f89ba4e93025..2198902329b5 100644</span>
<span class="p_header">--- a/arch/x86/lib/thunk_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/thunk_64.S</span>
<span class="p_chunk">@@ -49,9 +49,7 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_PREEMPT
 	THUNK ___preempt_schedule, preempt_schedule
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-	THUNK ___preempt_schedule_context, preempt_schedule_context</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	THUNK ___preempt_schedule_notrace, preempt_schedule_notrace</span>
 #endif
 
 #if defined(CONFIG_TRACE_IRQFLAGS) \
<span class="p_header">diff --git a/arch/x86/lib/usercopy_32.c b/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">index e2f5e21c03b3..91d93b95bd86 100644</span>
<span class="p_header">--- a/arch/x86/lib/usercopy_32.c</span>
<span class="p_header">+++ b/arch/x86/lib/usercopy_32.c</span>
<span class="p_chunk">@@ -647,7 +647,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(__copy_from_user_ll_nocache_nozero);</span>
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from kernel space to user space.
  *
<span class="p_chunk">@@ -668,7 +669,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(_copy_to_user);</span>
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Copy data from user space to kernel space.
  *
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 181c53bac3a7..9dc909841739 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/hugetlb.h&gt;		/* hstate_index_to_shift	*/
 #include &lt;linux/prefetch.h&gt;		/* prefetchw			*/
 #include &lt;linux/context_tracking.h&gt;	/* exception_enter(), ...	*/
<span class="p_add">+#include &lt;linux/uaccess.h&gt;		/* faulthandler_disabled()	*/</span>
 
 #include &lt;asm/traps.h&gt;			/* dotraplinkage, ...		*/
 #include &lt;asm/pgalloc.h&gt;		/* pgd_*(), ...			*/
<span class="p_chunk">@@ -1126,9 +1127,9 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 
 	/*
 	 * If we&#39;re in an interrupt, have no user context or are running
<span class="p_del">-	 * in an atomic region then we must not take the fault:</span>
<span class="p_add">+	 * in a region with pagefaults disabled then we must not take the fault</span>
 	 */
<span class="p_del">-	if (unlikely(in_atomic() || !mm)) {</span>
<span class="p_add">+	if (unlikely(faulthandler_disabled() || !mm)) {</span>
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/mm/highmem_32.c b/arch/x86/mm/highmem_32.c</span>
<span class="p_header">index 4500142bc4aa..eecb207a2037 100644</span>
<span class="p_header">--- a/arch/x86/mm/highmem_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/highmem_32.c</span>
<span class="p_chunk">@@ -35,7 +35,7 @@</span> <span class="p_context"> void *kmap_atomic_prot(struct page *page, pgprot_t prot)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_del">-	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */</span>
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	if (!PageHighMem(page))
<span class="p_chunk">@@ -100,6 +100,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 #endif
 
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_header">diff --git a/arch/x86/mm/iomap_32.c b/arch/x86/mm/iomap_32.c</span>
<span class="p_header">index 9ca35fc60cfe..2b7ece0e103a 100644</span>
<span class="p_header">--- a/arch/x86/mm/iomap_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/iomap_32.c</span>
<span class="p_chunk">@@ -59,6 +59,7 @@</span> <span class="p_context"> void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)</span>
 	unsigned long vaddr;
 	int idx, type;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	type = kmap_atomic_idx_push();
<span class="p_chunk">@@ -117,5 +118,6 @@</span> <span class="p_context"> iounmap_atomic(void __iomem *kvaddr)</span>
 	}
 
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL_GPL(iounmap_atomic);
<span class="p_header">diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c</span>
<span class="p_header">index 9e3571a6535c..83a44a33cfa1 100644</span>
<span class="p_header">--- a/arch/xtensa/mm/fault.c</span>
<span class="p_header">+++ b/arch/xtensa/mm/fault.c</span>
<span class="p_chunk">@@ -15,10 +15,10 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/hardirq.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 #include &lt;asm/pgalloc.h&gt;
 
 DEFINE_PER_CPU(unsigned long, asid_cache) = ASID_USER_FIRST;
<span class="p_chunk">@@ -57,7 +57,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs)</span>
 	/* If we&#39;re in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
<span class="p_del">-	if (in_atomic() || !mm) {</span>
<span class="p_add">+	if (faulthandler_disabled() || !mm) {</span>
 		bad_page_fault(regs, address, SIGSEGV);
 		return;
 	}
<span class="p_header">diff --git a/arch/xtensa/mm/highmem.c b/arch/xtensa/mm/highmem.c</span>
<span class="p_header">index 8cfb71ec0937..184ceadccc1a 100644</span>
<span class="p_header">--- a/arch/xtensa/mm/highmem.c</span>
<span class="p_header">+++ b/arch/xtensa/mm/highmem.c</span>
<span class="p_chunk">@@ -42,6 +42,7 @@</span> <span class="p_context"> void *kmap_atomic(struct page *page)</span>
 	enum fixed_addresses idx;
 	unsigned long vaddr;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	if (!PageHighMem(page))
 		return page_address(page);
<span class="p_chunk">@@ -79,6 +80,7 @@</span> <span class="p_context"> void __kunmap_atomic(void *kvaddr)</span>
 	}
 
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 EXPORT_SYMBOL(__kunmap_atomic);
 
<span class="p_header">diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c</span>
<span class="p_header">index 5f13f4d0bcce..1e28ddb656b8 100644</span>
<span class="p_header">--- a/block/blk-mq-cpumap.c</span>
<span class="p_header">+++ b/block/blk-mq-cpumap.c</span>
<span class="p_chunk">@@ -24,7 +24,7 @@</span> <span class="p_context"> static int get_first_sibling(unsigned int cpu)</span>
 {
 	unsigned int ret;
 
<span class="p_del">-	ret = cpumask_first(topology_thread_cpumask(cpu));</span>
<span class="p_add">+	ret = cpumask_first(topology_sibling_cpumask(cpu));</span>
 	if (ret &lt; nr_cpu_ids)
 		return ret;
 
<span class="p_header">diff --git a/drivers/acpi/acpi_pad.c b/drivers/acpi/acpi_pad.c</span>
<span class="p_header">index 6bc9cbc01ad6..00b39802d7ec 100644</span>
<span class="p_header">--- a/drivers/acpi/acpi_pad.c</span>
<span class="p_header">+++ b/drivers/acpi/acpi_pad.c</span>
<span class="p_chunk">@@ -105,7 +105,7 @@</span> <span class="p_context"> static void round_robin_cpu(unsigned int tsk_index)</span>
 	mutex_lock(&amp;round_robin_lock);
 	cpumask_clear(tmp);
 	for_each_cpu(cpu, pad_busy_cpus)
<span class="p_del">-		cpumask_or(tmp, tmp, topology_thread_cpumask(cpu));</span>
<span class="p_add">+		cpumask_or(tmp, tmp, topology_sibling_cpumask(cpu));</span>
 	cpumask_andnot(tmp, cpu_online_mask, tmp);
 	/* avoid HT sibilings if possible */
 	if (cpumask_empty(tmp))
<span class="p_header">diff --git a/drivers/base/topology.c b/drivers/base/topology.c</span>
<span class="p_header">index 6491f45200a7..8b7d7f8e5851 100644</span>
<span class="p_header">--- a/drivers/base/topology.c</span>
<span class="p_header">+++ b/drivers/base/topology.c</span>
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> static DEVICE_ATTR_RO(physical_package_id);</span>
 define_id_show_func(core_id);
 static DEVICE_ATTR_RO(core_id);
 
<span class="p_del">-define_siblings_show_func(thread_siblings, thread_cpumask);</span>
<span class="p_add">+define_siblings_show_func(thread_siblings, sibling_cpumask);</span>
 static DEVICE_ATTR_RO(thread_siblings);
 static DEVICE_ATTR_RO(thread_siblings_list);
 
<span class="p_header">diff --git a/drivers/cpufreq/acpi-cpufreq.c b/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_header">index b0c18ed8d83f..0136dfcdabf0 100644</span>
<span class="p_header">--- a/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_header">+++ b/drivers/cpufreq/acpi-cpufreq.c</span>
<span class="p_chunk">@@ -699,13 +699,14 @@</span> <span class="p_context"> static int acpi_cpufreq_cpu_init(struct cpufreq_policy *policy)</span>
 	dmi_check_system(sw_any_bug_dmi_table);
 	if (bios_with_sw_any_bug &amp;&amp; !policy_is_shared(policy)) {
 		policy-&gt;shared_type = CPUFREQ_SHARED_TYPE_ALL;
<span class="p_del">-		cpumask_copy(policy-&gt;cpus, cpu_core_mask(cpu));</span>
<span class="p_add">+		cpumask_copy(policy-&gt;cpus, topology_core_cpumask(cpu));</span>
 	}
 
 	if (check_amd_hwpstate_cpu(cpu) &amp;&amp; !acpi_pstate_strict) {
 		cpumask_clear(policy-&gt;cpus);
 		cpumask_set_cpu(cpu, policy-&gt;cpus);
<span class="p_del">-		cpumask_copy(data-&gt;freqdomain_cpus, cpu_sibling_mask(cpu));</span>
<span class="p_add">+		cpumask_copy(data-&gt;freqdomain_cpus,</span>
<span class="p_add">+			     topology_sibling_cpumask(cpu));</span>
 		policy-&gt;shared_type = CPUFREQ_SHARED_TYPE_HW;
 		pr_info_once(PFX &quot;overriding BIOS provided _PSD data\n&quot;);
 	}
<span class="p_header">diff --git a/drivers/cpufreq/p4-clockmod.c b/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_header">index 529cfd92158f..5dd95dab580d 100644</span>
<span class="p_header">--- a/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_header">+++ b/drivers/cpufreq/p4-clockmod.c</span>
<span class="p_chunk">@@ -172,7 +172,7 @@</span> <span class="p_context"> static int cpufreq_p4_cpu_init(struct cpufreq_policy *policy)</span>
 	unsigned int i;
 
 #ifdef CONFIG_SMP
<span class="p_del">-	cpumask_copy(policy-&gt;cpus, cpu_sibling_mask(policy-&gt;cpu));</span>
<span class="p_add">+	cpumask_copy(policy-&gt;cpus, topology_sibling_cpumask(policy-&gt;cpu));</span>
 #endif
 
 	/* Errata workaround */
<span class="p_header">diff --git a/drivers/cpufreq/powernow-k8.c b/drivers/cpufreq/powernow-k8.c</span>
<span class="p_header">index f9ce7e4bf0fe..5c035d04d827 100644</span>
<span class="p_header">--- a/drivers/cpufreq/powernow-k8.c</span>
<span class="p_header">+++ b/drivers/cpufreq/powernow-k8.c</span>
<span class="p_chunk">@@ -57,13 +57,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU(struct powernow_k8_data *, powernow_data);</span>
 
 static struct cpufreq_driver cpufreq_amd64_driver;
 
<span class="p_del">-#ifndef CONFIG_SMP</span>
<span class="p_del">-static inline const struct cpumask *cpu_core_mask(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return cpumask_of(0);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /* Return a frequency in MHz, given an input fid */
 static u32 find_freq_from_fid(u32 fid)
 {
<span class="p_chunk">@@ -620,7 +613,7 @@</span> <span class="p_context"> static int fill_powernow_table(struct powernow_k8_data *data,</span>
 
 	pr_debug(&quot;cfid 0x%x, cvid 0x%x\n&quot;, data-&gt;currfid, data-&gt;currvid);
 	data-&gt;powernow_table = powernow_table;
<span class="p_del">-	if (cpumask_first(cpu_core_mask(data-&gt;cpu)) == data-&gt;cpu)</span>
<span class="p_add">+	if (cpumask_first(topology_core_cpumask(data-&gt;cpu)) == data-&gt;cpu)</span>
 		print_basics(data);
 
 	for (j = 0; j &lt; data-&gt;numps; j++)
<span class="p_chunk">@@ -784,7 +777,7 @@</span> <span class="p_context"> static int powernow_k8_cpu_init_acpi(struct powernow_k8_data *data)</span>
 		CPUFREQ_TABLE_END;
 	data-&gt;powernow_table = powernow_table;
 
<span class="p_del">-	if (cpumask_first(cpu_core_mask(data-&gt;cpu)) == data-&gt;cpu)</span>
<span class="p_add">+	if (cpumask_first(topology_core_cpumask(data-&gt;cpu)) == data-&gt;cpu)</span>
 		print_basics(data);
 
 	/* notify BIOS that we exist */
<span class="p_chunk">@@ -1090,7 +1083,7 @@</span> <span class="p_context"> static int powernowk8_cpu_init(struct cpufreq_policy *pol)</span>
 	if (rc != 0)
 		goto err_out_exit_acpi;
 
<span class="p_del">-	cpumask_copy(pol-&gt;cpus, cpu_core_mask(pol-&gt;cpu));</span>
<span class="p_add">+	cpumask_copy(pol-&gt;cpus, topology_core_cpumask(pol-&gt;cpu));</span>
 	data-&gt;available_cores = pol-&gt;cpus;
 
 	/* min/max the cpu is capable of */
<span class="p_header">diff --git a/drivers/cpufreq/speedstep-ich.c b/drivers/cpufreq/speedstep-ich.c</span>
<span class="p_header">index e56d632a8b21..37555c6b86a7 100644</span>
<span class="p_header">--- a/drivers/cpufreq/speedstep-ich.c</span>
<span class="p_header">+++ b/drivers/cpufreq/speedstep-ich.c</span>
<span class="p_chunk">@@ -292,7 +292,7 @@</span> <span class="p_context"> static int speedstep_cpu_init(struct cpufreq_policy *policy)</span>
 
 	/* only run on CPU to be set, or on its sibling */
 #ifdef CONFIG_SMP
<span class="p_del">-	cpumask_copy(policy-&gt;cpus, cpu_sibling_mask(policy-&gt;cpu));</span>
<span class="p_add">+	cpumask_copy(policy-&gt;cpus, topology_sibling_cpumask(policy-&gt;cpu));</span>
 #endif
 	policy_cpu = cpumask_any_and(policy-&gt;cpus, cpu_online_mask);
 
<span class="p_header">diff --git a/drivers/crypto/vmx/aes.c b/drivers/crypto/vmx/aes.c</span>
<span class="p_header">index ab300ea19434..a9064e36e7b5 100644</span>
<span class="p_header">--- a/drivers/crypto/vmx/aes.c</span>
<span class="p_header">+++ b/drivers/crypto/vmx/aes.c</span>
<span class="p_chunk">@@ -78,12 +78,14 @@</span> <span class="p_context"> static int p8_aes_setkey(struct crypto_tfm *tfm, const u8 *key,</span>
     int ret;
     struct p8_aes_ctx *ctx = crypto_tfm_ctx(tfm);
 
<span class="p_add">+    preempt_disable();</span>
     pagefault_disable();
     enable_kernel_altivec();
     ret = aes_p8_set_encrypt_key(key, keylen * 8, &amp;ctx-&gt;enc_key);
     ret += aes_p8_set_decrypt_key(key, keylen * 8, &amp;ctx-&gt;dec_key);
     pagefault_enable();
<span class="p_del">-    </span>
<span class="p_add">+    preempt_enable();</span>
<span class="p_add">+</span>
     ret += crypto_cipher_setkey(ctx-&gt;fallback, key, keylen);
     return ret;
 }
<span class="p_chunk">@@ -95,10 +97,12 @@</span> <span class="p_context"> static void p8_aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)</span>
     if (in_interrupt()) {
         crypto_cipher_encrypt_one(ctx-&gt;fallback, dst, src);
     } else {
<span class="p_add">+	preempt_disable();</span>
         pagefault_disable();
         enable_kernel_altivec();
         aes_p8_encrypt(src, dst, &amp;ctx-&gt;enc_key);
         pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
     }
 }
 
<span class="p_chunk">@@ -109,10 +113,12 @@</span> <span class="p_context"> static void p8_aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)</span>
     if (in_interrupt()) {
         crypto_cipher_decrypt_one(ctx-&gt;fallback, dst, src);
     } else {
<span class="p_add">+	preempt_disable();</span>
         pagefault_disable();
         enable_kernel_altivec();
         aes_p8_decrypt(src, dst, &amp;ctx-&gt;dec_key);
         pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
     }
 }
 
<span class="p_header">diff --git a/drivers/crypto/vmx/aes_cbc.c b/drivers/crypto/vmx/aes_cbc.c</span>
<span class="p_header">index 1a559b7dddb5..477284abdd11 100644</span>
<span class="p_header">--- a/drivers/crypto/vmx/aes_cbc.c</span>
<span class="p_header">+++ b/drivers/crypto/vmx/aes_cbc.c</span>
<span class="p_chunk">@@ -79,11 +79,13 @@</span> <span class="p_context"> static int p8_aes_cbc_setkey(struct crypto_tfm *tfm, const u8 *key,</span>
     int ret;
     struct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);
 
<span class="p_add">+    preempt_disable();</span>
     pagefault_disable();
     enable_kernel_altivec();
     ret = aes_p8_set_encrypt_key(key, keylen * 8, &amp;ctx-&gt;enc_key);
     ret += aes_p8_set_decrypt_key(key, keylen * 8, &amp;ctx-&gt;dec_key);
     pagefault_enable();
<span class="p_add">+    preempt_enable();</span>
 
     ret += crypto_blkcipher_setkey(ctx-&gt;fallback, key, keylen);
     return ret;
<span class="p_chunk">@@ -106,6 +108,7 @@</span> <span class="p_context"> static int p8_aes_cbc_encrypt(struct blkcipher_desc *desc,</span>
     if (in_interrupt()) {
         ret = crypto_blkcipher_encrypt(&amp;fallback_desc, dst, src, nbytes);
     } else {
<span class="p_add">+	preempt_disable();</span>
         pagefault_disable();
         enable_kernel_altivec();
 
<span class="p_chunk">@@ -119,6 +122,7 @@</span> <span class="p_context"> static int p8_aes_cbc_encrypt(struct blkcipher_desc *desc,</span>
 	}
 
         pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
     }
 
     return ret;
<span class="p_chunk">@@ -141,6 +145,7 @@</span> <span class="p_context"> static int p8_aes_cbc_decrypt(struct blkcipher_desc *desc,</span>
     if (in_interrupt()) {
         ret = crypto_blkcipher_decrypt(&amp;fallback_desc, dst, src, nbytes);
     } else {
<span class="p_add">+	preempt_disable();</span>
         pagefault_disable();
         enable_kernel_altivec();
 
<span class="p_chunk">@@ -154,6 +159,7 @@</span> <span class="p_context"> static int p8_aes_cbc_decrypt(struct blkcipher_desc *desc,</span>
 		}
 
         pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
     }
 
     return ret;
<span class="p_header">diff --git a/drivers/crypto/vmx/ghash.c b/drivers/crypto/vmx/ghash.c</span>
<span class="p_header">index d0ffe277af5c..f255ec4a04d4 100644</span>
<span class="p_header">--- a/drivers/crypto/vmx/ghash.c</span>
<span class="p_header">+++ b/drivers/crypto/vmx/ghash.c</span>
<span class="p_chunk">@@ -114,11 +114,13 @@</span> <span class="p_context"> static int p8_ghash_setkey(struct crypto_shash *tfm, const u8 *key,</span>
     if (keylen != GHASH_KEY_LEN)
         return -EINVAL;
 
<span class="p_add">+    preempt_disable();</span>
     pagefault_disable();
     enable_kernel_altivec();
     enable_kernel_fp();
     gcm_init_p8(ctx-&gt;htable, (const u64 *) key);
     pagefault_enable();
<span class="p_add">+    preempt_enable();</span>
     return crypto_shash_setkey(ctx-&gt;fallback, key, keylen);
 }
 
<span class="p_chunk">@@ -140,23 +142,27 @@</span> <span class="p_context"> static int p8_ghash_update(struct shash_desc *desc,</span>
             }
             memcpy(dctx-&gt;buffer + dctx-&gt;bytes, src,
                     GHASH_DIGEST_SIZE - dctx-&gt;bytes);
<span class="p_add">+	    preempt_disable();</span>
             pagefault_disable();
             enable_kernel_altivec();
             enable_kernel_fp();
             gcm_ghash_p8(dctx-&gt;shash, ctx-&gt;htable, dctx-&gt;buffer,
                     GHASH_DIGEST_SIZE);
             pagefault_enable();
<span class="p_add">+	    preempt_enable();</span>
             src += GHASH_DIGEST_SIZE - dctx-&gt;bytes;
             srclen -= GHASH_DIGEST_SIZE - dctx-&gt;bytes;
             dctx-&gt;bytes = 0;
         }
         len = srclen &amp; ~(GHASH_DIGEST_SIZE - 1);
         if (len) {
<span class="p_add">+	    preempt_disable();</span>
             pagefault_disable();
             enable_kernel_altivec();
             enable_kernel_fp();
             gcm_ghash_p8(dctx-&gt;shash, ctx-&gt;htable, src, len);
             pagefault_enable();
<span class="p_add">+	    preempt_enable();</span>
             src += len;
             srclen -= len;
         }
<span class="p_chunk">@@ -180,12 +186,14 @@</span> <span class="p_context"> static int p8_ghash_final(struct shash_desc *desc, u8 *out)</span>
         if (dctx-&gt;bytes) {
             for (i = dctx-&gt;bytes; i &lt; GHASH_DIGEST_SIZE; i++)
                 dctx-&gt;buffer[i] = 0;
<span class="p_add">+	    preempt_disable();</span>
             pagefault_disable();
             enable_kernel_altivec();
             enable_kernel_fp();
             gcm_ghash_p8(dctx-&gt;shash, ctx-&gt;htable, dctx-&gt;buffer,
                     GHASH_DIGEST_SIZE);
             pagefault_enable();
<span class="p_add">+	    preempt_enable();</span>
             dctx-&gt;bytes = 0;
         }
         memcpy(out, dctx-&gt;shash, GHASH_DIGEST_SIZE);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_header">index a3190e793ed4..cc552a4c1f3b 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 #include &quot;i915_trace.h&quot;
 #include &quot;intel_drv.h&quot;
 #include &lt;linux/dma_remapping.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #define  __EXEC_OBJECT_HAS_PIN (1&lt;&lt;31)
 #define  __EXEC_OBJECT_HAS_FENCE (1&lt;&lt;30)
<span class="p_chunk">@@ -465,7 +466,7 @@</span> <span class="p_context"> i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,</span>
 	}
 
 	/* We can&#39;t wait for rendering with pagefaults disabled */
<span class="p_del">-	if (obj-&gt;active &amp;&amp; in_atomic())</span>
<span class="p_add">+	if (obj-&gt;active &amp;&amp; pagefault_disabled())</span>
 		return -EFAULT;
 
 	if (use_cpu_reloc(obj))
<span class="p_header">diff --git a/drivers/hwmon/coretemp.c b/drivers/hwmon/coretemp.c</span>
<span class="p_header">index ed303ba3a593..3e03379e7c5d 100644</span>
<span class="p_header">--- a/drivers/hwmon/coretemp.c</span>
<span class="p_header">+++ b/drivers/hwmon/coretemp.c</span>
<span class="p_chunk">@@ -63,7 +63,8 @@</span> <span class="p_context"> MODULE_PARM_DESC(tjmax, &quot;TjMax value in degrees Celsius&quot;);</span>
 #define TO_ATTR_NO(cpu)		(TO_CORE_ID(cpu) + BASE_SYSFS_ATTR_NO)
 
 #ifdef CONFIG_SMP
<span class="p_del">-#define for_each_sibling(i, cpu)	for_each_cpu(i, cpu_sibling_mask(cpu))</span>
<span class="p_add">+#define for_each_sibling(i, cpu) \</span>
<span class="p_add">+	for_each_cpu(i, topology_sibling_cpumask(cpu))</span>
 #else
 #define for_each_sibling(i, cpu)	for (i = 0; false; )
 #endif
<span class="p_header">diff --git a/drivers/net/ethernet/sfc/efx.c b/drivers/net/ethernet/sfc/efx.c</span>
<span class="p_header">index 4b00545a3ace..65944dd8bf6b 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/sfc/efx.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/sfc/efx.c</span>
<span class="p_chunk">@@ -1304,7 +1304,7 @@</span> <span class="p_context"> static unsigned int efx_wanted_parallelism(struct efx_nic *efx)</span>
 			if (!cpumask_test_cpu(cpu, thread_mask)) {
 				++count;
 				cpumask_or(thread_mask, thread_mask,
<span class="p_del">-					   topology_thread_cpumask(cpu));</span>
<span class="p_add">+					   topology_sibling_cpumask(cpu));</span>
 			}
 		}
 
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/libcfs/linux/linux-cpu.c b/drivers/staging/lustre/lustre/libcfs/linux/linux-cpu.c</span>
<span class="p_header">index cc3ab351943e..f9262243f935 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/libcfs/linux/linux-cpu.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/libcfs/linux/linux-cpu.c</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> static void cfs_cpu_core_siblings(int cpu, cpumask_t *mask)</span>
 /* return cpumask of HTs in the same core */
 static void cfs_cpu_ht_siblings(int cpu, cpumask_t *mask)
 {
<span class="p_del">-	cpumask_copy(mask, topology_thread_cpumask(cpu));</span>
<span class="p_add">+	cpumask_copy(mask, topology_sibling_cpumask(cpu));</span>
 }
 
 static void cfs_node_to_cpumask(int node, cpumask_t *mask)
<span class="p_header">diff --git a/drivers/staging/lustre/lustre/ptlrpc/service.c b/drivers/staging/lustre/lustre/ptlrpc/service.c</span>
<span class="p_header">index 8e61421515cb..344189ac5698 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/lustre/ptlrpc/service.c</span>
<span class="p_header">+++ b/drivers/staging/lustre/lustre/ptlrpc/service.c</span>
<span class="p_chunk">@@ -557,7 +557,7 @@</span> <span class="p_context"> ptlrpc_server_nthreads_check(struct ptlrpc_service *svc,</span>
 		 * there are.
 		 */
 		/* weight is # of HTs */
<span class="p_del">-		if (cpumask_weight(topology_thread_cpumask(0)) &gt; 1) {</span>
<span class="p_add">+		if (cpumask_weight(topology_sibling_cpumask(0)) &gt; 1) {</span>
 			/* depress thread factor for hyper-thread */
 			factor = factor - (factor &gt;&gt; 1) + (factor &gt;&gt; 3);
 		}
<span class="p_chunk">@@ -2768,7 +2768,7 @@</span> <span class="p_context"> int ptlrpc_hr_init(void)</span>
 
 	init_waitqueue_head(&amp;ptlrpc_hr.hr_waitq);
 
<span class="p_del">-	weight = cpumask_weight(topology_thread_cpumask(0));</span>
<span class="p_add">+	weight = cpumask_weight(topology_sibling_cpumask(0));</span>
 
 	cfs_percpt_for_each(hrp, i, ptlrpc_hr.hr_partitions) {
 		hrp-&gt;hrp_cpt = i;
<span class="p_header">diff --git a/include/asm-generic/futex.h b/include/asm-generic/futex.h</span>
<span class="p_header">index b59b5a52637e..e56272c919b5 100644</span>
<span class="p_header">--- a/include/asm-generic/futex.h</span>
<span class="p_header">+++ b/include/asm-generic/futex.h</span>
<span class="p_chunk">@@ -8,8 +8,7 @@</span> <span class="p_context"></span>
 #ifndef CONFIG_SMP
 /*
  * The following implementation only for uniprocessor machines.
<span class="p_del">- * For UP, it&#39;s relies on the fact that pagefault_disable() also disables</span>
<span class="p_del">- * preemption to ensure mutual exclusion.</span>
<span class="p_add">+ * It relies on preempt_disable() ensuring mutual exclusion.</span>
  *
  */
 
<span class="p_chunk">@@ -38,6 +37,7 @@</span> <span class="p_context"> futex_atomic_op_inuser(int encoded_op, u32 __user *uaddr)</span>
 	if (encoded_op &amp; (FUTEX_OP_OPARG_SHIFT &lt;&lt; 28))
 		oparg = 1 &lt;&lt; oparg;
 
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 
 	ret = -EFAULT;
<span class="p_chunk">@@ -72,6 +72,7 @@</span> <span class="p_context"> futex_atomic_op_inuser(int encoded_op, u32 __user *uaddr)</span>
 
 out_pagefault_enable:
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 
 	if (ret == 0) {
 		switch (cmp) {
<span class="p_chunk">@@ -106,6 +107,7 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,</span>
 {
 	u32 val;
 
<span class="p_add">+	preempt_disable();</span>
 	if (unlikely(get_user(val, uaddr) != 0))
 		return -EFAULT;
 
<span class="p_chunk">@@ -113,6 +115,7 @@</span> <span class="p_context"> futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,</span>
 		return -EFAULT;
 
 	*uval = val;
<span class="p_add">+	preempt_enable();</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/include/asm-generic/preempt.h b/include/asm-generic/preempt.h</span>
<span class="p_header">index eb6f9e6c3075..d0a7a4753db2 100644</span>
<span class="p_header">--- a/include/asm-generic/preempt.h</span>
<span class="p_header">+++ b/include/asm-generic/preempt.h</span>
<span class="p_chunk">@@ -79,11 +79,8 @@</span> <span class="p_context"> static __always_inline bool should_resched(void)</span>
 #ifdef CONFIG_PREEMPT
 extern asmlinkage void preempt_schedule(void);
 #define __preempt_schedule() preempt_schedule()
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-extern asmlinkage void preempt_schedule_context(void);</span>
<span class="p_del">-#define __preempt_schedule_context() preempt_schedule_context()</span>
<span class="p_del">-#endif</span>
<span class="p_add">+extern asmlinkage void preempt_schedule_notrace(void);</span>
<span class="p_add">+#define __preempt_schedule_notrace() preempt_schedule_notrace()</span>
 #endif /* CONFIG_PREEMPT */
 
 #endif /* __ASM_PREEMPT_H */
<span class="p_header">diff --git a/include/linux/bottom_half.h b/include/linux/bottom_half.h</span>
<span class="p_header">index 86c12c93e3cf..8fdcb783197d 100644</span>
<span class="p_header">--- a/include/linux/bottom_half.h</span>
<span class="p_header">+++ b/include/linux/bottom_half.h</span>
<span class="p_chunk">@@ -2,7 +2,6 @@</span> <span class="p_context"></span>
 #define _LINUX_BH_H
 
 #include &lt;linux/preempt.h&gt;
<span class="p_del">-#include &lt;linux/preempt_mask.h&gt;</span>
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
<span class="p_header">diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h</span>
<span class="p_header">index f4af03404b97..dfd59d6bc6f0 100644</span>
<span class="p_header">--- a/include/linux/hardirq.h</span>
<span class="p_header">+++ b/include/linux/hardirq.h</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 #ifndef LINUX_HARDIRQ_H
 #define LINUX_HARDIRQ_H
 
<span class="p_del">-#include &lt;linux/preempt_mask.h&gt;</span>
<span class="p_add">+#include &lt;linux/preempt.h&gt;</span>
 #include &lt;linux/lockdep.h&gt;
 #include &lt;linux/ftrace_irq.h&gt;
 #include &lt;linux/vtime.h&gt;
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index 9286a46b7d69..6aefcd0031a6 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -65,6 +65,7 @@</span> <span class="p_context"> static inline void kunmap(struct page *page)</span>
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	return page_address(page);
 }
<span class="p_chunk">@@ -73,6 +74,7 @@</span> <span class="p_context"> static inline void *kmap_atomic(struct page *page)</span>
 static inline void __kunmap_atomic(void *addr)
 {
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 
 #define kmap_atomic_pfn(pfn)	kmap_atomic(pfn_to_page(pfn))
<span class="p_header">diff --git a/include/linux/init_task.h b/include/linux/init_task.h</span>
<span class="p_header">index 696d22312b31..bb9b075f0eb0 100644</span>
<span class="p_header">--- a/include/linux/init_task.h</span>
<span class="p_header">+++ b/include/linux/init_task.h</span>
<span class="p_chunk">@@ -50,9 +50,8 @@</span> <span class="p_context"> extern struct fs_struct init_fs;</span>
 	.cpu_timers	= INIT_CPU_TIMERS(sig.cpu_timers),		\
 	.rlim		= INIT_RLIMITS,					\
 	.cputimer	= { 						\
<span class="p_del">-		.cputime = INIT_CPUTIME,				\</span>
<span class="p_del">-		.running = 0,						\</span>
<span class="p_del">-		.lock = __RAW_SPIN_LOCK_UNLOCKED(sig.cputimer.lock),	\</span>
<span class="p_add">+		.cputime_atomic	= INIT_CPUTIME_ATOMIC,			\</span>
<span class="p_add">+		.running	= 0,					\</span>
 	},								\
 	.cred_guard_mutex =						\
 		 __MUTEX_INITIALIZER(sig.cred_guard_mutex),		\
<span class="p_header">diff --git a/include/linux/io-mapping.h b/include/linux/io-mapping.h</span>
<span class="p_header">index 657fab4efab3..c27dde7215b5 100644</span>
<span class="p_header">--- a/include/linux/io-mapping.h</span>
<span class="p_header">+++ b/include/linux/io-mapping.h</span>
<span class="p_chunk">@@ -141,6 +141,7 @@</span> <span class="p_context"> static inline void __iomem *</span>
 io_mapping_map_atomic_wc(struct io_mapping *mapping,
 			 unsigned long offset)
 {
<span class="p_add">+	preempt_disable();</span>
 	pagefault_disable();
 	return ((char __force __iomem *) mapping) + offset;
 }
<span class="p_chunk">@@ -149,6 +150,7 @@</span> <span class="p_context"> static inline void</span>
 io_mapping_unmap_atomic(void __iomem *vaddr)
 {
 	pagefault_enable();
<span class="p_add">+	preempt_enable();</span>
 }
 
 /* Non-atomic map/unmap */
<span class="p_header">diff --git a/include/linux/kernel.h b/include/linux/kernel.h</span>
<span class="p_header">index 3a5b48e52a9e..060dd7b61c6d 100644</span>
<span class="p_header">--- a/include/linux/kernel.h</span>
<span class="p_header">+++ b/include/linux/kernel.h</span>
<span class="p_chunk">@@ -244,7 +244,8 @@</span> <span class="p_context"> static inline u32 reciprocal_scale(u32 val, u32 ep_ro)</span>
 
 #if defined(CONFIG_MMU) &amp;&amp; \
 	(defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP))
<span class="p_del">-void might_fault(void);</span>
<span class="p_add">+#define might_fault() __might_fault(__FILE__, __LINE__)</span>
<span class="p_add">+void __might_fault(const char *file, int line);</span>
 #else
 static inline void might_fault(void) { }
 #endif
<span class="p_header">diff --git a/include/linux/lglock.h b/include/linux/lglock.h</span>
<span class="p_header">index 0081f000e34b..c92ebd100d9b 100644</span>
<span class="p_header">--- a/include/linux/lglock.h</span>
<span class="p_header">+++ b/include/linux/lglock.h</span>
<span class="p_chunk">@@ -52,10 +52,15 @@</span> <span class="p_context"> struct lglock {</span>
 	static struct lglock name = { .lock = &amp;name ## _lock }
 
 void lg_lock_init(struct lglock *lg, char *name);
<span class="p_add">+</span>
 void lg_local_lock(struct lglock *lg);
 void lg_local_unlock(struct lglock *lg);
 void lg_local_lock_cpu(struct lglock *lg, int cpu);
 void lg_local_unlock_cpu(struct lglock *lg, int cpu);
<span class="p_add">+</span>
<span class="p_add">+void lg_double_lock(struct lglock *lg, int cpu1, int cpu2);</span>
<span class="p_add">+void lg_double_unlock(struct lglock *lg, int cpu1, int cpu2);</span>
<span class="p_add">+</span>
 void lg_global_lock(struct lglock *lg);
 void lg_global_unlock(struct lglock *lg);
 
<span class="p_header">diff --git a/include/linux/preempt.h b/include/linux/preempt.h</span>
<span class="p_header">index de83b4eb1642..0f1534acaf60 100644</span>
<span class="p_header">--- a/include/linux/preempt.h</span>
<span class="p_header">+++ b/include/linux/preempt.h</span>
<span class="p_chunk">@@ -10,13 +10,117 @@</span> <span class="p_context"></span>
 #include &lt;linux/list.h&gt;
 
 /*
<span class="p_del">- * We use the MSB mostly because its available; see &lt;linux/preempt_mask.h&gt; for</span>
<span class="p_del">- * the other bits -- can&#39;t include that header due to inclusion hell.</span>
<span class="p_add">+ * We put the hardirq and softirq counter into the preemption</span>
<span class="p_add">+ * counter. The bitmask has the following meaning:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * - bits 0-7 are the preemption count (max preemption depth: 256)</span>
<span class="p_add">+ * - bits 8-15 are the softirq count (max # of softirqs: 256)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hardirq count could in theory be the same as the number of</span>
<span class="p_add">+ * interrupts in the system, but we run all interrupt handlers with</span>
<span class="p_add">+ * interrupts disabled, so we cannot have nesting interrupts. Though</span>
<span class="p_add">+ * there are a few palaeontologic drivers which reenable interrupts in</span>
<span class="p_add">+ * the handler, so we need more than one bit here.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *         PREEMPT_MASK:	0x000000ff</span>
<span class="p_add">+ *         SOFTIRQ_MASK:	0x0000ff00</span>
<span class="p_add">+ *         HARDIRQ_MASK:	0x000f0000</span>
<span class="p_add">+ *             NMI_MASK:	0x00100000</span>
<span class="p_add">+ *       PREEMPT_ACTIVE:	0x00200000</span>
<span class="p_add">+ * PREEMPT_NEED_RESCHED:	0x80000000</span>
  */
<span class="p_add">+#define PREEMPT_BITS	8</span>
<span class="p_add">+#define SOFTIRQ_BITS	8</span>
<span class="p_add">+#define HARDIRQ_BITS	4</span>
<span class="p_add">+#define NMI_BITS	1</span>
<span class="p_add">+</span>
<span class="p_add">+#define PREEMPT_SHIFT	0</span>
<span class="p_add">+#define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)</span>
<span class="p_add">+#define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)</span>
<span class="p_add">+#define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __IRQ_MASK(x)	((1UL &lt;&lt; (x))-1)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) &lt;&lt; PREEMPT_SHIFT)</span>
<span class="p_add">+#define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) &lt;&lt; SOFTIRQ_SHIFT)</span>
<span class="p_add">+#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) &lt;&lt; HARDIRQ_SHIFT)</span>
<span class="p_add">+#define NMI_MASK	(__IRQ_MASK(NMI_BITS)     &lt;&lt; NMI_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PREEMPT_OFFSET	(1UL &lt;&lt; PREEMPT_SHIFT)</span>
<span class="p_add">+#define SOFTIRQ_OFFSET	(1UL &lt;&lt; SOFTIRQ_SHIFT)</span>
<span class="p_add">+#define HARDIRQ_OFFSET	(1UL &lt;&lt; HARDIRQ_SHIFT)</span>
<span class="p_add">+#define NMI_OFFSET	(1UL &lt;&lt; NMI_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PREEMPT_ACTIVE_BITS	1</span>
<span class="p_add">+#define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)</span>
<span class="p_add">+#define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) &lt;&lt; PREEMPT_ACTIVE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+/* We use the MSB mostly because its available */</span>
 #define PREEMPT_NEED_RESCHED	0x80000000
 
<span class="p_add">+/* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */</span>
 #include &lt;asm/preempt.h&gt;
 
<span class="p_add">+#define hardirq_count()	(preempt_count() &amp; HARDIRQ_MASK)</span>
<span class="p_add">+#define softirq_count()	(preempt_count() &amp; SOFTIRQ_MASK)</span>
<span class="p_add">+#define irq_count()	(preempt_count() &amp; (HARDIRQ_MASK | SOFTIRQ_MASK \</span>
<span class="p_add">+				 | NMI_MASK))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Are we doing bottom half or hardware interrupt processing?</span>
<span class="p_add">+ * Are we in a softirq context? Interrupt context?</span>
<span class="p_add">+ * in_softirq - Are we currently processing softirq or have bh disabled?</span>
<span class="p_add">+ * in_serving_softirq - Are we currently processing softirq?</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define in_irq()		(hardirq_count())</span>
<span class="p_add">+#define in_softirq()		(softirq_count())</span>
<span class="p_add">+#define in_interrupt()		(irq_count())</span>
<span class="p_add">+#define in_serving_softirq()	(softirq_count() &amp; SOFTIRQ_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Are we in NMI context?</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define in_nmi()	(preempt_count() &amp; NMI_MASK)</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_PREEMPT_COUNT)</span>
<span class="p_add">+# define PREEMPT_DISABLE_OFFSET 1</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define PREEMPT_DISABLE_OFFSET 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The preempt_count offset needed for things like:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  spin_lock_bh()</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Which need to disable both preemption (CONFIG_PREEMPT_COUNT) and</span>
<span class="p_add">+ * softirqs, such that unlock sequences of:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  spin_unlock();</span>
<span class="p_add">+ *  local_bh_enable();</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Work as expected.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define SOFTIRQ_LOCK_OFFSET (SOFTIRQ_DISABLE_OFFSET + PREEMPT_DISABLE_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Are we running in atomic context?  WARNING: this macro cannot</span>
<span class="p_add">+ * always detect atomic context; in particular, it cannot know about</span>
<span class="p_add">+ * held spinlocks in non-preemptible kernels.  Thus it should not be</span>
<span class="p_add">+ * used in the general case to determine whether sleeping is possible.</span>
<span class="p_add">+ * Do not use in_atomic() in driver code.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define in_atomic()	(preempt_count() != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check whether we were atomic before we did preempt_disable():</span>
<span class="p_add">+ * (used by the scheduler)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define in_atomic_preempt_off() \</span>
<span class="p_add">+		((preempt_count() &amp; ~PREEMPT_ACTIVE) != PREEMPT_DISABLE_OFFSET)</span>
<span class="p_add">+</span>
 #if defined(CONFIG_DEBUG_PREEMPT) || defined(CONFIG_PREEMPT_TRACER)
 extern void preempt_count_add(int val);
 extern void preempt_count_sub(int val);
<span class="p_chunk">@@ -33,6 +137,18 @@</span> <span class="p_context"> extern void preempt_count_sub(int val);</span>
 #define preempt_count_inc() preempt_count_add(1)
 #define preempt_count_dec() preempt_count_sub(1)
 
<span class="p_add">+#define preempt_active_enter() \</span>
<span class="p_add">+do { \</span>
<span class="p_add">+	preempt_count_add(PREEMPT_ACTIVE + PREEMPT_DISABLE_OFFSET); \</span>
<span class="p_add">+	barrier(); \</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define preempt_active_exit() \</span>
<span class="p_add">+do { \</span>
<span class="p_add">+	barrier(); \</span>
<span class="p_add">+	preempt_count_sub(PREEMPT_ACTIVE + PREEMPT_DISABLE_OFFSET); \</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PREEMPT_COUNT
 
 #define preempt_disable() \
<span class="p_chunk">@@ -49,6 +165,8 @@</span> <span class="p_context"> do { \</span>
 
 #define preempt_enable_no_resched() sched_preempt_enable_no_resched()
 
<span class="p_add">+#define preemptible()	(preempt_count() == 0 &amp;&amp; !irqs_disabled())</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PREEMPT
 #define preempt_enable() \
 do { \
<span class="p_chunk">@@ -57,52 +175,46 @@</span> <span class="p_context"> do { \</span>
 		__preempt_schedule(); \
 } while (0)
 
<span class="p_add">+#define preempt_enable_notrace() \</span>
<span class="p_add">+do { \</span>
<span class="p_add">+	barrier(); \</span>
<span class="p_add">+	if (unlikely(__preempt_count_dec_and_test())) \</span>
<span class="p_add">+		__preempt_schedule_notrace(); \</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
 #define preempt_check_resched() \
 do { \
 	if (should_resched()) \
 		__preempt_schedule(); \
 } while (0)
 
<span class="p_del">-#else</span>
<span class="p_add">+#else /* !CONFIG_PREEMPT */</span>
 #define preempt_enable() \
 do { \
 	barrier(); \
 	preempt_count_dec(); \
 } while (0)
<span class="p_del">-#define preempt_check_resched() do { } while (0)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#define preempt_disable_notrace() \</span>
<span class="p_del">-do { \</span>
<span class="p_del">-	__preempt_count_inc(); \</span>
<span class="p_del">-	barrier(); \</span>
<span class="p_del">-} while (0)</span>
 
<span class="p_del">-#define preempt_enable_no_resched_notrace() \</span>
<span class="p_add">+#define preempt_enable_notrace() \</span>
 do { \
 	barrier(); \
 	__preempt_count_dec(); \
 } while (0)
 
<span class="p_del">-#ifdef CONFIG_PREEMPT</span>
<span class="p_del">-</span>
<span class="p_del">-#ifndef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-#define __preempt_schedule_context() __preempt_schedule()</span>
<span class="p_del">-#endif</span>
<span class="p_add">+#define preempt_check_resched() do { } while (0)</span>
<span class="p_add">+#endif /* CONFIG_PREEMPT */</span>
 
<span class="p_del">-#define preempt_enable_notrace() \</span>
<span class="p_add">+#define preempt_disable_notrace() \</span>
 do { \
<span class="p_add">+	__preempt_count_inc(); \</span>
 	barrier(); \
<span class="p_del">-	if (unlikely(__preempt_count_dec_and_test())) \</span>
<span class="p_del">-		__preempt_schedule_context(); \</span>
 } while (0)
<span class="p_del">-#else</span>
<span class="p_del">-#define preempt_enable_notrace() \</span>
<span class="p_add">+</span>
<span class="p_add">+#define preempt_enable_no_resched_notrace() \</span>
 do { \
 	barrier(); \
 	__preempt_count_dec(); \
 } while (0)
<span class="p_del">-#endif</span>
 
 #else /* !CONFIG_PREEMPT_COUNT */
 
<span class="p_chunk">@@ -121,6 +233,7 @@</span> <span class="p_context"> do { \</span>
 #define preempt_disable_notrace()		barrier()
 #define preempt_enable_no_resched_notrace()	barrier()
 #define preempt_enable_notrace()		barrier()
<span class="p_add">+#define preemptible()				0</span>
 
 #endif /* CONFIG_PREEMPT_COUNT */
 
<span class="p_header">diff --git a/include/linux/preempt_mask.h b/include/linux/preempt_mask.h</span>
deleted file mode 100644
<span class="p_header">index dbeec4d4a3be..000000000000</span>
<span class="p_header">--- a/include/linux/preempt_mask.h</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,117 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-#ifndef LINUX_PREEMPT_MASK_H</span>
<span class="p_del">-#define LINUX_PREEMPT_MASK_H</span>
<span class="p_del">-</span>
<span class="p_del">-#include &lt;linux/preempt.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * We put the hardirq and softirq counter into the preemption</span>
<span class="p_del">- * counter. The bitmask has the following meaning:</span>
<span class="p_del">- *</span>
<span class="p_del">- * - bits 0-7 are the preemption count (max preemption depth: 256)</span>
<span class="p_del">- * - bits 8-15 are the softirq count (max # of softirqs: 256)</span>
<span class="p_del">- *</span>
<span class="p_del">- * The hardirq count could in theory be the same as the number of</span>
<span class="p_del">- * interrupts in the system, but we run all interrupt handlers with</span>
<span class="p_del">- * interrupts disabled, so we cannot have nesting interrupts. Though</span>
<span class="p_del">- * there are a few palaeontologic drivers which reenable interrupts in</span>
<span class="p_del">- * the handler, so we need more than one bit here.</span>
<span class="p_del">- *</span>
<span class="p_del">- * PREEMPT_MASK:	0x000000ff</span>
<span class="p_del">- * SOFTIRQ_MASK:	0x0000ff00</span>
<span class="p_del">- * HARDIRQ_MASK:	0x000f0000</span>
<span class="p_del">- *     NMI_MASK:	0x00100000</span>
<span class="p_del">- * PREEMPT_ACTIVE:	0x00200000</span>
<span class="p_del">- */</span>
<span class="p_del">-#define PREEMPT_BITS	8</span>
<span class="p_del">-#define SOFTIRQ_BITS	8</span>
<span class="p_del">-#define HARDIRQ_BITS	4</span>
<span class="p_del">-#define NMI_BITS	1</span>
<span class="p_del">-</span>
<span class="p_del">-#define PREEMPT_SHIFT	0</span>
<span class="p_del">-#define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)</span>
<span class="p_del">-#define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)</span>
<span class="p_del">-#define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)</span>
<span class="p_del">-</span>
<span class="p_del">-#define __IRQ_MASK(x)	((1UL &lt;&lt; (x))-1)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PREEMPT_MASK	(__IRQ_MASK(PREEMPT_BITS) &lt;&lt; PREEMPT_SHIFT)</span>
<span class="p_del">-#define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) &lt;&lt; SOFTIRQ_SHIFT)</span>
<span class="p_del">-#define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) &lt;&lt; HARDIRQ_SHIFT)</span>
<span class="p_del">-#define NMI_MASK	(__IRQ_MASK(NMI_BITS)     &lt;&lt; NMI_SHIFT)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PREEMPT_OFFSET	(1UL &lt;&lt; PREEMPT_SHIFT)</span>
<span class="p_del">-#define SOFTIRQ_OFFSET	(1UL &lt;&lt; SOFTIRQ_SHIFT)</span>
<span class="p_del">-#define HARDIRQ_OFFSET	(1UL &lt;&lt; HARDIRQ_SHIFT)</span>
<span class="p_del">-#define NMI_OFFSET	(1UL &lt;&lt; NMI_SHIFT)</span>
<span class="p_del">-</span>
<span class="p_del">-#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PREEMPT_ACTIVE_BITS	1</span>
<span class="p_del">-#define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)</span>
<span class="p_del">-#define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) &lt;&lt; PREEMPT_ACTIVE_SHIFT)</span>
<span class="p_del">-</span>
<span class="p_del">-#define hardirq_count()	(preempt_count() &amp; HARDIRQ_MASK)</span>
<span class="p_del">-#define softirq_count()	(preempt_count() &amp; SOFTIRQ_MASK)</span>
<span class="p_del">-#define irq_count()	(preempt_count() &amp; (HARDIRQ_MASK | SOFTIRQ_MASK \</span>
<span class="p_del">-				 | NMI_MASK))</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Are we doing bottom half or hardware interrupt processing?</span>
<span class="p_del">- * Are we in a softirq context? Interrupt context?</span>
<span class="p_del">- * in_softirq - Are we currently processing softirq or have bh disabled?</span>
<span class="p_del">- * in_serving_softirq - Are we currently processing softirq?</span>
<span class="p_del">- */</span>
<span class="p_del">-#define in_irq()		(hardirq_count())</span>
<span class="p_del">-#define in_softirq()		(softirq_count())</span>
<span class="p_del">-#define in_interrupt()		(irq_count())</span>
<span class="p_del">-#define in_serving_softirq()	(softirq_count() &amp; SOFTIRQ_OFFSET)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Are we in NMI context?</span>
<span class="p_del">- */</span>
<span class="p_del">-#define in_nmi()	(preempt_count() &amp; NMI_MASK)</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_PREEMPT_COUNT)</span>
<span class="p_del">-# define PREEMPT_CHECK_OFFSET 1</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define PREEMPT_CHECK_OFFSET 0</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * The preempt_count offset needed for things like:</span>
<span class="p_del">- *</span>
<span class="p_del">- *  spin_lock_bh()</span>
<span class="p_del">- *</span>
<span class="p_del">- * Which need to disable both preemption (CONFIG_PREEMPT_COUNT) and</span>
<span class="p_del">- * softirqs, such that unlock sequences of:</span>
<span class="p_del">- *</span>
<span class="p_del">- *  spin_unlock();</span>
<span class="p_del">- *  local_bh_enable();</span>
<span class="p_del">- *</span>
<span class="p_del">- * Work as expected.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define SOFTIRQ_LOCK_OFFSET (SOFTIRQ_DISABLE_OFFSET + PREEMPT_CHECK_OFFSET)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Are we running in atomic context?  WARNING: this macro cannot</span>
<span class="p_del">- * always detect atomic context; in particular, it cannot know about</span>
<span class="p_del">- * held spinlocks in non-preemptible kernels.  Thus it should not be</span>
<span class="p_del">- * used in the general case to determine whether sleeping is possible.</span>
<span class="p_del">- * Do not use in_atomic() in driver code.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define in_atomic()	((preempt_count() &amp; ~PREEMPT_ACTIVE) != 0)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Check whether we were atomic before we did preempt_disable():</span>
<span class="p_del">- * (used by the scheduler, *after* releasing the kernel lock)</span>
<span class="p_del">- */</span>
<span class="p_del">-#define in_atomic_preempt_off() \</span>
<span class="p_del">-		((preempt_count() &amp; ~PREEMPT_ACTIVE) != PREEMPT_CHECK_OFFSET)</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_PREEMPT_COUNT</span>
<span class="p_del">-# define preemptible()	(preempt_count() == 0 &amp;&amp; !irqs_disabled())</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define preemptible()	0</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* LINUX_PREEMPT_MASK_H */</span>
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 26a2e6122734..7de815c6fa78 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> struct sched_param {</span>
 #include &lt;linux/errno.h&gt;
 #include &lt;linux/nodemask.h&gt;
 #include &lt;linux/mm_types.h&gt;
<span class="p_del">-#include &lt;linux/preempt_mask.h&gt;</span>
<span class="p_add">+#include &lt;linux/preempt.h&gt;</span>
 
 #include &lt;asm/page.h&gt;
 #include &lt;asm/ptrace.h&gt;
<span class="p_chunk">@@ -173,7 +173,12 @@</span> <span class="p_context"> extern unsigned long nr_iowait_cpu(int cpu);</span>
 extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
 
 extern void calc_global_load(unsigned long ticks);
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_NO_HZ_COMMON)</span>
 extern void update_cpu_load_nohz(void);
<span class="p_add">+#else</span>
<span class="p_add">+static inline void update_cpu_load_nohz(void) { }</span>
<span class="p_add">+#endif</span>
 
 extern unsigned long get_parent_ip(unsigned long addr);
 
<span class="p_chunk">@@ -213,9 +218,10 @@</span> <span class="p_context"> print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);</span>
 #define TASK_WAKEKILL		128
 #define TASK_WAKING		256
 #define TASK_PARKED		512
<span class="p_del">-#define TASK_STATE_MAX		1024</span>
<span class="p_add">+#define TASK_NOLOAD		1024</span>
<span class="p_add">+#define TASK_STATE_MAX		2048</span>
 
<span class="p_del">-#define TASK_STATE_TO_CHAR_STR &quot;RSDTtXZxKWP&quot;</span>
<span class="p_add">+#define TASK_STATE_TO_CHAR_STR &quot;RSDTtXZxKWPN&quot;</span>
 
 extern char ___assert_task_state[1 - 2*!!(
 		sizeof(TASK_STATE_TO_CHAR_STR)-1 != ilog2(TASK_STATE_MAX)+1)];
<span class="p_chunk">@@ -225,6 +231,8 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
 #define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)
 
<span class="p_add">+#define TASK_IDLE		(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)</span>
<span class="p_add">+</span>
 /* Convenience macros for the sake of wake_up */
 #define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
 #define TASK_ALL		(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)
<span class="p_chunk">@@ -240,7 +248,8 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 			((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task-&gt;state &amp; TASK_UNINTERRUPTIBLE) != 0 &amp;&amp; \
<span class="p_del">-				 (task-&gt;flags &amp; PF_FROZEN) == 0)</span>
<span class="p_add">+				 (task-&gt;flags &amp; PF_FROZEN) == 0 &amp;&amp; \</span>
<span class="p_add">+				 (task-&gt;state &amp; TASK_NOLOAD) == 0)</span>
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
<span class="p_chunk">@@ -567,6 +576,23 @@</span> <span class="p_context"> struct task_cputime {</span>
 		.sum_exec_runtime = 0,				\
 	}
 
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the atomic variant of task_cputime, which can be used for</span>
<span class="p_add">+ * storing and updating task_cputime statistics without locking.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct task_cputime_atomic {</span>
<span class="p_add">+	atomic64_t utime;</span>
<span class="p_add">+	atomic64_t stime;</span>
<span class="p_add">+	atomic64_t sum_exec_runtime;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define INIT_CPUTIME_ATOMIC \</span>
<span class="p_add">+	(struct task_cputime_atomic) {				\</span>
<span class="p_add">+		.utime = ATOMIC64_INIT(0),			\</span>
<span class="p_add">+		.stime = ATOMIC64_INIT(0),			\</span>
<span class="p_add">+		.sum_exec_runtime = ATOMIC64_INIT(0),		\</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PREEMPT_COUNT
 #define PREEMPT_DISABLED	(1 + PREEMPT_ENABLED)
 #else
<span class="p_chunk">@@ -584,18 +610,16 @@</span> <span class="p_context"> struct task_cputime {</span>
 
 /**
  * struct thread_group_cputimer - thread group interval timer counts
<span class="p_del">- * @cputime:		thread group interval timers.</span>
<span class="p_add">+ * @cputime_atomic:	atomic thread group interval timers.</span>
  * @running:		non-zero when there are timers running and
  * 			@cputime receives updates.
<span class="p_del">- * @lock:		lock for fields in this struct.</span>
  *
  * This structure contains the version of task_cputime, above, that is
  * used for thread group CPU timer calculations.
  */
 struct thread_group_cputimer {
<span class="p_del">-	struct task_cputime cputime;</span>
<span class="p_add">+	struct task_cputime_atomic cputime_atomic;</span>
 	int running;
<span class="p_del">-	raw_spinlock_t lock;</span>
 };
 
 #include &lt;linux/rwsem.h&gt;
<span class="p_chunk">@@ -900,6 +924,50 @@</span> <span class="p_context"> enum cpu_idle_type {</span>
 #define SCHED_CAPACITY_SCALE	(1L &lt;&lt; SCHED_CAPACITY_SHIFT)
 
 /*
<span class="p_add">+ * Wake-queues are lists of tasks with a pending wakeup, whose</span>
<span class="p_add">+ * callers have already marked the task as woken internally,</span>
<span class="p_add">+ * and can thus carry on. A common use case is being able to</span>
<span class="p_add">+ * do the wakeups once the corresponding user lock as been</span>
<span class="p_add">+ * released.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We hold reference to each task in the list across the wakeup,</span>
<span class="p_add">+ * thus guaranteeing that the memory is still valid by the time</span>
<span class="p_add">+ * the actual wakeups are performed in wake_up_q().</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * One per task suffices, because there&#39;s never a need for a task to be</span>
<span class="p_add">+ * in two wake queues simultaneously; it is forbidden to abandon a task</span>
<span class="p_add">+ * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is</span>
<span class="p_add">+ * already in a wake queue, the wakeup will happen soon and the second</span>
<span class="p_add">+ * waker can just skip it.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The WAKE_Q macro declares and initializes the list head.</span>
<span class="p_add">+ * wake_up_q() does NOT reinitialize the list; it&#39;s expected to be</span>
<span class="p_add">+ * called near the end of a function, where the fact that the queue is</span>
<span class="p_add">+ * not used again will be easy to see by inspection.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that this can cause spurious wakeups. schedule() callers</span>
<span class="p_add">+ * must ensure the call is done inside a loop, confirming that the</span>
<span class="p_add">+ * wakeup condition has in fact occurred.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct wake_q_node {</span>
<span class="p_add">+	struct wake_q_node *next;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct wake_q_head {</span>
<span class="p_add">+	struct wake_q_node *first;</span>
<span class="p_add">+	struct wake_q_node **lastp;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)</span>
<span class="p_add">+</span>
<span class="p_add">+#define WAKE_Q(name)					\</span>
<span class="p_add">+	struct wake_q_head name = { WAKE_Q_TAIL, &amp;name.first }</span>
<span class="p_add">+</span>
<span class="p_add">+extern void wake_q_add(struct wake_q_head *head,</span>
<span class="p_add">+		       struct task_struct *task);</span>
<span class="p_add">+extern void wake_up_q(struct wake_q_head *head);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * sched-domains (multiprocessor balancing) declarations:
  */
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -1334,8 +1402,6 @@</span> <span class="p_context"> struct task_struct {</span>
 	int rcu_read_lock_nesting;
 	union rcu_special rcu_read_unlock_special;
 	struct list_head rcu_node_entry;
<span class="p_del">-#endif /* #ifdef CONFIG_PREEMPT_RCU */</span>
<span class="p_del">-#ifdef CONFIG_PREEMPT_RCU</span>
 	struct rcu_node *rcu_blocked_node;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 #ifdef CONFIG_TASKS_RCU
<span class="p_chunk">@@ -1369,7 +1435,7 @@</span> <span class="p_context"> struct task_struct {</span>
 	int exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
<span class="p_del">-	unsigned int jobctl;	/* JOBCTL_*, siglock protected */</span>
<span class="p_add">+	unsigned long jobctl;	/* JOBCTL_*, siglock protected */</span>
 
 	/* Used for emulating ABI behavior of previous Linux versions */
 	unsigned int personality;
<span class="p_chunk">@@ -1511,6 +1577,8 @@</span> <span class="p_context"> struct task_struct {</span>
 	/* Protection of the PI data structures: */
 	raw_spinlock_t pi_lock;
 
<span class="p_add">+	struct wake_q_node wake_q;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */
 	struct rb_root pi_waiters;
<span class="p_chunk">@@ -1724,6 +1792,7 @@</span> <span class="p_context"> struct task_struct {</span>
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	unsigned long	task_state_change;
 #endif
<span class="p_add">+	int pagefault_disabled;</span>
 };
 
 /* Future-safe accessor for struct task_struct&#39;s cpus_allowed. */
<span class="p_chunk">@@ -2077,22 +2146,22 @@</span> <span class="p_context"> TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)</span>
 #define JOBCTL_TRAPPING_BIT	21	/* switching to TRACED */
 #define JOBCTL_LISTENING_BIT	22	/* ptracer is listening for events */
 
<span class="p_del">-#define JOBCTL_STOP_DEQUEUED	(1 &lt;&lt; JOBCTL_STOP_DEQUEUED_BIT)</span>
<span class="p_del">-#define JOBCTL_STOP_PENDING	(1 &lt;&lt; JOBCTL_STOP_PENDING_BIT)</span>
<span class="p_del">-#define JOBCTL_STOP_CONSUME	(1 &lt;&lt; JOBCTL_STOP_CONSUME_BIT)</span>
<span class="p_del">-#define JOBCTL_TRAP_STOP	(1 &lt;&lt; JOBCTL_TRAP_STOP_BIT)</span>
<span class="p_del">-#define JOBCTL_TRAP_NOTIFY	(1 &lt;&lt; JOBCTL_TRAP_NOTIFY_BIT)</span>
<span class="p_del">-#define JOBCTL_TRAPPING		(1 &lt;&lt; JOBCTL_TRAPPING_BIT)</span>
<span class="p_del">-#define JOBCTL_LISTENING	(1 &lt;&lt; JOBCTL_LISTENING_BIT)</span>
<span class="p_add">+#define JOBCTL_STOP_DEQUEUED	(1UL &lt;&lt; JOBCTL_STOP_DEQUEUED_BIT)</span>
<span class="p_add">+#define JOBCTL_STOP_PENDING	(1UL &lt;&lt; JOBCTL_STOP_PENDING_BIT)</span>
<span class="p_add">+#define JOBCTL_STOP_CONSUME	(1UL &lt;&lt; JOBCTL_STOP_CONSUME_BIT)</span>
<span class="p_add">+#define JOBCTL_TRAP_STOP	(1UL &lt;&lt; JOBCTL_TRAP_STOP_BIT)</span>
<span class="p_add">+#define JOBCTL_TRAP_NOTIFY	(1UL &lt;&lt; JOBCTL_TRAP_NOTIFY_BIT)</span>
<span class="p_add">+#define JOBCTL_TRAPPING		(1UL &lt;&lt; JOBCTL_TRAPPING_BIT)</span>
<span class="p_add">+#define JOBCTL_LISTENING	(1UL &lt;&lt; JOBCTL_LISTENING_BIT)</span>
 
 #define JOBCTL_TRAP_MASK	(JOBCTL_TRAP_STOP | JOBCTL_TRAP_NOTIFY)
 #define JOBCTL_PENDING_MASK	(JOBCTL_STOP_PENDING | JOBCTL_TRAP_MASK)
 
 extern bool task_set_jobctl_pending(struct task_struct *task,
<span class="p_del">-				    unsigned int mask);</span>
<span class="p_add">+				    unsigned long mask);</span>
 extern void task_clear_jobctl_trapping(struct task_struct *task);
 extern void task_clear_jobctl_pending(struct task_struct *task,
<span class="p_del">-				      unsigned int mask);</span>
<span class="p_add">+				      unsigned long mask);</span>
 
 static inline void rcu_copy_process(struct task_struct *p)
 {
<span class="p_chunk">@@ -2962,11 +3031,6 @@</span> <span class="p_context"> static __always_inline bool need_resched(void)</span>
 void thread_group_cputime(struct task_struct *tsk, struct task_cputime *times);
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times);
 
<span class="p_del">-static inline void thread_group_cputime_init(struct signal_struct *sig)</span>
<span class="p_del">-{</span>
<span class="p_del">-	raw_spin_lock_init(&amp;sig-&gt;cputimer.lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * Reevaluate whether the task has signals pending delivery.
  * Wake the task if so.
<span class="p_chunk">@@ -3080,13 +3144,13 @@</span> <span class="p_context"> static inline void mm_update_next_owner(struct mm_struct *mm)</span>
 static inline unsigned long task_rlimit(const struct task_struct *tsk,
 		unsigned int limit)
 {
<span class="p_del">-	return ACCESS_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_cur);</span>
<span class="p_add">+	return READ_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_cur);</span>
 }
 
 static inline unsigned long task_rlimit_max(const struct task_struct *tsk,
 		unsigned int limit)
 {
<span class="p_del">-	return ACCESS_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_max);</span>
<span class="p_add">+	return READ_ONCE(tsk-&gt;signal-&gt;rlim[limit].rlim_max);</span>
 }
 
 static inline unsigned long rlimit(unsigned int limit)
<span class="p_header">diff --git a/include/linux/topology.h b/include/linux/topology.h</span>
<span class="p_header">index 909b6e43b694..73ddad1e0fa3 100644</span>
<span class="p_header">--- a/include/linux/topology.h</span>
<span class="p_header">+++ b/include/linux/topology.h</span>
<span class="p_chunk">@@ -191,8 +191,8 @@</span> <span class="p_context"> static inline int cpu_to_mem(int cpu)</span>
 #ifndef topology_core_id
 #define topology_core_id(cpu)			((void)(cpu), 0)
 #endif
<span class="p_del">-#ifndef topology_thread_cpumask</span>
<span class="p_del">-#define topology_thread_cpumask(cpu)		cpumask_of(cpu)</span>
<span class="p_add">+#ifndef topology_sibling_cpumask</span>
<span class="p_add">+#define topology_sibling_cpumask(cpu)		cpumask_of(cpu)</span>
 #endif
 #ifndef topology_core_cpumask
 #define topology_core_cpumask(cpu)		cpumask_of(cpu)
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> static inline int cpu_to_mem(int cpu)</span>
 #ifdef CONFIG_SCHED_SMT
 static inline const struct cpumask *cpu_smt_mask(int cpu)
 {
<span class="p_del">-	return topology_thread_cpumask(cpu);</span>
<span class="p_add">+	return topology_sibling_cpumask(cpu);</span>
 }
 #endif
 
<span class="p_header">diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h</span>
<span class="p_header">index ecd3319dac33..ae572c138607 100644</span>
<span class="p_header">--- a/include/linux/uaccess.h</span>
<span class="p_header">+++ b/include/linux/uaccess.h</span>
<span class="p_chunk">@@ -1,21 +1,30 @@</span> <span class="p_context"></span>
 #ifndef __LINUX_UACCESS_H__
 #define __LINUX_UACCESS_H__
 
<span class="p_del">-#include &lt;linux/preempt.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;asm/uaccess.h&gt;
 
<span class="p_add">+static __always_inline void pagefault_disabled_inc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	current-&gt;pagefault_disabled++;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void pagefault_disabled_dec(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	current-&gt;pagefault_disabled--;</span>
<span class="p_add">+	WARN_ON(current-&gt;pagefault_disabled &lt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
<span class="p_del">- * These routines enable/disable the pagefault handler in that</span>
<span class="p_del">- * it will not take any locks and go straight to the fixup table.</span>
<span class="p_add">+ * These routines enable/disable the pagefault handler. If disabled, it will</span>
<span class="p_add">+ * not take any locks and go straight to the fixup table.</span>
  *
<span class="p_del">- * They have great resemblance to the preempt_disable/enable calls</span>
<span class="p_del">- * and in fact they are identical; this is because currently there is</span>
<span class="p_del">- * no other way to make the pagefault handlers do this. So we do</span>
<span class="p_del">- * disable preemption but we don&#39;t necessarily care about that.</span>
<span class="p_add">+ * User access methods will not sleep when called from a pagefault_disabled()</span>
<span class="p_add">+ * environment.</span>
  */
 static inline void pagefault_disable(void)
 {
<span class="p_del">-	preempt_count_inc();</span>
<span class="p_add">+	pagefault_disabled_inc();</span>
 	/*
 	 * make sure to have issued the store before a pagefault
 	 * can hit.
<span class="p_chunk">@@ -25,18 +34,31 @@</span> <span class="p_context"> static inline void pagefault_disable(void)</span>
 
 static inline void pagefault_enable(void)
 {
<span class="p_del">-#ifndef CONFIG_PREEMPT</span>
 	/*
 	 * make sure to issue those last loads/stores before enabling
 	 * the pagefault handler again.
 	 */
 	barrier();
<span class="p_del">-	preempt_count_dec();</span>
<span class="p_del">-#else</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	pagefault_disabled_dec();</span>
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Is the pagefault handler disabled? If so, user access methods will not sleep.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define pagefault_disabled() (current-&gt;pagefault_disabled != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The pagefault handler is in general disabled by pagefault_disable() or</span>
<span class="p_add">+ * when in irq context (via in_atomic()).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function should only be used by the fault handlers. Other users should</span>
<span class="p_add">+ * stick to pagefault_disabled().</span>
<span class="p_add">+ * Please NEVER use preempt_disable() to disable the fault handler. With</span>
<span class="p_add">+ * !CONFIG_PREEMPT_COUNT, this is like a NOP. So the handler won&#39;t be disabled.</span>
<span class="p_add">+ * in_atomic() will report different values based on !CONFIG_PREEMPT_COUNT.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define faulthandler_disabled() (pagefault_disabled() || in_atomic())</span>
<span class="p_add">+</span>
 #ifndef ARCH_HAS_NOCACHE_UACCESS
 
 static inline unsigned long __copy_from_user_inatomic_nocache(void *to,
<span class="p_header">diff --git a/include/linux/wait.h b/include/linux/wait.h</span>
<span class="p_header">index 2db83349865b..d69ac4ecc88b 100644</span>
<span class="p_header">--- a/include/linux/wait.h</span>
<span class="p_header">+++ b/include/linux/wait.h</span>
<span class="p_chunk">@@ -969,7 +969,7 @@</span> <span class="p_context"> extern int bit_wait_io_timeout(struct wait_bit_key *);</span>
  * on that signal.
  */
 static inline int
<span class="p_del">-wait_on_bit(void *word, int bit, unsigned mode)</span>
<span class="p_add">+wait_on_bit(unsigned long *word, int bit, unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_bit(bit, word))
<span class="p_chunk">@@ -994,7 +994,7 @@</span> <span class="p_context"> wait_on_bit(void *word, int bit, unsigned mode)</span>
  * on that signal.
  */
 static inline int
<span class="p_del">-wait_on_bit_io(void *word, int bit, unsigned mode)</span>
<span class="p_add">+wait_on_bit_io(unsigned long *word, int bit, unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_bit(bit, word))
<span class="p_chunk">@@ -1020,7 +1020,8 @@</span> <span class="p_context"> wait_on_bit_io(void *word, int bit, unsigned mode)</span>
  * received a signal and the mode permitted wakeup on that signal.
  */
 static inline int
<span class="p_del">-wait_on_bit_timeout(void *word, int bit, unsigned mode, unsigned long timeout)</span>
<span class="p_add">+wait_on_bit_timeout(unsigned long *word, int bit, unsigned mode,</span>
<span class="p_add">+		    unsigned long timeout)</span>
 {
 	might_sleep();
 	if (!test_bit(bit, word))
<span class="p_chunk">@@ -1047,7 +1048,8 @@</span> <span class="p_context"> wait_on_bit_timeout(void *word, int bit, unsigned mode, unsigned long timeout)</span>
  * on that signal.
  */
 static inline int
<span class="p_del">-wait_on_bit_action(void *word, int bit, wait_bit_action_f *action, unsigned mode)</span>
<span class="p_add">+wait_on_bit_action(unsigned long *word, int bit, wait_bit_action_f *action,</span>
<span class="p_add">+		   unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_bit(bit, word))
<span class="p_chunk">@@ -1075,7 +1077,7 @@</span> <span class="p_context"> wait_on_bit_action(void *word, int bit, wait_bit_action_f *action, unsigned mode</span>
  * the @mode allows that signal to wake the process.
  */
 static inline int
<span class="p_del">-wait_on_bit_lock(void *word, int bit, unsigned mode)</span>
<span class="p_add">+wait_on_bit_lock(unsigned long *word, int bit, unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_and_set_bit(bit, word))
<span class="p_chunk">@@ -1099,7 +1101,7 @@</span> <span class="p_context"> wait_on_bit_lock(void *word, int bit, unsigned mode)</span>
  * the @mode allows that signal to wake the process.
  */
 static inline int
<span class="p_del">-wait_on_bit_lock_io(void *word, int bit, unsigned mode)</span>
<span class="p_add">+wait_on_bit_lock_io(unsigned long *word, int bit, unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_and_set_bit(bit, word))
<span class="p_chunk">@@ -1125,7 +1127,8 @@</span> <span class="p_context"> wait_on_bit_lock_io(void *word, int bit, unsigned mode)</span>
  * the @mode allows that signal to wake the process.
  */
 static inline int
<span class="p_del">-wait_on_bit_lock_action(void *word, int bit, wait_bit_action_f *action, unsigned mode)</span>
<span class="p_add">+wait_on_bit_lock_action(unsigned long *word, int bit, wait_bit_action_f *action,</span>
<span class="p_add">+			unsigned mode)</span>
 {
 	might_sleep();
 	if (!test_and_set_bit(bit, word))
<span class="p_header">diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h</span>
<span class="p_header">index 30fedaf3e56a..d57a575fe31f 100644</span>
<span class="p_header">--- a/include/trace/events/sched.h</span>
<span class="p_header">+++ b/include/trace/events/sched.h</span>
<span class="p_chunk">@@ -147,7 +147,8 @@</span> <span class="p_context"> TRACE_EVENT(sched_switch,</span>
 		  __print_flags(__entry-&gt;prev_state &amp; (TASK_STATE_MAX-1), &quot;|&quot;,
 				{ 1, &quot;S&quot;} , { 2, &quot;D&quot; }, { 4, &quot;T&quot; }, { 8, &quot;t&quot; },
 				{ 16, &quot;Z&quot; }, { 32, &quot;X&quot; }, { 64, &quot;x&quot; },
<span class="p_del">-				{ 128, &quot;K&quot; }, { 256, &quot;W&quot; }, { 512, &quot;P&quot; }) : &quot;R&quot;,</span>
<span class="p_add">+				{ 128, &quot;K&quot; }, { 256, &quot;W&quot; }, { 512, &quot;P&quot; },</span>
<span class="p_add">+				{ 1024, &quot;N&quot; }) : &quot;R&quot;,</span>
 		__entry-&gt;prev_state &amp; TASK_STATE_MAX ? &quot;+&quot; : &quot;&quot;,
 		__entry-&gt;next_comm, __entry-&gt;next_pid, __entry-&gt;next_prio)
 );
<span class="p_header">diff --git a/ipc/mqueue.c b/ipc/mqueue.c</span>
<span class="p_header">index 3aaea7ffd077..a24ba9fe5bb8 100644</span>
<span class="p_header">--- a/ipc/mqueue.c</span>
<span class="p_header">+++ b/ipc/mqueue.c</span>
<span class="p_chunk">@@ -47,8 +47,7 @@</span> <span class="p_context"></span>
 #define RECV		1
 
 #define STATE_NONE	0
<span class="p_del">-#define STATE_PENDING	1</span>
<span class="p_del">-#define STATE_READY	2</span>
<span class="p_add">+#define STATE_READY	1</span>
 
 struct posix_msg_tree_node {
 	struct rb_node		rb_node;
<span class="p_chunk">@@ -571,15 +570,12 @@</span> <span class="p_context"> static int wq_sleep(struct mqueue_inode_info *info, int sr,</span>
 	wq_add(info, sr, ewp);
 
 	for (;;) {
<span class="p_del">-		set_current_state(TASK_INTERRUPTIBLE);</span>
<span class="p_add">+		__set_current_state(TASK_INTERRUPTIBLE);</span>
 
 		spin_unlock(&amp;info-&gt;lock);
 		time = schedule_hrtimeout_range_clock(timeout, 0,
 			HRTIMER_MODE_ABS, CLOCK_REALTIME);
 
<span class="p_del">-		while (ewp-&gt;state == STATE_PENDING)</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-</span>
 		if (ewp-&gt;state == STATE_READY) {
 			retval = 0;
 			goto out;
<span class="p_chunk">@@ -907,11 +903,15 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mq_unlink, const char __user *, u_name)</span>
  * list of waiting receivers. A sender checks that list before adding the new
  * message into the message array. If there is a waiting receiver, then it
  * bypasses the message array and directly hands the message over to the
<span class="p_del">- * receiver.</span>
<span class="p_del">- * The receiver accepts the message and returns without grabbing the queue</span>
<span class="p_del">- * spinlock. Therefore an intermediate STATE_PENDING state and memory barriers</span>
<span class="p_del">- * are necessary. The same algorithm is used for sysv semaphores, see</span>
<span class="p_del">- * ipc/sem.c for more details.</span>
<span class="p_add">+ * receiver. The receiver accepts the message and returns without grabbing the</span>
<span class="p_add">+ * queue spinlock:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * - Set pointer to message.</span>
<span class="p_add">+ * - Queue the receiver task for later wakeup (without the info-&gt;lock).</span>
<span class="p_add">+ * - Update its state to STATE_READY. Now the receiver can continue.</span>
<span class="p_add">+ * - Wake up the process after the lock is dropped. Should the process wake up</span>
<span class="p_add">+ *   before this wakeup (due to a timeout or a signal) it will either see</span>
<span class="p_add">+ *   STATE_READY and continue or acquire the lock to check the state again.</span>
  *
  * The same algorithm is used for senders.
  */
<span class="p_chunk">@@ -919,21 +919,29 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mq_unlink, const char __user *, u_name)</span>
 /* pipelined_send() - send a message directly to the task waiting in
  * sys_mq_timedreceive() (without inserting message into a queue).
  */
<span class="p_del">-static inline void pipelined_send(struct mqueue_inode_info *info,</span>
<span class="p_add">+static inline void pipelined_send(struct wake_q_head *wake_q,</span>
<span class="p_add">+				  struct mqueue_inode_info *info,</span>
 				  struct msg_msg *message,
 				  struct ext_wait_queue *receiver)
 {
 	receiver-&gt;msg = message;
 	list_del(&amp;receiver-&gt;list);
<span class="p_del">-	receiver-&gt;state = STATE_PENDING;</span>
<span class="p_del">-	wake_up_process(receiver-&gt;task);</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_add">+	wake_q_add(wake_q, receiver-&gt;task);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Rely on the implicit cmpxchg barrier from wake_q_add such</span>
<span class="p_add">+	 * that we can ensure that updating receiver-&gt;state is the last</span>
<span class="p_add">+	 * write operation: As once set, the receiver can continue,</span>
<span class="p_add">+	 * and if we don&#39;t have the reference count from the wake_q,</span>
<span class="p_add">+	 * yet, at that point we can later have a use-after-free</span>
<span class="p_add">+	 * condition and bogus wakeup.</span>
<span class="p_add">+	 */</span>
 	receiver-&gt;state = STATE_READY;
 }
 
 /* pipelined_receive() - if there is task waiting in sys_mq_timedsend()
  * gets its message and put to the queue (we have one free place for sure). */
<span class="p_del">-static inline void pipelined_receive(struct mqueue_inode_info *info)</span>
<span class="p_add">+static inline void pipelined_receive(struct wake_q_head *wake_q,</span>
<span class="p_add">+				     struct mqueue_inode_info *info)</span>
 {
 	struct ext_wait_queue *sender = wq_get_first_waiter(info, SEND);
 
<span class="p_chunk">@@ -944,10 +952,9 @@</span> <span class="p_context"> static inline void pipelined_receive(struct mqueue_inode_info *info)</span>
 	}
 	if (msg_insert(sender-&gt;msg, info))
 		return;
<span class="p_add">+</span>
 	list_del(&amp;sender-&gt;list);
<span class="p_del">-	sender-&gt;state = STATE_PENDING;</span>
<span class="p_del">-	wake_up_process(sender-&gt;task);</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_add">+	wake_q_add(wake_q, sender-&gt;task);</span>
 	sender-&gt;state = STATE_READY;
 }
 
<span class="p_chunk">@@ -965,6 +972,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,</span>
 	struct timespec ts;
 	struct posix_msg_tree_node *new_leaf = NULL;
 	int ret = 0;
<span class="p_add">+	WAKE_Q(wake_q);</span>
 
 	if (u_abs_timeout) {
 		int res = prepare_timeout(u_abs_timeout, &amp;expires, &amp;ts);
<span class="p_chunk">@@ -1049,7 +1057,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,</span>
 	} else {
 		receiver = wq_get_first_waiter(info, RECV);
 		if (receiver) {
<span class="p_del">-			pipelined_send(info, msg_ptr, receiver);</span>
<span class="p_add">+			pipelined_send(&amp;wake_q, info, msg_ptr, receiver);</span>
 		} else {
 			/* adds message to the queue */
 			ret = msg_insert(msg_ptr, info);
<span class="p_chunk">@@ -1062,6 +1070,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,</span>
 	}
 out_unlock:
 	spin_unlock(&amp;info-&gt;lock);
<span class="p_add">+	wake_up_q(&amp;wake_q);</span>
 out_free:
 	if (ret)
 		free_msg(msg_ptr);
<span class="p_chunk">@@ -1149,14 +1158,17 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes, char __user *, u_msg_ptr,</span>
 			msg_ptr = wait.msg;
 		}
 	} else {
<span class="p_add">+		WAKE_Q(wake_q);</span>
<span class="p_add">+</span>
 		msg_ptr = msg_get(info);
 
 		inode-&gt;i_atime = inode-&gt;i_mtime = inode-&gt;i_ctime =
 				CURRENT_TIME;
 
 		/* There is now free space in queue. */
<span class="p_del">-		pipelined_receive(info);</span>
<span class="p_add">+		pipelined_receive(&amp;wake_q, info);</span>
 		spin_unlock(&amp;info-&gt;lock);
<span class="p_add">+		wake_up_q(&amp;wake_q);</span>
 		ret = 0;
 	}
 	if (ret == 0) {
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 03c1eaaa6ef5..0bb88b555550 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -1091,10 +1091,7 @@</span> <span class="p_context"> static void posix_cpu_timers_init_group(struct signal_struct *sig)</span>
 {
 	unsigned long cpu_limit;
 
<span class="p_del">-	/* Thread group counters. */</span>
<span class="p_del">-	thread_group_cputime_init(sig);</span>
<span class="p_del">-</span>
<span class="p_del">-	cpu_limit = ACCESS_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_cur);</span>
<span class="p_add">+	cpu_limit = READ_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_cur);</span>
 	if (cpu_limit != RLIM_INFINITY) {
 		sig-&gt;cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
 		sig-&gt;cputimer.running = 1;
<span class="p_chunk">@@ -1396,6 +1393,9 @@</span> <span class="p_context"> static struct task_struct *copy_process(unsigned long clone_flags,</span>
 	p-&gt;hardirq_context = 0;
 	p-&gt;softirq_context = 0;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	p-&gt;pagefault_disabled = 0;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_LOCKDEP
 	p-&gt;lockdep_depth = 0; /* no locks held yet */
 	p-&gt;curr_chain_key = 0;
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 2579e407ff67..f9984c363e9a 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -1090,9 +1090,11 @@</span> <span class="p_context"> static void __unqueue_futex(struct futex_q *q)</span>
 
 /*
  * The hash bucket lock must be held when this is called.
<span class="p_del">- * Afterwards, the futex_q must not be accessed.</span>
<span class="p_add">+ * Afterwards, the futex_q must not be accessed. Callers</span>
<span class="p_add">+ * must ensure to later call wake_up_q() for the actual</span>
<span class="p_add">+ * wakeups to occur.</span>
  */
<span class="p_del">-static void wake_futex(struct futex_q *q)</span>
<span class="p_add">+static void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)</span>
 {
 	struct task_struct *p = q-&gt;task;
 
<span class="p_chunk">@@ -1100,14 +1102,10 @@</span> <span class="p_context"> static void wake_futex(struct futex_q *q)</span>
 		return;
 
 	/*
<span class="p_del">-	 * We set q-&gt;lock_ptr = NULL _before_ we wake up the task. If</span>
<span class="p_del">-	 * a non-futex wake up happens on another CPU then the task</span>
<span class="p_del">-	 * might exit and p would dereference a non-existing task</span>
<span class="p_del">-	 * struct. Prevent this by holding a reference on p across the</span>
<span class="p_del">-	 * wake up.</span>
<span class="p_add">+	 * Queue the task for later wakeup for after we&#39;ve released</span>
<span class="p_add">+	 * the hb-&gt;lock. wake_q_add() grabs reference to p.</span>
 	 */
<span class="p_del">-	get_task_struct(p);</span>
<span class="p_del">-</span>
<span class="p_add">+	wake_q_add(wake_q, p);</span>
 	__unqueue_futex(q);
 	/*
 	 * The waiting task can free the futex_q as soon as
<span class="p_chunk">@@ -1117,9 +1115,6 @@</span> <span class="p_context"> static void wake_futex(struct futex_q *q)</span>
 	 */
 	smp_wmb();
 	q-&gt;lock_ptr = NULL;
<span class="p_del">-</span>
<span class="p_del">-	wake_up_state(p, TASK_NORMAL);</span>
<span class="p_del">-	put_task_struct(p);</span>
 }
 
 static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this)
<span class="p_chunk">@@ -1217,6 +1212,7 @@</span> <span class="p_context"> futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)</span>
 	struct futex_q *this, *next;
 	union futex_key key = FUTEX_KEY_INIT;
 	int ret;
<span class="p_add">+	WAKE_Q(wake_q);</span>
 
 	if (!bitset)
 		return -EINVAL;
<span class="p_chunk">@@ -1244,13 +1240,14 @@</span> <span class="p_context"> futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)</span>
 			if (!(this-&gt;bitset &amp; bitset))
 				continue;
 
<span class="p_del">-			wake_futex(this);</span>
<span class="p_add">+			mark_wake_futex(&amp;wake_q, this);</span>
 			if (++ret &gt;= nr_wake)
 				break;
 		}
 	}
 
 	spin_unlock(&amp;hb-&gt;lock);
<span class="p_add">+	wake_up_q(&amp;wake_q);</span>
 out_put_key:
 	put_futex_key(&amp;key);
 out:
<span class="p_chunk">@@ -1269,6 +1266,7 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 	struct futex_hash_bucket *hb1, *hb2;
 	struct futex_q *this, *next;
 	int ret, op_ret;
<span class="p_add">+	WAKE_Q(wake_q);</span>
 
 retry:
 	ret = get_futex_key(uaddr1, flags &amp; FLAGS_SHARED, &amp;key1, VERIFY_READ);
<span class="p_chunk">@@ -1320,7 +1318,7 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 				ret = -EINVAL;
 				goto out_unlock;
 			}
<span class="p_del">-			wake_futex(this);</span>
<span class="p_add">+			mark_wake_futex(&amp;wake_q, this);</span>
 			if (++ret &gt;= nr_wake)
 				break;
 		}
<span class="p_chunk">@@ -1334,7 +1332,7 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 					ret = -EINVAL;
 					goto out_unlock;
 				}
<span class="p_del">-				wake_futex(this);</span>
<span class="p_add">+				mark_wake_futex(&amp;wake_q, this);</span>
 				if (++op_ret &gt;= nr_wake2)
 					break;
 			}
<span class="p_chunk">@@ -1344,6 +1342,7 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 
 out_unlock:
 	double_unlock_hb(hb1, hb2);
<span class="p_add">+	wake_up_q(&amp;wake_q);</span>
 out_put_keys:
 	put_futex_key(&amp;key2);
 out_put_key1:
<span class="p_chunk">@@ -1503,6 +1502,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 	struct futex_pi_state *pi_state = NULL;
 	struct futex_hash_bucket *hb1, *hb2;
 	struct futex_q *this, *next;
<span class="p_add">+	WAKE_Q(wake_q);</span>
 
 	if (requeue_pi) {
 		/*
<span class="p_chunk">@@ -1679,7 +1679,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 		 * woken by futex_unlock_pi().
 		 */
 		if (++task_count &lt;= nr_wake &amp;&amp; !requeue_pi) {
<span class="p_del">-			wake_futex(this);</span>
<span class="p_add">+			mark_wake_futex(&amp;wake_q, this);</span>
 			continue;
 		}
 
<span class="p_chunk">@@ -1719,6 +1719,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 out_unlock:
 	free_pi_state(pi_state);
 	double_unlock_hb(hb1, hb2);
<span class="p_add">+	wake_up_q(&amp;wake_q);</span>
 	hb_waiters_dec(hb2);
 
 	/*
<span class="p_header">diff --git a/kernel/locking/lglock.c b/kernel/locking/lglock.c</span>
<span class="p_header">index 86ae2aebf004..951cfcd10b4a 100644</span>
<span class="p_header">--- a/kernel/locking/lglock.c</span>
<span class="p_header">+++ b/kernel/locking/lglock.c</span>
<span class="p_chunk">@@ -60,6 +60,28 @@</span> <span class="p_context"> void lg_local_unlock_cpu(struct lglock *lg, int cpu)</span>
 }
 EXPORT_SYMBOL(lg_local_unlock_cpu);
 
<span class="p_add">+void lg_double_lock(struct lglock *lg, int cpu1, int cpu2)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(cpu1 == cpu2);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* lock in cpu order, just like lg_global_lock */</span>
<span class="p_add">+	if (cpu2 &lt; cpu1)</span>
<span class="p_add">+		swap(cpu1, cpu2);</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	lock_acquire_shared(&amp;lg-&gt;lock_dep_map, 0, 0, NULL, _RET_IP_);</span>
<span class="p_add">+	arch_spin_lock(per_cpu_ptr(lg-&gt;lock, cpu1));</span>
<span class="p_add">+	arch_spin_lock(per_cpu_ptr(lg-&gt;lock, cpu2));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void lg_double_unlock(struct lglock *lg, int cpu1, int cpu2)</span>
<span class="p_add">+{</span>
<span class="p_add">+	lock_release(&amp;lg-&gt;lock_dep_map, 1, _RET_IP_);</span>
<span class="p_add">+	arch_spin_unlock(per_cpu_ptr(lg-&gt;lock, cpu1));</span>
<span class="p_add">+	arch_spin_unlock(per_cpu_ptr(lg-&gt;lock, cpu2));</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void lg_global_lock(struct lglock *lg)
 {
 	int i;
<span class="p_header">diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile</span>
<span class="p_header">index 46be87024875..67687973ce80 100644</span>
<span class="p_header">--- a/kernel/sched/Makefile</span>
<span class="p_header">+++ b/kernel/sched/Makefile</span>
<span class="p_chunk">@@ -11,7 +11,7 @@</span> <span class="p_context"> ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)</span>
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
<span class="p_del">-obj-y += core.o proc.o clock.o cputime.o</span>
<span class="p_add">+obj-y += core.o loadavg.o clock.o cputime.o</span>
 obj-y += idle_task.o fair.o rt.o deadline.o stop_task.o
 obj-y += wait.o completion.o idle.o
 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o
<span class="p_header">diff --git a/kernel/sched/auto_group.c b/kernel/sched/auto_group.c</span>
<span class="p_header">index eae160dd669d..750ed601ddf7 100644</span>
<span class="p_header">--- a/kernel/sched/auto_group.c</span>
<span class="p_header">+++ b/kernel/sched/auto_group.c</span>
<span class="p_chunk">@@ -1,5 +1,3 @@</span> <span class="p_context"></span>
<span class="p_del">-#ifdef CONFIG_SCHED_AUTOGROUP</span>
<span class="p_del">-</span>
 #include &quot;sched.h&quot;
 
 #include &lt;linux/proc_fs.h&gt;
<span class="p_chunk">@@ -141,7 +139,7 @@</span> <span class="p_context"> autogroup_move_group(struct task_struct *p, struct autogroup *ag)</span>
 
 	p-&gt;signal-&gt;autogroup = autogroup_kref_get(ag);
 
<span class="p_del">-	if (!ACCESS_ONCE(sysctl_sched_autogroup_enabled))</span>
<span class="p_add">+	if (!READ_ONCE(sysctl_sched_autogroup_enabled))</span>
 		goto out;
 
 	for_each_thread(p, t)
<span class="p_chunk">@@ -249,5 +247,3 @@</span> <span class="p_context"> int autogroup_path(struct task_group *tg, char *buf, int buflen)</span>
 	return snprintf(buf, buflen, &quot;%s-%ld&quot;, &quot;/autogroup&quot;, tg-&gt;autogroup-&gt;id);
 }
 #endif /* CONFIG_SCHED_DEBUG */
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_SCHED_AUTOGROUP */</span>
<span class="p_header">diff --git a/kernel/sched/auto_group.h b/kernel/sched/auto_group.h</span>
<span class="p_header">index 8bd047142816..890c95f2587a 100644</span>
<span class="p_header">--- a/kernel/sched/auto_group.h</span>
<span class="p_header">+++ b/kernel/sched/auto_group.h</span>
<span class="p_chunk">@@ -29,7 +29,7 @@</span> <span class="p_context"> extern bool task_wants_autogroup(struct task_struct *p, struct task_group *tg);</span>
 static inline struct task_group *
 autogroup_task_group(struct task_struct *p, struct task_group *tg)
 {
<span class="p_del">-	int enabled = ACCESS_ONCE(sysctl_sched_autogroup_enabled);</span>
<span class="p_add">+	int enabled = READ_ONCE(sysctl_sched_autogroup_enabled);</span>
 
 	if (enabled &amp;&amp; task_wants_autogroup(p, tg))
 		return p-&gt;signal-&gt;autogroup-&gt;tg;
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 123673291ffb..10338ce78be4 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -511,7 +511,7 @@</span> <span class="p_context"> static bool set_nr_and_not_polling(struct task_struct *p)</span>
 static bool set_nr_if_polling(struct task_struct *p)
 {
 	struct thread_info *ti = task_thread_info(p);
<span class="p_del">-	typeof(ti-&gt;flags) old, val = ACCESS_ONCE(ti-&gt;flags);</span>
<span class="p_add">+	typeof(ti-&gt;flags) old, val = READ_ONCE(ti-&gt;flags);</span>
 
 	for (;;) {
 		if (!(val &amp; _TIF_POLLING_NRFLAG))
<span class="p_chunk">@@ -541,6 +541,52 @@</span> <span class="p_context"> static bool set_nr_if_polling(struct task_struct *p)</span>
 #endif
 #endif
 
<span class="p_add">+void wake_q_add(struct wake_q_head *head, struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct wake_q_node *node = &amp;task-&gt;wake_q;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Atomically grab the task, if -&gt;wake_q is !nil already it means</span>
<span class="p_add">+	 * its already queued (either by us or someone else) and will get the</span>
<span class="p_add">+	 * wakeup due to that.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This cmpxchg() implies a full barrier, which pairs with the write</span>
<span class="p_add">+	 * barrier implied by the wakeup in wake_up_list().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (cmpxchg(&amp;node-&gt;next, NULL, WAKE_Q_TAIL))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	get_task_struct(task);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The head is context local, there can be no concurrency.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	*head-&gt;lastp = node;</span>
<span class="p_add">+	head-&gt;lastp = &amp;node-&gt;next;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void wake_up_q(struct wake_q_head *head)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct wake_q_node *node = head-&gt;first;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (node != WAKE_Q_TAIL) {</span>
<span class="p_add">+		struct task_struct *task;</span>
<span class="p_add">+</span>
<span class="p_add">+		task = container_of(node, struct task_struct, wake_q);</span>
<span class="p_add">+		BUG_ON(!task);</span>
<span class="p_add">+		/* task can safely be re-inserted now */</span>
<span class="p_add">+		node = node-&gt;next;</span>
<span class="p_add">+		task-&gt;wake_q.next = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * wake_up_process() implies a wmb() to pair with the queueing</span>
<span class="p_add">+		 * in wake_q_add() so as not to miss wakeups.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		wake_up_process(task);</span>
<span class="p_add">+		put_task_struct(task);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * resched_curr - mark rq&#39;s current task &#39;to be rescheduled now&#39;.
  *
<span class="p_chunk">@@ -2105,12 +2151,15 @@</span> <span class="p_context"> void wake_up_new_task(struct task_struct *p)</span>
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
<span class="p_add">+static struct static_key preempt_notifier_key = STATIC_KEY_INIT_FALSE;</span>
<span class="p_add">+</span>
 /**
  * preempt_notifier_register - tell me when current is being preempted &amp; rescheduled
  * @notifier: notifier struct to register
  */
 void preempt_notifier_register(struct preempt_notifier *notifier)
 {
<span class="p_add">+	static_key_slow_inc(&amp;preempt_notifier_key);</span>
 	hlist_add_head(&amp;notifier-&gt;link, &amp;current-&gt;preempt_notifiers);
 }
 EXPORT_SYMBOL_GPL(preempt_notifier_register);
<span class="p_chunk">@@ -2119,15 +2168,16 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(preempt_notifier_register);</span>
  * preempt_notifier_unregister - no longer interested in preemption notifications
  * @notifier: notifier struct to unregister
  *
<span class="p_del">- * This is safe to call from within a preemption notifier.</span>
<span class="p_add">+ * This is *not* safe to call from within a preemption notifier.</span>
  */
 void preempt_notifier_unregister(struct preempt_notifier *notifier)
 {
 	hlist_del(&amp;notifier-&gt;link);
<span class="p_add">+	static_key_slow_dec(&amp;preempt_notifier_key);</span>
 }
 EXPORT_SYMBOL_GPL(preempt_notifier_unregister);
 
<span class="p_del">-static void fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
<span class="p_add">+static void __fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
 {
 	struct preempt_notifier *notifier;
 
<span class="p_chunk">@@ -2135,9 +2185,15 @@</span> <span class="p_context"> static void fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
 		notifier-&gt;ops-&gt;sched_in(notifier, raw_smp_processor_id());
 }
 
<span class="p_add">+static __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (static_key_false(&amp;preempt_notifier_key))</span>
<span class="p_add">+		__fire_sched_in_preempt_notifiers(curr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void
<span class="p_del">-fire_sched_out_preempt_notifiers(struct task_struct *curr,</span>
<span class="p_del">-				 struct task_struct *next)</span>
<span class="p_add">+__fire_sched_out_preempt_notifiers(struct task_struct *curr,</span>
<span class="p_add">+				   struct task_struct *next)</span>
 {
 	struct preempt_notifier *notifier;
 
<span class="p_chunk">@@ -2145,13 +2201,21 @@</span> <span class="p_context"> fire_sched_out_preempt_notifiers(struct task_struct *curr,</span>
 		notifier-&gt;ops-&gt;sched_out(notifier, next);
 }
 
<span class="p_add">+static __always_inline void</span>
<span class="p_add">+fire_sched_out_preempt_notifiers(struct task_struct *curr,</span>
<span class="p_add">+				 struct task_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (static_key_false(&amp;preempt_notifier_key))</span>
<span class="p_add">+		__fire_sched_out_preempt_notifiers(curr, next);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #else /* !CONFIG_PREEMPT_NOTIFIERS */
 
<span class="p_del">-static void fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
<span class="p_add">+static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr)</span>
 {
 }
 
<span class="p_del">-static void</span>
<span class="p_add">+static inline void</span>
 fire_sched_out_preempt_notifiers(struct task_struct *curr,
 				 struct task_struct *next)
 {
<span class="p_chunk">@@ -2397,9 +2461,9 @@</span> <span class="p_context"> unsigned long nr_iowait_cpu(int cpu)</span>
 
 void get_iowait_load(unsigned long *nr_waiters, unsigned long *load)
 {
<span class="p_del">-	struct rq *this = this_rq();</span>
<span class="p_del">-	*nr_waiters = atomic_read(&amp;this-&gt;nr_iowait);</span>
<span class="p_del">-	*load = this-&gt;cpu_load[0];</span>
<span class="p_add">+	struct rq *rq = this_rq();</span>
<span class="p_add">+	*nr_waiters = atomic_read(&amp;rq-&gt;nr_iowait);</span>
<span class="p_add">+	*load = rq-&gt;load.weight;</span>
 }
 
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -2497,6 +2561,7 @@</span> <span class="p_context"> void scheduler_tick(void)</span>
 	update_rq_clock(rq);
 	curr-&gt;sched_class-&gt;task_tick(rq, curr, 0);
 	update_cpu_load_active(rq);
<span class="p_add">+	calc_global_load_tick(rq);</span>
 	raw_spin_unlock(&amp;rq-&gt;lock);
 
 	perf_event_task_tick();
<span class="p_chunk">@@ -2525,7 +2590,7 @@</span> <span class="p_context"> void scheduler_tick(void)</span>
 u64 scheduler_tick_max_deferment(void)
 {
 	struct rq *rq = this_rq();
<span class="p_del">-	unsigned long next, now = ACCESS_ONCE(jiffies);</span>
<span class="p_add">+	unsigned long next, now = READ_ONCE(jiffies);</span>
 
 	next = rq-&gt;last_sched_tick + HZ;
 
<span class="p_chunk">@@ -2726,9 +2791,7 @@</span> <span class="p_context"> pick_next_task(struct rq *rq, struct task_struct *prev)</span>
  *          - return from syscall or exception to user-space
  *          - return from interrupt-handler to user-space
  *
<span class="p_del">- * WARNING: all callers must re-check need_resched() afterward and reschedule</span>
<span class="p_del">- * accordingly in case an event triggered the need for rescheduling (such as</span>
<span class="p_del">- * an interrupt waking up a task) while preemption was disabled in __schedule().</span>
<span class="p_add">+ * WARNING: must be called with preemption disabled!</span>
  */
 static void __sched __schedule(void)
 {
<span class="p_chunk">@@ -2737,7 +2800,6 @@</span> <span class="p_context"> static void __sched __schedule(void)</span>
 	struct rq *rq;
 	int cpu;
 
<span class="p_del">-	preempt_disable();</span>
 	cpu = smp_processor_id();
 	rq = cpu_rq(cpu);
 	rcu_note_context_switch();
<span class="p_chunk">@@ -2801,8 +2863,6 @@</span> <span class="p_context"> static void __sched __schedule(void)</span>
 		raw_spin_unlock_irq(&amp;rq-&gt;lock);
 
 	post_schedule(rq);
<span class="p_del">-</span>
<span class="p_del">-	sched_preempt_enable_no_resched();</span>
 }
 
 static inline void sched_submit_work(struct task_struct *tsk)
<span class="p_chunk">@@ -2823,7 +2883,9 @@</span> <span class="p_context"> asmlinkage __visible void __sched schedule(void)</span>
 
 	sched_submit_work(tsk);
 	do {
<span class="p_add">+		preempt_disable();</span>
 		__schedule();
<span class="p_add">+		sched_preempt_enable_no_resched();</span>
 	} while (need_resched());
 }
 EXPORT_SYMBOL(schedule);
<span class="p_chunk">@@ -2862,15 +2924,14 @@</span> <span class="p_context"> void __sched schedule_preempt_disabled(void)</span>
 static void __sched notrace preempt_schedule_common(void)
 {
 	do {
<span class="p_del">-		__preempt_count_add(PREEMPT_ACTIVE);</span>
<span class="p_add">+		preempt_active_enter();</span>
 		__schedule();
<span class="p_del">-		__preempt_count_sub(PREEMPT_ACTIVE);</span>
<span class="p_add">+		preempt_active_exit();</span>
 
 		/*
 		 * Check again in case we missed a preemption opportunity
 		 * between schedule and now.
 		 */
<span class="p_del">-		barrier();</span>
 	} while (need_resched());
 }
 
<span class="p_chunk">@@ -2894,9 +2955,8 @@</span> <span class="p_context"> asmlinkage __visible void __sched notrace preempt_schedule(void)</span>
 NOKPROBE_SYMBOL(preempt_schedule);
 EXPORT_SYMBOL(preempt_schedule);
 
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
 /**
<span class="p_del">- * preempt_schedule_context - preempt_schedule called by tracing</span>
<span class="p_add">+ * preempt_schedule_notrace - preempt_schedule called by tracing</span>
  *
  * The tracing infrastructure uses preempt_enable_notrace to prevent
  * recursion and tracing preempt enabling caused by the tracing
<span class="p_chunk">@@ -2909,7 +2969,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(preempt_schedule);</span>
  * instead of preempt_schedule() to exit user context if needed before
  * calling the scheduler.
  */
<span class="p_del">-asmlinkage __visible void __sched notrace preempt_schedule_context(void)</span>
<span class="p_add">+asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)</span>
 {
 	enum ctx_state prev_ctx;
 
<span class="p_chunk">@@ -2917,7 +2977,13 @@</span> <span class="p_context"> asmlinkage __visible void __sched notrace preempt_schedule_context(void)</span>
 		return;
 
 	do {
<span class="p_del">-		__preempt_count_add(PREEMPT_ACTIVE);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Use raw __prempt_count() ops that don&#39;t call function.</span>
<span class="p_add">+		 * We can&#39;t call functions before disabling preemption which</span>
<span class="p_add">+		 * disarm preemption tracing recursions.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		__preempt_count_add(PREEMPT_ACTIVE + PREEMPT_DISABLE_OFFSET);</span>
<span class="p_add">+		barrier();</span>
 		/*
 		 * Needs preempt disabled in case user_exit() is traced
 		 * and the tracer calls preempt_enable_notrace() causing
<span class="p_chunk">@@ -2927,12 +2993,11 @@</span> <span class="p_context"> asmlinkage __visible void __sched notrace preempt_schedule_context(void)</span>
 		__schedule();
 		exception_exit(prev_ctx);
 
<span class="p_del">-		__preempt_count_sub(PREEMPT_ACTIVE);</span>
 		barrier();
<span class="p_add">+		__preempt_count_sub(PREEMPT_ACTIVE + PREEMPT_DISABLE_OFFSET);</span>
 	} while (need_resched());
 }
<span class="p_del">-EXPORT_SYMBOL_GPL(preempt_schedule_context);</span>
<span class="p_del">-#endif /* CONFIG_CONTEXT_TRACKING */</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(preempt_schedule_notrace);</span>
 
 #endif /* CONFIG_PREEMPT */
 
<span class="p_chunk">@@ -2952,17 +3017,11 @@</span> <span class="p_context"> asmlinkage __visible void __sched preempt_schedule_irq(void)</span>
 	prev_state = exception_enter();
 
 	do {
<span class="p_del">-		__preempt_count_add(PREEMPT_ACTIVE);</span>
<span class="p_add">+		preempt_active_enter();</span>
 		local_irq_enable();
 		__schedule();
 		local_irq_disable();
<span class="p_del">-		__preempt_count_sub(PREEMPT_ACTIVE);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Check again in case we missed a preemption opportunity</span>
<span class="p_del">-		 * between schedule and now.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		barrier();</span>
<span class="p_add">+		preempt_active_exit();</span>
 	} while (need_resched());
 
 	exception_exit(prev_state);
<span class="p_chunk">@@ -3040,7 +3099,6 @@</span> <span class="p_context"> void rt_mutex_setprio(struct task_struct *p, int prio)</span>
 		if (!dl_prio(p-&gt;normal_prio) ||
 		    (pi_task &amp;&amp; dl_entity_preempt(&amp;pi_task-&gt;dl, &amp;p-&gt;dl))) {
 			p-&gt;dl.dl_boosted = 1;
<span class="p_del">-			p-&gt;dl.dl_throttled = 0;</span>
 			enqueue_flag = ENQUEUE_REPLENISH;
 		} else
 			p-&gt;dl.dl_boosted = 0;
<span class="p_chunk">@@ -5314,7 +5372,7 @@</span> <span class="p_context"> static struct notifier_block migration_notifier = {</span>
 	.priority = CPU_PRI_MIGRATION,
 };
 
<span class="p_del">-static void __cpuinit set_cpu_rq_start_time(void)</span>
<span class="p_add">+static void set_cpu_rq_start_time(void)</span>
 {
 	int cpu = smp_processor_id();
 	struct rq *rq = cpu_rq(cpu);
<span class="p_chunk">@@ -7734,11 +7792,11 @@</span> <span class="p_context"> static long sched_group_rt_runtime(struct task_group *tg)</span>
 	return rt_runtime_us;
 }
 
<span class="p_del">-static int sched_group_set_rt_period(struct task_group *tg, long rt_period_us)</span>
<span class="p_add">+static int sched_group_set_rt_period(struct task_group *tg, u64 rt_period_us)</span>
 {
 	u64 rt_runtime, rt_period;
 
<span class="p_del">-	rt_period = (u64)rt_period_us * NSEC_PER_USEC;</span>
<span class="p_add">+	rt_period = rt_period_us * NSEC_PER_USEC;</span>
 	rt_runtime = tg-&gt;rt_bandwidth.rt_runtime;
 
 	return tg_set_rt_bandwidth(tg, rt_period, rt_runtime);
<span class="p_header">diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c</span>
<span class="p_header">index 8394b1ee600c..f5a64ffad176 100644</span>
<span class="p_header">--- a/kernel/sched/cputime.c</span>
<span class="p_header">+++ b/kernel/sched/cputime.c</span>
<span class="p_chunk">@@ -567,7 +567,7 @@</span> <span class="p_context"> static void cputime_advance(cputime_t *counter, cputime_t new)</span>
 {
 	cputime_t old;
 
<span class="p_del">-	while (new &gt; (old = ACCESS_ONCE(*counter)))</span>
<span class="p_add">+	while (new &gt; (old = READ_ONCE(*counter)))</span>
 		cmpxchg_cputime(counter, old, new);
 }
 
<span class="p_header">diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c</span>
<span class="p_header">index 5e95145088fd..392e8fb94db3 100644</span>
<span class="p_header">--- a/kernel/sched/deadline.c</span>
<span class="p_header">+++ b/kernel/sched/deadline.c</span>
<span class="p_chunk">@@ -640,7 +640,7 @@</span> <span class="p_context"> void init_dl_task_timer(struct sched_dl_entity *dl_se)</span>
 }
 
 static
<span class="p_del">-int dl_runtime_exceeded(struct rq *rq, struct sched_dl_entity *dl_se)</span>
<span class="p_add">+int dl_runtime_exceeded(struct sched_dl_entity *dl_se)</span>
 {
 	return (dl_se-&gt;runtime &lt;= 0);
 }
<span class="p_chunk">@@ -684,7 +684,7 @@</span> <span class="p_context"> static void update_curr_dl(struct rq *rq)</span>
 	sched_rt_avg_update(rq, delta_exec);
 
 	dl_se-&gt;runtime -= dl_se-&gt;dl_yielded ? 0 : delta_exec;
<span class="p_del">-	if (dl_runtime_exceeded(rq, dl_se)) {</span>
<span class="p_add">+	if (dl_runtime_exceeded(dl_se)) {</span>
 		dl_se-&gt;dl_throttled = 1;
 		__dequeue_task_dl(rq, curr, 0);
 		if (unlikely(!start_dl_timer(dl_se, curr-&gt;dl.dl_boosted)))
<span class="p_chunk">@@ -995,7 +995,7 @@</span> <span class="p_context"> select_task_rq_dl(struct task_struct *p, int cpu, int sd_flag, int flags)</span>
 	rq = cpu_rq(cpu);
 
 	rcu_read_lock();
<span class="p_del">-	curr = ACCESS_ONCE(rq-&gt;curr); /* unlocked access */</span>
<span class="p_add">+	curr = READ_ONCE(rq-&gt;curr); /* unlocked access */</span>
 
 	/*
 	 * If we are dealing with a -deadline task, we must
<span class="p_chunk">@@ -1012,7 +1012,9 @@</span> <span class="p_context"> select_task_rq_dl(struct task_struct *p, int cpu, int sd_flag, int flags)</span>
 	    (p-&gt;nr_cpus_allowed &gt; 1)) {
 		int target = find_later_rq(p);
 
<span class="p_del">-		if (target != -1)</span>
<span class="p_add">+		if (target != -1 &amp;&amp;</span>
<span class="p_add">+				dl_time_before(p-&gt;dl.deadline,</span>
<span class="p_add">+					cpu_rq(target)-&gt;dl.earliest_dl.curr))</span>
 			cpu = target;
 	}
 	rcu_read_unlock();
<span class="p_chunk">@@ -1230,6 +1232,32 @@</span> <span class="p_context"> static struct task_struct *pick_next_earliest_dl_task(struct rq *rq, int cpu)</span>
 	return NULL;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Return the earliest pushable rq&#39;s task, which is suitable to be executed</span>
<span class="p_add">+ * on the CPU, NULL otherwise:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct task_struct *pick_earliest_pushable_dl_task(struct rq *rq, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rb_node *next_node = rq-&gt;dl.pushable_dl_tasks_leftmost;</span>
<span class="p_add">+	struct task_struct *p = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!has_pushable_dl_tasks(rq))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+next_node:</span>
<span class="p_add">+	if (next_node) {</span>
<span class="p_add">+		p = rb_entry(next_node, struct task_struct, pushable_dl_tasks);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pick_dl_task(rq, p, cpu))</span>
<span class="p_add">+			return p;</span>
<span class="p_add">+</span>
<span class="p_add">+		next_node = rb_next(next_node);</span>
<span class="p_add">+		goto next_node;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static DEFINE_PER_CPU(cpumask_var_t, local_cpu_mask_dl);
 
 static int find_later_rq(struct task_struct *task)
<span class="p_chunk">@@ -1333,6 +1361,17 @@</span> <span class="p_context"> static struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq)</span>
 
 		later_rq = cpu_rq(cpu);
 
<span class="p_add">+		if (!dl_time_before(task-&gt;dl.deadline,</span>
<span class="p_add">+					later_rq-&gt;dl.earliest_dl.curr)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Target rq has tasks of equal or earlier deadline,</span>
<span class="p_add">+			 * retrying does not release any lock and is unlikely</span>
<span class="p_add">+			 * to yield a different result.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			later_rq = NULL;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* Retry if something changed. */
 		if (double_lock_balance(rq, later_rq)) {
 			if (unlikely(task_rq(task) != rq ||
<span class="p_chunk">@@ -1514,7 +1553,7 @@</span> <span class="p_context"> static int pull_dl_task(struct rq *this_rq)</span>
 		if (src_rq-&gt;dl.dl_nr_running &lt;= 1)
 			goto skip;
 
<span class="p_del">-		p = pick_next_earliest_dl_task(src_rq, this_cpu);</span>
<span class="p_add">+		p = pick_earliest_pushable_dl_task(src_rq, this_cpu);</span>
 
 		/*
 		 * We found a task to be pulled if:
<span class="p_chunk">@@ -1659,7 +1698,7 @@</span> <span class="p_context"> static void rq_offline_dl(struct rq *rq)</span>
 	cpudl_clear_freecpu(&amp;rq-&gt;rd-&gt;cpudl, rq-&gt;cpu);
 }
 
<span class="p_del">-void init_sched_dl_class(void)</span>
<span class="p_add">+void __init init_sched_dl_class(void)</span>
 {
 	unsigned int i;
 
<span class="p_header">diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c</span>
<span class="p_header">index a245c1fc6f0a..704683cc9042 100644</span>
<span class="p_header">--- a/kernel/sched/debug.c</span>
<span class="p_header">+++ b/kernel/sched/debug.c</span>
<span class="p_chunk">@@ -132,12 +132,14 @@</span> <span class="p_context"> print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)</span>
 		p-&gt;prio);
 #ifdef CONFIG_SCHEDSTATS
 	SEQ_printf(m, &quot;%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld&quot;,
<span class="p_del">-		SPLIT_NS(p-&gt;se.vruntime),</span>
<span class="p_add">+		SPLIT_NS(p-&gt;se.statistics.wait_sum),</span>
 		SPLIT_NS(p-&gt;se.sum_exec_runtime),
 		SPLIT_NS(p-&gt;se.statistics.sum_sleep_runtime));
 #else
<span class="p_del">-	SEQ_printf(m, &quot;%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld&quot;,</span>
<span class="p_del">-		0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);</span>
<span class="p_add">+	SEQ_printf(m, &quot;%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld&quot;,</span>
<span class="p_add">+		0LL, 0L,</span>
<span class="p_add">+		SPLIT_NS(p-&gt;se.sum_exec_runtime),</span>
<span class="p_add">+		0LL, 0L);</span>
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, &quot; %d&quot;, task_node(p));
<span class="p_chunk">@@ -156,7 +158,7 @@</span> <span class="p_context"> static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)</span>
 	SEQ_printf(m,
 	&quot;\nrunnable tasks:\n&quot;
 	&quot;            task   PID         tree-key  switches  prio&quot;
<span class="p_del">-	&quot;     exec-runtime         sum-exec        sum-sleep\n&quot;</span>
<span class="p_add">+	&quot;     wait-time             sum-exec        sum-sleep\n&quot;</span>
 	&quot;------------------------------------------------------&quot;
 	&quot;----------------------------------------------------\n&quot;);
 
<span class="p_chunk">@@ -582,6 +584,7 @@</span> <span class="p_context"> void proc_sched_show_task(struct task_struct *p, struct seq_file *m)</span>
 	nr_switches = p-&gt;nvcsw + p-&gt;nivcsw;
 
 #ifdef CONFIG_SCHEDSTATS
<span class="p_add">+	PN(se.statistics.sum_sleep_runtime);</span>
 	PN(se.statistics.wait_start);
 	PN(se.statistics.sleep_start);
 	PN(se.statistics.block_start);
<span class="p_header">diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c</span>
<span class="p_header">index ffeaa4105e48..4b6e5f63d9af 100644</span>
<span class="p_header">--- a/kernel/sched/fair.c</span>
<span class="p_header">+++ b/kernel/sched/fair.c</span>
<span class="p_chunk">@@ -141,9 +141,9 @@</span> <span class="p_context"> static inline void update_load_set(struct load_weight *lw, unsigned long w)</span>
  *
  * This idea comes from the SD scheduler of Con Kolivas:
  */
<span class="p_del">-static int get_update_sysctl_factor(void)</span>
<span class="p_add">+static unsigned int get_update_sysctl_factor(void)</span>
 {
<span class="p_del">-	unsigned int cpus = min_t(int, num_online_cpus(), 8);</span>
<span class="p_add">+	unsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);</span>
 	unsigned int factor;
 
 	switch (sysctl_sched_tunable_scaling) {
<span class="p_chunk">@@ -576,7 +576,7 @@</span> <span class="p_context"> int sched_proc_update_handler(struct ctl_table *table, int write,</span>
 		loff_t *ppos)
 {
 	int ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
<span class="p_del">-	int factor = get_update_sysctl_factor();</span>
<span class="p_add">+	unsigned int factor = get_update_sysctl_factor();</span>
 
 	if (ret || !write)
 		return ret;
<span class="p_chunk">@@ -834,7 +834,7 @@</span> <span class="p_context"> static unsigned int task_nr_scan_windows(struct task_struct *p)</span>
 
 static unsigned int task_scan_min(struct task_struct *p)
 {
<span class="p_del">-	unsigned int scan_size = ACCESS_ONCE(sysctl_numa_balancing_scan_size);</span>
<span class="p_add">+	unsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size);</span>
 	unsigned int scan, floor;
 	unsigned int windows = 1;
 
<span class="p_chunk">@@ -1198,11 +1198,9 @@</span> <span class="p_context"> static void task_numa_assign(struct task_numa_env *env,</span>
 static bool load_too_imbalanced(long src_load, long dst_load,
 				struct task_numa_env *env)
 {
<span class="p_add">+	long imb, old_imb;</span>
<span class="p_add">+	long orig_src_load, orig_dst_load;</span>
 	long src_capacity, dst_capacity;
<span class="p_del">-	long orig_src_load;</span>
<span class="p_del">-	long load_a, load_b;</span>
<span class="p_del">-	long moved_load;</span>
<span class="p_del">-	long imb;</span>
 
 	/*
 	 * The load is corrected for the CPU capacity available on each node.
<span class="p_chunk">@@ -1215,39 +1213,30 @@</span> <span class="p_context"> static bool load_too_imbalanced(long src_load, long dst_load,</span>
 	dst_capacity = env-&gt;dst_stats.compute_capacity;
 
 	/* We care about the slope of the imbalance, not the direction. */
<span class="p_del">-	load_a = dst_load;</span>
<span class="p_del">-	load_b = src_load;</span>
<span class="p_del">-	if (load_a &lt; load_b)</span>
<span class="p_del">-		swap(load_a, load_b);</span>
<span class="p_add">+	if (dst_load &lt; src_load)</span>
<span class="p_add">+		swap(dst_load, src_load);</span>
 
 	/* Is the difference below the threshold? */
<span class="p_del">-	imb = load_a * src_capacity * 100 -</span>
<span class="p_del">-		load_b * dst_capacity * env-&gt;imbalance_pct;</span>
<span class="p_add">+	imb = dst_load * src_capacity * 100 -</span>
<span class="p_add">+	      src_load * dst_capacity * env-&gt;imbalance_pct;</span>
 	if (imb &lt;= 0)
 		return false;
 
 	/*
 	 * The imbalance is above the allowed threshold.
<span class="p_del">-	 * Allow a move that brings us closer to a balanced situation,</span>
<span class="p_del">-	 * without moving things past the point of balance.</span>
<span class="p_add">+	 * Compare it with the old imbalance.</span>
 	 */
 	orig_src_load = env-&gt;src_stats.load;
<span class="p_add">+	orig_dst_load = env-&gt;dst_stats.load;</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * In a task swap, there will be one load moving from src to dst,</span>
<span class="p_del">-	 * and another moving back. This is the net sum of both moves.</span>
<span class="p_del">-	 * A simple task move will always have a positive value.</span>
<span class="p_del">-	 * Allow the move if it brings the system closer to a balanced</span>
<span class="p_del">-	 * situation, without crossing over the balance point.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	moved_load = orig_src_load - src_load;</span>
<span class="p_add">+	if (orig_dst_load &lt; orig_src_load)</span>
<span class="p_add">+		swap(orig_dst_load, orig_src_load);</span>
 
<span class="p_del">-	if (moved_load &gt; 0)</span>
<span class="p_del">-		/* Moving src -&gt; dst. Did we overshoot balance? */</span>
<span class="p_del">-		return src_load * dst_capacity &lt; dst_load * src_capacity;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		/* Moving dst -&gt; src. Did we overshoot balance? */</span>
<span class="p_del">-		return dst_load * src_capacity &lt; src_load * dst_capacity;</span>
<span class="p_add">+	old_imb = orig_dst_load * src_capacity * 100 -</span>
<span class="p_add">+		  orig_src_load * dst_capacity * env-&gt;imbalance_pct;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Would this change make things worse? */</span>
<span class="p_add">+	return (imb &gt; old_imb);</span>
 }
 
 /*
<span class="p_chunk">@@ -1409,6 +1398,30 @@</span> <span class="p_context"> static void task_numa_find_cpu(struct task_numa_env *env,</span>
 	}
 }
 
<span class="p_add">+/* Only move tasks to a NUMA node less busy than the current node. */</span>
<span class="p_add">+static bool numa_has_capacity(struct task_numa_env *env)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct numa_stats *src = &amp;env-&gt;src_stats;</span>
<span class="p_add">+	struct numa_stats *dst = &amp;env-&gt;dst_stats;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (src-&gt;has_free_capacity &amp;&amp; !dst-&gt;has_free_capacity)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Only consider a task move if the source has a higher load</span>
<span class="p_add">+	 * than the destination, corrected for CPU capacity on each node.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *      src-&gt;load                dst-&gt;load</span>
<span class="p_add">+	 * --------------------- vs ---------------------</span>
<span class="p_add">+	 * src-&gt;compute_capacity    dst-&gt;compute_capacity</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (src-&gt;load * dst-&gt;compute_capacity &gt;</span>
<span class="p_add">+	    dst-&gt;load * src-&gt;compute_capacity)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int task_numa_migrate(struct task_struct *p)
 {
 	struct task_numa_env env = {
<span class="p_chunk">@@ -1463,7 +1476,8 @@</span> <span class="p_context"> static int task_numa_migrate(struct task_struct *p)</span>
 	update_numa_stats(&amp;env.dst_stats, env.dst_nid);
 
 	/* Try to find a spot on the preferred nid. */
<span class="p_del">-	task_numa_find_cpu(&amp;env, taskimp, groupimp);</span>
<span class="p_add">+	if (numa_has_capacity(&amp;env))</span>
<span class="p_add">+		task_numa_find_cpu(&amp;env, taskimp, groupimp);</span>
 
 	/*
 	 * Look at other nodes in these cases:
<span class="p_chunk">@@ -1494,7 +1508,8 @@</span> <span class="p_context"> static int task_numa_migrate(struct task_struct *p)</span>
 			env.dist = dist;
 			env.dst_nid = nid;
 			update_numa_stats(&amp;env.dst_stats, env.dst_nid);
<span class="p_del">-			task_numa_find_cpu(&amp;env, taskimp, groupimp);</span>
<span class="p_add">+			if (numa_has_capacity(&amp;env))</span>
<span class="p_add">+				task_numa_find_cpu(&amp;env, taskimp, groupimp);</span>
 		}
 	}
 
<span class="p_chunk">@@ -1794,7 +1809,12 @@</span> <span class="p_context"> static void task_numa_placement(struct task_struct *p)</span>
 	u64 runtime, period;
 	spinlock_t *group_lock = NULL;
 
<span class="p_del">-	seq = ACCESS_ONCE(p-&gt;mm-&gt;numa_scan_seq);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The p-&gt;mm-&gt;numa_scan_seq field gets updated without</span>
<span class="p_add">+	 * exclusive access. Use READ_ONCE() here to ensure</span>
<span class="p_add">+	 * that the field is read in a single access:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	seq = READ_ONCE(p-&gt;mm-&gt;numa_scan_seq);</span>
 	if (p-&gt;numa_scan_seq == seq)
 		return;
 	p-&gt;numa_scan_seq = seq;
<span class="p_chunk">@@ -1938,7 +1958,7 @@</span> <span class="p_context"> static void task_numa_group(struct task_struct *p, int cpupid, int flags,</span>
 	}
 
 	rcu_read_lock();
<span class="p_del">-	tsk = ACCESS_ONCE(cpu_rq(cpu)-&gt;curr);</span>
<span class="p_add">+	tsk = READ_ONCE(cpu_rq(cpu)-&gt;curr);</span>
 
 	if (!cpupid_match_pid(tsk, cpupid))
 		goto no_join;
<span class="p_chunk">@@ -2107,7 +2127,15 @@</span> <span class="p_context"> void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)</span>
 
 static void reset_ptenuma_scan(struct task_struct *p)
 {
<span class="p_del">-	ACCESS_ONCE(p-&gt;mm-&gt;numa_scan_seq)++;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We only did a read acquisition of the mmap sem, so</span>
<span class="p_add">+	 * p-&gt;mm-&gt;numa_scan_seq is written to without exclusive access</span>
<span class="p_add">+	 * and the update is not guaranteed to be atomic. That&#39;s not</span>
<span class="p_add">+	 * much of an issue though, since this is just used for</span>
<span class="p_add">+	 * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not</span>
<span class="p_add">+	 * expensive, to avoid any form of compiler optimizations:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WRITE_ONCE(p-&gt;mm-&gt;numa_scan_seq, READ_ONCE(p-&gt;mm-&gt;numa_scan_seq) + 1);</span>
 	p-&gt;mm-&gt;numa_scan_offset = 0;
 }
 
<span class="p_chunk">@@ -4323,6 +4351,189 @@</span> <span class="p_context"> static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)</span>
 }
 
 #ifdef CONFIG_SMP
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * per rq &#39;load&#39; arrray crap; XXX kill this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The exact cpuload at various idx values, calculated at every tick would be</span>
<span class="p_add">+ * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called</span>
<span class="p_add">+ * on nth tick when cpu may be busy, then we have:</span>
<span class="p_add">+ * load = ((2^idx - 1) / 2^idx)^(n-1) * load</span>
<span class="p_add">+ * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * decay_load_missed() below does efficient calculation of</span>
<span class="p_add">+ * load = ((2^idx - 1) / 2^idx)^(n-1) * load</span>
<span class="p_add">+ * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The calculation is approximated on a 128 point scale.</span>
<span class="p_add">+ * degrade_zero_ticks is the number of ticks after which load at any</span>
<span class="p_add">+ * particular idx is approximated to be zero.</span>
<span class="p_add">+ * degrade_factor is a precomputed table, a row for each load idx.</span>
<span class="p_add">+ * Each column corresponds to degradation factor for a power of two ticks,</span>
<span class="p_add">+ * based on 128 point scale.</span>
<span class="p_add">+ * Example:</span>
<span class="p_add">+ * row 2, col 3 (=12) says that the degradation at load idx 2 after</span>
<span class="p_add">+ * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With this power of 2 load factors, we can degrade the load n times</span>
<span class="p_add">+ * by looking at 1 bits in n and doing as many mult/shift instead of</span>
<span class="p_add">+ * n mult/shifts needed by the exact degradation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define DEGRADE_SHIFT		7</span>
<span class="p_add">+static const unsigned char</span>
<span class="p_add">+		degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};</span>
<span class="p_add">+static const unsigned char</span>
<span class="p_add">+		degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {</span>
<span class="p_add">+					{0, 0, 0, 0, 0, 0, 0, 0},</span>
<span class="p_add">+					{64, 32, 8, 0, 0, 0, 0, 0},</span>
<span class="p_add">+					{96, 72, 40, 12, 1, 0, 0},</span>
<span class="p_add">+					{112, 98, 75, 43, 15, 1, 0},</span>
<span class="p_add">+					{120, 112, 98, 76, 45, 16, 2} };</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Update cpu_load for any missed ticks, due to tickless idle. The backlog</span>
<span class="p_add">+ * would be when CPU is idle and so we just decay the old load without</span>
<span class="p_add">+ * adding any new load.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static unsigned long</span>
<span class="p_add">+decay_load_missed(unsigned long load, unsigned long missed_updates, int idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int j = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!missed_updates)</span>
<span class="p_add">+		return load;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (missed_updates &gt;= degrade_zero_ticks[idx])</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (idx == 1)</span>
<span class="p_add">+		return load &gt;&gt; missed_updates;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (missed_updates) {</span>
<span class="p_add">+		if (missed_updates % 2)</span>
<span class="p_add">+			load = (load * degrade_factor[idx][j]) &gt;&gt; DEGRADE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+		missed_updates &gt;&gt;= 1;</span>
<span class="p_add">+		j++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return load;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Update rq-&gt;cpu_load[] statistics. This function is usually called every</span>
<span class="p_add">+ * scheduler tick (TICK_NSEC). With tickless idle this will not be called</span>
<span class="p_add">+ * every tick. We fix it up based on jiffies.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __update_cpu_load(struct rq *this_rq, unsigned long this_load,</span>
<span class="p_add">+			      unsigned long pending_updates)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, scale;</span>
<span class="p_add">+</span>
<span class="p_add">+	this_rq-&gt;nr_load_updates++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Update our load: */</span>
<span class="p_add">+	this_rq-&gt;cpu_load[0] = this_load; /* Fasttrack for idx 0 */</span>
<span class="p_add">+	for (i = 1, scale = 2; i &lt; CPU_LOAD_IDX_MAX; i++, scale += scale) {</span>
<span class="p_add">+		unsigned long old_load, new_load;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* scale is effectively 1 &lt;&lt; i now, and &gt;&gt; i divides by scale */</span>
<span class="p_add">+</span>
<span class="p_add">+		old_load = this_rq-&gt;cpu_load[i];</span>
<span class="p_add">+		old_load = decay_load_missed(old_load, pending_updates - 1, i);</span>
<span class="p_add">+		new_load = this_load;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Round up the averaging division if load is increasing. This</span>
<span class="p_add">+		 * prevents us from getting stuck on 9 if the load is 10, for</span>
<span class="p_add">+		 * example.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (new_load &gt; old_load)</span>
<span class="p_add">+			new_load += scale - 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		this_rq-&gt;cpu_load[i] = (old_load * (scale - 1) + new_load) &gt;&gt; i;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	sched_avg_update(this_rq);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_NO_HZ_COMMON</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * There is no sane way to deal with nohz on smp when using jiffies because the</span>
<span class="p_add">+ * cpu doing the jiffies update might drift wrt the cpu doing the jiffy reading</span>
<span class="p_add">+ * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Therefore we cannot use the delta approach from the regular tick since that</span>
<span class="p_add">+ * would seriously skew the load calculation. However we&#39;ll make do for those</span>
<span class="p_add">+ * updates happening while idle (nohz_idle_balance) or coming out of idle</span>
<span class="p_add">+ * (tick_nohz_idle_exit).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This means we might still be one tick off for nohz periods.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called from nohz_idle_balance() to update the load ratings before doing the</span>
<span class="p_add">+ * idle balance.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void update_idle_cpu_load(struct rq *this_rq)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long curr_jiffies = READ_ONCE(jiffies);</span>
<span class="p_add">+	unsigned long load = this_rq-&gt;cfs.runnable_load_avg;</span>
<span class="p_add">+	unsigned long pending_updates;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * bail if there&#39;s load or we&#39;re actually up-to-date.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (load || curr_jiffies == this_rq-&gt;last_load_update_tick)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pending_updates = curr_jiffies - this_rq-&gt;last_load_update_tick;</span>
<span class="p_add">+	this_rq-&gt;last_load_update_tick = curr_jiffies;</span>
<span class="p_add">+</span>
<span class="p_add">+	__update_cpu_load(this_rq, load, pending_updates);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called from tick_nohz_idle_exit() -- try and fix up the ticks we missed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void update_cpu_load_nohz(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq *this_rq = this_rq();</span>
<span class="p_add">+	unsigned long curr_jiffies = READ_ONCE(jiffies);</span>
<span class="p_add">+	unsigned long pending_updates;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (curr_jiffies == this_rq-&gt;last_load_update_tick)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock(&amp;this_rq-&gt;lock);</span>
<span class="p_add">+	pending_updates = curr_jiffies - this_rq-&gt;last_load_update_tick;</span>
<span class="p_add">+	if (pending_updates) {</span>
<span class="p_add">+		this_rq-&gt;last_load_update_tick = curr_jiffies;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We were idle, this means load 0, the current load might be</span>
<span class="p_add">+		 * !0 due to remote wakeups and the sort.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		__update_cpu_load(this_rq, 0, pending_updates);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	raw_spin_unlock(&amp;this_rq-&gt;lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_NO_HZ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called from scheduler_tick()</span>
<span class="p_add">+ */</span>
<span class="p_add">+void update_cpu_load_active(struct rq *this_rq)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long load = this_rq-&gt;cfs.runnable_load_avg;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * See the mess around update_idle_cpu_load() / update_cpu_load_nohz().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	this_rq-&gt;last_load_update_tick = jiffies;</span>
<span class="p_add">+	__update_cpu_load(this_rq, load, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Used instead of source_load when we know the type == 0 */
 static unsigned long weighted_cpuload(const int cpu)
 {
<span class="p_chunk">@@ -4375,7 +4586,7 @@</span> <span class="p_context"> static unsigned long capacity_orig_of(int cpu)</span>
 static unsigned long cpu_avg_load_per_task(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
<span class="p_del">-	unsigned long nr_running = ACCESS_ONCE(rq-&gt;cfs.h_nr_running);</span>
<span class="p_add">+	unsigned long nr_running = READ_ONCE(rq-&gt;cfs.h_nr_running);</span>
 	unsigned long load_avg = rq-&gt;cfs.runnable_load_avg;
 
 	if (nr_running)
<span class="p_chunk">@@ -5126,18 +5337,21 @@</span> <span class="p_context"> pick_next_task_fair(struct rq *rq, struct task_struct *prev)</span>
 		 * entity, update_curr() will update its vruntime, otherwise
 		 * forget we&#39;ve ever seen it.
 		 */
<span class="p_del">-		if (curr &amp;&amp; curr-&gt;on_rq)</span>
<span class="p_del">-			update_curr(cfs_rq);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			curr = NULL;</span>
<span class="p_add">+		if (curr) {</span>
<span class="p_add">+			if (curr-&gt;on_rq)</span>
<span class="p_add">+				update_curr(cfs_rq);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				curr = NULL;</span>
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * This call to check_cfs_rq_runtime() will do the throttle and</span>
<span class="p_del">-		 * dequeue its entity in the parent(s). Therefore the &#39;simple&#39;</span>
<span class="p_del">-		 * nr_running test will indeed be correct.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (unlikely(check_cfs_rq_runtime(cfs_rq)))</span>
<span class="p_del">-			goto simple;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This call to check_cfs_rq_runtime() will do the</span>
<span class="p_add">+			 * throttle and dequeue its entity in the parent(s).</span>
<span class="p_add">+			 * Therefore the &#39;simple&#39; nr_running test will indeed</span>
<span class="p_add">+			 * be correct.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (unlikely(check_cfs_rq_runtime(cfs_rq)))</span>
<span class="p_add">+				goto simple;</span>
<span class="p_add">+		}</span>
 
 		se = pick_next_entity(cfs_rq, curr);
 		cfs_rq = group_cfs_rq(se);
<span class="p_chunk">@@ -5467,10 +5681,15 @@</span> <span class="p_context"> static int task_hot(struct task_struct *p, struct lb_env *env)</span>
 }
 
 #ifdef CONFIG_NUMA_BALANCING
<span class="p_del">-/* Returns true if the destination node has incurred more faults */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns true if the destination node is the preferred node.</span>
<span class="p_add">+ * Needs to match fbq_classify_rq(): if there is a runnable task</span>
<span class="p_add">+ * that is not on its preferred node, we should identify it.</span>
<span class="p_add">+ */</span>
 static bool migrate_improves_locality(struct task_struct *p, struct lb_env *env)
 {
 	struct numa_group *numa_group = rcu_dereference(p-&gt;numa_group);
<span class="p_add">+	unsigned long src_faults, dst_faults;</span>
 	int src_nid, dst_nid;
 
 	if (!sched_feat(NUMA_FAVOUR_HIGHER) || !p-&gt;numa_faults ||
<span class="p_chunk">@@ -5484,29 +5703,30 @@</span> <span class="p_context"> static bool migrate_improves_locality(struct task_struct *p, struct lb_env *env)</span>
 	if (src_nid == dst_nid)
 		return false;
 
<span class="p_del">-	if (numa_group) {</span>
<span class="p_del">-		/* Task is already in the group&#39;s interleave set. */</span>
<span class="p_del">-		if (node_isset(src_nid, numa_group-&gt;active_nodes))</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Task is moving into the group&#39;s interleave set. */</span>
<span class="p_del">-		if (node_isset(dst_nid, numa_group-&gt;active_nodes))</span>
<span class="p_del">-			return true;</span>
<span class="p_del">-</span>
<span class="p_del">-		return group_faults(p, dst_nid) &gt; group_faults(p, src_nid);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	/* Encourage migration to the preferred node. */
 	if (dst_nid == p-&gt;numa_preferred_nid)
 		return true;
 
<span class="p_del">-	return task_faults(p, dst_nid) &gt; task_faults(p, src_nid);</span>
<span class="p_add">+	/* Migrating away from the preferred node is bad. */</span>
<span class="p_add">+	if (src_nid == p-&gt;numa_preferred_nid)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (numa_group) {</span>
<span class="p_add">+		src_faults = group_faults(p, src_nid);</span>
<span class="p_add">+		dst_faults = group_faults(p, dst_nid);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		src_faults = task_faults(p, src_nid);</span>
<span class="p_add">+		dst_faults = task_faults(p, dst_nid);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return dst_faults &gt; src_faults;</span>
 }
 
 
 static bool migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
 {
 	struct numa_group *numa_group = rcu_dereference(p-&gt;numa_group);
<span class="p_add">+	unsigned long src_faults, dst_faults;</span>
 	int src_nid, dst_nid;
 
 	if (!sched_feat(NUMA) || !sched_feat(NUMA_RESIST_LOWER))
<span class="p_chunk">@@ -5521,23 +5741,23 @@</span> <span class="p_context"> static bool migrate_degrades_locality(struct task_struct *p, struct lb_env *env)</span>
 	if (src_nid == dst_nid)
 		return false;
 
<span class="p_del">-	if (numa_group) {</span>
<span class="p_del">-		/* Task is moving within/into the group&#39;s interleave set. */</span>
<span class="p_del">-		if (node_isset(dst_nid, numa_group-&gt;active_nodes))</span>
<span class="p_del">-			return false;</span>
<span class="p_add">+	/* Migrating away from the preferred node is bad. */</span>
<span class="p_add">+	if (src_nid == p-&gt;numa_preferred_nid)</span>
<span class="p_add">+		return true;</span>
 
<span class="p_del">-		/* Task is moving out of the group&#39;s interleave set. */</span>
<span class="p_del">-		if (node_isset(src_nid, numa_group-&gt;active_nodes))</span>
<span class="p_del">-			return true;</span>
<span class="p_add">+	/* Encourage migration to the preferred node. */</span>
<span class="p_add">+	if (dst_nid == p-&gt;numa_preferred_nid)</span>
<span class="p_add">+		return false;</span>
 
<span class="p_del">-		return group_faults(p, dst_nid) &lt; group_faults(p, src_nid);</span>
<span class="p_add">+	if (numa_group) {</span>
<span class="p_add">+		src_faults = group_faults(p, src_nid);</span>
<span class="p_add">+		dst_faults = group_faults(p, dst_nid);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		src_faults = task_faults(p, src_nid);</span>
<span class="p_add">+		dst_faults = task_faults(p, dst_nid);</span>
 	}
 
<span class="p_del">-	/* Migrating away from the preferred node is always bad. */</span>
<span class="p_del">-	if (src_nid == p-&gt;numa_preferred_nid)</span>
<span class="p_del">-		return true;</span>
<span class="p_del">-</span>
<span class="p_del">-	return task_faults(p, dst_nid) &lt; task_faults(p, src_nid);</span>
<span class="p_add">+	return dst_faults &lt; src_faults;</span>
 }
 
 #else
<span class="p_chunk">@@ -6037,8 +6257,8 @@</span> <span class="p_context"> static unsigned long scale_rt_capacity(int cpu)</span>
 	 * Since we&#39;re reading these variables without serialization make sure
 	 * we read them once before doing sanity checks on them.
 	 */
<span class="p_del">-	age_stamp = ACCESS_ONCE(rq-&gt;age_stamp);</span>
<span class="p_del">-	avg = ACCESS_ONCE(rq-&gt;rt_avg);</span>
<span class="p_add">+	age_stamp = READ_ONCE(rq-&gt;age_stamp);</span>
<span class="p_add">+	avg = READ_ONCE(rq-&gt;rt_avg);</span>
 	delta = __rq_clock_broken(rq) - age_stamp;
 
 	if (unlikely(delta &lt; 0))
<span class="p_header">diff --git a/kernel/sched/proc.c b/kernel/sched/loadavg.c</span>
similarity index 62%
rename from kernel/sched/proc.c
rename to kernel/sched/loadavg.c
<span class="p_header">index 8ecd552fe4f2..ef7159012cf3 100644</span>
<span class="p_header">--- a/kernel/sched/proc.c</span>
<span class="p_header">+++ b/kernel/sched/loadavg.c</span>
<span class="p_chunk">@@ -1,7 +1,9 @@</span> <span class="p_context"></span>
 /*
<span class="p_del">- *  kernel/sched/proc.c</span>
<span class="p_add">+ * kernel/sched/loadavg.c</span>
  *
<span class="p_del">- *  Kernel load calculations, forked from sched/core.c</span>
<span class="p_add">+ * This file contains the magic bits required to compute the global loadavg</span>
<span class="p_add">+ * figure. Its a silly number but people think its important. We go through</span>
<span class="p_add">+ * great pains to make it work on big machines and tickless kernels.</span>
  */
 
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -81,7 +83,7 @@</span> <span class="p_context"> long calc_load_fold_active(struct rq *this_rq)</span>
 	long nr_active, delta = 0;
 
 	nr_active = this_rq-&gt;nr_running;
<span class="p_del">-	nr_active += (long) this_rq-&gt;nr_uninterruptible;</span>
<span class="p_add">+	nr_active += (long)this_rq-&gt;nr_uninterruptible;</span>
 
 	if (nr_active != this_rq-&gt;calc_load_active) {
 		delta = nr_active - this_rq-&gt;calc_load_active;
<span class="p_chunk">@@ -186,6 +188,7 @@</span> <span class="p_context"> void calc_load_enter_idle(void)</span>
 	delta = calc_load_fold_active(this_rq);
 	if (delta) {
 		int idx = calc_load_write_idx();
<span class="p_add">+</span>
 		atomic_long_add(delta, &amp;calc_load_idle[idx]);
 	}
 }
<span class="p_chunk">@@ -241,18 +244,20 @@</span> <span class="p_context"> fixed_power_int(unsigned long x, unsigned int frac_bits, unsigned int n)</span>
 {
 	unsigned long result = 1UL &lt;&lt; frac_bits;
 
<span class="p_del">-	if (n) for (;;) {</span>
<span class="p_del">-		if (n &amp; 1) {</span>
<span class="p_del">-			result *= x;</span>
<span class="p_del">-			result += 1UL &lt;&lt; (frac_bits - 1);</span>
<span class="p_del">-			result &gt;&gt;= frac_bits;</span>
<span class="p_add">+	if (n) {</span>
<span class="p_add">+		for (;;) {</span>
<span class="p_add">+			if (n &amp; 1) {</span>
<span class="p_add">+				result *= x;</span>
<span class="p_add">+				result += 1UL &lt;&lt; (frac_bits - 1);</span>
<span class="p_add">+				result &gt;&gt;= frac_bits;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			n &gt;&gt;= 1;</span>
<span class="p_add">+			if (!n)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			x *= x;</span>
<span class="p_add">+			x += 1UL &lt;&lt; (frac_bits - 1);</span>
<span class="p_add">+			x &gt;&gt;= frac_bits;</span>
 		}
<span class="p_del">-		n &gt;&gt;= 1;</span>
<span class="p_del">-		if (!n)</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		x *= x;</span>
<span class="p_del">-		x += 1UL &lt;&lt; (frac_bits - 1);</span>
<span class="p_del">-		x &gt;&gt;= frac_bits;</span>
 	}
 
 	return result;
<span class="p_chunk">@@ -285,7 +290,6 @@</span> <span class="p_context"> static unsigned long</span>
 calc_load_n(unsigned long load, unsigned long exp,
 	    unsigned long active, unsigned int n)
 {
<span class="p_del">-</span>
 	return calc_load(load, fixed_power_int(exp, FSHIFT, n), active);
 }
 
<span class="p_chunk">@@ -339,6 +343,8 @@</span> <span class="p_context"> static inline void calc_global_nohz(void) { }</span>
 /*
  * calc_load - update the avenrun load estimates 10 ticks after the
  * CPUs have updated calc_load_tasks.
<span class="p_add">+ *</span>
<span class="p_add">+ * Called from the global timer code.</span>
  */
 void calc_global_load(unsigned long ticks)
 {
<span class="p_chunk">@@ -370,10 +376,10 @@</span> <span class="p_context"> void calc_global_load(unsigned long ticks)</span>
 }
 
 /*
<span class="p_del">- * Called from update_cpu_load() to periodically update this CPU&#39;s</span>
<span class="p_add">+ * Called from scheduler_tick() to periodically update this CPU&#39;s</span>
  * active count.
  */
<span class="p_del">-static void calc_load_account_active(struct rq *this_rq)</span>
<span class="p_add">+void calc_global_load_tick(struct rq *this_rq)</span>
 {
 	long delta;
 
<span class="p_chunk">@@ -386,199 +392,3 @@</span> <span class="p_context"> static void calc_load_account_active(struct rq *this_rq)</span>
 
 	this_rq-&gt;calc_load_update += LOAD_FREQ;
 }
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * End of global load-average stuff</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * The exact cpuload at various idx values, calculated at every tick would be</span>
<span class="p_del">- * load = (2^idx - 1) / 2^idx * load + 1 / 2^idx * cur_load</span>
<span class="p_del">- *</span>
<span class="p_del">- * If a cpu misses updates for n-1 ticks (as it was idle) and update gets called</span>
<span class="p_del">- * on nth tick when cpu may be busy, then we have:</span>
<span class="p_del">- * load = ((2^idx - 1) / 2^idx)^(n-1) * load</span>
<span class="p_del">- * load = (2^idx - 1) / 2^idx) * load + 1 / 2^idx * cur_load</span>
<span class="p_del">- *</span>
<span class="p_del">- * decay_load_missed() below does efficient calculation of</span>
<span class="p_del">- * load = ((2^idx - 1) / 2^idx)^(n-1) * load</span>
<span class="p_del">- * avoiding 0..n-1 loop doing load = ((2^idx - 1) / 2^idx) * load</span>
<span class="p_del">- *</span>
<span class="p_del">- * The calculation is approximated on a 128 point scale.</span>
<span class="p_del">- * degrade_zero_ticks is the number of ticks after which load at any</span>
<span class="p_del">- * particular idx is approximated to be zero.</span>
<span class="p_del">- * degrade_factor is a precomputed table, a row for each load idx.</span>
<span class="p_del">- * Each column corresponds to degradation factor for a power of two ticks,</span>
<span class="p_del">- * based on 128 point scale.</span>
<span class="p_del">- * Example:</span>
<span class="p_del">- * row 2, col 3 (=12) says that the degradation at load idx 2 after</span>
<span class="p_del">- * 8 ticks is 12/128 (which is an approximation of exact factor 3^8/4^8).</span>
<span class="p_del">- *</span>
<span class="p_del">- * With this power of 2 load factors, we can degrade the load n times</span>
<span class="p_del">- * by looking at 1 bits in n and doing as many mult/shift instead of</span>
<span class="p_del">- * n mult/shifts needed by the exact degradation.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define DEGRADE_SHIFT		7</span>
<span class="p_del">-static const unsigned char</span>
<span class="p_del">-		degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128};</span>
<span class="p_del">-static const unsigned char</span>
<span class="p_del">-		degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = {</span>
<span class="p_del">-					{0, 0, 0, 0, 0, 0, 0, 0},</span>
<span class="p_del">-					{64, 32, 8, 0, 0, 0, 0, 0},</span>
<span class="p_del">-					{96, 72, 40, 12, 1, 0, 0},</span>
<span class="p_del">-					{112, 98, 75, 43, 15, 1, 0},</span>
<span class="p_del">-					{120, 112, 98, 76, 45, 16, 2} };</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Update cpu_load for any missed ticks, due to tickless idle. The backlog</span>
<span class="p_del">- * would be when CPU is idle and so we just decay the old load without</span>
<span class="p_del">- * adding any new load.</span>
<span class="p_del">- */</span>
<span class="p_del">-static unsigned long</span>
<span class="p_del">-decay_load_missed(unsigned long load, unsigned long missed_updates, int idx)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int j = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!missed_updates)</span>
<span class="p_del">-		return load;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (missed_updates &gt;= degrade_zero_ticks[idx])</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (idx == 1)</span>
<span class="p_del">-		return load &gt;&gt; missed_updates;</span>
<span class="p_del">-</span>
<span class="p_del">-	while (missed_updates) {</span>
<span class="p_del">-		if (missed_updates % 2)</span>
<span class="p_del">-			load = (load * degrade_factor[idx][j]) &gt;&gt; DEGRADE_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-		missed_updates &gt;&gt;= 1;</span>
<span class="p_del">-		j++;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return load;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Update rq-&gt;cpu_load[] statistics. This function is usually called every</span>
<span class="p_del">- * scheduler tick (TICK_NSEC). With tickless idle this will not be called</span>
<span class="p_del">- * every tick. We fix it up based on jiffies.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void __update_cpu_load(struct rq *this_rq, unsigned long this_load,</span>
<span class="p_del">-			      unsigned long pending_updates)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i, scale;</span>
<span class="p_del">-</span>
<span class="p_del">-	this_rq-&gt;nr_load_updates++;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Update our load: */</span>
<span class="p_del">-	this_rq-&gt;cpu_load[0] = this_load; /* Fasttrack for idx 0 */</span>
<span class="p_del">-	for (i = 1, scale = 2; i &lt; CPU_LOAD_IDX_MAX; i++, scale += scale) {</span>
<span class="p_del">-		unsigned long old_load, new_load;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* scale is effectively 1 &lt;&lt; i now, and &gt;&gt; i divides by scale */</span>
<span class="p_del">-</span>
<span class="p_del">-		old_load = this_rq-&gt;cpu_load[i];</span>
<span class="p_del">-		old_load = decay_load_missed(old_load, pending_updates - 1, i);</span>
<span class="p_del">-		new_load = this_load;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Round up the averaging division if load is increasing. This</span>
<span class="p_del">-		 * prevents us from getting stuck on 9 if the load is 10, for</span>
<span class="p_del">-		 * example.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (new_load &gt; old_load)</span>
<span class="p_del">-			new_load += scale - 1;</span>
<span class="p_del">-</span>
<span class="p_del">-		this_rq-&gt;cpu_load[i] = (old_load * (scale - 1) + new_load) &gt;&gt; i;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	sched_avg_update(this_rq);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-static inline unsigned long get_rq_runnable_load(struct rq *rq)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return rq-&gt;cfs.runnable_load_avg;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else</span>
<span class="p_del">-static inline unsigned long get_rq_runnable_load(struct rq *rq)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return rq-&gt;load.weight;</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_NO_HZ_COMMON</span>
<span class="p_del">-/*</span>
<span class="p_del">- * There is no sane way to deal with nohz on smp when using jiffies because the</span>
<span class="p_del">- * cpu doing the jiffies update might drift wrt the cpu doing the jiffy reading</span>
<span class="p_del">- * causing off-by-one errors in observed deltas; {0,2} instead of {1,1}.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Therefore we cannot use the delta approach from the regular tick since that</span>
<span class="p_del">- * would seriously skew the load calculation. However we&#39;ll make do for those</span>
<span class="p_del">- * updates happening while idle (nohz_idle_balance) or coming out of idle</span>
<span class="p_del">- * (tick_nohz_idle_exit).</span>
<span class="p_del">- *</span>
<span class="p_del">- * This means we might still be one tick off for nohz periods.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Called from nohz_idle_balance() to update the load ratings before doing the</span>
<span class="p_del">- * idle balance.</span>
<span class="p_del">- */</span>
<span class="p_del">-void update_idle_cpu_load(struct rq *this_rq)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long curr_jiffies = ACCESS_ONCE(jiffies);</span>
<span class="p_del">-	unsigned long load = get_rq_runnable_load(this_rq);</span>
<span class="p_del">-	unsigned long pending_updates;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * bail if there&#39;s load or we&#39;re actually up-to-date.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (load || curr_jiffies == this_rq-&gt;last_load_update_tick)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pending_updates = curr_jiffies - this_rq-&gt;last_load_update_tick;</span>
<span class="p_del">-	this_rq-&gt;last_load_update_tick = curr_jiffies;</span>
<span class="p_del">-</span>
<span class="p_del">-	__update_cpu_load(this_rq, load, pending_updates);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Called from tick_nohz_idle_exit() -- try and fix up the ticks we missed.</span>
<span class="p_del">- */</span>
<span class="p_del">-void update_cpu_load_nohz(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct rq *this_rq = this_rq();</span>
<span class="p_del">-	unsigned long curr_jiffies = ACCESS_ONCE(jiffies);</span>
<span class="p_del">-	unsigned long pending_updates;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (curr_jiffies == this_rq-&gt;last_load_update_tick)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	raw_spin_lock(&amp;this_rq-&gt;lock);</span>
<span class="p_del">-	pending_updates = curr_jiffies - this_rq-&gt;last_load_update_tick;</span>
<span class="p_del">-	if (pending_updates) {</span>
<span class="p_del">-		this_rq-&gt;last_load_update_tick = curr_jiffies;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We were idle, this means load 0, the current load might be</span>
<span class="p_del">-		 * !0 due to remote wakeups and the sort.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		__update_cpu_load(this_rq, 0, pending_updates);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	raw_spin_unlock(&amp;this_rq-&gt;lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_NO_HZ */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Called from scheduler_tick()</span>
<span class="p_del">- */</span>
<span class="p_del">-void update_cpu_load_active(struct rq *this_rq)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long load = get_rq_runnable_load(this_rq);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * See the mess around update_idle_cpu_load() / update_cpu_load_nohz().</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	this_rq-&gt;last_load_update_tick = jiffies;</span>
<span class="p_del">-	__update_cpu_load(this_rq, load, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	calc_load_account_active(this_rq);</span>
<span class="p_del">-}</span>
<span class="p_header">diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c</span>
<span class="p_header">index 575da76a3874..560d2fa623c3 100644</span>
<span class="p_header">--- a/kernel/sched/rt.c</span>
<span class="p_header">+++ b/kernel/sched/rt.c</span>
<span class="p_chunk">@@ -1323,7 +1323,7 @@</span> <span class="p_context"> select_task_rq_rt(struct task_struct *p, int cpu, int sd_flag, int flags)</span>
 	rq = cpu_rq(cpu);
 
 	rcu_read_lock();
<span class="p_del">-	curr = ACCESS_ONCE(rq-&gt;curr); /* unlocked access */</span>
<span class="p_add">+	curr = READ_ONCE(rq-&gt;curr); /* unlocked access */</span>
 
 	/*
 	 * If the current task on @p&#39;s runqueue is an RT task, then
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index e0e129993958..d62b2882232b 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -26,8 +26,14 @@</span> <span class="p_context"> extern __read_mostly int scheduler_running;</span>
 extern unsigned long calc_load_update;
 extern atomic_long_t calc_load_tasks;
 
<span class="p_add">+extern void calc_global_load_tick(struct rq *this_rq);</span>
 extern long calc_load_fold_active(struct rq *this_rq);
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
 extern void update_cpu_load_active(struct rq *this_rq);
<span class="p_add">+#else</span>
<span class="p_add">+static inline void update_cpu_load_active(struct rq *this_rq) { }</span>
<span class="p_add">+#endif</span>
 
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
<span class="p_chunk">@@ -707,7 +713,7 @@</span> <span class="p_context"> DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);</span>
 
 static inline u64 __rq_clock_broken(struct rq *rq)
 {
<span class="p_del">-	return ACCESS_ONCE(rq-&gt;clock);</span>
<span class="p_add">+	return READ_ONCE(rq-&gt;clock);</span>
 }
 
 static inline u64 rq_clock(struct rq *rq)
<span class="p_chunk">@@ -1284,7 +1290,6 @@</span> <span class="p_context"> extern void update_max_interval(void);</span>
 extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
<span class="p_del">-extern void init_sched_dl_class(void);</span>
 
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
<span class="p_chunk">@@ -1298,8 +1303,6 @@</span> <span class="p_context"> extern void init_dl_task_timer(struct sched_dl_entity *dl_se);</span>
 
 unsigned long to_ratio(u64 period, u64 runtime);
 
<span class="p_del">-extern void update_idle_cpu_load(struct rq *this_rq);</span>
<span class="p_del">-</span>
 extern void init_task_runnable_average(struct task_struct *p);
 
 static inline void add_nr_running(struct rq *rq, unsigned count)
<span class="p_header">diff --git a/kernel/sched/stats.h b/kernel/sched/stats.h</span>
<span class="p_header">index 4ab704339656..077ebbd5e10f 100644</span>
<span class="p_header">--- a/kernel/sched/stats.h</span>
<span class="p_header">+++ b/kernel/sched/stats.h</span>
<span class="p_chunk">@@ -174,7 +174,8 @@</span> <span class="p_context"> static inline bool cputimer_running(struct task_struct *tsk)</span>
 {
 	struct thread_group_cputimer *cputimer = &amp;tsk-&gt;signal-&gt;cputimer;
 
<span class="p_del">-	if (!cputimer-&gt;running)</span>
<span class="p_add">+	/* Check if cputimer isn&#39;t running. This is accessed without locking. */</span>
<span class="p_add">+	if (!READ_ONCE(cputimer-&gt;running))</span>
 		return false;
 
 	/*
<span class="p_chunk">@@ -215,9 +216,7 @@</span> <span class="p_context"> static inline void account_group_user_time(struct task_struct *tsk,</span>
 	if (!cputimer_running(tsk))
 		return;
 
<span class="p_del">-	raw_spin_lock(&amp;cputimer-&gt;lock);</span>
<span class="p_del">-	cputimer-&gt;cputime.utime += cputime;</span>
<span class="p_del">-	raw_spin_unlock(&amp;cputimer-&gt;lock);</span>
<span class="p_add">+	atomic64_add(cputime, &amp;cputimer-&gt;cputime_atomic.utime);</span>
 }
 
 /**
<span class="p_chunk">@@ -238,9 +237,7 @@</span> <span class="p_context"> static inline void account_group_system_time(struct task_struct *tsk,</span>
 	if (!cputimer_running(tsk))
 		return;
 
<span class="p_del">-	raw_spin_lock(&amp;cputimer-&gt;lock);</span>
<span class="p_del">-	cputimer-&gt;cputime.stime += cputime;</span>
<span class="p_del">-	raw_spin_unlock(&amp;cputimer-&gt;lock);</span>
<span class="p_add">+	atomic64_add(cputime, &amp;cputimer-&gt;cputime_atomic.stime);</span>
 }
 
 /**
<span class="p_chunk">@@ -261,7 +258,5 @@</span> <span class="p_context"> static inline void account_group_exec_runtime(struct task_struct *tsk,</span>
 	if (!cputimer_running(tsk))
 		return;
 
<span class="p_del">-	raw_spin_lock(&amp;cputimer-&gt;lock);</span>
<span class="p_del">-	cputimer-&gt;cputime.sum_exec_runtime += ns;</span>
<span class="p_del">-	raw_spin_unlock(&amp;cputimer-&gt;lock);</span>
<span class="p_add">+	atomic64_add(ns, &amp;cputimer-&gt;cputime_atomic.sum_exec_runtime);</span>
 }
<span class="p_header">diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c</span>
<span class="p_header">index 852143a79f36..2ccec988d6b7 100644</span>
<span class="p_header">--- a/kernel/sched/wait.c</span>
<span class="p_header">+++ b/kernel/sched/wait.c</span>
<span class="p_chunk">@@ -601,7 +601,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(bit_wait_io);</span>
 
 __sched int bit_wait_timeout(struct wait_bit_key *word)
 {
<span class="p_del">-	unsigned long now = ACCESS_ONCE(jiffies);</span>
<span class="p_add">+	unsigned long now = READ_ONCE(jiffies);</span>
 	if (signal_pending_state(current-&gt;state, current))
 		return 1;
 	if (time_after_eq(now, word-&gt;timeout))
<span class="p_chunk">@@ -613,7 +613,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(bit_wait_timeout);</span>
 
 __sched int bit_wait_io_timeout(struct wait_bit_key *word)
 {
<span class="p_del">-	unsigned long now = ACCESS_ONCE(jiffies);</span>
<span class="p_add">+	unsigned long now = READ_ONCE(jiffies);</span>
 	if (signal_pending_state(current-&gt;state, current))
 		return 1;
 	if (time_after_eq(now, word-&gt;timeout))
<span class="p_header">diff --git a/kernel/signal.c b/kernel/signal.c</span>
<span class="p_header">index d51c5ddd855c..f19833b5db3c 100644</span>
<span class="p_header">--- a/kernel/signal.c</span>
<span class="p_header">+++ b/kernel/signal.c</span>
<span class="p_chunk">@@ -245,7 +245,7 @@</span> <span class="p_context"> static inline void print_dropped_signal(int sig)</span>
  * RETURNS:
  * %true if @mask is set, %false if made noop because @task was dying.
  */
<span class="p_del">-bool task_set_jobctl_pending(struct task_struct *task, unsigned int mask)</span>
<span class="p_add">+bool task_set_jobctl_pending(struct task_struct *task, unsigned long mask)</span>
 {
 	BUG_ON(mask &amp; ~(JOBCTL_PENDING_MASK | JOBCTL_STOP_CONSUME |
 			JOBCTL_STOP_SIGMASK | JOBCTL_TRAPPING));
<span class="p_chunk">@@ -297,7 +297,7 @@</span> <span class="p_context"> void task_clear_jobctl_trapping(struct task_struct *task)</span>
  * CONTEXT:
  * Must be called with @task-&gt;sighand-&gt;siglock held.
  */
<span class="p_del">-void task_clear_jobctl_pending(struct task_struct *task, unsigned int mask)</span>
<span class="p_add">+void task_clear_jobctl_pending(struct task_struct *task, unsigned long mask)</span>
 {
 	BUG_ON(mask &amp; ~JOBCTL_PENDING_MASK);
 
<span class="p_chunk">@@ -2000,7 +2000,7 @@</span> <span class="p_context"> static bool do_signal_stop(int signr)</span>
 	struct signal_struct *sig = current-&gt;signal;
 
 	if (!(current-&gt;jobctl &amp; JOBCTL_STOP_PENDING)) {
<span class="p_del">-		unsigned int gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;</span>
<span class="p_add">+		unsigned long gstop = JOBCTL_STOP_PENDING | JOBCTL_STOP_CONSUME;</span>
 		struct task_struct *t;
 
 		/* signr will be recorded in task-&gt;jobctl for retries */
<span class="p_header">diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c</span>
<span class="p_header">index 695f0c6cd169..fd643d8c4b42 100644</span>
<span class="p_header">--- a/kernel/stop_machine.c</span>
<span class="p_header">+++ b/kernel/stop_machine.c</span>
<span class="p_chunk">@@ -211,25 +211,6 @@</span> <span class="p_context"> static int multi_cpu_stop(void *data)</span>
 	return err;
 }
 
<span class="p_del">-struct irq_cpu_stop_queue_work_info {</span>
<span class="p_del">-	int cpu1;</span>
<span class="p_del">-	int cpu2;</span>
<span class="p_del">-	struct cpu_stop_work *work1;</span>
<span class="p_del">-	struct cpu_stop_work *work2;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * This function is always run with irqs and preemption disabled.</span>
<span class="p_del">- * This guarantees that both work1 and work2 get queued, before</span>
<span class="p_del">- * our local migrate thread gets the chance to preempt us.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void irq_cpu_stop_queue_work(void *arg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct irq_cpu_stop_queue_work_info *info = arg;</span>
<span class="p_del">-	cpu_stop_queue_work(info-&gt;cpu1, info-&gt;work1);</span>
<span class="p_del">-	cpu_stop_queue_work(info-&gt;cpu2, info-&gt;work2);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * stop_two_cpus - stops two cpus
  * @cpu1: the cpu to stop
<span class="p_chunk">@@ -245,7 +226,6 @@</span> <span class="p_context"> int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *</span>
 {
 	struct cpu_stop_done done;
 	struct cpu_stop_work work1, work2;
<span class="p_del">-	struct irq_cpu_stop_queue_work_info call_args;</span>
 	struct multi_stop_data msdata;
 
 	preempt_disable();
<span class="p_chunk">@@ -262,13 +242,6 @@</span> <span class="p_context"> int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *</span>
 		.done = &amp;done
 	};
 
<span class="p_del">-	call_args = (struct irq_cpu_stop_queue_work_info){</span>
<span class="p_del">-		.cpu1 = cpu1,</span>
<span class="p_del">-		.cpu2 = cpu2,</span>
<span class="p_del">-		.work1 = &amp;work1,</span>
<span class="p_del">-		.work2 = &amp;work2,</span>
<span class="p_del">-	};</span>
<span class="p_del">-</span>
 	cpu_stop_init_done(&amp;done, 2);
 	set_state(&amp;msdata, MULTI_STOP_PREPARE);
 
<span class="p_chunk">@@ -285,16 +258,11 @@</span> <span class="p_context"> int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *</span>
 		return -ENOENT;
 	}
 
<span class="p_del">-	lg_local_lock(&amp;stop_cpus_lock);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Queuing needs to be done by the lowest numbered CPU, to ensure</span>
<span class="p_del">-	 * that works are always queued in the same order on every CPU.</span>
<span class="p_del">-	 * This prevents deadlocks.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	smp_call_function_single(min(cpu1, cpu2),</span>
<span class="p_del">-				 &amp;irq_cpu_stop_queue_work,</span>
<span class="p_del">-				 &amp;call_args, 1);</span>
<span class="p_del">-	lg_local_unlock(&amp;stop_cpus_lock);</span>
<span class="p_add">+	lg_double_lock(&amp;stop_cpus_lock, cpu1, cpu2);</span>
<span class="p_add">+	cpu_stop_queue_work(cpu1, &amp;work1);</span>
<span class="p_add">+	cpu_stop_queue_work(cpu2, &amp;work2);</span>
<span class="p_add">+	lg_double_unlock(&amp;stop_cpus_lock, cpu1, cpu2);</span>
<span class="p_add">+</span>
 	preempt_enable();
 
 	wait_for_completion(&amp;done.completion);
<span class="p_header">diff --git a/kernel/time/posix-cpu-timers.c b/kernel/time/posix-cpu-timers.c</span>
<span class="p_header">index 0075da74abf0..892e3dae0aac 100644</span>
<span class="p_header">--- a/kernel/time/posix-cpu-timers.c</span>
<span class="p_header">+++ b/kernel/time/posix-cpu-timers.c</span>
<span class="p_chunk">@@ -196,39 +196,62 @@</span> <span class="p_context"> static int cpu_clock_sample(const clockid_t which_clock, struct task_struct *p,</span>
 	return 0;
 }
 
<span class="p_del">-static void update_gt_cputime(struct task_cputime *a, struct task_cputime *b)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Set cputime to sum_cputime if sum_cputime &gt; cputime. Use cmpxchg</span>
<span class="p_add">+ * to avoid race conditions with concurrent updates to cputime.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __update_gt_cputime(atomic64_t *cputime, u64 sum_cputime)</span>
 {
<span class="p_del">-	if (b-&gt;utime &gt; a-&gt;utime)</span>
<span class="p_del">-		a-&gt;utime = b-&gt;utime;</span>
<span class="p_add">+	u64 curr_cputime;</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	curr_cputime = atomic64_read(cputime);</span>
<span class="p_add">+	if (sum_cputime &gt; curr_cputime) {</span>
<span class="p_add">+		if (atomic64_cmpxchg(cputime, curr_cputime, sum_cputime) != curr_cputime)</span>
<span class="p_add">+			goto retry;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (b-&gt;stime &gt; a-&gt;stime)</span>
<span class="p_del">-		a-&gt;stime = b-&gt;stime;</span>
<span class="p_add">+static void update_gt_cputime(struct task_cputime_atomic *cputime_atomic, struct task_cputime *sum)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__update_gt_cputime(&amp;cputime_atomic-&gt;utime, sum-&gt;utime);</span>
<span class="p_add">+	__update_gt_cputime(&amp;cputime_atomic-&gt;stime, sum-&gt;stime);</span>
<span class="p_add">+	__update_gt_cputime(&amp;cputime_atomic-&gt;sum_exec_runtime, sum-&gt;sum_exec_runtime);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (b-&gt;sum_exec_runtime &gt; a-&gt;sum_exec_runtime)</span>
<span class="p_del">-		a-&gt;sum_exec_runtime = b-&gt;sum_exec_runtime;</span>
<span class="p_add">+/* Sample task_cputime_atomic values in &quot;atomic_timers&quot;, store results in &quot;times&quot;. */</span>
<span class="p_add">+static inline void sample_cputime_atomic(struct task_cputime *times,</span>
<span class="p_add">+					 struct task_cputime_atomic *atomic_times)</span>
<span class="p_add">+{</span>
<span class="p_add">+	times-&gt;utime = atomic64_read(&amp;atomic_times-&gt;utime);</span>
<span class="p_add">+	times-&gt;stime = atomic64_read(&amp;atomic_times-&gt;stime);</span>
<span class="p_add">+	times-&gt;sum_exec_runtime = atomic64_read(&amp;atomic_times-&gt;sum_exec_runtime);</span>
 }
 
 void thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)
 {
 	struct thread_group_cputimer *cputimer = &amp;tsk-&gt;signal-&gt;cputimer;
 	struct task_cputime sum;
<span class="p_del">-	unsigned long flags;</span>
 
<span class="p_del">-	if (!cputimer-&gt;running) {</span>
<span class="p_add">+	/* Check if cputimer isn&#39;t running. This is accessed without locking. */</span>
<span class="p_add">+	if (!READ_ONCE(cputimer-&gt;running)) {</span>
 		/*
 		 * The POSIX timer interface allows for absolute time expiry
 		 * values through the TIMER_ABSTIME flag, therefore we have
<span class="p_del">-		 * to synchronize the timer to the clock every time we start</span>
<span class="p_del">-		 * it.</span>
<span class="p_add">+		 * to synchronize the timer to the clock every time we start it.</span>
 		 */
 		thread_group_cputime(tsk, &amp;sum);
<span class="p_del">-		raw_spin_lock_irqsave(&amp;cputimer-&gt;lock, flags);</span>
<span class="p_del">-		cputimer-&gt;running = 1;</span>
<span class="p_del">-		update_gt_cputime(&amp;cputimer-&gt;cputime, &amp;sum);</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		raw_spin_lock_irqsave(&amp;cputimer-&gt;lock, flags);</span>
<span class="p_del">-	*times = cputimer-&gt;cputime;</span>
<span class="p_del">-	raw_spin_unlock_irqrestore(&amp;cputimer-&gt;lock, flags);</span>
<span class="p_add">+		update_gt_cputime(&amp;cputimer-&gt;cputime_atomic, &amp;sum);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We&#39;re setting cputimer-&gt;running without a lock. Ensure</span>
<span class="p_add">+		 * this only gets written to in one operation. We set</span>
<span class="p_add">+		 * running after update_gt_cputime() as a small optimization,</span>
<span class="p_add">+		 * but barriers are not required because update_gt_cputime()</span>
<span class="p_add">+		 * can handle concurrent updates.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WRITE_ONCE(cputimer-&gt;running, 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	sample_cputime_atomic(times, &amp;cputimer-&gt;cputime_atomic);</span>
 }
 
 /*
<span class="p_chunk">@@ -582,7 +605,8 @@</span> <span class="p_context"> bool posix_cpu_timers_can_stop_tick(struct task_struct *tsk)</span>
 	if (!task_cputime_zero(&amp;tsk-&gt;cputime_expires))
 		return false;
 
<span class="p_del">-	if (tsk-&gt;signal-&gt;cputimer.running)</span>
<span class="p_add">+	/* Check if cputimer is running. This is accessed without locking. */</span>
<span class="p_add">+	if (READ_ONCE(tsk-&gt;signal-&gt;cputimer.running))</span>
 		return false;
 
 	return true;
<span class="p_chunk">@@ -852,10 +876,10 @@</span> <span class="p_context"> static void check_thread_timers(struct task_struct *tsk,</span>
 	/*
 	 * Check for the special case thread timers.
 	 */
<span class="p_del">-	soft = ACCESS_ONCE(sig-&gt;rlim[RLIMIT_RTTIME].rlim_cur);</span>
<span class="p_add">+	soft = READ_ONCE(sig-&gt;rlim[RLIMIT_RTTIME].rlim_cur);</span>
 	if (soft != RLIM_INFINITY) {
 		unsigned long hard =
<span class="p_del">-			ACCESS_ONCE(sig-&gt;rlim[RLIMIT_RTTIME].rlim_max);</span>
<span class="p_add">+			READ_ONCE(sig-&gt;rlim[RLIMIT_RTTIME].rlim_max);</span>
 
 		if (hard != RLIM_INFINITY &amp;&amp;
 		    tsk-&gt;rt.timeout &gt; DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {
<span class="p_chunk">@@ -882,14 +906,12 @@</span> <span class="p_context"> static void check_thread_timers(struct task_struct *tsk,</span>
 	}
 }
 
<span class="p_del">-static void stop_process_timers(struct signal_struct *sig)</span>
<span class="p_add">+static inline void stop_process_timers(struct signal_struct *sig)</span>
 {
 	struct thread_group_cputimer *cputimer = &amp;sig-&gt;cputimer;
<span class="p_del">-	unsigned long flags;</span>
 
<span class="p_del">-	raw_spin_lock_irqsave(&amp;cputimer-&gt;lock, flags);</span>
<span class="p_del">-	cputimer-&gt;running = 0;</span>
<span class="p_del">-	raw_spin_unlock_irqrestore(&amp;cputimer-&gt;lock, flags);</span>
<span class="p_add">+	/* Turn off cputimer-&gt;running. This is done without locking. */</span>
<span class="p_add">+	WRITE_ONCE(cputimer-&gt;running, 0);</span>
 }
 
 static u32 onecputick;
<span class="p_chunk">@@ -958,11 +980,11 @@</span> <span class="p_context"> static void check_process_timers(struct task_struct *tsk,</span>
 			 SIGPROF);
 	check_cpu_itimer(tsk, &amp;sig-&gt;it[CPUCLOCK_VIRT], &amp;virt_expires, utime,
 			 SIGVTALRM);
<span class="p_del">-	soft = ACCESS_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_cur);</span>
<span class="p_add">+	soft = READ_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_cur);</span>
 	if (soft != RLIM_INFINITY) {
 		unsigned long psecs = cputime_to_secs(ptime);
 		unsigned long hard =
<span class="p_del">-			ACCESS_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_max);</span>
<span class="p_add">+			READ_ONCE(sig-&gt;rlim[RLIMIT_CPU].rlim_max);</span>
 		cputime_t x;
 		if (psecs &gt;= hard) {
 			/*
<span class="p_chunk">@@ -1111,12 +1133,11 @@</span> <span class="p_context"> static inline int fastpath_timer_check(struct task_struct *tsk)</span>
 	}
 
 	sig = tsk-&gt;signal;
<span class="p_del">-	if (sig-&gt;cputimer.running) {</span>
<span class="p_add">+	/* Check if cputimer is running. This is accessed without locking. */</span>
<span class="p_add">+	if (READ_ONCE(sig-&gt;cputimer.running)) {</span>
 		struct task_cputime group_sample;
 
<span class="p_del">-		raw_spin_lock(&amp;sig-&gt;cputimer.lock);</span>
<span class="p_del">-		group_sample = sig-&gt;cputimer.cputime;</span>
<span class="p_del">-		raw_spin_unlock(&amp;sig-&gt;cputimer.lock);</span>
<span class="p_add">+		sample_cputime_atomic(&amp;group_sample, &amp;sig-&gt;cputimer.cputime_atomic);</span>
 
 		if (task_cputime_expired(&amp;group_sample, &amp;sig-&gt;cputime_expires))
 			return 1;
<span class="p_chunk">@@ -1157,7 +1178,7 @@</span> <span class="p_context"> void run_posix_cpu_timers(struct task_struct *tsk)</span>
 	 * If there are any active process wide timers (POSIX 1.b, itimers,
 	 * RLIMIT_CPU) cputimer must be running.
 	 */
<span class="p_del">-	if (tsk-&gt;signal-&gt;cputimer.running)</span>
<span class="p_add">+	if (READ_ONCE(tsk-&gt;signal-&gt;cputimer.running))</span>
 		check_process_timers(tsk, &amp;firing);
 
 	/*
<span class="p_header">diff --git a/lib/cpu_rmap.c b/lib/cpu_rmap.c</span>
<span class="p_header">index 4f134d8907a7..f610b2a10b3e 100644</span>
<span class="p_header">--- a/lib/cpu_rmap.c</span>
<span class="p_header">+++ b/lib/cpu_rmap.c</span>
<span class="p_chunk">@@ -191,7 +191,7 @@</span> <span class="p_context"> int cpu_rmap_update(struct cpu_rmap *rmap, u16 index,</span>
 	/* Update distances based on topology */
 	for_each_cpu(cpu, update_mask) {
 		if (cpu_rmap_copy_neigh(rmap, cpu,
<span class="p_del">-					topology_thread_cpumask(cpu), 1))</span>
<span class="p_add">+					topology_sibling_cpumask(cpu), 1))</span>
 			continue;
 		if (cpu_rmap_copy_neigh(rmap, cpu,
 					topology_core_cpumask(cpu), 2))
<span class="p_header">diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="p_header">index 3d2aa27b845b..061550de77bc 100644</span>
<span class="p_header">--- a/lib/radix-tree.c</span>
<span class="p_header">+++ b/lib/radix-tree.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/string.h&gt;
 #include &lt;linux/bitops.h&gt;
 #include &lt;linux/rcupdate.h&gt;
<span class="p_del">-#include &lt;linux/preempt_mask.h&gt;		/* in_interrupt() */</span>
<span class="p_add">+#include &lt;linux/preempt.h&gt;		/* in_interrupt() */</span>
 
 
 /*
<span class="p_header">diff --git a/lib/strnlen_user.c b/lib/strnlen_user.c</span>
<span class="p_header">index a28df5206d95..36c15a2889e4 100644</span>
<span class="p_header">--- a/lib/strnlen_user.c</span>
<span class="p_header">+++ b/lib/strnlen_user.c</span>
<span class="p_chunk">@@ -84,7 +84,8 @@</span> <span class="p_context"> static inline long do_strnlen_user(const char __user *src, unsigned long count,</span>
  * @str: The string to measure.
  * @count: Maximum count (including NUL character)
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_chunk">@@ -113,7 +114,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(strnlen_user);</span>
  * strlen_user: - Get the size of a user string INCLUDING final NUL.
  * @str: The string to measure.
  *
<span class="p_del">- * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ * Context: User context only. This function may sleep if pagefaults are</span>
<span class="p_add">+ *          enabled.</span>
  *
  * Get the size of a NUL-terminated string in user space.
  *
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 22e037e3364e..17734c3c1183 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -3737,7 +3737,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 }
 
 #if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)
<span class="p_del">-void might_fault(void)</span>
<span class="p_add">+void __might_fault(const char *file, int line)</span>
 {
 	/*
 	 * Some code (nfs/sunrpc) uses socket ops on kernel memory while
<span class="p_chunk">@@ -3747,21 +3747,15 @@</span> <span class="p_context"> void might_fault(void)</span>
 	 */
 	if (segment_eq(get_fs(), KERNEL_DS))
 		return;
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * it would be nicer only to annotate paths which are not under</span>
<span class="p_del">-	 * pagefault_disable, however that requires a larger audit and</span>
<span class="p_del">-	 * providing helpers like get_user_atomic.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (in_atomic())</span>
<span class="p_add">+	if (pagefault_disabled())</span>
 		return;
<span class="p_del">-</span>
<span class="p_del">-	__might_sleep(__FILE__, __LINE__, 0);</span>
<span class="p_del">-</span>
<span class="p_add">+	__might_sleep(file, line, 0);</span>
<span class="p_add">+#if defined(CONFIG_DEBUG_ATOMIC_SLEEP)</span>
 	if (current-&gt;mm)
 		might_lock_read(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_add">+#endif</span>
 }
<span class="p_del">-EXPORT_SYMBOL(might_fault);</span>
<span class="p_add">+EXPORT_SYMBOL(__might_fault);</span>
 #endif
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



