
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/fpu changes for v4.10 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/fpu changes for v4.10</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 12, 2016, 9:48 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161212094818.GA19630@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9470297/mbox/"
   >mbox</a>
|
   <a href="/patch/9470297/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9470297/">/patch/9470297/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BB00660573 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Dec 2016 09:48:38 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AE475283A6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Dec 2016 09:48:38 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A0A3C28460; Mon, 12 Dec 2016 09:48:38 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A9998283A6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Dec 2016 09:48:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932312AbcLLJs1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 12 Dec 2016 04:48:27 -0500
Received: from mail-wm0-f65.google.com ([74.125.82.65]:34823 &quot;EHLO
	mail-wm0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753679AbcLLJsZ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 12 Dec 2016 04:48:25 -0500
Received: by mail-wm0-f65.google.com with SMTP id a20so10051824wme.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 12 Dec 2016 01:48:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=hJ2Qlx+tbA0DRUWVfDVDjZ3bKWBDOnw+FfLcdq/ae+g=;
	b=VC6ZxOXQMEUxzeqOr0Oh3h+NZtEyVc98tmaDSGArBSfBxOrY4yt5/5ikHzNSoMwqS8
	QzqaWsQouvnqBovbTdVq1ZF8K+5oha+8dDaphtFnPQkFu4y4NOQjD2xYkqrN3oUi5/FU
	7BIL/L+ahqsSaZxX8TEdBZTR/Vy7ltbDHKFpEKZ+GBjG92oNw1R/iCwytyayhhiU61bB
	qRvYiT8aZpJPoyzhqM9XLRPmbU597E3gIvkZsw7TjQ3Var5QNMWSw3OW1/q83z9lkUki
	p+IgsSFV3D9Wo8AOM6owV/MTyw2O9W372VDJngN0k4cr8qVoBzkgOfvEDs2Kh9ecWfSe
	/s5w==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=hJ2Qlx+tbA0DRUWVfDVDjZ3bKWBDOnw+FfLcdq/ae+g=;
	b=ADHuygbhezF24jN3Bx90SWKrsvILWCQCT5lNssMFhzfdUb3DSP+JoAHFF1ItcfqLZz
	QatW40NktKgR57iNe9j0WbxCoTw+nVqxrVxFhdOZbpDL/f1pR/n05pKDXhBL87jsPGCZ
	a7+BtnDGS5gNiCj35msEi4e6Kb3VVGGkoj4w/6O7RA5rI34zFivOwMe2X0PW+72zLRlQ
	sK0gkjUWCMRFtjPKtBnTNun8MJtmDnsdPAmDpU3jLmDVymywDJxIYTU2KQJqk7IJyQ8Y
	qb9s4VXy3EbXiQ7QIWINyhfNs9gAS1KpJ3ehKsK4iPcXsQz5ozPL7WF4i9JxiJMqFR6c
	VDHA==
X-Gm-Message-State: AKaTC033G4leMwyosvhsQg46dxochelx0SyJytu/Pm5d7/TD+9pM5Ilnnvzm57bc+u7Hrg==
X-Received: by 10.28.129.81 with SMTP id c78mr17695456wmd.94.1481536102213; 
	Mon, 12 Dec 2016 01:48:22 -0800 (PST)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	l6sm34568982wmd.5.2016.12.12.01.48.20
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 12 Dec 2016 01:48:20 -0800 (PST)
Date: Mon, 12 Dec 2016 10:48:18 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86/fpu changes for v4.10
Message-ID: &lt;20161212094818.GA19630@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Dec. 12, 2016, 9:48 a.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-fpu-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-fpu-for-linus

   # HEAD: 064e6a8ba61a751625478f656c6f76a6f37a009e Merge branch &#39;linus&#39; into x86/fpu, to resolve conflicts

The main changes in this cycle were:

 - Do a large round of simplifications after all CPUs do &#39;eager&#39; FPU context 
   switching in v4.9: remove CR0 twiddling, remove leftover eager/lazy bts, etc. 
   (Andy Lutomirski)

 - More FPU code simplifications: remov struct fpu::counter, clarify nomenclature, 
   remove unnecessary arguments/functions and better structure the code.
   (Rik van Riel)

  out-of-topic modifications in x86-fpu-for-linus:
  --------------------------------------------------
  drivers/char/hw_random/via-rng.c   # 5a83d60c074d: x86/fpu: Remove irq_ts_save(
  drivers/crypto/padlock-aes.c       # 5a83d60c074d: x86/fpu: Remove irq_ts_save(
  drivers/crypto/padlock-sha.c       # 5a83d60c074d: x86/fpu: Remove irq_ts_save(
  drivers/lguest/hypercalls.c        # cd95ea81f256: x86/fpu, lguest: Remove CR0.
  drivers/lguest/lg.h                # cd95ea81f256: x86/fpu, lguest: Remove CR0.
  drivers/lguest/x86/core.c          # cd95ea81f256: x86/fpu, lguest: Remove CR0.
  include/linux/kvm_host.h           # 3d42de25d290: x86/fpu, kvm: Remove KVM vcp

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (13):
      x86/crypto, x86/fpu: Remove X86_FEATURE_EAGER_FPU #ifdef from the crc32c code
      x86/fpu: Hard-disable lazy FPU mode
      x86/fpu: Remove the XFEATURE_MASK_EAGER/LAZY distinction
      x86/fpu: Remove use_eager_fpu()
      x86/fpu: Finish excising &#39;eagerfpu&#39;
      x86/fpu: Get rid of two redundant clts() calls
      x86/fpu: Stop saving and restoring CR0.TS in fpu__init_check_bugs()
      x86/fpu: Remove irq_ts_save() and irq_ts_restore()
      x86/fpu, kvm: Remove host CR0.TS manipulation
      x86/fpu, lguest: Remove CR0.TS support
      x86/fpu: Handle #NM without FPU emulation as an error
      x86/fpu: Remove stts()
      x86/fpu: Remove clts()

Rik van Riel (7):
      x86/fpu: Remove struct fpu::counter
      x86/fpu, kvm: Remove KVM vcpu-&gt;fpu_counter
      x86/fpu: Rename lazy restore functions to &quot;register state valid&quot;
      x86/fpu: Remove __fpregs_(de)activate()
      x86/fpu: Split old &amp; new FPU code paths
      x86/fpu: Remove &#39;cpu&#39; argument from __cpu_invalidate_fpregs_state()
      x86/fpu: Split old_fpu &amp; new_fpu handling into separate functions


 Documentation/kernel-parameters.txt      |   6 --
 arch/x86/crypto/crc32c-intel_glue.c      |  22 +----
 arch/x86/include/asm/cpufeatures.h       |   1 -
 arch/x86/include/asm/fpu/api.h           |  10 ---
 arch/x86/include/asm/fpu/internal.h      | 139 ++++++++++---------------------
 arch/x86/include/asm/fpu/types.h         |  34 --------
 arch/x86/include/asm/fpu/xstate.h        |  17 ++--
 arch/x86/include/asm/lguest_hcall.h      |   1 -
 arch/x86/include/asm/paravirt.h          |   5 --
 arch/x86/include/asm/paravirt_types.h    |   2 -
 arch/x86/include/asm/special_insns.h     |  13 ---
 arch/x86/include/asm/trace/fpu.h         |   5 +-
 arch/x86/kernel/fpu/bugs.c               |   7 --
 arch/x86/kernel/fpu/core.c               |  74 ++--------------
 arch/x86/kernel/fpu/init.c               | 107 +-----------------------
 arch/x86/kernel/fpu/signal.c             |   8 +-
 arch/x86/kernel/fpu/xstate.c             |   9 --
 arch/x86/kernel/paravirt.c               |   1 -
 arch/x86/kernel/paravirt_patch_32.c      |   2 -
 arch/x86/kernel/paravirt_patch_64.c      |   2 -
 arch/x86/kernel/process_32.c             |   5 +-
 arch/x86/kernel/process_64.c             |   5 +-
 arch/x86/kernel/smpboot.c                |   2 +-
 arch/x86/kernel/traps.c                  |  20 ++++-
 arch/x86/kvm/cpuid.c                     |   4 +-
 arch/x86/kvm/vmx.c                       |  12 +--
 arch/x86/kvm/x86.c                       |  19 +----
 arch/x86/lguest/boot.c                   |  29 ++-----
 arch/x86/mm/pkeys.c                      |   3 +-
 arch/x86/xen/enlighten.c                 |  13 ---
 drivers/char/hw_random/via-rng.c         |   8 +-
 drivers/crypto/padlock-aes.c             |  23 +----
 drivers/crypto/padlock-sha.c             |  18 ----
 drivers/lguest/hypercalls.c              |   4 -
 drivers/lguest/lg.h                      |   1 -
 drivers/lguest/x86/core.c                |  19 +----
 include/linux/kvm_host.h                 |   1 -
 tools/arch/x86/include/asm/cpufeatures.h |   1 -
 38 files changed, 105 insertions(+), 547 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index 37babf91f2cb..459b301137c2 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -1074,12 +1074,6 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	nopku		[X86] Disable Memory Protection Keys CPU feature found
 			in some Intel CPUs.
 
<span class="p_del">-	eagerfpu=	[X86]</span>
<span class="p_del">-			on	enable eager fpu restore</span>
<span class="p_del">-			off	disable eager fpu restore</span>
<span class="p_del">-			auto	selects the default scheme, which automatically</span>
<span class="p_del">-				enables eagerfpu restore for xsaveopt.</span>
<span class="p_del">-</span>
 	module.async_probe [KNL]
 			Enable asynchronous probe on this module.
 
<span class="p_header">diff --git a/arch/x86/crypto/crc32c-intel_glue.c b/arch/x86/crypto/crc32c-intel_glue.c</span>
<span class="p_header">index 0857b1a1de3b..c194d5717ae5 100644</span>
<span class="p_header">--- a/arch/x86/crypto/crc32c-intel_glue.c</span>
<span class="p_header">+++ b/arch/x86/crypto/crc32c-intel_glue.c</span>
<span class="p_chunk">@@ -48,26 +48,13 @@</span> <span class="p_context"></span>
 #ifdef CONFIG_X86_64
 /*
  * use carryless multiply version of crc32c when buffer
<span class="p_del">- * size is &gt;= 512 (when eager fpu is enabled) or</span>
<span class="p_del">- * &gt;= 1024 (when eager fpu is disabled) to account</span>
<span class="p_add">+ * size is &gt;= 512 to account</span>
  * for fpu state save/restore overhead.
  */
<span class="p_del">-#define CRC32C_PCL_BREAKEVEN_EAGERFPU	512</span>
<span class="p_del">-#define CRC32C_PCL_BREAKEVEN_NOEAGERFPU	1024</span>
<span class="p_add">+#define CRC32C_PCL_BREAKEVEN	512</span>
 
 asmlinkage unsigned int crc_pcl(const u8 *buffer, int len,
 				unsigned int crc_init);
<span class="p_del">-static int crc32c_pcl_breakeven = CRC32C_PCL_BREAKEVEN_EAGERFPU;</span>
<span class="p_del">-#if defined(X86_FEATURE_EAGER_FPU)</span>
<span class="p_del">-#define set_pcl_breakeven_point()					\</span>
<span class="p_del">-do {									\</span>
<span class="p_del">-	if (!use_eager_fpu())						\</span>
<span class="p_del">-		crc32c_pcl_breakeven = CRC32C_PCL_BREAKEVEN_NOEAGERFPU;	\</span>
<span class="p_del">-} while (0)</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define set_pcl_breakeven_point()					\</span>
<span class="p_del">-	(crc32c_pcl_breakeven = CRC32C_PCL_BREAKEVEN_NOEAGERFPU)</span>
<span class="p_del">-#endif</span>
 #endif /* CONFIG_X86_64 */
 
 static u32 crc32c_intel_le_hw_byte(u32 crc, unsigned char const *data, size_t length)
<span class="p_chunk">@@ -190,7 +177,7 @@</span> <span class="p_context"> static int crc32c_pcl_intel_update(struct shash_desc *desc, const u8 *data,</span>
 	 * use faster PCL version if datasize is large enough to
 	 * overcome kernel fpu state save/restore overhead
 	 */
<span class="p_del">-	if (len &gt;= crc32c_pcl_breakeven &amp;&amp; irq_fpu_usable()) {</span>
<span class="p_add">+	if (len &gt;= CRC32C_PCL_BREAKEVEN &amp;&amp; irq_fpu_usable()) {</span>
 		kernel_fpu_begin();
 		*crcp = crc_pcl(data, len, *crcp);
 		kernel_fpu_end();
<span class="p_chunk">@@ -202,7 +189,7 @@</span> <span class="p_context"> static int crc32c_pcl_intel_update(struct shash_desc *desc, const u8 *data,</span>
 static int __crc32c_pcl_intel_finup(u32 *crcp, const u8 *data, unsigned int len,
 				u8 *out)
 {
<span class="p_del">-	if (len &gt;= crc32c_pcl_breakeven &amp;&amp; irq_fpu_usable()) {</span>
<span class="p_add">+	if (len &gt;= CRC32C_PCL_BREAKEVEN &amp;&amp; irq_fpu_usable()) {</span>
 		kernel_fpu_begin();
 		*(__le32 *)out = ~cpu_to_le32(crc_pcl(data, len, *crcp));
 		kernel_fpu_end();
<span class="p_chunk">@@ -261,7 +248,6 @@</span> <span class="p_context"> static int __init crc32c_intel_mod_init(void)</span>
 		alg.update = crc32c_pcl_intel_update;
 		alg.finup = crc32c_pcl_intel_finup;
 		alg.digest = crc32c_pcl_intel_digest;
<span class="p_del">-		set_pcl_breakeven_point();</span>
 	}
 #endif
 	return crypto_register_shash(&amp;alg);
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index a39629206864..cddd5d06e1cb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -104,7 +104,6 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_EXTD_APICID	( 3*32+26) /* has extended APICID (8 bits) */
 #define X86_FEATURE_AMD_DCM     ( 3*32+27) /* multi-node processor */
 #define X86_FEATURE_APERFMPERF	( 3*32+28) /* APERFMPERF */
<span class="p_del">-#define X86_FEATURE_EAGER_FPU	( 3*32+29) /* &quot;eagerfpu&quot; Non lazy FPU restore */</span>
 #define X86_FEATURE_NONSTOP_TSC_S3 ( 3*32+30) /* TSC doesn&#39;t stop in S3 state */
 
 /* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h</span>
<span class="p_header">index 1429a7c736db..0877ae018fc9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/api.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/api.h</span>
<span class="p_chunk">@@ -27,16 +27,6 @@</span> <span class="p_context"> extern void kernel_fpu_end(void);</span>
 extern bool irq_fpu_usable(void);
 
 /*
<span class="p_del">- * Some instructions like VIA&#39;s padlock instructions generate a spurious</span>
<span class="p_del">- * DNA fault but don&#39;t modify SSE registers. And these instructions</span>
<span class="p_del">- * get used from interrupt context as well. To prevent these kernel instructions</span>
<span class="p_del">- * in interrupt context interacting wrongly with other user/kernel fpu usage, we</span>
<span class="p_del">- * should use them only in the context of irq_ts_save/restore()</span>
<span class="p_del">- */</span>
<span class="p_del">-extern int  irq_ts_save(void);</span>
<span class="p_del">-extern void irq_ts_restore(int TS_state);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Query the presence of one or more xfeatures. Works on any legacy CPU as well.
  *
  * If &#39;feature_name&#39; is set then put a human-readable description of
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_header">index 2737366ea583..d4a684997497 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_chunk">@@ -60,11 +60,6 @@</span> <span class="p_context"> extern u64 fpu__get_supported_xfeatures_mask(void);</span>
 /*
  * FPU related CPU feature flag helper routines:
  */
<span class="p_del">-static __always_inline __pure bool use_eager_fpu(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return static_cpu_has(X86_FEATURE_EAGER_FPU);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static __always_inline __pure bool use_xsaveopt(void)
 {
 	return static_cpu_has(X86_FEATURE_XSAVEOPT);
<span class="p_chunk">@@ -484,42 +479,42 @@</span> <span class="p_context"> extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size)</span>
 DECLARE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);
 
 /*
<span class="p_del">- * Must be run with preemption disabled: this clears the fpu_fpregs_owner_ctx,</span>
<span class="p_del">- * on this CPU.</span>
<span class="p_add">+ * The in-register FPU state for an FPU context on a CPU is assumed to be</span>
<span class="p_add">+ * valid if the fpu-&gt;last_cpu matches the CPU, and the fpu_fpregs_owner_ctx</span>
<span class="p_add">+ * matches the FPU.</span>
  *
<span class="p_del">- * This will disable any lazy FPU state restore of the current FPU state,</span>
<span class="p_del">- * but if the current thread owns the FPU, it will still be saved by.</span>
<span class="p_add">+ * If the FPU register state is valid, the kernel can skip restoring the</span>
<span class="p_add">+ * FPU state from memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Any code that clobbers the FPU registers or updates the in-memory</span>
<span class="p_add">+ * FPU state for a task MUST let the rest of the kernel know that the</span>
<span class="p_add">+ * FPU registers are no longer valid for this task.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Either one of these invalidation functions is enough. Invalidate</span>
<span class="p_add">+ * a resource you control: CPU if using the CPU for something else</span>
<span class="p_add">+ * (with preemption disabled), FPU for the current task, or a task that</span>
<span class="p_add">+ * is prevented from running by the current task.</span>
  */
<span class="p_del">-static inline void __cpu_disable_lazy_restore(unsigned int cpu)</span>
<span class="p_add">+static inline void __cpu_invalidate_fpregs_state(void)</span>
 {
<span class="p_del">-	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;</span>
<span class="p_add">+	__this_cpu_write(fpu_fpregs_owner_ctx, NULL);</span>
 }
 
<span class="p_del">-static inline int fpu_want_lazy_restore(struct fpu *fpu, unsigned int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) &amp;&amp; cpu == fpu-&gt;last_cpu;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Wrap lazy FPU TS handling in a &#39;hw fpregs activation/deactivation&#39;</span>
<span class="p_del">- * idiom, which is then paired with the sw-flag (fpregs_active) later on:</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __fpregs_activate_hw(void)</span>
<span class="p_add">+static inline void __fpu_invalidate_fpregs_state(struct fpu *fpu)</span>
 {
<span class="p_del">-	if (!use_eager_fpu())</span>
<span class="p_del">-		clts();</span>
<span class="p_add">+	fpu-&gt;last_cpu = -1;</span>
 }
 
<span class="p_del">-static inline void __fpregs_deactivate_hw(void)</span>
<span class="p_add">+static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)</span>
 {
<span class="p_del">-	if (!use_eager_fpu())</span>
<span class="p_del">-		stts();</span>
<span class="p_add">+	return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) &amp;&amp; cpu == fpu-&gt;last_cpu;</span>
 }
 
<span class="p_del">-/* Must be paired with an &#39;stts&#39; (fpregs_deactivate_hw()) after! */</span>
<span class="p_del">-static inline void __fpregs_deactivate(struct fpu *fpu)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These generally need preemption protection to work,</span>
<span class="p_add">+ * do try to avoid using these on their own:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void fpregs_deactivate(struct fpu *fpu)</span>
 {
 	WARN_ON_FPU(!fpu-&gt;fpregs_active);
 
<span class="p_chunk">@@ -528,8 +523,7 @@</span> <span class="p_context"> static inline void __fpregs_deactivate(struct fpu *fpu)</span>
 	trace_x86_fpu_regs_deactivated(fpu);
 }
 
<span class="p_del">-/* Must be paired with a &#39;clts&#39; (fpregs_activate_hw()) before! */</span>
<span class="p_del">-static inline void __fpregs_activate(struct fpu *fpu)</span>
<span class="p_add">+static inline void fpregs_activate(struct fpu *fpu)</span>
 {
 	WARN_ON_FPU(fpu-&gt;fpregs_active);
 
<span class="p_chunk">@@ -554,51 +548,19 @@</span> <span class="p_context"> static inline int fpregs_active(void)</span>
 }
 
 /*
<span class="p_del">- * Encapsulate the CR0.TS handling together with the</span>
<span class="p_del">- * software flag.</span>
<span class="p_del">- *</span>
<span class="p_del">- * These generally need preemption protection to work,</span>
<span class="p_del">- * do try to avoid using these on their own.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void fpregs_activate(struct fpu *fpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__fpregs_activate_hw();</span>
<span class="p_del">-	__fpregs_activate(fpu);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void fpregs_deactivate(struct fpu *fpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__fpregs_deactivate(fpu);</span>
<span class="p_del">-	__fpregs_deactivate_hw();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * FPU state switching for scheduling.
  *
  * This is a two-stage process:
  *
<span class="p_del">- *  - switch_fpu_prepare() saves the old state and</span>
<span class="p_del">- *    sets the new state of the CR0.TS bit. This is</span>
<span class="p_del">- *    done within the context of the old process.</span>
<span class="p_add">+ *  - switch_fpu_prepare() saves the old state.</span>
<span class="p_add">+ *    This is done within the context of the old process.</span>
  *
  *  - switch_fpu_finish() restores the new state as
  *    necessary.
  */
<span class="p_del">-typedef struct { int preload; } fpu_switch_t;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline fpu_switch_t</span>
<span class="p_del">-switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)</span>
<span class="p_add">+static inline void</span>
<span class="p_add">+switch_fpu_prepare(struct fpu *old_fpu, int cpu)</span>
 {
<span class="p_del">-	fpu_switch_t fpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If the task has used the math, pre-load the FPU on xsave processors</span>
<span class="p_del">-	 * or if the past 5 consecutive context-switches used math.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	fpu.preload = static_cpu_has(X86_FEATURE_FPU) &amp;&amp;</span>
<span class="p_del">-		      new_fpu-&gt;fpstate_active &amp;&amp;</span>
<span class="p_del">-		      (use_eager_fpu() || new_fpu-&gt;counter &gt; 5);</span>
<span class="p_del">-</span>
 	if (old_fpu-&gt;fpregs_active) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu-&gt;last_cpu = -1;
<span class="p_chunk">@@ -608,29 +570,8 @@</span> <span class="p_context"> switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)</span>
 		/* But leave fpu_fpregs_owner_ctx! */
 		old_fpu-&gt;fpregs_active = 0;
 		trace_x86_fpu_regs_deactivated(old_fpu);
<span class="p_del">-</span>
<span class="p_del">-		/* Don&#39;t change CR0.TS if we just switch! */</span>
<span class="p_del">-		if (fpu.preload) {</span>
<span class="p_del">-			new_fpu-&gt;counter++;</span>
<span class="p_del">-			__fpregs_activate(new_fpu);</span>
<span class="p_del">-			trace_x86_fpu_regs_activated(new_fpu);</span>
<span class="p_del">-			prefetch(&amp;new_fpu-&gt;state);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			__fpregs_deactivate_hw();</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		old_fpu-&gt;counter = 0;</span>
<span class="p_add">+	} else</span>
 		old_fpu-&gt;last_cpu = -1;
<span class="p_del">-		if (fpu.preload) {</span>
<span class="p_del">-			new_fpu-&gt;counter++;</span>
<span class="p_del">-			if (fpu_want_lazy_restore(new_fpu, cpu))</span>
<span class="p_del">-				fpu.preload = 0;</span>
<span class="p_del">-			else</span>
<span class="p_del">-				prefetch(&amp;new_fpu-&gt;state);</span>
<span class="p_del">-			fpregs_activate(new_fpu);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return fpu;</span>
 }
 
 /*
<span class="p_chunk">@@ -638,15 +579,19 @@</span> <span class="p_context"> switch_fpu_prepare(struct fpu *old_fpu, struct fpu *new_fpu, int cpu)</span>
  */
 
 /*
<span class="p_del">- * By the time this gets called, we&#39;ve already cleared CR0.TS and</span>
<span class="p_del">- * given the process the FPU if we are going to preload the FPU</span>
<span class="p_del">- * state - all we need to do is to conditionally restore the register</span>
<span class="p_del">- * state itself.</span>
<span class="p_add">+ * Set up the userspace FPU context for the new task, if the task</span>
<span class="p_add">+ * has used the FPU.</span>
  */
<span class="p_del">-static inline void switch_fpu_finish(struct fpu *new_fpu, fpu_switch_t fpu_switch)</span>
<span class="p_add">+static inline void switch_fpu_finish(struct fpu *new_fpu, int cpu)</span>
 {
<span class="p_del">-	if (fpu_switch.preload)</span>
<span class="p_del">-		copy_kernel_to_fpregs(&amp;new_fpu-&gt;state);</span>
<span class="p_add">+	bool preload = static_cpu_has(X86_FEATURE_FPU) &amp;&amp;</span>
<span class="p_add">+		       new_fpu-&gt;fpstate_active;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (preload) {</span>
<span class="p_add">+		if (!fpregs_state_valid(new_fpu, cpu))</span>
<span class="p_add">+			copy_kernel_to_fpregs(&amp;new_fpu-&gt;state);</span>
<span class="p_add">+		fpregs_activate(new_fpu);</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h</span>
<span class="p_header">index 48df486b02f9..3c80f5b9c09d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/types.h</span>
<span class="p_chunk">@@ -322,17 +322,6 @@</span> <span class="p_context"> struct fpu {</span>
 	unsigned char			fpregs_active;
 
 	/*
<span class="p_del">-	 * @counter:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This counter contains the number of consecutive context switches</span>
<span class="p_del">-	 * during which the FPU stays used. If this is over a threshold, the</span>
<span class="p_del">-	 * lazy FPU restore logic becomes eager, to save the trap overhead.</span>
<span class="p_del">-	 * This is an unsigned char so that after 256 iterations the counter</span>
<span class="p_del">-	 * wraps and the context switch behavior turns lazy again; this is to</span>
<span class="p_del">-	 * deal with bursty apps that only use the FPU for a short time:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	unsigned char			counter;</span>
<span class="p_del">-	/*</span>
 	 * @state:
 	 *
 	 * In-memory copy of all FPU registers that we save/restore
<span class="p_chunk">@@ -340,29 +329,6 @@</span> <span class="p_context"> struct fpu {</span>
 	 * the registers in the FPU are more recent than this state
 	 * copy. If the task context-switches away then they get
 	 * saved here and represent the FPU state.
<span class="p_del">-	 *</span>
<span class="p_del">-	 * After context switches there may be a (short) time period</span>
<span class="p_del">-	 * during which the in-FPU hardware registers are unchanged</span>
<span class="p_del">-	 * and still perfectly match this state, if the tasks</span>
<span class="p_del">-	 * scheduled afterwards are not using the FPU.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This is the &#39;lazy restore&#39; window of optimization, which</span>
<span class="p_del">-	 * we track though &#39;fpu_fpregs_owner_ctx&#39; and &#39;fpu-&gt;last_cpu&#39;.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * We detect whether a subsequent task uses the FPU via setting</span>
<span class="p_del">-	 * CR0::TS to 1, which causes any FPU use to raise a #NM fault.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * During this window, if the task gets scheduled again, we</span>
<span class="p_del">-	 * might be able to skip having to do a restore from this</span>
<span class="p_del">-	 * memory buffer to the hardware registers - at the cost of</span>
<span class="p_del">-	 * incurring the overhead of #NM fault traps.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Note that on modern CPUs that support the XSAVEOPT (or other</span>
<span class="p_del">-	 * optimized XSAVE instructions), we don&#39;t use #NM traps anymore,</span>
<span class="p_del">-	 * as the hardware can track whether FPU registers need saving</span>
<span class="p_del">-	 * or not. On such CPUs we activate the non-lazy (&#39;eagerfpu&#39;)</span>
<span class="p_del">-	 * logic, which unconditionally saves/restores all FPU state</span>
<span class="p_del">-	 * across context switches. (if FPU state exists.)</span>
 	 */
 	union fpregs_state		state;
 	/*
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">index 430bacf73074..1b2799e0699a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_chunk">@@ -21,21 +21,16 @@</span> <span class="p_context"></span>
 /* Supervisor features */
 #define XFEATURE_MASK_SUPERVISOR (XFEATURE_MASK_PT)
 
<span class="p_del">-/* Supported features which support lazy state saving */</span>
<span class="p_del">-#define XFEATURE_MASK_LAZY	(XFEATURE_MASK_FP | \</span>
<span class="p_add">+/* All currently supported features */</span>
<span class="p_add">+#define XCNTXT_MASK		(XFEATURE_MASK_FP | \</span>
 				 XFEATURE_MASK_SSE | \
 				 XFEATURE_MASK_YMM | \
 				 XFEATURE_MASK_OPMASK | \
 				 XFEATURE_MASK_ZMM_Hi256 | \
<span class="p_del">-				 XFEATURE_MASK_Hi16_ZMM)</span>
<span class="p_del">-</span>
<span class="p_del">-/* Supported features which require eager state saving */</span>
<span class="p_del">-#define XFEATURE_MASK_EAGER	(XFEATURE_MASK_BNDREGS | \</span>
<span class="p_del">-				 XFEATURE_MASK_BNDCSR | \</span>
<span class="p_del">-				 XFEATURE_MASK_PKRU)</span>
<span class="p_del">-</span>
<span class="p_del">-/* All currently supported features */</span>
<span class="p_del">-#define XCNTXT_MASK	(XFEATURE_MASK_LAZY | XFEATURE_MASK_EAGER)</span>
<span class="p_add">+				 XFEATURE_MASK_Hi16_ZMM	 | \</span>
<span class="p_add">+				 XFEATURE_MASK_PKRU | \</span>
<span class="p_add">+				 XFEATURE_MASK_BNDREGS | \</span>
<span class="p_add">+				 XFEATURE_MASK_BNDCSR)</span>
 
 #ifdef CONFIG_X86_64
 #define REX_PREFIX	&quot;0x48, &quot;
<span class="p_header">diff --git a/arch/x86/include/asm/lguest_hcall.h b/arch/x86/include/asm/lguest_hcall.h</span>
<span class="p_header">index ef01fef3eebc..6c119cfae218 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/lguest_hcall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/lguest_hcall.h</span>
<span class="p_chunk">@@ -9,7 +9,6 @@</span> <span class="p_context"></span>
 #define LHCALL_FLUSH_TLB	5
 #define LHCALL_LOAD_IDT_ENTRY	6
 #define LHCALL_SET_STACK	7
<span class="p_del">-#define LHCALL_TS		8</span>
 #define LHCALL_SET_CLOCKEVENT	9
 #define LHCALL_HALT		10
 #define LHCALL_SET_PMD		13
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index ce932812f142..f1fb4dbe9a3e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -41,11 +41,6 @@</span> <span class="p_context"> static inline void set_debugreg(unsigned long val, int reg)</span>
 	PVOP_VCALL2(pv_cpu_ops.set_debugreg, reg, val);
 }
 
<span class="p_del">-static inline void clts(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALL0(pv_cpu_ops.clts);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline unsigned long read_cr0(void)
 {
 	return PVOP_CALL0(unsigned long, pv_cpu_ops.read_cr0);
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 0f400c0e4979..545426aa61ef 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -103,8 +103,6 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 	unsigned long (*get_debugreg)(int regno);
 	void (*set_debugreg)(int regno, unsigned long value);
 
<span class="p_del">-	void (*clts)(void);</span>
<span class="p_del">-</span>
 	unsigned long (*read_cr0)(void);
 	void (*write_cr0)(unsigned long);
 
<span class="p_header">diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h</span>
<span class="p_header">index 19a2224f9e16..12af3e35edfa 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/special_insns.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/special_insns.h</span>
<span class="p_chunk">@@ -6,11 +6,6 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/nops.h&gt;
 
<span class="p_del">-static inline void native_clts(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	asm volatile(&quot;clts&quot;);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * Volatile isn&#39;t enough to prevent the compiler from reordering the
  * read/write functions for the control registers and messing everything up.
<span class="p_chunk">@@ -208,16 +203,8 @@</span> <span class="p_context"> static inline void load_gs_index(unsigned selector)</span>
 
 #endif
 
<span class="p_del">-/* Clear the &#39;TS&#39; bit */</span>
<span class="p_del">-static inline void clts(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	native_clts();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif/* CONFIG_PARAVIRT */
 
<span class="p_del">-#define stts() write_cr0(read_cr0() | X86_CR0_TS)</span>
<span class="p_del">-</span>
 static inline void clflush(volatile void *__p)
 {
 	asm volatile(&quot;clflush %0&quot; : &quot;+m&quot; (*(volatile char __force *)__p));
<span class="p_header">diff --git a/arch/x86/include/asm/trace/fpu.h b/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_header">index 9217ab1f5bf6..342e59789fcd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/trace/fpu.h</span>
<span class="p_chunk">@@ -14,7 +14,6 @@</span> <span class="p_context"> DECLARE_EVENT_CLASS(x86_fpu,</span>
 		__field(struct fpu *, fpu)
 		__field(bool, fpregs_active)
 		__field(bool, fpstate_active)
<span class="p_del">-		__field(int, counter)</span>
 		__field(u64, xfeatures)
 		__field(u64, xcomp_bv)
 		),
<span class="p_chunk">@@ -23,17 +22,15 @@</span> <span class="p_context"> DECLARE_EVENT_CLASS(x86_fpu,</span>
 		__entry-&gt;fpu		= fpu;
 		__entry-&gt;fpregs_active	= fpu-&gt;fpregs_active;
 		__entry-&gt;fpstate_active	= fpu-&gt;fpstate_active;
<span class="p_del">-		__entry-&gt;counter	= fpu-&gt;counter;</span>
 		if (boot_cpu_has(X86_FEATURE_OSXSAVE)) {
 			__entry-&gt;xfeatures = fpu-&gt;state.xsave.header.xfeatures;
 			__entry-&gt;xcomp_bv  = fpu-&gt;state.xsave.header.xcomp_bv;
 		}
 	),
<span class="p_del">-	TP_printk(&quot;x86/fpu: %p fpregs_active: %d fpstate_active: %d counter: %d xfeatures: %llx xcomp_bv: %llx&quot;,</span>
<span class="p_add">+	TP_printk(&quot;x86/fpu: %p fpregs_active: %d fpstate_active: %d xfeatures: %llx xcomp_bv: %llx&quot;,</span>
 			__entry-&gt;fpu,
 			__entry-&gt;fpregs_active,
 			__entry-&gt;fpstate_active,
<span class="p_del">-			__entry-&gt;counter,</span>
 			__entry-&gt;xfeatures,
 			__entry-&gt;xcomp_bv
 	)
<span class="p_header">diff --git a/arch/x86/kernel/fpu/bugs.c b/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_header">index aad34aafc0e0..d913047f832c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/bugs.c</span>
<span class="p_chunk">@@ -23,17 +23,12 @@</span> <span class="p_context"> static double __initdata y = 3145727.0;</span>
  */
 void __init fpu__init_check_bugs(void)
 {
<span class="p_del">-	u32 cr0_saved;</span>
 	s32 fdiv_bug;
 
 	/* kernel_fpu_begin/end() relies on patched alternative instructions. */
 	if (!boot_cpu_has(X86_FEATURE_FPU))
 		return;
 
<span class="p_del">-	/* We might have CR0::TS set already, clear it: */</span>
<span class="p_del">-	cr0_saved = read_cr0();</span>
<span class="p_del">-	write_cr0(cr0_saved &amp; ~X86_CR0_TS);</span>
<span class="p_del">-</span>
 	kernel_fpu_begin();
 
 	/*
<span class="p_chunk">@@ -56,8 +51,6 @@</span> <span class="p_context"> void __init fpu__init_check_bugs(void)</span>
 
 	kernel_fpu_end();
 
<span class="p_del">-	write_cr0(cr0_saved);</span>
<span class="p_del">-</span>
 	if (fdiv_bug) {
 		set_cpu_bug(&amp;boot_cpu_data, X86_BUG_FDIV);
 		pr_warn(&quot;Hmm, FPU with FDIV bug\n&quot;);
<span class="p_header">diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c</span>
<span class="p_header">index ebb4e95fbd74..e4e97a5355ce 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/core.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/core.c</span>
<span class="p_chunk">@@ -58,27 +58,9 @@</span> <span class="p_context"> static bool kernel_fpu_disabled(void)</span>
 	return this_cpu_read(in_kernel_fpu);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Were we in an interrupt that interrupted kernel mode?</span>
<span class="p_del">- *</span>
<span class="p_del">- * On others, we can do a kernel_fpu_begin/end() pair *ONLY* if that</span>
<span class="p_del">- * pair does nothing at all: the thread must not have fpu (so</span>
<span class="p_del">- * that we don&#39;t try to save the FPU state), and TS must</span>
<span class="p_del">- * be set (so that the clts/stts pair does nothing that is</span>
<span class="p_del">- * visible in the interrupted kernel thread).</span>
<span class="p_del">- *</span>
<span class="p_del">- * Except for the eagerfpu case when we return true; in the likely case</span>
<span class="p_del">- * the thread has FPU but we are not going to set/clear TS.</span>
<span class="p_del">- */</span>
 static bool interrupted_kernel_fpu_idle(void)
 {
<span class="p_del">-	if (kernel_fpu_disabled())</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (use_eager_fpu())</span>
<span class="p_del">-		return true;</span>
<span class="p_del">-</span>
<span class="p_del">-	return !current-&gt;thread.fpu.fpregs_active &amp;&amp; (read_cr0() &amp; X86_CR0_TS);</span>
<span class="p_add">+	return !kernel_fpu_disabled();</span>
 }
 
 /*
<span class="p_chunk">@@ -125,8 +107,7 @@</span> <span class="p_context"> void __kernel_fpu_begin(void)</span>
 		 */
 		copy_fpregs_to_fpstate(fpu);
 	} else {
<span class="p_del">-		this_cpu_write(fpu_fpregs_owner_ctx, NULL);</span>
<span class="p_del">-		__fpregs_activate_hw();</span>
<span class="p_add">+		__cpu_invalidate_fpregs_state();</span>
 	}
 }
 EXPORT_SYMBOL(__kernel_fpu_begin);
<span class="p_chunk">@@ -137,8 +118,6 @@</span> <span class="p_context"> void __kernel_fpu_end(void)</span>
 
 	if (fpu-&gt;fpregs_active)
 		copy_kernel_to_fpregs(&amp;fpu-&gt;state);
<span class="p_del">-	else</span>
<span class="p_del">-		__fpregs_deactivate_hw();</span>
 
 	kernel_fpu_enable();
 }
<span class="p_chunk">@@ -159,35 +138,6 @@</span> <span class="p_context"> void kernel_fpu_end(void)</span>
 EXPORT_SYMBOL_GPL(kernel_fpu_end);
 
 /*
<span class="p_del">- * CR0::TS save/restore functions:</span>
<span class="p_del">- */</span>
<span class="p_del">-int irq_ts_save(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If in process context and not atomic, we can take a spurious DNA fault.</span>
<span class="p_del">-	 * Otherwise, doing clts() in process context requires disabling preemption</span>
<span class="p_del">-	 * or some heavy lifting like kernel_fpu_begin()</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!in_atomic())</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (read_cr0() &amp; X86_CR0_TS) {</span>
<span class="p_del">-		clts();</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(irq_ts_save);</span>
<span class="p_del">-</span>
<span class="p_del">-void irq_ts_restore(int TS_state)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (TS_state)</span>
<span class="p_del">-		stts();</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(irq_ts_restore);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Save the FPU state (mark it for reload if necessary):
  *
  * This only ever gets called for the current task.
<span class="p_chunk">@@ -200,10 +150,7 @@</span> <span class="p_context"> void fpu__save(struct fpu *fpu)</span>
 	trace_x86_fpu_before_save(fpu);
 	if (fpu-&gt;fpregs_active) {
 		if (!copy_fpregs_to_fpstate(fpu)) {
<span class="p_del">-			if (use_eager_fpu())</span>
<span class="p_del">-				copy_kernel_to_fpregs(&amp;fpu-&gt;state);</span>
<span class="p_del">-			else</span>
<span class="p_del">-				fpregs_deactivate(fpu);</span>
<span class="p_add">+			copy_kernel_to_fpregs(&amp;fpu-&gt;state);</span>
 		}
 	}
 	trace_x86_fpu_after_save(fpu);
<span class="p_chunk">@@ -247,7 +194,6 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(fpstate_init);</span>
 
 int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu)
 {
<span class="p_del">-	dst_fpu-&gt;counter = 0;</span>
 	dst_fpu-&gt;fpregs_active = 0;
 	dst_fpu-&gt;last_cpu = -1;
 
<span class="p_chunk">@@ -260,8 +206,7 @@</span> <span class="p_context"> int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu)</span>
 	 * Don&#39;t let &#39;init optimized&#39; areas of the XSAVE area
 	 * leak into the child task:
 	 */
<span class="p_del">-	if (use_eager_fpu())</span>
<span class="p_del">-		memset(&amp;dst_fpu-&gt;state.xsave, 0, fpu_kernel_xstate_size);</span>
<span class="p_add">+	memset(&amp;dst_fpu-&gt;state.xsave, 0, fpu_kernel_xstate_size);</span>
 
 	/*
 	 * Save current FPU registers directly into the child
<span class="p_chunk">@@ -283,10 +228,7 @@</span> <span class="p_context"> int fpu__copy(struct fpu *dst_fpu, struct fpu *src_fpu)</span>
 		memcpy(&amp;src_fpu-&gt;state, &amp;dst_fpu-&gt;state,
 		       fpu_kernel_xstate_size);
 
<span class="p_del">-		if (use_eager_fpu())</span>
<span class="p_del">-			copy_kernel_to_fpregs(&amp;src_fpu-&gt;state);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			fpregs_deactivate(src_fpu);</span>
<span class="p_add">+		copy_kernel_to_fpregs(&amp;src_fpu-&gt;state);</span>
 	}
 	preempt_enable();
 
<span class="p_chunk">@@ -366,7 +308,7 @@</span> <span class="p_context"> void fpu__activate_fpstate_write(struct fpu *fpu)</span>
 
 	if (fpu-&gt;fpstate_active) {
 		/* Invalidate any lazy state: */
<span class="p_del">-		fpu-&gt;last_cpu = -1;</span>
<span class="p_add">+		__fpu_invalidate_fpregs_state(fpu);</span>
 	} else {
 		fpstate_init(&amp;fpu-&gt;state);
 		trace_x86_fpu_init_state(fpu);
<span class="p_chunk">@@ -409,7 +351,7 @@</span> <span class="p_context"> void fpu__current_fpstate_write_begin(void)</span>
 	 * ensures we will not be lazy and skip a XRSTOR in the
 	 * future.
 	 */
<span class="p_del">-	fpu-&gt;last_cpu = -1;</span>
<span class="p_add">+	__fpu_invalidate_fpregs_state(fpu);</span>
 }
 
 /*
<span class="p_chunk">@@ -459,7 +401,6 @@</span> <span class="p_context"> void fpu__restore(struct fpu *fpu)</span>
 	trace_x86_fpu_before_restore(fpu);
 	fpregs_activate(fpu);
 	copy_kernel_to_fpregs(&amp;fpu-&gt;state);
<span class="p_del">-	fpu-&gt;counter++;</span>
 	trace_x86_fpu_after_restore(fpu);
 	kernel_fpu_enable();
 }
<span class="p_chunk">@@ -477,7 +418,6 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(fpu__restore);</span>
 void fpu__drop(struct fpu *fpu)
 {
 	preempt_disable();
<span class="p_del">-	fpu-&gt;counter = 0;</span>
 
 	if (fpu-&gt;fpregs_active) {
 		/* Ignore delayed exceptions from user space */
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 2f2b8c7ccb85..60dece392b3a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -10,18 +10,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/init.h&gt;
 
 /*
<span class="p_del">- * Initialize the TS bit in CR0 according to the style of context-switches</span>
<span class="p_del">- * we are using:</span>
<span class="p_del">- */</span>
<span class="p_del">-static void fpu__init_cpu_ctx_switch(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!boot_cpu_has(X86_FEATURE_EAGER_FPU))</span>
<span class="p_del">-		stts();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		clts();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Initialize the registers found in all CPUs, CR0 and CR4:
  */
 static void fpu__init_cpu_generic(void)
<span class="p_chunk">@@ -58,7 +46,6 @@</span> <span class="p_context"> void fpu__init_cpu(void)</span>
 {
 	fpu__init_cpu_generic();
 	fpu__init_cpu_xstate();
<span class="p_del">-	fpu__init_cpu_ctx_switch();</span>
 }
 
 /*
<span class="p_chunk">@@ -233,82 +220,16 @@</span> <span class="p_context"> static void __init fpu__init_system_xstate_size_legacy(void)</span>
 }
 
 /*
<span class="p_del">- * FPU context switching strategies:</span>
<span class="p_del">- *</span>
<span class="p_del">- * Against popular belief, we don&#39;t do lazy FPU saves, due to the</span>
<span class="p_del">- * task migration complications it brings on SMP - we only do</span>
<span class="p_del">- * lazy FPU restores.</span>
<span class="p_del">- *</span>
<span class="p_del">- * &#39;lazy&#39; is the traditional strategy, which is based on setting</span>
<span class="p_del">- * CR0::TS to 1 during context-switch (instead of doing a full</span>
<span class="p_del">- * restore of the FPU state), which causes the first FPU instruction</span>
<span class="p_del">- * after the context switch (whenever it is executed) to fault - at</span>
<span class="p_del">- * which point we lazily restore the FPU state into FPU registers.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Tasks are of course under no obligation to execute FPU instructions,</span>
<span class="p_del">- * so it can easily happen that another context-switch occurs without</span>
<span class="p_del">- * a single FPU instruction being executed. If we eventually switch</span>
<span class="p_del">- * back to the original task (that still owns the FPU) then we have</span>
<span class="p_del">- * not only saved the restores along the way, but we also have the</span>
<span class="p_del">- * FPU ready to be used for the original task.</span>
<span class="p_del">- *</span>
<span class="p_del">- * &#39;lazy&#39; is deprecated because it&#39;s almost never a performance win</span>
<span class="p_del">- * and it&#39;s much more complicated than &#39;eager&#39;.</span>
<span class="p_del">- *</span>
<span class="p_del">- * &#39;eager&#39; switching is by default on all CPUs, there we switch the FPU</span>
<span class="p_del">- * state during every context switch, regardless of whether the task</span>
<span class="p_del">- * has used FPU instructions in that time slice or not. This is done</span>
<span class="p_del">- * because modern FPU context saving instructions are able to optimize</span>
<span class="p_del">- * state saving and restoration in hardware: they can detect both</span>
<span class="p_del">- * unused and untouched FPU state and optimize accordingly.</span>
<span class="p_del">- *</span>
<span class="p_del">- * [ Note that even in &#39;lazy&#39; mode we might optimize context switches</span>
<span class="p_del">- *   to use &#39;eager&#39; restores, if we detect that a task is using the FPU</span>
<span class="p_del">- *   frequently. See the fpu-&gt;counter logic in fpu/internal.h for that. ]</span>
<span class="p_del">- */</span>
<span class="p_del">-static enum { ENABLE, DISABLE } eagerfpu = ENABLE;</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Find supported xfeatures based on cpu features and command-line input.
  * This must be called after fpu__init_parse_early_param() is called and
  * xfeatures_mask is enumerated.
  */
 u64 __init fpu__get_supported_xfeatures_mask(void)
 {
<span class="p_del">-	/* Support all xfeatures known to us */</span>
<span class="p_del">-	if (eagerfpu != DISABLE)</span>
<span class="p_del">-		return XCNTXT_MASK;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Warning of xfeatures being disabled for no eagerfpu mode */</span>
<span class="p_del">-	if (xfeatures_mask &amp; XFEATURE_MASK_EAGER) {</span>
<span class="p_del">-		pr_err(&quot;x86/fpu: eagerfpu switching disabled, disabling the following xstate features: 0x%llx.\n&quot;,</span>
<span class="p_del">-			xfeatures_mask &amp; XFEATURE_MASK_EAGER);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Return a mask that masks out all features requiring eagerfpu mode */</span>
<span class="p_del">-	return ~XFEATURE_MASK_EAGER;</span>
<span class="p_add">+	return XCNTXT_MASK;</span>
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Disable features dependent on eagerfpu.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void __init fpu__clear_eager_fpu_features(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_MPX);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Pick the FPU context switching strategy:</span>
<span class="p_del">- *</span>
<span class="p_del">- * When eagerfpu is AUTO or ENABLE, we ensure it is ENABLE if either of</span>
<span class="p_del">- * the following is true:</span>
<span class="p_del">- *</span>
<span class="p_del">- * (1) the cpu has xsaveopt, as it has the optimization and doing eager</span>
<span class="p_del">- *     FPU switching has a relatively low cost compared to a plain xsave;</span>
<span class="p_del">- * (2) the cpu has xsave features (e.g. MPX) that depend on eager FPU</span>
<span class="p_del">- *     switching. Should the kernel boot with noxsaveopt, we support MPX</span>
<span class="p_del">- *     with eager FPU switching at a higher cost.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* Legacy code to initialize eager fpu mode. */</span>
 static void __init fpu__init_system_ctx_switch(void)
 {
 	static bool on_boot_cpu __initdata = 1;
<span class="p_chunk">@@ -317,17 +238,6 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
 	on_boot_cpu = 0;
 
 	WARN_ON_FPU(current-&gt;thread.fpu.fpstate_active);
<span class="p_del">-</span>
<span class="p_del">-	if (boot_cpu_has(X86_FEATURE_XSAVEOPT) &amp;&amp; eagerfpu != DISABLE)</span>
<span class="p_del">-		eagerfpu = ENABLE;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (xfeatures_mask &amp; XFEATURE_MASK_EAGER)</span>
<span class="p_del">-		eagerfpu = ENABLE;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (eagerfpu == ENABLE)</span>
<span class="p_del">-		setup_force_cpu_cap(X86_FEATURE_EAGER_FPU);</span>
<span class="p_del">-</span>
<span class="p_del">-	printk(KERN_INFO &quot;x86/fpu: Using &#39;%s&#39; FPU context switches.\n&quot;, eagerfpu == ENABLE ? &quot;eager&quot; : &quot;lazy&quot;);</span>
 }
 
 /*
<span class="p_chunk">@@ -336,11 +246,6 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
  */
 static void __init fpu__init_parse_early_param(void)
 {
<span class="p_del">-	if (cmdline_find_option_bool(boot_command_line, &quot;eagerfpu=off&quot;)) {</span>
<span class="p_del">-		eagerfpu = DISABLE;</span>
<span class="p_del">-		fpu__clear_eager_fpu_features();</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	if (cmdline_find_option_bool(boot_command_line, &quot;no387&quot;))
 		setup_clear_cpu_cap(X86_FEATURE_FPU);
 
<span class="p_chunk">@@ -375,14 +280,6 @@</span> <span class="p_context"> void __init fpu__init_system(struct cpuinfo_x86 *c)</span>
 	 */
 	fpu__init_cpu();
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * But don&#39;t leave CR0::TS set yet, as some of the FPU setup</span>
<span class="p_del">-	 * methods depend on being able to execute FPU instructions</span>
<span class="p_del">-	 * that will fault on a set TS, such as the FXSAVE in</span>
<span class="p_del">-	 * fpu__init_system_mxcsr().</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	clts();</span>
<span class="p_del">-</span>
 	fpu__init_system_generic();
 	fpu__init_system_xstate_size_legacy();
 	fpu__init_system_xstate();
<span class="p_header">diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c</span>
<span class="p_header">index a184c210efba..83c23c230b4c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/signal.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/signal.c</span>
<span class="p_chunk">@@ -340,11 +340,9 @@</span> <span class="p_context"> static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)</span>
 		}
 
 		fpu-&gt;fpstate_active = 1;
<span class="p_del">-		if (use_eager_fpu()) {</span>
<span class="p_del">-			preempt_disable();</span>
<span class="p_del">-			fpu__restore(fpu);</span>
<span class="p_del">-			preempt_enable();</span>
<span class="p_del">-		}</span>
<span class="p_add">+		preempt_disable();</span>
<span class="p_add">+		fpu__restore(fpu);</span>
<span class="p_add">+		preempt_enable();</span>
 
 		return err;
 	} else {
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index 095ef7ddd6ae..c7c11cc988b7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -890,15 +890,6 @@</span> <span class="p_context"> int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,</span>
 	 */
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return -EINVAL;
<span class="p_del">-	/*</span>
<span class="p_del">-	 * For most XSAVE components, this would be an arduous task:</span>
<span class="p_del">-	 * brining fpstate up to date with fpregs, updating fpstate,</span>
<span class="p_del">-	 * then re-populating fpregs.  But, for components that are</span>
<span class="p_del">-	 * never lazily managed, we can just access the fpregs</span>
<span class="p_del">-	 * directly.  PKRU is never managed lazily, so we can just</span>
<span class="p_del">-	 * manipulate it directly.  Make sure it stays that way.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	WARN_ON_ONCE(!use_eager_fpu());</span>
 
 	/* Set the bits we need in PKRU:  */
 	if (init_val &amp; PKEY_DISABLE_ACCESS)
<span class="p_header">diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c</span>
<span class="p_header">index bbf3d5933eaa..a1bfba0f7234 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt.c</span>
<span class="p_chunk">@@ -328,7 +328,6 @@</span> <span class="p_context"> __visible struct pv_cpu_ops pv_cpu_ops = {</span>
 	.cpuid = native_cpuid,
 	.get_debugreg = native_get_debugreg,
 	.set_debugreg = native_set_debugreg,
<span class="p_del">-	.clts = native_clts,</span>
 	.read_cr0 = native_read_cr0,
 	.write_cr0 = native_write_cr0,
 	.read_cr4 = native_read_cr4,
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index 920c6ae08592..d3f7f14bb328 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -8,7 +8,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_cpu_ops, iret, &quot;iret&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;mov %cr2, %eax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);
<span class="p_del">-DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);</span>
 
 #if defined(CONFIG_PARAVIRT_SPINLOCKS)
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);
<span class="p_chunk">@@ -48,7 +47,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr2);
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
<span class="p_del">-		PATCH_SITE(pv_cpu_ops, clts);</span>
 #if defined(CONFIG_PARAVIRT_SPINLOCKS)
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index bb3840cedb4f..915a4c0b217c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -10,7 +10,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;movq %cr2, %rax&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;movq %cr3, %rax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;movq %rdi, %cr3&quot;);
 DEF_NATIVE(pv_mmu_ops, flush_tlb_single, &quot;invlpg (%rdi)&quot;);
<span class="p_del">-DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, wbinvd, &quot;wbinvd&quot;);
 
 DEF_NATIVE(pv_cpu_ops, usergs_sysret64, &quot;swapgs; sysretq&quot;);
<span class="p_chunk">@@ -58,7 +57,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr2);
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
<span class="p_del">-		PATCH_SITE(pv_cpu_ops, clts);</span>
 		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
 		PATCH_SITE(pv_cpu_ops, wbinvd);
 #if defined(CONFIG_PARAVIRT_SPINLOCKS)
<span class="p_header">diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c</span>
<span class="p_header">index bd7be8efdc4c..7dc8c9c3d801 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_32.c</span>
<span class="p_chunk">@@ -232,11 +232,10 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	struct fpu *next_fpu = &amp;next-&gt;fpu;
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &amp;per_cpu(cpu_tss, cpu);
<span class="p_del">-	fpu_switch_t fpu_switch;</span>
 
 	/* never put a printk in __switch_to... printk() calls wake_up*() indirectly */
 
<span class="p_del">-	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);</span>
<span class="p_add">+	switch_fpu_prepare(prev_fpu, cpu);</span>
 
 	/*
 	 * Save away %gs. No need to save %fs, as it was saved on the
<span class="p_chunk">@@ -295,7 +294,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	if (prev-&gt;gs | next-&gt;gs)
 		lazy_load_gs(next-&gt;gs);
 
<span class="p_del">-	switch_fpu_finish(next_fpu, fpu_switch);</span>
<span class="p_add">+	switch_fpu_finish(next_fpu, cpu);</span>
 
 	this_cpu_write(current_task, next_p);
 
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index b3760b3c1ca0..9c3a7b04e59e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -265,9 +265,8 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 	int cpu = smp_processor_id();
 	struct tss_struct *tss = &amp;per_cpu(cpu_tss, cpu);
 	unsigned prev_fsindex, prev_gsindex;
<span class="p_del">-	fpu_switch_t fpu_switch;</span>
 
<span class="p_del">-	fpu_switch = switch_fpu_prepare(prev_fpu, next_fpu, cpu);</span>
<span class="p_add">+	switch_fpu_prepare(prev_fpu, cpu);</span>
 
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
<span class="p_chunk">@@ -417,7 +416,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 		prev-&gt;gsbase = 0;
 	prev-&gt;gsindex = prev_gsindex;
 
<span class="p_del">-	switch_fpu_finish(next_fpu, fpu_switch);</span>
<span class="p_add">+	switch_fpu_finish(next_fpu, cpu);</span>
 
 	/*
 	 * Switch the PDA and FPU contexts.
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 42f5eb7b4f6c..d29c85250108 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -1111,7 +1111,7 @@</span> <span class="p_context"> int native_cpu_up(unsigned int cpu, struct task_struct *tidle)</span>
 		return err;
 
 	/* the FPU context is blank, nobody can own it */
<span class="p_del">-	__cpu_disable_lazy_restore(cpu);</span>
<span class="p_add">+	per_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;</span>
 
 	common_cpu_up(cpu, tidle);
 
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index bd4e3d4d3625..bf0c6d049080 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -853,6 +853,8 @@</span> <span class="p_context"> do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)</span>
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
<span class="p_add">+	unsigned long cr0;</span>
<span class="p_add">+</span>
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), &quot;entry code didn&#39;t wake RCU&quot;);
 
 #ifdef CONFIG_MATH_EMULATION
<span class="p_chunk">@@ -866,10 +868,20 @@</span> <span class="p_context"> do_device_not_available(struct pt_regs *regs, long error_code)</span>
 		return;
 	}
 #endif
<span class="p_del">-	fpu__restore(&amp;current-&gt;thread.fpu); /* interrupts still off */</span>
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	cond_local_irq_enable(regs);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This should not happen. */</span>
<span class="p_add">+	cr0 = read_cr0();</span>
<span class="p_add">+	if (WARN(cr0 &amp; X86_CR0_TS, &quot;CR0.TS was set&quot;)) {</span>
<span class="p_add">+		/* Try to fix it up and carry on. */</span>
<span class="p_add">+		write_cr0(cr0 &amp; ~X86_CR0_TS);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Something terrible happened, and we&#39;re better off trying</span>
<span class="p_add">+		 * to kill the task than getting stuck in a never-ending</span>
<span class="p_add">+		 * loop of #NM faults.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		die(&quot;unexpected #NM exception&quot;, regs, error_code);</span>
<span class="p_add">+	}</span>
 }
 NOKPROBE_SYMBOL(do_device_not_available);
 
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="p_header">index afa7bbb596cd..0aefb626fa8f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.c</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.c</span>
<span class="p_chunk">@@ -16,7 +16,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/uaccess.h&gt;
<span class="p_del">-#include &lt;asm/fpu/internal.h&gt; /* For use_eager_fpu.  Ugh! */</span>
 #include &lt;asm/user.h&gt;
 #include &lt;asm/fpu/xstate.h&gt;
 #include &quot;cpuid.h&quot;
<span class="p_chunk">@@ -114,8 +113,7 @@</span> <span class="p_context"> int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
 	if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))
 		best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);
 
<span class="p_del">-	if (use_eager_fpu())</span>
<span class="p_del">-		kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="p_add">+	kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
 
 	/*
 	 * The existing code assumes virtual address is 48-bit in the canonical
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 5382b82462fc..3980da515fd0 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -2145,12 +2145,6 @@</span> <span class="p_context"> static void __vmx_load_host_state(struct vcpu_vmx *vmx)</span>
 #endif
 	if (vmx-&gt;host_state.msr_host_bndcfgs)
 		wrmsrl(MSR_IA32_BNDCFGS, vmx-&gt;host_state.msr_host_bndcfgs);
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If the FPU is not active (through the host task or</span>
<span class="p_del">-	 * the guest vcpu), then restore the cr0.TS bit.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!fpregs_active() &amp;&amp; !vmx-&gt;vcpu.guest_fpu_loaded)</span>
<span class="p_del">-		stts();</span>
 	load_gdt(this_cpu_ptr(&amp;host_gdt));
 }
 
<span class="p_chunk">@@ -4845,9 +4839,11 @@</span> <span class="p_context"> static void vmx_set_constant_host_state(struct vcpu_vmx *vmx)</span>
 	u32 low32, high32;
 	unsigned long tmpl;
 	struct desc_ptr dt;
<span class="p_del">-	unsigned long cr4;</span>
<span class="p_add">+	unsigned long cr0, cr4;</span>
 
<span class="p_del">-	vmcs_writel(HOST_CR0, read_cr0() &amp; ~X86_CR0_TS);  /* 22.2.3 */</span>
<span class="p_add">+	cr0 = read_cr0();</span>
<span class="p_add">+	WARN_ON(cr0 &amp; X86_CR0_TS);</span>
<span class="p_add">+	vmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */</span>
 	vmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */
 
 	/* Save the most likely value for this task&#39;s CR4 in the VMCS. */
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 04c5d96b1d67..56900f3e7bcf 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -5081,11 +5081,6 @@</span> <span class="p_context"> static void emulator_get_fpu(struct x86_emulate_ctxt *ctxt)</span>
 {
 	preempt_disable();
 	kvm_load_guest_fpu(emul_to_vcpu(ctxt));
<span class="p_del">-	/*</span>
<span class="p_del">-	 * CR0.TS may reference the host fpu state, not the guest fpu state,</span>
<span class="p_del">-	 * so it may be clear at this point.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	clts();</span>
 }
 
 static void emulator_put_fpu(struct x86_emulate_ctxt *ctxt)
<span class="p_chunk">@@ -7407,25 +7402,13 @@</span> <span class="p_context"> void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)</span>
 
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	if (!vcpu-&gt;guest_fpu_loaded) {</span>
<span class="p_del">-		vcpu-&gt;fpu_counter = 0;</span>
<span class="p_add">+	if (!vcpu-&gt;guest_fpu_loaded)</span>
 		return;
<span class="p_del">-	}</span>
 
 	vcpu-&gt;guest_fpu_loaded = 0;
 	copy_fpregs_to_fpstate(&amp;vcpu-&gt;arch.guest_fpu);
 	__kernel_fpu_end();
 	++vcpu-&gt;stat.fpu_reload;
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If using eager FPU mode, or if the guest is a frequent user</span>
<span class="p_del">-	 * of the FPU, just leave the FPU active for next time.</span>
<span class="p_del">-	 * Every 255 times fpu_counter rolls over to 0; a guest that uses</span>
<span class="p_del">-	 * the FPU in bursts will revert to loading it on demand.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!use_eager_fpu()) {</span>
<span class="p_del">-		if (++vcpu-&gt;fpu_counter &lt; 5)</span>
<span class="p_del">-			kvm_make_request(KVM_REQ_DEACTIVATE_FPU, vcpu);</span>
<span class="p_del">-	}</span>
 	trace_kvm_fpu(0);
 }
 
<span class="p_header">diff --git a/arch/x86/lguest/boot.c b/arch/x86/lguest/boot.c</span>
<span class="p_header">index 25da5bc8d83d..4ca0d78adcf0 100644</span>
<span class="p_header">--- a/arch/x86/lguest/boot.c</span>
<span class="p_header">+++ b/arch/x86/lguest/boot.c</span>
<span class="p_chunk">@@ -497,38 +497,24 @@</span> <span class="p_context"> static void lguest_cpuid(unsigned int *ax, unsigned int *bx,</span>
  * a whole series of functions like read_cr0() and write_cr0().
  *
  * We start with cr0.  cr0 allows you to turn on and off all kinds of basic
<span class="p_del">- * features, but Linux only really cares about one: the horrifically-named Task</span>
<span class="p_del">- * Switched (TS) bit at bit 3 (ie. 8)</span>
<span class="p_add">+ * features, but the only cr0 bit that Linux ever used at runtime was the</span>
<span class="p_add">+ * horrifically-named Task Switched (TS) bit at bit 3 (ie. 8)</span>
  *
  * What does the TS bit do?  Well, it causes the CPU to trap (interrupt 7) if
  * the floating point unit is used.  Which allows us to restore FPU state
<span class="p_del">- * lazily after a task switch, and Linux uses that gratefully, but wouldn&#39;t a</span>
<span class="p_del">- * name like &quot;FPUTRAP bit&quot; be a little less cryptic?</span>
<span class="p_add">+ * lazily after a task switch if we wanted to, but wouldn&#39;t a name like</span>
<span class="p_add">+ * &quot;FPUTRAP bit&quot; be a little less cryptic?</span>
  *
<span class="p_del">- * We store cr0 locally because the Host never changes it.  The Guest sometimes</span>
<span class="p_del">- * wants to read it and we&#39;d prefer not to bother the Host unnecessarily.</span>
<span class="p_add">+ * Fortunately, Linux keeps it simple and doesn&#39;t use TS, so we can ignore</span>
<span class="p_add">+ * cr0.</span>
  */
<span class="p_del">-static unsigned long current_cr0;</span>
 static void lguest_write_cr0(unsigned long val)
 {
<span class="p_del">-	lazy_hcall1(LHCALL_TS, val &amp; X86_CR0_TS);</span>
<span class="p_del">-	current_cr0 = val;</span>
 }
 
 static unsigned long lguest_read_cr0(void)
 {
<span class="p_del">-	return current_cr0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Intel provided a special instruction to clear the TS bit for people too cool</span>
<span class="p_del">- * to use write_cr0() to do it.  This &quot;clts&quot; instruction is faster, because all</span>
<span class="p_del">- * the vowels have been optimized out.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void lguest_clts(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	lazy_hcall1(LHCALL_TS, 0);</span>
<span class="p_del">-	current_cr0 &amp;= ~X86_CR0_TS;</span>
<span class="p_add">+	return 0;</span>
 }
 
 /*
<span class="p_chunk">@@ -1432,7 +1418,6 @@</span> <span class="p_context"> __init void lguest_init(void)</span>
 	pv_cpu_ops.load_tls = lguest_load_tls;
 	pv_cpu_ops.get_debugreg = lguest_get_debugreg;
 	pv_cpu_ops.set_debugreg = lguest_set_debugreg;
<span class="p_del">-	pv_cpu_ops.clts = lguest_clts;</span>
 	pv_cpu_ops.read_cr0 = lguest_read_cr0;
 	pv_cpu_ops.write_cr0 = lguest_write_cr0;
 	pv_cpu_ops.read_cr4 = lguest_read_cr4;
<span class="p_header">diff --git a/arch/x86/mm/pkeys.c b/arch/x86/mm/pkeys.c</span>
<span class="p_header">index f88ce0e5efd9..2dab69a706ec 100644</span>
<span class="p_header">--- a/arch/x86/mm/pkeys.c</span>
<span class="p_header">+++ b/arch/x86/mm/pkeys.c</span>
<span class="p_chunk">@@ -141,8 +141,7 @@</span> <span class="p_context"> u32 init_pkru_value = PKRU_AD_KEY( 1) | PKRU_AD_KEY( 2) | PKRU_AD_KEY( 3) |</span>
  * Called from the FPU code when creating a fresh set of FPU
  * registers.  This is called from a very specific context where
  * we know the FPU regstiers are safe for use and we can use PKRU
<span class="p_del">- * directly.  The fact that PKRU is only available when we are</span>
<span class="p_del">- * using eagerfpu mode makes this possible.</span>
<span class="p_add">+ * directly.</span>
  */
 void copy_init_pkru_to_fpregs(void)
 {
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index bdd855685403..ced7027b3fbc 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -980,17 +980,6 @@</span> <span class="p_context"> static void xen_io_delay(void)</span>
 {
 }
 
<span class="p_del">-static void xen_clts(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct multicall_space mcs;</span>
<span class="p_del">-</span>
<span class="p_del">-	mcs = xen_mc_entry(0);</span>
<span class="p_del">-</span>
<span class="p_del">-	MULTI_fpu_taskswitch(mcs.mc, 0);</span>
<span class="p_del">-</span>
<span class="p_del">-	xen_mc_issue(PARAVIRT_LAZY_CPU);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static DEFINE_PER_CPU(unsigned long, xen_cr0_value);
 
 static unsigned long xen_read_cr0(void)
<span class="p_chunk">@@ -1233,8 +1222,6 @@</span> <span class="p_context"> static const struct pv_cpu_ops xen_cpu_ops __initconst = {</span>
 	.set_debugreg = xen_set_debugreg,
 	.get_debugreg = xen_get_debugreg,
 
<span class="p_del">-	.clts = xen_clts,</span>
<span class="p_del">-</span>
 	.read_cr0 = xen_read_cr0,
 	.write_cr0 = xen_write_cr0,
 
<span class="p_header">diff --git a/drivers/char/hw_random/via-rng.c b/drivers/char/hw_random/via-rng.c</span>
<span class="p_header">index 44ce80606944..d1f5bb534e0e 100644</span>
<span class="p_header">--- a/drivers/char/hw_random/via-rng.c</span>
<span class="p_header">+++ b/drivers/char/hw_random/via-rng.c</span>
<span class="p_chunk">@@ -70,21 +70,17 @@</span> <span class="p_context"> enum {</span>
  * until we have 4 bytes, thus returning a u32 at a time,
  * instead of the current u8-at-a-time.
  *
<span class="p_del">- * Padlock instructions can generate a spurious DNA fault, so</span>
<span class="p_del">- * we have to call them in the context of irq_ts_save/restore()</span>
<span class="p_add">+ * Padlock instructions can generate a spurious DNA fault, but the</span>
<span class="p_add">+ * kernel doesn&#39;t use CR0.TS, so this doesn&#39;t matter.</span>
  */
 
 static inline u32 xstore(u32 *addr, u32 edx_in)
 {
 	u32 eax_out;
<span class="p_del">-	int ts_state;</span>
<span class="p_del">-</span>
<span class="p_del">-	ts_state = irq_ts_save();</span>
 
 	asm(&quot;.byte 0x0F,0xA7,0xC0 /* xstore %%edi (addr=%0) */&quot;
 		: &quot;=m&quot; (*addr), &quot;=a&quot; (eax_out), &quot;+d&quot; (edx_in), &quot;+D&quot; (addr));
 
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 	return eax_out;
 }
 
<span class="p_header">diff --git a/drivers/crypto/padlock-aes.c b/drivers/crypto/padlock-aes.c</span>
<span class="p_header">index 441e86b23571..b3869748cc6b 100644</span>
<span class="p_header">--- a/drivers/crypto/padlock-aes.c</span>
<span class="p_header">+++ b/drivers/crypto/padlock-aes.c</span>
<span class="p_chunk">@@ -183,8 +183,8 @@</span> <span class="p_context"> static inline void padlock_store_cword(struct cword *cword)</span>
 
 /*
  * While the padlock instructions don&#39;t use FP/SSE registers, they
<span class="p_del">- * generate a spurious DNA fault when cr0.ts is &#39;1&#39;. These instructions</span>
<span class="p_del">- * should be used only inside the irq_ts_save/restore() context</span>
<span class="p_add">+ * generate a spurious DNA fault when CR0.TS is &#39;1&#39;.  Fortunately,</span>
<span class="p_add">+ * the kernel doesn&#39;t use CR0.TS.</span>
  */
 
 static inline void rep_xcrypt_ecb(const u8 *input, u8 *output, void *key,
<span class="p_chunk">@@ -298,24 +298,18 @@</span> <span class="p_context"> static inline u8 *padlock_xcrypt_cbc(const u8 *input, u8 *output, void *key,</span>
 static void aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)
 {
 	struct aes_ctx *ctx = aes_ctx(tfm);
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.encrypt);
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	ecb_crypt(in, out, ctx-&gt;E, &amp;ctx-&gt;cword.encrypt, 1);
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 	padlock_store_cword(&amp;ctx-&gt;cword.encrypt);
 }
 
 static void aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)
 {
 	struct aes_ctx *ctx = aes_ctx(tfm);
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.encrypt);
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	ecb_crypt(in, out, ctx-&gt;D, &amp;ctx-&gt;cword.decrypt, 1);
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 	padlock_store_cword(&amp;ctx-&gt;cword.encrypt);
 }
 
<span class="p_chunk">@@ -346,14 +340,12 @@</span> <span class="p_context"> static int ecb_aes_encrypt(struct blkcipher_desc *desc,</span>
 	struct aes_ctx *ctx = blk_aes_ctx(desc-&gt;tfm);
 	struct blkcipher_walk walk;
 	int err;
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.encrypt);
 
 	blkcipher_walk_init(&amp;walk, dst, src, nbytes);
 	err = blkcipher_walk_virt(desc, &amp;walk);
 
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	while ((nbytes = walk.nbytes)) {
 		padlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,
 				   ctx-&gt;E, &amp;ctx-&gt;cword.encrypt,
<span class="p_chunk">@@ -361,7 +353,6 @@</span> <span class="p_context"> static int ecb_aes_encrypt(struct blkcipher_desc *desc,</span>
 		nbytes &amp;= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &amp;walk, nbytes);
 	}
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 
 	padlock_store_cword(&amp;ctx-&gt;cword.encrypt);
 
<span class="p_chunk">@@ -375,14 +366,12 @@</span> <span class="p_context"> static int ecb_aes_decrypt(struct blkcipher_desc *desc,</span>
 	struct aes_ctx *ctx = blk_aes_ctx(desc-&gt;tfm);
 	struct blkcipher_walk walk;
 	int err;
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.decrypt);
 
 	blkcipher_walk_init(&amp;walk, dst, src, nbytes);
 	err = blkcipher_walk_virt(desc, &amp;walk);
 
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	while ((nbytes = walk.nbytes)) {
 		padlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,
 				   ctx-&gt;D, &amp;ctx-&gt;cword.decrypt,
<span class="p_chunk">@@ -390,7 +379,6 @@</span> <span class="p_context"> static int ecb_aes_decrypt(struct blkcipher_desc *desc,</span>
 		nbytes &amp;= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &amp;walk, nbytes);
 	}
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 
 	padlock_store_cword(&amp;ctx-&gt;cword.encrypt);
 
<span class="p_chunk">@@ -425,14 +413,12 @@</span> <span class="p_context"> static int cbc_aes_encrypt(struct blkcipher_desc *desc,</span>
 	struct aes_ctx *ctx = blk_aes_ctx(desc-&gt;tfm);
 	struct blkcipher_walk walk;
 	int err;
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.encrypt);
 
 	blkcipher_walk_init(&amp;walk, dst, src, nbytes);
 	err = blkcipher_walk_virt(desc, &amp;walk);
 
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	while ((nbytes = walk.nbytes)) {
 		u8 *iv = padlock_xcrypt_cbc(walk.src.virt.addr,
 					    walk.dst.virt.addr, ctx-&gt;E,
<span class="p_chunk">@@ -442,7 +428,6 @@</span> <span class="p_context"> static int cbc_aes_encrypt(struct blkcipher_desc *desc,</span>
 		nbytes &amp;= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &amp;walk, nbytes);
 	}
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 
 	padlock_store_cword(&amp;ctx-&gt;cword.decrypt);
 
<span class="p_chunk">@@ -456,14 +441,12 @@</span> <span class="p_context"> static int cbc_aes_decrypt(struct blkcipher_desc *desc,</span>
 	struct aes_ctx *ctx = blk_aes_ctx(desc-&gt;tfm);
 	struct blkcipher_walk walk;
 	int err;
<span class="p_del">-	int ts_state;</span>
 
 	padlock_reset_key(&amp;ctx-&gt;cword.encrypt);
 
 	blkcipher_walk_init(&amp;walk, dst, src, nbytes);
 	err = blkcipher_walk_virt(desc, &amp;walk);
 
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	while ((nbytes = walk.nbytes)) {
 		padlock_xcrypt_cbc(walk.src.virt.addr, walk.dst.virt.addr,
 				   ctx-&gt;D, walk.iv, &amp;ctx-&gt;cword.decrypt,
<span class="p_chunk">@@ -472,8 +455,6 @@</span> <span class="p_context"> static int cbc_aes_decrypt(struct blkcipher_desc *desc,</span>
 		err = blkcipher_walk_done(desc, &amp;walk, nbytes);
 	}
 
<span class="p_del">-	irq_ts_restore(ts_state);</span>
<span class="p_del">-</span>
 	padlock_store_cword(&amp;ctx-&gt;cword.encrypt);
 
 	return err;
<span class="p_header">diff --git a/drivers/crypto/padlock-sha.c b/drivers/crypto/padlock-sha.c</span>
<span class="p_header">index 8c5f90647b7a..bc72d20c32c3 100644</span>
<span class="p_header">--- a/drivers/crypto/padlock-sha.c</span>
<span class="p_header">+++ b/drivers/crypto/padlock-sha.c</span>
<span class="p_chunk">@@ -89,7 +89,6 @@</span> <span class="p_context"> static int padlock_sha1_finup(struct shash_desc *desc, const u8 *in,</span>
 	struct sha1_state state;
 	unsigned int space;
 	unsigned int leftover;
<span class="p_del">-	int ts_state;</span>
 	int err;
 
 	dctx-&gt;fallback.flags = desc-&gt;flags &amp; CRYPTO_TFM_REQ_MAY_SLEEP;
<span class="p_chunk">@@ -120,14 +119,11 @@</span> <span class="p_context"> static int padlock_sha1_finup(struct shash_desc *desc, const u8 *in,</span>
 
 	memcpy(result, &amp;state.state, SHA1_DIGEST_SIZE);
 
<span class="p_del">-	/* prevent taking the spurious DNA fault with padlock. */</span>
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xc8&quot; /* rep xsha1 */
 		      : \
 		      : &quot;c&quot;((unsigned long)state.count + count), \
 			&quot;a&quot;((unsigned long)state.count), \
 			&quot;S&quot;(in), &quot;D&quot;(result));
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 
 	padlock_output_block((uint32_t *)result, (uint32_t *)out, 5);
 
<span class="p_chunk">@@ -155,7 +151,6 @@</span> <span class="p_context"> static int padlock_sha256_finup(struct shash_desc *desc, const u8 *in,</span>
 	struct sha256_state state;
 	unsigned int space;
 	unsigned int leftover;
<span class="p_del">-	int ts_state;</span>
 	int err;
 
 	dctx-&gt;fallback.flags = desc-&gt;flags &amp; CRYPTO_TFM_REQ_MAY_SLEEP;
<span class="p_chunk">@@ -186,14 +181,11 @@</span> <span class="p_context"> static int padlock_sha256_finup(struct shash_desc *desc, const u8 *in,</span>
 
 	memcpy(result, &amp;state.state, SHA256_DIGEST_SIZE);
 
<span class="p_del">-	/* prevent taking the spurious DNA fault with padlock. */</span>
<span class="p_del">-	ts_state = irq_ts_save();</span>
 	asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xd0&quot; /* rep xsha256 */
 		      : \
 		      : &quot;c&quot;((unsigned long)state.count + count), \
 			&quot;a&quot;((unsigned long)state.count), \
 			&quot;S&quot;(in), &quot;D&quot;(result));
<span class="p_del">-	irq_ts_restore(ts_state);</span>
 
 	padlock_output_block((uint32_t *)result, (uint32_t *)out, 8);
 
<span class="p_chunk">@@ -312,7 +304,6 @@</span> <span class="p_context"> static int padlock_sha1_update_nano(struct shash_desc *desc,</span>
 	u8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__
 		((aligned(STACK_ALIGN)));
 	u8 *dst = PTR_ALIGN(&amp;buf[0], PADLOCK_ALIGNMENT);
<span class="p_del">-	int ts_state;</span>
 
 	partial = sctx-&gt;count &amp; 0x3f;
 	sctx-&gt;count += len;
<span class="p_chunk">@@ -328,23 +319,19 @@</span> <span class="p_context"> static int padlock_sha1_update_nano(struct shash_desc *desc,</span>
 			memcpy(sctx-&gt;buffer + partial, data,
 				done + SHA1_BLOCK_SIZE);
 			src = sctx-&gt;buffer;
<span class="p_del">-			ts_state = irq_ts_save();</span>
 			asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xc8&quot;
 			: &quot;+S&quot;(src), &quot;+D&quot;(dst) \
 			: &quot;a&quot;((long)-1), &quot;c&quot;((unsigned long)1));
<span class="p_del">-			irq_ts_restore(ts_state);</span>
 			done += SHA1_BLOCK_SIZE;
 			src = data + done;
 		}
 
 		/* Process the left bytes from the input data */
 		if (len - done &gt;= SHA1_BLOCK_SIZE) {
<span class="p_del">-			ts_state = irq_ts_save();</span>
 			asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xc8&quot;
 			: &quot;+S&quot;(src), &quot;+D&quot;(dst)
 			: &quot;a&quot;((long)-1),
 			&quot;c&quot;((unsigned long)((len - done) / SHA1_BLOCK_SIZE)));
<span class="p_del">-			irq_ts_restore(ts_state);</span>
 			done += ((len - done) - (len - done) % SHA1_BLOCK_SIZE);
 			src = data + done;
 		}
<span class="p_chunk">@@ -401,7 +388,6 @@</span> <span class="p_context"> static int padlock_sha256_update_nano(struct shash_desc *desc, const u8 *data,</span>
 	u8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__
 		((aligned(STACK_ALIGN)));
 	u8 *dst = PTR_ALIGN(&amp;buf[0], PADLOCK_ALIGNMENT);
<span class="p_del">-	int ts_state;</span>
 
 	partial = sctx-&gt;count &amp; 0x3f;
 	sctx-&gt;count += len;
<span class="p_chunk">@@ -417,23 +403,19 @@</span> <span class="p_context"> static int padlock_sha256_update_nano(struct shash_desc *desc, const u8 *data,</span>
 			memcpy(sctx-&gt;buf + partial, data,
 				done + SHA256_BLOCK_SIZE);
 			src = sctx-&gt;buf;
<span class="p_del">-			ts_state = irq_ts_save();</span>
 			asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xd0&quot;
 			: &quot;+S&quot;(src), &quot;+D&quot;(dst)
 			: &quot;a&quot;((long)-1), &quot;c&quot;((unsigned long)1));
<span class="p_del">-			irq_ts_restore(ts_state);</span>
 			done += SHA256_BLOCK_SIZE;
 			src = data + done;
 		}
 
 		/* Process the left bytes from input data*/
 		if (len - done &gt;= SHA256_BLOCK_SIZE) {
<span class="p_del">-			ts_state = irq_ts_save();</span>
 			asm volatile (&quot;.byte 0xf3,0x0f,0xa6,0xd0&quot;
 			: &quot;+S&quot;(src), &quot;+D&quot;(dst)
 			: &quot;a&quot;((long)-1),
 			&quot;c&quot;((unsigned long)((len - done) / 64)));
<span class="p_del">-			irq_ts_restore(ts_state);</span>
 			done += ((len - done) - (len - done) % 64);
 			src = data + done;
 		}
<span class="p_header">diff --git a/drivers/lguest/hypercalls.c b/drivers/lguest/hypercalls.c</span>
<span class="p_header">index 19a32280731d..601f81c04873 100644</span>
<span class="p_header">--- a/drivers/lguest/hypercalls.c</span>
<span class="p_header">+++ b/drivers/lguest/hypercalls.c</span>
<span class="p_chunk">@@ -109,10 +109,6 @@</span> <span class="p_context"> static void do_hcall(struct lg_cpu *cpu, struct hcall_args *args)</span>
 	case LHCALL_SET_CLOCKEVENT:
 		guest_set_clockevent(cpu, args-&gt;arg1);
 		break;
<span class="p_del">-	case LHCALL_TS:</span>
<span class="p_del">-		/* This sets the TS flag, as we saw used in run_guest(). */</span>
<span class="p_del">-		cpu-&gt;ts = args-&gt;arg1;</span>
<span class="p_del">-		break;</span>
 	case LHCALL_HALT:
 		/* Similarly, this sets the halted flag for run_guest(). */
 		cpu-&gt;halted = 1;
<span class="p_header">diff --git a/drivers/lguest/lg.h b/drivers/lguest/lg.h</span>
<span class="p_header">index 69b3814afd2f..2356a2318034 100644</span>
<span class="p_header">--- a/drivers/lguest/lg.h</span>
<span class="p_header">+++ b/drivers/lguest/lg.h</span>
<span class="p_chunk">@@ -43,7 +43,6 @@</span> <span class="p_context"> struct lg_cpu {</span>
 	struct mm_struct *mm; 	/* == tsk-&gt;mm, but that becomes NULL on exit */
 
 	u32 cr2;
<span class="p_del">-	int ts;</span>
 	u32 esp1;
 	u16 ss1;
 
<span class="p_header">diff --git a/drivers/lguest/x86/core.c b/drivers/lguest/x86/core.c</span>
<span class="p_header">index 6e9042e3d2a9..743253fc638f 100644</span>
<span class="p_header">--- a/drivers/lguest/x86/core.c</span>
<span class="p_header">+++ b/drivers/lguest/x86/core.c</span>
<span class="p_chunk">@@ -247,14 +247,6 @@</span> <span class="p_context"> unsigned long *lguest_arch_regptr(struct lg_cpu *cpu, size_t reg_off, bool any)</span>
 void lguest_arch_run_guest(struct lg_cpu *cpu)
 {
 	/*
<span class="p_del">-	 * Remember the awfully-named TS bit?  If the Guest has asked to set it</span>
<span class="p_del">-	 * we set it now, so we can trap and pass that trap to the Guest if it</span>
<span class="p_del">-	 * uses the FPU.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (cpu-&gt;ts &amp;&amp; fpregs_active())</span>
<span class="p_del">-		stts();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
 	 * SYSENTER is an optimized way of doing system calls.  We can&#39;t allow
 	 * it because it always jumps to privilege level 0.  A normal Guest
 	 * won&#39;t try it because we don&#39;t advertise it in CPUID, but a malicious
<span class="p_chunk">@@ -282,10 +274,6 @@</span> <span class="p_context"> void lguest_arch_run_guest(struct lg_cpu *cpu)</span>
 	 if (boot_cpu_has(X86_FEATURE_SEP))
 		wrmsr(MSR_IA32_SYSENTER_CS, __KERNEL_CS, 0);
 
<span class="p_del">-	/* Clear the host TS bit if it was set above. */</span>
<span class="p_del">-	if (cpu-&gt;ts &amp;&amp; fpregs_active())</span>
<span class="p_del">-		clts();</span>
<span class="p_del">-</span>
 	/*
 	 * If the Guest page faulted, then the cr2 register will tell us the
 	 * bad virtual address.  We have to grab this now, because once we
<span class="p_chunk">@@ -421,12 +409,7 @@</span> <span class="p_context"> void lguest_arch_handle_trap(struct lg_cpu *cpu)</span>
 			kill_guest(cpu, &quot;Writing cr2&quot;);
 		break;
 	case 7: /* We&#39;ve intercepted a Device Not Available fault. */
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If the Guest doesn&#39;t want to know, we already restored the</span>
<span class="p_del">-		 * Floating Point Unit, so we just continue without telling it.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (!cpu-&gt;ts)</span>
<span class="p_del">-			return;</span>
<span class="p_add">+		/* No special handling is needed here. */</span>
 		break;
 	case 32 ... 255:
 		/* This might be a syscall. */
<span class="p_header">diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="p_header">index 01c0b9cc3915..cfc212d1cd60 100644</span>
<span class="p_header">--- a/include/linux/kvm_host.h</span>
<span class="p_header">+++ b/include/linux/kvm_host.h</span>
<span class="p_chunk">@@ -224,7 +224,6 @@</span> <span class="p_context"> struct kvm_vcpu {</span>
 
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
<span class="p_del">-	unsigned char fpu_counter;</span>
 	struct swait_queue_head wq;
 	struct pid *pid;
 	int sigset_active;
<span class="p_header">diff --git a/tools/arch/x86/include/asm/cpufeatures.h b/tools/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index a39629206864..cddd5d06e1cb 100644</span>
<span class="p_header">--- a/tools/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/tools/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -104,7 +104,6 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_EXTD_APICID	( 3*32+26) /* has extended APICID (8 bits) */
 #define X86_FEATURE_AMD_DCM     ( 3*32+27) /* multi-node processor */
 #define X86_FEATURE_APERFMPERF	( 3*32+28) /* APERFMPERF */
<span class="p_del">-#define X86_FEATURE_EAGER_FPU	( 3*32+29) /* &quot;eagerfpu&quot; Non lazy FPU restore */</span>
 #define X86_FEATURE_NONSTOP_TSC_S3 ( 3*32+30) /* TSC doesn&#39;t stop in S3 state */
 
 /* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



