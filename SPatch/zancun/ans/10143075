
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4.4,18/37] kaiser: enhanced by kernel and user PCIDs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4.4,18/37] kaiser: enhanced by kernel and user PCIDs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 3, 2018, 8:11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180103195057.786692340@linuxfoundation.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10143075/mbox/"
   >mbox</a>
|
   <a href="/patch/10143075/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10143075/">/patch/10143075/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	449BD60594 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 20:12:23 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 33947292B0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 20:12:23 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 28800292CE; Wed,  3 Jan 2018 20:12:23 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 170DB292C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 20:12:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751797AbeACUMT (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 3 Jan 2018 15:12:19 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:60916 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751689AbeACUMQ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 3 Jan 2018 15:12:16 -0500
Received: from localhost (LFbn-1-12258-90.w90-92.abo.wanadoo.fr
	[90.92.71.90])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 2385E89F;
	Wed,  3 Jan 2018 20:12:14 +0000 (UTC)
From: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	stable@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Jiri Kosina &lt;jkosina@suse.cz&gt;
Subject: [PATCH 4.4 18/37] kaiser: enhanced by kernel and user PCIDs
Date: Wed,  3 Jan 2018 21:11:24 +0100
Message-Id: &lt;20180103195057.786692340@linuxfoundation.org&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20180103195056.837404126@linuxfoundation.org&gt;
References: &lt;20180103195056.837404126@linuxfoundation.org&gt;
User-Agent: quilt/0.65
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 3, 2018, 8:11 p.m.</div>
<pre class="content">
4.4-stable review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>


Merged performance improvements to Kaiser, using distinct kernel
and user Process Context Identifiers to minimize the TLB flushing.
<span class="signed-off-by">
Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="acked-by">Acked-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>
<span class="signed-off-by">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span>
---
 arch/x86/entry/entry_64.S                   |   10 ++++-
 arch/x86/entry/entry_64_compat.S            |    1 
 arch/x86/include/asm/cpufeature.h           |    1 
 arch/x86/include/asm/kaiser.h               |   15 ++++++-
 arch/x86/include/asm/pgtable_types.h        |   26 +++++++++++++
 arch/x86/include/asm/tlbflush.h             |   54 +++++++++++++++++++++++-----
 arch/x86/include/uapi/asm/processor-flags.h |    3 +
 arch/x86/kernel/cpu/common.c                |   34 +++++++++++++++++
 arch/x86/kvm/x86.c                          |    3 +
 arch/x86/mm/kaiser.c                        |    7 +++
 arch/x86/mm/tlb.c                           |   46 ++++++++++++++++++++++-
 11 files changed, 182 insertions(+), 18 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -1291,7 +1291,10 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	/* %rax is saved above, so OK to clobber here */
 	movq	%cr3, %rax
 	pushq	%rax
<span class="p_del">-	andq	$(~KAISER_SHADOW_PGD_OFFSET), %rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* Add back kernel PCID and &quot;no flush&quot; bit */</span>
<span class="p_add">+	orq	X86_CR3_PCID_KERN_VAR, %rax</span>
 	movq	%rax, %cr3
 #endif
 	call	do_nmi
<span class="p_chunk">@@ -1532,7 +1535,10 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 	/* %rax is saved above, so OK to clobber here */
 	movq	%cr3, %rax
 	pushq	%rax
<span class="p_del">-	andq	$(~KAISER_SHADOW_PGD_OFFSET), %rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* Add back kernel PCID and &quot;no flush&quot; bit */</span>
<span class="p_add">+	orq	X86_CR3_PCID_KERN_VAR, %rax</span>
 	movq	%rax, %cr3
 #endif
 
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/irqflags.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/pgtable_types.h&gt;</span>
 #include &lt;asm/kaiser.h&gt;
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/err.h&gt;
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -187,6 +187,7 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_ARAT	( 7*32+ 1) /* Always Running APIC Timer */
 #define X86_FEATURE_CPB		( 7*32+ 2) /* AMD Core Performance Boost */
 #define X86_FEATURE_EPB		( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE ( 7*32+ 4) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
 #define X86_FEATURE_PLN		( 7*32+ 5) /* Intel Power Limit Notification */
 #define X86_FEATURE_PTS		( 7*32+ 6) /* Intel Package Thermal Status */
 #define X86_FEATURE_DTHERM	( 7*32+ 7) /* Digital Thermal Sensor */
<span class="p_header">--- a/arch/x86/include/asm/kaiser.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h</span>
<span class="p_chunk">@@ -1,5 +1,8 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_KAISER_H
 #define _ASM_X86_KAISER_H
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/processor-flags.h&gt; /* For PCID constants */</span>
<span class="p_add">+</span>
 /*
  * This file includes the definitions for the KAISER feature.
  * KAISER is a counter measure against x86_64 side channel attacks on
<span class="p_chunk">@@ -21,13 +24,21 @@</span> <span class="p_context"></span>
 
 .macro _SWITCH_TO_KERNEL_CR3 reg
 movq %cr3, \reg
<span class="p_del">-andq $(~KAISER_SHADOW_PGD_OFFSET), \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+orq  X86_CR3_PCID_KERN_VAR, \reg</span>
 movq \reg, %cr3
 .endm
 
 .macro _SWITCH_TO_USER_CR3 reg
 movq %cr3, \reg
<span class="p_del">-orq $(KAISER_SHADOW_PGD_OFFSET), \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This can obviously be one instruction by putting the</span>
<span class="p_add">+ * KAISER_SHADOW_PGD_OFFSET bit in the X86_CR3_PCID_USER_VAR.</span>
<span class="p_add">+ * But, just leave it now for simplicity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+orq  X86_CR3_PCID_USER_VAR, \reg</span>
<span class="p_add">+orq  $(KAISER_SHADOW_PGD_OFFSET), \reg</span>
 movq \reg, %cr3
 .endm
 
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -106,6 +106,32 @@</span> <span class="p_context"></span>
 			 _PAGE_SOFT_DIRTY)
 #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
 
<span class="p_add">+/* The ASID is the lower 12 bits of CR3 */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_MASK  (_AC((1&lt;&lt;12)-1,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Mask for all the PCID-related bits in CR3: */</span>
<span class="p_add">+#define X86_CR3_PCID_MASK       (X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_MASK)</span>
<span class="p_add">+#if defined(CONFIG_KAISER) &amp;&amp; defined(CONFIG_X86_64)</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x4,UL))</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x6,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x0,UL))</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x0,UL))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PCIDs are unsupported on 32-bit and none of these bits can be</span>
<span class="p_add">+ * set in CR3:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * The cache modes defined here are used to translate between pure SW usage
  * and the HW defined cache mode bits and/or PAT entries.
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -12,7 +12,6 @@</span> <span class="p_context"> static inline void __invpcid(unsigned lo</span>
 			     unsigned long type)
 {
 	struct { u64 d[2]; } desc = { { pcid, addr } };
<span class="p_del">-</span>
 	/*
 	 * The memory clobber is because the whole point is to invalidate
 	 * stale TLB entries and, especially if we&#39;re flushing global
<span class="p_chunk">@@ -133,14 +132,25 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_upda</span>
 
 static inline void __native_flush_tlb(void)
 {
<span class="p_add">+	if (!cpu_feature_enabled(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+	 	/*</span>
<span class="p_add">+		 * If current-&gt;mm == NULL then we borrow a mm which may change during a</span>
<span class="p_add">+		 * task switch and therefore we must not be preempted while we write CR3</span>
<span class="p_add">+		 * back:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		preempt_disable();</span>
<span class="p_add">+		native_write_cr3(native_read_cr3());</span>
<span class="p_add">+		preempt_enable();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
 	/*
<span class="p_del">-	 * If current-&gt;mm == NULL then we borrow a mm which may change during a</span>
<span class="p_del">-	 * task switch and therefore we must not be preempted while we write CR3</span>
<span class="p_del">-	 * back:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	native_write_cr3(native_read_cr3());</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_add">+	 * We are no longer using globals with KAISER, so a</span>
<span class="p_add">+	 * &quot;nonglobals&quot; flush would work too. But, this is more</span>
<span class="p_add">+	 * conservative.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note, this works with CR4.PCIDE=0 or 1.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invpcid_flush_all();</span>
 }
 
 static inline void __native_flush_tlb_global_irq_disabled(void)
<span class="p_chunk">@@ -162,6 +172,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb_gl</span>
 		/*
 		 * Using INVPCID is considerably faster than a pair of writes
 		 * to CR4 sandwiched inside an IRQ flag save/restore.
<span class="p_add">+		 *</span>
<span class="p_add">+	 	 * Note, this works with CR4.PCIDE=0 or 1.</span>
 		 */
 		invpcid_flush_all();
 		return;
<span class="p_chunk">@@ -181,7 +193,31 @@</span> <span class="p_context"> static inline void __native_flush_tlb_gl</span>
 
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_del">-	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SIMICS #GP&#39;s if you run INVPCID with type 2/3</span>
<span class="p_add">+	 * and X86_CR4_PCIDE clear.  Shame!</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The ASIDs used below are hard-coded.  But, we must not</span>
<span class="p_add">+	 * call invpcid(type=1/2) before CR4.PCIDE=1.  Just call</span>
<span class="p_add">+	 * invpcid in the case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE)) {</span>
<span class="p_add">+		asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Flush the address out of both PCIDs. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * An optimization here might be to determine addresses</span>
<span class="p_add">+	 * that are only kernel-mapped and only flush the kernel</span>
<span class="p_add">+	 * ASID.  But, userspace flushes are probably much more</span>
<span class="p_add">+	 * important performance-wise.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Make sure to do only a single invpcid when KAISER is</span>
<span class="p_add">+	 * disabled and we have only a single ASID.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (X86_CR3_PCID_ASID_KERN != X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+		invpcid_flush_one(X86_CR3_PCID_ASID_KERN, addr);</span>
<span class="p_add">+	invpcid_flush_one(X86_CR3_PCID_ASID_USER, addr);</span>
 }
 
 static inline void __flush_tlb_all(void)
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -77,7 +77,8 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -321,11 +321,45 @@</span> <span class="p_context"> static __always_inline void setup_smap(s</span>
 	}
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * These can have bit 63 set, so we can not just use a plain &quot;or&quot;</span>
<span class="p_add">+ * instruction to get their value or&#39;d into CR3.  It would take</span>
<span class="p_add">+ * another register.  So, we use a memory reference to these</span>
<span class="p_add">+ * instead.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is also handy because systems that do not support</span>
<span class="p_add">+ * PCIDs just end up or&#39;ing a 0 into their CR3, which does</span>
<span class="p_add">+ * no harm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+__aligned(PAGE_SIZE) unsigned long X86_CR3_PCID_KERN_VAR = 0;</span>
<span class="p_add">+__aligned(PAGE_SIZE) unsigned long X86_CR3_PCID_USER_VAR = 0;</span>
<span class="p_add">+</span>
 static void setup_pcid(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_PCID)) {
 		if (cpu_has(c, X86_FEATURE_PGE)) {
 			cr4_set_bits(X86_CR4_PCIDE);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * These variables are used by the entry/exit</span>
<span class="p_add">+			 * code to change PCIDs.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+			X86_CR3_PCID_KERN_VAR = X86_CR3_PCID_KERN_NOFLUSH;</span>
<span class="p_add">+			X86_CR3_PCID_USER_VAR = X86_CR3_PCID_USER_NOFLUSH;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * INVPCID has two &quot;groups&quot; of types:</span>
<span class="p_add">+			 * 1/2: Invalidate an individual address</span>
<span class="p_add">+			 * 3/4: Invalidate all contexts</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * 1/2 take a PCID, but 3/4 do not.  So, 3/4</span>
<span class="p_add">+			 * ignore the PCID argument in the descriptor.</span>
<span class="p_add">+			 * But, we have to be careful not to call 1/2</span>
<span class="p_add">+			 * with an actual non-zero PCID in them before</span>
<span class="p_add">+			 * we do the above cr4_set_bits().</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (cpu_has(c, X86_FEATURE_INVPCID))</span>
<span class="p_add">+				set_cpu_cap(c, X86_FEATURE_INVPCID_SINGLE);</span>
 		} else {
 			/*
 			 * flush_tlb_all(), as currently implemented, won&#39;t
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -759,7 +759,8 @@</span> <span class="p_context"> int kvm_set_cr4(struct kvm_vcpu *vcpu, u</span>
 			return 1;
 
 		/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */
<span class="p_del">-		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_MASK) || !is_long_mode(vcpu))</span>
<span class="p_add">+		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_ASID_MASK) ||</span>
<span class="p_add">+		    !is_long_mode(vcpu))</span>
 			return 1;
 	}
 
<span class="p_header">--- a/arch/x86/mm/kaiser.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c</span>
<span class="p_chunk">@@ -240,6 +240,8 @@</span> <span class="p_context"> static void __init kaiser_init_all_pgds(</span>
 } while (0)
 
 extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];
<span class="p_add">+extern unsigned long X86_CR3_PCID_KERN_VAR;</span>
<span class="p_add">+extern unsigned long X86_CR3_PCID_USER_VAR;</span>
 /*
  * If anything in here fails, we will likely die on one of the
  * first kernel-&gt;user transitions and init will die.  But, we
<span class="p_chunk">@@ -290,6 +292,11 @@</span> <span class="p_context"> void __init kaiser_init(void)</span>
 	kaiser_add_user_map_early(&amp;debug_idt_table,
 				  sizeof(gate_desc) * NR_VECTORS,
 				  __PAGE_KERNEL);
<span class="p_add">+</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;X86_CR3_PCID_KERN_VAR, PAGE_SIZE,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;X86_CR3_PCID_USER_VAR, PAGE_SIZE,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
 }
 
 /* Add a mapping to the shadow mapping, and synchronize the mappings */
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -34,6 +34,46 @@</span> <span class="p_context"> struct flush_tlb_info {</span>
 	unsigned long flush_end;
 };
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3 = __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * KAISER, plus PCIDs needs some extra work here.  But,</span>
<span class="p_add">+	 * if either of features is not present, we need no</span>
<span class="p_add">+	 * PCIDs here and just do a normal, full TLB flush with</span>
<span class="p_add">+	 * the write_cr3()</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_KAISER) ||</span>
<span class="p_add">+	    !cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_add">+		goto out_set_cr3;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We reuse the same PCID for different tasks, so we must</span>
<span class="p_add">+	 * flush all the entires for the PCID out when we change</span>
<span class="p_add">+	 * tasks.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	new_mm_cr3 = X86_CR3_PCID_KERN_FLUSH | __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The flush from load_cr3() may leave old TLB entries</span>
<span class="p_add">+	 * for userspace in place.  We must flush that context</span>
<span class="p_add">+	 * separately.  We can theoretically delay doing this</span>
<span class="p_add">+	 * until we actually load up the userspace CR3, but</span>
<span class="p_add">+	 * that&#39;s a bit tricky.  We have to have the &quot;need to</span>
<span class="p_add">+	 * flush userspace PCID&quot; bit per-cpu and check it in the</span>
<span class="p_add">+	 * exit-to-userspace paths.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invpcid_flush_single_context(X86_CR3_PCID_ASID_USER);</span>
<span class="p_add">+</span>
<span class="p_add">+out_set_cr3:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * We cannot call mmdrop() because we are in interrupt context,
  * instead update mm-&gt;cpu_vm_mask.
<span class="p_chunk">@@ -45,7 +85,7 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		BUG();
 	if (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {
 		cpumask_clear_cpu(cpu, mm_cpumask(active_mm));
<span class="p_del">-		load_cr3(swapper_pg_dir);</span>
<span class="p_add">+		load_new_mm_cr3(swapper_pg_dir);</span>
 		/*
 		 * This gets called in the idle path where RCU
 		 * functions differently.  Tracing normally
<span class="p_chunk">@@ -105,7 +145,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 		 * ordering guarantee we need.
 		 *
 		 */
<span class="p_del">-		load_cr3(next-&gt;pgd);</span>
<span class="p_add">+		load_new_mm_cr3(next-&gt;pgd);</span>
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 
<span class="p_chunk">@@ -152,7 +192,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 			 * As above, load_cr3() is serializing and orders TLB
 			 * fills with respect to the mm_cpumask write.
 			 */
<span class="p_del">-			load_cr3(next-&gt;pgd);</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd);</span>
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 			load_mm_cr4(next);
 			load_mm_ldt(next);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



