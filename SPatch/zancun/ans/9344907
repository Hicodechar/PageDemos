
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,1/1] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,1/1] mm/hugetlb: fix memory offline with hugepage size &gt; memory block size</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 22, 2016, 9:51 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160922095137.GC11875@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9344907/mbox/"
   >mbox</a>
|
   <a href="/patch/9344907/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9344907/">/patch/9344907/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0E09C60757 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 09:51:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F408F285FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 09:51:57 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E899C2A936; Thu, 22 Sep 2016 09:51:57 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 658EB285FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Sep 2016 09:51:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757079AbcIVJvv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Sep 2016 05:51:51 -0400
Received: from mail-wm0-f67.google.com ([74.125.82.67]:33519 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756160AbcIVJvp (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Sep 2016 05:51:45 -0400
Received: by mail-wm0-f67.google.com with SMTP id w84so13054196wmg.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 22 Sep 2016 02:51:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:date:from:to:cc:subject:message-id:references
	:mime-version:content-disposition:in-reply-to:user-agent;
	bh=Y1ur8SFqUnDwpT92JTZI2+s8clEP5P6ZimhPwMTRWbg=;
	b=g9EzIxm72dhmLDLyge9yRBMLmqe/fyFjI3lEaj9CWKLakAFC8ab6rmFAZjnF/aajyx
	ZwsbaQNh3HAWJs+enUVZD/AqT1/ECcCUbgUkC9vt/ZXCjg8o+iINvmnTS6r0Z7ajUvTF
	Hm38zUF0BaS+sJNflsesHBFSKHvoayq4GfQ/qPNpKN+yJ57juuJwn9PtlflNES7z2HuG
	H+whzMlRtg5IFo2KWYdwpY9ureHv5EwvXJRb2IoxB75mE0PcUZjHxXw6uoB79oPKEqy/
	yhacUSQWWBeV0tgBDzQTfnBhPuBuSGAaxrtP6lBkfqxZArb5+aLw6fGaglvrj62rrmE1
	Mk6Q==
X-Gm-Message-State: AE9vXwOg0NbYoNu+9hLmlqz6Ls7yzddI+q7ukcJnw7fN+F2mk0jIIggy4Mx5Vcru3egujA==
X-Received: by 10.194.233.102 with SMTP id tv6mr1169553wjc.35.1474537899345; 
	Thu, 22 Sep 2016 02:51:39 -0700 (PDT)
Received: from localhost ([213.151.95.130]) by smtp.gmail.com with ESMTPSA id
	u124sm36853058wmu.10.2016.09.22.02.51.38
	(version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
	Thu, 22 Sep 2016 02:51:38 -0700 (PDT)
Date: Thu, 22 Sep 2016 11:51:37 +0200
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	&quot;Aneesh Kumar K . V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt;
Subject: Re: [PATCH v2 1/1] mm/hugetlb: fix memory offline with hugepage size
	&gt; memory block size
Message-ID: &lt;20160922095137.GC11875@dhcp22.suse.cz&gt;
References: &lt;20160920155354.54403-1-gerald.schaefer@de.ibm.com&gt;
	&lt;20160920155354.54403-2-gerald.schaefer@de.ibm.com&gt;
	&lt;05d701d213d1$7fb70880$7f251980$@alibaba-inc.com&gt;
	&lt;20160921143534.0dd95fe7@thinkpad&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20160921143534.0dd95fe7@thinkpad&gt;
User-Agent: Mutt/1.6.0 (2016-04-01)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Sept. 22, 2016, 9:51 a.m.</div>
<pre class="content">
On Wed 21-09-16 14:35:34, Gerald Schaefer wrote:
<span class="quote">&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt; hstate from page_hstate().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt; use the compound head page in dissolve_free_huge_page().</span>

OK so dissolve_free_huge_page will work also on tail pages now which
makes some sense. I would appreciate also few words why do we want to
sacrifice something as precious as gigantic page rather than fail the
page block offline. Dave pointed out dim offline usecase for example.
<span class="quote">
&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt; the lock when actually removing a hugepage.</span>

From a quick look it seems this has been broken since introduced by
c8721bbbdd36 (&quot;mm: memory-hotplug: enable memory hotplug to handle
hugepage&quot;). Do we want to have this backported to stable? In any way
Fixes: SHA1 would be really nice.
<span class="quote">
&gt; Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>

Other than that looks good to me, although there is a room for
improvements here. See below
<span class="quote">
&gt; ---</span>
<span class="quote">&gt; Changes in v2:</span>
<span class="quote">&gt; - Update comment in dissolve_free_huge_pages()</span>
<span class="quote">&gt; - Change locking in dissolve_free_huge_page()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  mm/hugetlb.c | 31 +++++++++++++++++++------------</span>
<span class="quote">&gt;  1 file changed, 19 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 87e11d8..1522af8 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1441,23 +1441,30 @@ static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt; +	struct hstate *h;</span>
<span class="quote">&gt; +	int nid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (page_count(head))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	h = page_hstate(head);</span>
<span class="quote">&gt; +	nid = page_to_nid(head);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Dissolve free hugepages in a given pfn range. Used by memory hotplug to</span>
<span class="quote">&gt;   * make specified memory blocks removable from the system.</span>
<span class="quote">&gt; - * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="quote">&gt; + * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="quote">&gt; + * part of it lies within the given range.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1466,9 +1473,9 @@ void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
<span class="quote">&gt;  	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="quote">&gt; -		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; +		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="quote">&gt; +			dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt;  }</span>

we can return the number of freed pages from dissolve_free_huge_page and
move by the approapriate number of pfns. Nothing to really lose sleep
about but no rocket science either. An early break out if the page is
used would be nice as well. Something like the following, probably a
separate patch on top of yours.
---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 22, 2016, 1:45 p.m.</div>
<pre class="content">
On Thu, 22 Sep 2016 11:51:37 +0200
Michal Hocko &lt;mhocko@kernel.org&gt; wrote:
<span class="quote">
&gt; On Wed 21-09-16 14:35:34, Gerald Schaefer wrote:</span>
<span class="quote">&gt; &gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt; &gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt; &gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt; &gt; with a size &gt; memory block size.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt; &gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt; &gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt; &gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt; &gt; hstate from page_hstate().</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt; &gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK so dissolve_free_huge_page will work also on tail pages now which</span>
<span class="quote">&gt; makes some sense. I would appreciate also few words why do we want to</span>
<span class="quote">&gt; sacrifice something as precious as gigantic page rather than fail the</span>
<span class="quote">&gt; page block offline. Dave pointed out dim offline usecase for example.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt; &gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From a quick look it seems this has been broken since introduced by</span>
<span class="quote">&gt; c8721bbbdd36 (&quot;mm: memory-hotplug: enable memory hotplug to handle</span>
<span class="quote">&gt; hugepage&quot;). Do we want to have this backported to stable? In any way</span>
<span class="quote">&gt; Fixes: SHA1 would be really nice.</span>

That&#39;s true, I&#39;ll send a v3.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Gerald Schaefer &lt;gerald.schaefer@de.ibm.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Other than that looks good to me, although there is a room for</span>
<span class="quote">&gt; improvements here. See below</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt; Changes in v2:</span>
<span class="quote">&gt; &gt; - Update comment in dissolve_free_huge_pages()</span>
<span class="quote">&gt; &gt; - Change locking in dissolve_free_huge_page()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  mm/hugetlb.c | 31 +++++++++++++++++++------------</span>
<span class="quote">&gt; &gt;  1 file changed, 19 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; index 87e11d8..1522af8 100644</span>
<span class="quote">&gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; @@ -1441,23 +1441,30 @@ static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	struct page *head = compound_head(page);</span>
<span class="quote">&gt; &gt; +	struct hstate *h;</span>
<span class="quote">&gt; &gt; +	int nid;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (page_count(head))</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	h = page_hstate(head);</span>
<span class="quote">&gt; &gt; +	nid = page_to_nid(head);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; &gt; -	if (PageHuge(page) &amp;&amp; !page_count(page)) {</span>
<span class="quote">&gt; &gt; -		struct hstate *h = page_hstate(page);</span>
<span class="quote">&gt; &gt; -		int nid = page_to_nid(page);</span>
<span class="quote">&gt; &gt; -		list_del(&amp;page-&gt;lru);</span>
<span class="quote">&gt; &gt; -		h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; &gt; -		h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; &gt; -		h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; &gt; -		update_and_free_page(h, page);</span>
<span class="quote">&gt; &gt; -	}</span>
<span class="quote">&gt; &gt; +	list_del(&amp;head-&gt;lru);</span>
<span class="quote">&gt; &gt; +	h-&gt;free_huge_pages--;</span>
<span class="quote">&gt; &gt; +	h-&gt;free_huge_pages_node[nid]--;</span>
<span class="quote">&gt; &gt; +	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt; &gt; +	update_and_free_page(h, head);</span>
<span class="quote">&gt; &gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt;   * Dissolve free hugepages in a given pfn range. Used by memory hotplug to</span>
<span class="quote">&gt; &gt;   * make specified memory blocks removable from the system.</span>
<span class="quote">&gt; &gt; - * Note that start_pfn should aligned with (minimum) hugepage size.</span>
<span class="quote">&gt; &gt; + * Note that this will dissolve a free gigantic hugepage completely, if any</span>
<span class="quote">&gt; &gt; + * part of it lies within the given range.</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; @@ -1466,9 +1473,9 @@ void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt; &gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt; &gt;  		return;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	VM_BUG_ON(!IS_ALIGNED(start_pfn, 1 &lt;&lt; minimum_order));</span>
<span class="quote">&gt; &gt;  	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="quote">&gt; &gt; -		dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; &gt; +		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="quote">&gt; &gt; +			dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; we can return the number of freed pages from dissolve_free_huge_page and</span>
<span class="quote">&gt; move by the approapriate number of pfns. Nothing to really lose sleep</span>
<span class="quote">&gt; about but no rocket science either. An early break out if the page is</span>
<span class="quote">&gt; used would be nice as well. Something like the following, probably a</span>
<span class="quote">&gt; separate patch on top of yours.</span>

Hmm, not sure if this is really worth the effort and the (small) added
complexity. It would surely be worth it for the current code, where we
also have the spinlock involved even for non-huge pages. After this patch
however, dissolve_free_huge_page() will only be called for hugepages,
and the early break-out is also there, although the page_count() check
could probably be moved out from dissolve_free_huge_page() and into the
loop, I&#39;ll try this for v3.

The loop count will also not be greatly reduced, at least when there
are only hugepages of minimum_order in the memory block, or no hugepages
at all, it will not improve anything. In any other case the PageHuge()
check in the loop will already prevent unnecessary calls to
dissolve_free_huge_page().
<span class="quote">
&gt; ---</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 029a80b90cea..d230900f571e 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1434,17 +1434,17 @@ static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Dissolve a given free hugepage into free buddy pages. This function does</span>
<span class="quote">&gt; - * nothing for in-use (including surplus) hugepages.</span>
<span class="quote">&gt; + * Dissolve a given free hugepage into free buddy pages. Returns number</span>
<span class="quote">&gt; + * of freed pages or EBUSY if the page is in use.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt; +static int dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *head = compound_head(page);</span>
<span class="quote">&gt;  	struct hstate *h;</span>
<span class="quote">&gt;  	int nid;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (page_count(head))</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; +		return -EBUSY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	h = page_hstate(head);</span>
<span class="quote">&gt;  	nid = page_to_nid(head);</span>
<span class="quote">&gt; @@ -1456,6 +1456,8 @@ static void dissolve_free_huge_page(struct page *page)</span>
<span class="quote">&gt;  	h-&gt;max_huge_pages--;</span>
<span class="quote">&gt;  	update_and_free_page(h, head);</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 1 &lt;&lt; h-&gt;order;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1471,9 +1473,18 @@ void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
<span class="quote">&gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="quote">&gt; -		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="quote">&gt; -			dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; +	for (pfn = start_pfn; pfn &lt; end_pfn; )</span>
<span class="quote">&gt; +		int nr_pages;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!PageHuge(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +			pfn += 1 &lt;&lt; minimum_order;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		nr_pages = dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="quote">&gt; +		if (IS_ERR(nr_pages))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		pfn += nr_pages;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=158981">Rui Teng</a> - Sept. 23, 2016, 6:40 a.m.</div>
<pre class="content">
On 9/22/16 5:51 PM, Michal Hocko wrote:
<span class="quote">&gt; On Wed 21-09-16 14:35:34, Gerald Schaefer wrote:</span>
<span class="quote">&gt;&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt;&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt;&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt;&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt;&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt;&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt;&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt;&gt; hstate from page_hstate().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt;&gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; OK so dissolve_free_huge_page will work also on tail pages now which</span>
<span class="quote">&gt; makes some sense. I would appreciate also few words why do we want to</span>
<span class="quote">&gt; sacrifice something as precious as gigantic page rather than fail the</span>
<span class="quote">&gt; page block offline. Dave pointed out dim offline usecase for example.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt;&gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; From a quick look it seems this has been broken since introduced by</span>
<span class="quote">&gt; c8721bbbdd36 (&quot;mm: memory-hotplug: enable memory hotplug to handle</span>
<span class="quote">&gt; hugepage&quot;). Do we want to have this backported to stable? In any way</span>
<span class="quote">&gt; Fixes: SHA1 would be really nice.</span>
<span class="quote">&gt;</span>

If the huge page hot-plug function was introduced by c8721bbbdd36, and
it has already indicated that the gigantic page is not supported:

	&quot;As for larger hugepages (1GB for x86_64), it&#39;s not easy to do
	hotremove over them because it&#39;s larger than memory block.  So
	we now simply leave it to fail as it is.&quot;

Is it possible that the gigantic page hot-plugin has never been
supported?

I made another patch for this problem, and also tried to apply the
first version of this patch on my system too. But they only postpone
the error happened. The HugePages_Free will be changed from 2 to 1, if I 
offline a huge page. I think it does not have a correct roll back.

# cat /proc/meminfo | grep -i huge
AnonHugePages:         0 kB
HugePages_Total:       2
HugePages_Free:        1
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:   16777216 kB

I will make more test on it, but can any one confirm that this function 
has been implemented and tested before?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2816">Gerald Schaefer</a> - Sept. 23, 2016, 11:03 a.m.</div>
<pre class="content">
On Fri, 23 Sep 2016 14:40:33 +0800
Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt; wrote:
<span class="quote">
&gt; On 9/22/16 5:51 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 21-09-16 14:35:34, Gerald Schaefer wrote:</span>
<span class="quote">&gt; &gt;&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt; &gt;&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt; &gt;&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt; &gt;&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt; &gt;&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt; &gt;&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt; &gt;&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt; &gt;&gt; hstate from page_hstate().</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt; &gt;&gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; OK so dissolve_free_huge_page will work also on tail pages now which</span>
<span class="quote">&gt; &gt; makes some sense. I would appreciate also few words why do we want to</span>
<span class="quote">&gt; &gt; sacrifice something as precious as gigantic page rather than fail the</span>
<span class="quote">&gt; &gt; page block offline. Dave pointed out dim offline usecase for example.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt; &gt;&gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; From a quick look it seems this has been broken since introduced by</span>
<span class="quote">&gt; &gt; c8721bbbdd36 (&quot;mm: memory-hotplug: enable memory hotplug to handle</span>
<span class="quote">&gt; &gt; hugepage&quot;). Do we want to have this backported to stable? In any way</span>
<span class="quote">&gt; &gt; Fixes: SHA1 would be really nice.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the huge page hot-plug function was introduced by c8721bbbdd36, and</span>
<span class="quote">&gt; it has already indicated that the gigantic page is not supported:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	&quot;As for larger hugepages (1GB for x86_64), it&#39;s not easy to do</span>
<span class="quote">&gt; 	hotremove over them because it&#39;s larger than memory block.  So</span>
<span class="quote">&gt; 	we now simply leave it to fail as it is.&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it possible that the gigantic page hot-plugin has never been</span>
<span class="quote">&gt; supported?</span>

Offlining blocks with gigantic pages only fails when they are in-use,
I guess that was meant by the description. Maybe it was also meant to
fail in any case, but that was not was the patch did.

With free gigantic pages, it looks like it only ever worked when
offlining the first block of a gigantic page. And as long as you only
have gigantic pages, the VM_BUG_ON() would actually have triggered on
every block that is not gigantic-page-aligned, even if the block is not
part of any gigantic page at all.

Given the age of the patch it is a little bit surprising that it never
struck anyone, and that we now have found it on two architectures at
once :-)
<span class="quote">
&gt; </span>
<span class="quote">&gt; I made another patch for this problem, and also tried to apply the</span>
<span class="quote">&gt; first version of this patch on my system too. But they only postpone</span>
<span class="quote">&gt; the error happened. The HugePages_Free will be changed from 2 to 1, if I </span>
<span class="quote">&gt; offline a huge page. I think it does not have a correct roll back.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; # cat /proc/meminfo | grep -i huge</span>
<span class="quote">&gt; AnonHugePages:         0 kB</span>
<span class="quote">&gt; HugePages_Total:       2</span>
<span class="quote">&gt; HugePages_Free:        1</span>
<span class="quote">&gt; HugePages_Rsvd:        0</span>
<span class="quote">&gt; HugePages_Surp:        0</span>
<span class="quote">&gt; Hugepagesize:   16777216 kB</span>

HugePages_Free is supposed to be reduced when offlining a block, but
then HugePages_Total should also be reduced, so that is strange. On my
system both were reduced. Does this happen with any version of my patch?

What do you mean with postpone the error? Can you reproduce the BUG_ON
or the addressing exception with my patch?
<span class="quote">
&gt; </span>
<span class="quote">&gt; I will make more test on it, but can any one confirm that this function </span>
<span class="quote">&gt; has been implemented and tested before?</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=158981">Rui Teng</a> - Sept. 26, 2016, 2:49 a.m.</div>
<pre class="content">
On 9/23/16 7:03 PM, Gerald Schaefer wrote:
<span class="quote">&gt; On Fri, 23 Sep 2016 14:40:33 +0800</span>
<span class="quote">&gt; Rui Teng &lt;rui.teng@linux.vnet.ibm.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On 9/22/16 5:51 PM, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt; On Wed 21-09-16 14:35:34, Gerald Schaefer wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; dissolve_free_huge_pages() will either run into the VM_BUG_ON() or a</span>
<span class="quote">&gt;&gt;&gt;&gt; list corruption and addressing exception when trying to set a memory</span>
<span class="quote">&gt;&gt;&gt;&gt; block offline that is part (but not the first part) of a hugetlb page</span>
<span class="quote">&gt;&gt;&gt;&gt; with a size &gt; memory block size.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; When no other smaller hugetlb page sizes are present, the VM_BUG_ON()</span>
<span class="quote">&gt;&gt;&gt;&gt; will trigger directly. In the other case we will run into an addressing</span>
<span class="quote">&gt;&gt;&gt;&gt; exception later, because dissolve_free_huge_page() will not work on the</span>
<span class="quote">&gt;&gt;&gt;&gt; head page of the compound hugetlb page which will result in a NULL</span>
<span class="quote">&gt;&gt;&gt;&gt; hstate from page_hstate().</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; To fix this, first remove the VM_BUG_ON() because it is wrong, and then</span>
<span class="quote">&gt;&gt;&gt;&gt; use the compound head page in dissolve_free_huge_page().</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; OK so dissolve_free_huge_page will work also on tail pages now which</span>
<span class="quote">&gt;&gt;&gt; makes some sense. I would appreciate also few words why do we want to</span>
<span class="quote">&gt;&gt;&gt; sacrifice something as precious as gigantic page rather than fail the</span>
<span class="quote">&gt;&gt;&gt; page block offline. Dave pointed out dim offline usecase for example.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Also change locking in dissolve_free_huge_page(), so that it only takes</span>
<span class="quote">&gt;&gt;&gt;&gt; the lock when actually removing a hugepage.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; From a quick look it seems this has been broken since introduced by</span>
<span class="quote">&gt;&gt;&gt; c8721bbbdd36 (&quot;mm: memory-hotplug: enable memory hotplug to handle</span>
<span class="quote">&gt;&gt;&gt; hugepage&quot;). Do we want to have this backported to stable? In any way</span>
<span class="quote">&gt;&gt;&gt; Fixes: SHA1 would be really nice.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If the huge page hot-plug function was introduced by c8721bbbdd36, and</span>
<span class="quote">&gt;&gt; it has already indicated that the gigantic page is not supported:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 	&quot;As for larger hugepages (1GB for x86_64), it&#39;s not easy to do</span>
<span class="quote">&gt;&gt; 	hotremove over them because it&#39;s larger than memory block.  So</span>
<span class="quote">&gt;&gt; 	we now simply leave it to fail as it is.&quot;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is it possible that the gigantic page hot-plugin has never been</span>
<span class="quote">&gt;&gt; supported?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Offlining blocks with gigantic pages only fails when they are in-use,</span>
<span class="quote">&gt; I guess that was meant by the description. Maybe it was also meant to</span>
<span class="quote">&gt; fail in any case, but that was not was the patch did.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; With free gigantic pages, it looks like it only ever worked when</span>
<span class="quote">&gt; offlining the first block of a gigantic page. And as long as you only</span>
<span class="quote">&gt; have gigantic pages, the VM_BUG_ON() would actually have triggered on</span>
<span class="quote">&gt; every block that is not gigantic-page-aligned, even if the block is not</span>
<span class="quote">&gt; part of any gigantic page at all.</span>

I have not met the VM_BUG_ON() issue on my powerpc architecture. Seems
it does not always have the align issue on other architectures.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Given the age of the patch it is a little bit surprising that it never</span>
<span class="quote">&gt; struck anyone, and that we now have found it on two architectures at</span>
<span class="quote">&gt; once :-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I made another patch for this problem, and also tried to apply the</span>
<span class="quote">&gt;&gt; first version of this patch on my system too. But they only postpone</span>
<span class="quote">&gt;&gt; the error happened. The HugePages_Free will be changed from 2 to 1, if I</span>
<span class="quote">&gt;&gt; offline a huge page. I think it does not have a correct roll back.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; # cat /proc/meminfo | grep -i huge</span>
<span class="quote">&gt;&gt; AnonHugePages:         0 kB</span>
<span class="quote">&gt;&gt; HugePages_Total:       2</span>
<span class="quote">&gt;&gt; HugePages_Free:        1</span>
<span class="quote">&gt;&gt; HugePages_Rsvd:        0</span>
<span class="quote">&gt;&gt; HugePages_Surp:        0</span>
<span class="quote">&gt;&gt; Hugepagesize:   16777216 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; HugePages_Free is supposed to be reduced when offlining a block, but</span>
<span class="quote">&gt; then HugePages_Total should also be reduced, so that is strange. On my</span>
<span class="quote">&gt; system both were reduced. Does this happen with any version of my patch?</span>

No, I only tested your first version. I do not have any question on
your patch, because the error was not introduced by your patch.
<span class="quote">
&gt;</span>
<span class="quote">&gt; What do you mean with postpone the error? Can you reproduce the BUG_ON</span>
<span class="quote">&gt; or the addressing exception with my patch?</span>

I mean the gigantic offlining function does not work at all on my
environment, even if the correct head page has been found. My method is
to filter all the tail pages out, and your method is to find head page
from tail pages.

Since you can offline gigantic page successful, I think such function
is supported now. I will debug the problem on my environment.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I will make more test on it, but can any one confirm that this function</span>
<span class="quote">&gt;&gt; has been implemented and tested before?</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 029a80b90cea..d230900f571e 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1434,17 +1434,17 @@</span> <span class="p_context"> static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
 }
 
 /*
<span class="p_del">- * Dissolve a given free hugepage into free buddy pages. This function does</span>
<span class="p_del">- * nothing for in-use (including surplus) hugepages.</span>
<span class="p_add">+ * Dissolve a given free hugepage into free buddy pages. Returns number</span>
<span class="p_add">+ * of freed pages or EBUSY if the page is in use.</span>
  */
<span class="p_del">-static void dissolve_free_huge_page(struct page *page)</span>
<span class="p_add">+static int dissolve_free_huge_page(struct page *page)</span>
 {
 	struct page *head = compound_head(page);
 	struct hstate *h;
 	int nid;
 
 	if (page_count(head))
<span class="p_del">-		return;</span>
<span class="p_add">+		return -EBUSY;</span>
 
 	h = page_hstate(head);
 	nid = page_to_nid(head);
<span class="p_chunk">@@ -1456,6 +1456,8 @@</span> <span class="p_context"> static void dissolve_free_huge_page(struct page *page)</span>
 	h-&gt;max_huge_pages--;
 	update_and_free_page(h, head);
 	spin_unlock(&amp;hugetlb_lock);
<span class="p_add">+</span>
<span class="p_add">+	return 1 &lt;&lt; h-&gt;order;</span>
 }
 
 /*
<span class="p_chunk">@@ -1471,9 +1473,18 @@</span> <span class="p_context"> void dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 	if (!hugepages_supported())
 		return;
 
<span class="p_del">-	for (pfn = start_pfn; pfn &lt; end_pfn; pfn += 1 &lt;&lt; minimum_order)</span>
<span class="p_del">-		if (PageHuge(pfn_to_page(pfn)))</span>
<span class="p_del">-			dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="p_add">+	for (pfn = start_pfn; pfn &lt; end_pfn; )</span>
<span class="p_add">+		int nr_pages;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!PageHuge(pfn_to_page(pfn))) {</span>
<span class="p_add">+			pfn += 1 &lt;&lt; minimum_order;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		nr_pages = dissolve_free_huge_page(pfn_to_page(pfn));</span>
<span class="p_add">+		if (IS_ERR(nr_pages))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		pfn += nr_pages;</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



