
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[6/9] lib/interval_tree: Fast overlap detection - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [6/9] lib/interval_tree: Fast overlap detection</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=106071">Davidlohr Bueso</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 29, 2017, 5:15 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170629171553.2146-7-dave@stgolabs.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9817465/mbox/"
   >mbox</a>
|
   <a href="/patch/9817465/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9817465/">/patch/9817465/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	26B2160365 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 17:17:26 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F21E6203C0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 17:17:23 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E596A26E97; Thu, 29 Jun 2017 17:17:23 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B312C262AE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Jun 2017 17:17:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753465AbdF2RRR (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 29 Jun 2017 13:17:17 -0400
Received: from smtp2.provo.novell.com ([137.65.250.81]:57107 &quot;EHLO
	smtp2.provo.novell.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753422AbdF2RQv (ORCPT
	&lt;rfc822;groupwise-linux-kernel@vger.kernel.org:0:0&gt;);
	Thu, 29 Jun 2017 13:16:51 -0400
Received: from linux-80c1.suse (prv-ext-foundry1int.gns.novell.com
	[137.65.251.240])
	by smtp2.provo.novell.com with ESMTP (TLS encrypted);
	Thu, 29 Jun 2017 11:16:43 -0600
From: Davidlohr Bueso &lt;dave@stgolabs.net&gt;
To: mingo@kernel.org, peterz@infradead.org, akpm@linux-foundation.org
Cc: torvalds@linux-foundation.org, jack@suse.cz,
	kirill.shutemov@linux.intel.com, ldufour@linux.vnet.ibm.com,
	mhocko@suse.com, mgorman@techsingularity.net, dave@stgolabs.net,
	linux-kernel@vger.kernel.org, David Airlie &lt;airlied@linux.ie&gt;,
	dri-devel@lists.freedesktop.org, &quot;Michael S. Tsirkin&quot; &lt;mst@redhat.com&gt;,
	Jason Wang &lt;jasowang@redhat.com&gt;, Doug Ledford &lt;dledford@redhat.com&gt;,
	Christian Benvenuti &lt;benve@cisco.com&gt;,
	linux-rdma@vger.kernel.org, Davidlohr Bueso &lt;dbueso@suse.de&gt;
Subject: [PATCH 6/9] lib/interval_tree: Fast overlap detection
Date: Thu, 29 Jun 2017 10:15:50 -0700
Message-Id: &lt;20170629171553.2146-7-dave@stgolabs.net&gt;
X-Mailer: git-send-email 2.12.0
In-Reply-To: &lt;20170629171553.2146-1-dave@stgolabs.net&gt;
References: &lt;20170629171553.2146-1-dave@stgolabs.net&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106071">Davidlohr Bueso</a> - June 29, 2017, 5:15 p.m.</div>
<pre class="content">
Allow interval trees to quickly check for overlaps to avoid
unnecesary tree lookups in interval_tree_iter_first().

As of this patch, all interval tree flavors will require
using a &#39;rb_root_cached&#39; such that we can have the leftmost
node easily available. While most users will make use of this
feature, those with special functions (in addition to the generic
insert, delete, search calls) will avoid using the cached
option as they can do funky things with insertions -- for example,
vma_interval_tree_insert_after().

Cc: David Airlie &lt;airlied@linux.ie&gt;
Cc: dri-devel@lists.freedesktop.org
Cc: &quot;Michael S. Tsirkin&quot; &lt;mst@redhat.com&gt;
Cc: Jason Wang &lt;jasowang@redhat.com&gt;
Cc: Doug Ledford &lt;dledford@redhat.com&gt;
Cc: Christian Benvenuti &lt;benve@cisco.com&gt;
Cc: linux-rdma@vger.kernel.org
<span class="acked-by">Acked-by: Christian KÃ¶nig &lt;christian.koenig@amd.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Davidlohr Bueso &lt;dbueso@suse.de&gt;</span>
---
This is part of the rbtree internal caching series:
https://lkml.org/lkml/2017/6/8/857

 drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c             |  8 ++--
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c             |  7 ++--
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h             |  2 +-
 drivers/gpu/drm/drm_mm.c                           | 19 +++++----
 drivers/gpu/drm/drm_vma_manager.c                  |  2 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c            |  6 +--
 drivers/gpu/drm/radeon/radeon.h                    |  2 +-
 drivers/gpu/drm/radeon/radeon_mn.c                 |  8 ++--
 drivers/gpu/drm/radeon/radeon_vm.c                 |  7 ++--
 drivers/infiniband/core/umem_rbtree.c              |  4 +-
 drivers/infiniband/core/uverbs_cmd.c               |  2 +-
 drivers/infiniband/hw/hfi1/mmu_rb.c                | 10 ++---
 drivers/infiniband/hw/usnic/usnic_uiom.c           |  6 +--
 drivers/infiniband/hw/usnic/usnic_uiom.h           |  2 +-
 .../infiniband/hw/usnic/usnic_uiom_interval_tree.c | 15 +++----
 .../infiniband/hw/usnic/usnic_uiom_interval_tree.h | 12 +++---
 drivers/vhost/vhost.c                              |  2 +-
 drivers/vhost/vhost.h                              |  2 +-
 fs/hugetlbfs/inode.c                               |  6 +--
 fs/inode.c                                         |  2 +-
 include/drm/drm_mm.h                               |  2 +-
 include/linux/fs.h                                 |  4 +-
 include/linux/interval_tree.h                      |  8 ++--
 include/linux/interval_tree_generic.h              | 46 +++++++++++++++++-----
 include/linux/mm.h                                 | 17 ++++----
 include/linux/rmap.h                               |  4 +-
 include/rdma/ib_umem_odp.h                         | 11 ++++--
 include/rdma/ib_verbs.h                            |  2 +-
 lib/interval_tree_test.c                           |  4 +-
 mm/interval_tree.c                                 | 10 ++---
 mm/memory.c                                        |  4 +-
 mm/mmap.c                                          | 10 ++---
 mm/rmap.c                                          |  4 +-
 33 files changed, 145 insertions(+), 105 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">index 38f739fb727b..3f8aef21b9a6 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_mn.c</span>
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> struct amdgpu_mn {</span>
 
 	/* objects protected by lock */
 	struct mutex		lock;
<span class="p_del">-	struct rb_root		objects;</span>
<span class="p_add">+	struct rb_root_cached	objects;</span>
 };
 
 struct amdgpu_mn_node {
<span class="p_chunk">@@ -76,8 +76,8 @@</span> <span class="p_context"> static void amdgpu_mn_destroy(struct work_struct *work)</span>
 	mutex_lock(&amp;adev-&gt;mn_lock);
 	mutex_lock(&amp;rmn-&gt;lock);
 	hash_del(&amp;rmn-&gt;node);
<span class="p_del">-	rbtree_postorder_for_each_entry_safe(node, next_node, &amp;rmn-&gt;objects,</span>
<span class="p_del">-					     it.rb) {</span>
<span class="p_add">+	rbtree_postorder_for_each_entry_safe(node, next_node,</span>
<span class="p_add">+					     &amp;rmn-&gt;objects.rb_root, it.rb) {</span>
 		list_for_each_entry_safe(bo, next_bo, &amp;node-&gt;bos, mn_list) {
 			bo-&gt;mn = NULL;
 			list_del_init(&amp;bo-&gt;mn_list);
<span class="p_chunk">@@ -252,7 +252,7 @@</span> <span class="p_context"> static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)</span>
 	rmn-&gt;mm = mm;
 	rmn-&gt;mn.ops = &amp;amdgpu_mn_ops;
 	mutex_init(&amp;rmn-&gt;lock);
<span class="p_del">-	rmn-&gt;objects = RB_ROOT;</span>
<span class="p_add">+	rmn-&gt;objects = RB_ROOT_CACHED;</span>
 
 	r = __mmu_notifier_register(&amp;rmn-&gt;mn, mm);
 	if (r)
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c</span>
<span class="p_header">index 5795f81369f0..f872e2179bbd 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c</span>
<span class="p_chunk">@@ -2405,7 +2405,7 @@</span> <span class="p_context"> int amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,</span>
 	int r, i;
 	u64 flags;
 
<span class="p_del">-	vm-&gt;va = RB_ROOT;</span>
<span class="p_add">+	vm-&gt;va = RB_ROOT_CACHED;</span>
 	vm-&gt;client_id = atomic64_inc_return(&amp;adev-&gt;vm_manager.client_counter);
 	for (i = 0; i &lt; AMDGPU_MAX_VMHUBS; i++)
 		vm-&gt;reserved_vmid[i] = NULL;
<span class="p_chunk">@@ -2512,10 +2512,11 @@</span> <span class="p_context"> void amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)</span>
 
 	amd_sched_entity_fini(vm-&gt;entity.sched, &amp;vm-&gt;entity);
 
<span class="p_del">-	if (!RB_EMPTY_ROOT(&amp;vm-&gt;va)) {</span>
<span class="p_add">+	if (!RB_EMPTY_ROOT(&amp;vm-&gt;va.rb_root)) {</span>
 		dev_err(adev-&gt;dev, &quot;still active bo inside vm\n&quot;);
 	}
<span class="p_del">-	rbtree_postorder_for_each_entry_safe(mapping, tmp, &amp;vm-&gt;va, rb) {</span>
<span class="p_add">+	rbtree_postorder_for_each_entry_safe(mapping, tmp,</span>
<span class="p_add">+					     &amp;vm-&gt;va.rb_root, rb) {</span>
 		list_del(&amp;mapping-&gt;list);
 		amdgpu_vm_it_remove(mapping, &amp;vm-&gt;va);
 		kfree(mapping);
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h</span>
<span class="p_header">index 936f158bc5ec..ebffc1253f85 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h</span>
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> struct amdgpu_vm_pt {</span>
 
 struct amdgpu_vm {
 	/* tree of virtual addresses mapped */
<span class="p_del">-	struct rb_root		va;</span>
<span class="p_add">+	struct rb_root_cached	va;</span>
 
 	/* protecting invalidated */
 	spinlock_t		status_lock;
<span class="p_header">diff --git a/drivers/gpu/drm/drm_mm.c b/drivers/gpu/drm/drm_mm.c</span>
<span class="p_header">index f794089d30ac..61a1c8ea74bc 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/drm_mm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/drm_mm.c</span>
<span class="p_chunk">@@ -169,7 +169,7 @@</span> <span class="p_context"> INTERVAL_TREE_DEFINE(struct drm_mm_node, rb,</span>
 struct drm_mm_node *
 __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last)
 {
<span class="p_del">-	return drm_mm_interval_tree_iter_first((struct rb_root *)&amp;mm-&gt;interval_tree,</span>
<span class="p_add">+	return drm_mm_interval_tree_iter_first((struct rb_root_cached *)&amp;mm-&gt;interval_tree,</span>
 					       start, last) ?: (struct drm_mm_node *)&amp;mm-&gt;head_node;
 }
 EXPORT_SYMBOL(__drm_mm_interval_first);
<span class="p_chunk">@@ -180,6 +180,7 @@</span> <span class="p_context"> static void drm_mm_interval_tree_add_node(struct drm_mm_node *hole_node,</span>
 	struct drm_mm *mm = hole_node-&gt;mm;
 	struct rb_node **link, *rb;
 	struct drm_mm_node *parent;
<span class="p_add">+	bool leftmost = true;</span>
 
 	node-&gt;__subtree_last = LAST(node);
 
<span class="p_chunk">@@ -196,9 +197,10 @@</span> <span class="p_context"> static void drm_mm_interval_tree_add_node(struct drm_mm_node *hole_node,</span>
 
 		rb = &amp;hole_node-&gt;rb;
 		link = &amp;hole_node-&gt;rb.rb_right;
<span class="p_add">+		leftmost = false;</span>
 	} else {
 		rb = NULL;
<span class="p_del">-		link = &amp;mm-&gt;interval_tree.rb_node;</span>
<span class="p_add">+		link = &amp;mm-&gt;interval_tree.rb_root.rb_node;</span>
 	}
 
 	while (*link) {
<span class="p_chunk">@@ -208,14 +210,15 @@</span> <span class="p_context"> static void drm_mm_interval_tree_add_node(struct drm_mm_node *hole_node,</span>
 			parent-&gt;__subtree_last = node-&gt;__subtree_last;
 		if (node-&gt;start &lt; parent-&gt;start)
 			link = &amp;parent-&gt;rb.rb_left;
<span class="p_del">-		else</span>
<span class="p_add">+		else {</span>
 			link = &amp;parent-&gt;rb.rb_right;
<span class="p_add">+			leftmost = true;</span>
<span class="p_add">+		}</span>
 	}
 
 	rb_link_node(&amp;node-&gt;rb, rb, link);
<span class="p_del">-	rb_insert_augmented(&amp;node-&gt;rb,</span>
<span class="p_del">-			    &amp;mm-&gt;interval_tree,</span>
<span class="p_del">-			    &amp;drm_mm_interval_tree_augment);</span>
<span class="p_add">+	rb_insert_augmented_cached(&amp;node-&gt;rb, &amp;mm-&gt;interval_tree, leftmost,</span>
<span class="p_add">+				   &amp;drm_mm_interval_tree_augment);</span>
 }
 
 #define RB_INSERT(root, member, expr) do { \
<span class="p_chunk">@@ -577,7 +580,7 @@</span> <span class="p_context"> void drm_mm_replace_node(struct drm_mm_node *old, struct drm_mm_node *new)</span>
 	*new = *old;
 
 	list_replace(&amp;old-&gt;node_list, &amp;new-&gt;node_list);
<span class="p_del">-	rb_replace_node(&amp;old-&gt;rb, &amp;new-&gt;rb, &amp;old-&gt;mm-&gt;interval_tree);</span>
<span class="p_add">+	rb_replace_node(&amp;old-&gt;rb, &amp;new-&gt;rb, &amp;old-&gt;mm-&gt;interval_tree.rb_root);</span>
 
 	if (drm_mm_hole_follows(old)) {
 		list_replace(&amp;old-&gt;hole_stack, &amp;new-&gt;hole_stack);
<span class="p_chunk">@@ -863,7 +866,7 @@</span> <span class="p_context"> void drm_mm_init(struct drm_mm *mm, u64 start, u64 size)</span>
 	mm-&gt;color_adjust = NULL;
 
 	INIT_LIST_HEAD(&amp;mm-&gt;hole_stack);
<span class="p_del">-	mm-&gt;interval_tree = RB_ROOT;</span>
<span class="p_add">+	mm-&gt;interval_tree = RB_ROOT_CACHED;</span>
 	mm-&gt;holes_size = RB_ROOT;
 	mm-&gt;holes_addr = RB_ROOT;
 
<span class="p_header">diff --git a/drivers/gpu/drm/drm_vma_manager.c b/drivers/gpu/drm/drm_vma_manager.c</span>
<span class="p_header">index d9100b565198..28f1226576f8 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/drm_vma_manager.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/drm_vma_manager.c</span>
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> struct drm_vma_offset_node *drm_vma_offset_lookup_locked(struct drm_vma_offset_m</span>
 	struct rb_node *iter;
 	unsigned long offset;
 
<span class="p_del">-	iter = mgr-&gt;vm_addr_space_mm.interval_tree.rb_node;</span>
<span class="p_add">+	iter = mgr-&gt;vm_addr_space_mm.interval_tree.rb_root.rb_node;</span>
 	best = NULL;
 
 	while (likely(iter)) {
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index ccd09e8419f5..71dddf66baaa 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -49,7 +49,7 @@</span> <span class="p_context"> struct i915_mmu_notifier {</span>
 	spinlock_t lock;
 	struct hlist_node node;
 	struct mmu_notifier mn;
<span class="p_del">-	struct rb_root objects;</span>
<span class="p_add">+	struct rb_root_cached objects;</span>
 	struct workqueue_struct *wq;
 };
 
<span class="p_chunk">@@ -123,7 +123,7 @@</span> <span class="p_context"> static void i915_gem_userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,</span>
 	struct interval_tree_node *it;
 	LIST_HEAD(cancelled);
 
<span class="p_del">-	if (RB_EMPTY_ROOT(&amp;mn-&gt;objects))</span>
<span class="p_add">+	if (RB_EMPTY_ROOT(&amp;mn-&gt;objects.rb_root))</span>
 		return;
 
 	/* interval ranges are inclusive, but invalidate range is exclusive */
<span class="p_chunk">@@ -172,7 +172,7 @@</span> <span class="p_context"> i915_mmu_notifier_create(struct mm_struct *mm)</span>
 
 	spin_lock_init(&amp;mn-&gt;lock);
 	mn-&gt;mn.ops = &amp;i915_gem_userptr_notifier;
<span class="p_del">-	mn-&gt;objects = RB_ROOT;</span>
<span class="p_add">+	mn-&gt;objects = RB_ROOT_CACHED;</span>
 	mn-&gt;wq = alloc_workqueue(&quot;i915-userptr-release&quot;, WQ_UNBOUND, 0);
 	if (mn-&gt;wq == NULL) {
 		kfree(mn);
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon.h b/drivers/gpu/drm/radeon/radeon.h</span>
<span class="p_header">index 5008f3d4cccc..10d0dd146808 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon.h</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon.h</span>
<span class="p_chunk">@@ -924,7 +924,7 @@</span> <span class="p_context"> struct radeon_vm_id {</span>
 struct radeon_vm {
 	struct mutex		mutex;
 
<span class="p_del">-	struct rb_root		va;</span>
<span class="p_add">+	struct rb_root_cached	va;</span>
 
 	/* protecting invalidated and freed */
 	spinlock_t		status_lock;
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">index 896f2cf51e4e..1d62288b7ee3 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_chunk">@@ -50,7 +50,7 @@</span> <span class="p_context"> struct radeon_mn {</span>
 
 	/* objects protected by lock */
 	struct mutex		lock;
<span class="p_del">-	struct rb_root		objects;</span>
<span class="p_add">+	struct rb_root_cached	objects;</span>
 };
 
 struct radeon_mn_node {
<span class="p_chunk">@@ -75,8 +75,8 @@</span> <span class="p_context"> static void radeon_mn_destroy(struct work_struct *work)</span>
 	mutex_lock(&amp;rdev-&gt;mn_lock);
 	mutex_lock(&amp;rmn-&gt;lock);
 	hash_del(&amp;rmn-&gt;node);
<span class="p_del">-	rbtree_postorder_for_each_entry_safe(node, next_node, &amp;rmn-&gt;objects,</span>
<span class="p_del">-					     it.rb) {</span>
<span class="p_add">+	rbtree_postorder_for_each_entry_safe(node, next_node,</span>
<span class="p_add">+					     &amp;rmn-&gt;objects.rb_root, it.rb) {</span>
 
 		interval_tree_remove(&amp;node-&gt;it, &amp;rmn-&gt;objects);
 		list_for_each_entry_safe(bo, next_bo, &amp;node-&gt;bos, mn_list) {
<span class="p_chunk">@@ -205,7 +205,7 @@</span> <span class="p_context"> static struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)</span>
 	rmn-&gt;mm = mm;
 	rmn-&gt;mn.ops = &amp;radeon_mn_ops;
 	mutex_init(&amp;rmn-&gt;lock);
<span class="p_del">-	rmn-&gt;objects = RB_ROOT;</span>
<span class="p_add">+	rmn-&gt;objects = RB_ROOT_CACHED;</span>
 	
 	r = __mmu_notifier_register(&amp;rmn-&gt;mn, mm);
 	if (r)
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_vm.c b/drivers/gpu/drm/radeon/radeon_vm.c</span>
<span class="p_header">index 5f68245579a3..f44777a6c2e8 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_vm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_vm.c</span>
<span class="p_chunk">@@ -1185,7 +1185,7 @@</span> <span class="p_context"> int radeon_vm_init(struct radeon_device *rdev, struct radeon_vm *vm)</span>
 		vm-&gt;ids[i].last_id_use = NULL;
 	}
 	mutex_init(&amp;vm-&gt;mutex);
<span class="p_del">-	vm-&gt;va = RB_ROOT;</span>
<span class="p_add">+	vm-&gt;va = RB_ROOT_CACHED;</span>
 	spin_lock_init(&amp;vm-&gt;status_lock);
 	INIT_LIST_HEAD(&amp;vm-&gt;invalidated);
 	INIT_LIST_HEAD(&amp;vm-&gt;freed);
<span class="p_chunk">@@ -1232,10 +1232,11 @@</span> <span class="p_context"> void radeon_vm_fini(struct radeon_device *rdev, struct radeon_vm *vm)</span>
 	struct radeon_bo_va *bo_va, *tmp;
 	int i, r;
 
<span class="p_del">-	if (!RB_EMPTY_ROOT(&amp;vm-&gt;va)) {</span>
<span class="p_add">+	if (!RB_EMPTY_ROOT(&amp;vm-&gt;va.rb_root)) {</span>
 		dev_err(rdev-&gt;dev, &quot;still active bo inside vm\n&quot;);
 	}
<span class="p_del">-	rbtree_postorder_for_each_entry_safe(bo_va, tmp, &amp;vm-&gt;va, it.rb) {</span>
<span class="p_add">+	rbtree_postorder_for_each_entry_safe(bo_va, tmp,</span>
<span class="p_add">+					     &amp;vm-&gt;va.rb_root, it.rb) {</span>
 		interval_tree_remove(&amp;bo_va-&gt;it, &amp;vm-&gt;va);
 		r = radeon_bo_reserve(bo_va-&gt;bo, false);
 		if (!r) {
<span class="p_header">diff --git a/drivers/infiniband/core/umem_rbtree.c b/drivers/infiniband/core/umem_rbtree.c</span>
<span class="p_header">index d176597b4d78..fc801920e341 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_rbtree.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_rbtree.c</span>
<span class="p_chunk">@@ -72,7 +72,7 @@</span> <span class="p_context"> INTERVAL_TREE_DEFINE(struct umem_odp_node, rb, u64, __subtree_last,</span>
 /* @last is not a part of the interval. See comment for function
  * node_last.
  */
<span class="p_del">-int rbt_ib_umem_for_each_in_range(struct rb_root *root,</span>
<span class="p_add">+int rbt_ib_umem_for_each_in_range(struct rb_root_cached *root,</span>
 				  u64 start, u64 last,
 				  umem_call_back cb,
 				  void *cookie)
<span class="p_chunk">@@ -95,7 +95,7 @@</span> <span class="p_context"> int rbt_ib_umem_for_each_in_range(struct rb_root *root,</span>
 }
 EXPORT_SYMBOL(rbt_ib_umem_for_each_in_range);
 
<span class="p_del">-struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root *root,</span>
<span class="p_add">+struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root_cached *root,</span>
 				       u64 addr, u64 length)
 {
 	struct umem_odp_node *node;
<span class="p_header">diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_header">index 0ad3b05405d8..f73d4153dbd0 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> ssize_t ib_uverbs_get_context(struct ib_uverbs_file *file,</span>
 	ucontext-&gt;closing = 0;
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
<span class="p_del">-	ucontext-&gt;umem_tree = RB_ROOT;</span>
<span class="p_add">+	ucontext-&gt;umem_tree = RB_ROOT_CACHED;</span>
 	init_rwsem(&amp;ucontext-&gt;umem_rwsem);
 	ucontext-&gt;odp_mrs_count = 0;
 	INIT_LIST_HEAD(&amp;ucontext-&gt;no_private_counters);
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/mmu_rb.c b/drivers/infiniband/hw/hfi1/mmu_rb.c</span>
<span class="p_header">index ccbf52c8ff6f..1835447dcd73 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/mmu_rb.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/mmu_rb.c</span>
<span class="p_chunk">@@ -54,7 +54,7 @@</span> <span class="p_context"></span>
 
 struct mmu_rb_handler {
 	struct mmu_notifier mn;
<span class="p_del">-	struct rb_root root;</span>
<span class="p_add">+	struct rb_root_cached root;</span>
 	void *ops_arg;
 	spinlock_t lock;        /* protect the RB tree */
 	struct mmu_rb_ops *ops;
<span class="p_chunk">@@ -111,7 +111,7 @@</span> <span class="p_context"> int hfi1_mmu_rb_register(void *ops_arg, struct mm_struct *mm,</span>
 	if (!handlr)
 		return -ENOMEM;
 
<span class="p_del">-	handlr-&gt;root = RB_ROOT;</span>
<span class="p_add">+	handlr-&gt;root = RB_ROOT_CACHED;</span>
 	handlr-&gt;ops = ops;
 	handlr-&gt;ops_arg = ops_arg;
 	INIT_HLIST_NODE(&amp;handlr-&gt;mn.hlist);
<span class="p_chunk">@@ -152,9 +152,9 @@</span> <span class="p_context"> void hfi1_mmu_rb_unregister(struct mmu_rb_handler *handler)</span>
 	INIT_LIST_HEAD(&amp;del_list);
 
 	spin_lock_irqsave(&amp;handler-&gt;lock, flags);
<span class="p_del">-	while ((node = rb_first(&amp;handler-&gt;root))) {</span>
<span class="p_add">+	while ((node = rb_first_cached(&amp;handler-&gt;root))) {</span>
 		rbnode = rb_entry(node, struct mmu_rb_node, node);
<span class="p_del">-		rb_erase(node, &amp;handler-&gt;root);</span>
<span class="p_add">+		rb_erase_cached(node, &amp;handler-&gt;root);</span>
 		/* move from LRU list to delete list */
 		list_move(&amp;rbnode-&gt;list, &amp;del_list);
 	}
<span class="p_chunk">@@ -305,7 +305,7 @@</span> <span class="p_context"> static void mmu_notifier_mem_invalidate(struct mmu_notifier *mn,</span>
 {
 	struct mmu_rb_handler *handler =
 		container_of(mn, struct mmu_rb_handler, mn);
<span class="p_del">-	struct rb_root *root = &amp;handler-&gt;root;</span>
<span class="p_add">+	struct rb_root_cached *root = &amp;handler-&gt;root;</span>
 	struct mmu_rb_node *node, *ptr = NULL;
 	unsigned long flags;
 	bool added = false;
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">index c49db7c33979..4381c0a9a873 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_chunk">@@ -227,7 +227,7 @@</span> <span class="p_context"> static void __usnic_uiom_reg_release(struct usnic_uiom_pd *pd,</span>
 	vpn_last = vpn_start + npages - 1;
 
 	spin_lock(&amp;pd-&gt;lock);
<span class="p_del">-	usnic_uiom_remove_interval(&amp;pd-&gt;rb_root, vpn_start,</span>
<span class="p_add">+	usnic_uiom_remove_interval(&amp;pd-&gt;root, vpn_start,</span>
 					vpn_last, &amp;rm_intervals);
 	usnic_uiom_unmap_sorted_intervals(&amp;rm_intervals, pd);
 
<span class="p_chunk">@@ -379,7 +379,7 @@</span> <span class="p_context"> struct usnic_uiom_reg *usnic_uiom_reg_get(struct usnic_uiom_pd *pd,</span>
 	err = usnic_uiom_get_intervals_diff(vpn_start, vpn_last,
 						(writable) ? IOMMU_WRITE : 0,
 						IOMMU_WRITE,
<span class="p_del">-						&amp;pd-&gt;rb_root,</span>
<span class="p_add">+						&amp;pd-&gt;root,</span>
 						&amp;sorted_diff_intervals);
 	if (err) {
 		usnic_err(&quot;Failed disjoint interval vpn [0x%lx,0x%lx] err %d\n&quot;,
<span class="p_chunk">@@ -395,7 +395,7 @@</span> <span class="p_context"> struct usnic_uiom_reg *usnic_uiom_reg_get(struct usnic_uiom_pd *pd,</span>
 
 	}
 
<span class="p_del">-	err = usnic_uiom_insert_interval(&amp;pd-&gt;rb_root, vpn_start, vpn_last,</span>
<span class="p_add">+	err = usnic_uiom_insert_interval(&amp;pd-&gt;root, vpn_start, vpn_last,</span>
 					(writable) ? IOMMU_WRITE : 0);
 	if (err) {
 		usnic_err(&quot;Failed insert interval vpn [0x%lx,0x%lx] err %d\n&quot;,
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.h b/drivers/infiniband/hw/usnic/usnic_uiom.h</span>
<span class="p_header">index 45ca7c1613a7..431efe4143f4 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.h</span>
<span class="p_chunk">@@ -55,7 +55,7 @@</span> <span class="p_context"> struct usnic_uiom_dev {</span>
 struct usnic_uiom_pd {
 	struct iommu_domain		*domain;
 	spinlock_t			lock;
<span class="p_del">-	struct rb_root			rb_root;</span>
<span class="p_add">+	struct rb_root_cached		root;</span>
 	struct list_head		devs;
 	int				dev_cnt;
 };
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.c b/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.c</span>
<span class="p_header">index 42b4b4c4e452..d399523206c7 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.c</span>
<span class="p_chunk">@@ -100,9 +100,9 @@</span> <span class="p_context"> static int interval_cmp(void *priv, struct list_head *a, struct list_head *b)</span>
 }
 
 static void
<span class="p_del">-find_intervals_intersection_sorted(struct rb_root *root, unsigned long start,</span>
<span class="p_del">-					unsigned long last,</span>
<span class="p_del">-					struct list_head *list)</span>
<span class="p_add">+find_intervals_intersection_sorted(struct rb_root_cached *root,</span>
<span class="p_add">+				   unsigned long start, unsigned long last,</span>
<span class="p_add">+				   struct list_head *list)</span>
 {
 	struct usnic_uiom_interval_node *node;
 
<span class="p_chunk">@@ -118,7 +118,7 @@</span> <span class="p_context"> find_intervals_intersection_sorted(struct rb_root *root, unsigned long start,</span>
 
 int usnic_uiom_get_intervals_diff(unsigned long start, unsigned long last,
 					int flags, int flag_mask,
<span class="p_del">-					struct rb_root *root,</span>
<span class="p_add">+					struct rb_root_cached *root,</span>
 					struct list_head *diff_set)
 {
 	struct usnic_uiom_interval_node *interval, *tmp;
<span class="p_chunk">@@ -175,7 +175,7 @@</span> <span class="p_context"> void usnic_uiom_put_interval_set(struct list_head *intervals)</span>
 		kfree(interval);
 }
 
<span class="p_del">-int usnic_uiom_insert_interval(struct rb_root *root, unsigned long start,</span>
<span class="p_add">+int usnic_uiom_insert_interval(struct rb_root_cached *root, unsigned long start,</span>
 				unsigned long last, int flags)
 {
 	struct usnic_uiom_interval_node *interval, *tmp;
<span class="p_chunk">@@ -246,8 +246,9 @@</span> <span class="p_context"> int usnic_uiom_insert_interval(struct rb_root *root, unsigned long start,</span>
 	return err;
 }
 
<span class="p_del">-void usnic_uiom_remove_interval(struct rb_root *root, unsigned long start,</span>
<span class="p_del">-				unsigned long last, struct list_head *removed)</span>
<span class="p_add">+void usnic_uiom_remove_interval(struct rb_root_cached *root,</span>
<span class="p_add">+				unsigned long start, unsigned long last,</span>
<span class="p_add">+				struct list_head *removed)</span>
 {
 	struct usnic_uiom_interval_node *interval;
 
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.h b/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.h</span>
<span class="p_header">index c0b0b876ab90..1d7fc3226bca 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom_interval_tree.h</span>
<span class="p_chunk">@@ -48,12 +48,12 @@</span> <span class="p_context"> struct usnic_uiom_interval_node {</span>
 
 extern void
 usnic_uiom_interval_tree_insert(struct usnic_uiom_interval_node *node,
<span class="p_del">-					struct rb_root *root);</span>
<span class="p_add">+					struct rb_root_cached *root);</span>
 extern void
 usnic_uiom_interval_tree_remove(struct usnic_uiom_interval_node *node,
<span class="p_del">-					struct rb_root *root);</span>
<span class="p_add">+					struct rb_root_cached *root);</span>
 extern struct usnic_uiom_interval_node *
<span class="p_del">-usnic_uiom_interval_tree_iter_first(struct rb_root *root,</span>
<span class="p_add">+usnic_uiom_interval_tree_iter_first(struct rb_root_cached *root,</span>
 					unsigned long start,
 					unsigned long last);
 extern struct usnic_uiom_interval_node *
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> usnic_uiom_interval_tree_iter_next(struct usnic_uiom_interval_node *node,</span>
  * Inserts {start...last} into {root}.  If there are overlaps,
  * nodes will be broken up and merged
  */
<span class="p_del">-int usnic_uiom_insert_interval(struct rb_root *root,</span>
<span class="p_add">+int usnic_uiom_insert_interval(struct rb_root_cached *root,</span>
 				unsigned long start, unsigned long last,
 				int flags);
 /*
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> int usnic_uiom_insert_interval(struct rb_root *root,</span>
  * &#39;removed.&#39; The caller is responsibile for freeing memory of nodes in
  * &#39;removed.&#39;
  */
<span class="p_del">-void usnic_uiom_remove_interval(struct rb_root *root,</span>
<span class="p_add">+void usnic_uiom_remove_interval(struct rb_root_cached *root,</span>
 				unsigned long start, unsigned long last,
 				struct list_head *removed);
 /*
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> void usnic_uiom_remove_interval(struct rb_root *root,</span>
 int usnic_uiom_get_intervals_diff(unsigned long start,
 					unsigned long last, int flags,
 					int flag_mask,
<span class="p_del">-					struct rb_root *root,</span>
<span class="p_add">+					struct rb_root_cached *root,</span>
 					struct list_head *diff_set);
 /* Call this to free diff_set returned by usnic_uiom_get_intervals_diff */
 void usnic_uiom_put_interval_set(struct list_head *intervals);
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index e4613a3c362d..88dc214de068 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -1272,7 +1272,7 @@</span> <span class="p_context"> static struct vhost_umem *vhost_umem_alloc(void)</span>
 	if (!umem)
 		return NULL;
 
<span class="p_del">-	umem-&gt;umem_tree = RB_ROOT;</span>
<span class="p_add">+	umem-&gt;umem_tree = RB_ROOT_CACHED;</span>
 	umem-&gt;numem = 0;
 	INIT_LIST_HEAD(&amp;umem-&gt;umem_list);
 
<span class="p_header">diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h</span>
<span class="p_header">index f72095868b93..a0278ba6a8b4 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.h</span>
<span class="p_header">+++ b/drivers/vhost/vhost.h</span>
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> struct vhost_umem_node {</span>
 };
 
 struct vhost_umem {
<span class="p_del">-	struct rb_root umem_tree;</span>
<span class="p_add">+	struct rb_root_cached umem_tree;</span>
 	struct list_head umem_list;
 	int numem;
 };
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 52388611635e..9e755d37c575 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -334,7 +334,7 @@</span> <span class="p_context"> static void remove_huge_page(struct page *page)</span>
 }
 
 static void
<span class="p_del">-hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)</span>
<span class="p_add">+hugetlb_vmdelete_list(struct rb_root_cached *root, pgoff_t start, pgoff_t end)</span>
 {
 	struct vm_area_struct *vma;
 
<span class="p_chunk">@@ -514,7 +514,7 @@</span> <span class="p_context"> static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)</span>
 
 	i_size_write(inode, offset);
 	i_mmap_lock_write(mapping);
<span class="p_del">-	if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap))</span>
<span class="p_add">+	if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap.rb_root))</span>
 		hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap, pgoff, 0);
 	i_mmap_unlock_write(mapping);
 	remove_inode_hugepages(inode, offset, LLONG_MAX);
<span class="p_chunk">@@ -539,7 +539,7 @@</span> <span class="p_context"> static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)</span>
 
 		inode_lock(inode);
 		i_mmap_lock_write(mapping);
<span class="p_del">-		if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap))</span>
<span class="p_add">+		if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap.rb_root))</span>
 			hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap,
 						hole_start &gt;&gt; PAGE_SHIFT,
 						hole_end  &gt;&gt; PAGE_SHIFT);
<span class="p_header">diff --git a/fs/inode.c b/fs/inode.c</span>
<span class="p_header">index 5cbc8e6e9390..419a5c83b689 100644</span>
<span class="p_header">--- a/fs/inode.c</span>
<span class="p_header">+++ b/fs/inode.c</span>
<span class="p_chunk">@@ -353,7 +353,7 @@</span> <span class="p_context"> void address_space_init_once(struct address_space *mapping)</span>
 	init_rwsem(&amp;mapping-&gt;i_mmap_rwsem);
 	INIT_LIST_HEAD(&amp;mapping-&gt;private_list);
 	spin_lock_init(&amp;mapping-&gt;private_lock);
<span class="p_del">-	mapping-&gt;i_mmap = RB_ROOT;</span>
<span class="p_add">+	mapping-&gt;i_mmap = RB_ROOT_CACHED;</span>
 }
 EXPORT_SYMBOL(address_space_init_once);
 
<span class="p_header">diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h</span>
<span class="p_header">index 49b292e98fec..8d10fc97801c 100644</span>
<span class="p_header">--- a/include/drm/drm_mm.h</span>
<span class="p_header">+++ b/include/drm/drm_mm.h</span>
<span class="p_chunk">@@ -172,7 +172,7 @@</span> <span class="p_context"> struct drm_mm {</span>
 	 * according to the (increasing) start address of the memory node. */
 	struct drm_mm_node head_node;
 	/* Keep an interval_tree for fast lookup of drm_mm_nodes by address. */
<span class="p_del">-	struct rb_root interval_tree;</span>
<span class="p_add">+	struct rb_root_cached interval_tree;</span>
 	struct rb_root holes_size;
 	struct rb_root holes_addr;
 
<span class="p_header">diff --git a/include/linux/fs.h b/include/linux/fs.h</span>
<span class="p_header">index 5d11d4335125..57d7baa734cb 100644</span>
<span class="p_header">--- a/include/linux/fs.h</span>
<span class="p_header">+++ b/include/linux/fs.h</span>
<span class="p_chunk">@@ -389,7 +389,7 @@</span> <span class="p_context"> struct address_space {</span>
 	struct radix_tree_root	page_tree;	/* radix tree of all pages */
 	spinlock_t		tree_lock;	/* and lock protecting it */
 	atomic_t		i_mmap_writable;/* count VM_SHARED mappings */
<span class="p_del">-	struct rb_root		i_mmap;		/* tree of private and shared mappings */</span>
<span class="p_add">+	struct rb_root_cached	i_mmap;		/* tree of private and shared mappings */</span>
 	struct rw_semaphore	i_mmap_rwsem;	/* protect tree, count, list */
 	/* Protected by tree_lock together with the radix tree */
 	unsigned long		nrpages;	/* number of total pages */
<span class="p_chunk">@@ -483,7 +483,7 @@</span> <span class="p_context"> static inline void i_mmap_unlock_read(struct address_space *mapping)</span>
  */
 static inline int mapping_mapped(struct address_space *mapping)
 {
<span class="p_del">-	return	!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap);</span>
<span class="p_add">+	return	!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap.rb_root);</span>
 }
 
 /*
<span class="p_header">diff --git a/include/linux/interval_tree.h b/include/linux/interval_tree.h</span>
<span class="p_header">index 724556aa3c95..202ee1283f4b 100644</span>
<span class="p_header">--- a/include/linux/interval_tree.h</span>
<span class="p_header">+++ b/include/linux/interval_tree.h</span>
<span class="p_chunk">@@ -11,13 +11,15 @@</span> <span class="p_context"> struct interval_tree_node {</span>
 };
 
 extern void
<span class="p_del">-interval_tree_insert(struct interval_tree_node *node, struct rb_root *root);</span>
<span class="p_add">+interval_tree_insert(struct interval_tree_node *node,</span>
<span class="p_add">+		     struct rb_root_cached *root);</span>
 
 extern void
<span class="p_del">-interval_tree_remove(struct interval_tree_node *node, struct rb_root *root);</span>
<span class="p_add">+interval_tree_remove(struct interval_tree_node *node,</span>
<span class="p_add">+		     struct rb_root_cached *root);</span>
 
 extern struct interval_tree_node *
<span class="p_del">-interval_tree_iter_first(struct rb_root *root,</span>
<span class="p_add">+interval_tree_iter_first(struct rb_root_cached *root,</span>
 			 unsigned long start, unsigned long last);
 
 extern struct interval_tree_node *
<span class="p_header">diff --git a/include/linux/interval_tree_generic.h b/include/linux/interval_tree_generic.h</span>
<span class="p_header">index 58370e1862ad..f096423c8cbd 100644</span>
<span class="p_header">--- a/include/linux/interval_tree_generic.h</span>
<span class="p_header">+++ b/include/linux/interval_tree_generic.h</span>
<span class="p_chunk">@@ -65,11 +65,13 @@</span> <span class="p_context"> RB_DECLARE_CALLBACKS(static, ITPREFIX ## _augment, ITSTRUCT, ITRB,	      \</span>
 									      \
 /* Insert / remove interval nodes from the tree */			      \
 									      \
<span class="p_del">-ITSTATIC void ITPREFIX ## _insert(ITSTRUCT *node, struct rb_root *root)	      \</span>
<span class="p_add">+ITSTATIC void ITPREFIX ## _insert(ITSTRUCT *node,			      \</span>
<span class="p_add">+				  struct rb_root_cached *root)	 	      \</span>
 {									      \
<span class="p_del">-	struct rb_node **link = &amp;root-&gt;rb_node, *rb_parent = NULL;	      \</span>
<span class="p_add">+	struct rb_node **link = &amp;root-&gt;rb_root.rb_node, *rb_parent = NULL;    \</span>
 	ITTYPE start = ITSTART(node), last = ITLAST(node);		      \
 	ITSTRUCT *parent;						      \
<span class="p_add">+	bool leftmost = true;						      \</span>
 									      \
 	while (*link) {							      \
 		rb_parent = *link;					      \
<span class="p_chunk">@@ -78,18 +80,22 @@</span> <span class="p_context"> ITSTATIC void ITPREFIX ## _insert(ITSTRUCT *node, struct rb_root *root)	      \</span>
 			parent-&gt;ITSUBTREE = last;			      \
 		if (start &lt; ITSTART(parent))				      \
 			link = &amp;parent-&gt;ITRB.rb_left;			      \
<span class="p_del">-		else							      \</span>
<span class="p_add">+		else {							      \</span>
 			link = &amp;parent-&gt;ITRB.rb_right;			      \
<span class="p_add">+			leftmost = false;				      \</span>
<span class="p_add">+		}							      \</span>
 	}								      \
 									      \
 	node-&gt;ITSUBTREE = last;						      \
 	rb_link_node(&amp;node-&gt;ITRB, rb_parent, link);			      \
<span class="p_del">-	rb_insert_augmented(&amp;node-&gt;ITRB, root, &amp;ITPREFIX ## _augment);	      \</span>
<span class="p_add">+	rb_insert_augmented_cached(&amp;node-&gt;ITRB, root,			      \</span>
<span class="p_add">+				   leftmost, &amp;ITPREFIX ## _augment);	      \</span>
 }									      \
 									      \
<span class="p_del">-ITSTATIC void ITPREFIX ## _remove(ITSTRUCT *node, struct rb_root *root)	      \</span>
<span class="p_add">+ITSTATIC void ITPREFIX ## _remove(ITSTRUCT *node,			      \</span>
<span class="p_add">+				  struct rb_root_cached *root)		      \</span>
 {									      \
<span class="p_del">-	rb_erase_augmented(&amp;node-&gt;ITRB, root, &amp;ITPREFIX ## _augment);	      \</span>
<span class="p_add">+	rb_erase_augmented_cached(&amp;node-&gt;ITRB, root, &amp;ITPREFIX ## _augment);  \</span>
 }									      \
 									      \
 /*									      \
<span class="p_chunk">@@ -140,15 +146,35 @@</span> <span class="p_context"> ITPREFIX ## _subtree_search(ITSTRUCT *node, ITTYPE start, ITTYPE last)	      \</span>
 }									      \
 									      \
 ITSTATIC ITSTRUCT *							      \
<span class="p_del">-ITPREFIX ## _iter_first(struct rb_root *root, ITTYPE start, ITTYPE last)      \</span>
<span class="p_add">+ITPREFIX ## _iter_first(struct rb_root_cached *root,			      \</span>
<span class="p_add">+			ITTYPE start, ITTYPE last)			      \</span>
 {									      \
<span class="p_del">-	ITSTRUCT *node;							      \</span>
<span class="p_add">+	ITSTRUCT *node, *leftmost;					      \</span>
 									      \
<span class="p_del">-	if (!root-&gt;rb_node)						      \</span>
<span class="p_add">+	if (!root-&gt;rb_root.rb_node)					      \</span>
 		return NULL;						      \
<span class="p_del">-	node = rb_entry(root-&gt;rb_node, ITSTRUCT, ITRB);			      \</span>
<span class="p_add">+									      \</span>
<span class="p_add">+	/*								      \</span>
<span class="p_add">+	 * Fastpath range intersection/overlap between A: [a0, a1] and	      \</span>
<span class="p_add">+	 * B: [b0, b1] is given by:					      \</span>
<span class="p_add">+	 *								      \</span>
<span class="p_add">+	 *         a0 &lt;= b1 &amp;&amp; b0 &lt;= a1					      \</span>
<span class="p_add">+	 *								      \</span>
<span class="p_add">+	 *  ... where A holds the lock range and B holds the smallest	      \</span>
<span class="p_add">+	 * &#39;start&#39; and largest &#39;last&#39; in the tree. For the later, we	      \</span>
<span class="p_add">+	 * rely on the root node, which by augmented interval tree	      \</span>
<span class="p_add">+	 * property, holds the largest value in its last-in-subtree.	      \</span>
<span class="p_add">+	 * This allows mitigating some of the tree walk overhead for	      \</span>
<span class="p_add">+	 * for non-intersecting ranges, maintained and consulted in O(1).     \</span>
<span class="p_add">+	 */								      \</span>
<span class="p_add">+	node = rb_entry(root-&gt;rb_root.rb_node, ITSTRUCT, ITRB);		      \</span>
 	if (node-&gt;ITSUBTREE &lt; start)					      \
 		return NULL;						      \
<span class="p_add">+									      \</span>
<span class="p_add">+	leftmost = rb_entry(root-&gt;rb_leftmost, ITSTRUCT, ITRB);		      \</span>
<span class="p_add">+	if (ITSTART(leftmost) &gt; last)					      \</span>
<span class="p_add">+		return NULL;						      \</span>
<span class="p_add">+									      \</span>
 	return ITPREFIX ## _subtree_search(node, start, last);		      \
 }									      \
 									      \
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 46b9ac5e8569..3a2652efbbfb 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1992,13 +1992,13 @@</span> <span class="p_context"> extern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);</span>
 
 /* interval_tree.c */
 void vma_interval_tree_insert(struct vm_area_struct *node,
<span class="p_del">-			      struct rb_root *root);</span>
<span class="p_add">+			      struct rb_root_cached *root);</span>
 void vma_interval_tree_insert_after(struct vm_area_struct *node,
 				    struct vm_area_struct *prev,
<span class="p_del">-				    struct rb_root *root);</span>
<span class="p_add">+				    struct rb_root_cached *root);</span>
 void vma_interval_tree_remove(struct vm_area_struct *node,
<span class="p_del">-			      struct rb_root *root);</span>
<span class="p_del">-struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,</span>
<span class="p_add">+			      struct rb_root_cached *root);</span>
<span class="p_add">+struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root_cached *root,</span>
 				unsigned long start, unsigned long last);
 struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,
 				unsigned long start, unsigned long last);
<span class="p_chunk">@@ -2008,11 +2008,12 @@</span> <span class="p_context"> struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,</span>
 	     vma; vma = vma_interval_tree_iter_next(vma, start, last))
 
 void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
<span class="p_del">-				   struct rb_root *root);</span>
<span class="p_add">+				   struct rb_root_cached *root);</span>
 void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
<span class="p_del">-				   struct rb_root *root);</span>
<span class="p_del">-struct anon_vma_chain *anon_vma_interval_tree_iter_first(</span>
<span class="p_del">-	struct rb_root *root, unsigned long start, unsigned long last);</span>
<span class="p_add">+				   struct rb_root_cached *root);</span>
<span class="p_add">+struct anon_vma_chain *</span>
<span class="p_add">+anon_vma_interval_tree_iter_first(struct rb_root_cached *root,</span>
<span class="p_add">+				  unsigned long start, unsigned long last);</span>
 struct anon_vma_chain *anon_vma_interval_tree_iter_next(
 	struct anon_vma_chain *node, unsigned long start, unsigned long last);
 #ifdef CONFIG_DEBUG_VM_RB
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index 43ef2c30cb0f..22c298c6cc26 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -55,7 +55,9 @@</span> <span class="p_context"> struct anon_vma {</span>
 	 * is serialized by a system wide lock only visible to
 	 * mm_take_all_locks() (mm_all_locks_mutex).
 	 */
<span class="p_del">-	struct rb_root rb_root;	/* Interval tree of private &quot;related&quot; vmas */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Interval tree of private &quot;related&quot; vmas */</span>
<span class="p_add">+	struct rb_root_cached rb_root;</span>
 };
 
 /*
<span class="p_header">diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h</span>
<span class="p_header">index fb67554aabd6..5eb7f5bc8248 100644</span>
<span class="p_header">--- a/include/rdma/ib_umem_odp.h</span>
<span class="p_header">+++ b/include/rdma/ib_umem_odp.h</span>
<span class="p_chunk">@@ -111,22 +111,25 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 start_offset, u64 bcnt,</span>
 void ib_umem_odp_unmap_dma_pages(struct ib_umem *umem, u64 start_offset,
 				 u64 bound);
 
<span class="p_del">-void rbt_ib_umem_insert(struct umem_odp_node *node, struct rb_root *root);</span>
<span class="p_del">-void rbt_ib_umem_remove(struct umem_odp_node *node, struct rb_root *root);</span>
<span class="p_add">+void rbt_ib_umem_insert(struct umem_odp_node *node,</span>
<span class="p_add">+			struct rb_root_cached *root);</span>
<span class="p_add">+void rbt_ib_umem_remove(struct umem_odp_node *node,</span>
<span class="p_add">+			struct rb_root_cached *root);</span>
 typedef int (*umem_call_back)(struct ib_umem *item, u64 start, u64 end,
 			      void *cookie);
 /*
  * Call the callback on each ib_umem in the range. Returns the logical or of
  * the return values of the functions called.
  */
<span class="p_del">-int rbt_ib_umem_for_each_in_range(struct rb_root *root, u64 start, u64 end,</span>
<span class="p_add">+int rbt_ib_umem_for_each_in_range(struct rb_root_cached *root,</span>
<span class="p_add">+				  u64 start, u64 end,</span>
 				  umem_call_back cb, void *cookie);
 
 /*
  * Find first region intersecting with address range.
  * Return NULL if not found
  */
<span class="p_del">-struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root *root,</span>
<span class="p_add">+struct ib_umem_odp *rbt_ib_umem_lookup(struct rb_root_cached *root,</span>
 				       u64 addr, u64 length);
 
 static inline int ib_umem_mmu_notifier_retry(struct ib_umem *item,
<span class="p_header">diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h</span>
<span class="p_header">index 0e480a5630d4..3b54b19a8eac 100644</span>
<span class="p_header">--- a/include/rdma/ib_verbs.h</span>
<span class="p_header">+++ b/include/rdma/ib_verbs.h</span>
<span class="p_chunk">@@ -1417,7 +1417,7 @@</span> <span class="p_context"> struct ib_ucontext {</span>
 
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
<span class="p_del">-	struct rb_root      umem_tree;</span>
<span class="p_add">+	struct rb_root_cached   umem_tree;</span>
 	/*
 	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
 	 * mmu notifiers registration.
<span class="p_header">diff --git a/lib/interval_tree_test.c b/lib/interval_tree_test.c</span>
<span class="p_header">index df495fe81421..0e343fd29570 100644</span>
<span class="p_header">--- a/lib/interval_tree_test.c</span>
<span class="p_header">+++ b/lib/interval_tree_test.c</span>
<span class="p_chunk">@@ -19,14 +19,14 @@</span> <span class="p_context"> __param(bool, search_all, false, &quot;Searches will iterate all nodes in the tree&quot;);</span>
 
 __param(uint, max_endpoint, ~0, &quot;Largest value for the interval&#39;s endpoint&quot;);
 
<span class="p_del">-static struct rb_root root = RB_ROOT;</span>
<span class="p_add">+static struct rb_root_cached root = RB_ROOT_CACHED;</span>
 static struct interval_tree_node *nodes = NULL;
 static u32 *queries = NULL;
 
 static struct rnd_state rnd;
 
 static inline unsigned long
<span class="p_del">-search(struct rb_root *root, unsigned long start, unsigned long last)</span>
<span class="p_add">+search(struct rb_root_cached *root, unsigned long start, unsigned long last)</span>
 {
 	struct interval_tree_node *node;
 	unsigned long results = 0;
<span class="p_header">diff --git a/mm/interval_tree.c b/mm/interval_tree.c</span>
<span class="p_header">index f2c2492681bf..b47664358796 100644</span>
<span class="p_header">--- a/mm/interval_tree.c</span>
<span class="p_header">+++ b/mm/interval_tree.c</span>
<span class="p_chunk">@@ -28,7 +28,7 @@</span> <span class="p_context"> INTERVAL_TREE_DEFINE(struct vm_area_struct, shared.rb,</span>
 /* Insert node immediately after prev in the interval tree */
 void vma_interval_tree_insert_after(struct vm_area_struct *node,
 				    struct vm_area_struct *prev,
<span class="p_del">-				    struct rb_root *root)</span>
<span class="p_add">+				    struct rb_root_cached *root)</span>
 {
 	struct rb_node **link;
 	struct vm_area_struct *parent;
<span class="p_chunk">@@ -55,7 +55,7 @@</span> <span class="p_context"> void vma_interval_tree_insert_after(struct vm_area_struct *node,</span>
 
 	node-&gt;shared.rb_subtree_last = last;
 	rb_link_node(&amp;node-&gt;shared.rb, &amp;parent-&gt;shared.rb, link);
<span class="p_del">-	rb_insert_augmented(&amp;node-&gt;shared.rb, root,</span>
<span class="p_add">+	rb_insert_augmented(&amp;node-&gt;shared.rb, &amp;root-&gt;rb_root,</span>
 			    &amp;vma_interval_tree_augment);
 }
 
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,</span>
 		     static inline, __anon_vma_interval_tree)
 
 void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
<span class="p_del">-				   struct rb_root *root)</span>
<span class="p_add">+				   struct rb_root_cached *root)</span>
 {
 #ifdef CONFIG_DEBUG_VM_RB
 	node-&gt;cached_vma_start = avc_start_pgoff(node);
<span class="p_chunk">@@ -84,13 +84,13 @@</span> <span class="p_context"> void anon_vma_interval_tree_insert(struct anon_vma_chain *node,</span>
 }
 
 void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
<span class="p_del">-				   struct rb_root *root)</span>
<span class="p_add">+				   struct rb_root_cached *root)</span>
 {
 	__anon_vma_interval_tree_remove(node, root);
 }
 
 struct anon_vma_chain *
<span class="p_del">-anon_vma_interval_tree_iter_first(struct rb_root *root,</span>
<span class="p_add">+anon_vma_interval_tree_iter_first(struct rb_root_cached *root,</span>
 				  unsigned long first, unsigned long last)
 {
 	return __anon_vma_interval_tree_iter_first(root, first, last);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index cbb57194687e..d533a8913535 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2593,7 +2593,7 @@</span> <span class="p_context"> static void unmap_mapping_range_vma(struct vm_area_struct *vma,</span>
 	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
 }
 
<span class="p_del">-static inline void unmap_mapping_range_tree(struct rb_root *root,</span>
<span class="p_add">+static inline void unmap_mapping_range_tree(struct rb_root_cached *root,</span>
 					    struct zap_details *details)
 {
 	struct vm_area_struct *vma;
<span class="p_chunk">@@ -2657,7 +2657,7 @@</span> <span class="p_context"> void unmap_mapping_range(struct address_space *mapping,</span>
 		details.last_index = ULONG_MAX;
 
 	i_mmap_lock_write(mapping);
<span class="p_del">-	if (unlikely(!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap)))</span>
<span class="p_add">+	if (unlikely(!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap.rb_root)))</span>
 		unmap_mapping_range_tree(&amp;mapping-&gt;i_mmap, &amp;details);
 	i_mmap_unlock_write(mapping);
 }
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 7f8cfe9d9b4d..81d731030a6f 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -684,7 +684,7 @@</span> <span class="p_context"> int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct vm_area_struct *next = vma-&gt;vm_next, *orig_vma = vma;
 	struct address_space *mapping = NULL;
<span class="p_del">-	struct rb_root *root = NULL;</span>
<span class="p_add">+	struct rb_root_cached *root = NULL;</span>
 	struct anon_vma *anon_vma = NULL;
 	struct file *file = vma-&gt;vm_file;
 	bool start_changed = false, end_changed = false;
<span class="p_chunk">@@ -3317,7 +3317,7 @@</span> <span class="p_context"> static DEFINE_MUTEX(mm_all_locks_mutex);</span>
 
 static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)
 {
<span class="p_del">-	if (!test_bit(0, (unsigned long *) &amp;anon_vma-&gt;root-&gt;rb_root.rb_node)) {</span>
<span class="p_add">+	if (!test_bit(0, (unsigned long *) &amp;anon_vma-&gt;rb_root.rb_root.rb_node)) {</span>
 		/*
 		 * The LSB of head.next can&#39;t change from under us
 		 * because we hold the mm_all_locks_mutex.
<span class="p_chunk">@@ -3333,7 +3333,7 @@</span> <span class="p_context"> static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)</span>
 		 * anon_vma-&gt;root-&gt;rwsem.
 		 */
 		if (__test_and_set_bit(0, (unsigned long *)
<span class="p_del">-				       &amp;anon_vma-&gt;root-&gt;rb_root.rb_node))</span>
<span class="p_add">+				       &amp;anon_vma-&gt;root-&gt;rb_root.rb_root.rb_node))</span>
 			BUG();
 	}
 }
<span class="p_chunk">@@ -3435,7 +3435,7 @@</span> <span class="p_context"> int mm_take_all_locks(struct mm_struct *mm)</span>
 
 static void vm_unlock_anon_vma(struct anon_vma *anon_vma)
 {
<span class="p_del">-	if (test_bit(0, (unsigned long *) &amp;anon_vma-&gt;root-&gt;rb_root.rb_node)) {</span>
<span class="p_add">+	if (test_bit(0, (unsigned long *) &amp;anon_vma-&gt;root-&gt;rb_root.rb_root.rb_node)) {</span>
 		/*
 		 * The LSB of head.next can&#39;t change to 0 from under
 		 * us because we hold the mm_all_locks_mutex.
<span class="p_chunk">@@ -3449,7 +3449,7 @@</span> <span class="p_context"> static void vm_unlock_anon_vma(struct anon_vma *anon_vma)</span>
 		 * anon_vma-&gt;root-&gt;rwsem.
 		 */
 		if (!__test_and_clear_bit(0, (unsigned long *)
<span class="p_del">-					  &amp;anon_vma-&gt;root-&gt;rb_root.rb_node))</span>
<span class="p_add">+					  &amp;anon_vma-&gt;root-&gt;rb_root.rb_root.rb_node))</span>
 			BUG();
 		anon_vma_unlock_write(anon_vma);
 	}
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index ced14f1af6dc..ad479e5e081d 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -390,7 +390,7 @@</span> <span class="p_context"> void unlink_anon_vmas(struct vm_area_struct *vma)</span>
 		 * Leave empty anon_vmas on the list - we&#39;ll need
 		 * to free them outside the lock.
 		 */
<span class="p_del">-		if (RB_EMPTY_ROOT(&amp;anon_vma-&gt;rb_root)) {</span>
<span class="p_add">+		if (RB_EMPTY_ROOT(&amp;anon_vma-&gt;rb_root.rb_root)) {</span>
 			anon_vma-&gt;parent-&gt;degree--;
 			continue;
 		}
<span class="p_chunk">@@ -424,7 +424,7 @@</span> <span class="p_context"> static void anon_vma_ctor(void *data)</span>
 
 	init_rwsem(&amp;anon_vma-&gt;rwsem);
 	atomic_set(&amp;anon_vma-&gt;refcount, 0);
<span class="p_del">-	anon_vma-&gt;rb_root = RB_ROOT;</span>
<span class="p_add">+	anon_vma-&gt;rb_root = RB_ROOT_CACHED;</span>
 }
 
 void __init anon_vma_init(void)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



