
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[02/13] iommu: Introduce Interface for IOMMU TLB Flushing - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [02/13] iommu: Introduce Interface for IOMMU TLB Flushing</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 17, 2017, 12:56 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1502974596-23835-3-git-send-email-joro@8bytes.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9906285/mbox/"
   >mbox</a>
|
   <a href="/patch/9906285/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9906285/">/patch/9906285/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B5E5C60386 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 13:03:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9BA4428AE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 13:03:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9030028B03; Thu, 17 Aug 2017 13:03:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C11F128AE1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 13:03:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753280AbdHQND3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 17 Aug 2017 09:03:29 -0400
Received: from 8bytes.org ([81.169.241.247]:43240 &quot;EHLO theia.8bytes.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752676AbdHQM44 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 17 Aug 2017 08:56:56 -0400
Received: by theia.8bytes.org (Postfix, from userid 1000)
	id 95D1B1EA; Thu, 17 Aug 2017 14:56:54 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple; d=8bytes.org; s=mail-1; 
	t=1502974614; bh=rp15LZ7IYgk7wEaSovJ59eBT2gPhdTI5Xy82rpPcBnA=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=Pb2ZPp8Z5UwazELyO073KV/YkzfuTlnm+j2Pu4V4Kbn1tISydD+enfCZkHcE5AtLE
	AMTDrv9GGAzosSXhh5lNsj8RItHk+bNNjvZ8OwOIuqw4i60avaCMDzKdx2Mz1U+UMV
	hKNiMeQDqcgOCF1MLOwMxQ2oa5kym71SlwfjZI1Sk9ZGzi9IqxT3wfNWU08kJAWWrU
	FbXrdqIRQa4Pb17OAZfT6K7zWM1bWxktNpI3EeDE/FlQvzd/JuT9RekaAaT/DHb05G
	E360+mJJDy3TalmDGFixfSkX+4LErxL5Oj1w5TH3dDWmcOsrWc/hNPET7j/lba0yKz
	oBV8c3UGh8f4w==
From: Joerg Roedel &lt;joro@8bytes.org&gt;
To: iommu@lists.linux-foundation.org
Cc: linux-kernel@vger.kernel.org,
	Suravee Suthikulpanit &lt;Suravee.Suthikulpanit@amd.com&gt;,
	Joerg Roedel &lt;jroedel@suse.de&gt;,
	Alex Williamson &lt;alex.williamson@redhat.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, Robin Murphy &lt;robin.murphy@arm.com&gt;
Subject: [PATCH 02/13] iommu: Introduce Interface for IOMMU TLB Flushing
Date: Thu, 17 Aug 2017 14:56:25 +0200
Message-Id: &lt;1502974596-23835-3-git-send-email-joro@8bytes.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1502974596-23835-1-git-send-email-joro@8bytes.org&gt;
References: &lt;1502974596-23835-1-git-send-email-joro@8bytes.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a> - Aug. 17, 2017, 12:56 p.m.</div>
<pre class="content">
<span class="from">From: Joerg Roedel &lt;jroedel@suse.de&gt;</span>

With the current IOMMU-API the hardware TLBs have to be
flushed in every iommu_map(), iommu_map_sg(), and
iommu_unmap() call.

For unmapping large amounts of address space, like it
happens when a KVM domain with assigned devices is
destroyed, this causes thousands of unnecessary TLB flushes
in the IOMMU hardware because the unmap call-back runs for
every unmapped physical page.

With the TLB Flush Interface introduced here the need to
clean the hardware TLBs is removed from the iommu_map/unmap
functions. Users now have to explicitly call these functions
to sync the page-table changes to the hardware.

Three functions are introduced:

	* iommu_flush_tlb_all() - Flushes all TLB entries
	                          associated with that
				  domain. TLBs entries are
				  flushed when this function
				  returns.

	* iommu_tlb_range_add() - This will add a given
				  range to the flush queue
				  for this domain.

	* iommu_tlb_sync() - Flushes all queued ranges from
			     the hardware TLBs. Returns when
			     the flush is finished.

The semantic of this interface is intentionally similar to
the iommu_gather_ops from the io-pgtable code.

Additionally, this patch introduces synchronized versions of
the iommu_map(), iommu_map_sg(), and iommu_unmap()
functions. They will be used by current users of the
IOMMU-API, before they are optimized to the unsynchronized
versions.

Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: Will Deacon &lt;will.deacon@arm.com&gt;
Cc: Robin Murphy &lt;robin.murphy@arm.com&gt;
<span class="signed-off-by">Signed-off-by: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
---
 drivers/iommu/iommu.c | 26 +++++++++++++++++
 include/linux/iommu.h | 80 ++++++++++++++++++++++++++++++++++++++++++++++++++-
 2 files changed, 105 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Aug. 17, 2017, 4:32 p.m.</div>
<pre class="content">
Hi Joerg,

I really like the idea of this, but I have a couple of questions and
comments below.

On Thu, Aug 17, 2017 at 02:56:25PM +0200, Joerg Roedel wrote:
<span class="quote">&gt; From: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With the current IOMMU-API the hardware TLBs have to be</span>
<span class="quote">&gt; flushed in every iommu_map(), iommu_map_sg(), and</span>
<span class="quote">&gt; iommu_unmap() call.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For unmapping large amounts of address space, like it</span>
<span class="quote">&gt; happens when a KVM domain with assigned devices is</span>
<span class="quote">&gt; destroyed, this causes thousands of unnecessary TLB flushes</span>
<span class="quote">&gt; in the IOMMU hardware because the unmap call-back runs for</span>
<span class="quote">&gt; every unmapped physical page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With the TLB Flush Interface introduced here the need to</span>
<span class="quote">&gt; clean the hardware TLBs is removed from the iommu_map/unmap</span>
<span class="quote">&gt; functions. Users now have to explicitly call these functions</span>
<span class="quote">&gt; to sync the page-table changes to the hardware.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Three functions are introduced:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	* iommu_flush_tlb_all() - Flushes all TLB entries</span>
<span class="quote">&gt; 	                          associated with that</span>
<span class="quote">&gt; 				  domain. TLBs entries are</span>
<span class="quote">&gt; 				  flushed when this function</span>
<span class="quote">&gt; 				  returns.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	* iommu_tlb_range_add() - This will add a given</span>
<span class="quote">&gt; 				  range to the flush queue</span>
<span class="quote">&gt; 				  for this domain.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	* iommu_tlb_sync() - Flushes all queued ranges from</span>
<span class="quote">&gt; 			     the hardware TLBs. Returns when</span>
<span class="quote">&gt; 			     the flush is finished.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The semantic of this interface is intentionally similar to</span>
<span class="quote">&gt; the iommu_gather_ops from the io-pgtable code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Additionally, this patch introduces synchronized versions of</span>
<span class="quote">&gt; the iommu_map(), iommu_map_sg(), and iommu_unmap()</span>
<span class="quote">&gt; functions. They will be used by current users of the</span>
<span class="quote">&gt; IOMMU-API, before they are optimized to the unsynchronized</span>
<span class="quote">&gt; versions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; Cc: Robin Murphy &lt;robin.murphy@arm.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/iommu/iommu.c | 26 +++++++++++++++++</span>
<span class="quote">&gt;  include/linux/iommu.h | 80 ++++++++++++++++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  2 files changed, 105 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c</span>
<span class="quote">&gt; index 3f6ea16..816e248 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/iommu.c</span>
<span class="quote">&gt; @@ -527,6 +527,8 @@ static int iommu_group_create_direct_mappings(struct iommu_group *group,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	iommu_flush_tlb_all(domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	iommu_put_resv_regions(dev, &amp;mappings);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1556,6 +1558,18 @@ int iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(iommu_map);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt; +		   phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int ret = iommu_map(domain, iova, paddr, size, prot);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_range_add(domain, iova, size);</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain);</span>

Many IOMMUs don&#39;t need these callbacks on -&gt;map operations, but they won&#39;t
be able to distinguish them easily with this API. Could you add a flags
parameter or something to the iommu_tlb_* functions, please?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL_GPL(iommu_map_sync);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	size_t unmapped_page, unmapped = 0;</span>
<span class="quote">&gt; @@ -1608,6 +1622,18 @@ size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(iommu_unmap);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +size_t iommu_unmap_sync(struct iommu_domain *domain,</span>
<span class="quote">&gt; +			unsigned long iova, size_t size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	size_t ret = iommu_unmap(domain, iova, size);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_range_add(domain, iova, size);</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain);</span>

I think we will struggle to implement this efficiently on ARM SMMUv3. The
way invalidation works there is that there is a single in-memory command
queue into which we can put TLB invalidation commands (they are inserted
under a lock). These are then processed asynchronously by the hardware, and
you can complete them by inserting a SYNC command and waiting for that to
be consumed by the SMMU. Sounds like a perfect fit, right?

The problem is that we want to add those invalidation commands as early
as possible, so that they can be processed by the hardware concurrently
with us unmapping other pages. That means adding the invalidation commands
in the -&gt;unmap callback and not bothering to implement -&gt;iotlb_range_add
callback at all. Then, we will do the sync in -&gt;iotlb_sync. This falls
apart if somebody decides to use iommu_flush_tlb_all(), where we would
prefer not to insert all of the invalidation commands in unmap and instead
insert a single invalidate-all command, followed up with a SYNC.

In other words, we really need the information about the invalidation as
part of the unmap call.

Any ideas?

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101841">Joerg Roedel</a> - Aug. 17, 2017, 4:50 p.m.</div>
<pre class="content">
Hi Will,

On Thu, Aug 17, 2017 at 05:32:35PM +0100, Will Deacon wrote:
<span class="quote">&gt; I really like the idea of this, but I have a couple of questions and</span>
<span class="quote">&gt; comments below.</span>

Great, this together with the common iova-flush it should make it
possible to solve the performance problems of the dma-iommu code.
<span class="quote">
&gt; &gt; +int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt; &gt; +		   phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int ret = iommu_map(domain, iova, paddr, size, prot);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	iommu_tlb_range_add(domain, iova, size);</span>
<span class="quote">&gt; &gt; +	iommu_tlb_sync(domain);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Many IOMMUs don&#39;t need these callbacks on -&gt;map operations, but they won&#39;t</span>
<span class="quote">&gt; be able to distinguish them easily with this API. Could you add a flags</span>
<span class="quote">&gt; parameter or something to the iommu_tlb_* functions, please?</span>

Yeah, this is only needed for virtualized IOMMUs that have a non-present
cache. My idea was to let the iommu-drivers tell the common code whether
the iommu needs it and the code above just checks a flag and omits the
calls to the flush-functions then.

Problem currently is how to get this information from
&#39;struct iommu_device&#39; to &#39;struct iommu_domain&#39;. As a workaround I
consider a per-domain flag in the iommu drivers which checks whether any
unmap has happened and just do nothing on the flush-call-back if there
were none.
<span class="quote">
&gt; I think we will struggle to implement this efficiently on ARM SMMUv3. The</span>
<span class="quote">&gt; way invalidation works there is that there is a single in-memory command</span>
<span class="quote">&gt; queue into which we can put TLB invalidation commands (they are inserted</span>
<span class="quote">&gt; under a lock). These are then processed asynchronously by the hardware, and</span>
<span class="quote">&gt; you can complete them by inserting a SYNC command and waiting for that to</span>
<span class="quote">&gt; be consumed by the SMMU. Sounds like a perfect fit, right?</span>

Yes, its basically the same as way as it works on AMD-Vi and Intel VT-d.
<span class="quote">
&gt; The problem is that we want to add those invalidation commands as early</span>
<span class="quote">&gt; as possible, so that they can be processed by the hardware concurrently</span>
<span class="quote">&gt; with us unmapping other pages.</span>

I think that&#39;s a bad idea, because then you re-introduce the performance
problems again because everyone will spin on the cmd-queue lock in the
unmap path of the dma-api.
<span class="quote">
&gt; That means adding the invalidation commands in the -&gt;unmap callback</span>
<span class="quote">&gt; and not bothering to implement -&gt;iotlb_range_add callback at all.</span>
<span class="quote">&gt; Then, we will do the sync in -&gt;iotlb_sync. This falls apart if</span>
<span class="quote">&gt; somebody decides to use iommu_flush_tlb_all(), where we would prefer</span>
<span class="quote">&gt; not to insert all of the invalidation commands in unmap and instead</span>
<span class="quote">&gt; insert a single invalidate-all command, followed up with a SYNC.</span>

This problem can be solved with the deferred iova flushing code I posted
to the ML. When a queue fills up, iommu_flush_tlb_all() is called and
every entry that was unmapped before can be released. This works well on
x86, are there reasons it wouldn&#39;t on ARM?

Regards,

	Joerg
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Aug. 17, 2017, 5:17 p.m.</div>
<pre class="content">
Hi Joerg,

On Thu, Aug 17, 2017 at 06:50:40PM +0200, Joerg Roedel wrote:
<span class="quote">&gt; On Thu, Aug 17, 2017 at 05:32:35PM +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; I really like the idea of this, but I have a couple of questions and</span>
<span class="quote">&gt; &gt; comments below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Great, this together with the common iova-flush it should make it</span>
<span class="quote">&gt; possible to solve the performance problems of the dma-iommu code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; +int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt; &gt; &gt; +		   phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	int ret = iommu_map(domain, iova, paddr, size, prot);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	iommu_tlb_range_add(domain, iova, size);</span>
<span class="quote">&gt; &gt; &gt; +	iommu_tlb_sync(domain);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Many IOMMUs don&#39;t need these callbacks on -&gt;map operations, but they won&#39;t</span>
<span class="quote">&gt; &gt; be able to distinguish them easily with this API. Could you add a flags</span>
<span class="quote">&gt; &gt; parameter or something to the iommu_tlb_* functions, please?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, this is only needed for virtualized IOMMUs that have a non-present</span>
<span class="quote">&gt; cache. My idea was to let the iommu-drivers tell the common code whether</span>
<span class="quote">&gt; the iommu needs it and the code above just checks a flag and omits the</span>
<span class="quote">&gt; calls to the flush-functions then.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Problem currently is how to get this information from</span>
<span class="quote">&gt; &#39;struct iommu_device&#39; to &#39;struct iommu_domain&#39;. As a workaround I</span>
<span class="quote">&gt; consider a per-domain flag in the iommu drivers which checks whether any</span>
<span class="quote">&gt; unmap has happened and just do nothing on the flush-call-back if there</span>
<span class="quote">&gt; were none.</span>

Given that this can all happen concurrently, I really don&#39;t like the idea of
having to track things with a flag. We&#39;d end up introducing atomics and/or
over-invalidating the TLBs.
<span class="quote">
&gt; &gt; I think we will struggle to implement this efficiently on ARM SMMUv3. The</span>
<span class="quote">&gt; &gt; way invalidation works there is that there is a single in-memory command</span>
<span class="quote">&gt; &gt; queue into which we can put TLB invalidation commands (they are inserted</span>
<span class="quote">&gt; &gt; under a lock). These are then processed asynchronously by the hardware, and</span>
<span class="quote">&gt; &gt; you can complete them by inserting a SYNC command and waiting for that to</span>
<span class="quote">&gt; &gt; be consumed by the SMMU. Sounds like a perfect fit, right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, its basically the same as way as it works on AMD-Vi and Intel VT-d.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The problem is that we want to add those invalidation commands as early</span>
<span class="quote">&gt; &gt; as possible, so that they can be processed by the hardware concurrently</span>
<span class="quote">&gt; &gt; with us unmapping other pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think that&#39;s a bad idea, because then you re-introduce the performance</span>
<span class="quote">&gt; problems again because everyone will spin on the cmd-queue lock in the</span>
<span class="quote">&gt; unmap path of the dma-api.</span>

We don&#39;t actually tend to see issues adding the TLB invalidation commands
under the lock -- the vast majority of the overhead comes from the SYNC.
Besides, I don&#39;t see how adding the commands in the -&gt;iotlb_range_add
callback is any better: it still happens on unmap and it still needs to
take the lock.

If we had something like an -&gt;unmap_all_sync callback, we could incorporate
the TLB invalidation into that.
<span class="quote">
&gt; &gt; That means adding the invalidation commands in the -&gt;unmap callback</span>
<span class="quote">&gt; &gt; and not bothering to implement -&gt;iotlb_range_add callback at all.</span>
<span class="quote">&gt; &gt; Then, we will do the sync in -&gt;iotlb_sync. This falls apart if</span>
<span class="quote">&gt; &gt; somebody decides to use iommu_flush_tlb_all(), where we would prefer</span>
<span class="quote">&gt; &gt; not to insert all of the invalidation commands in unmap and instead</span>
<span class="quote">&gt; &gt; insert a single invalidate-all command, followed up with a SYNC.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This problem can be solved with the deferred iova flushing code I posted</span>
<span class="quote">&gt; to the ML. When a queue fills up, iommu_flush_tlb_all() is called and</span>
<span class="quote">&gt; every entry that was unmapped before can be released. This works well on</span>
<span class="quote">&gt; x86, are there reasons it wouldn&#39;t on ARM?</span>

There are a few reasons I&#39;m not rushing to move to the deferred flushing
code for ARM:

  1. The performance numbers we have suggest that we can achieve near-native
     performance without needing to do that.

  2. We can free page-table pages in unmap, but that&#39;s not safe if we defer
     flushing

  3. Flushing the whole TLB is undesirable and not something we currently
     need to do

  4. There are security implications of deferring the unmap and I&#39;m aware
     of a security research group that use this to gain root privileges.

  5. *If* performance figures end up showing that deferring the flush is
     worthwhile, I would rather use an RCU-based approach for protecting
     the page tables, like we do on the CPU.

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101841">Joerg Roedel</a> - Aug. 17, 2017, 9:20 p.m.</div>
<pre class="content">
Hi Will,

On Thu, Aug 17, 2017 at 06:17:05PM +0100, Will Deacon wrote:
<span class="quote">&gt; On Thu, Aug 17, 2017 at 06:50:40PM +0200, Joerg Roedel wrote:</span>
<span class="quote">&gt; &gt; Problem currently is how to get this information from</span>
<span class="quote">&gt; &gt; &#39;struct iommu_device&#39; to &#39;struct iommu_domain&#39;. As a workaround I</span>
<span class="quote">&gt; &gt; consider a per-domain flag in the iommu drivers which checks whether any</span>
<span class="quote">&gt; &gt; unmap has happened and just do nothing on the flush-call-back if there</span>
<span class="quote">&gt; &gt; were none.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that this can all happen concurrently, I really don&#39;t like the idea of</span>
<span class="quote">&gt; having to track things with a flag. We&#39;d end up introducing atomics and/or</span>
<span class="quote">&gt; over-invalidating the TLBs.</span>

Okay, I look into a better solution for that.
<span class="quote">
&gt; We don&#39;t actually tend to see issues adding the TLB invalidation commands</span>
<span class="quote">&gt; under the lock -- the vast majority of the overhead comes from the SYNC.</span>
<span class="quote">&gt; Besides, I don&#39;t see how adding the commands in the -&gt;iotlb_range_add</span>
<span class="quote">&gt; callback is any better: it still happens on unmap and it still needs to</span>
<span class="quote">&gt; take the lock.</span>

With the deferred flushing you don&#39;t flush anything in the unmap path in
most cases.  All you do there is to add the unmapped iova-range to a
per-cpu list (its actually a ring-buffer). Only when that buffer is full
you do a flush_tlb_all() on the domain and then free all the iova
ranges.

With the flush-counters you can also see which entries in your buffer
have already been flushed from the IO/TLB by another CPU, so that you
can release them right away without any further flush. This way its less
likely that the buffer fills up.

In my tests on x86 I got the flush-rate down to ~1800 flushes/sec at a
network packet rate of 1.45 million pps.
<span class="quote">
&gt; There are a few reasons I&#39;m not rushing to move to the deferred flushing</span>
<span class="quote">&gt; code for ARM:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   1. The performance numbers we have suggest that we can achieve near-native</span>
<span class="quote">&gt;      performance without needing to do that.</span>

Hard to believe when all CPUs fight for the cmd-buffer lock, especially
when you have around 96 CPUs :) Can you share the performance numbers
you have and what you measured?
<span class="quote">
&gt;   2. We can free page-table pages in unmap, but that&#39;s not safe if we defer</span>
<span class="quote">&gt;      flushing</span>

Right, VT-d has the same problem and solved it with a free-list of pages
that is passed to the deferred flushing code. When the IO/TLB is flushed
it calls back into the driver which then frees the pages.
<span class="quote">
&gt;   3. Flushing the whole TLB is undesirable and not something we currently</span>
<span class="quote">&gt;      need to do</span>

Is the TLB-refill cost higher than the time needed to add a
flush-command for every unmapped range?
<span class="quote">
&gt;   4. There are security implications of deferring the unmap and I&#39;m aware</span>
<span class="quote">&gt;      of a security research group that use this to gain root privileges.</span>

Interesting, can you share more about that?
<span class="quote">
&gt;   5. *If* performance figures end up showing that deferring the flush is</span>
<span class="quote">&gt;      worthwhile, I would rather use an RCU-based approach for protecting</span>
<span class="quote">&gt;      the page tables, like we do on the CPU.</span>

Yeah, I don&#39;t like the entry_dtor_cb() I introduced for that very much, so if
there are better solutions I am all ears.


Regards,

	Joerg
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Aug. 18, 2017, 3:16 p.m.</div>
<pre class="content">
Hi Joerg,

On Thu, Aug 17, 2017 at 11:20:54PM +0200, Joerg Roedel wrote:
<span class="quote">&gt; On Thu, Aug 17, 2017 at 06:17:05PM +0100, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; On Thu, Aug 17, 2017 at 06:50:40PM +0200, Joerg Roedel wrote:</span>
<span class="quote">&gt; &gt; &gt; Problem currently is how to get this information from</span>
<span class="quote">&gt; &gt; &gt; &#39;struct iommu_device&#39; to &#39;struct iommu_domain&#39;. As a workaround I</span>
<span class="quote">&gt; &gt; &gt; consider a per-domain flag in the iommu drivers which checks whether any</span>
<span class="quote">&gt; &gt; &gt; unmap has happened and just do nothing on the flush-call-back if there</span>
<span class="quote">&gt; &gt; &gt; were none.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Given that this can all happen concurrently, I really don&#39;t like the idea of</span>
<span class="quote">&gt; &gt; having to track things with a flag. We&#39;d end up introducing atomics and/or</span>
<span class="quote">&gt; &gt; over-invalidating the TLBs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Okay, I look into a better solution for that.</span>

Thanks. One possibility is that IOMMU drivers requiring TLB invalidation on
-&gt;map can set a flag on the domain when they allocate it, which the IOMMU
core can test for later on.
<span class="quote">
&gt; &gt; We don&#39;t actually tend to see issues adding the TLB invalidation commands</span>
<span class="quote">&gt; &gt; under the lock -- the vast majority of the overhead comes from the SYNC.</span>
<span class="quote">&gt; &gt; Besides, I don&#39;t see how adding the commands in the -&gt;iotlb_range_add</span>
<span class="quote">&gt; &gt; callback is any better: it still happens on unmap and it still needs to</span>
<span class="quote">&gt; &gt; take the lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With the deferred flushing you don&#39;t flush anything in the unmap path in</span>
<span class="quote">&gt; most cases.  All you do there is to add the unmapped iova-range to a</span>
<span class="quote">&gt; per-cpu list (its actually a ring-buffer). Only when that buffer is full</span>
<span class="quote">&gt; you do a flush_tlb_all() on the domain and then free all the iova</span>
<span class="quote">&gt; ranges.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With the flush-counters you can also see which entries in your buffer</span>
<span class="quote">&gt; have already been flushed from the IO/TLB by another CPU, so that you</span>
<span class="quote">&gt; can release them right away without any further flush. This way its less</span>
<span class="quote">&gt; likely that the buffer fills up.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In my tests on x86 I got the flush-rate down to ~1800 flushes/sec at a</span>
<span class="quote">&gt; network packet rate of 1.45 million pps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; There are a few reasons I&#39;m not rushing to move to the deferred flushing</span>
<span class="quote">&gt; &gt; code for ARM:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   1. The performance numbers we have suggest that we can achieve near-native</span>
<span class="quote">&gt; &gt;      performance without needing to do that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hard to believe when all CPUs fight for the cmd-buffer lock, especially</span>
<span class="quote">&gt; when you have around 96 CPUs :) Can you share the performance numbers</span>
<span class="quote">&gt; you have and what you measured?</span>

I can only point to numbers that have already been posted to the list, but
the numbers in this thread here are promising:

https://marc.info/?i=c1d85f28-c57b-4414-3504-16afb3a19ce0%40codeaurora.org

Note that patch 1 in that series doesn&#39;t remove the locking, it just removes
the heavy barrier instruction after advancing the queue pointer (and we
can easily do that with your series, deferring it to the sync).

As I said, if the locking does turn out to be a problem and this is backed
up by profiling data, then I&#39;ll look into it, but at the moment it doesn&#39;t
appear to be the case.

Just a thought, but if we could return a token from unmap to pass to the
TLB invalidation functions, then it would be possible to do a spin_trylock
on the command queue lock in -&gt;unmap. If you get the lock, then you put the
command in, if you don&#39;t then you set something in the token and the
add_range would see that flag and insert the commands then.
<span class="quote">
&gt; &gt;   2. We can free page-table pages in unmap, but that&#39;s not safe if we defer</span>
<span class="quote">&gt; &gt;      flushing</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right, VT-d has the same problem and solved it with a free-list of pages</span>
<span class="quote">&gt; that is passed to the deferred flushing code. When the IO/TLB is flushed</span>
<span class="quote">&gt; it calls back into the driver which then frees the pages.</span>

There are some situations where we cannot defer. For example, if we map 2MB
at the PMD level in the page table, but 4k of that region is then unmapped.
In this case, we have to allocate a new level but we cannot plumb it into
the table without performing (and completing) TLB maintenance.
<span class="quote">
&gt; &gt;   3. Flushing the whole TLB is undesirable and not something we currently</span>
<span class="quote">&gt; &gt;      need to do</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is the TLB-refill cost higher than the time needed to add a</span>
<span class="quote">&gt; flush-command for every unmapped range?</span>

The TLB-refill cost is likely to be significantly higher for an SMMU than
the CPU, because the TLBs tend to be distributed with a centralised table
walker. Furthermore, doing an invalidate-all on a domain may cause other
masters in the domain to miss a deadline, which is a common complaint we&#39;ve
had from graphics folks in the past (who basically pin their buffers and
rely on never missing).
<span class="quote">
&gt; &gt;   4. There are security implications of deferring the unmap and I&#39;m aware</span>
<span class="quote">&gt; &gt;      of a security research group that use this to gain root privileges.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Interesting, can you share more about that?</span>

Unfortunately, the paper isn&#39;t public yet so I can&#39;t say more right now.
When it&#39;s published, I&#39;ll point you to it!
<span class="quote">
&gt; &gt;   5. *If* performance figures end up showing that deferring the flush is</span>
<span class="quote">&gt; &gt;      worthwhile, I would rather use an RCU-based approach for protecting</span>
<span class="quote">&gt; &gt;      the page tables, like we do on the CPU.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, I don&#39;t like the entry_dtor_cb() I introduced for that very much, so if</span>
<span class="quote">&gt; there are better solutions I am all ears.</span>

I think it should be possible to protect the page tables with RCU and then
run the TLB invalidation, freeing and IOVA reclaim as part of the callback.
It&#39;s not simple to implement, though, because you run into concurrency
issues where the callback can run in parallel with unmappers etc.

Will
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c</span>
<span class="p_header">index 3f6ea16..816e248 100644</span>
<span class="p_header">--- a/drivers/iommu/iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/iommu.c</span>
<span class="p_chunk">@@ -527,6 +527,8 @@</span> <span class="p_context"> static int iommu_group_create_direct_mappings(struct iommu_group *group,</span>
 
 	}
 
<span class="p_add">+	iommu_flush_tlb_all(domain);</span>
<span class="p_add">+</span>
 out:
 	iommu_put_resv_regions(dev, &amp;mappings);
 
<span class="p_chunk">@@ -1556,6 +1558,18 @@</span> <span class="p_context"> int iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
 }
 EXPORT_SYMBOL_GPL(iommu_map);
 
<span class="p_add">+int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="p_add">+		   phys_addr_t paddr, size_t size, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = iommu_map(domain, iova, paddr, size, prot);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_range_add(domain, iova, size);</span>
<span class="p_add">+	iommu_tlb_sync(domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(iommu_map_sync);</span>
<span class="p_add">+</span>
 size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)
 {
 	size_t unmapped_page, unmapped = 0;
<span class="p_chunk">@@ -1608,6 +1622,18 @@</span> <span class="p_context"> size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova, size_t size)</span>
 }
 EXPORT_SYMBOL_GPL(iommu_unmap);
 
<span class="p_add">+size_t iommu_unmap_sync(struct iommu_domain *domain,</span>
<span class="p_add">+			unsigned long iova, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t ret = iommu_unmap(domain, iova, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_range_add(domain, iova, size);</span>
<span class="p_add">+	iommu_tlb_sync(domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(iommu_unmap_sync);</span>
<span class="p_add">+</span>
 size_t default_iommu_map_sg(struct iommu_domain *domain, unsigned long iova,
 			 struct scatterlist *sg, unsigned int nents, int prot)
 {
<span class="p_header">diff --git a/include/linux/iommu.h b/include/linux/iommu.h</span>
<span class="p_header">index 2cb54ad..7f9c114 100644</span>
<span class="p_header">--- a/include/linux/iommu.h</span>
<span class="p_header">+++ b/include/linux/iommu.h</span>
<span class="p_chunk">@@ -167,6 +167,10 @@</span> <span class="p_context"> struct iommu_resv_region {</span>
  * @map: map a physically contiguous memory region to an iommu domain
  * @unmap: unmap a physically contiguous memory region from an iommu domain
  * @map_sg: map a scatter-gather list of physically contiguous memory chunks
<span class="p_add">+ * @flush_tlb_all: Synchronously flush all hardware TLBs for this domain</span>
<span class="p_add">+ * @tlb_range_add: Add a given iova range to the flush queue for this domain</span>
<span class="p_add">+ * @tlb_sync: Flush all queued ranges from the hardware TLBs and empty flush</span>
<span class="p_add">+ *            queue</span>
  * to an iommu domain
  * @iova_to_phys: translate iova to physical address
  * @add_device: add device to iommu grouping
<span class="p_chunk">@@ -199,6 +203,10 @@</span> <span class="p_context"> struct iommu_ops {</span>
 		     size_t size);
 	size_t (*map_sg)(struct iommu_domain *domain, unsigned long iova,
 			 struct scatterlist *sg, unsigned int nents, int prot);
<span class="p_add">+	void (*flush_iotlb_all)(struct iommu_domain *domain);</span>
<span class="p_add">+	void (*iotlb_range_add)(struct iommu_domain *domain,</span>
<span class="p_add">+				unsigned long iova, size_t size);</span>
<span class="p_add">+	void (*iotlb_sync)(struct iommu_domain *domain);</span>
 	phys_addr_t (*iova_to_phys)(struct iommu_domain *domain, dma_addr_t iova);
 	int (*add_device)(struct device *dev);
 	void (*remove_device)(struct device *dev);
<span class="p_chunk">@@ -285,8 +293,12 @@</span> <span class="p_context"> extern void iommu_detach_device(struct iommu_domain *domain,</span>
 extern struct iommu_domain *iommu_get_domain_for_dev(struct device *dev);
 extern int iommu_map(struct iommu_domain *domain, unsigned long iova,
 		     phys_addr_t paddr, size_t size, int prot);
<span class="p_add">+extern int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="p_add">+			  phys_addr_t paddr, size_t size, int prot);</span>
 extern size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova,
<span class="p_del">-		       size_t size);</span>
<span class="p_add">+			  size_t size);</span>
<span class="p_add">+extern size_t iommu_unmap_sync(struct iommu_domain *domain,</span>
<span class="p_add">+			       unsigned long iova, size_t size);</span>
 extern size_t default_iommu_map_sg(struct iommu_domain *domain, unsigned long iova,
 				struct scatterlist *sg,unsigned int nents,
 				int prot);
<span class="p_chunk">@@ -343,6 +355,25 @@</span> <span class="p_context"> extern void iommu_domain_window_disable(struct iommu_domain *domain, u32 wnd_nr)</span>
 extern int report_iommu_fault(struct iommu_domain *domain, struct device *dev,
 			      unsigned long iova, int flags);
 
<span class="p_add">+static inline void iommu_flush_tlb_all(struct iommu_domain *domain)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (domain-&gt;ops-&gt;flush_iotlb_all)</span>
<span class="p_add">+		domain-&gt;ops-&gt;flush_iotlb_all(domain);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void iommu_tlb_range_add(struct iommu_domain *domain,</span>
<span class="p_add">+				       unsigned long iova, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (domain-&gt;ops-&gt;iotlb_range_add)</span>
<span class="p_add">+		domain-&gt;ops-&gt;iotlb_range_add(domain, iova, size);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void iommu_tlb_sync(struct iommu_domain *domain)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (domain-&gt;ops-&gt;iotlb_sync)</span>
<span class="p_add">+		domain-&gt;ops-&gt;iotlb_sync(domain);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline size_t iommu_map_sg(struct iommu_domain *domain,
 				  unsigned long iova, struct scatterlist *sg,
 				  unsigned int nents, int prot)
<span class="p_chunk">@@ -350,6 +381,20 @@</span> <span class="p_context"> static inline size_t iommu_map_sg(struct iommu_domain *domain,</span>
 	return domain-&gt;ops-&gt;map_sg(domain, iova, sg, nents, prot);
 }
 
<span class="p_add">+static inline size_t iommu_map_sg_sync(struct iommu_domain *domain,</span>
<span class="p_add">+				       unsigned long iova,</span>
<span class="p_add">+				       struct scatterlist *sg,</span>
<span class="p_add">+				       unsigned int nents, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t size = domain-&gt;ops-&gt;map_sg(domain, iova, sg, nents, prot);</span>
<span class="p_add">+	if (size &gt; 0) {</span>
<span class="p_add">+		iommu_tlb_range_add(domain, iova, size);</span>
<span class="p_add">+		iommu_tlb_sync(domain);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* PCI device grouping function */
 extern struct iommu_group *pci_device_group(struct device *dev);
 /* Generic device grouping function */
<span class="p_chunk">@@ -430,12 +475,24 @@</span> <span class="p_context"> static inline int iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
 	return -ENODEV;
 }
 
<span class="p_add">+static inline int iommu_map_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="p_add">+				 phys_addr_t paddr, int gfp_order, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int iommu_unmap(struct iommu_domain *domain, unsigned long iova,
 			      int gfp_order)
 {
 	return -ENODEV;
 }
 
<span class="p_add">+static inline int iommu_unmap_sync(struct iommu_domain *domain, unsigned long iova,</span>
<span class="p_add">+				   int gfp_order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline size_t iommu_map_sg(struct iommu_domain *domain,
 				  unsigned long iova, struct scatterlist *sg,
 				  unsigned int nents, int prot)
<span class="p_chunk">@@ -443,6 +500,27 @@</span> <span class="p_context"> static inline size_t iommu_map_sg(struct iommu_domain *domain,</span>
 	return -ENODEV;
 }
 
<span class="p_add">+static inline size_t iommu_map_sg_sync(struct iommu_domain *domain,</span>
<span class="p_add">+				       unsigned long iova,</span>
<span class="p_add">+				       struct scatterlist *sg,</span>
<span class="p_add">+				       unsigned int nents, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void iommu_flush_tlb_all(struct iommu_domain *domain)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void iommu_tlb_range_add(struct iommu_domain *domain,</span>
<span class="p_add">+				       unsigned long iova, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void iommu_tlb_sync(struct iommu_domain *domain)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int iommu_domain_window_enable(struct iommu_domain *domain,
 					     u32 wnd_nr, phys_addr_t paddr,
 					     u64 size, int prot)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



