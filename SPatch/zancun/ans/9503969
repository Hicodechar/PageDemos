
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,34/55] KVM: arm/arm64: Abstract stage-2 MMU state into a separate structure - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,34/55] KVM: arm/arm64: Abstract stage-2 MMU state into a separate structure</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171407">Jintack Lim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 9, 2017, 6:24 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1483943091-1364-35-git-send-email-jintack@cs.columbia.edu&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9503969/mbox/"
   >mbox</a>
|
   <a href="/patch/9503969/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9503969/">/patch/9503969/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2D83D60757 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:34:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1B9DD280D0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:34:03 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0FE792811C; Mon,  9 Jan 2017 06:34:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6F6B02815E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:34:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1034020AbdAIGdz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 9 Jan 2017 01:33:55 -0500
Received: from outprodmail01.cc.columbia.edu ([128.59.72.39]:38439 &quot;EHLO
	outprodmail01.cc.columbia.edu&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S939742AbdAIG0U (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 9 Jan 2017 01:26:20 -0500
Received: from hazelnut (hazelnut.cc.columbia.edu [128.59.213.250])
	by outprodmail01.cc.columbia.edu (8.14.4/8.14.4) with ESMTP id
	v096Q9uu018089
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 9 Jan 2017 01:26:16 -0500
Received: from hazelnut (localhost.localdomain [127.0.0.1])
	by hazelnut (Postfix) with ESMTP id D1BA68B
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon,  9 Jan 2017 01:26:16 -0500 (EST)
Received: from sendprodmail01.cc.columbia.edu
	(sendprodmail01.cc.columbia.edu [128.59.72.13])
	by hazelnut (Postfix) with ESMTP id B9C358B
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon,  9 Jan 2017 01:26:16 -0500 (EST)
Received: from mail-qk0-f199.google.com (mail-qk0-f199.google.com
	[209.85.220.199])
	by sendprodmail01.cc.columbia.edu (8.14.4/8.14.4) with ESMTP id
	v096QGp6041361
	(version=TLSv1/SSLv3 cipher=AES128-GCM-SHA256 bits=128 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 9 Jan 2017 01:26:16 -0500
Received: by mail-qk0-f199.google.com with SMTP id c69so110044699qkg.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sun, 08 Jan 2017 22:26:16 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=vP+ClwbYviG3CABNCwjdYhlf7gcsiZeRCGpTWnCijTQ=;
	b=SQJx1Bwl7IflrGL7JT4AL4P8JOzBBLVS/ujs9yNZSz6MmL7ZCVpa/81bTAX6/8sdRJ
	o8TPDBib1d5lrSgpmFRjR/jo08dBosg/NFxss6y7/xXBILGfEMoSRubcs0v4iVEbtoX6
	Sn5tAZ8crqxxne521tB7ne9waRx6hG7NAFKy2CaqNNH3RQiPsMirJjSn3o5XnLBhIbHA
	7FrDRprO1nkBFs7gumPbIeAYQUSskR4gCZuSHcAJ/vmjZ7JAaLsQdAdqFoKl1OtdzsDH
	vU/yiEhVcp667E0uR555S9l0VPZldnXpdB1gTWcR1V7FqBo9JyWVhaoC0zPnHpikTaqQ
	eK0w==
X-Gm-Message-State: AIkVDXLmK7NMj1nynwSszza8Up5B6Lq714MSoeZUljQW2NOimT2Ri/8FRSe68ceGx8m3QDyVjbYFufi9CRdnePwWqO/KVE5D+bflbKjyOqFrE2LuC50CXAGrzj8YrtEcC6dS9aYhSp2mI3nMB2Y2TnpgtkQ=
X-Received: by 10.55.45.4 with SMTP id t4mr56825016qkh.145.1483943176017;
	Sun, 08 Jan 2017 22:26:16 -0800 (PST)
X-Received: by 10.55.45.4 with SMTP id t4mr56824983qkh.145.1483943175580;
	Sun, 08 Jan 2017 22:26:15 -0800 (PST)
Received: from jintack.cs.columbia.edu
	([2001:18d8:ffff:16:21a:4aff:feaa:f900])
	by smtp.gmail.com with ESMTPSA id
	h3sm8623257qtc.6.2017.01.08.22.26.14
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Sun, 08 Jan 2017 22:26:15 -0800 (PST)
From: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
To: christoffer.dall@linaro.org, marc.zyngier@arm.com,
	pbonzini@redhat.com, rkrcmar@redhat.com, linux@armlinux.org.uk,
	catalin.marinas@arm.com, will.deacon@arm.com,
	vladimir.murzin@arm.com, suzuki.poulose@arm.com,
	mark.rutland@arm.com, james.morse@arm.com,
	lorenzo.pieralisi@arm.com, kevin.brodsky@arm.com,
	wcohen@redhat.com, shankerd@codeaurora.org, geoff@infradead.org,
	andre.przywara@arm.com, eric.auger@redhat.com,
	anna-maria@linutronix.de, shihwei@cs.columbia.edu,
	linux-arm-kernel@lists.infradead.org, kvmarm@lists.cs.columbia.edu,
	kvm@vger.kernel.org, linux-kernel@vger.kernel.org
Cc: jintack@cs.columbia.edu
Subject: [RFC 34/55] KVM: arm/arm64: Abstract stage-2 MMU state into a
	separate structure
Date: Mon,  9 Jan 2017 01:24:30 -0500
Message-Id: &lt;1483943091-1364-35-git-send-email-jintack@cs.columbia.edu&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1483943091-1364-1-git-send-email-jintack@cs.columbia.edu&gt;
References: &lt;1483943091-1364-1-git-send-email-jintack@cs.columbia.edu&gt;
X-No-Spam-Score: Local
X-Scanned-By: MIMEDefang 2.78 on 128.59.72.13
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171407">Jintack Lim</a> - Jan. 9, 2017, 6:24 a.m.</div>
<pre class="content">
<span class="from">From: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>

Abstract stage-2 MMU state into a separate structure and change all
callers referring to page tables, VMIDs, and the VTTBR to use this new
indirection.

This is about to become very handy when using shadow stage-2 page
tables.
<span class="signed-off-by">
Signed-off-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Jintack Lim &lt;jintack@cs.columbia.edu&gt;</span>
---
 arch/arm/include/asm/kvm_asm.h    |   7 +-
 arch/arm/include/asm/kvm_host.h   |  26 ++++---
 arch/arm/kvm/arm.c                |  34 +++++----
 arch/arm/kvm/hyp/switch.c         |   5 +-
 arch/arm/kvm/hyp/tlb.c            |  18 ++---
 arch/arm/kvm/mmu.c                | 146 +++++++++++++++++++++-----------------
 arch/arm64/include/asm/kvm_asm.h  |   7 +-
 arch/arm64/include/asm/kvm_host.h |  10 ++-
 arch/arm64/kvm/hyp/switch.c       |   5 +-
 arch/arm64/kvm/hyp/tlb.c          |  20 +++---
 10 files changed, 159 insertions(+), 119 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_asm.h b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">index 8ef0538..36e3856 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -57,6 +57,7 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;
<span class="p_add">+struct kvm_s2_mmu;</span>
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
<span class="p_chunk">@@ -64,9 +65,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm *kvm);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">index d5423ab..f84a59c 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -53,9 +53,21 @@</span> <span class="p_context"></span>
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 void kvm_reset_coprocs(struct kvm_vcpu *vcpu);
 
<span class="p_del">-struct kvm_arch {</span>
<span class="p_del">-	/* VTTBR value associated with below pgd and vmid */</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
<span class="p_add">+	/* The VMID generation used for the virt. memory system */</span>
<span class="p_add">+	u64    vmid_gen;</span>
<span class="p_add">+	u32    vmid;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Stage-2 page table */</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* VTTBR value associated with above pgd and vmid */</span>
 	u64    vttbr;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_arch {</span>
<span class="p_add">+	/* Stage 2 paging state for the VM */</span>
<span class="p_add">+	struct kvm_s2_mmu mmu;</span>
 
 	/* The last vcpu id that ran on each physical CPU */
 	int __percpu *last_vcpu_ran;
<span class="p_chunk">@@ -68,13 +80,6 @@</span> <span class="p_context"> struct kvm_arch {</span>
 	 * here.
 	 */
 
<span class="p_del">-	/* The VMID generation used for the virt. memory system */</span>
<span class="p_del">-	u64    vmid_gen;</span>
<span class="p_del">-	u32    vmid;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Stage-2 page table */</span>
<span class="p_del">-	pgd_t *pgd;</span>
<span class="p_del">-</span>
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
 	int max_vcpus;
<span class="p_chunk">@@ -188,6 +193,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Detect first run of a vcpu */
 	bool has_run_once;
<span class="p_add">+</span>
<span class="p_add">+	/* Stage 2 paging state used by the hardware on next switch */</span>
<span class="p_add">+	struct kvm_s2_mmu *hw_mmu;</span>
 };
 
 struct kvm_vm_stat {
<span class="p_header">diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c</span>
<span class="p_header">index 436bf5a..eb3e709 100644</span>
<span class="p_header">--- a/arch/arm/kvm/arm.c</span>
<span class="p_header">+++ b/arch/arm/kvm/arm.c</span>
<span class="p_chunk">@@ -139,7 +139,7 @@</span> <span class="p_context"> int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)</span>
 	kvm_timer_init(kvm);
 
 	/* Mark the initial VMID generation invalid */
<span class="p_del">-	kvm-&gt;arch.vmid_gen = 0;</span>
<span class="p_add">+	kvm-&gt;arch.mmu.vmid_gen = 0;</span>
 
 	/* The maximum number of VCPUs is limited by the host&#39;s GIC model */
 	kvm-&gt;arch.max_vcpus = vgic_present ?
<span class="p_chunk">@@ -321,6 +321,8 @@</span> <span class="p_context"> int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)</span>
 
 	kvm_arm_reset_debug_ptr(vcpu);
 
<span class="p_add">+	vcpu-&gt;arch.hw_mmu = &amp;vcpu-&gt;kvm-&gt;arch.mmu;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -335,7 +337,7 @@</span> <span class="p_context"> void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	 * over-invalidation doesn&#39;t affect correctness.
 	 */
 	if (*last_ran != vcpu-&gt;vcpu_id) {
<span class="p_del">-		kvm_call_hyp(__kvm_tlb_flush_local_vmid, vcpu);</span>
<span class="p_add">+		kvm_call_hyp(__kvm_tlb_flush_local_vmid, &amp;vcpu-&gt;kvm-&gt;arch.mmu);</span>
 		*last_ran = vcpu-&gt;vcpu_id;
 	}
 
<span class="p_chunk">@@ -423,25 +425,26 @@</span> <span class="p_context"> void force_vm_exit(const cpumask_t *mask)</span>
  * VMID for the new generation, we must flush necessary caches and TLBs on all
  * CPUs.
  */
<span class="p_del">-static bool need_new_vmid_gen(struct kvm *kvm)</span>
<span class="p_add">+static bool need_new_vmid_gen(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	return unlikely(kvm-&gt;arch.vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
<span class="p_add">+	return unlikely(mmu-&gt;vmid_gen != atomic64_read(&amp;kvm_vmid_gen));</span>
 }
 
 /**
  * update_vttbr - Update the VTTBR with a valid VMID before the guest runs
<span class="p_del">- * @kvm	The guest that we are about to run</span>
<span class="p_add">+ * @kvm:	The guest that we are about to run</span>
<span class="p_add">+ * @mmu:	The stage-2 translation context to update</span>
  *
  * Called from kvm_arch_vcpu_ioctl_run before entering the guest to ensure the
  * VM has a valid VMID, otherwise assigns a new one and flushes corresponding
  * caches and TLBs.
  */
<span class="p_del">-static void update_vttbr(struct kvm *kvm)</span>
<span class="p_add">+static void update_vttbr(struct kvm *kvm, struct kvm_s2_mmu *mmu)</span>
 {
 	phys_addr_t pgd_phys;
 	u64 vmid;
 
<span class="p_del">-	if (!need_new_vmid_gen(kvm))</span>
<span class="p_add">+	if (!need_new_vmid_gen(mmu))</span>
 		return;
 
 	spin_lock(&amp;kvm_vmid_lock);
<span class="p_chunk">@@ -451,7 +454,7 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm)</span>
 	 * already allocated a valid vmid for this vm, then this vcpu should
 	 * use the same vmid.
 	 */
<span class="p_del">-	if (!need_new_vmid_gen(kvm)) {</span>
<span class="p_add">+	if (!need_new_vmid_gen(mmu)) {</span>
 		spin_unlock(&amp;kvm_vmid_lock);
 		return;
 	}
<span class="p_chunk">@@ -475,16 +478,17 @@</span> <span class="p_context"> static void update_vttbr(struct kvm *kvm)</span>
 		kvm_call_hyp(__kvm_flush_vm_context);
 	}
 
<span class="p_del">-	kvm-&gt;arch.vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_del">-	kvm-&gt;arch.vmid = kvm_next_vmid;</span>
<span class="p_add">+	mmu-&gt;vmid_gen = atomic64_read(&amp;kvm_vmid_gen);</span>
<span class="p_add">+	mmu-&gt;vmid = kvm_next_vmid;</span>
 	kvm_next_vmid++;
 	kvm_next_vmid &amp;= (1 &lt;&lt; kvm_vmid_bits) - 1;
 
 	/* update vttbr to be used with the new vmid */
<span class="p_del">-	pgd_phys = virt_to_phys(kvm-&gt;arch.pgd);</span>
<span class="p_add">+	pgd_phys = virt_to_phys(mmu-&gt;pgd);</span>
 	BUG_ON(pgd_phys &amp; ~VTTBR_BADDR_MASK);
<span class="p_del">-	vmid = ((u64)(kvm-&gt;arch.vmid) &lt;&lt; VTTBR_VMID_SHIFT) &amp; VTTBR_VMID_MASK(kvm_vmid_bits);</span>
<span class="p_del">-	kvm-&gt;arch.vttbr = pgd_phys | vmid;</span>
<span class="p_add">+	vmid = ((u64)(mmu-&gt;vmid) &lt;&lt; VTTBR_VMID_SHIFT) &amp;</span>
<span class="p_add">+	       VTTBR_VMID_MASK(kvm_vmid_bits);</span>
<span class="p_add">+	mmu-&gt;vttbr = pgd_phys | vmid;</span>
 
 	spin_unlock(&amp;kvm_vmid_lock);
 }
<span class="p_chunk">@@ -611,7 +615,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 */
 		cond_resched();
 
<span class="p_del">-		update_vttbr(vcpu-&gt;kvm);</span>
<span class="p_add">+		update_vttbr(vcpu-&gt;kvm, vcpu-&gt;arch.hw_mmu);</span>
 
 		if (vcpu-&gt;arch.power_off || vcpu-&gt;arch.pause)
 			vcpu_sleep(vcpu);
<span class="p_chunk">@@ -636,7 +640,7 @@</span> <span class="p_context"> int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 			run-&gt;exit_reason = KVM_EXIT_INTR;
 		}
 
<span class="p_del">-		if (ret &lt;= 0 || need_new_vmid_gen(vcpu-&gt;kvm) ||</span>
<span class="p_add">+		if (ret &lt;= 0 || need_new_vmid_gen(vcpu-&gt;arch.hw_mmu) ||</span>
 			vcpu-&gt;arch.power_off || vcpu-&gt;arch.pause) {
 			local_irq_enable();
 			kvm_pmu_sync_hwstate(vcpu);
<span class="p_header">diff --git a/arch/arm/kvm/hyp/switch.c b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">index 92678b7..6f99de1 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -73,8 +73,9 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(vcpu-&gt;kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	write_sysreg(vcpu-&gt;arch.midr, VPIDR);
 }
 
<span class="p_header">diff --git a/arch/arm/kvm/hyp/tlb.c b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">index 6d810af..56f0a49 100644</span>
<span class="p_header">--- a/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -34,13 +34,13 @@</span> <span class="p_context"></span>
  * As v7 does not support flushing per IPA, just nuke the whole TLB
  * instead, ignoring the ipa value.
  */
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALLIS);
<span class="p_chunk">@@ -50,17 +50,17 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
 	write_sysreg(0, VTTBR);
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+					 phys_addr_t ipa)</span>
 {
<span class="p_del">-	__kvm_tlb_flush_vmid(kvm);</span>
<span class="p_add">+	__kvm_tlb_flush_vmid(mmu);</span>
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(kern_hyp_va(vcpu)-&gt;kvm);</span>
<span class="p_del">-</span>
 	/* Switch to requested VMID */
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, VTTBR);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, VTTBR);</span>
 	isb();
 
 	write_sysreg(0, TLBIALL);
<span class="p_header">diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="p_header">index 57cb671..a27a204 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmu.c</span>
<span class="p_chunk">@@ -63,9 +63,9 @@</span> <span class="p_context"> void kvm_flush_remote_tlbs(struct kvm *kvm)</span>
 	kvm_call_hyp(__kvm_tlb_flush_vmid, kvm);
 }
 
<span class="p_del">-static void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+static void kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa)</span>
 {
<span class="p_del">-	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, kvm, ipa);</span>
<span class="p_add">+	kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ipa);</span>
 }
 
 /*
<span class="p_chunk">@@ -102,13 +102,14 @@</span> <span class="p_context"> static bool kvm_is_device_pfn(unsigned long pfn)</span>
  * Function clears a PMD entry, flushes addr 1st and 2nd stage TLBs. Marks all
  * pages in the range dirty.
  */
<span class="p_del">-static void stage2_dissolve_pmd(struct kvm *kvm, phys_addr_t addr, pmd_t *pmd)</span>
<span class="p_add">+static void stage2_dissolve_pmd(struct kvm_s2_mmu *mmu, phys_addr_t addr,</span>
<span class="p_add">+				pmd_t *pmd)</span>
 {
 	if (!pmd_thp_or_huge(*pmd))
 		return;
 
 	pmd_clear(pmd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	put_page(virt_to_page(pmd));
 }
 
<span class="p_chunk">@@ -144,31 +145,34 @@</span> <span class="p_context"> static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)</span>
 	return p;
 }
 
<span class="p_del">-static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pgd_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pgd_t *pgd, phys_addr_t addr)</span>
 {
 	pud_t *pud_table __maybe_unused = stage2_pud_offset(pgd, 0UL);
 	stage2_pgd_clear(pgd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	stage2_pud_free(pud_table);
 	put_page(virt_to_page(pgd));
 }
 
<span class="p_del">-static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pud_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pud_t *pud, phys_addr_t addr)</span>
 {
 	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);
 	VM_BUG_ON(stage2_pud_huge(*pud));
 	stage2_pud_clear(pud);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	stage2_pmd_free(pmd_table);
 	put_page(virt_to_page(pud));
 }
 
<span class="p_del">-static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pmd_entry(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+				   pmd_t *pmd, phys_addr_t addr)</span>
 {
 	pte_t *pte_table = pte_offset_kernel(pmd, 0);
 	VM_BUG_ON(pmd_thp_or_huge(*pmd));
 	pmd_clear(pmd);
<span class="p_del">-	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+	kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	pte_free_kernel(NULL, pte_table);
 	put_page(virt_to_page(pmd));
 }
<span class="p_chunk">@@ -193,7 +197,7 @@</span> <span class="p_context"> static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr</span>
  * the corresponding TLBs, we call kvm_flush_dcache_p*() to make sure
  * the IO subsystem will never hit in the cache.
  */
<span class="p_del">-static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="p_add">+static void unmap_stage2_ptes(struct kvm_s2_mmu *mmu, pmd_t *pmd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t start_addr = addr;
<span class="p_chunk">@@ -205,7 +209,7 @@</span> <span class="p_context"> static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 			pte_t old_pte = *pte;
 
 			kvm_set_pte(pte, __pte(0));
<span class="p_del">-			kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+			kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 
 			/* No need to invalidate the cache for device mappings */
 			if (!kvm_is_device_pfn(pte_pfn(old_pte)))
<span class="p_chunk">@@ -216,10 +220,10 @@</span> <span class="p_context"> static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
 	if (stage2_pte_table_empty(start_pte))
<span class="p_del">-		clear_stage2_pmd_entry(kvm, pmd, start_addr);</span>
<span class="p_add">+		clear_stage2_pmd_entry(mmu, pmd, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="p_add">+static void unmap_stage2_pmds(struct kvm_s2_mmu *mmu, pud_t *pud,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
<span class="p_chunk">@@ -233,22 +237,22 @@</span> <span class="p_context"> static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
 				pmd_t old_pmd = *pmd;
 
 				pmd_clear(pmd);
<span class="p_del">-				kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+				kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 
 				kvm_flush_dcache_pmd(old_pmd);
 
 				put_page(virt_to_page(pmd));
 			} else {
<span class="p_del">-				unmap_stage2_ptes(kvm, pmd, addr, next);</span>
<span class="p_add">+				unmap_stage2_ptes(mmu, pmd, addr, next);</span>
 			}
 		}
 	} while (pmd++, addr = next, addr != end);
 
 	if (stage2_pmd_table_empty(start_pmd))
<span class="p_del">-		clear_stage2_pud_entry(kvm, pud, start_addr);</span>
<span class="p_add">+		clear_stage2_pud_entry(mmu, pud, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
<span class="p_add">+static void unmap_stage2_puds(struct kvm_s2_mmu *mmu, pgd_t *pgd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
<span class="p_chunk">@@ -262,17 +266,17 @@</span> <span class="p_context"> static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
 				pud_t old_pud = *pud;
 
 				stage2_pud_clear(pud);
<span class="p_del">-				kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+				kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 				kvm_flush_dcache_pud(old_pud);
 				put_page(virt_to_page(pud));
 			} else {
<span class="p_del">-				unmap_stage2_pmds(kvm, pud, addr, next);</span>
<span class="p_add">+				unmap_stage2_pmds(mmu, pud, addr, next);</span>
 			}
 		}
 	} while (pud++, addr = next, addr != end);
 
 	if (stage2_pud_table_empty(start_pud))
<span class="p_del">-		clear_stage2_pgd_entry(kvm, pgd, start_addr);</span>
<span class="p_add">+		clear_stage2_pgd_entry(mmu, pgd, start_addr);</span>
 }
 
 /**
<span class="p_chunk">@@ -286,17 +290,18 @@</span> <span class="p_context"> static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
  * destroying the VM), otherwise another faulting VCPU may come in and mess
  * with things behind our backs.
  */
<span class="p_del">-static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
<span class="p_add">+static void unmap_stage2_range(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			       phys_addr_t start, u64 size)</span>
 {
 	pgd_t *pgd;
 	phys_addr_t addr = start, end = start + size;
 	phys_addr_t next;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		next = stage2_pgd_addr_end(addr, end);
 		if (!stage2_pgd_none(*pgd))
<span class="p_del">-			unmap_stage2_puds(kvm, pgd, addr, next);</span>
<span class="p_add">+			unmap_stage2_puds(mmu, pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -348,7 +353,7 @@</span> <span class="p_context"> static void stage2_flush_puds(pgd_t *pgd,</span>
 	} while (pud++, addr = next, addr != end);
 }
 
<span class="p_del">-static void stage2_flush_memslot(struct kvm *kvm,</span>
<span class="p_add">+static void stage2_flush_memslot(struct kvm_s2_mmu *mmu,</span>
 				 struct kvm_memory_slot *memslot)
 {
 	phys_addr_t addr = memslot-&gt;base_gfn &lt;&lt; PAGE_SHIFT;
<span class="p_chunk">@@ -356,7 +361,7 @@</span> <span class="p_context"> static void stage2_flush_memslot(struct kvm *kvm,</span>
 	phys_addr_t next;
 	pgd_t *pgd;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		next = stage2_pgd_addr_end(addr, end);
 		stage2_flush_puds(pgd, addr, next);
<span class="p_chunk">@@ -381,7 +386,7 @@</span> <span class="p_context"> static void stage2_flush_vm(struct kvm *kvm)</span>
 
 	slots = kvm_memslots(kvm);
 	kvm_for_each_memslot(memslot, slots)
<span class="p_del">-		stage2_flush_memslot(kvm, memslot);</span>
<span class="p_add">+		stage2_flush_memslot(&amp;kvm-&gt;arch.mmu, memslot);</span>
 
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	srcu_read_unlock(&amp;kvm-&gt;srcu, idx);
<span class="p_chunk">@@ -733,8 +738,9 @@</span> <span class="p_context"> int create_hyp_io_mappings(void *from, void *to, phys_addr_t phys_addr)</span>
 int kvm_alloc_stage2_pgd(struct kvm *kvm)
 {
 	pgd_t *pgd;
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
 
<span class="p_del">-	if (kvm-&gt;arch.pgd != NULL) {</span>
<span class="p_add">+	if (mmu-&gt;pgd != NULL) {</span>
 		kvm_err(&quot;kvm_arch already initialized?\n&quot;);
 		return -EINVAL;
 	}
<span class="p_chunk">@@ -744,11 +750,12 @@</span> <span class="p_context"> int kvm_alloc_stage2_pgd(struct kvm *kvm)</span>
 	if (!pgd)
 		return -ENOMEM;
 
<span class="p_del">-	kvm-&gt;arch.pgd = pgd;</span>
<span class="p_add">+	mmu-&gt;pgd = pgd;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_del">-static void stage2_unmap_memslot(struct kvm *kvm,</span>
<span class="p_add">+static void stage2_unmap_memslot(struct kvm_s2_mmu *mmu,</span>
 				 struct kvm_memory_slot *memslot)
 {
 	hva_t hva = memslot-&gt;userspace_addr;
<span class="p_chunk">@@ -783,7 +790,7 @@</span> <span class="p_context"> static void stage2_unmap_memslot(struct kvm *kvm,</span>
 
 		if (!(vma-&gt;vm_flags &amp; VM_PFNMAP)) {
 			gpa_t gpa = addr + (vm_start - memslot-&gt;userspace_addr);
<span class="p_del">-			unmap_stage2_range(kvm, gpa, vm_end - vm_start);</span>
<span class="p_add">+			unmap_stage2_range(mmu, gpa, vm_end - vm_start);</span>
 		}
 		hva = vm_end;
 	} while (hva &lt; reg_end);
<span class="p_chunk">@@ -807,7 +814,7 @@</span> <span class="p_context"> void stage2_unmap_vm(struct kvm *kvm)</span>
 
 	slots = kvm_memslots(kvm);
 	kvm_for_each_memslot(memslot, slots)
<span class="p_del">-		stage2_unmap_memslot(kvm, memslot);</span>
<span class="p_add">+		stage2_unmap_memslot(&amp;kvm-&gt;arch.mmu, memslot);</span>
 
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	srcu_read_unlock(&amp;kvm-&gt;srcu, idx);
<span class="p_chunk">@@ -826,22 +833,25 @@</span> <span class="p_context"> void stage2_unmap_vm(struct kvm *kvm)</span>
  */
 void kvm_free_stage2_pgd(struct kvm *kvm)
 {
<span class="p_del">-	if (kvm-&gt;arch.pgd == NULL)</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = &amp;kvm-&gt;arch.mmu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mmu-&gt;pgd == NULL)</span>
 		return;
 
<span class="p_del">-	unmap_stage2_range(kvm, 0, KVM_PHYS_SIZE);</span>
<span class="p_add">+	unmap_stage2_range(mmu, 0, KVM_PHYS_SIZE);</span>
 	/* Free the HW pgd, one page at a time */
<span class="p_del">-	free_pages_exact(kvm-&gt;arch.pgd, S2_PGD_SIZE);</span>
<span class="p_del">-	kvm-&gt;arch.pgd = NULL;</span>
<span class="p_add">+	free_pages_exact(mmu-&gt;pgd, S2_PGD_SIZE);</span>
<span class="p_add">+	mmu-&gt;pgd = NULL;</span>
 }
 
<span class="p_del">-static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static pud_t *stage2_get_pud(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			     struct kvm_mmu_memory_cache *cache,</span>
 			     phys_addr_t addr)
 {
 	pgd_t *pgd;
 	pud_t *pud;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	if (WARN_ON(stage2_pgd_none(*pgd))) {
 		if (!cache)
 			return NULL;
<span class="p_chunk">@@ -853,13 +863,14 @@</span> <span class="p_context"> static pud_t *stage2_get_pud(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
 	return stage2_pud_offset(pgd, addr);
 }
 
<span class="p_del">-static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static pmd_t *stage2_get_pmd(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			     struct kvm_mmu_memory_cache *cache,</span>
 			     phys_addr_t addr)
 {
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_del">-	pud = stage2_get_pud(kvm, cache, addr);</span>
<span class="p_add">+	pud = stage2_get_pud(mmu, cache, addr);</span>
 	if (stage2_pud_none(*pud)) {
 		if (!cache)
 			return NULL;
<span class="p_chunk">@@ -871,12 +882,13 @@</span> <span class="p_context"> static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache</span>
 	return stage2_pmd_offset(pud, addr);
 }
 
<span class="p_del">-static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
<span class="p_add">+static int stage2_set_pmd_huge(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			       struct kvm_mmu_memory_cache</span>
 			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
 {
 	pmd_t *pmd, old_pmd;
 
<span class="p_del">-	pmd = stage2_get_pmd(kvm, cache, addr);</span>
<span class="p_add">+	pmd = stage2_get_pmd(mmu, cache, addr);</span>
 	VM_BUG_ON(!pmd);
 
 	/*
<span class="p_chunk">@@ -893,7 +905,7 @@</span> <span class="p_context"> static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
 	old_pmd = *pmd;
 	if (pmd_present(old_pmd)) {
 		pmd_clear(pmd);
<span class="p_del">-		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+		kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	} else {
 		get_page(virt_to_page(pmd));
 	}
<span class="p_chunk">@@ -902,7 +914,8 @@</span> <span class="p_context"> static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache</span>
 	return 0;
 }
 
<span class="p_del">-static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
<span class="p_add">+static int stage2_set_pte(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			  struct kvm_mmu_memory_cache *cache,</span>
 			  phys_addr_t addr, const pte_t *new_pte,
 			  unsigned long flags)
 {
<span class="p_chunk">@@ -914,7 +927,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	VM_BUG_ON(logging_active &amp;&amp; !cache);
 
 	/* Create stage-2 page table mapping - Levels 0 and 1 */
<span class="p_del">-	pmd = stage2_get_pmd(kvm, cache, addr);</span>
<span class="p_add">+	pmd = stage2_get_pmd(mmu, cache, addr);</span>
 	if (!pmd) {
 		/*
 		 * Ignore calls from kvm_set_spte_hva for unallocated
<span class="p_chunk">@@ -928,7 +941,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	 * allocate page.
 	 */
 	if (logging_active)
<span class="p_del">-		stage2_dissolve_pmd(kvm, addr, pmd);</span>
<span class="p_add">+		stage2_dissolve_pmd(mmu, addr, pmd);</span>
 
 	/* Create stage-2 page mappings - Level 2 */
 	if (pmd_none(*pmd)) {
<span class="p_chunk">@@ -948,7 +961,7 @@</span> <span class="p_context"> static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,</span>
 	old_pte = *pte;
 	if (pte_present(old_pte)) {
 		kvm_set_pte(pte, __pte(0));
<span class="p_del">-		kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="p_add">+		kvm_tlb_flush_vmid_ipa(mmu, addr);</span>
 	} else {
 		get_page(virt_to_page(pte));
 	}
<span class="p_chunk">@@ -1008,7 +1021,7 @@</span> <span class="p_context"> int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,</span>
 		if (ret)
 			goto out;
 		spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-		ret = stage2_set_pte(kvm, &amp;cache, addr, &amp;pte,</span>
<span class="p_add">+		ret = stage2_set_pte(&amp;kvm-&gt;arch.mmu, &amp;cache, addr, &amp;pte,</span>
 						KVM_S2PTE_FLAG_IS_IOMAP);
 		spin_unlock(&amp;kvm-&gt;mmu_lock);
 		if (ret)
<span class="p_chunk">@@ -1146,12 +1159,13 @@</span> <span class="p_context"> static void  stage2_wp_puds(pgd_t *pgd, phys_addr_t addr, phys_addr_t end)</span>
  * @addr:	Start address of range
  * @end:	End address of range
  */
<span class="p_del">-static void stage2_wp_range(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)</span>
<span class="p_add">+static void stage2_wp_range(struct kvm *kvm, struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+			    phys_addr_t addr, phys_addr_t end)</span>
 {
 	pgd_t *pgd;
 	phys_addr_t next;
 
<span class="p_del">-	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="p_add">+	pgd = mmu-&gt;pgd + stage2_pgd_index(addr);</span>
 	do {
 		/*
 		 * Release kvm_mmu_lock periodically if the memory region is
<span class="p_chunk">@@ -1190,7 +1204,7 @@</span> <span class="p_context"> void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot)</span>
 	phys_addr_t end = (memslot-&gt;base_gfn + memslot-&gt;npages) &lt;&lt; PAGE_SHIFT;
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	stage2_wp_range(kvm, start, end);</span>
<span class="p_add">+	stage2_wp_range(kvm, &amp;kvm-&gt;arch.mmu, start, end);</span>
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	kvm_flush_remote_tlbs(kvm);
 }
<span class="p_chunk">@@ -1214,7 +1228,7 @@</span> <span class="p_context"> static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,</span>
 	phys_addr_t start = (base_gfn +  __ffs(mask)) &lt;&lt; PAGE_SHIFT;
 	phys_addr_t end = (base_gfn + __fls(mask) + 1) &lt;&lt; PAGE_SHIFT;
 
<span class="p_del">-	stage2_wp_range(kvm, start, end);</span>
<span class="p_add">+	stage2_wp_range(kvm, &amp;kvm-&gt;arch.mmu, start, end);</span>
 }
 
 /*
<span class="p_chunk">@@ -1253,6 +1267,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 	bool fault_ipa_uncached;
 	bool logging_active = memslot_is_logging(memslot);
 	unsigned long flags = 0;
<span class="p_add">+	struct kvm_s2_mmu *mmu = vcpu-&gt;arch.hw_mmu;</span>
 
 	write_fault = kvm_is_write_fault(vcpu);
 	if (fault_status == FSC_PERM &amp;&amp; !write_fault) {
<span class="p_chunk">@@ -1347,7 +1362,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 			kvm_set_pfn_dirty(pfn);
 		}
 		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);
<span class="p_del">-		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &amp;new_pmd);</span>
<span class="p_add">+		ret = stage2_set_pmd_huge(mmu, memcache, fault_ipa, &amp;new_pmd);</span>
 	} else {
 		pte_t new_pte = pfn_pte(pfn, mem_type);
 
<span class="p_chunk">@@ -1357,7 +1372,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 			mark_page_dirty(kvm, gfn);
 		}
 		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
<span class="p_del">-		ret = stage2_set_pte(kvm, memcache, fault_ipa, &amp;new_pte, flags);</span>
<span class="p_add">+		ret = stage2_set_pte(mmu, memcache, fault_ipa, &amp;new_pte, flags);</span>
 	}
 
 out_unlock:
<span class="p_chunk">@@ -1385,7 +1400,7 @@</span> <span class="p_context"> static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
 
 	spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
 
<span class="p_del">-	pmd = stage2_get_pmd(vcpu-&gt;kvm, NULL, fault_ipa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(vcpu-&gt;arch.hw_mmu, NULL, fault_ipa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		goto out;
 
<span class="p_chunk">@@ -1553,7 +1568,7 @@</span> <span class="p_context"> static int handle_hva_to_gpa(struct kvm *kvm,</span>
 
 static int kvm_unmap_hva_handler(struct kvm *kvm, gpa_t gpa, void *data)
 {
<span class="p_del">-	unmap_stage2_range(kvm, gpa, PAGE_SIZE);</span>
<span class="p_add">+	unmap_stage2_range(&amp;kvm-&gt;arch.mmu, gpa, PAGE_SIZE);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1561,7 +1576,7 @@</span> <span class="p_context"> int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)</span>
 {
 	unsigned long end = hva + PAGE_SIZE;
 
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return 0;
 
 	trace_kvm_unmap_hva(hva);
<span class="p_chunk">@@ -1572,7 +1587,7 @@</span> <span class="p_context"> int kvm_unmap_hva(struct kvm *kvm, unsigned long hva)</span>
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end)
 {
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return 0;
 
 	trace_kvm_unmap_hva_range(start, end);
<span class="p_chunk">@@ -1591,7 +1606,7 @@</span> <span class="p_context"> static int kvm_set_spte_handler(struct kvm *kvm, gpa_t gpa, void *data)</span>
 	 * therefore stage2_set_pte() never needs to clear out a huge PMD
 	 * through this calling path.
 	 */
<span class="p_del">-	stage2_set_pte(kvm, NULL, gpa, pte, 0);</span>
<span class="p_add">+	stage2_set_pte(&amp;kvm-&gt;arch.mmu, NULL, gpa, pte, 0);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1601,7 +1616,7 @@</span> <span class="p_context"> void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)</span>
 	unsigned long end = hva + PAGE_SIZE;
 	pte_t stage2_pte;
 
<span class="p_del">-	if (!kvm-&gt;arch.pgd)</span>
<span class="p_add">+	if (!kvm-&gt;arch.mmu.pgd)</span>
 		return;
 
 	trace_kvm_set_spte_hva(hva);
<span class="p_chunk">@@ -1614,7 +1629,7 @@</span> <span class="p_context"> static int kvm_age_hva_handler(struct kvm *kvm, gpa_t gpa, void *data)</span>
 	pmd_t *pmd;
 	pte_t *pte;
 
<span class="p_del">-	pmd = stage2_get_pmd(kvm, NULL, gpa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(&amp;kvm-&gt;arch.mmu, NULL, gpa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		return 0;
 
<span class="p_chunk">@@ -1633,7 +1648,7 @@</span> <span class="p_context"> static int kvm_test_age_hva_handler(struct kvm *kvm, gpa_t gpa, void *data)</span>
 	pmd_t *pmd;
 	pte_t *pte;
 
<span class="p_del">-	pmd = stage2_get_pmd(kvm, NULL, gpa);</span>
<span class="p_add">+	pmd = stage2_get_pmd(&amp;kvm-&gt;arch.mmu, NULL, gpa);</span>
 	if (!pmd || pmd_none(*pmd))	/* Nothing there */
 		return 0;
 
<span class="p_chunk">@@ -1864,9 +1879,10 @@</span> <span class="p_context"> int kvm_arch_prepare_memory_region(struct kvm *kvm,</span>
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
 	if (ret)
<span class="p_del">-		unmap_stage2_range(kvm, mem-&gt;guest_phys_addr, mem-&gt;memory_size);</span>
<span class="p_add">+		unmap_stage2_range(&amp;kvm-&gt;arch.mmu, mem-&gt;guest_phys_addr,</span>
<span class="p_add">+				   mem-&gt;memory_size);</span>
 	else
<span class="p_del">-		stage2_flush_memslot(kvm, memslot);</span>
<span class="p_add">+		stage2_flush_memslot(&amp;kvm-&gt;arch.mmu, memslot);</span>
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 	return ret;
 }
<span class="p_chunk">@@ -1907,7 +1923,7 @@</span> <span class="p_context"> void kvm_arch_flush_shadow_memslot(struct kvm *kvm,</span>
 	phys_addr_t size = slot-&gt;npages &lt;&lt; PAGE_SHIFT;
 
 	spin_lock(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	unmap_stage2_range(kvm, gpa, size);</span>
<span class="p_add">+	unmap_stage2_range(&amp;kvm-&gt;arch.mmu, gpa, size);</span>
 	spin_unlock(&amp;kvm-&gt;mmu_lock);
 }
 
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">index ec3553eb..ed8139f 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;
<span class="p_add">+struct kvm_s2_mmu;</span>
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
<span class="p_chunk">@@ -52,9 +53,9 @@</span> <span class="p_context"></span>
 extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
<span class="p_del">-extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);</span>
<span class="p_del">-extern void __kvm_tlb_flush_vmid(struct kvm *kvm);</span>
<span class="p_del">-extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa);</span>
<span class="p_add">+extern void __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu);</span>
<span class="p_add">+extern void __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">index ed78d73..954d6de 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -50,7 +50,7 @@</span> <span class="p_context"></span>
 int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
<span class="p_del">-struct kvm_arch {</span>
<span class="p_add">+struct kvm_s2_mmu {</span>
 	/* The VMID generation used for the virt. memory system */
 	u64    vmid_gen;
 	u32    vmid;
<span class="p_chunk">@@ -61,6 +61,11 @@</span> <span class="p_context"> struct kvm_arch {</span>
 
 	/* VTTBR value associated with above pgd and vmid */
 	u64    vttbr;
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct kvm_arch {</span>
<span class="p_add">+	/* Stage 2 paging state for the VM */</span>
<span class="p_add">+	struct kvm_s2_mmu mmu;</span>
 
 	/* The last vcpu id that ran on each physical CPU */
 	int __percpu *last_vcpu_ran;
<span class="p_chunk">@@ -326,6 +331,9 @@</span> <span class="p_context"> struct kvm_vcpu_arch {</span>
 
 	/* Detect first run of a vcpu */
 	bool has_run_once;
<span class="p_add">+</span>
<span class="p_add">+	/* Stage 2 paging state used by the hardware on next switch */</span>
<span class="p_add">+	struct kvm_s2_mmu *hw_mmu;</span>
 };
 
 #define vcpu_gp_regs(v)		(&amp;(v)-&gt;arch.ctxt.gp_regs)
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">index b7c8c30..3207009a 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/switch.c</span>
<span class="p_chunk">@@ -135,8 +135,9 @@</span> <span class="p_context"> static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)</span>
 
 static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(vcpu-&gt;kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	struct kvm_s2_mmu *mmu = kern_hyp_va(vcpu-&gt;arch.hw_mmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 }
 
 static void __hyp_text __deactivate_vm(struct kvm_vcpu *vcpu)
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">index 88e2f2b..71a62ea 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -17,13 +17,14 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/kvm_hyp.h&gt;
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu,</span>
<span class="p_add">+					 phys_addr_t ipa)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 	isb();
 
 	/*
<span class="p_chunk">@@ -48,13 +49,13 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)</span>
 	write_sysreg(0, vttbr_el2);
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_vmid(struct kvm_s2_mmu *mmu)</span>
 {
 	dsb(ishst);
 
 	/* Switch to requested VMID */
<span class="p_del">-	kvm = kern_hyp_va(kvm);</span>
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 	isb();
 
 	asm volatile(&quot;tlbi vmalls12e1is&quot; : : );
<span class="p_chunk">@@ -64,12 +65,11 @@</span> <span class="p_context"> void __hyp_text __kvm_tlb_flush_vmid(struct kvm *kvm)</span>
 	write_sysreg(0, vttbr_el2);
 }
 
<span class="p_del">-void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+void __hyp_text __kvm_tlb_flush_local_vmid(struct kvm_s2_mmu *mmu)</span>
 {
<span class="p_del">-	struct kvm *kvm = kern_hyp_va(kern_hyp_va(vcpu)-&gt;kvm);</span>
<span class="p_del">-</span>
 	/* Switch to requested VMID */
<span class="p_del">-	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	mmu = kern_hyp_va(mmu);</span>
<span class="p_add">+	write_sysreg(mmu-&gt;vttbr, vttbr_el2);</span>
 	isb();
 
 	asm volatile(&quot;tlbi vmalle1&quot; : : );

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



