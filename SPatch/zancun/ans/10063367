
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,03/16] iommu: introduce iommu invalidate API function - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,03/16] iommu: introduce iommu invalidate API function</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7040">Jacob Pan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 17, 2017, 6:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1510944914-54430-4-git-send-email-jacob.jun.pan@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10063367/mbox/"
   >mbox</a>
|
   <a href="/patch/10063367/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10063367/">/patch/10063367/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	56D0D601D3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:54:35 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5014B2AD80
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:54:35 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 44D732AD88; Fri, 17 Nov 2017 18:54:35 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8BDD62AD87
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Nov 2017 18:54:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S966254AbdKQSyb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Nov 2017 13:54:31 -0500
Received: from mga03.intel.com ([134.134.136.65]:54137 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S933221AbdKQSyX (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Nov 2017 13:54:23 -0500
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga103.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	17 Nov 2017 10:54:15 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.44,410,1505804400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;3461166&quot;
Received: from jacob-builder.jf.intel.com ([10.7.199.155])
	by orsmga003.jf.intel.com with ESMTP; 17 Nov 2017 10:54:14 -0800
From: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;
To: iommu@lists.linux-foundation.org, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Joerg Roedel &lt;joro@8bytes.org&gt;, David Woodhouse &lt;dwmw2@infradead.org&gt;,
	Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	Rafael Wysocki &lt;rafael.j.wysocki@intel.com&gt;,
	Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: &quot;Liu, Yi L&quot; &lt;yi.l.liu@intel.com&gt;, Lan Tianyu &lt;tianyu.lan@intel.com&gt;,
	&quot;Tian, Kevin&quot; &lt;kevin.tian@intel.com&gt;, Raj Ashok &lt;ashok.raj@intel.com&gt;,
	Jean Delvare &lt;khali@linux-fr.org&gt;,
	&quot;Christoph Hellwig&quot; &lt;hch@infradead.org&gt;,
	&quot;Liu, Yi L&quot; &lt;yi.l.liu@linux.intel.com&gt;, Liu@vger.kernel.org,
	Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;
Subject: [PATCH v3 03/16] iommu: introduce iommu invalidate API function
Date: Fri, 17 Nov 2017 10:55:01 -0800
Message-Id: &lt;1510944914-54430-4-git-send-email-jacob.jun.pan@linux.intel.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1510944914-54430-1-git-send-email-jacob.jun.pan@linux.intel.com&gt;
References: &lt;1510944914-54430-1-git-send-email-jacob.jun.pan@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7040">Jacob Pan</a> - Nov. 17, 2017, 6:55 p.m.</div>
<pre class="content">
<span class="from">From: &quot;Liu, Yi L&quot; &lt;yi.l.liu@linux.intel.com&gt;</span>

When an SVM capable device is assigned to a guest, the first level page
tables are owned by the guest and the guest PASID table pointer is
linked to the device context entry of the physical IOMMU.

Host IOMMU driver has no knowledge of caching structure updates unless
the guest invalidation activities are passed down to the host. The
primary usage is derived from emulated IOMMU in the guest, where QEMU
can trap invalidation activities before passing them down to the
host/physical IOMMU.
Since the invalidation data are obtained from user space and will be
written into physical IOMMU, we must allow security check at various
layers. Therefore, generic invalidation data format are proposed here,
model specific IOMMU drivers need to convert them into their own format.
<span class="signed-off-by">
Signed-off-by: Liu, Yi L &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Ashok Raj &lt;ashok.raj@intel.com&gt;</span>
---
 drivers/iommu/iommu.c      | 14 +++++++++++
 include/linux/iommu.h      | 12 +++++++++
 include/uapi/linux/iommu.h | 62 ++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 88 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=97951">Jean-Philippe Brucker</a> - Nov. 24, 2017, 12:04 p.m.</div>
<pre class="content">
Hi,

On 17/11/17 18:55, Jacob Pan wrote:
<span class="quote">&gt; From: &quot;Liu, Yi L&quot; &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When an SVM capable device is assigned to a guest, the first level page</span>
<span class="quote">&gt; tables are owned by the guest and the guest PASID table pointer is</span>
<span class="quote">&gt; linked to the device context entry of the physical IOMMU.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Host IOMMU driver has no knowledge of caching structure updates unless</span>
<span class="quote">&gt; the guest invalidation activities are passed down to the host. The</span>
<span class="quote">&gt; primary usage is derived from emulated IOMMU in the guest, where QEMU</span>
<span class="quote">&gt; can trap invalidation activities before passing them down to the</span>
<span class="quote">&gt; host/physical IOMMU.</span>
<span class="quote">&gt; Since the invalidation data are obtained from user space and will be</span>
<span class="quote">&gt; written into physical IOMMU, we must allow security check at various</span>
<span class="quote">&gt; layers. Therefore, generic invalidation data format are proposed here,</span>
<span class="quote">&gt; model specific IOMMU drivers need to convert them into their own format.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Liu, Yi L &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Ashok Raj &lt;ashok.raj@intel.com&gt;</span>
[...]
<span class="quote">&gt;  #endif /* __LINUX_IOMMU_H */</span>
<span class="quote">&gt; diff --git a/include/uapi/linux/iommu.h b/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; index 651ad5d..039ba36 100644</span>
<span class="quote">&gt; --- a/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; +++ b/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; @@ -36,4 +36,66 @@ struct pasid_table_config {</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +enum iommu_inv_granularity {</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_GLOBAL,		/* all TLBs invalidated */</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_DOMAIN,		/* all TLBs associated with a domain */</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_DEVICE,		/* caching structure associated with a</span>
<span class="quote">&gt; +					 * device ID</span>
<span class="quote">&gt; +					 */</span>

I thought you were planning on removing these? If we do need global
invalidation, for example the guest clears the whole PASID table and
doesn&#39;t want to send individual GRANU_ALL_PASID invalidations, maybe keep
only GRANU_DOMAIN?
<span class="quote">
&gt; +	IOMMU_INV_GRANU_DOMAIN_PAGE,	/* address range with a domain */</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_ALL_PASID,	/* cache of a given PASID */</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_PASID_SEL,	/* only invalidate specified PASID */</span>

GRANU_PASID_SEL seems redundant, don&#39;t you already get it by default with
GRANU_ALL_PASID and GRANU_DOMAIN_PAGE (with IOMMU_INVALIDATE_PASID_TAGGED
flag)?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_NG_ALL_PASID,	/* non-global within all PASIDs */</span>
<span class="quote">&gt; +	IOMMU_INV_GRANU_NG_PASID,	/* non-global within a PASIDs */</span>

Don&#39;t you get the &quot;NG&quot; behavior by not passing the
IOMMU_INVALIDATE_GLOBAL_PAGE flag defined below?
<span class="quote">
&gt; +	IOMMU_INV_GRANU_PAGE_PASID,	/* page-selective within a PASID */</span>

And don&#39;t you get this with GRANU_DOMAIN_PAGE+IOMMU_INVALIDATE_PASID_TAGGED?
<span class="quote">
&gt; +	IOMMU_INV_NR_GRANU,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +enum iommu_inv_type {</span>
<span class="quote">&gt; +	IOMMU_INV_TYPE_DTLB,	/* device IOTLB */</span>
<span class="quote">&gt; +	IOMMU_INV_TYPE_TLB,	/* IOMMU paging structure cache */</span>
<span class="quote">&gt; +	IOMMU_INV_TYPE_PASID,	/* PASID cache */</span>
<span class="quote">&gt; +	IOMMU_INV_TYPE_CONTEXT,	/* device context entry cache */</span>
<span class="quote">&gt; +	IOMMU_INV_NR_TYPE</span>
<span class="quote">&gt; +};</span>

When the guest removes a PASID entry, it would have to send DTLB, TLB and
PASID invalidations separately? Could we define this inv_type as
cumulative, to avoid redundant invalidation requests:

* TYPE_DTLB only invalidates ATC entries.
* TYPE_TLB invalidates both ATC and IOTLB entries.
* TYPE_PASID invalidates all ATC and IOTLB entries for a PASID, and also
the PASID cache entry.
* TYPE_CONTEXT invalidates all. Although is it needed by userspace or just
here fore completeness? &quot;CONTEXT&quot; is specific to VT-d (doesn&#39;t exist on
AMD and has a different meaning on SMMU), how about &quot;DEVICE&quot; instead?

This is important because invalidation will probably become the
bottleneck. The guest shouldn&#39;t have to send DTLB and TLB invalidation
separately after each unmapping.
<span class="quote">
&gt; +/**</span>
<span class="quote">&gt; + * Translation cache invalidation header that contains mandatory meta data.</span>
<span class="quote">&gt; + * @version:	info format version, expecting future extesions</span>
<span class="quote">&gt; + * @type:	type of translation cache to be invalidated</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct tlb_invalidate_hdr {</span>
<span class="quote">&gt; +	__u32 version;</span>
<span class="quote">&gt; +#define TLB_INV_HDR_VERSION_1 1</span>
<span class="quote">&gt; +	enum iommu_inv_type type;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * Translation cache invalidation information, contains generic IOMMU</span>
<span class="quote">&gt; + * data which can be parsed based on model ID by model specific drivers.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @granularity:	requested invalidation granularity, type dependent</span>
<span class="quote">&gt; + * @size:		2^size of 4K pages, 0 for 4k, 9 for 2MB, etc.</span>

Having only power of two invalidation seems too restrictive for a software
interface. You might have the same problem as above, where the guest or
userspace needs to send lots of invalidation requests, They could be
multiplexed by passing an arbitrary range instead. How about making @size
a __u64?
<span class="quote">
&gt; + * @pasid:		processor address space ID value per PCI spec.</span>
<span class="quote">&gt; + * @addr:		page address to be invalidated</span>
<span class="quote">&gt; + * @flags	IOMMU_INVALIDATE_PASID_TAGGED: DMA with PASID tagged,</span>
<span class="quote">&gt; + *						@pasid validity can be</span>
<span class="quote">&gt; + *						deduced from @granularity</span>

What&#39;s the use for this PASID_TAGGED flag if it doesn&#39;t define the @pasid
validity?
<span class="quote">
&gt; + *		IOMMU_INVALIDATE_ADDR_LEAF: leaf paging entries</span>

LEAF could be reused for multi-level PASID tables, when your first-level
table is already in place and you install a leaf entry, so maybe this
could be:

&quot;IOMMU_INVALIDATE_LEAF: only invalidate leaf table entry&quot;

Thanks,
Jean
<span class="quote">
&gt; + *		IOMMU_INVALIDATE_GLOBAL_PAGE: global pages&gt; + *</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct tlb_invalidate_info {</span>
<span class="quote">&gt; +	struct tlb_invalidate_hdr	hdr;</span>
<span class="quote">&gt; +	enum iommu_inv_granularity	granularity;</span>
<span class="quote">&gt; +	__u32		flags;</span>
<span class="quote">&gt; +#define IOMMU_INVALIDATE_NO_PASID	(1 &lt;&lt; 0)</span>
<span class="quote">&gt; +#define IOMMU_INVALIDATE_ADDR_LEAF	(1 &lt;&lt; 1)</span>
<span class="quote">&gt; +#define IOMMU_INVALIDATE_GLOBAL_PAGE	(1 &lt;&lt; 2)</span>
<span class="quote">&gt; +#define IOMMU_INVALIDATE_PASID_TAGGED	(1 &lt;&lt; 3)</span>
<span class="quote">&gt; +	__u8		size;</span>
<span class="quote">&gt; +	__u32		pasid;</span>
<span class="quote">&gt; +	__u64		addr;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt;  #endif /* _UAPI_IOMMU_H */</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=97951">Jean-Philippe Brucker</a> - Dec. 15, 2017, 7:02 p.m.</div>
<pre class="content">
A quick update on invalidations before I leave for holidays, since we&#39;re
struggling to define useful semantics. I worked on the virtio-iommu
prototype for vSVA, so I tried to pin down what I think is needed for vSVA
invalidation in the host. I don&#39;t know whether the VT-d and AMD emulations
can translate all of this from guest commands.

Scope selects which entries are invalidated, and flags cherry-pick what
caches to invalidate. For example a guest might remove GBs of sparse
mappings, and decide that it would be quicker to invalidate the whole
context instead of one at a time. Then it would set only flags = (TLB |
DEV_TLB) with scope = PASID. If the guest clears one entry in the PASID
table, then it would send scope = PASID and flags = (LEAF | CONFIG | TLB |
DEV_TLB). On an ARM system the guest can invalidate TLBs with CPU
instructions, but can&#39;t invalidate ATCs. So it would send an invalidate
with flags = (LEAF | TLB) and scope = VA.

enum iommu_sva_inval_scope {
	IOMMU_INVALIDATE_DOMAIN	= 1,
	IOMMU_INVALIDATE_PASID,
	IOMMU_INVALIDATE_VA,
};

/* Only invalidate leaf entry. Applies to PASID table if scope == PASID or
 * page tables if scope == VA. */
#define IOMMU_INVALIDATE_LEAF		(1 &lt;&lt; 0)
/* Invalidate cached PASID table configuration */
#define IOMMU_INVALIDATE_CONFIG		(1 &lt;&lt; 1)
/* Invalidate IOTLBs */
#define IOMMU_INVALIDATE_TLB		(1 &lt;&lt; 2)
/* Invalidate ATCs */
#define IOMMU_INVALIDATE_DEV_TLB	(1 &lt;&lt; 3)
/* + Need a global flag? */

struct iommu_sva_invalidate {
	enum iommu_sva_inval_scope	scope;
	u32				flags;
	u32				pasid;
	u64				iova;
	u64				size;
	/* Arch-specific, format is determined at bind time */
	union {
		struct {
			u16		asid;
			u8		granule;
		} arm;
	}
};

ARM needs two more fields. A 16-bit @asid (Address Space ID) targets TLB
entries and may be different from the PASID (up to the guest to decide),
which targets ATC and config entries.

@granule is the TLB granule that we&#39;re invalidating. For instance if the
guest just unmapped a few 2M huge pages, it sets @granule to 21 bits, so
we issue less invalidation commands, since we only need to evict huge TLB
entries. I&#39;m not sure about other architecture but I&#39;d be surprised if
this wasn&#39;t more common. Should we move it to the common part?


int iommu_sva_invalidate(struct iommu_domain *domain,
			 struct iommu_sva_invalidate *inval);

And so the host driver implementation is roughly:
--------------------------------------------------------------------------
bool leaf	= flags &amp; IOMMU_INVALIDATE_LEAF;
bool config	= flags &amp; IOMMU_INVALIDATE_CONFIG;
bool tlb	= flags &amp; IOMMU_INVALIDATE_TLB;
bool atc	= flags &amp; IOMMU_INVALIDATE_DEV_TLB;

if (config) {
	switch (scope) {
	case IOMMU_INVALIDATE_PASID:
		inval_cached_pasid_entry(domain, pasid, leaf);
		break;
	case IOMMU_INVALIDATE_DOMAIN:
		inval_all_cached_pasid_entries(domain);
		break;
	default:
		return -EINVAL;
	}

	/* Wait for caches to be clean, then invalidate TLBs */
	sync_commands();
}

if (tlb) {
	switch (scope) {
	case IOMMU_INVALIDATE_VA:
		inval_tlb_entries(domain, asid, iova, size, granule,
				  leaf);
		break;
	case IOMMU_INVALIDATE_PASID:
		inval_all_tlb_entries_for_asid(domain, asid);
		break;
	case IOMMU_INVALIDATE_DOMAIN:
		inval_all_tlb_entries(domain);
		break;
	default:
		return -EINVAL;
	}

	/* Wait for TLBs to be clean, then invalidate ATCs. */
	sync_commands();
}

if (atc) {
	/* ATC invalidations are sent to all devices in the domain */
	switch (scope) {
	case IOMMU_INVALIDATE_VA:
		inval_atc_entries(domain, pasid, iova, size);
		break;
	case IOMMU_INVALIDATE_PASID:
		/* Covers the full address space */
		inval_all_atc_entries_for_pasid(domain, pasid);
		break;
	case IOMMU_INVALIDATE_DOMAIN:
		/* Set Global Invalidate */
		inval_all_atc_entries(domain);
		break;
	default:
		return -EINVAL;
	}

	sync_commands();
}

/* Then return to guest. */
--------------------------------------------------------------------------

I think this covers what we need and allows userspace or the guest to
gather multiple invalidations into a single request/ioctl.

I don&#39;t think per-device ATC invalidation is needed, but might be wrong.
According to ATS it is implicit when the guest resets the device (FLR) or
disables the ATS capability. Are there other use-cases than reset? I still
need to see how QEMU handles when a device is detached from a domain (e.g.
its device table entry set to invalid). Kvmtool has one VFIO container per
device so can simply unmap-all to clear caches and TLBs when this happens.

Hope this helps,
Jean
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7040">Jacob Pan</a> - Dec. 28, 2017, 7:25 p.m.</div>
<pre class="content">
On Fri, 24 Nov 2017 12:04:31 +0000
Jean-Philippe Brucker &lt;jean-philippe.brucker@arm.com&gt; wrote:
<span class="quote">
&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 17/11/17 18:55, Jacob Pan wrote:</span>
<span class="quote">&gt; &gt; From: &quot;Liu, Yi L&quot; &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When an SVM capable device is assigned to a guest, the first level</span>
<span class="quote">&gt; &gt; page tables are owned by the guest and the guest PASID table</span>
<span class="quote">&gt; &gt; pointer is linked to the device context entry of the physical IOMMU.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Host IOMMU driver has no knowledge of caching structure updates</span>
<span class="quote">&gt; &gt; unless the guest invalidation activities are passed down to the</span>
<span class="quote">&gt; &gt; host. The primary usage is derived from emulated IOMMU in the</span>
<span class="quote">&gt; &gt; guest, where QEMU can trap invalidation activities before passing</span>
<span class="quote">&gt; &gt; them down to the host/physical IOMMU.</span>
<span class="quote">&gt; &gt; Since the invalidation data are obtained from user space and will be</span>
<span class="quote">&gt; &gt; written into physical IOMMU, we must allow security check at various</span>
<span class="quote">&gt; &gt; layers. Therefore, generic invalidation data format are proposed</span>
<span class="quote">&gt; &gt; here, model specific IOMMU drivers need to convert them into their</span>
<span class="quote">&gt; &gt; own format.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Liu, Yi L &lt;yi.l.liu@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Jacob Pan &lt;jacob.jun.pan@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Ashok Raj &lt;ashok.raj@intel.com&gt;  </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt;  #endif /* __LINUX_IOMMU_H */</span>
<span class="quote">&gt; &gt; diff --git a/include/uapi/linux/iommu.h b/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; &gt; index 651ad5d..039ba36 100644</span>
<span class="quote">&gt; &gt; --- a/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; &gt; +++ b/include/uapi/linux/iommu.h</span>
<span class="quote">&gt; &gt; @@ -36,4 +36,66 @@ struct pasid_table_config {</span>
<span class="quote">&gt; &gt;  	};</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +enum iommu_inv_granularity {</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_GLOBAL,		/* all TLBs</span>
<span class="quote">&gt; &gt; invalidated */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_DOMAIN,		/* all TLBs</span>
<span class="quote">&gt; &gt; associated with a domain */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_DEVICE,		/* caching</span>
<span class="quote">&gt; &gt; structure associated with a</span>
<span class="quote">&gt; &gt; +					 * device ID</span>
<span class="quote">&gt; &gt; +					 */  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought you were planning on removing these? If we do need global</span>
<span class="quote">&gt; invalidation, for example the guest clears the whole PASID table and</span>
<span class="quote">&gt; doesn&#39;t want to send individual GRANU_ALL_PASID invalidations, maybe</span>
<span class="quote">&gt; keep only GRANU_DOMAIN?</span>
<span class="quote">&gt; </span>
yes, we can remove global and keep domain &amp; pasid.
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_DOMAIN_PAGE,	/* address range with</span>
<span class="quote">&gt; &gt; a domain */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_ALL_PASID,	/* cache of a given</span>
<span class="quote">&gt; &gt; PASID */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_PASID_SEL,	/* only invalidate</span>
<span class="quote">&gt; &gt; specified PASID */  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; GRANU_PASID_SEL seems redundant, don&#39;t you already get it by default</span>
<span class="quote">&gt; with GRANU_ALL_PASID and GRANU_DOMAIN_PAGE (with</span>
<span class="quote">&gt; IOMMU_INVALIDATE_PASID_TAGGED flag)?</span>
<span class="quote">&gt; </span>
yes, you can deduce from certain combinations of flags. My thinking
was for an easy look up from generic flags to model specific
fields. Same as the one below. I will try to consolidate based on your
input in the next version.
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_NG_ALL_PASID,	/* non-global within</span>
<span class="quote">&gt; &gt; all PASIDs */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_NG_PASID,	/* non-global within a</span>
<span class="quote">&gt; &gt; PASIDs */  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Don&#39;t you get the &quot;NG&quot; behavior by not passing the</span>
<span class="quote">&gt; IOMMU_INVALIDATE_GLOBAL_PAGE flag defined below?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	IOMMU_INV_GRANU_PAGE_PASID,	/* page-selective</span>
<span class="quote">&gt; &gt; within a PASID */  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And don&#39;t you get this with</span>
<span class="quote">&gt; GRANU_DOMAIN_PAGE+IOMMU_INVALIDATE_PASID_TAGGED?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	IOMMU_INV_NR_GRANU,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +enum iommu_inv_type {</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_TYPE_DTLB,	/* device IOTLB */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_TYPE_TLB,	/* IOMMU paging structure cache</span>
<span class="quote">&gt; &gt; */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_TYPE_PASID,	/* PASID cache */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_TYPE_CONTEXT,	/* device context entry</span>
<span class="quote">&gt; &gt; cache */</span>
<span class="quote">&gt; &gt; +	IOMMU_INV_NR_TYPE</span>
<span class="quote">&gt; &gt; +};  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When the guest removes a PASID entry, it would have to send DTLB, TLB</span>
<span class="quote">&gt; and PASID invalidations separately? Could we define this inv_type as</span>
<span class="quote">&gt; cumulative, to avoid redundant invalidation requests:</span>
<span class="quote">&gt; </span>
That is a good idea, but it will require some change to VT-d driver.
For emulated IOMMU and current VT-d driver, we do send separate
requests for PASID cache, followed by IOTLB/DTLB invalidation. But we do
have a caching mode capability bit to tell the driver whether it is
running on a real IOMMU or not. So we can combine and reduce
invalidation overhead as you said below. Not sure about AMD though?
<span class="quote">
&gt; * TYPE_DTLB only invalidates ATC entries.</span>
<span class="quote">&gt; * TYPE_TLB invalidates both ATC and IOTLB entries.</span>
<span class="quote">&gt; * TYPE_PASID invalidates all ATC and IOTLB entries for a PASID, and</span>
<span class="quote">&gt; also the PASID cache entry.</span>
Sounds good to me.
<span class="quote">
&gt; * TYPE_CONTEXT invalidates all. Although is it needed by userspace or</span>
<span class="quote">&gt; just here fore completeness? &quot;CONTEXT&quot; is specific to VT-d (doesn&#39;t</span>
<span class="quote">&gt; exist on AMD and has a different meaning on SMMU), how about &quot;DEVICE&quot;</span>
<span class="quote">&gt; instead?</span>
It is here for completeness. context entry is set during bind/unbind
pasid table call. I can remove it for now.
<span class="quote">&gt; </span>
<span class="quote">&gt; This is important because invalidation will probably become the</span>
<span class="quote">&gt; bottleneck. The guest shouldn&#39;t have to send DTLB and TLB invalidation</span>
<span class="quote">&gt; separately after each unmapping.</span>
<span class="quote">&gt; </span>
Agreed, i will change the VT-d driver to accommodate that. i.e. For
emulated IOMMU (Caching Mode == 1), no need to send redundant
invalidation request.
<span class="quote">&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * Translation cache invalidation header that contains mandatory</span>
<span class="quote">&gt; &gt; meta data.</span>
<span class="quote">&gt; &gt; + * @version:	info format version, expecting future extesions</span>
<span class="quote">&gt; &gt; + * @type:	type of translation cache to be invalidated</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +struct tlb_invalidate_hdr {</span>
<span class="quote">&gt; &gt; +	__u32 version;</span>
<span class="quote">&gt; &gt; +#define TLB_INV_HDR_VERSION_1 1</span>
<span class="quote">&gt; &gt; +	enum iommu_inv_type type;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * Translation cache invalidation information, contains generic</span>
<span class="quote">&gt; &gt; IOMMU</span>
<span class="quote">&gt; &gt; + * data which can be parsed based on model ID by model specific</span>
<span class="quote">&gt; &gt; drivers.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @granularity:	requested invalidation granularity, type</span>
<span class="quote">&gt; &gt; dependent</span>
<span class="quote">&gt; &gt; + * @size:		2^size of 4K pages, 0 for 4k, 9 for 2MB,</span>
<span class="quote">&gt; &gt; etc.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having only power of two invalidation seems too restrictive for a</span>
<span class="quote">&gt; software interface. You might have the same problem as above, where</span>
<span class="quote">&gt; the guest or userspace needs to send lots of invalidation requests,</span>
<span class="quote">&gt; They could be multiplexed by passing an arbitrary range instead. How</span>
<span class="quote">&gt; about making @size a __u64?</span>
<span class="quote">&gt; </span>
Sure if you have such need for non power of two. So it will be __u64 of
4k pages?
<span class="quote">
&gt; &gt; + * @pasid:		processor address space ID value per PCI</span>
<span class="quote">&gt; &gt; spec.</span>
<span class="quote">&gt; &gt; + * @addr:		page address to be invalidated</span>
<span class="quote">&gt; &gt; + * @flags	IOMMU_INVALIDATE_PASID_TAGGED: DMA with PASID</span>
<span class="quote">&gt; &gt; tagged,</span>
<span class="quote">&gt; &gt; + *						@pasid validity</span>
<span class="quote">&gt; &gt; can be</span>
<span class="quote">&gt; &gt; + *						deduced from</span>
<span class="quote">&gt; &gt; @granularity  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What&#39;s the use for this PASID_TAGGED flag if it doesn&#39;t define the</span>
<span class="quote">&gt; @pasid validity?</span>
<span class="quote">&gt; </span>
VT-d uses different table format based on this PASID_TAGGED flag. With
PASID_TAGGED set, @pasid could still be invalid if the granularity is
not at PASID selective level.
<span class="quote">&gt; &gt; + *		IOMMU_INVALIDATE_ADDR_LEAF: leaf paging entries  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; LEAF could be reused for multi-level PASID tables, when your</span>
<span class="quote">&gt; first-level table is already in place and you install a leaf entry,</span>
<span class="quote">&gt; so maybe this could be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;IOMMU_INVALIDATE_LEAF: only invalidate leaf table entry&quot;</span>
<span class="quote">&gt; </span>
Sounds good. Assume we will only have 2 levels for the foreseeable
future.
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Jean</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; + *		IOMMU_INVALIDATE_GLOBAL_PAGE: global pages&gt; + *</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +struct tlb_invalidate_info {</span>
<span class="quote">&gt; &gt; +	struct tlb_invalidate_hdr	hdr;</span>
<span class="quote">&gt; &gt; +	enum iommu_inv_granularity	granularity;</span>
<span class="quote">&gt; &gt; +	__u32		flags;</span>
<span class="quote">&gt; &gt; +#define IOMMU_INVALIDATE_NO_PASID	(1 &lt;&lt; 0)</span>
<span class="quote">&gt; &gt; +#define IOMMU_INVALIDATE_ADDR_LEAF	(1 &lt;&lt; 1)</span>
<span class="quote">&gt; &gt; +#define IOMMU_INVALIDATE_GLOBAL_PAGE	(1 &lt;&lt; 2)</span>
<span class="quote">&gt; &gt; +#define IOMMU_INVALIDATE_PASID_TAGGED	(1 &lt;&lt; 3)</span>
<span class="quote">&gt; &gt; +	__u8		size;</span>
<span class="quote">&gt; &gt; +	__u32		pasid;</span>
<span class="quote">&gt; &gt; +	__u64		addr;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt;  #endif /* _UAPI_IOMMU_H */</span>
<span class="quote">&gt; &gt;   </span>
<span class="quote">&gt; </span>

[Jacob Pan]
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=97951">Jean-Philippe Brucker</a> - Jan. 10, 2018, noon</div>
<pre class="content">
On 28/12/17 19:25, Jacob Pan wrote:
[...]
<span class="quote">&gt;&gt;&gt; + * @size:		2^size of 4K pages, 0 for 4k, 9 for 2MB,</span>
<span class="quote">&gt;&gt;&gt; etc.  </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Having only power of two invalidation seems too restrictive for a</span>
<span class="quote">&gt;&gt; software interface. You might have the same problem as above, where</span>
<span class="quote">&gt;&gt; the guest or userspace needs to send lots of invalidation requests,</span>
<span class="quote">&gt;&gt; They could be multiplexed by passing an arbitrary range instead. How</span>
<span class="quote">&gt;&gt; about making @size a __u64?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; Sure if you have such need for non power of two. So it will be __u64 of</span>
<span class="quote">&gt; 4k pages?</span>

4k granule would work for us right now, but other architectures may plan
to support arbitrary sizes. The map/unmap API does support arbitrary
sizes, so it might be better to have a byte granularity in @size.

Thanks,
Jean
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c</span>
<span class="p_header">index c7e0d64..829e9e9 100644</span>
<span class="p_header">--- a/drivers/iommu/iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/iommu.c</span>
<span class="p_chunk">@@ -1341,6 +1341,20 @@</span> <span class="p_context"> void iommu_unbind_pasid_table(struct iommu_domain *domain, struct device *dev)</span>
 }
 EXPORT_SYMBOL_GPL(iommu_unbind_pasid_table);
 
<span class="p_add">+int iommu_sva_invalidate(struct iommu_domain *domain,</span>
<span class="p_add">+		struct device *dev, struct tlb_invalidate_info *inv_info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!domain-&gt;ops-&gt;sva_invalidate))</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = domain-&gt;ops-&gt;sva_invalidate(domain, dev, inv_info);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(iommu_sva_invalidate);</span>
<span class="p_add">+</span>
 static void __iommu_detach_device(struct iommu_domain *domain,
 				  struct device *dev)
 {
<span class="p_header">diff --git a/include/linux/iommu.h b/include/linux/iommu.h</span>
<span class="p_header">index 0f6f6c5..da684a7 100644</span>
<span class="p_header">--- a/include/linux/iommu.h</span>
<span class="p_header">+++ b/include/linux/iommu.h</span>
<span class="p_chunk">@@ -190,6 +190,7 @@</span> <span class="p_context"> struct iommu_resv_region {</span>
  * @pgsize_bitmap: bitmap of all possible supported page sizes
  * @bind_pasid_table: bind pasid table pointer for guest SVM
  * @unbind_pasid_table: unbind pasid table pointer and restore defaults
<span class="p_add">+ * @sva_invalidate: invalidate translation caches of shared virtual address</span>
  */
 struct iommu_ops {
 	bool (*capable)(enum iommu_cap);
<span class="p_chunk">@@ -243,6 +244,8 @@</span> <span class="p_context"> struct iommu_ops {</span>
 				struct pasid_table_config *pasidt_binfo);
 	void (*unbind_pasid_table)(struct iommu_domain *domain,
 				struct device *dev);
<span class="p_add">+	int (*sva_invalidate)(struct iommu_domain *domain,</span>
<span class="p_add">+		struct device *dev, struct tlb_invalidate_info *inv_info);</span>
 
 	unsigned long pgsize_bitmap;
 };
<span class="p_chunk">@@ -309,6 +312,9 @@</span> <span class="p_context"> extern int iommu_bind_pasid_table(struct iommu_domain *domain,</span>
 		struct device *dev, struct pasid_table_config *pasidt_binfo);
 extern void iommu_unbind_pasid_table(struct iommu_domain *domain,
 				struct device *dev);
<span class="p_add">+extern int iommu_sva_invalidate(struct iommu_domain *domain,</span>
<span class="p_add">+		struct device *dev, struct tlb_invalidate_info *inv_info);</span>
<span class="p_add">+</span>
 extern struct iommu_domain *iommu_get_domain_for_dev(struct device *dev);
 extern int iommu_map(struct iommu_domain *domain, unsigned long iova,
 		     phys_addr_t paddr, size_t size, int prot);
<span class="p_chunk">@@ -720,6 +726,12 @@</span> <span class="p_context"> void iommu_unbind_pasid_table(struct iommu_domain *domain, struct device *dev)</span>
 {
 }
 
<span class="p_add">+static inline int iommu_sva_invalidate(struct iommu_domain *domain,</span>
<span class="p_add">+		struct device *dev, struct tlb_invalidate_info *inv_info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return -EINVAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* CONFIG_IOMMU_API */
 
 #endif /* __LINUX_IOMMU_H */
<span class="p_header">diff --git a/include/uapi/linux/iommu.h b/include/uapi/linux/iommu.h</span>
<span class="p_header">index 651ad5d..039ba36 100644</span>
<span class="p_header">--- a/include/uapi/linux/iommu.h</span>
<span class="p_header">+++ b/include/uapi/linux/iommu.h</span>
<span class="p_chunk">@@ -36,4 +36,66 @@</span> <span class="p_context"> struct pasid_table_config {</span>
 	};
 };
 
<span class="p_add">+enum iommu_inv_granularity {</span>
<span class="p_add">+	IOMMU_INV_GRANU_GLOBAL,		/* all TLBs invalidated */</span>
<span class="p_add">+	IOMMU_INV_GRANU_DOMAIN,		/* all TLBs associated with a domain */</span>
<span class="p_add">+	IOMMU_INV_GRANU_DEVICE,		/* caching structure associated with a</span>
<span class="p_add">+					 * device ID</span>
<span class="p_add">+					 */</span>
<span class="p_add">+	IOMMU_INV_GRANU_DOMAIN_PAGE,	/* address range with a domain */</span>
<span class="p_add">+	IOMMU_INV_GRANU_ALL_PASID,	/* cache of a given PASID */</span>
<span class="p_add">+	IOMMU_INV_GRANU_PASID_SEL,	/* only invalidate specified PASID */</span>
<span class="p_add">+</span>
<span class="p_add">+	IOMMU_INV_GRANU_NG_ALL_PASID,	/* non-global within all PASIDs */</span>
<span class="p_add">+	IOMMU_INV_GRANU_NG_PASID,	/* non-global within a PASIDs */</span>
<span class="p_add">+	IOMMU_INV_GRANU_PAGE_PASID,	/* page-selective within a PASID */</span>
<span class="p_add">+	IOMMU_INV_NR_GRANU,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+enum iommu_inv_type {</span>
<span class="p_add">+	IOMMU_INV_TYPE_DTLB,	/* device IOTLB */</span>
<span class="p_add">+	IOMMU_INV_TYPE_TLB,	/* IOMMU paging structure cache */</span>
<span class="p_add">+	IOMMU_INV_TYPE_PASID,	/* PASID cache */</span>
<span class="p_add">+	IOMMU_INV_TYPE_CONTEXT,	/* device context entry cache */</span>
<span class="p_add">+	IOMMU_INV_NR_TYPE</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * Translation cache invalidation header that contains mandatory meta data.</span>
<span class="p_add">+ * @version:	info format version, expecting future extesions</span>
<span class="p_add">+ * @type:	type of translation cache to be invalidated</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct tlb_invalidate_hdr {</span>
<span class="p_add">+	__u32 version;</span>
<span class="p_add">+#define TLB_INV_HDR_VERSION_1 1</span>
<span class="p_add">+	enum iommu_inv_type type;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * Translation cache invalidation information, contains generic IOMMU</span>
<span class="p_add">+ * data which can be parsed based on model ID by model specific drivers.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @granularity:	requested invalidation granularity, type dependent</span>
<span class="p_add">+ * @size:		2^size of 4K pages, 0 for 4k, 9 for 2MB, etc.</span>
<span class="p_add">+ * @pasid:		processor address space ID value per PCI spec.</span>
<span class="p_add">+ * @addr:		page address to be invalidated</span>
<span class="p_add">+ * @flags	IOMMU_INVALIDATE_PASID_TAGGED: DMA with PASID tagged,</span>
<span class="p_add">+ *						@pasid validity can be</span>
<span class="p_add">+ *						deduced from @granularity</span>
<span class="p_add">+ *		IOMMU_INVALIDATE_ADDR_LEAF: leaf paging entries</span>
<span class="p_add">+ *		IOMMU_INVALIDATE_GLOBAL_PAGE: global pages</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct tlb_invalidate_info {</span>
<span class="p_add">+	struct tlb_invalidate_hdr	hdr;</span>
<span class="p_add">+	enum iommu_inv_granularity	granularity;</span>
<span class="p_add">+	__u32		flags;</span>
<span class="p_add">+#define IOMMU_INVALIDATE_NO_PASID	(1 &lt;&lt; 0)</span>
<span class="p_add">+#define IOMMU_INVALIDATE_ADDR_LEAF	(1 &lt;&lt; 1)</span>
<span class="p_add">+#define IOMMU_INVALIDATE_GLOBAL_PAGE	(1 &lt;&lt; 2)</span>
<span class="p_add">+#define IOMMU_INVALIDATE_PASID_TAGGED	(1 &lt;&lt; 3)</span>
<span class="p_add">+	__u8		size;</span>
<span class="p_add">+	__u32		pasid;</span>
<span class="p_add">+	__u64		addr;</span>
<span class="p_add">+};</span>
 #endif /* _UAPI_IOMMU_H */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



