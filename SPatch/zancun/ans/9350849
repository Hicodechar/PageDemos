
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v1,05/12] mm: thp: check pmd migration entry in common path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v1,05/12] mm: thp: check pmd migration entry in common path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 26, 2016, 3:22 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160926152234.14809-6-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9350849/mbox/"
   >mbox</a>
|
   <a href="/patch/9350849/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9350849/">/patch/9350849/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	052FF6077B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 26 Sep 2016 15:24:43 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EA18328D67
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 26 Sep 2016 15:24:42 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DE43228DA2; Mon, 26 Sep 2016 15:24:42 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D28D628D51
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 26 Sep 2016 15:24:40 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S941620AbcIZPYP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 26 Sep 2016 11:24:15 -0400
Received: from out2-smtp.messagingengine.com ([66.111.4.26]:53591 &quot;EHLO
	out2-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S941581AbcIZPYK (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 26 Sep 2016 11:24:10 -0400
Received: from compute7.internal (compute7.nyi.internal [10.202.2.47])
	by mailout.nyi.internal (Postfix) with ESMTP id 4ACC12067C;
	Mon, 26 Sep 2016 11:24:09 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute7.internal (MEProxy); Mon, 26 Sep 2016 11:24:09 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-sasl-enc:x-sasl-enc; s=mesmtp; bh=nxIx2bezOM9EjoRWVjqE1eWGKNQ
	=; b=snWK/lyfXE1ThY2SxM3nV1Jnr/2oov7edEd6eNATIJwROLrBN6m8TwZzMJS
	/j8sg0xiwoMJ2zlTLDjzmJLpaKWEGdXRwxyAfNZ8xnqLG9VbBifgh/S4IBP3/Ah6
	/I5ZnwdCCDIA7EIp9QEttBmMupmkkghnDYpPFDOPZh6kdni8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-sasl-enc:x-sasl-enc; s=smtpout; bh=nxIx
	2bezOM9EjoRWVjqE1eWGKNQ=; b=b1w2AHtNgmkGeQyl4R+LiY83qWZpM6D36avU
	apnzLPqEAL6FPCqNESfXlQi3Bto7VNUjiAmxwuzUG9k47XZ1aE0IHGQtPU2HktcP
	d3U6fsVqxXhoapS2UrjntWqt8z1vFFznLeEcp3z6uOBmSBWKuwCpVIjQ8HShFnJq
	k6NeIc8=
X-Sasl-enc: 48BCdJDdGTOT320r91oVPZbbHuoMfU/iLr4nomk9zRDS 1474903449
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id DAFECCCE92;
	Mon, 26 Sep 2016 11:24:08 -0400 (EDT)
From: zi.yan@sent.com
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: benh@kernel.crashing.org, mgorman@techsingularity.net,
	kirill.shutemov@linux.intel.com, akpm@linux-foundation.org,
	dave.hansen@linux.intel.com, n-horiguchi@ah.jp.nec.com,
	Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;
Subject: [PATCH v1 05/12] mm: thp: check pmd migration entry in common path
Date: Mon, 26 Sep 2016 11:22:27 -0400
Message-Id: &lt;20160926152234.14809-6-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;20160926152234.14809-1-zi.yan@sent.com&gt;
References: &lt;20160926152234.14809-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - Sept. 26, 2016, 3:22 p.m.</div>
<pre class="content">
<span class="from">From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

If one of callers of page migration starts to handle thp, memory management code
start to see pmd migration entry, so we need to prepare for it before enabling.
This patch changes various code point which checks the status of given pmds in
order to prevent race between thp migration and the pmd-related works.
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
---
 arch/x86/mm/gup.c  |  3 +++
 fs/proc/task_mmu.c | 20 +++++++-------
 mm/gup.c           |  8 ++++++
 mm/huge_memory.c   | 76 +++++++++++++++++++++++++++++++++++++++++++++++-------
 mm/memcontrol.c    |  2 ++
 mm/memory.c        |  5 ++++
 6 files changed, 95 insertions(+), 19 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
<span class="p_header">index b8b6a60..72d0bef 100644</span>
<span class="p_header">--- a/arch/x86/mm/gup.c</span>
<span class="p_header">+++ b/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/memremap.h&gt;
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_chunk">@@ -225,6 +226,8 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		if (pmd_none(pmd))
 			return 0;
 		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+				return 0;</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index f6fa99e..60f6ce3 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -931,6 +931,9 @@</span> <span class="p_context"> static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
 
 	ptl = pmd_trans_huge_lock(pmd, vma);
 	if (ptl) {
<span class="p_add">+		if (unlikely(is_pmd_migration_entry(*pmd)))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+</span>
 		if (cp-&gt;type == CLEAR_REFS_SOFT_DIRTY) {
 			clear_soft_dirty_pmd(vma, addr, pmd);
 			goto out;
<span class="p_chunk">@@ -1215,19 +1218,18 @@</span> <span class="p_context"> static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
 	if (ptl) {
 		u64 flags = 0, frame = 0;
 		pmd_t pmd = *pmdp;
<span class="p_add">+		struct page *page;</span>
 
 		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))
 			flags |= PM_SOFT_DIRTY;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Currently pmd for thp is always present because thp</span>
<span class="p_del">-		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="p_del">-		 * (split in such cases instead.)</span>
<span class="p_del">-		 * This if-check is just to prepare for future implementation.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (pmd_present(pmd)) {</span>
<span class="p_del">-			struct page *page = pmd_page(pmd);</span>
<span class="p_del">-</span>
<span class="p_add">+		if (is_pmd_migration_entry(pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+			frame = swp_type(entry) |</span>
<span class="p_add">+				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+		} else if (pmd_present(pmd)) {</span>
<span class="p_add">+			page = pmd_page(pmd);</span>
 			if (page_mapcount(page) == 1)
 				flags |= PM_MMAP_EXCLUSIVE;
 
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 96b2b2f..ef56be2 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -272,6 +272,11 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
 	}
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (flags &amp; FOLL_SPLIT) {
 		int ret;
 		page = pmd_page(*pmd);
<span class="p_chunk">@@ -1362,6 +1367,9 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 			return 0;
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 0cd39ef..f4fcfc7 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -787,6 +787,19 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		goto out_unlock;
 	}
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="p_add">+		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+			make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+			pmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	src_page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!PageHead(src_page), src_page);
 	get_page(src_page);
<span class="p_chunk">@@ -952,6 +965,9 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
 	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))
 		goto out_unlock;
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(*fe-&gt;pmd)))</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);
 	/*
<span class="p_chunk">@@ -1077,7 +1093,15 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		goto out;
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		if (!is_migration_entry(entry))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
<span class="p_add">+</span>
 	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);
 	if (flags &amp; FOLL_TOUCH)
 		touch_pmd(vma, addr, pmd);
<span class="p_chunk">@@ -1273,6 +1297,9 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	if (is_huge_zero_pmd(orig_pmd))
 		goto out;
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(orig_pmd)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	/*
 	 * If other processes are mapping this page, we couldn&#39;t discard
<span class="p_chunk">@@ -1348,21 +1375,40 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		tlb_remove_page(tlb, pmd_page(orig_pmd));
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_del">-		if (PageAnon(page)) {</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			if (PageAnon(page)) {</span>
<span class="p_add">+				pgtable_t pgtable;</span>
<span class="p_add">+				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_add">+				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			swp_entry_t entry;</span>
 			pgtable_t pgtable;
<span class="p_add">+</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="p_add">+</span>
<span class="p_add">+			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
 			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);
 			pte_free(tlb-&gt;mm, pgtable);
 			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+</span>
<span class="p_add">+			migration = 1;</span>
 		}
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -1445,6 +1491,11 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			return ret;
 		}
 
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			spin_unlock(ptl);</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (!prot_numa || !pmd_protnone(*pmd)) {
 			entry = pmdp_huge_get_and_clear_notify(mm, addr, pmd);
 			entry = pmd_modify(entry, newprot);
<span class="p_chunk">@@ -1656,6 +1707,11 @@</span> <span class="p_context"> void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 
 	if (pmd_trans_huge(*pmd)) {
 		page = pmd_page(*pmd);
<span class="p_add">+</span>
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (PageMlocked(page))
 			clear_page_mlock(page);
 	} else if (!pmd_devmap(*pmd))
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 4be518d..421ac4ff 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4649,6 +4649,8 @@</span> <span class="p_context"> static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 	struct page *page = NULL;
 	enum mc_target_type ret = MC_TARGET_NONE;
 
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd)))</span>
<span class="p_add">+		return ret;</span>
 	page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!page || !PageHead(page), page);
 	if (!(mc.flags &amp; MOVE_ANON))
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 83be99d..3ad3bb2 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -3590,6 +3590,11 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 
 		barrier();
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
<span class="p_add">+			if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="p_add">+				pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			if (pmd_protnone(orig_pmd))
 				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



