
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 3.16.45 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 3.16.45</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 3, 2017, 3:37 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1499096278.2707.15.camel@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9823645/mbox/"
   >mbox</a>
|
   <a href="/patch/9823645/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9823645/">/patch/9823645/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	59A4560246 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:20 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4351F285EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:20 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 349DF285F2; Mon,  3 Jul 2017 15:38:20 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-5.4 required=2.0 tests=BAYES_00, BODY_ENHANCEMENT2,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 02125285EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Jul 2017 15:38:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754417AbdGCPiP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 3 Jul 2017 11:38:15 -0400
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:50779 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752222AbdGCPiH (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 3 Jul 2017 11:38:07 -0400
Received: from 82-70-136-246.dsl.in-addr.zen.co.uk ([82.70.136.246]
	helo=deadeye) by shadbolt.decadent.org.uk with esmtps
	(TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256) (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dS3QF-0006Aa-FE; Mon, 03 Jul 2017 16:37:59 +0100
Received: from ben by deadeye with local (Exim 4.89)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dS3QE-0004tm-Lb; Mon, 03 Jul 2017 16:37:58 +0100
Message-ID: &lt;1499096278.2707.15.camel@decadent.org.uk&gt;
Subject: Linux 3.16.45
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, Jiri Slaby &lt;jslaby@suse.cz&gt;,
	stable@vger.kernel.org
Cc: lwn@lwn.net
Date: Mon, 03 Jul 2017 16:37:58 +0100
Content-Type: multipart/signed; micalg=&quot;pgp-sha512&quot;;
	protocol=&quot;application/pgp-signature&quot;;
	boundary=&quot;=-SO4gYKpFKkHHJamZraaY&quot;
X-Mailer: Evolution 3.22.6-1 
Mime-Version: 1.0
X-SA-Exim-Connect-IP: 82.70.136.246
X-SA-Exim-Mail-From: ben@decadent.org.uk
X-SA-Exim-Scanned: No (on shadbolt.decadent.org.uk);
	SAEximRunCond expanded to false
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - July 3, 2017, 3:37 p.m.</div>
<pre class="content">
I&#39;m announcing the release of the 3.16.45 kernel.

All users of the 3.16 kernel series should upgrade.

The updated 3.16.y git tree can be found at:
        https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git linux-3.16.y
and can be browsed at the normal kernel.org git web browser:
        https://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git

The diff from 3.16.44 is attached to this message.

Ben.

------------

 Documentation/kernel-parameters.txt     |   7 ++
 Makefile                                |   2 +-
 arch/arc/mm/mmap.c                      |   2 +-
 arch/arm/mm/mmap.c                      |   4 +-
 arch/frv/mm/elf-fdpic.c                 |   2 +-
 arch/mips/mm/mmap.c                     |   2 +-
 arch/parisc/kernel/sys_parisc.c         |  15 +--
 arch/powerpc/mm/slice.c                 |   2 +-
 arch/sh/mm/mmap.c                       |   4 +-
 arch/sparc/kernel/sys_sparc_64.c        |   4 +-
 arch/sparc/mm/hugetlbpage.c             |   2 +-
 arch/tile/mm/hugetlbpage.c              |   2 +-
 arch/x86/include/asm/kvm_emulate.h      |   1 +
 arch/x86/kernel/sys_x86_64.c            |   4 +-
 arch/x86/kvm/emulate.c                  |   1 +
 arch/x86/kvm/x86.c                      |  53 +++++------
 arch/x86/mm/hugetlbpage.c               |   2 +-
 arch/xtensa/kernel/syscall.c            |   2 +-
 drivers/gpu/drm/vmwgfx/vmwgfx_surface.c |   3 +
 drivers/regulator/core.c                |   2 +
 fs/hugetlbfs/inode.c                    |   2 +-
 fs/proc/task_mmu.c                      |   4 -
 include/linux/mm.h                      |  53 +++++------
 mm/gup.c                                |   5 -
 mm/memory.c                             |  38 --------
 mm/mmap.c                               | 160 +++++++++++++++++++-------------
 net/rxrpc/ar-key.c                      |  64 +++++++------
 27 files changed, 219 insertions(+), 223 deletions(-)

Ben Hutchings (1):
      Linux 3.16.45

David Howells (1):
      rxrpc: Fix several cases where a padded len isn&#39;t checked in ticket decode

Helge Deller (1):
      Allow stack to grow up to address space limit

Hugh Dickins (2):
      mm: larger stack guard gap, between vmas
      mm: fix new crash in unmapped_area_topdown()

Paolo Bonzini (1):
      KVM: x86: fix singlestepping over syscall

Seung-Woo Kim (1):
      regulator: core: Fix regualtor_ena_gpio_free not to access pin after freeing

Vladis Dronov (1):
      drm/vmwgfx: limit the number of mip levels in vmw_gb_surface_define_ioctl()
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index cb76a3cf0763..86c91be8647c 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -3154,6 +3154,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 1680e3f85a06..6effa4401a09 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 3
 PATCHLEVEL = 16
<span class="p_del">-SUBLEVEL = 44</span>
<span class="p_add">+SUBLEVEL = 45</span>
 EXTRAVERSION =
 NAME = Museum of Fishiegoodies
 
<span class="p_header">diff --git a/arch/arc/mm/mmap.c b/arch/arc/mm/mmap.c</span>
<span class="p_header">index 2e06d56e987b..cf4ae6958240 100644</span>
<span class="p_header">--- a/arch/arc/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arc/mm/mmap.c</span>
<span class="p_chunk">@@ -64,7 +64,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/arm/mm/mmap.c b/arch/arm/mm/mmap.c</span>
<span class="p_header">index 5e85ed371364..8f9d1cf505dd 100644</span>
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/frv/mm/elf-fdpic.c b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">index 836f14707a62..efa59f1f8022 100644</span>
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_header">diff --git a/arch/mips/mm/mmap.c b/arch/mips/mm/mmap.c</span>
<span class="p_header">index f1baadd56e82..9be924f08f34 100644</span>
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/parisc/kernel/sys_parisc.c b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">index 5aba01ac457f..4dda73c44fee 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_chunk">@@ -88,7 +88,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	unsigned long task_size = TASK_SIZE;
 	int do_color_align, last_mmap;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -115,9 +115,10 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		else
 			addr = PAGE_ALIGN(addr);
 
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			goto found_addr;
 	}
 
<span class="p_chunk">@@ -141,7 +142,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	int do_color_align, last_mmap;
<span class="p_chunk">@@ -175,9 +176,11 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			addr = COLOR_ALIGN(addr, last_mmap, pgoff);
 		else
 			addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			goto found_addr;
 	}
 
<span class="p_header">diff --git a/arch/powerpc/mm/slice.c b/arch/powerpc/mm/slice.c</span>
<span class="p_header">index b0c75cc15efc..7477de0e6e3c 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -103,7 +103,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_struct *mm, unsigned long addr,</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_header">diff --git a/arch/sh/mm/mmap.c b/arch/sh/mm/mmap.c</span>
<span class="p_header">index 6777177807c2..7df7d5944188 100644</span>
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/sparc/kernel/sys_sparc_64.c b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">index c690c8e16a96..7f0f7c01b297 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -118,7 +118,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,7 +181,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index d329537739c6..15034695b839 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/tile/mm/hugetlbpage.c b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">index e514899e1100..94c240634fc6 100644</span>
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -265,7 +265,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/x86/include/asm/kvm_emulate.h b/arch/x86/include/asm/kvm_emulate.h</span>
<span class="p_header">index a04fe4eb237d..c10f7175bef3 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kvm_emulate.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kvm_emulate.h</span>
<span class="p_chunk">@@ -274,6 +274,7 @@</span> <span class="p_context"> struct x86_emulate_ctxt {</span>
 	bool guest_mode; /* guest running a nested guest */
 	bool perm_ok; /* do not check permissions if true */
 	bool ud;	/* inject an #UD if host doesn&#39;t support insn */
<span class="p_add">+	bool tf;	/* TF value before instruction (after for syscall/sysret) */</span>
 
 	bool have_exception;
 	struct x86_exception exception;
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 30277e27431a..d050393d3be2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -127,7 +127,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c</span>
<span class="p_header">index 35f8884817fa..3ef118df2547 100644</span>
<span class="p_header">--- a/arch/x86/kvm/emulate.c</span>
<span class="p_header">+++ b/arch/x86/kvm/emulate.c</span>
<span class="p_chunk">@@ -2310,6 +2310,7 @@</span> <span class="p_context"> static int em_syscall(struct x86_emulate_ctxt *ctxt)</span>
 		ctxt-&gt;eflags &amp;= ~(EFLG_VM | EFLG_IF | EFLG_RF);
 	}
 
<span class="p_add">+	ctxt-&gt;tf = (ctxt-&gt;eflags &amp; X86_EFLAGS_TF) != 0;</span>
 	return X86EMUL_CONTINUE;
 }
 
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 7dcb958a8054..75bf438eb2e5 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -4942,6 +4942,8 @@</span> <span class="p_context"> static void init_emulate_ctxt(struct kvm_vcpu *vcpu)</span>
 	kvm_x86_ops-&gt;get_cs_db_l_bits(vcpu, &amp;cs_db, &amp;cs_l);
 
 	ctxt-&gt;eflags = kvm_get_rflags(vcpu);
<span class="p_add">+	ctxt-&gt;tf = (ctxt-&gt;eflags &amp; X86_EFLAGS_TF) != 0;</span>
<span class="p_add">+</span>
 	ctxt-&gt;eip = kvm_rip_read(vcpu);
 	ctxt-&gt;mode = (!is_protmode(vcpu))		? X86EMUL_MODE_REAL :
 		     (ctxt-&gt;eflags &amp; X86_EFLAGS_VM)	? X86EMUL_MODE_VM86 :
<span class="p_chunk">@@ -5132,38 +5134,26 @@</span> <span class="p_context"> static int kvm_vcpu_check_hw_bp(unsigned long addr, u32 type, u32 dr7,</span>
 	return dr6;
 }
 
<span class="p_del">-static void kvm_vcpu_check_singlestep(struct kvm_vcpu *vcpu, int *r)</span>
<span class="p_add">+static void kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu, int *r)</span>
 {
 	struct kvm_run *kvm_run = vcpu-&gt;run;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Use the &quot;raw&quot; value to see if TF was passed to the processor.</span>
<span class="p_del">-	 * Note that the new value of the flags has not been saved yet.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * This is correct even for TF set by the guest, because &quot;the</span>
<span class="p_del">-	 * processor will not generate this exception after the instruction</span>
<span class="p_del">-	 * that sets the TF flag&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	unsigned long rflags = kvm_x86_ops-&gt;get_rflags(vcpu);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(rflags &amp; X86_EFLAGS_TF)) {</span>
<span class="p_del">-		if (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_SINGLESTEP) {</span>
<span class="p_del">-			kvm_run-&gt;debug.arch.dr6 = DR6_BS | DR6_FIXED_1;</span>
<span class="p_del">-			kvm_run-&gt;debug.arch.pc = vcpu-&gt;arch.singlestep_rip;</span>
<span class="p_del">-			kvm_run-&gt;debug.arch.exception = DB_VECTOR;</span>
<span class="p_del">-			kvm_run-&gt;exit_reason = KVM_EXIT_DEBUG;</span>
<span class="p_del">-			*r = EMULATE_USER_EXIT;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			vcpu-&gt;arch.emulate_ctxt.eflags &amp;= ~X86_EFLAGS_TF;</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * &quot;Certain debug exceptions may clear bit 0-3.  The</span>
<span class="p_del">-			 * remaining contents of the DR6 register are never</span>
<span class="p_del">-			 * cleared by the processor&quot;.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			vcpu-&gt;arch.dr6 &amp;= ~15;</span>
<span class="p_del">-			vcpu-&gt;arch.dr6 |= DR6_BS;</span>
<span class="p_del">-			kvm_queue_exception(vcpu, DB_VECTOR);</span>
<span class="p_del">-		}</span>
<span class="p_add">+	if (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_SINGLESTEP) {</span>
<span class="p_add">+		kvm_run-&gt;debug.arch.dr6 = DR6_BS | DR6_FIXED_1;</span>
<span class="p_add">+		kvm_run-&gt;debug.arch.pc = vcpu-&gt;arch.singlestep_rip;</span>
<span class="p_add">+		kvm_run-&gt;debug.arch.exception = DB_VECTOR;</span>
<span class="p_add">+		kvm_run-&gt;exit_reason = KVM_EXIT_DEBUG;</span>
<span class="p_add">+		*r = EMULATE_USER_EXIT;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		vcpu-&gt;arch.emulate_ctxt.eflags &amp;= ~X86_EFLAGS_TF;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * &quot;Certain debug exceptions may clear bit 0-3.  The</span>
<span class="p_add">+		 * remaining contents of the DR6 register are never</span>
<span class="p_add">+		 * cleared by the processor&quot;.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		vcpu-&gt;arch.dr6 &amp;= ~15;</span>
<span class="p_add">+		vcpu-&gt;arch.dr6 |= DR6_BS;</span>
<span class="p_add">+		kvm_queue_exception(vcpu, DB_VECTOR);</span>
 	}
 }
 
<span class="p_chunk">@@ -5316,8 +5306,9 @@</span> <span class="p_context"> int x86_emulate_instruction(struct kvm_vcpu *vcpu,</span>
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		vcpu-&gt;arch.emulate_regs_need_sync_to_vcpu = false;
 		kvm_rip_write(vcpu, ctxt-&gt;eip);
<span class="p_del">-		if (r == EMULATE_DONE)</span>
<span class="p_del">-			kvm_vcpu_check_singlestep(vcpu, &amp;r);</span>
<span class="p_add">+		if (r == EMULATE_DONE &amp;&amp;</span>
<span class="p_add">+		    (ctxt-&gt;tf || (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_SINGLESTEP)))</span>
<span class="p_add">+			kvm_vcpu_do_singlestep(vcpu, &amp;r);</span>
 		kvm_set_rflags(vcpu, ctxt-&gt;eflags);
 	} else
 		vcpu-&gt;arch.emulate_regs_need_sync_to_vcpu = true;
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 006cc914994b..fd277c4dd2e0 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -156,7 +156,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/xtensa/kernel/syscall.c b/arch/xtensa/kernel/syscall.c</span>
<span class="p_header">index 5d3f7a119ed1..1ff0b92eeae7 100644</span>
<span class="p_header">--- a/arch/xtensa/kernel/syscall.c</span>
<span class="p_header">+++ b/arch/xtensa/kernel/syscall.c</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c b/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c</span>
<span class="p_header">index 67623797bd55..14551e27814c 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_surface.c</span>
<span class="p_chunk">@@ -1251,6 +1251,9 @@</span> <span class="p_context"> int vmw_gb_surface_define_ioctl(struct drm_device *dev, void *data,</span>
 	const struct svga3d_surface_desc *desc;
 	uint32_t backup_handle;
 
<span class="p_add">+	if (req-&gt;mip_levels &gt; DRM_VMW_MAX_MIP_LEVELS)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	if (unlikely(vmw_user_surface_size == 0))
 		vmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +
 			128;
<span class="p_header">diff --git a/drivers/regulator/core.c b/drivers/regulator/core.c</span>
<span class="p_header">index a2e836478549..71fb2037e0b9 100644</span>
<span class="p_header">--- a/drivers/regulator/core.c</span>
<span class="p_header">+++ b/drivers/regulator/core.c</span>
<span class="p_chunk">@@ -1709,6 +1709,8 @@</span> <span class="p_context"> static void regulator_ena_gpio_free(struct regulator_dev *rdev)</span>
 				gpio_free(pin-&gt;gpio);
 				list_del(&amp;pin-&gt;list);
 				kfree(pin);
<span class="p_add">+				rdev-&gt;ena_pin = NULL;</span>
<span class="p_add">+				return;</span>
 			} else {
 				pin-&gt;request_count--;
 			}
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index e3ac491424fc..3a4af9d401c3 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -171,7 +171,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 5825f6d944ce..fb073b9fe25e 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -273,11 +273,7 @@</span> <span class="p_context"> show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_setwidth(m, 25 + sizeof(void *) * 6 - 1);
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu &quot;,
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index e648c24e8410..35f8b59b37a7 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1241,34 +1241,6 @@</span> <span class="p_context"> int set_page_dirty_lock(struct page *page);</span>
 int clear_page_dirty_for_io(struct page *page);
 int get_cmdline(struct task_struct *task, char *buffer, int buflen);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern pid_t
 vm_is_stack(struct task_struct *task, struct vm_area_struct *vma, int in_group);
 
<span class="p_chunk">@@ -1914,6 +1886,7 @@</span> <span class="p_context"> void page_cache_async_readahead(struct address_space *mapping,</span>
 
 unsigned long max_sane_readahead(unsigned long nr);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1942,6 +1915,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * m</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 2e4b9873b5aa..4d2fd27b33ab 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -266,11 +266,6 @@</span> <span class="p_context"> static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,</span>
 	unsigned int fault_flags = 0;
 	int ret;
 
<span class="p_del">-	/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-	if ((*flags &amp; FOLL_MLOCK) &amp;&amp;</span>
<span class="p_del">-			(stack_guard_page_start(vma, address) ||</span>
<span class="p_del">-			 stack_guard_page_end(vma, address + PAGE_SIZE)))</span>
<span class="p_del">-		return -ENOENT;</span>
 	if (*flags &amp; FOLL_WRITE)
 		fault_flags |= FAULT_FLAG_WRITE;
 	if (nonblocking)
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 4e03447b1ef2..ea9698d1b865 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2589,40 +2589,6 @@</span> <span class="p_context"> static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -2641,10 +2607,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index b49641901093..03b34261aa08 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -266,6 +266,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 	bool populate;
 
<span class="p_chunk">@@ -311,7 +312,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -334,10 +336,22 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 
 static long vma_compute_subtree_gap(struct vm_area_struct *vma)
 {
<span class="p_del">-	unsigned long max, subtree_gap;</span>
<span class="p_del">-	max = vma-&gt;vm_start;</span>
<span class="p_del">-	if (vma-&gt;vm_prev)</span>
<span class="p_del">-		max -= vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_add">+	unsigned long max, prev_end, subtree_gap;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we</span>
<span class="p_add">+	 * allow two stack_guard_gaps between them here, and when choosing</span>
<span class="p_add">+	 * an unmapped area; whereas when expanding we only require one.</span>
<span class="p_add">+	 * That&#39;s a little inconsistent, but keeps the code here simpler.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max = vm_start_gap(vma);</span>
<span class="p_add">+	if (vma-&gt;vm_prev) {</span>
<span class="p_add">+		prev_end = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+		if (max &gt; prev_end)</span>
<span class="p_add">+			max -= prev_end;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			max = 0;</span>
<span class="p_add">+	}</span>
 	if (vma-&gt;vm_rb.rb_left) {
 		subtree_gap = rb_entry(vma-&gt;vm_rb.rb_left,
 				struct vm_area_struct, vm_rb)-&gt;rb_subtree_gap;
<span class="p_chunk">@@ -426,7 +440,7 @@</span> <span class="p_context"> static void validate_mm(struct mm_struct *mm)</span>
 			anon_vma_unlock_read(anon_vma);
 		}
 
<span class="p_del">-		highest_address = vma-&gt;vm_end;</span>
<span class="p_add">+		highest_address = vm_end_gap(vma);</span>
 		vma = vma-&gt;vm_next;
 		i++;
 	}
<span class="p_chunk">@@ -594,7 +608,7 @@</span> <span class="p_context"> void __vma_link_rb(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_next)
 		vma_gap_update(vma-&gt;vm_next);
 	else
<span class="p_del">-		mm-&gt;highest_vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 
 	/*
 	 * vma-&gt;vm_prev wasn&#39;t known when we followed the rbtree to find the
<span class="p_chunk">@@ -846,7 +860,7 @@</span> <span class="p_context"> again:			remove_next = 1 + (end &gt; next-&gt;vm_end);</span>
 			vma_gap_update(vma);
 		if (end_changed) {
 			if (!next)
<span class="p_del">-				mm-&gt;highest_vm_end = end;</span>
<span class="p_add">+				mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 			else if (!adjust_next)
 				vma_gap_update(next);
 		}
<span class="p_chunk">@@ -889,7 +903,7 @@</span> <span class="p_context"> again:			remove_next = 1 + (end &gt; next-&gt;vm_end);</span>
 		else if (next)
 			vma_gap_update(next);
 		else
<span class="p_del">-			mm-&gt;highest_vm_end = end;</span>
<span class="p_add">+			VM_WARN_ON(mm-&gt;highest_vm_end != vm_end_gap(vma));</span>
 	}
 	if (insert &amp;&amp; file)
 		uprobe_mmap(insert);
<span class="p_chunk">@@ -1702,7 +1716,7 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_unmapped_area_info *info)</span>
 
 	while (true) {
 		/* Visit left subtree if it looks promising */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &gt;= low_limit &amp;&amp; vma-&gt;vm_rb.rb_left) {
 			struct vm_area_struct *left =
 				rb_entry(vma-&gt;vm_rb.rb_left,
<span class="p_chunk">@@ -1713,12 +1727,13 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_unmapped_area_info *info)</span>
 			}
 		}
 
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start &gt; high_limit)
 			return -ENOMEM;
<span class="p_del">-		if (gap_end &gt;= low_limit &amp;&amp; gap_end - gap_start &gt;= length)</span>
<span class="p_add">+		if (gap_end &gt;= low_limit &amp;&amp;</span>
<span class="p_add">+		    gap_end &gt; gap_start &amp;&amp; gap_end - gap_start &gt;= length)</span>
 			goto found;
 
 		/* Visit right subtree if it looks promising */
<span class="p_chunk">@@ -1740,8 +1755,8 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_unmapped_area_info *info)</span>
 			vma = rb_entry(rb_parent(prev),
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_left) {
<span class="p_del">-				gap_start = vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_del">-				gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+				gap_start = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+				gap_end = vm_start_gap(vma);</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1805,7 +1820,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)</span>
 
 	while (true) {
 		/* Visit right subtree if it looks promising */
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 		if (gap_start &lt;= high_limit &amp;&amp; vma-&gt;vm_rb.rb_right) {
 			struct vm_area_struct *right =
 				rb_entry(vma-&gt;vm_rb.rb_right,
<span class="p_chunk">@@ -1818,10 +1833,11 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)</span>
 
 check_current:
 		/* Check if current node has a suitable gap */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &lt; low_limit)
 			return -ENOMEM;
<span class="p_del">-		if (gap_start &lt;= high_limit &amp;&amp; gap_end - gap_start &gt;= length)</span>
<span class="p_add">+		if (gap_start &lt;= high_limit &amp;&amp;</span>
<span class="p_add">+		    gap_end &gt; gap_start &amp;&amp; gap_end - gap_start &gt;= length)</span>
 			goto found;
 
 		/* Visit left subtree if it looks promising */
<span class="p_chunk">@@ -1844,7 +1860,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)</span>
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_right) {
 				gap_start = vma-&gt;vm_prev ?
<span class="p_del">-					vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+					vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1882,7 +1898,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct vm_unmapped_area_info info;
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
<span class="p_chunk">@@ -1893,9 +1909,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1918,7 +1935,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -1933,9 +1950,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -2061,21 +2079,19 @@</span> <span class="p_context"> find_vma_prev(struct mm_struct *mm, unsigned long addr,</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -2116,16 +2132,32 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, uns</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error = 0;
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
<span class="p_del">-	/* Guard against wrapping around to address 0. */</span>
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	/* Guard against exceeding limits of the address space. */</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (address &gt;= TASK_SIZE)</span>
 		return -ENOMEM;
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Guard against overflow */</span>
<span class="p_add">+	if (gap_addr &lt; address || gap_addr &gt; TASK_SIZE)</span>
<span class="p_add">+		gap_addr = TASK_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
 
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
<span class="p_chunk">@@ -2167,7 +2199,7 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 				if (vma-&gt;vm_next)
 					vma_gap_update(vma-&gt;vm_next);
 				else
<span class="p_del">-					vma-&gt;vm_mm-&gt;highest_vm_end = address;</span>
<span class="p_add">+					vma-&gt;vm_mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 				spin_unlock(&amp;vma-&gt;vm_mm-&gt;page_table_lock);
 
 				perf_event_mmap(vma);
<span class="p_chunk">@@ -2187,6 +2219,8 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
 	address &amp;= PAGE_MASK;
<span class="p_chunk">@@ -2194,6 +2228,17 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 	if (error)
 		return error;
 
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_chunk">@@ -2245,28 +2290,25 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 	return error;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Note how expand_stack() refuses to expand the stack all the way to</span>
<span class="p_del">- * abut the next virtual mapping, *unless* that mapping itself is also</span>
<span class="p_del">- * a stack mapping. We want to leave room for a guard page, after all</span>
<span class="p_del">- * (the guard page itself is not added here, that is done by the</span>
<span class="p_del">- * actual page faulting logic)</span>
<span class="p_del">- *</span>
<span class="p_del">- * This matches the behavior of the guard page logic (see mm/memory.c:</span>
<span class="p_del">- * check_stack_guard_page()), which only allows the guard page to be</span>
<span class="p_del">- * removed under these circumstances.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *next;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	next = vma-&gt;vm_next;</span>
<span class="p_del">-	if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE) {</span>
<span class="p_del">-		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_upwards(vma, address);
 }
 
<span class="p_chunk">@@ -2288,14 +2330,6 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 #else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *prev;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	prev = vma-&gt;vm_prev;</span>
<span class="p_del">-	if (prev &amp;&amp; prev-&gt;vm_end == address) {</span>
<span class="p_del">-		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_downwards(vma, address);
 }
 
<span class="p_chunk">@@ -2391,7 +2425,7 @@</span> <span class="p_context"> detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		vma-&gt;vm_prev = prev;
 		vma_gap_update(vma);
 	} else
<span class="p_del">-		mm-&gt;highest_vm_end = prev ? prev-&gt;vm_end : 0;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = prev ? vm_end_gap(prev) : 0;</span>
 	tail_vma-&gt;vm_next = NULL;
 
 	/* Kill the cache */
<span class="p_header">diff --git a/net/rxrpc/ar-key.c b/net/rxrpc/ar-key.c</span>
<span class="p_header">index 0ad080790a32..99d5a5ff812e 100644</span>
<span class="p_header">--- a/net/rxrpc/ar-key.c</span>
<span class="p_header">+++ b/net/rxrpc/ar-key.c</span>
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 				       unsigned int *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned int toklen = *_toklen, n_parts, loop, tmp;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, n_parts, loop, tmp, paddedlen;</span>
 
 	/* there must be at least one name, and at least #names+1 length
 	 * words */
<span class="p_chunk">@@ -243,16 +243,16 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 		toklen -= 4;
 		if (tmp &lt;= 0 || tmp &gt; AFSTOKEN_STRING_MAX)
 			return -EINVAL;
<span class="p_del">-		if (tmp &gt; toklen)</span>
<span class="p_add">+		paddedlen = (tmp + 3) &amp; ~3;</span>
<span class="p_add">+		if (paddedlen &gt; toklen)</span>
 			return -EINVAL;
 		princ-&gt;name_parts[loop] = kmalloc(tmp + 1, GFP_KERNEL);
 		if (!princ-&gt;name_parts[loop])
 			return -ENOMEM;
 		memcpy(princ-&gt;name_parts[loop], xdr, tmp);
 		princ-&gt;name_parts[loop][tmp] = 0;
<span class="p_del">-		tmp = (tmp + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= tmp;</span>
<span class="p_del">-		xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	if (toklen &lt; 4)
<span class="p_chunk">@@ -261,16 +261,16 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_principal(struct krb5_principal *princ,</span>
 	toklen -= 4;
 	if (tmp &lt;= 0 || tmp &gt; AFSTOKEN_K5_REALM_MAX)
 		return -EINVAL;
<span class="p_del">-	if (tmp &gt; toklen)</span>
<span class="p_add">+	paddedlen = (tmp + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
 		return -EINVAL;
 	princ-&gt;realm = kmalloc(tmp + 1, GFP_KERNEL);
 	if (!princ-&gt;realm)
 		return -ENOMEM;
 	memcpy(princ-&gt;realm, xdr, tmp);
 	princ-&gt;realm[tmp] = 0;
<span class="p_del">-	tmp = (tmp + 3) &amp; ~3;</span>
<span class="p_del">-	toklen -= tmp;</span>
<span class="p_del">-	xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+	toklen -= paddedlen;</span>
<span class="p_add">+	xdr += paddedlen &gt;&gt; 2;</span>
 
 	_debug(&quot;%s/...@%s&quot;, princ-&gt;name_parts[0], princ-&gt;realm);
 
<span class="p_chunk">@@ -289,7 +289,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_tagged_data(struct krb5_tagged_data *td,</span>
 					 unsigned int *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned int toklen = *_toklen, len;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, len, paddedlen;</span>
 
 	/* there must be at least one tag and one length word */
 	if (toklen &lt;= 8)
<span class="p_chunk">@@ -303,15 +303,17 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_tagged_data(struct krb5_tagged_data *td,</span>
 	toklen -= 8;
 	if (len &gt; max_data_size)
 		return -EINVAL;
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
<span class="p_add">+		return -EINVAL;</span>
 	td-&gt;data_len = len;
 
 	if (len &gt; 0) {
 		td-&gt;data = kmemdup(xdr, len, GFP_KERNEL);
 		if (!td-&gt;data)
 			return -ENOMEM;
<span class="p_del">-		len = (len + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= len;</span>
<span class="p_del">-		xdr += len &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	_debug(&quot;tag %x len %x&quot;, td-&gt;tag, td-&gt;data_len);
<span class="p_chunk">@@ -383,7 +385,7 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 				    const __be32 **_xdr, unsigned int *_toklen)
 {
 	const __be32 *xdr = *_xdr;
<span class="p_del">-	unsigned int toklen = *_toklen, len;</span>
<span class="p_add">+	unsigned int toklen = *_toklen, len, paddedlen;</span>
 
 	/* there must be at least one length word */
 	if (toklen &lt;= 4)
<span class="p_chunk">@@ -395,6 +397,9 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 	toklen -= 4;
 	if (len &gt; AFSTOKEN_K5_TIX_MAX)
 		return -EINVAL;
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; toklen)</span>
<span class="p_add">+		return -EINVAL;</span>
 	*_tktlen = len;
 
 	_debug(&quot;ticket len %u&quot;, len);
<span class="p_chunk">@@ -403,9 +408,8 @@</span> <span class="p_context"> static int rxrpc_krb5_decode_ticket(u8 **_ticket, u16 *_tktlen,</span>
 		*_ticket = kmemdup(xdr, len, GFP_KERNEL);
 		if (!*_ticket)
 			return -ENOMEM;
<span class="p_del">-		len = (len + 3) &amp; ~3;</span>
<span class="p_del">-		toklen -= len;</span>
<span class="p_del">-		xdr += len &gt;&gt; 2;</span>
<span class="p_add">+		toklen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 	}
 
 	*_xdr = xdr;
<span class="p_chunk">@@ -549,7 +553,7 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 {
 	const __be32 *xdr = data, *token;
 	const char *cp;
<span class="p_del">-	unsigned int len, tmp, loop, ntoken, toklen, sec_ix;</span>
<span class="p_add">+	unsigned int len, paddedlen, loop, ntoken, toklen, sec_ix;</span>
 	int ret;
 
 	_enter(&quot;,{%x,%x,%x,%x},%zu&quot;,
<span class="p_chunk">@@ -574,22 +578,21 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 	if (len &lt; 1 || len &gt; AFSTOKEN_CELL_MAX)
 		goto not_xdr;
 	datalen -= 4;
<span class="p_del">-	tmp = (len + 3) &amp; ~3;</span>
<span class="p_del">-	if (tmp &gt; datalen)</span>
<span class="p_add">+	paddedlen = (len + 3) &amp; ~3;</span>
<span class="p_add">+	if (paddedlen &gt; datalen)</span>
 		goto not_xdr;
 
 	cp = (const char *) xdr;
 	for (loop = 0; loop &lt; len; loop++)
 		if (!isprint(cp[loop]))
 			goto not_xdr;
<span class="p_del">-	if (len &lt; tmp)</span>
<span class="p_del">-		for (; loop &lt; tmp; loop++)</span>
<span class="p_del">-			if (cp[loop])</span>
<span class="p_del">-				goto not_xdr;</span>
<span class="p_add">+	for (; loop &lt; paddedlen; loop++)</span>
<span class="p_add">+		if (cp[loop])</span>
<span class="p_add">+			goto not_xdr;</span>
 	_debug(&quot;cellname: [%u/%u] &#39;%*.*s&#39;&quot;,
<span class="p_del">-	       len, tmp, len, len, (const char *) xdr);</span>
<span class="p_del">-	datalen -= tmp;</span>
<span class="p_del">-	xdr += tmp &gt;&gt; 2;</span>
<span class="p_add">+	       len, paddedlen, len, len, (const char *) xdr);</span>
<span class="p_add">+	datalen -= paddedlen;</span>
<span class="p_add">+	xdr += paddedlen &gt;&gt; 2;</span>
 
 	/* get the token count */
 	if (datalen &lt; 12)
<span class="p_chunk">@@ -610,10 +613,11 @@</span> <span class="p_context"> static int rxrpc_instantiate_xdr(struct key *key, const void *data, size_t datal</span>
 		sec_ix = ntohl(*xdr);
 		datalen -= 4;
 		_debug(&quot;token: [%x/%zx] %x&quot;, toklen, datalen, sec_ix);
<span class="p_del">-		if (toklen &lt; 20 || toklen &gt; datalen)</span>
<span class="p_add">+		paddedlen = (toklen + 3) &amp; ~3;</span>
<span class="p_add">+		if (toklen &lt; 20 || toklen &gt; datalen || paddedlen &gt; datalen)</span>
 			goto not_xdr;
<span class="p_del">-		datalen -= (toklen + 3) &amp; ~3;</span>
<span class="p_del">-		xdr += (toklen + 3) &gt;&gt; 2;</span>
<span class="p_add">+		datalen -= paddedlen;</span>
<span class="p_add">+		xdr += paddedlen &gt;&gt; 2;</span>
 
 	} while (--loop &gt; 0);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



