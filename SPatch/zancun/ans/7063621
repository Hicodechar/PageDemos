
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[07/12] mm, page_alloc: Distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [07/12] mm, page_alloc: Distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 24, 2015, 12:09 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1440418191-10894-8-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7063621/mbox/"
   >mbox</a>
|
   <a href="/patch/7063621/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7063621/">/patch/7063621/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 2B02F9F344
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Aug 2015 12:12:02 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 65A50205F3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Aug 2015 12:11:57 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 66FDA20711
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Aug 2015 12:11:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754506AbbHXMLb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 24 Aug 2015 08:11:31 -0400
Received: from outbound-smtp02.blacknight.com ([81.17.249.8]:33770 &quot;EHLO
	outbound-smtp02.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1754119AbbHXMJz (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 24 Aug 2015 08:09:55 -0400
Received: from mail.blacknight.com (pemlinmail02.blacknight.ie
	[81.17.254.11])
	by outbound-smtp02.blacknight.com (Postfix) with ESMTPS id 24E6D990DE
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 24 Aug 2015 12:09:54 +0000 (UTC)
Received: (qmail 26576 invoked from network); 24 Aug 2015 12:09:53 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.230.90])
	by 81.17.254.9 with ESMTPA; 24 Aug 2015 12:09:53 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, David Rientjes &lt;rientjes@google.com&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	Linux-MM &lt;linux-mm@kvack.org&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 07/12] mm,
	page_alloc: Distinguish between being unable to sleep,
	unwilling to sleep and avoiding waking kswapd
Date: Mon, 24 Aug 2015 13:09:46 +0100
Message-Id: &lt;1440418191-10894-8-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.4.6
In-Reply-To: &lt;1440418191-10894-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1440418191-10894-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Aug. 24, 2015, 12:09 p.m.</div>
<pre class="content">
__GFP_WAIT has been used to identify atomic context in callers that hold
spinlocks or are in interrupts. They are expected to be high priority and
have access one of two watermarks lower than &quot;min&quot; which can be referred
to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower
watermark and can be called the &quot;high priority reserve&quot;.

Over time, callers had a requirement to not block when fallback options
were available. Some have abused __GFP_WAIT leading to a situation where
an optimisitic allocation with a fallback option can access atomic reserves.

This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
cannot sleep and have no alternative. High priority users continue to use
__GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are
willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers
that want to wake kswapd for background reclaim. __GFP_WAIT is redefined
as a caller that is willing to enter direct reclaim and wake kswapd for
background reclaim.

This patch then converts a number of sites

o __GFP_ATOMIC is used by callers that are high priority and have memory
  pools for those requests. GFP_ATOMIC uses this flag.

o Callers that have a limited mempool to guarantee forward progress use
  __GFP_DIRECT_RECLAIM. bio allocations fall into this category where
  kswapd will still be woken but atomic reserves are not used as there
  is a one-entry mempool to guarantee progress.

o Callers that are checking if they are non-blocking should use the
  helper gfpflags_allow_blocking() where possible. This is because
  checking for __GFP_WAIT as was done historically now can trigger false
  positives. Some exceptions like dm-crypt.c exist where the code intent
  is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
  flag manipulations.

o Callers that built their own GFP flags instead of starting with GFP_KERNEL
  and friends now also need to specify __GFP_KSWAPD_RECLAIM.

The first key hazard to watch out for is callers that removed __GFP_WAIT
and was depending on access to atomic reserves for inconspicuous reasons.
In some cases it may be appropriate for them to use __GFP_HIGH.

The second key hazard is callers that assembled their own combination of
GFP flags instead of starting with something like GFP_KERNEL. They may
now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless
if it&#39;s missed in most cases as other activity will wake kswapd.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
---
 Documentation/vm/balance                           | 14 ++++---
 arch/arm/mm/dma-mapping.c                          |  4 +-
 arch/arm/xen/mm.c                                  |  2 +-
 arch/arm64/mm/dma-mapping.c                        |  4 +-
 arch/x86/kernel/pci-dma.c                          |  2 +-
 block/bio.c                                        | 26 ++++++------
 block/blk-core.c                                   | 16 ++++----
 block/blk-ioc.c                                    |  2 +-
 block/blk-mq-tag.c                                 |  2 +-
 block/blk-mq.c                                     |  8 ++--
 block/cfq-iosched.c                                |  4 +-
 drivers/block/drbd/drbd_receiver.c                 |  3 +-
 drivers/block/osdblk.c                             |  2 +-
 drivers/connector/connector.c                      |  3 +-
 drivers/firewire/core-cdev.c                       |  2 +-
 drivers/gpu/drm/i915/i915_gem.c                    |  2 +-
 drivers/infiniband/core/sa_query.c                 |  2 +-
 drivers/iommu/amd_iommu.c                          |  2 +-
 drivers/iommu/intel-iommu.c                        |  2 +-
 drivers/md/dm-crypt.c                              |  6 +--
 drivers/md/dm-kcopyd.c                             |  2 +-
 drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c     |  2 +-
 drivers/media/pci/solo6x10/solo6x10-v4l2.c         |  2 +-
 drivers/media/pci/tw68/tw68-video.c                |  2 +-
 drivers/mtd/mtdcore.c                              |  3 +-
 drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c    |  2 +-
 drivers/staging/android/ion/ion_system_heap.c      |  2 +-
 .../lustre/include/linux/libcfs/libcfs_private.h   |  2 +-
 drivers/usb/host/u132-hcd.c                        |  2 +-
 drivers/video/fbdev/vermilion/vermilion.c          |  2 +-
 fs/btrfs/disk-io.c                                 |  2 +-
 fs/btrfs/extent_io.c                               | 14 +++----
 fs/btrfs/volumes.c                                 |  4 +-
 fs/ext3/super.c                                    |  2 +-
 fs/ext4/super.c                                    |  2 +-
 fs/fscache/cookie.c                                |  2 +-
 fs/fscache/page.c                                  |  6 +--
 fs/jbd/transaction.c                               |  4 +-
 fs/jbd2/transaction.c                              |  4 +-
 fs/nfs/file.c                                      |  6 +--
 fs/xfs/xfs_qm.c                                    |  2 +-
 include/linux/gfp.h                                | 46 ++++++++++++++++------
 include/linux/skbuff.h                             |  6 +--
 include/net/sock.h                                 |  2 +-
 include/trace/events/gfpflags.h                    |  5 ++-
 kernel/audit.c                                     |  6 +--
 kernel/locking/lockdep.c                           |  2 +-
 kernel/power/snapshot.c                            |  2 +-
 kernel/smp.c                                       |  2 +-
 lib/idr.c                                          |  4 +-
 lib/radix-tree.c                                   | 10 ++---
 mm/backing-dev.c                                   |  2 +-
 mm/dmapool.c                                       |  2 +-
 mm/memcontrol.c                                    |  8 ++--
 mm/mempool.c                                       | 10 ++---
 mm/migrate.c                                       |  2 +-
 mm/page_alloc.c                                    | 43 ++++++++++++--------
 mm/slab.c                                          | 18 ++++-----
 mm/slub.c                                          |  6 +--
 mm/vmalloc.c                                       |  2 +-
 mm/vmscan.c                                        |  4 +-
 mm/zswap.c                                         |  5 ++-
 net/core/skbuff.c                                  |  8 ++--
 net/core/sock.c                                    |  6 ++-
 net/netlink/af_netlink.c                           |  2 +-
 net/rxrpc/ar-connection.c                          |  2 +-
 net/sctp/associola.c                               |  2 +-
 67 files changed, 211 insertions(+), 173 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Aug. 25, 2015, 3:37 p.m.</div>
<pre class="content">
On 08/24/2015 02:09 PM, Mel Gorman wrote:
<span class="quote">&gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; background reclaim.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch then converts a number of sites</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o __GFP_ATOMIC is used by callers that are high priority and have memory</span>
<span class="quote">&gt;    pools for those requests. GFP_ATOMIC uses this flag.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that have a limited mempool to guarantee forward progress use</span>
<span class="quote">&gt;    __GFP_DIRECT_RECLAIM. bio allocations fall into this category where</span>

      ^ __GFP_KSWAPD_RECLAIM ? (missed it previously)
<span class="quote">
&gt;    kswapd will still be woken but atomic reserves are not used as there</span>
<span class="quote">&gt;    is a one-entry mempool to guarantee progress.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that are checking if they are non-blocking should use the</span>
<span class="quote">&gt;    helper gfpflags_allow_blocking() where possible. This is because</span>
<span class="quote">&gt;    checking for __GFP_WAIT as was done historically now can trigger false</span>
<span class="quote">&gt;    positives. Some exceptions like dm-crypt.c exist where the code intent</span>
<span class="quote">&gt;    is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to</span>
<span class="quote">&gt;    flag manipulations.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that built their own GFP flags instead of starting with GFP_KERNEL</span>
<span class="quote">&gt;    and friends now also need to specify __GFP_KSWAPD_RECLAIM.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; In some cases it may be appropriate for them to use __GFP_HIGH.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The second key hazard is callers that assembled their own combination of</span>
<span class="quote">&gt; GFP flags instead of starting with something like GFP_KERNEL. They may</span>
<span class="quote">&gt; now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless</span>
<span class="quote">&gt; if it&#39;s missed in most cases as other activity will wake kswapd.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>

Thanks for the effort!
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>

Just last few bits:
<span class="quote">
&gt; @@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;   		return false;</span>
<span class="quote">&gt;   	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt;   		return false;</span>
<span class="quote">&gt; -	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt;   		return false;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>

IIUC ignore_gfp_wait tells it to assume that reclaimers will eventually 
succeed (for some reason?), so they shouldn&#39;t fail. Probably to focus 
the testing on atomic allocations. But your change makes atomic 
allocation never fail, so that goes against the knob IMHO?
<span class="quote">
&gt; @@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;   		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt;   		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt;   			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; -	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt;   		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   	if (fmt) {</span>

This caught me previously and I convinced myself that it&#39;s OK, but now 
I&#39;m not anymore. IIUC this is to not filter nodes by mems_allowed during 
printing, if the allocation itself wasn&#39;t limited? In that case it 
should probably only look at __GFP_ATOMIC after this patch? As that&#39;s 
the only thing that determines ALLOC_CPUSET.
I don&#39;t know where in_interrupt() comes from, but it was probably 
considered in the past, as can be seen in zlc_setup()?

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Aug. 25, 2015, 3:48 p.m.</div>
<pre class="content">
On 08/24/2015 02:09 PM, Mel Gorman wrote:
<span class="quote">&gt; The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; In some cases it may be appropriate for them to use __GFP_HIGH.</span>

Hm so I think this hazard should be expanded. If such caller comes from 
interrupt and doesn&#39;t use __GFP_ATOMIC, the ALLOC_CPUSET with 
restrictions taken from the interrupted process will also apply to him?

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 26, 2015, 1:05 p.m.</div>
<pre class="content">
On Mon 24-08-15 13:09:46, Mel Gorman wrote:
<span class="quote">&gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; background reclaim.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch then converts a number of sites</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o __GFP_ATOMIC is used by callers that are high priority and have memory</span>
<span class="quote">&gt;   pools for those requests. GFP_ATOMIC uses this flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that have a limited mempool to guarantee forward progress use</span>
<span class="quote">&gt;   __GFP_DIRECT_RECLAIM. bio allocations fall into this category where</span>
<span class="quote">&gt;   kswapd will still be woken but atomic reserves are not used as there</span>
<span class="quote">&gt;   is a one-entry mempool to guarantee progress.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that are checking if they are non-blocking should use the</span>
<span class="quote">&gt;   helper gfpflags_allow_blocking() where possible. This is because</span>
<span class="quote">&gt;   checking for __GFP_WAIT as was done historically now can trigger false</span>
<span class="quote">&gt;   positives. Some exceptions like dm-crypt.c exist where the code intent</span>
<span class="quote">&gt;   is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to</span>
<span class="quote">&gt;   flag manipulations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that built their own GFP flags instead of starting with GFP_KERNEL</span>
<span class="quote">&gt;   and friends now also need to specify __GFP_KSWAPD_RECLAIM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; In some cases it may be appropriate for them to use __GFP_HIGH.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The second key hazard is callers that assembled their own combination of</span>
<span class="quote">&gt; GFP flags instead of starting with something like GFP_KERNEL. They may</span>
<span class="quote">&gt; now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless</span>
<span class="quote">&gt; if it&#39;s missed in most cases as other activity will wake kswapd.</span>

JFYI mmotm tree has
https://git.kernel.org/cgit/linux/kernel/git/mhocko/mm.git/commit/?h=since-4.1&amp;id=5f467d4fb5fc32ecf209c10f93aae81cebfe69c3
which falls into ~__GFP_DIRECT_RECLAIM category.

I have tried to look at all the changed places as well and haven&#39;t
spotted anything problematic.
<span class="quote">
&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  Documentation/vm/balance                           | 14 ++++---</span>
<span class="quote">&gt;  arch/arm/mm/dma-mapping.c                          |  4 +-</span>
<span class="quote">&gt;  arch/arm/xen/mm.c                                  |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c                        |  4 +-</span>
<span class="quote">&gt;  arch/x86/kernel/pci-dma.c                          |  2 +-</span>
<span class="quote">&gt;  block/bio.c                                        | 26 ++++++------</span>
<span class="quote">&gt;  block/blk-core.c                                   | 16 ++++----</span>
<span class="quote">&gt;  block/blk-ioc.c                                    |  2 +-</span>
<span class="quote">&gt;  block/blk-mq-tag.c                                 |  2 +-</span>
<span class="quote">&gt;  block/blk-mq.c                                     |  8 ++--</span>
<span class="quote">&gt;  block/cfq-iosched.c                                |  4 +-</span>
<span class="quote">&gt;  drivers/block/drbd/drbd_receiver.c                 |  3 +-</span>
<span class="quote">&gt;  drivers/block/osdblk.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/connector/connector.c                      |  3 +-</span>
<span class="quote">&gt;  drivers/firewire/core-cdev.c                       |  2 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/i915/i915_gem.c                    |  2 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/sa_query.c                 |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/amd_iommu.c                          |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/intel-iommu.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/md/dm-crypt.c                              |  6 +--</span>
<span class="quote">&gt;  drivers/md/dm-kcopyd.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c     |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2.c         |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/tw68/tw68-video.c                |  2 +-</span>
<span class="quote">&gt;  drivers/mtd/mtdcore.c                              |  3 +-</span>
<span class="quote">&gt;  drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c    |  2 +-</span>
<span class="quote">&gt;  drivers/staging/android/ion/ion_system_heap.c      |  2 +-</span>
<span class="quote">&gt;  .../lustre/include/linux/libcfs/libcfs_private.h   |  2 +-</span>
<span class="quote">&gt;  drivers/usb/host/u132-hcd.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/video/fbdev/vermilion/vermilion.c          |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/disk-io.c                                 |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/extent_io.c                               | 14 +++----</span>
<span class="quote">&gt;  fs/btrfs/volumes.c                                 |  4 +-</span>
<span class="quote">&gt;  fs/ext3/super.c                                    |  2 +-</span>
<span class="quote">&gt;  fs/ext4/super.c                                    |  2 +-</span>
<span class="quote">&gt;  fs/fscache/cookie.c                                |  2 +-</span>
<span class="quote">&gt;  fs/fscache/page.c                                  |  6 +--</span>
<span class="quote">&gt;  fs/jbd/transaction.c                               |  4 +-</span>
<span class="quote">&gt;  fs/jbd2/transaction.c                              |  4 +-</span>
<span class="quote">&gt;  fs/nfs/file.c                                      |  6 +--</span>
<span class="quote">&gt;  fs/xfs/xfs_qm.c                                    |  2 +-</span>
<span class="quote">&gt;  include/linux/gfp.h                                | 46 ++++++++++++++++------</span>
<span class="quote">&gt;  include/linux/skbuff.h                             |  6 +--</span>
<span class="quote">&gt;  include/net/sock.h                                 |  2 +-</span>
<span class="quote">&gt;  include/trace/events/gfpflags.h                    |  5 ++-</span>
<span class="quote">&gt;  kernel/audit.c                                     |  6 +--</span>
<span class="quote">&gt;  kernel/locking/lockdep.c                           |  2 +-</span>
<span class="quote">&gt;  kernel/power/snapshot.c                            |  2 +-</span>
<span class="quote">&gt;  kernel/smp.c                                       |  2 +-</span>
<span class="quote">&gt;  lib/idr.c                                          |  4 +-</span>
<span class="quote">&gt;  lib/radix-tree.c                                   | 10 ++---</span>
<span class="quote">&gt;  mm/backing-dev.c                                   |  2 +-</span>
<span class="quote">&gt;  mm/dmapool.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c                                    |  8 ++--</span>
<span class="quote">&gt;  mm/mempool.c                                       | 10 ++---</span>
<span class="quote">&gt;  mm/migrate.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/page_alloc.c                                    | 43 ++++++++++++--------</span>
<span class="quote">&gt;  mm/slab.c                                          | 18 ++++-----</span>
<span class="quote">&gt;  mm/slub.c                                          |  6 +--</span>
<span class="quote">&gt;  mm/vmalloc.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/vmscan.c                                        |  4 +-</span>
<span class="quote">&gt;  mm/zswap.c                                         |  5 ++-</span>
<span class="quote">&gt;  net/core/skbuff.c                                  |  8 ++--</span>
<span class="quote">&gt;  net/core/sock.c                                    |  6 ++-</span>
<span class="quote">&gt;  net/netlink/af_netlink.c                           |  2 +-</span>
<span class="quote">&gt;  net/rxrpc/ar-connection.c                          |  2 +-</span>
<span class="quote">&gt;  net/sctp/associola.c                               |  2 +-</span>
<span class="quote">&gt;  67 files changed, 211 insertions(+), 173 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/vm/balance b/Documentation/vm/balance</span>
<span class="quote">&gt; index c46e68cf9344..964595481af6 100644</span>
<span class="quote">&gt; --- a/Documentation/vm/balance</span>
<span class="quote">&gt; +++ b/Documentation/vm/balance</span>
<span class="quote">&gt; @@ -1,12 +1,14 @@</span>
<span class="quote">&gt;  Started Jan 2000 by Kanoj Sarcar &lt;kanoj@sgi.com&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -Memory balancing is needed for non __GFP_WAIT as well as for non</span>
<span class="quote">&gt; -__GFP_IO allocations.</span>
<span class="quote">&gt; +Memory balancing is needed for !__GFP_ATOMIC and !__GFP_KSWAPD_RECLAIM as</span>
<span class="quote">&gt; +well as for non __GFP_IO allocations.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -There are two reasons to be requesting non __GFP_WAIT allocations:</span>
<span class="quote">&gt; -the caller can not sleep (typically intr context), or does not want</span>
<span class="quote">&gt; -to incur cost overheads of page stealing and possible swap io for</span>
<span class="quote">&gt; -whatever reasons.</span>
<span class="quote">&gt; +The first reason why a caller may avoid reclaim is that the caller can not</span>
<span class="quote">&gt; +sleep due to holding a spinlock or is in interrupt context. The second may</span>
<span class="quote">&gt; +be that the caller is willing to fail the allocation without incurring the</span>
<span class="quote">&gt; +overhead of page reclaim. This may happen for opportunistic high-order</span>
<span class="quote">&gt; +allocation requests that have order-0 fallback options. In such cases,</span>
<span class="quote">&gt; +the caller may also wish to avoid waking kswapd.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  __GFP_IO allocation requests are made to prevent file system deadlocks.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; index cba12f34ff77..f999f0987a3e 100644</span>
<span class="quote">&gt; --- a/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -650,7 +650,7 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (is_coherent || nommu())</span>
<span class="quote">&gt;  		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);</span>
<span class="quote">&gt; -	else if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	else if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  		addr = __alloc_from_pool(size, &amp;page);</span>
<span class="quote">&gt;  	else if (!dev_get_cma_area(dev))</span>
<span class="quote">&gt;  		addr = __alloc_remap_buffer(dev, size, gfp, prot, &amp;page, caller, want_vaddr);</span>
<span class="quote">&gt; @@ -1369,7 +1369,7 @@ static void *arm_iommu_alloc_attrs(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	*handle = DMA_ERROR_CODE;</span>
<span class="quote">&gt;  	size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  		return __iommu_alloc_atomic(dev, size, handle);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; index 03e75fef15b8..86809bd2026d 100644</span>
<span class="quote">&gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@</span>
<span class="quote">&gt;  unsigned long xen_get_swiotlb_free_pages(unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct memblock_region *reg;</span>
<span class="quote">&gt; -	gfp_t flags = __GFP_NOWARN;</span>
<span class="quote">&gt; +	gfp_t flags = __GFP_NOWARN|___GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_memblock(memory, reg) {</span>
<span class="quote">&gt;  		if (reg-&gt;base &lt; (phys_addr_t)0xffffffff) {</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index d16a1cead23f..1f10b2503af8 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -100,7 +100,7 @@ static void *__dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	if (IS_ENABLED(CONFIG_ZONE_DMA) &amp;&amp;</span>
<span class="quote">&gt;  	    dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(32))</span>
<span class="quote">&gt;  		flags |= GFP_DMA;</span>
<span class="quote">&gt; -	if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; (flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  		void *addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -147,7 +147,7 @@ static void *__dma_alloc(struct device *dev, size_t size,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!coherent &amp;&amp; !(flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		struct page *page = NULL;</span>
<span class="quote">&gt;  		void *addr = __alloc_from_pool(size, &amp;page, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; index 353972c1946c..0310e73e6b57 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; @@ -101,7 +101,7 @@ void *dma_generic_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	page = NULL;</span>
<span class="quote">&gt;  	/* CMA can be used only in the context which permits sleeping */</span>
<span class="quote">&gt; -	if (flag &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flag)) {</span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, count, get_order(size));</span>
<span class="quote">&gt;  		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="quote">&gt;  			dma_release_from_contiguous(dev, page, count);</span>
<span class="quote">&gt; diff --git a/block/bio.c b/block/bio.c</span>
<span class="quote">&gt; index d6e5ba3399f0..fbc558b50e67 100644</span>
<span class="quote">&gt; --- a/block/bio.c</span>
<span class="quote">&gt; +++ b/block/bio.c</span>
<span class="quote">&gt; @@ -211,7 +211,7 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;  		bvl = mempool_alloc(pool, gfp_mask);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		struct biovec_slab *bvs = bvec_slabs + *idx;</span>
<span class="quote">&gt; -		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_WAIT | __GFP_IO);</span>
<span class="quote">&gt; +		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM | __GFP_IO);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Make this allocation restricted and don&#39;t dump info on</span>
<span class="quote">&gt; @@ -221,11 +221,11 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;  		__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -		 * Try a slab allocation. If this fails and __GFP_WAIT</span>
<span class="quote">&gt; +		 * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM</span>
<span class="quote">&gt;  		 * is set, retry with the 1-entry mempool</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		bvl = kmem_cache_alloc(bvs-&gt;slab, __gfp_mask);</span>
<span class="quote">&gt; -		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_WAIT))) {</span>
<span class="quote">&gt; +		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))) {</span>
<span class="quote">&gt;  			*idx = BIOVEC_MAX_IDX;</span>
<span class="quote">&gt;  			goto fallback;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -393,12 +393,12 @@ static void punt_bios_to_rescuer(struct bio_set *bs)</span>
<span class="quote">&gt;   *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is</span>
<span class="quote">&gt;   *   backed by the @bs&#39;s mempool.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be</span>
<span class="quote">&gt; - *   able to allocate a bio. This is due to the mempool guarantees. To make this</span>
<span class="quote">&gt; - *   work, callers must never allocate more than 1 bio at a time from this pool.</span>
<span class="quote">&gt; - *   Callers that need to allocate more than 1 bio must always submit the</span>
<span class="quote">&gt; - *   previously allocated bio for IO before attempting to allocate a new one.</span>
<span class="quote">&gt; - *   Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt; + *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will</span>
<span class="quote">&gt; + *   always be able to allocate a bio. This is due to the mempool guarantees.</span>
<span class="quote">&gt; + *   To make this work, callers must never allocate more than 1 bio at a time</span>
<span class="quote">&gt; + *   from this pool. Callers that need to allocate more than 1 bio must always</span>
<span class="quote">&gt; + *   submit the previously allocated bio for IO before attempting to allocate</span>
<span class="quote">&gt; + *   a new one. Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *   Note that when running under generic_make_request() (i.e. any block</span>
<span class="quote">&gt;   *   driver), bios are not submitted until after you return - see the code in</span>
<span class="quote">&gt; @@ -457,13 +457,13 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
<span class="quote">&gt;  		 * We solve this, and guarantee forward progress, with a rescuer</span>
<span class="quote">&gt;  		 * workqueue per bio_set. If we go to allocate and there are</span>
<span class="quote">&gt;  		 * bios on current-&gt;bio_list, we first try the allocation</span>
<span class="quote">&gt; -		 * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="quote">&gt; -		 * would be blocking to the rescuer workqueue before we retry</span>
<span class="quote">&gt; -		 * with the original gfp_flags.</span>
<span class="quote">&gt; +		 * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="quote">&gt; +		 * bios we would be blocking to the rescuer workqueue before</span>
<span class="quote">&gt; +		 * we retry with the original gfp_flags.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="quote">&gt; -			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);</span>
<span class="quote">&gt;  		if (!p &amp;&amp; gfp_mask != saved_gfp) {</span>
<span class="quote">&gt; diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="quote">&gt; index 627ed0c593fb..e3605acaaffc 100644</span>
<span class="quote">&gt; --- a/block/blk-core.c</span>
<span class="quote">&gt; +++ b/block/blk-core.c</span>
<span class="quote">&gt; @@ -1156,8 +1156,8 @@ static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
<span class="quote">&gt;   * @bio: bio to allocate request for (can be %NULL)</span>
<span class="quote">&gt;   * @gfp_mask: allocation mask</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="quote">&gt; - * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; + * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="quote">&gt; + * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Must be called with @q-&gt;queue_lock held and,</span>
<span class="quote">&gt;   * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.</span>
<span class="quote">&gt; @@ -1177,7 +1177,7 @@ static struct request *get_request(struct request_queue *q, int rw_flags,</span>
<span class="quote">&gt;  	if (!IS_ERR(rq))</span>
<span class="quote">&gt;  		return rq;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt;  		blk_put_rl(rl);</span>
<span class="quote">&gt;  		return rq;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1255,11 +1255,11 @@ EXPORT_SYMBOL(blk_get_request);</span>
<span class="quote">&gt;   * BUG.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * WARNING: When allocating/cloning a bio-chain, careful consideration should be</span>
<span class="quote">&gt; - * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="quote">&gt; - * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="quote">&gt; - * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="quote">&gt; - * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="quote">&gt; - * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; + * given to how you allocate bios. In particular, you cannot use</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="quote">&gt; + * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="quote">&gt; + * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="quote">&gt; + * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt;   * If possible a big IO should be split into smaller parts when allocation</span>
<span class="quote">&gt;   * fails. Partial allocation should not be an error, or you risk a live-lock.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="quote">&gt; index 1a27f45ec776..381cb50a673c 100644</span>
<span class="quote">&gt; --- a/block/blk-ioc.c</span>
<span class="quote">&gt; +++ b/block/blk-ioc.c</span>
<span class="quote">&gt; @@ -289,7 +289,7 @@ struct io_context *get_task_io_context(struct task_struct *task,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct io_context *ioc;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		task_lock(task);</span>
<span class="quote">&gt; diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="quote">&gt; index 9b6e28830b82..a8b46659ce4e 100644</span>
<span class="quote">&gt; --- a/block/blk-mq-tag.c</span>
<span class="quote">&gt; +++ b/block/blk-mq-tag.c</span>
<span class="quote">&gt; @@ -264,7 +264,7 @@ static int bt_get(struct blk_mq_alloc_data *data,</span>
<span class="quote">&gt;  	if (tag != -1)</span>
<span class="quote">&gt;  		return tag;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	bs = bt_wait_ptr(bt, hctx);</span>
<span class="quote">&gt; diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="quote">&gt; index 7d842db59699..7d80379d7a38 100644</span>
<span class="quote">&gt; --- a/block/blk-mq.c</span>
<span class="quote">&gt; +++ b/block/blk-mq.c</span>
<span class="quote">&gt; @@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
<span class="quote">&gt;  		if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  			return -EBUSY;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		ret = wait_event_interruptible(q-&gt;mq_freeze_wq,</span>
<span class="quote">&gt; @@ -261,11 +261,11 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;  	hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt; -	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="quote">&gt; +	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
<span class="quote">&gt;  			reserved, ctx, hctx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt; -	if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;  		__blk_mq_run_hw_queue(hctx);</span>
<span class="quote">&gt;  		blk_mq_put_ctx(ctx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1221,7 +1221,7 @@ static struct request *blk_mq_map_request(struct request_queue *q,</span>
<span class="quote">&gt;  		ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;  		hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt;  		blk_mq_set_alloc_data(&amp;alloc_data, q,</span>
<span class="quote">&gt; -				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);</span>
<span class="quote">&gt; +				__GFP_WAIT|__GFP_HIGH, false, ctx, hctx);</span>
<span class="quote">&gt;  		rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt;  		ctx = alloc_data.ctx;</span>
<span class="quote">&gt;  		hctx = alloc_data.hctx;</span>
<span class="quote">&gt; diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c</span>
<span class="quote">&gt; index c62bb2e650b8..ecd1d1b61382 100644</span>
<span class="quote">&gt; --- a/block/cfq-iosched.c</span>
<span class="quote">&gt; +++ b/block/cfq-iosched.c</span>
<span class="quote">&gt; @@ -3674,7 +3674,7 @@ cfq_find_alloc_queue(struct cfq_data *cfqd, bool is_sync, struct cfq_io_cq *cic,</span>
<span class="quote">&gt;  		if (new_cfqq) {</span>
<span class="quote">&gt;  			cfqq = new_cfqq;</span>
<span class="quote">&gt;  			new_cfqq = NULL;</span>
<span class="quote">&gt; -		} else if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +		} else if (gfpflags_allow_blocking(gfp_mask)) {</span>
<span class="quote">&gt;  			rcu_read_unlock();</span>
<span class="quote">&gt;  			spin_unlock_irq(cfqd-&gt;queue-&gt;queue_lock);</span>
<span class="quote">&gt;  			new_cfqq = kmem_cache_alloc_node(cfq_pool,</span>
<span class="quote">&gt; @@ -4289,7 +4289,7 @@ cfq_set_request(struct request_queue *q, struct request *rq, struct bio *bio,</span>
<span class="quote">&gt;  	const bool is_sync = rq_is_sync(rq);</span>
<span class="quote">&gt;  	struct cfq_queue *cfqq;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irq(q-&gt;queue_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/block/drbd/drbd_receiver.c b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; index c097909c589c..b4b5680ac6ad 100644</span>
<span class="quote">&gt; --- a/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; +++ b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; @@ -357,7 +357,8 @@ drbd_alloc_peer_req(struct drbd_peer_device *peer_device, u64 id, sector_t secto</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (has_payload &amp;&amp; data_size) {</span>
<span class="quote">&gt; -		page = drbd_alloc_pages(peer_device, nr_pages, (gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +		page = drbd_alloc_pages(peer_device, nr_pages,</span>
<span class="quote">&gt; +					gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  		if (!page)</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/drivers/block/osdblk.c b/drivers/block/osdblk.c</span>
<span class="quote">&gt; index e22942596207..1b709a4e3b5e 100644</span>
<span class="quote">&gt; --- a/drivers/block/osdblk.c</span>
<span class="quote">&gt; +++ b/drivers/block/osdblk.c</span>
<span class="quote">&gt; @@ -271,7 +271,7 @@ static struct bio *bio_chain_clone(struct bio *old_chain, gfp_t gfpmask)</span>
<span class="quote">&gt;  			goto err_out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		tmp-&gt;bi_bdev = NULL;</span>
<span class="quote">&gt; -		gfpmask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +		gfpmask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  		tmp-&gt;bi_next = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!new_chain)</span>
<span class="quote">&gt; diff --git a/drivers/connector/connector.c b/drivers/connector/connector.c</span>
<span class="quote">&gt; index 30f522848c73..d7373ca69c99 100644</span>
<span class="quote">&gt; --- a/drivers/connector/connector.c</span>
<span class="quote">&gt; +++ b/drivers/connector/connector.c</span>
<span class="quote">&gt; @@ -124,7 +124,8 @@ int cn_netlink_send_mult(struct cn_msg *msg, u16 len, u32 portid, u32 __group,</span>
<span class="quote">&gt;  	if (group)</span>
<span class="quote">&gt;  		return netlink_broadcast(dev-&gt;nls, skb, portid, group,</span>
<span class="quote">&gt;  					 gfp_mask);</span>
<span class="quote">&gt; -	return netlink_unicast(dev-&gt;nls, skb, portid, !(gfp_mask&amp;__GFP_WAIT));</span>
<span class="quote">&gt; +	return netlink_unicast(dev-&gt;nls, skb, portid,</span>
<span class="quote">&gt; +			!gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(cn_netlink_send_mult);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/firewire/core-cdev.c b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; index 2a3973a7c441..36a7c2d89a01 100644</span>
<span class="quote">&gt; --- a/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; +++ b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; @@ -486,7 +486,7 @@ static int ioctl_get_info(struct client *client, union ioctl_arg *arg)</span>
<span class="quote">&gt;  static int add_client_resource(struct client *client,</span>
<span class="quote">&gt;  			       struct client_resource *resource, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; index 52b446b27b4d..c2b45081c5ab 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; @@ -2225,7 +2225,7 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	mapping = file_inode(obj-&gt;base.filp)-&gt;i_mapping;</span>
<span class="quote">&gt;  	gfp = mapping_gfp_mask(mapping);</span>
<span class="quote">&gt; -	gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +	gfp |= __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;  	gfp &amp;= ~(__GFP_IO | __GFP_WAIT);</span>
<span class="quote">&gt;  	sg = st-&gt;sgl;</span>
<span class="quote">&gt;  	st-&gt;nents = 0;</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; index ca919f429666..7474d79ffac0 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; @@ -619,7 +619,7 @@ static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  	int ret, id;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; index 658ee39e6569..95d4c70dc7b1 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; @@ -2755,7 +2755,7 @@ static void *alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));</span>
<span class="quote">&gt;  	if (!page) {</span>
<span class="quote">&gt; -		if (!(flag &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (!gfpflags_allow_blocking(flag))</span>
<span class="quote">&gt;  			return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, size &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; index 0649b94f5958..f77becf3d8d8 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; @@ -3566,7 +3566,7 @@ static void *intel_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  			flags |= GFP_DMA32;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		unsigned int count = size &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, count, order);</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; index 0f48fed44a17..6dda08385309 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; @@ -993,7 +993,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  	struct bio_vec *bvec;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		mutex_lock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, cc-&gt;bs);</span>
<span class="quote">&gt; @@ -1009,7 +1009,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  		if (!page) {</span>
<span class="quote">&gt;  			crypt_free_buffer_pages(cc, clone);</span>
<span class="quote">&gt;  			bio_put(clone);</span>
<span class="quote">&gt; -			gfp_mask |= __GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask |= __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  			goto retry;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1026,7 +1026,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  return_clone:</span>
<span class="quote">&gt; -	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		mutex_unlock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return clone;</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; index 3a7cade5e27d..1452ed9aacb4 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; @@ -244,7 +244,7 @@ static int kcopyd_get_pages(struct dm_kcopyd_client *kc,</span>
<span class="quote">&gt;  	*pages = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY);</span>
<span class="quote">&gt; +		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;  		if (unlikely(!pl)) {</span>
<span class="quote">&gt;  			/* Use reserved pages */</span>
<span class="quote">&gt;  			pl = kc-&gt;pages;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; index 53fff5425c13..fb2cb4bdc0c1 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; @@ -1291,7 +1291,7 @@ static struct solo_enc_dev *solo_enc_alloc(struct solo_dev *solo_dev,</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.ops = &amp;solo_enc_video_qops;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.drv_priv = solo_enc;</span>
<span class="quote">&gt; -	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.lock = &amp;solo_enc-&gt;lock;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2.c b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; index 63ae8a61f603..bde77b22340c 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; @@ -675,7 +675,7 @@ int solo_v4l2_init(struct solo_dev *solo_dev, unsigned nr)</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.mem_ops = &amp;vb2_dma_contig_memops;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.drv_priv = solo_dev;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt; -	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.lock = &amp;solo_dev-&gt;lock;</span>
<span class="quote">&gt;  	ret = vb2_queue_init(&amp;solo_dev-&gt;vidq);</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/tw68/tw68-video.c b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; index 8355e55b4e8e..e556f989aaab 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; @@ -975,7 +975,7 @@ int tw68_video_init2(struct tw68_dev *dev, int video_nr)</span>
<span class="quote">&gt;  	dev-&gt;vidq.ops = &amp;tw68_video_qops;</span>
<span class="quote">&gt;  	dev-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;  	dev-&gt;vidq.drv_priv = dev;</span>
<span class="quote">&gt; -	dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	dev-&gt;vidq.buf_struct_size = sizeof(struct tw68_buf);</span>
<span class="quote">&gt;  	dev-&gt;vidq.lock = &amp;dev-&gt;lock;</span>
<span class="quote">&gt;  	dev-&gt;vidq.min_buffers_needed = 2;</span>
<span class="quote">&gt; diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; index 8bbbb751bf45..2dfb291a47c6 100644</span>
<span class="quote">&gt; --- a/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; +++ b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; @@ -1188,8 +1188,7 @@ EXPORT_SYMBOL_GPL(mtd_writev);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void *mtd_kmalloc_up_to(const struct mtd_info *mtd, size_t *size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	gfp_t flags = __GFP_NOWARN | __GFP_WAIT |</span>
<span class="quote">&gt; -		       __GFP_NORETRY | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +	gfp_t flags = __GFP_NOWARN | __GFP_DIRECT_RECLAIM | __GFP_NORETRY;</span>
<span class="quote">&gt;  	size_t min_alloc = max_t(size_t, mtd-&gt;writesize, PAGE_SIZE);</span>
<span class="quote">&gt;  	void *kbuf;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; index f7fbdc9d1325..3a407e59acab 100644</span>
<span class="quote">&gt; --- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; +++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; @@ -689,7 +689,7 @@ static void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (fp-&gt;rx_frag_size) {</span>
<span class="quote">&gt;  		/* GFP_KERNEL allocations are used only during initialization */</span>
<span class="quote">&gt; -		if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (unlikely(gfpflags_allow_blocking(gfp_mask)))</span>
<span class="quote">&gt;  			return (void *)__get_free_page(gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		return netdev_alloc_frag(fp-&gt;rx_frag_size);</span>
<span class="quote">&gt; diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; index da2a63c0a9ba..2615e0ae4f0a 100644</span>
<span class="quote">&gt; --- a/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; +++ b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; @@ -27,7 +27,7 @@</span>
<span class="quote">&gt;  #include &quot;ion_priv.h&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |</span>
<span class="quote">&gt; -				     __GFP_NORETRY) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +				     __GFP_NORETRY) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);</span>
<span class="quote">&gt;  static const unsigned int orders[] = {8, 4, 0};</span>
<span class="quote">&gt;  static const int num_orders = ARRAY_SIZE(orders);</span>
<span class="quote">&gt; diff --git a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; index ed37d26eb20d..5b0756cb6576 100644</span>
<span class="quote">&gt; --- a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; +++ b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; @@ -113,7 +113,7 @@ do {						\</span>
<span class="quote">&gt;  do {									    \</span>
<span class="quote">&gt;  	LASSERT(!in_interrupt() ||					    \</span>
<span class="quote">&gt;  		((size) &lt;= LIBCFS_VMALLOC_SIZE &amp;&amp;			    \</span>
<span class="quote">&gt; -		 ((mask) &amp; __GFP_WAIT) == 0));				    \</span>
<span class="quote">&gt; +		 !gfpflags_allow_blocking(mask)));			    \</span>
<span class="quote">&gt;  } while (0)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define LIBCFS_ALLOC_POST(ptr, size)					    \</span>
<span class="quote">&gt; diff --git a/drivers/usb/host/u132-hcd.c b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; index d51687780b61..8d4c1806e32f 100644</span>
<span class="quote">&gt; --- a/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; +++ b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; @@ -2247,7 +2247,7 @@ static int u132_urb_enqueue(struct usb_hcd *hcd, struct urb *urb,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct u132 *u132 = hcd_to_u132(hcd);</span>
<span class="quote">&gt;  	if (irqs_disabled()) {</span>
<span class="quote">&gt; -		if (__GFP_WAIT &amp; mem_flags) {</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(mem_flags)) {</span>
<span class="quote">&gt;  			printk(KERN_ERR &quot;invalid context for function that migh&quot;</span>
<span class="quote">&gt;  				&quot;t sleep\n&quot;);</span>
<span class="quote">&gt;  			return -EINVAL;</span>
<span class="quote">&gt; diff --git a/drivers/video/fbdev/vermilion/vermilion.c b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; index 6b70d7f62b2f..1c1e95a0b8fa 100644</span>
<span class="quote">&gt; --- a/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; +++ b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; @@ -99,7 +99,7 @@ static int vmlfb_alloc_vram_area(struct vram_area *va, unsigned max_order,</span>
<span class="quote">&gt;  		 * below the first 16MB.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		flags = __GFP_DMA | __GFP_HIGH;</span>
<span class="quote">&gt; +		flags = __GFP_DMA | __GFP_HIGH | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  		va-&gt;logical =</span>
<span class="quote">&gt;  			 __get_free_pages(flags, --max_order);</span>
<span class="quote">&gt;  	} while (va-&gt;logical == 0 &amp;&amp; max_order &gt; min_order);</span>
<span class="quote">&gt; diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; index f556c3732c2c..3dd4792b8099 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; @@ -2566,7 +2566,7 @@ int open_ctree(struct super_block *sb,</span>
<span class="quote">&gt;  	fs_info-&gt;commit_interval = BTRFS_DEFAULT_COMMIT_INTERVAL;</span>
<span class="quote">&gt;  	fs_info-&gt;avg_delayed_ref_runtime = NSEC_PER_SEC &gt;&gt; 6; /* div by 64 */</span>
<span class="quote">&gt;  	/* readahead state */</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	spin_lock_init(&amp;fs_info-&gt;reada_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	fs_info-&gt;thread_pool_size = min_t(unsigned long,</span>
<span class="quote">&gt; diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; index 02d05817cbdf..c8a6cdcbef2b 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; @@ -594,7 +594,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (bits &amp; (EXTENT_IOBITS | EXTENT_BOUNDARY))</span>
<span class="quote">&gt;  		clear = 1;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Don&#39;t care for allocation failure here because we might end</span>
<span class="quote">&gt;  		 * up not needing the pre-allocated extent state at all, which</span>
<span class="quote">&gt; @@ -718,7 +718,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -850,7 +850,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	bits |= EXTENT_FIRST_DELALLOC;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		prealloc = alloc_extent_state(mask);</span>
<span class="quote">&gt;  		BUG_ON(!prealloc);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1028,7 +1028,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1076,7 +1076,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	btrfs_debug_check_extent_io_range(tree, start, end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Best effort, don&#39;t worry if extent state allocation fails</span>
<span class="quote">&gt;  		 * here for the first iteration. We might have a cached state</span>
<span class="quote">&gt; @@ -1253,7 +1253,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	first_iteration = false;</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt; @@ -4265,7 +4265,7 @@ int try_release_extent_mapping(struct extent_map_tree *map,</span>
<span class="quote">&gt;  	u64 start = page_offset(page);</span>
<span class="quote">&gt;  	u64 end = start + PAGE_CACHE_SIZE - 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if ((mask &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask) &amp;&amp;</span>
<span class="quote">&gt;  	    page-&gt;mapping-&gt;host-&gt;i_size &gt; 16 * 1024 * 1024) {</span>
<span class="quote">&gt;  		u64 len;</span>
<span class="quote">&gt;  		while (start &lt;= end) {</span>
<span class="quote">&gt; diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; index fbe7c104531c..b1968f36a39b 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/volumes.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; @@ -156,8 +156,8 @@ static struct btrfs_device *__alloc_device(void)</span>
<span class="quote">&gt;  	spin_lock_init(&amp;dev-&gt;reada_lock);</span>
<span class="quote">&gt;  	atomic_set(&amp;dev-&gt;reada_in_flight, 0);</span>
<span class="quote">&gt;  	atomic_set(&amp;dev-&gt;dev_stats_ccnt, 0);</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dev;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/fs/ext3/super.c b/fs/ext3/super.c</span>
<span class="quote">&gt; index 5ed0044fbb37..9004c786716f 100644</span>
<span class="quote">&gt; --- a/fs/ext3/super.c</span>
<span class="quote">&gt; +++ b/fs/ext3/super.c</span>
<span class="quote">&gt; @@ -750,7 +750,7 @@ static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	if (journal)</span>
<span class="quote">&gt;  		return journal_try_to_free_buffers(journal, page, </span>
<span class="quote">&gt; -						   wait &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	return try_to_free_buffers(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="quote">&gt; index 58987b5c514b..abe76d41ef1e 100644</span>
<span class="quote">&gt; --- a/fs/ext4/super.c</span>
<span class="quote">&gt; +++ b/fs/ext4/super.c</span>
<span class="quote">&gt; @@ -1045,7 +1045,7 @@ static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	if (journal)</span>
<span class="quote">&gt;  		return jbd2_journal_try_to_free_buffers(journal, page,</span>
<span class="quote">&gt; -							wait &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	return try_to_free_buffers(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/fs/fscache/cookie.c b/fs/fscache/cookie.c</span>
<span class="quote">&gt; index d403c69bee08..4304072161aa 100644</span>
<span class="quote">&gt; --- a/fs/fscache/cookie.c</span>
<span class="quote">&gt; +++ b/fs/fscache/cookie.c</span>
<span class="quote">&gt; @@ -111,7 +111,7 @@ struct fscache_cookie *__fscache_acquire_cookie(</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* radix tree insertion won&#39;t use the preallocation pool unless it&#39;s</span>
<span class="quote">&gt;  	 * told it may not wait */</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	switch (cookie-&gt;def-&gt;type) {</span>
<span class="quote">&gt;  	case FSCACHE_COOKIE_TYPE_INDEX:</span>
<span class="quote">&gt; diff --git a/fs/fscache/page.c b/fs/fscache/page.c</span>
<span class="quote">&gt; index 483bbc613bf0..79483b3d8c6f 100644</span>
<span class="quote">&gt; --- a/fs/fscache/page.c</span>
<span class="quote">&gt; +++ b/fs/fscache/page.c</span>
<span class="quote">&gt; @@ -58,7 +58,7 @@ bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * decide whether a page can be released, possibly by cancelling a store to it</span>
<span class="quote">&gt; - * - we&#39;re allowed to sleep if __GFP_WAIT is flagged</span>
<span class="quote">&gt; + * - we&#39;re allowed to sleep if __GFP_DIRECT_RECLAIM is flagged</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  				  struct page *page,</span>
<span class="quote">&gt; @@ -122,7 +122,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  	 * allocator as the work threads writing to the cache may all end up</span>
<span class="quote">&gt;  	 * sleeping on memory allocation, so we may need to impose a timeout</span>
<span class="quote">&gt;  	 * too. */</span>
<span class="quote">&gt; -	if (!(gfp &amp; __GFP_WAIT) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt; +	if (!(gfp &amp; __GFP_DIRECT_RECLAIM) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt;  		fscache_stat(&amp;fscache_n_store_vmscan_busy);</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -132,7 +132,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  		_debug(&quot;fscache writeout timeout page: %p{%lx}&quot;,</span>
<span class="quote">&gt;  			page, page-&gt;index);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	gfp &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +	gfp &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  	goto try_again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(__fscache_maybe_release_page);</span>
<span class="quote">&gt; diff --git a/fs/jbd/transaction.c b/fs/jbd/transaction.c</span>
<span class="quote">&gt; index 1695ba8334a2..f45b90ba7c5c 100644</span>
<span class="quote">&gt; --- a/fs/jbd/transaction.c</span>
<span class="quote">&gt; +++ b/fs/jbd/transaction.c</span>
<span class="quote">&gt; @@ -1690,8 +1690,8 @@ __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
<span class="quote">&gt;   * @journal: journal for operation</span>
<span class="quote">&gt;   * @page: to try and free</span>
<span class="quote">&gt;   * @gfp_mask: we use the mask to detect how hard should we try to release</span>
<span class="quote">&gt; - * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="quote">&gt; - * release the buffers.</span>
<span class="quote">&gt; + * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="quote">&gt; + * code to release the buffers.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * For all the buffers on this page,</span>
<span class="quote">&gt; diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; index f3d06174b051..06e18bcdb888 100644</span>
<span class="quote">&gt; --- a/fs/jbd2/transaction.c</span>
<span class="quote">&gt; +++ b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; @@ -1893,8 +1893,8 @@ __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
<span class="quote">&gt;   * @journal: journal for operation</span>
<span class="quote">&gt;   * @page: to try and free</span>
<span class="quote">&gt;   * @gfp_mask: we use the mask to detect how hard should we try to release</span>
<span class="quote">&gt; - * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="quote">&gt; - * release the buffers.</span>
<span class="quote">&gt; + * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="quote">&gt; + * code to release the buffers.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * For all the buffers on this page,</span>
<span class="quote">&gt; diff --git a/fs/nfs/file.c b/fs/nfs/file.c</span>
<span class="quote">&gt; index cc4fa1ed61fc..be6821967ec6 100644</span>
<span class="quote">&gt; --- a/fs/nfs/file.c</span>
<span class="quote">&gt; +++ b/fs/nfs/file.c</span>
<span class="quote">&gt; @@ -480,8 +480,8 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;  	dfprintk(PAGECACHE, &quot;NFS: release_page(%p)\n&quot;, page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Always try to initiate a &#39;commit&#39; if relevant, but only</span>
<span class="quote">&gt; -	 * wait for it if __GFP_WAIT is set.  Even then, only wait 1</span>
<span class="quote">&gt; -	 * second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt; +	 * wait for it if the caller allows blocking.  Even then,</span>
<span class="quote">&gt; +	 * only wait 1 second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt;  	 * Waiting indefinitely can cause deadlocks when the NFS</span>
<span class="quote">&gt;  	 * server is on this machine, when a new TCP connection is</span>
<span class="quote">&gt;  	 * needed and in other rare cases.  There is no particular</span>
<span class="quote">&gt; @@ -491,7 +491,7 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;  	if (mapping) {</span>
<span class="quote">&gt;  		struct nfs_server *nfss = NFS_SERVER(mapping-&gt;host);</span>
<span class="quote">&gt;  		nfs_commit_inode(mapping-&gt;host, 0);</span>
<span class="quote">&gt; -		if ((gfp &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(gfp) &amp;&amp;</span>
<span class="quote">&gt;  		    !bdi_write_congested(&amp;nfss-&gt;backing_dev_info)) {</span>
<span class="quote">&gt;  			wait_on_page_bit_killable_timeout(page, PG_private,</span>
<span class="quote">&gt;  							  HZ);</span>
<span class="quote">&gt; diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; index eac9549efd52..587174fd4f2c 100644</span>
<span class="quote">&gt; --- a/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; +++ b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; @@ -525,7 +525,7 @@ xfs_qm_shrink_scan(</span>
<span class="quote">&gt;  	unsigned long		freed;</span>
<span class="quote">&gt;  	int			error;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT))</span>
<span class="quote">&gt; +	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_DIRECT_RECLAIM)) != (__GFP_FS|__GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	INIT_LIST_HEAD(&amp;isol.buffers);</span>
<span class="quote">&gt; diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt; index a10347ca5053..bd1937977d84 100644</span>
<span class="quote">&gt; --- a/include/linux/gfp.h</span>
<span class="quote">&gt; +++ b/include/linux/gfp.h</span>
<span class="quote">&gt; @@ -29,12 +29,13 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define ___GFP_NOMEMALLOC	0x10000u</span>
<span class="quote">&gt;  #define ___GFP_HARDWALL		0x20000u</span>
<span class="quote">&gt;  #define ___GFP_THISNODE		0x40000u</span>
<span class="quote">&gt; -#define ___GFP_WAIT		0x80000u</span>
<span class="quote">&gt; +#define ___GFP_ATOMIC		0x80000u</span>
<span class="quote">&gt;  #define ___GFP_NOACCOUNT	0x100000u</span>
<span class="quote">&gt;  #define ___GFP_NOTRACK		0x200000u</span>
<span class="quote">&gt; -#define ___GFP_NO_KSWAPD	0x400000u</span>
<span class="quote">&gt; +#define ___GFP_DIRECT_RECLAIM	0x400000u</span>
<span class="quote">&gt;  #define ___GFP_OTHER_NODE	0x800000u</span>
<span class="quote">&gt;  #define ___GFP_WRITE		0x1000000u</span>
<span class="quote">&gt; +#define ___GFP_KSWAPD_RECLAIM	0x2000000u</span>
<span class="quote">&gt;  /* If the above are modified, __GFP_BITS_SHIFT may need updating */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -68,7 +69,7 @@ struct vm_area_struct;</span>
<span class="quote">&gt;   * __GFP_MOVABLE: Flag that this page will be movable by the page migration</span>
<span class="quote">&gt;   * mechanism or reclaimed</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define __GFP_WAIT	((__force gfp_t)___GFP_WAIT)	/* Can wait and reschedule? */</span>
<span class="quote">&gt; +#define __GFP_ATOMIC	((__force gfp_t)___GFP_ATOMIC)  /* Caller cannot wait or reschedule */</span>
<span class="quote">&gt;  #define __GFP_HIGH	((__force gfp_t)___GFP_HIGH)	/* Should access emergency pools? */</span>
<span class="quote">&gt;  #define __GFP_IO	((__force gfp_t)___GFP_IO)	/* Can start physical IO? */</span>
<span class="quote">&gt;  #define __GFP_FS	((__force gfp_t)___GFP_FS)	/* Can call down to low-level FS? */</span>
<span class="quote">&gt; @@ -91,23 +92,37 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define __GFP_NOACCOUNT	((__force gfp_t)___GFP_NOACCOUNT) /* Don&#39;t account to kmemcg */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK	((__force gfp_t)___GFP_NOTRACK)  /* Don&#39;t track with kmemcheck */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define __GFP_NO_KSWAPD	((__force gfp_t)___GFP_NO_KSWAPD)</span>
<span class="quote">&gt;  #define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */</span>
<span class="quote">&gt;  #define __GFP_WRITE	((__force gfp_t)___GFP_WRITE)	/* Allocator intends to dirty page */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="quote">&gt; + * wake kswapd to reclaim pages in the background until the high</span>
<span class="quote">&gt; + * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="quote">&gt; + * avoid unnecessary delays when a fallback option is available but</span>
<span class="quote">&gt; + * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="quote">&gt; + * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="quote">&gt; + * disruption.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __GFP_WAIT (__GFP_DIRECT_RECLAIM|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="quote">&gt; +#define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * This may seem redundant, but it&#39;s a way of annotating false positives vs.</span>
<span class="quote">&gt;   * allocations that simply cannot be supported (e.g. page tables).</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt; +#define __GFP_BITS_SHIFT 26	/* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt;  #define __GFP_BITS_MASK ((__force gfp_t)((1 &lt;&lt; __GFP_BITS_SHIFT) - 1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* This equals 0, but use constants in case they ever change */</span>
<span class="quote">&gt; -#define GFP_NOWAIT	(GFP_ATOMIC &amp; ~__GFP_HIGH)</span>
<span class="quote">&gt; -/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */</span>
<span class="quote">&gt; -#define GFP_ATOMIC	(__GFP_HIGH)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * GFP_ATOMIC callers can not sleep, need the allocation to succeed.</span>
<span class="quote">&gt; + * A lower watermark is applied to allow access to &quot;atomic reserves&quot;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define GFP_ATOMIC	(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_NOWAIT	(__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  #define GFP_NOIO	(__GFP_WAIT)</span>
<span class="quote">&gt;  #define GFP_NOFS	(__GFP_WAIT | __GFP_IO)</span>
<span class="quote">&gt;  #define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; @@ -116,10 +131,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; -#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; -#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; -			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; -			 __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; +#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_TRANSHUGE	((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; +			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &amp; \</span>
<span class="quote">&gt; +			 ~__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* This mask makes up all the page movable related flags */</span>
<span class="quote">&gt;  #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)</span>
<span class="quote">&gt; @@ -161,6 +176,11 @@ static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)</span>
<span class="quote">&gt;  	return (gfp_flags &amp; GFP_MOVABLE_MASK) &gt;&gt; GFP_MOVABLE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return gfp_flags &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt;  #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h</span>
<span class="quote">&gt; index 22b6d9ca1654..55c4a9175801 100644</span>
<span class="quote">&gt; --- a/include/linux/skbuff.h</span>
<span class="quote">&gt; +++ b/include/linux/skbuff.h</span>
<span class="quote">&gt; @@ -1109,7 +1109,7 @@ static inline int skb_cloned(const struct sk_buff *skb)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int skb_unclone(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (skb_cloned(skb))</span>
<span class="quote">&gt;  		return pskb_expand_head(skb, 0, 0, pri);</span>
<span class="quote">&gt; @@ -1193,7 +1193,7 @@ static inline int skb_shared(const struct sk_buff *skb)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  	if (skb_shared(skb)) {</span>
<span class="quote">&gt;  		struct sk_buff *nskb = skb_clone(skb, pri);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1229,7 +1229,7 @@ static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_unshare(struct sk_buff *skb,</span>
<span class="quote">&gt;  					  gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  	if (skb_cloned(skb)) {</span>
<span class="quote">&gt;  		struct sk_buff *nskb = skb_copy(skb, pri);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/net/sock.h b/include/net/sock.h</span>
<span class="quote">&gt; index f21f0708ec59..cec0c4b634dc 100644</span>
<span class="quote">&gt; --- a/include/net/sock.h</span>
<span class="quote">&gt; +++ b/include/net/sock.h</span>
<span class="quote">&gt; @@ -2035,7 +2035,7 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct page_frag *sk_page_frag(struct sock *sk)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (sk-&gt;sk_allocation &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(sk-&gt;sk_allocation))</span>
<span class="quote">&gt;  		return &amp;current-&gt;task_frag;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return &amp;sk-&gt;sk_frag;</span>
<span class="quote">&gt; diff --git a/include/trace/events/gfpflags.h b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; index d6fd8e5b14b7..dde6bf092c8a 100644</span>
<span class="quote">&gt; --- a/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; +++ b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; @@ -20,7 +20,7 @@</span>
<span class="quote">&gt;  	{(unsigned long)GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)GFP_NOIO,		&quot;GFP_NOIO&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_HIGH,		&quot;GFP_HIGH&quot;},		\</span>
<span class="quote">&gt; -	{(unsigned long)__GFP_WAIT,		&quot;GFP_WAIT&quot;},		\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_IO,		&quot;GFP_IO&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_COLD,		&quot;GFP_COLD&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_NOWARN,		&quot;GFP_NOWARN&quot;},		\</span>
<span class="quote">&gt; @@ -36,7 +36,8 @@</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_RECLAIMABLE,	&quot;GFP_RECLAIMABLE&quot;},	\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_MOVABLE,		&quot;GFP_MOVABLE&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_NOTRACK,		&quot;GFP_NOTRACK&quot;},		\</span>
<span class="quote">&gt; -	{(unsigned long)__GFP_NO_KSWAPD,	&quot;GFP_NO_KSWAPD&quot;},	\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_DIRECT_RECLAIM,	&quot;GFP_DIRECT_RECLAIM&quot;},	\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_KSWAPD_RECLAIM,	&quot;GFP_KSWAPD_RECLAIM&quot;},	\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_OTHER_NODE,	&quot;GFP_OTHER_NODE&quot;}	\</span>
<span class="quote">&gt;  	) : &quot;GFP_NOWAIT&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/kernel/audit.c b/kernel/audit.c</span>
<span class="quote">&gt; index f9e6065346db..6ab7a55dbdff 100644</span>
<span class="quote">&gt; --- a/kernel/audit.c</span>
<span class="quote">&gt; +++ b/kernel/audit.c</span>
<span class="quote">&gt; @@ -1357,16 +1357,16 @@ struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	if (unlikely(audit_filter_type(type)))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_DIRECT_RECLAIM) {</span>
<span class="quote">&gt;  		if (audit_pid &amp;&amp; audit_pid == current-&gt;pid)</span>
<span class="quote">&gt; -			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  		else</span>
<span class="quote">&gt;  			reserve = 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (audit_backlog_limit</span>
<span class="quote">&gt;  	       &amp;&amp; skb_queue_len(&amp;audit_skb_queue) &gt; audit_backlog_limit + reserve) {</span>
<span class="quote">&gt; -		if (gfp_mask &amp; __GFP_WAIT &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt; +		if (gfp_mask &amp; __GFP_DIRECT_RECLAIM &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt;  			long sleep_time;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			sleep_time = timeout_start + audit_backlog_wait_time - jiffies;</span>
<span class="quote">&gt; diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; index 8acfbf773e06..9aa39f20f593 100644</span>
<span class="quote">&gt; --- a/kernel/locking/lockdep.c</span>
<span class="quote">&gt; +++ b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; @@ -2738,7 +2738,7 @@ static void __lockdep_trace_alloc(gfp_t gfp_mask, unsigned long flags)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* no reclaim without waiting on it */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* this guy won&#39;t enter reclaim */</span>
<span class="quote">&gt; diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c</span>
<span class="quote">&gt; index 5235dd4e1e2f..3a970604308f 100644</span>
<span class="quote">&gt; --- a/kernel/power/snapshot.c</span>
<span class="quote">&gt; +++ b/kernel/power/snapshot.c</span>
<span class="quote">&gt; @@ -1779,7 +1779,7 @@ alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)</span>
<span class="quote">&gt;  	while (to_alloc-- &gt; 0) {</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		page = alloc_image_page(__GFP_HIGHMEM);</span>
<span class="quote">&gt; +		page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;  		memory_bm_set_bit(bm, page_to_pfn(page));</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return nr_highmem;</span>
<span class="quote">&gt; diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="quote">&gt; index 07854477c164..d903c02223af 100644</span>
<span class="quote">&gt; --- a/kernel/smp.c</span>
<span class="quote">&gt; +++ b/kernel/smp.c</span>
<span class="quote">&gt; @@ -669,7 +669,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
<span class="quote">&gt;  	cpumask_var_t cpus;</span>
<span class="quote">&gt;  	int cpu, ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {</span>
<span class="quote">&gt;  		preempt_disable();</span>
<span class="quote">&gt; diff --git a/lib/idr.c b/lib/idr.c</span>
<span class="quote">&gt; index 5335c43adf46..6098336df267 100644</span>
<span class="quote">&gt; --- a/lib/idr.c</span>
<span class="quote">&gt; +++ b/lib/idr.c</span>
<span class="quote">&gt; @@ -399,7 +399,7 @@ void idr_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	 * allocation guarantee.  Disallow usage from those contexts.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	WARN_ON_ONCE(in_interrupt());</span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -453,7 +453,7 @@ int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	struct idr_layer *pa[MAX_IDR_LEVEL + 1];</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* sanity checks */</span>
<span class="quote">&gt;  	if (WARN_ON_ONCE(start &lt; 0))</span>
<span class="quote">&gt; diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="quote">&gt; index f9ebe1c82060..c3775ee46cd6 100644</span>
<span class="quote">&gt; --- a/lib/radix-tree.c</span>
<span class="quote">&gt; +++ b/lib/radix-tree.c</span>
<span class="quote">&gt; @@ -188,7 +188,7 @@ radix_tree_node_alloc(struct radix_tree_root *root)</span>
<span class="quote">&gt;  	 * preloading in the interrupt anyway as all the allocations have to</span>
<span class="quote">&gt;  	 * be atomic. So just do normal allocation when in interrupt.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt;  		struct radix_tree_preload *rtp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; @@ -249,7 +249,7 @@ radix_tree_node_free(struct radix_tree_node *node)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -286,12 +286,12 @@ static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/* Warn on non-sensical use... */</span>
<span class="quote">&gt; -	WARN_ON_ONCE(!(gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +	WARN_ON_ONCE(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  	return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt; @@ -303,7 +303,7 @@ EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_maybe_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;  	/* Preloading doesn&#39;t help anything with this gfp mask, skip it */</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt; diff --git a/mm/backing-dev.c b/mm/backing-dev.c</span>
<span class="quote">&gt; index dac5bf59309d..805ce70b72f3 100644</span>
<span class="quote">&gt; --- a/mm/backing-dev.c</span>
<span class="quote">&gt; +++ b/mm/backing-dev.c</span>
<span class="quote">&gt; @@ -632,7 +632,7 @@ struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct bdi_writeback *wb;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!memcg_css-&gt;parent)</span>
<span class="quote">&gt;  		return &amp;bdi-&gt;wb;</span>
<span class="quote">&gt; diff --git a/mm/dmapool.c b/mm/dmapool.c</span>
<span class="quote">&gt; index fd5fe4342e93..84dac666fc0c 100644</span>
<span class="quote">&gt; --- a/mm/dmapool.c</span>
<span class="quote">&gt; +++ b/mm/dmapool.c</span>
<span class="quote">&gt; @@ -323,7 +323,7 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,</span>
<span class="quote">&gt;  	size_t offset;</span>
<span class="quote">&gt;  	void *retval;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(mem_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(mem_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irqsave(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;  	list_for_each_entry(page, &amp;pool-&gt;page_list, page_list) {</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index acb93c554f6e..e34f6411da8c 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -2268,7 +2268,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	if (unlikely(task_in_memcg_oom(current)))</span>
<span class="quote">&gt;  		goto nomem;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		goto nomem;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);</span>
<span class="quote">&gt; @@ -2327,7 +2327,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	css_get_many(&amp;memcg-&gt;css, batch);</span>
<span class="quote">&gt;  	if (batch &gt; nr_pages)</span>
<span class="quote">&gt;  		refill_stock(memcg, batch - nr_pages);</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		goto done;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If the hierarchy is above the normal consumption range,</span>
<span class="quote">&gt; @@ -4696,8 +4696,8 @@ static int mem_cgroup_do_precharge(unsigned long count)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Try a single bulk charge without reclaim first */</span>
<span class="quote">&gt; -	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_WAIT, count);</span>
<span class="quote">&gt; +	/* Try a single bulk charge without reclaim first, kswapd may wake */</span>
<span class="quote">&gt; +	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM, count);</span>
<span class="quote">&gt;  	if (!ret) {</span>
<span class="quote">&gt;  		mc.precharge += count;</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt; diff --git a/mm/mempool.c b/mm/mempool.c</span>
<span class="quote">&gt; index 2cc08de8b1db..bfd2a0dd0e18 100644</span>
<span class="quote">&gt; --- a/mm/mempool.c</span>
<span class="quote">&gt; +++ b/mm/mempool.c</span>
<span class="quote">&gt; @@ -317,13 +317,13 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	gfp_t gfp_temp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_WARN_ON_ONCE(gfp_mask &amp; __GFP_ZERO);</span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NOMEMALLOC;	/* don&#39;t allocate emergency reserves */</span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NORETRY;	/* don&#39;t loop in __alloc_pages */</span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NOWARN;	/* failures are OK */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	gfp_temp = gfp_mask &amp; ~(__GFP_WAIT|__GFP_IO);</span>
<span class="quote">&gt; +	gfp_temp = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM|__GFP_IO);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  repeat_alloc:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -346,7 +346,7 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If</span>
<span class="quote">&gt; +	 * We use gfp mask w/o direct reclaim or IO for the first round.  If</span>
<span class="quote">&gt;  	 * alloc failed with that and @pool was empty, retry immediately.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (gfp_temp != gfp_mask) {</span>
<span class="quote">&gt; @@ -355,8 +355,8 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  		goto repeat_alloc;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* We must not sleep if !__GFP_WAIT */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	/* We must not sleep if !__GFP_DIRECT_RECLAIM */</span>
<span class="quote">&gt; +	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;  		spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index eb4267107d1f..0e16c4047638 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1564,7 +1564,7 @@ static struct page *alloc_misplaced_dst_page(struct page *page,</span>
<span class="quote">&gt;  					 (GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt;  					  __GFP_THISNODE | __GFP_NOMEMALLOC |</span>
<span class="quote">&gt;  					  __GFP_NORETRY | __GFP_NOWARN) &amp;</span>
<span class="quote">&gt; -					 ~GFP_IOFS, 0);</span>
<span class="quote">&gt; +					 ~(__GFP_IO | __GFP_FS), 0);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return newpage;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 32d1cec124bc..68f961bdfdf8 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -151,12 +151,12 @@ void pm_restrict_gfp_mask(void)</span>
<span class="quote">&gt;  	WARN_ON(!mutex_is_locked(&amp;pm_mutex));</span>
<span class="quote">&gt;  	WARN_ON(saved_gfp_mask);</span>
<span class="quote">&gt;  	saved_gfp_mask = gfp_allowed_mask;</span>
<span class="quote">&gt; -	gfp_allowed_mask &amp;= ~GFP_IOFS;</span>
<span class="quote">&gt; +	gfp_allowed_mask &amp;= ~(__GFP_IO | __GFP_FS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  bool pm_suspended_storage(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if ((gfp_allowed_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +	if ((gfp_allowed_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt; -	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; @@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;  		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt;  		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt;  			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; -	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt;  		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (fmt) {</span>
<span class="quote">&gt; @@ -2915,7 +2915,6 @@ static inline int</span>
<span class="quote">&gt;  gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;</span>
<span class="quote">&gt; -	const bool atomic = !(gfp_mask &amp; (__GFP_WAIT | __GFP_NO_KSWAPD));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* __GFP_HIGH is assumed to be the same as ALLOC_HIGH to save a branch. */</span>
<span class="quote">&gt;  	BUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);</span>
<span class="quote">&gt; @@ -2924,11 +2923,11 @@ gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	 * The caller may dip into page reserves a bit more if the caller</span>
<span class="quote">&gt;  	 * cannot run direct reclaim, or if the caller has realtime scheduling</span>
<span class="quote">&gt;  	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will</span>
<span class="quote">&gt; -	 * set both ALLOC_HARDER (atomic == true) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt; +	 * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	alloc_flags |= (__force int) (gfp_mask &amp; __GFP_HIGH);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (atomic) {</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_ATOMIC) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Not worth trying to allocate harder for __GFP_NOMEMALLOC even</span>
<span class="quote">&gt;  		 * if it can&#39;t schedule.</span>
<span class="quote">&gt; @@ -2965,11 +2964,16 @@ bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	return !!(gfp_to_alloc_flags(gfp_mask) &amp; ALLOC_NO_WATERMARKS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool is_thp_gfp_mask(gfp_t gfp_mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (gfp_mask &amp; (GFP_TRANSHUGE | __GFP_KSWAPD_RECLAIM)) == GFP_TRANSHUGE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline struct page *</span>
<span class="quote">&gt;  __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  						struct alloc_context *ac)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	const gfp_t wait = gfp_mask &amp; __GFP_WAIT;</span>
<span class="quote">&gt; +	bool can_direct_reclaim = gfp_mask &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	int alloc_flags;</span>
<span class="quote">&gt;  	unsigned long pages_reclaimed = 0;</span>
<span class="quote">&gt; @@ -2990,15 +2994,23 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; +	 * We also sanity check to catch abuse of atomic reserves being used by</span>
<span class="quote">&gt; +	 * callers that are not in atomic context.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (WARN_ON_ONCE((gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==</span>
<span class="quote">&gt; +				(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt; +		gfp_mask &amp;= ~__GFP_ATOMIC;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt;  	 * If this allocation cannot block and it is for a specific node, then</span>
<span class="quote">&gt;  	 * fail early.  There&#39;s no need to wakeup kswapd or retry for a</span>
<span class="quote">&gt;  	 * speculative node-specific allocation.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !wait)</span>
<span class="quote">&gt; +	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !can_direct_reclaim)</span>
<span class="quote">&gt;  		goto nopage;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_NO_KSWAPD))</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  		wake_all_kswapds(order, ac);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -3041,8 +3053,8 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Atomic allocations - we can&#39;t balance anything */</span>
<span class="quote">&gt; -	if (!wait) {</span>
<span class="quote">&gt; +	/* Caller is not willing to reclaim, we can&#39;t balance anything */</span>
<span class="quote">&gt; +	if (!can_direct_reclaim) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * All existing users of the deprecated __GFP_NOFAIL are</span>
<span class="quote">&gt;  		 * blockable, so warn of any new users that actually allow this</span>
<span class="quote">&gt; @@ -3072,7 +3084,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  		goto got_pg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Checks for THP-specific high-order allocations */</span>
<span class="quote">&gt; -	if ((gfp_mask &amp; GFP_TRANSHUGE) == GFP_TRANSHUGE) {</span>
<span class="quote">&gt; +	if (is_thp_gfp_mask(gfp_mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If compaction is deferred for high-order allocations, it is</span>
<span class="quote">&gt;  		 * because sync compaction recently failed. If this is the case</span>
<span class="quote">&gt; @@ -3107,8 +3119,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	 * fault, so use asynchronous memory compaction for THP unless it is</span>
<span class="quote">&gt;  	 * khugepaged trying to collapse.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if ((gfp_mask &amp; GFP_TRANSHUGE) != GFP_TRANSHUGE ||</span>
<span class="quote">&gt; -						(current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt; +	if (!is_thp_gfp_mask(gfp_mask) || (current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt;  		migration_mode = MIGRATE_SYNC_LIGHT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Try direct reclaim and then allocating */</span>
<span class="quote">&gt; @@ -3179,7 +3190,7 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	lockdep_trace_alloc(gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (should_fail_alloc_page(gfp_mask, order))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="quote">&gt; index 200e22412a16..f82bdb3eb1fc 100644</span>
<span class="quote">&gt; --- a/mm/slab.c</span>
<span class="quote">&gt; +++ b/mm/slab.c</span>
<span class="quote">&gt; @@ -1030,12 +1030,12 @@ static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Construct gfp mask to allocate from a specific node but do not invoke reclaim</span>
<span class="quote">&gt; - * or warn about failures.</span>
<span class="quote">&gt; + * Construct gfp mask to allocate from a specific node but do not direct reclaim</span>
<span class="quote">&gt; + * or warn about failures. kswapd may still wake to reclaim in the background.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline gfp_t gfp_exact_node(gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2625,7 +2625,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	offset *= cachep-&gt;colour_off;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_enable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -2655,7 +2655,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	cache_init_objs(cachep, page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	check_irq_off();</span>
<span class="quote">&gt;  	spin_lock(&amp;n-&gt;list_lock);</span>
<span class="quote">&gt; @@ -2669,7 +2669,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  opps1:</span>
<span class="quote">&gt;  	kmem_freepages(cachep, page);</span>
<span class="quote">&gt;  failed:</span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2861,7 +2861,7 @@ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,</span>
<span class="quote">&gt;  static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  						gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;  #if DEBUG</span>
<span class="quote">&gt;  	kmem_flagcheck(cachep, flags);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -3049,11 +3049,11 @@ static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  			local_irq_enable();</span>
<span class="quote">&gt;  		kmem_flagcheck(cache, flags);</span>
<span class="quote">&gt;  		page = kmem_getpages(cache, local_flags, numa_mem_id());</span>
<span class="quote">&gt; -		if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  			local_irq_disable();</span>
<span class="quote">&gt;  		if (page) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt; diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="quote">&gt; index 816df0016555..a4661c59ff54 100644</span>
<span class="quote">&gt; --- a/mm/slub.c</span>
<span class="quote">&gt; +++ b/mm/slub.c</span>
<span class="quote">&gt; @@ -1263,7 +1263,7 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;  	lockdep_trace_alloc(flags);</span>
<span class="quote">&gt; -	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (should_failslab(s-&gt;object_size, flags, s-&gt;flags))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; @@ -1339,7 +1339,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;  		local_irq_enable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flags |= s-&gt;allocflags;</span>
<span class="quote">&gt; @@ -1380,7 +1380,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  			kmemcheck_mark_unallocated_pages(page, pages);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="quote">&gt; index 2faaa2976447..9ad4dcb0631c 100644</span>
<span class="quote">&gt; --- a/mm/vmalloc.c</span>
<span class="quote">&gt; +++ b/mm/vmalloc.c</span>
<span class="quote">&gt; @@ -1617,7 +1617,7 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		area-&gt;pages[i] = page;</span>
<span class="quote">&gt; -		if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  			cond_resched();</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index e950134c4b9a..837c440d60a9 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -1465,7 +1465,7 @@ static int too_many_isolated(struct zone *zone, int file,</span>
<span class="quote">&gt;  	 * won&#39;t get blocked by normal direct-reclaimers, forming a circular</span>
<span class="quote">&gt;  	 * deadlock.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if ((sc-&gt;gfp_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +	if ((sc-&gt;gfp_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;  		inactive &gt;&gt;= 3;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return isolated &gt; inactive;</span>
<span class="quote">&gt; @@ -3764,7 +3764,7 @@ int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Do not scan if the allocation should not be delayed.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt;  		return ZONE_RECLAIM_NOSCAN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/mm/zswap.c b/mm/zswap.c</span>
<span class="quote">&gt; index 2d5727baed59..26104a68c972 100644</span>
<span class="quote">&gt; --- a/mm/zswap.c</span>
<span class="quote">&gt; +++ b/mm/zswap.c</span>
<span class="quote">&gt; @@ -684,7 +684,8 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* store */</span>
<span class="quote">&gt;  	len = dlen + sizeof(struct zswap_header);</span>
<span class="quote">&gt; -	ret = zpool_malloc(zswap_pool, len, __GFP_NORETRY | __GFP_NOWARN,</span>
<span class="quote">&gt; +	ret = zpool_malloc(zswap_pool, len,</span>
<span class="quote">&gt; +		__GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM,</span>
<span class="quote">&gt;  		&amp;handle);</span>
<span class="quote">&gt;  	if (ret == -ENOSPC) {</span>
<span class="quote">&gt;  		zswap_reject_compress_poor++;</span>
<span class="quote">&gt; @@ -900,7 +901,7 @@ static void __exit zswap_debugfs_exit(void) { }</span>
<span class="quote">&gt;  **********************************/</span>
<span class="quote">&gt;  static int __init init_zswap(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt; +	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pr_info(&quot;loading zswap\n&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="quote">&gt; index b6a19ca0f99e..6f025e2544de 100644</span>
<span class="quote">&gt; --- a/net/core/skbuff.c</span>
<span class="quote">&gt; +++ b/net/core/skbuff.c</span>
<span class="quote">&gt; @@ -414,7 +414,7 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,</span>
<span class="quote">&gt;  	len += NET_SKB_PAD;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;  		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;  		if (!skb)</span>
<span class="quote">&gt;  			goto skb_fail;</span>
<span class="quote">&gt; @@ -481,7 +481,7 @@ struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,</span>
<span class="quote">&gt;  	len += NET_SKB_PAD + NET_IP_ALIGN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;  		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;  		if (!skb)</span>
<span class="quote">&gt;  			goto skb_fail;</span>
<span class="quote">&gt; @@ -4452,7 +4452,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	gfp_head = gfp_mask;</span>
<span class="quote">&gt; -	if (gfp_head &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
<span class="quote">&gt;  		gfp_head |= __GFP_REPEAT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	*errcode = -ENOBUFS;</span>
<span class="quote">&gt; @@ -4467,7 +4467,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		while (order) {</span>
<span class="quote">&gt;  			if (npages &gt;= 1 &lt;&lt; order) {</span>
<span class="quote">&gt; -				page = alloc_pages((gfp_mask &amp; ~__GFP_WAIT) |</span>
<span class="quote">&gt; +				page = alloc_pages((gfp_mask &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt;  						   __GFP_COMP |</span>
<span class="quote">&gt;  						   __GFP_NOWARN |</span>
<span class="quote">&gt;  						   __GFP_NORETRY,</span>
<span class="quote">&gt; diff --git a/net/core/sock.c b/net/core/sock.c</span>
<span class="quote">&gt; index 193901d09757..02b705cc9eb3 100644</span>
<span class="quote">&gt; --- a/net/core/sock.c</span>
<span class="quote">&gt; +++ b/net/core/sock.c</span>
<span class="quote">&gt; @@ -1879,8 +1879,10 @@ bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pfrag-&gt;offset = 0;</span>
<span class="quote">&gt;  	if (SKB_FRAG_PAGE_ORDER) {</span>
<span class="quote">&gt; -		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_WAIT) | __GFP_COMP |</span>
<span class="quote">&gt; -					  __GFP_NOWARN | __GFP_NORETRY,</span>
<span class="quote">&gt; +		/* Avoid direct reclaim but allow kswapd to wake */</span>
<span class="quote">&gt; +		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt; +					  __GFP_COMP | __GFP_NOWARN |</span>
<span class="quote">&gt; +					  __GFP_NORETRY,</span>
<span class="quote">&gt;  					  SKB_FRAG_PAGE_ORDER);</span>
<span class="quote">&gt;  		if (likely(pfrag-&gt;page)) {</span>
<span class="quote">&gt;  			pfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;</span>
<span class="quote">&gt; diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; index 67d210477863..8283d90dde74 100644</span>
<span class="quote">&gt; --- a/net/netlink/af_netlink.c</span>
<span class="quote">&gt; +++ b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; @@ -2066,7 +2066,7 @@ int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid</span>
<span class="quote">&gt;  	consume_skb(info.skb2);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (info.delivered) {</span>
<span class="quote">&gt; -		if (info.congested &amp;&amp; (allocation &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (info.congested &amp;&amp; gfpflags_allow_blocking(allocation))</span>
<span class="quote">&gt;  			yield();</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/net/rxrpc/ar-connection.c b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; index 6631f4f1e39b..3b5de4b86058 100644</span>
<span class="quote">&gt; --- a/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; +++ b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; @@ -500,7 +500,7 @@ int rxrpc_connect_call(struct rxrpc_sock *rx,</span>
<span class="quote">&gt;  		if (bundle-&gt;num_conns &gt;= 20) {</span>
<span class="quote">&gt;  			_debug(&quot;too many conns&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -			if (!(gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +			if (!gfpflags_allow_blocking(gfp)) {</span>
<span class="quote">&gt;  				_leave(&quot; = -EAGAIN&quot;);</span>
<span class="quote">&gt;  				return -EAGAIN;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; diff --git a/net/sctp/associola.c b/net/sctp/associola.c</span>
<span class="quote">&gt; index 197c3f59ecbf..75369ae8de1e 100644</span>
<span class="quote">&gt; --- a/net/sctp/associola.c</span>
<span class="quote">&gt; +++ b/net/sctp/associola.c</span>
<span class="quote">&gt; @@ -1588,7 +1588,7 @@ int sctp_assoc_lookup_laddr(struct sctp_association *asoc,</span>
<span class="quote">&gt;  /* Set an association id for a given association */</span>
<span class="quote">&gt;  int sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp);</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* If the id is already assigned, keep it. */</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.4.6</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Aug. 26, 2015, 2:45 p.m.</div>
<pre class="content">
On Tue, Aug 25, 2015 at 05:37:59PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 08/24/2015 02:09 PM, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt;__GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; &gt;spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; &gt;have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; &gt;to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; &gt;watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; &gt;were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; &gt;an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; &gt;cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; &gt;__GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; &gt;willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; &gt;that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; &gt;as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; &gt;background reclaim.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;This patch then converts a number of sites</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;o __GFP_ATOMIC is used by callers that are high priority and have memory</span>
<span class="quote">&gt; &gt;   pools for those requests. GFP_ATOMIC uses this flag.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;o Callers that have a limited mempool to guarantee forward progress use</span>
<span class="quote">&gt; &gt;   __GFP_DIRECT_RECLAIM. bio allocations fall into this category where</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      ^ __GFP_KSWAPD_RECLAIM ? (missed it previously)</span>
<span class="quote">&gt; </span>

I updated the changelog to make this clearer.
<span class="quote">
&gt; &gt;   kswapd will still be woken but atomic reserves are not used as there</span>
<span class="quote">&gt; &gt;   is a one-entry mempool to guarantee progress.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;o Callers that are checking if they are non-blocking should use the</span>
<span class="quote">&gt; &gt;   helper gfpflags_allow_blocking() where possible. This is because</span>
<span class="quote">&gt; &gt;   checking for __GFP_WAIT as was done historically now can trigger false</span>
<span class="quote">&gt; &gt;   positives. Some exceptions like dm-crypt.c exist where the code intent</span>
<span class="quote">&gt; &gt;   is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to</span>
<span class="quote">&gt; &gt;   flag manipulations.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;o Callers that built their own GFP flags instead of starting with GFP_KERNEL</span>
<span class="quote">&gt; &gt;   and friends now also need to specify __GFP_KSWAPD_RECLAIM.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; &gt;and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; &gt;In some cases it may be appropriate for them to use __GFP_HIGH.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;The second key hazard is callers that assembled their own combination of</span>
<span class="quote">&gt; &gt;GFP flags instead of starting with something like GFP_KERNEL. They may</span>
<span class="quote">&gt; &gt;now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless</span>
<span class="quote">&gt; &gt;if it&#39;s missed in most cases as other activity will wake kswapd.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for the effort!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just last few bits:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;@@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt; &gt;  		return false;</span>
<span class="quote">&gt; &gt;  	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt; &gt;  		return false;</span>
<span class="quote">&gt; &gt;-	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt;+	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt; &gt;  		return false;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIUC ignore_gfp_wait tells it to assume that reclaimers will eventually</span>
<span class="quote">&gt; succeed (for some reason?), so they shouldn&#39;t fail. Probably to focus the</span>
<span class="quote">&gt; testing on atomic allocations. But your change makes atomic allocation never</span>
<span class="quote">&gt; fail, so that goes against the knob IMHO?</span>
<span class="quote">&gt; </span>

Fair point, I&#39;ll remove the __GFP_ATOMIC check. I felt this was a sensible
but then again deliberately failing allocations makes my brain twitch a
bit. In retrospect, someone who cared should add a ignore_gfp_atomic knob.
<span class="quote">
&gt; &gt;@@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt; &gt;  		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt; &gt;  		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt; &gt;  			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; &gt;-	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt;+	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt; &gt;  		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  	if (fmt) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This caught me previously and I convinced myself that it&#39;s OK, but now I&#39;m</span>
<span class="quote">&gt; not anymore. IIUC this is to not filter nodes by mems_allowed during</span>
<span class="quote">&gt; printing, if the allocation itself wasn&#39;t limited? In that case it should</span>
<span class="quote">&gt; probably only look at __GFP_ATOMIC after this patch? As that&#39;s the only</span>
<span class="quote">&gt; thing that determines ALLOC_CPUSET.</span>
<span class="quote">&gt; I don&#39;t know where in_interrupt() comes from, but it was probably considered</span>
<span class="quote">&gt; in the past, as can be seen in zlc_setup()?</span>
<span class="quote">&gt; </span>

I assumed the in_interrupt() thing was simply because cpusets were the
primary means of limiting allocations of interest to the author at the
time.

I guess now that I think about it more that a more sensible check would
be against __GFP_DIRECT_RECLAIM because that covers the interesting
cases.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Aug. 26, 2015, 4:24 p.m.</div>
<pre class="content">
On 08/26/2015 04:45 PM, Mel Gorman wrote:
<span class="quote">&gt; On Tue, Aug 25, 2015 at 05:37:59PM +0200, Vlastimil Babka wrote:</span>
<span class="quote">&gt;&gt;&gt; @@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;&gt;&gt;   		return false;</span>
<span class="quote">&gt;&gt;&gt;   	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt;&gt;&gt;   		return false;</span>
<span class="quote">&gt;&gt;&gt; -	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt;&gt;&gt; +	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt;&gt;&gt;   		return false;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; IIUC ignore_gfp_wait tells it to assume that reclaimers will eventually</span>
<span class="quote">&gt;&gt; succeed (for some reason?), so they shouldn&#39;t fail. Probably to focus the</span>
<span class="quote">&gt;&gt; testing on atomic allocations. But your change makes atomic allocation never</span>
<span class="quote">&gt;&gt; fail, so that goes against the knob IMHO?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Fair point, I&#39;ll remove the __GFP_ATOMIC check. I felt this was a sensible</span>
<span class="quote">&gt; but then again deliberately failing allocations makes my brain twitch a</span>
<span class="quote">&gt; bit. In retrospect, someone who cared should add a ignore_gfp_atomic knob.</span>

Thanks.
<span class="quote">
&gt;&gt;&gt; @@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;&gt;&gt;   		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt;&gt;&gt;   		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt;&gt;&gt;   			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;&gt;&gt; -	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt;&gt;&gt; +	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt;&gt;&gt;   		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   	if (fmt) {</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This caught me previously and I convinced myself that it&#39;s OK, but now I&#39;m</span>
<span class="quote">&gt;&gt; not anymore. IIUC this is to not filter nodes by mems_allowed during</span>
<span class="quote">&gt;&gt; printing, if the allocation itself wasn&#39;t limited? In that case it should</span>
<span class="quote">&gt;&gt; probably only look at __GFP_ATOMIC after this patch? As that&#39;s the only</span>
<span class="quote">&gt;&gt; thing that determines ALLOC_CPUSET.</span>
<span class="quote">&gt;&gt; I don&#39;t know where in_interrupt() comes from, but it was probably considered</span>
<span class="quote">&gt;&gt; in the past, as can be seen in zlc_setup()?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I assumed the in_interrupt() thing was simply because cpusets were the</span>
<span class="quote">&gt; primary means of limiting allocations of interest to the author at the</span>
<span class="quote">&gt; time.</span>

IIUC this hunk is unrelated to the previous one - not about limiting 
allocations, but printing allocation warnings. Which includes the state 
of nodes where the allocation was allowed to try. And 
~SHOW_MEM_FILTER_NODES means it was allowed everywhere, so the printing 
won&#39;t filter by mems_allowed.
<span class="quote">
&gt; I guess now that I think about it more that a more sensible check would</span>
<span class="quote">&gt; be against __GFP_DIRECT_RECLAIM because that covers the interesting</span>
<span class="quote">&gt; cases.</span>

I think the most robust check would be to rely on what was already 
prepared by gfp_to_alloc_flags(), instead of repeating it here. So add 
alloc_flags parameter to warn_alloc_failed(), and drop the filter when
- ALLOC_CPUSET is not set, as that disables the cpuset checks
- ALLOC_NO_WATERMARKS is set, as that allows calling
   __alloc_pages_high_priority() attempt which ignores cpusets

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Aug. 26, 2015, 6:10 p.m.</div>
<pre class="content">
On Wed, Aug 26, 2015 at 06:24:34PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 08/26/2015 04:45 PM, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt;On Tue, Aug 25, 2015 at 05:37:59PM +0200, Vlastimil Babka wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;@@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt; &gt;&gt;&gt;  		return false;</span>
<span class="quote">&gt; &gt;&gt;&gt;  	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt; &gt;&gt;&gt;  		return false;</span>
<span class="quote">&gt; &gt;&gt;&gt;-	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt;&gt;&gt;+	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt; &gt;&gt;&gt;  		return false;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;  	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;IIUC ignore_gfp_wait tells it to assume that reclaimers will eventually</span>
<span class="quote">&gt; &gt;&gt;succeed (for some reason?), so they shouldn&#39;t fail. Probably to focus the</span>
<span class="quote">&gt; &gt;&gt;testing on atomic allocations. But your change makes atomic allocation never</span>
<span class="quote">&gt; &gt;&gt;fail, so that goes against the knob IMHO?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Fair point, I&#39;ll remove the __GFP_ATOMIC check. I felt this was a sensible</span>
<span class="quote">&gt; &gt;but then again deliberately failing allocations makes my brain twitch a</span>
<span class="quote">&gt; &gt;bit. In retrospect, someone who cared should add a ignore_gfp_atomic knob.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;&gt;@@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt; &gt;&gt;&gt;  		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt; &gt;&gt;&gt;  		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt; &gt;&gt;&gt;  			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; &gt;&gt;&gt;-	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt;&gt;&gt;+	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt; &gt;&gt;&gt;  		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;  	if (fmt) {</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;This caught me previously and I convinced myself that it&#39;s OK, but now I&#39;m</span>
<span class="quote">&gt; &gt;&gt;not anymore. IIUC this is to not filter nodes by mems_allowed during</span>
<span class="quote">&gt; &gt;&gt;printing, if the allocation itself wasn&#39;t limited? In that case it should</span>
<span class="quote">&gt; &gt;&gt;probably only look at __GFP_ATOMIC after this patch? As that&#39;s the only</span>
<span class="quote">&gt; &gt;&gt;thing that determines ALLOC_CPUSET.</span>
<span class="quote">&gt; &gt;&gt;I don&#39;t know where in_interrupt() comes from, but it was probably considered</span>
<span class="quote">&gt; &gt;&gt;in the past, as can be seen in zlc_setup()?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;I assumed the in_interrupt() thing was simply because cpusets were the</span>
<span class="quote">&gt; &gt;primary means of limiting allocations of interest to the author at the</span>
<span class="quote">&gt; &gt;time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIUC this hunk is unrelated to the previous one - not about limiting</span>
<span class="quote">&gt; allocations, but printing allocation warnings. Which includes the state of</span>
<span class="quote">&gt; nodes where the allocation was allowed to try. And ~SHOW_MEM_FILTER_NODES</span>
<span class="quote">&gt; means it was allowed everywhere, so the printing won&#39;t filter by</span>
<span class="quote">&gt; mems_allowed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;I guess now that I think about it more that a more sensible check would</span>
<span class="quote">&gt; &gt;be against __GFP_DIRECT_RECLAIM because that covers the interesting</span>
<span class="quote">&gt; &gt;cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the most robust check would be to rely on what was already prepared</span>
<span class="quote">&gt; by gfp_to_alloc_flags(), instead of repeating it here. So add alloc_flags</span>
<span class="quote">&gt; parameter to warn_alloc_failed(), and drop the filter when</span>
<span class="quote">&gt; - ALLOC_CPUSET is not set, as that disables the cpuset checks</span>
<span class="quote">&gt; - ALLOC_NO_WATERMARKS is set, as that allows calling</span>
<span class="quote">&gt;   __alloc_pages_high_priority() attempt which ignores cpusets</span>
<span class="quote">&gt; </span>

warn_alloc_failed is used outside of page_alloc.c in a context that does
not have alloc_flags. It could be extended to take an extra parameter
that is ALLOC_CPUSET for the other callers or else split it into
__warn_alloc_failed (takes alloc_flags parameter) and warn_alloc_failed
(calls __warn_alloc_failed with ALLOC_CPUSET) but is it really worth it?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Aug. 27, 2015, 9:18 a.m.</div>
<pre class="content">
On 08/26/2015 08:10 PM, Mel Gorman wrote:
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think the most robust check would be to rely on what was already prepared</span>
<span class="quote">&gt;&gt; by gfp_to_alloc_flags(), instead of repeating it here. So add alloc_flags</span>
<span class="quote">&gt;&gt; parameter to warn_alloc_failed(), and drop the filter when</span>
<span class="quote">&gt;&gt; - ALLOC_CPUSET is not set, as that disables the cpuset checks</span>
<span class="quote">&gt;&gt; - ALLOC_NO_WATERMARKS is set, as that allows calling</span>
<span class="quote">&gt;&gt;    __alloc_pages_high_priority() attempt which ignores cpusets</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; warn_alloc_failed is used outside of page_alloc.c in a context that does</span>
<span class="quote">&gt; not have alloc_flags. It could be extended to take an extra parameter</span>
<span class="quote">&gt; that is ALLOC_CPUSET for the other callers or else split it into</span>
<span class="quote">&gt; __warn_alloc_failed (takes alloc_flags parameter) and warn_alloc_failed</span>
<span class="quote">&gt; (calls __warn_alloc_failed with ALLOC_CPUSET) but is it really worth it?</span>

Probably not. Testing lack of __GFP_DIRECT_RECLAIM is good enough until 
somebody cares more.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=39721">Joonsoo Kim</a> - Sept. 8, 2015, 6:49 a.m.</div>
<pre class="content">
2015-08-24 21:09 GMT+09:00 Mel Gorman &lt;mgorman@techsingularity.net&gt;:
<span class="quote">&gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; background reclaim.</span>

Hello, Mel.

I think that it is better to do one thing at one patch.
To distinguish real atomic, we just need to introduce __GFP_ATOMIC and
make GFP_ATOMIC to __GFP_ATOMIC | GFP_HARDER and change related
things. __GFP_WAIT changes isn&#39;t needed at all for this purpose. It can
reduce patch size and provides more good bisectability.

And, I don&#39;t think that introducing __GFP_KSWAPD_RECLAIM is good thing.
Basically, kswapd reclaim should be enforced. New flag makes user who manually
manipulate gfp flag more difficult. Without this change, your second hazard will
be disappeared although it is almost harmless.

And, I doubt that this big one shot change is preferable. AFAIK, even if changes
are one to one mapping and no functional difference, each one is made by
one patch and send it to correct maintainer. I guess there is some difficulty
in this patch to do like this, but, it could. Isn&#39;t it?

Some nitpicks are below.
<span class="quote">
&gt;</span>
<span class="quote">&gt; This patch then converts a number of sites</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o __GFP_ATOMIC is used by callers that are high priority and have memory</span>
<span class="quote">&gt;   pools for those requests. GFP_ATOMIC uses this flag.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that have a limited mempool to guarantee forward progress use</span>
<span class="quote">&gt;   __GFP_DIRECT_RECLAIM. bio allocations fall into this category where</span>
<span class="quote">&gt;   kswapd will still be woken but atomic reserves are not used as there</span>
<span class="quote">&gt;   is a one-entry mempool to guarantee progress.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that are checking if they are non-blocking should use the</span>
<span class="quote">&gt;   helper gfpflags_allow_blocking() where possible. This is because</span>
<span class="quote">&gt;   checking for __GFP_WAIT as was done historically now can trigger false</span>
<span class="quote">&gt;   positives. Some exceptions like dm-crypt.c exist where the code intent</span>
<span class="quote">&gt;   is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to</span>
<span class="quote">&gt;   flag manipulations.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; o Callers that built their own GFP flags instead of starting with GFP_KERNEL</span>
<span class="quote">&gt;   and friends now also need to specify __GFP_KSWAPD_RECLAIM.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; In some cases it may be appropriate for them to use __GFP_HIGH.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The second key hazard is callers that assembled their own combination of</span>
<span class="quote">&gt; GFP flags instead of starting with something like GFP_KERNEL. They may</span>
<span class="quote">&gt; now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless</span>
<span class="quote">&gt; if it&#39;s missed in most cases as other activity will wake kswapd.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/vm/balance                           | 14 ++++---</span>
<span class="quote">&gt;  arch/arm/mm/dma-mapping.c                          |  4 +-</span>
<span class="quote">&gt;  arch/arm/xen/mm.c                                  |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c                        |  4 +-</span>
<span class="quote">&gt;  arch/x86/kernel/pci-dma.c                          |  2 +-</span>
<span class="quote">&gt;  block/bio.c                                        | 26 ++++++------</span>
<span class="quote">&gt;  block/blk-core.c                                   | 16 ++++----</span>
<span class="quote">&gt;  block/blk-ioc.c                                    |  2 +-</span>
<span class="quote">&gt;  block/blk-mq-tag.c                                 |  2 +-</span>
<span class="quote">&gt;  block/blk-mq.c                                     |  8 ++--</span>
<span class="quote">&gt;  block/cfq-iosched.c                                |  4 +-</span>
<span class="quote">&gt;  drivers/block/drbd/drbd_receiver.c                 |  3 +-</span>
<span class="quote">&gt;  drivers/block/osdblk.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/connector/connector.c                      |  3 +-</span>
<span class="quote">&gt;  drivers/firewire/core-cdev.c                       |  2 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/i915/i915_gem.c                    |  2 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/sa_query.c                 |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/amd_iommu.c                          |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/intel-iommu.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/md/dm-crypt.c                              |  6 +--</span>
<span class="quote">&gt;  drivers/md/dm-kcopyd.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c     |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2.c         |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/tw68/tw68-video.c                |  2 +-</span>
<span class="quote">&gt;  drivers/mtd/mtdcore.c                              |  3 +-</span>
<span class="quote">&gt;  drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c    |  2 +-</span>
<span class="quote">&gt;  drivers/staging/android/ion/ion_system_heap.c      |  2 +-</span>
<span class="quote">&gt;  .../lustre/include/linux/libcfs/libcfs_private.h   |  2 +-</span>
<span class="quote">&gt;  drivers/usb/host/u132-hcd.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/video/fbdev/vermilion/vermilion.c          |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/disk-io.c                                 |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/extent_io.c                               | 14 +++----</span>
<span class="quote">&gt;  fs/btrfs/volumes.c                                 |  4 +-</span>
<span class="quote">&gt;  fs/ext3/super.c                                    |  2 +-</span>
<span class="quote">&gt;  fs/ext4/super.c                                    |  2 +-</span>
<span class="quote">&gt;  fs/fscache/cookie.c                                |  2 +-</span>
<span class="quote">&gt;  fs/fscache/page.c                                  |  6 +--</span>
<span class="quote">&gt;  fs/jbd/transaction.c                               |  4 +-</span>
<span class="quote">&gt;  fs/jbd2/transaction.c                              |  4 +-</span>
<span class="quote">&gt;  fs/nfs/file.c                                      |  6 +--</span>
<span class="quote">&gt;  fs/xfs/xfs_qm.c                                    |  2 +-</span>
<span class="quote">&gt;  include/linux/gfp.h                                | 46 ++++++++++++++++------</span>
<span class="quote">&gt;  include/linux/skbuff.h                             |  6 +--</span>
<span class="quote">&gt;  include/net/sock.h                                 |  2 +-</span>
<span class="quote">&gt;  include/trace/events/gfpflags.h                    |  5 ++-</span>
<span class="quote">&gt;  kernel/audit.c                                     |  6 +--</span>
<span class="quote">&gt;  kernel/locking/lockdep.c                           |  2 +-</span>
<span class="quote">&gt;  kernel/power/snapshot.c                            |  2 +-</span>
<span class="quote">&gt;  kernel/smp.c                                       |  2 +-</span>
<span class="quote">&gt;  lib/idr.c                                          |  4 +-</span>
<span class="quote">&gt;  lib/radix-tree.c                                   | 10 ++---</span>
<span class="quote">&gt;  mm/backing-dev.c                                   |  2 +-</span>
<span class="quote">&gt;  mm/dmapool.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c                                    |  8 ++--</span>
<span class="quote">&gt;  mm/mempool.c                                       | 10 ++---</span>
<span class="quote">&gt;  mm/migrate.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/page_alloc.c                                    | 43 ++++++++++++--------</span>
<span class="quote">&gt;  mm/slab.c                                          | 18 ++++-----</span>
<span class="quote">&gt;  mm/slub.c                                          |  6 +--</span>
<span class="quote">&gt;  mm/vmalloc.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/vmscan.c                                        |  4 +-</span>
<span class="quote">&gt;  mm/zswap.c                                         |  5 ++-</span>
<span class="quote">&gt;  net/core/skbuff.c                                  |  8 ++--</span>
<span class="quote">&gt;  net/core/sock.c                                    |  6 ++-</span>
<span class="quote">&gt;  net/netlink/af_netlink.c                           |  2 +-</span>
<span class="quote">&gt;  net/rxrpc/ar-connection.c                          |  2 +-</span>
<span class="quote">&gt;  net/sctp/associola.c                               |  2 +-</span>
<span class="quote">&gt;  67 files changed, 211 insertions(+), 173 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/Documentation/vm/balance b/Documentation/vm/balance</span>
<span class="quote">&gt; index c46e68cf9344..964595481af6 100644</span>
<span class="quote">&gt; --- a/Documentation/vm/balance</span>
<span class="quote">&gt; +++ b/Documentation/vm/balance</span>
<span class="quote">&gt; @@ -1,12 +1,14 @@</span>
<span class="quote">&gt;  Started Jan 2000 by Kanoj Sarcar &lt;kanoj@sgi.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -Memory balancing is needed for non __GFP_WAIT as well as for non</span>
<span class="quote">&gt; -__GFP_IO allocations.</span>
<span class="quote">&gt; +Memory balancing is needed for !__GFP_ATOMIC and !__GFP_KSWAPD_RECLAIM as</span>
<span class="quote">&gt; +well as for non __GFP_IO allocations.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -There are two reasons to be requesting non __GFP_WAIT allocations:</span>
<span class="quote">&gt; -the caller can not sleep (typically intr context), or does not want</span>
<span class="quote">&gt; -to incur cost overheads of page stealing and possible swap io for</span>
<span class="quote">&gt; -whatever reasons.</span>
<span class="quote">&gt; +The first reason why a caller may avoid reclaim is that the caller can not</span>
<span class="quote">&gt; +sleep due to holding a spinlock or is in interrupt context. The second may</span>
<span class="quote">&gt; +be that the caller is willing to fail the allocation without incurring the</span>
<span class="quote">&gt; +overhead of page reclaim. This may happen for opportunistic high-order</span>
<span class="quote">&gt; +allocation requests that have order-0 fallback options. In such cases,</span>
<span class="quote">&gt; +the caller may also wish to avoid waking kswapd.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  __GFP_IO allocation requests are made to prevent file system deadlocks.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; index cba12f34ff77..f999f0987a3e 100644</span>
<span class="quote">&gt; --- a/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -650,7 +650,7 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (is_coherent || nommu())</span>
<span class="quote">&gt;                 addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);</span>
<span class="quote">&gt; -       else if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       else if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;                 addr = __alloc_from_pool(size, &amp;page);</span>
<span class="quote">&gt;         else if (!dev_get_cma_area(dev))</span>
<span class="quote">&gt;                 addr = __alloc_remap_buffer(dev, size, gfp, prot, &amp;page, caller, want_vaddr);</span>
<span class="quote">&gt; @@ -1369,7 +1369,7 @@ static void *arm_iommu_alloc_attrs(struct device *dev, size_t size,</span>
<span class="quote">&gt;         *handle = DMA_ERROR_CODE;</span>
<span class="quote">&gt;         size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;                 return __iommu_alloc_atomic(dev, size, handle);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; index 03e75fef15b8..86809bd2026d 100644</span>
<span class="quote">&gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@</span>
<span class="quote">&gt;  unsigned long xen_get_swiotlb_free_pages(unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct memblock_region *reg;</span>
<span class="quote">&gt; -       gfp_t flags = __GFP_NOWARN;</span>
<span class="quote">&gt; +       gfp_t flags = __GFP_NOWARN|___GFP_KSWAPD_RECLAIM;</span>

Please use __XXX rather than ___XXX.
<span class="quote">
&gt;         for_each_memblock(memory, reg) {</span>
<span class="quote">&gt;                 if (reg-&gt;base &lt; (phys_addr_t)0xffffffff) {</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index d16a1cead23f..1f10b2503af8 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -100,7 +100,7 @@ static void *__dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;         if (IS_ENABLED(CONFIG_ZONE_DMA) &amp;&amp;</span>
<span class="quote">&gt;             dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(32))</span>
<span class="quote">&gt;                 flags |= GFP_DMA;</span>
<span class="quote">&gt; -       if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; (flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;                 struct page *page;</span>
<span class="quote">&gt;                 void *addr;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -147,7 +147,7 @@ static void *__dma_alloc(struct device *dev, size_t size,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!coherent &amp;&amp; !(flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;                 struct page *page = NULL;</span>
<span class="quote">&gt;                 void *addr = __alloc_from_pool(size, &amp;page, flags);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; index 353972c1946c..0310e73e6b57 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; @@ -101,7 +101,7 @@ void *dma_generic_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;         page = NULL;</span>
<span class="quote">&gt;         /* CMA can be used only in the context which permits sleeping */</span>
<span class="quote">&gt; -       if (flag &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(flag)) {</span>
<span class="quote">&gt;                 page = dma_alloc_from_contiguous(dev, count, get_order(size));</span>
<span class="quote">&gt;                 if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="quote">&gt;                         dma_release_from_contiguous(dev, page, count);</span>
<span class="quote">&gt; diff --git a/block/bio.c b/block/bio.c</span>
<span class="quote">&gt; index d6e5ba3399f0..fbc558b50e67 100644</span>
<span class="quote">&gt; --- a/block/bio.c</span>
<span class="quote">&gt; +++ b/block/bio.c</span>
<span class="quote">&gt; @@ -211,7 +211,7 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;                 bvl = mempool_alloc(pool, gfp_mask);</span>
<span class="quote">&gt;         } else {</span>
<span class="quote">&gt;                 struct biovec_slab *bvs = bvec_slabs + *idx;</span>
<span class="quote">&gt; -               gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_WAIT | __GFP_IO);</span>
<span class="quote">&gt; +               gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM | __GFP_IO);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * Make this allocation restricted and don&#39;t dump info on</span>
<span class="quote">&gt; @@ -221,11 +221,11 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;                 __gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt; -                * Try a slab allocation. If this fails and __GFP_WAIT</span>
<span class="quote">&gt; +                * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM</span>
<span class="quote">&gt;                  * is set, retry with the 1-entry mempool</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt;                 bvl = kmem_cache_alloc(bvs-&gt;slab, __gfp_mask);</span>
<span class="quote">&gt; -               if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_WAIT))) {</span>
<span class="quote">&gt; +               if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))) {</span>
<span class="quote">&gt;                         *idx = BIOVEC_MAX_IDX;</span>
<span class="quote">&gt;                         goto fallback;</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt; @@ -393,12 +393,12 @@ static void punt_bios_to_rescuer(struct bio_set *bs)</span>
<span class="quote">&gt;   *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is</span>
<span class="quote">&gt;   *   backed by the @bs&#39;s mempool.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be</span>
<span class="quote">&gt; - *   able to allocate a bio. This is due to the mempool guarantees. To make this</span>
<span class="quote">&gt; - *   work, callers must never allocate more than 1 bio at a time from this pool.</span>
<span class="quote">&gt; - *   Callers that need to allocate more than 1 bio must always submit the</span>
<span class="quote">&gt; - *   previously allocated bio for IO before attempting to allocate a new one.</span>
<span class="quote">&gt; - *   Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt; + *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will</span>
<span class="quote">&gt; + *   always be able to allocate a bio. This is due to the mempool guarantees.</span>
<span class="quote">&gt; + *   To make this work, callers must never allocate more than 1 bio at a time</span>
<span class="quote">&gt; + *   from this pool. Callers that need to allocate more than 1 bio must always</span>
<span class="quote">&gt; + *   submit the previously allocated bio for IO before attempting to allocate</span>
<span class="quote">&gt; + *   a new one. Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *   Note that when running under generic_make_request() (i.e. any block</span>
<span class="quote">&gt;   *   driver), bios are not submitted until after you return - see the code in</span>
<span class="quote">&gt; @@ -457,13 +457,13 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
<span class="quote">&gt;                  * We solve this, and guarantee forward progress, with a rescuer</span>
<span class="quote">&gt;                  * workqueue per bio_set. If we go to allocate and there are</span>
<span class="quote">&gt;                  * bios on current-&gt;bio_list, we first try the allocation</span>
<span class="quote">&gt; -                * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="quote">&gt; -                * would be blocking to the rescuer workqueue before we retry</span>
<span class="quote">&gt; -                * with the original gfp_flags.</span>
<span class="quote">&gt; +                * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="quote">&gt; +                * bios we would be blocking to the rescuer workqueue before</span>
<span class="quote">&gt; +                * we retry with the original gfp_flags.</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="quote">&gt; -                       gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +                       gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>

How about introduce helper function to mask out __GFP_DIRECT_RECLAIM?
It can be used many places.
<span class="quote">
&gt;                 p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);</span>
<span class="quote">&gt;                 if (!p &amp;&amp; gfp_mask != saved_gfp) {</span>
<span class="quote">&gt; diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="quote">&gt; index 627ed0c593fb..e3605acaaffc 100644</span>
<span class="quote">&gt; --- a/block/blk-core.c</span>
<span class="quote">&gt; +++ b/block/blk-core.c</span>
<span class="quote">&gt; @@ -1156,8 +1156,8 @@ static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
<span class="quote">&gt;   * @bio: bio to allocate request for (can be %NULL)</span>
<span class="quote">&gt;   * @gfp_mask: allocation mask</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="quote">&gt; - * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; + * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="quote">&gt; + * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Must be called with @q-&gt;queue_lock held and,</span>
<span class="quote">&gt;   * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.</span>
<span class="quote">&gt; @@ -1177,7 +1177,7 @@ static struct request *get_request(struct request_queue *q, int rw_flags,</span>
<span class="quote">&gt;         if (!IS_ERR(rq))</span>
<span class="quote">&gt;                 return rq;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt;                 blk_put_rl(rl);</span>
<span class="quote">&gt;                 return rq;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; @@ -1255,11 +1255,11 @@ EXPORT_SYMBOL(blk_get_request);</span>
<span class="quote">&gt;   * BUG.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * WARNING: When allocating/cloning a bio-chain, careful consideration should be</span>
<span class="quote">&gt; - * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="quote">&gt; - * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="quote">&gt; - * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="quote">&gt; - * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="quote">&gt; - * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; + * given to how you allocate bios. In particular, you cannot use</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="quote">&gt; + * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="quote">&gt; + * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="quote">&gt; + * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt;   * If possible a big IO should be split into smaller parts when allocation</span>
<span class="quote">&gt;   * fails. Partial allocation should not be an error, or you risk a live-lock.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="quote">&gt; index 1a27f45ec776..381cb50a673c 100644</span>
<span class="quote">&gt; --- a/block/blk-ioc.c</span>
<span class="quote">&gt; +++ b/block/blk-ioc.c</span>
<span class="quote">&gt; @@ -289,7 +289,7 @@ struct io_context *get_task_io_context(struct task_struct *task,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct io_context *ioc;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt;                 task_lock(task);</span>
<span class="quote">&gt; diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="quote">&gt; index 9b6e28830b82..a8b46659ce4e 100644</span>
<span class="quote">&gt; --- a/block/blk-mq-tag.c</span>
<span class="quote">&gt; +++ b/block/blk-mq-tag.c</span>
<span class="quote">&gt; @@ -264,7 +264,7 @@ static int bt_get(struct blk_mq_alloc_data *data,</span>
<span class="quote">&gt;         if (tag != -1)</span>
<span class="quote">&gt;                 return tag;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
<span class="quote">&gt;                 return -1;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         bs = bt_wait_ptr(bt, hctx);</span>
<span class="quote">&gt; diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="quote">&gt; index 7d842db59699..7d80379d7a38 100644</span>
<span class="quote">&gt; --- a/block/blk-mq.c</span>
<span class="quote">&gt; +++ b/block/blk-mq.c</span>
<span class="quote">&gt; @@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
<span class="quote">&gt;                 if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))</span>
<span class="quote">&gt;                         return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -               if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +               if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;                         return -EBUSY;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 ret = wait_event_interruptible(q-&gt;mq_freeze_wq,</span>
<span class="quote">&gt; @@ -261,11 +261,11 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;         hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt; -       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="quote">&gt; +       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
<span class="quote">&gt;                         reserved, ctx, hctx);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt; -       if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;                 __blk_mq_run_hw_queue(hctx);</span>
<span class="quote">&gt;                 blk_mq_put_ctx(ctx);</span>

Is there any reason not to use gfpflags_allow_nonblocking() here?
There are some places not using this helper and reason isn&#39;t
specified.

Thanks.
<span class="quote">
&gt; @@ -1221,7 +1221,7 @@ static struct request *blk_mq_map_request(struct request_queue *q,</span>
<span class="quote">&gt;                 ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;                 hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt;                 blk_mq_set_alloc_data(&amp;alloc_data, q,</span>
<span class="quote">&gt; -                               __GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);</span>
<span class="quote">&gt; +                               __GFP_WAIT|__GFP_HIGH, false, ctx, hctx);</span>
<span class="quote">&gt;                 rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt;                 ctx = alloc_data.ctx;</span>
<span class="quote">&gt;                 hctx = alloc_data.hctx;</span>
<span class="quote">&gt; diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c</span>
<span class="quote">&gt; index c62bb2e650b8..ecd1d1b61382 100644</span>
<span class="quote">&gt; --- a/block/cfq-iosched.c</span>
<span class="quote">&gt; +++ b/block/cfq-iosched.c</span>
<span class="quote">&gt; @@ -3674,7 +3674,7 @@ cfq_find_alloc_queue(struct cfq_data *cfqd, bool is_sync, struct cfq_io_cq *cic,</span>
<span class="quote">&gt;                 if (new_cfqq) {</span>
<span class="quote">&gt;                         cfqq = new_cfqq;</span>
<span class="quote">&gt;                         new_cfqq = NULL;</span>
<span class="quote">&gt; -               } else if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +               } else if (gfpflags_allow_blocking(gfp_mask)) {</span>
<span class="quote">&gt;                         rcu_read_unlock();</span>
<span class="quote">&gt;                         spin_unlock_irq(cfqd-&gt;queue-&gt;queue_lock);</span>
<span class="quote">&gt;                         new_cfqq = kmem_cache_alloc_node(cfq_pool,</span>
<span class="quote">&gt; @@ -4289,7 +4289,7 @@ cfq_set_request(struct request_queue *q, struct request *rq, struct bio *bio,</span>
<span class="quote">&gt;         const bool is_sync = rq_is_sync(rq);</span>
<span class="quote">&gt;         struct cfq_queue *cfqq;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         spin_lock_irq(q-&gt;queue_lock);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/block/drbd/drbd_receiver.c b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; index c097909c589c..b4b5680ac6ad 100644</span>
<span class="quote">&gt; --- a/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; +++ b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; @@ -357,7 +357,8 @@ drbd_alloc_peer_req(struct drbd_peer_device *peer_device, u64 id, sector_t secto</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (has_payload &amp;&amp; data_size) {</span>
<span class="quote">&gt; -               page = drbd_alloc_pages(peer_device, nr_pages, (gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +               page = drbd_alloc_pages(peer_device, nr_pages,</span>
<span class="quote">&gt; +                                       gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;                 if (!page)</span>
<span class="quote">&gt;                         goto fail;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; diff --git a/drivers/block/osdblk.c b/drivers/block/osdblk.c</span>
<span class="quote">&gt; index e22942596207..1b709a4e3b5e 100644</span>
<span class="quote">&gt; --- a/drivers/block/osdblk.c</span>
<span class="quote">&gt; +++ b/drivers/block/osdblk.c</span>
<span class="quote">&gt; @@ -271,7 +271,7 @@ static struct bio *bio_chain_clone(struct bio *old_chain, gfp_t gfpmask)</span>
<span class="quote">&gt;                         goto err_out;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 tmp-&gt;bi_bdev = NULL;</span>
<span class="quote">&gt; -               gfpmask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +               gfpmask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;                 tmp-&gt;bi_next = NULL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if (!new_chain)</span>
<span class="quote">&gt; diff --git a/drivers/connector/connector.c b/drivers/connector/connector.c</span>
<span class="quote">&gt; index 30f522848c73..d7373ca69c99 100644</span>
<span class="quote">&gt; --- a/drivers/connector/connector.c</span>
<span class="quote">&gt; +++ b/drivers/connector/connector.c</span>
<span class="quote">&gt; @@ -124,7 +124,8 @@ int cn_netlink_send_mult(struct cn_msg *msg, u16 len, u32 portid, u32 __group,</span>
<span class="quote">&gt;         if (group)</span>
<span class="quote">&gt;                 return netlink_broadcast(dev-&gt;nls, skb, portid, group,</span>
<span class="quote">&gt;                                          gfp_mask);</span>
<span class="quote">&gt; -       return netlink_unicast(dev-&gt;nls, skb, portid, !(gfp_mask&amp;__GFP_WAIT));</span>
<span class="quote">&gt; +       return netlink_unicast(dev-&gt;nls, skb, portid,</span>
<span class="quote">&gt; +                       !gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(cn_netlink_send_mult);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/firewire/core-cdev.c b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; index 2a3973a7c441..36a7c2d89a01 100644</span>
<span class="quote">&gt; --- a/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; +++ b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; @@ -486,7 +486,7 @@ static int ioctl_get_info(struct client *client, union ioctl_arg *arg)</span>
<span class="quote">&gt;  static int add_client_resource(struct client *client,</span>
<span class="quote">&gt;                                struct client_resource *resource, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;         unsigned long flags;</span>
<span class="quote">&gt;         int ret;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; index 52b446b27b4d..c2b45081c5ab 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; @@ -2225,7 +2225,7 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         mapping = file_inode(obj-&gt;base.filp)-&gt;i_mapping;</span>
<span class="quote">&gt;         gfp = mapping_gfp_mask(mapping);</span>
<span class="quote">&gt; -       gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +       gfp |= __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;         gfp &amp;= ~(__GFP_IO | __GFP_WAIT);</span>
<span class="quote">&gt;         sg = st-&gt;sgl;</span>
<span class="quote">&gt;         st-&gt;nents = 0;</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; index ca919f429666..7474d79ffac0 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; @@ -619,7 +619,7 @@ static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;         unsigned long flags;</span>
<span class="quote">&gt;         int ret, id;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; index 658ee39e6569..95d4c70dc7b1 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; @@ -2755,7 +2755,7 @@ static void *alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));</span>
<span class="quote">&gt;         if (!page) {</span>
<span class="quote">&gt; -               if (!(flag &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +               if (!gfpflags_allow_blocking(flag))</span>
<span class="quote">&gt;                         return NULL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 page = dma_alloc_from_contiguous(dev, size &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; index 0649b94f5958..f77becf3d8d8 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; @@ -3566,7 +3566,7 @@ static void *intel_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;                         flags |= GFP_DMA32;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (flags &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;                 unsigned int count = size &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 page = dma_alloc_from_contiguous(dev, count, order);</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; index 0f48fed44a17..6dda08385309 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; @@ -993,7 +993,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;         struct bio_vec *bvec;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -       if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;                 mutex_lock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, cc-&gt;bs);</span>
<span class="quote">&gt; @@ -1009,7 +1009,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;                 if (!page) {</span>
<span class="quote">&gt;                         crypt_free_buffer_pages(cc, clone);</span>
<span class="quote">&gt;                         bio_put(clone);</span>
<span class="quote">&gt; -                       gfp_mask |= __GFP_WAIT;</span>
<span class="quote">&gt; +                       gfp_mask |= __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;                         goto retry;</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -1026,7 +1026,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  return_clone:</span>
<span class="quote">&gt; -       if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;                 mutex_unlock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return clone;</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; index 3a7cade5e27d..1452ed9aacb4 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; @@ -244,7 +244,7 @@ static int kcopyd_get_pages(struct dm_kcopyd_client *kc,</span>
<span class="quote">&gt;         *pages = NULL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt; -               pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY);</span>
<span class="quote">&gt; +               pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;                 if (unlikely(!pl)) {</span>
<span class="quote">&gt;                         /* Use reserved pages */</span>
<span class="quote">&gt;                         pl = kc-&gt;pages;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; index 53fff5425c13..fb2cb4bdc0c1 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; @@ -1291,7 +1291,7 @@ static struct solo_enc_dev *solo_enc_alloc(struct solo_dev *solo_dev,</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.ops = &amp;solo_enc_video_qops;</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.drv_priv = solo_enc;</span>
<span class="quote">&gt; -       solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +       solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;         solo_enc-&gt;vidq.lock = &amp;solo_enc-&gt;lock;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2.c b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; index 63ae8a61f603..bde77b22340c 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; @@ -675,7 +675,7 @@ int solo_v4l2_init(struct solo_dev *solo_dev, unsigned nr)</span>
<span class="quote">&gt;         solo_dev-&gt;vidq.mem_ops = &amp;vb2_dma_contig_memops;</span>
<span class="quote">&gt;         solo_dev-&gt;vidq.drv_priv = solo_dev;</span>
<span class="quote">&gt;         solo_dev-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt; -       solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +       solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;         solo_dev-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;         solo_dev-&gt;vidq.lock = &amp;solo_dev-&gt;lock;</span>
<span class="quote">&gt;         ret = vb2_queue_init(&amp;solo_dev-&gt;vidq);</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/tw68/tw68-video.c b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; index 8355e55b4e8e..e556f989aaab 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; @@ -975,7 +975,7 @@ int tw68_video_init2(struct tw68_dev *dev, int video_nr)</span>
<span class="quote">&gt;         dev-&gt;vidq.ops = &amp;tw68_video_qops;</span>
<span class="quote">&gt;         dev-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;         dev-&gt;vidq.drv_priv = dev;</span>
<span class="quote">&gt; -       dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +       dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;         dev-&gt;vidq.buf_struct_size = sizeof(struct tw68_buf);</span>
<span class="quote">&gt;         dev-&gt;vidq.lock = &amp;dev-&gt;lock;</span>
<span class="quote">&gt;         dev-&gt;vidq.min_buffers_needed = 2;</span>
<span class="quote">&gt; diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; index 8bbbb751bf45..2dfb291a47c6 100644</span>
<span class="quote">&gt; --- a/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; +++ b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; @@ -1188,8 +1188,7 @@ EXPORT_SYMBOL_GPL(mtd_writev);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void *mtd_kmalloc_up_to(const struct mtd_info *mtd, size_t *size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       gfp_t flags = __GFP_NOWARN | __GFP_WAIT |</span>
<span class="quote">&gt; -                      __GFP_NORETRY | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +       gfp_t flags = __GFP_NOWARN | __GFP_DIRECT_RECLAIM | __GFP_NORETRY;</span>
<span class="quote">&gt;         size_t min_alloc = max_t(size_t, mtd-&gt;writesize, PAGE_SIZE);</span>
<span class="quote">&gt;         void *kbuf;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; index f7fbdc9d1325..3a407e59acab 100644</span>
<span class="quote">&gt; --- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; +++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; @@ -689,7 +689,7 @@ static void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         if (fp-&gt;rx_frag_size) {</span>
<span class="quote">&gt;                 /* GFP_KERNEL allocations are used only during initialization */</span>
<span class="quote">&gt; -               if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +               if (unlikely(gfpflags_allow_blocking(gfp_mask)))</span>
<span class="quote">&gt;                         return (void *)__get_free_page(gfp_mask);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 return netdev_alloc_frag(fp-&gt;rx_frag_size);</span>
<span class="quote">&gt; diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; index da2a63c0a9ba..2615e0ae4f0a 100644</span>
<span class="quote">&gt; --- a/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; +++ b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; @@ -27,7 +27,7 @@</span>
<span class="quote">&gt;  #include &quot;ion_priv.h&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |</span>
<span class="quote">&gt; -                                    __GFP_NORETRY) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +                                    __GFP_NORETRY) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);</span>
<span class="quote">&gt;  static const unsigned int orders[] = {8, 4, 0};</span>
<span class="quote">&gt;  static const int num_orders = ARRAY_SIZE(orders);</span>
<span class="quote">&gt; diff --git a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; index ed37d26eb20d..5b0756cb6576 100644</span>
<span class="quote">&gt; --- a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; +++ b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; @@ -113,7 +113,7 @@ do {                                                \</span>
<span class="quote">&gt;  do {                                                                       \</span>
<span class="quote">&gt;         LASSERT(!in_interrupt() ||                                          \</span>
<span class="quote">&gt;                 ((size) &lt;= LIBCFS_VMALLOC_SIZE &amp;&amp;                           \</span>
<span class="quote">&gt; -                ((mask) &amp; __GFP_WAIT) == 0));                              \</span>
<span class="quote">&gt; +                !gfpflags_allow_blocking(mask)));                          \</span>
<span class="quote">&gt;  } while (0)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #define LIBCFS_ALLOC_POST(ptr, size)                                       \</span>
<span class="quote">&gt; diff --git a/drivers/usb/host/u132-hcd.c b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; index d51687780b61..8d4c1806e32f 100644</span>
<span class="quote">&gt; --- a/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; +++ b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; @@ -2247,7 +2247,7 @@ static int u132_urb_enqueue(struct usb_hcd *hcd, struct urb *urb,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct u132 *u132 = hcd_to_u132(hcd);</span>
<span class="quote">&gt;         if (irqs_disabled()) {</span>
<span class="quote">&gt; -               if (__GFP_WAIT &amp; mem_flags) {</span>
<span class="quote">&gt; +               if (gfpflags_allow_blocking(mem_flags)) {</span>
<span class="quote">&gt;                         printk(KERN_ERR &quot;invalid context for function that migh&quot;</span>
<span class="quote">&gt;                                 &quot;t sleep\n&quot;);</span>
<span class="quote">&gt;                         return -EINVAL;</span>
<span class="quote">&gt; diff --git a/drivers/video/fbdev/vermilion/vermilion.c b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; index 6b70d7f62b2f..1c1e95a0b8fa 100644</span>
<span class="quote">&gt; --- a/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; +++ b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; @@ -99,7 +99,7 @@ static int vmlfb_alloc_vram_area(struct vram_area *va, unsigned max_order,</span>
<span class="quote">&gt;                  * below the first 16MB.</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -               flags = __GFP_DMA | __GFP_HIGH;</span>
<span class="quote">&gt; +               flags = __GFP_DMA | __GFP_HIGH | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;                 va-&gt;logical =</span>
<span class="quote">&gt;                          __get_free_pages(flags, --max_order);</span>
<span class="quote">&gt;         } while (va-&gt;logical == 0 &amp;&amp; max_order &gt; min_order);</span>
<span class="quote">&gt; diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; index f556c3732c2c..3dd4792b8099 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; @@ -2566,7 +2566,7 @@ int open_ctree(struct super_block *sb,</span>
<span class="quote">&gt;         fs_info-&gt;commit_interval = BTRFS_DEFAULT_COMMIT_INTERVAL;</span>
<span class="quote">&gt;         fs_info-&gt;avg_delayed_ref_runtime = NSEC_PER_SEC &gt;&gt; 6; /* div by 64 */</span>
<span class="quote">&gt;         /* readahead state */</span>
<span class="quote">&gt; -       INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +       INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;         spin_lock_init(&amp;fs_info-&gt;reada_lock);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         fs_info-&gt;thread_pool_size = min_t(unsigned long,</span>
<span class="quote">&gt; diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; index 02d05817cbdf..c8a6cdcbef2b 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; @@ -594,7 +594,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;         if (bits &amp; (EXTENT_IOBITS | EXTENT_BOUNDARY))</span>
<span class="quote">&gt;                 clear = 1;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -       if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * Don&#39;t care for allocation failure here because we might end</span>
<span class="quote">&gt;                  * up not needing the pre-allocated extent state at all, which</span>
<span class="quote">&gt; @@ -718,7 +718,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;         if (start &gt; end)</span>
<span class="quote">&gt;                 goto out;</span>
<span class="quote">&gt;         spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -       if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;                 cond_resched();</span>
<span class="quote">&gt;         goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -850,7 +850,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         bits |= EXTENT_FIRST_DELALLOC;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -       if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;                 prealloc = alloc_extent_state(mask);</span>
<span class="quote">&gt;                 BUG_ON(!prealloc);</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; @@ -1028,7 +1028,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;         if (start &gt; end)</span>
<span class="quote">&gt;                 goto out;</span>
<span class="quote">&gt;         spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -       if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;                 cond_resched();</span>
<span class="quote">&gt;         goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1076,7 +1076,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;         btrfs_debug_check_extent_io_range(tree, start, end);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -       if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * Best effort, don&#39;t worry if extent state allocation fails</span>
<span class="quote">&gt;                  * here for the first iteration. We might have a cached state</span>
<span class="quote">&gt; @@ -1253,7 +1253,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;         if (start &gt; end)</span>
<span class="quote">&gt;                 goto out;</span>
<span class="quote">&gt;         spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -       if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;                 cond_resched();</span>
<span class="quote">&gt;         first_iteration = false;</span>
<span class="quote">&gt;         goto again;</span>
<span class="quote">&gt; @@ -4265,7 +4265,7 @@ int try_release_extent_mapping(struct extent_map_tree *map,</span>
<span class="quote">&gt;         u64 start = page_offset(page);</span>
<span class="quote">&gt;         u64 end = start + PAGE_CACHE_SIZE - 1;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if ((mask &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(mask) &amp;&amp;</span>
<span class="quote">&gt;             page-&gt;mapping-&gt;host-&gt;i_size &gt; 16 * 1024 * 1024) {</span>
<span class="quote">&gt;                 u64 len;</span>
<span class="quote">&gt;                 while (start &lt;= end) {</span>
<span class="quote">&gt; diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; index fbe7c104531c..b1968f36a39b 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/volumes.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; @@ -156,8 +156,8 @@ static struct btrfs_device *__alloc_device(void)</span>
<span class="quote">&gt;         spin_lock_init(&amp;dev-&gt;reada_lock);</span>
<span class="quote">&gt;         atomic_set(&amp;dev-&gt;reada_in_flight, 0);</span>
<span class="quote">&gt;         atomic_set(&amp;dev-&gt;dev_stats_ccnt, 0);</span>
<span class="quote">&gt; -       INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; -       INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +       INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt; +       INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return dev;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/fs/ext3/super.c b/fs/ext3/super.c</span>
<span class="quote">&gt; index 5ed0044fbb37..9004c786716f 100644</span>
<span class="quote">&gt; --- a/fs/ext3/super.c</span>
<span class="quote">&gt; +++ b/fs/ext3/super.c</span>
<span class="quote">&gt; @@ -750,7 +750,7 @@ static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
<span class="quote">&gt;                 return 0;</span>
<span class="quote">&gt;         if (journal)</span>
<span class="quote">&gt;                 return journal_try_to_free_buffers(journal, page,</span>
<span class="quote">&gt; -                                                  wait &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +                                               wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;         return try_to_free_buffers(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="quote">&gt; index 58987b5c514b..abe76d41ef1e 100644</span>
<span class="quote">&gt; --- a/fs/ext4/super.c</span>
<span class="quote">&gt; +++ b/fs/ext4/super.c</span>
<span class="quote">&gt; @@ -1045,7 +1045,7 @@ static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
<span class="quote">&gt;                 return 0;</span>
<span class="quote">&gt;         if (journal)</span>
<span class="quote">&gt;                 return jbd2_journal_try_to_free_buffers(journal, page,</span>
<span class="quote">&gt; -                                                       wait &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +                                               wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;         return try_to_free_buffers(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/fs/fscache/cookie.c b/fs/fscache/cookie.c</span>
<span class="quote">&gt; index d403c69bee08..4304072161aa 100644</span>
<span class="quote">&gt; --- a/fs/fscache/cookie.c</span>
<span class="quote">&gt; +++ b/fs/fscache/cookie.c</span>
<span class="quote">&gt; @@ -111,7 +111,7 @@ struct fscache_cookie *__fscache_acquire_cookie(</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* radix tree insertion won&#39;t use the preallocation pool unless it&#39;s</span>
<span class="quote">&gt;          * told it may not wait */</span>
<span class="quote">&gt; -       INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +       INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         switch (cookie-&gt;def-&gt;type) {</span>
<span class="quote">&gt;         case FSCACHE_COOKIE_TYPE_INDEX:</span>
<span class="quote">&gt; diff --git a/fs/fscache/page.c b/fs/fscache/page.c</span>
<span class="quote">&gt; index 483bbc613bf0..79483b3d8c6f 100644</span>
<span class="quote">&gt; --- a/fs/fscache/page.c</span>
<span class="quote">&gt; +++ b/fs/fscache/page.c</span>
<span class="quote">&gt; @@ -58,7 +58,7 @@ bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * decide whether a page can be released, possibly by cancelling a store to it</span>
<span class="quote">&gt; - * - we&#39;re allowed to sleep if __GFP_WAIT is flagged</span>
<span class="quote">&gt; + * - we&#39;re allowed to sleep if __GFP_DIRECT_RECLAIM is flagged</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;                                   struct page *page,</span>
<span class="quote">&gt; @@ -122,7 +122,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;          * allocator as the work threads writing to the cache may all end up</span>
<span class="quote">&gt;          * sleeping on memory allocation, so we may need to impose a timeout</span>
<span class="quote">&gt;          * too. */</span>
<span class="quote">&gt; -       if (!(gfp &amp; __GFP_WAIT) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt; +       if (!(gfp &amp; __GFP_DIRECT_RECLAIM) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt;                 fscache_stat(&amp;fscache_n_store_vmscan_busy);</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; @@ -132,7 +132,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;                 _debug(&quot;fscache writeout timeout page: %p{%lx}&quot;,</span>
<span class="quote">&gt;                         page, page-&gt;index);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       gfp &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +       gfp &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;         goto try_again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(__fscache_maybe_release_page);</span>
<span class="quote">&gt; diff --git a/fs/jbd/transaction.c b/fs/jbd/transaction.c</span>
<span class="quote">&gt; index 1695ba8334a2..f45b90ba7c5c 100644</span>
<span class="quote">&gt; --- a/fs/jbd/transaction.c</span>
<span class="quote">&gt; +++ b/fs/jbd/transaction.c</span>
<span class="quote">&gt; @@ -1690,8 +1690,8 @@ __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
<span class="quote">&gt;   * @journal: journal for operation</span>
<span class="quote">&gt;   * @page: to try and free</span>
<span class="quote">&gt;   * @gfp_mask: we use the mask to detect how hard should we try to release</span>
<span class="quote">&gt; - * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="quote">&gt; - * release the buffers.</span>
<span class="quote">&gt; + * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="quote">&gt; + * code to release the buffers.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * For all the buffers on this page,</span>
<span class="quote">&gt; diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; index f3d06174b051..06e18bcdb888 100644</span>
<span class="quote">&gt; --- a/fs/jbd2/transaction.c</span>
<span class="quote">&gt; +++ b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; @@ -1893,8 +1893,8 @@ __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
<span class="quote">&gt;   * @journal: journal for operation</span>
<span class="quote">&gt;   * @page: to try and free</span>
<span class="quote">&gt;   * @gfp_mask: we use the mask to detect how hard should we try to release</span>
<span class="quote">&gt; - * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="quote">&gt; - * release the buffers.</span>
<span class="quote">&gt; + * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="quote">&gt; + * code to release the buffers.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * For all the buffers on this page,</span>
<span class="quote">&gt; diff --git a/fs/nfs/file.c b/fs/nfs/file.c</span>
<span class="quote">&gt; index cc4fa1ed61fc..be6821967ec6 100644</span>
<span class="quote">&gt; --- a/fs/nfs/file.c</span>
<span class="quote">&gt; +++ b/fs/nfs/file.c</span>
<span class="quote">&gt; @@ -480,8 +480,8 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;         dfprintk(PAGECACHE, &quot;NFS: release_page(%p)\n&quot;, page);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* Always try to initiate a &#39;commit&#39; if relevant, but only</span>
<span class="quote">&gt; -        * wait for it if __GFP_WAIT is set.  Even then, only wait 1</span>
<span class="quote">&gt; -        * second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt; +        * wait for it if the caller allows blocking.  Even then,</span>
<span class="quote">&gt; +        * only wait 1 second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt;          * Waiting indefinitely can cause deadlocks when the NFS</span>
<span class="quote">&gt;          * server is on this machine, when a new TCP connection is</span>
<span class="quote">&gt;          * needed and in other rare cases.  There is no particular</span>
<span class="quote">&gt; @@ -491,7 +491,7 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;         if (mapping) {</span>
<span class="quote">&gt;                 struct nfs_server *nfss = NFS_SERVER(mapping-&gt;host);</span>
<span class="quote">&gt;                 nfs_commit_inode(mapping-&gt;host, 0);</span>
<span class="quote">&gt; -               if ((gfp &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +               if (gfpflags_allow_blocking(gfp) &amp;&amp;</span>
<span class="quote">&gt;                     !bdi_write_congested(&amp;nfss-&gt;backing_dev_info)) {</span>
<span class="quote">&gt;                         wait_on_page_bit_killable_timeout(page, PG_private,</span>
<span class="quote">&gt;                                                           HZ);</span>
<span class="quote">&gt; diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; index eac9549efd52..587174fd4f2c 100644</span>
<span class="quote">&gt; --- a/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; +++ b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; @@ -525,7 +525,7 @@ xfs_qm_shrink_scan(</span>
<span class="quote">&gt;         unsigned long           freed;</span>
<span class="quote">&gt;         int                     error;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT))</span>
<span class="quote">&gt; +       if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_DIRECT_RECLAIM)) != (__GFP_FS|__GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;                 return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         INIT_LIST_HEAD(&amp;isol.buffers);</span>
<span class="quote">&gt; diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt; index a10347ca5053..bd1937977d84 100644</span>
<span class="quote">&gt; --- a/include/linux/gfp.h</span>
<span class="quote">&gt; +++ b/include/linux/gfp.h</span>
<span class="quote">&gt; @@ -29,12 +29,13 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define ___GFP_NOMEMALLOC      0x10000u</span>
<span class="quote">&gt;  #define ___GFP_HARDWALL                0x20000u</span>
<span class="quote">&gt;  #define ___GFP_THISNODE                0x40000u</span>
<span class="quote">&gt; -#define ___GFP_WAIT            0x80000u</span>
<span class="quote">&gt; +#define ___GFP_ATOMIC          0x80000u</span>
<span class="quote">&gt;  #define ___GFP_NOACCOUNT       0x100000u</span>
<span class="quote">&gt;  #define ___GFP_NOTRACK         0x200000u</span>
<span class="quote">&gt; -#define ___GFP_NO_KSWAPD       0x400000u</span>
<span class="quote">&gt; +#define ___GFP_DIRECT_RECLAIM  0x400000u</span>
<span class="quote">&gt;  #define ___GFP_OTHER_NODE      0x800000u</span>
<span class="quote">&gt;  #define ___GFP_WRITE           0x1000000u</span>
<span class="quote">&gt; +#define ___GFP_KSWAPD_RECLAIM  0x2000000u</span>
<span class="quote">&gt;  /* If the above are modified, __GFP_BITS_SHIFT may need updating */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -68,7 +69,7 @@ struct vm_area_struct;</span>
<span class="quote">&gt;   * __GFP_MOVABLE: Flag that this page will be movable by the page migration</span>
<span class="quote">&gt;   * mechanism or reclaimed</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define __GFP_WAIT     ((__force gfp_t)___GFP_WAIT)    /* Can wait and reschedule? */</span>
<span class="quote">&gt; +#define __GFP_ATOMIC   ((__force gfp_t)___GFP_ATOMIC)  /* Caller cannot wait or reschedule */</span>
<span class="quote">&gt;  #define __GFP_HIGH     ((__force gfp_t)___GFP_HIGH)    /* Should access emergency pools? */</span>
<span class="quote">&gt;  #define __GFP_IO       ((__force gfp_t)___GFP_IO)      /* Can start physical IO? */</span>
<span class="quote">&gt;  #define __GFP_FS       ((__force gfp_t)___GFP_FS)      /* Can call down to low-level FS? */</span>
<span class="quote">&gt; @@ -91,23 +92,37 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define __GFP_NOACCOUNT        ((__force gfp_t)___GFP_NOACCOUNT) /* Don&#39;t account to kmemcg */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK  ((__force gfp_t)___GFP_NOTRACK)  /* Don&#39;t track with kmemcheck */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define __GFP_NO_KSWAPD        ((__force gfp_t)___GFP_NO_KSWAPD)</span>
<span class="quote">&gt;  #define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */</span>
<span class="quote">&gt;  #define __GFP_WRITE    ((__force gfp_t)___GFP_WRITE)   /* Allocator intends to dirty page */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="quote">&gt; + * wake kswapd to reclaim pages in the background until the high</span>
<span class="quote">&gt; + * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="quote">&gt; + * avoid unnecessary delays when a fallback option is available but</span>
<span class="quote">&gt; + * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="quote">&gt; + * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="quote">&gt; + * disruption.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __GFP_WAIT (__GFP_DIRECT_RECLAIM|__GFP_KSWAPD_RECLAIM)</span>

Convention is that combination of gfp flags don&#39;t use __XXX.
<span class="quote">
&gt; +#define __GFP_DIRECT_RECLAIM   ((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="quote">&gt; +#define __GFP_KSWAPD_RECLAIM   ((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * This may seem redundant, but it&#39;s a way of annotating false positives vs.</span>
<span class="quote">&gt;   * allocations that simply cannot be supported (e.g. page tables).</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define __GFP_BITS_SHIFT 25    /* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt; +#define __GFP_BITS_SHIFT 26    /* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt;  #define __GFP_BITS_MASK ((__force gfp_t)((1 &lt;&lt; __GFP_BITS_SHIFT) - 1))</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -/* This equals 0, but use constants in case they ever change */</span>
<span class="quote">&gt; -#define GFP_NOWAIT     (GFP_ATOMIC &amp; ~__GFP_HIGH)</span>
<span class="quote">&gt; -/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */</span>
<span class="quote">&gt; -#define GFP_ATOMIC     (__GFP_HIGH)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * GFP_ATOMIC callers can not sleep, need the allocation to succeed.</span>
<span class="quote">&gt; + * A lower watermark is applied to allow access to &quot;atomic reserves&quot;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define GFP_ATOMIC     (__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_NOWAIT     (__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  #define GFP_NOIO       (__GFP_WAIT)</span>
<span class="quote">&gt;  #define GFP_NOFS       (__GFP_WAIT | __GFP_IO)</span>
<span class="quote">&gt;  #define GFP_KERNEL     (__GFP_WAIT | __GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; @@ -116,10 +131,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define GFP_USER       (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER   (GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER_MOVABLE   (GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; -#define GFP_IOFS       (__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; -#define GFP_TRANSHUGE  (GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; -                        __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; -                        __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; +#define GFP_IOFS       (__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_TRANSHUGE  ((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; +                        __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &amp; \</span>
<span class="quote">&gt; +                        ~__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* This mask makes up all the page movable related flags */</span>
<span class="quote">&gt;  #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)</span>
<span class="quote">&gt; @@ -161,6 +176,11 @@ static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)</span>
<span class="quote">&gt;         return (gfp_flags &amp; GFP_MOVABLE_MASK) &gt;&gt; GFP_MOVABLE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return gfp_flags &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt;  #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h</span>
<span class="quote">&gt; index 22b6d9ca1654..55c4a9175801 100644</span>
<span class="quote">&gt; --- a/include/linux/skbuff.h</span>
<span class="quote">&gt; +++ b/include/linux/skbuff.h</span>
<span class="quote">&gt; @@ -1109,7 +1109,7 @@ static inline int skb_cloned(const struct sk_buff *skb)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static inline int skb_unclone(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (skb_cloned(skb))</span>
<span class="quote">&gt;                 return pskb_expand_head(skb, 0, 0, pri);</span>
<span class="quote">&gt; @@ -1193,7 +1193,7 @@ static inline int skb_shared(const struct sk_buff *skb)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;         if (skb_shared(skb)) {</span>
<span class="quote">&gt;                 struct sk_buff *nskb = skb_clone(skb, pri);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -1229,7 +1229,7 @@ static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_unshare(struct sk_buff *skb,</span>
<span class="quote">&gt;                                           gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;         if (skb_cloned(skb)) {</span>
<span class="quote">&gt;                 struct sk_buff *nskb = skb_copy(skb, pri);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/include/net/sock.h b/include/net/sock.h</span>
<span class="quote">&gt; index f21f0708ec59..cec0c4b634dc 100644</span>
<span class="quote">&gt; --- a/include/net/sock.h</span>
<span class="quote">&gt; +++ b/include/net/sock.h</span>
<span class="quote">&gt; @@ -2035,7 +2035,7 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct page_frag *sk_page_frag(struct sock *sk)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       if (sk-&gt;sk_allocation &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(sk-&gt;sk_allocation))</span>
<span class="quote">&gt;                 return &amp;current-&gt;task_frag;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return &amp;sk-&gt;sk_frag;</span>
<span class="quote">&gt; diff --git a/include/trace/events/gfpflags.h b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; index d6fd8e5b14b7..dde6bf092c8a 100644</span>
<span class="quote">&gt; --- a/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; +++ b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; @@ -20,7 +20,7 @@</span>
<span class="quote">&gt;         {(unsigned long)GFP_ATOMIC,             &quot;GFP_ATOMIC&quot;},          \</span>
<span class="quote">&gt;         {(unsigned long)GFP_NOIO,               &quot;GFP_NOIO&quot;},            \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_HIGH,             &quot;GFP_HIGH&quot;},            \</span>
<span class="quote">&gt; -       {(unsigned long)__GFP_WAIT,             &quot;GFP_WAIT&quot;},            \</span>
<span class="quote">&gt; +       {(unsigned long)__GFP_ATOMIC,           &quot;GFP_ATOMIC&quot;},          \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_IO,               &quot;GFP_IO&quot;},              \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_COLD,             &quot;GFP_COLD&quot;},            \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_NOWARN,           &quot;GFP_NOWARN&quot;},          \</span>
<span class="quote">&gt; @@ -36,7 +36,8 @@</span>
<span class="quote">&gt;         {(unsigned long)__GFP_RECLAIMABLE,      &quot;GFP_RECLAIMABLE&quot;},     \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_MOVABLE,          &quot;GFP_MOVABLE&quot;},         \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_NOTRACK,          &quot;GFP_NOTRACK&quot;},         \</span>
<span class="quote">&gt; -       {(unsigned long)__GFP_NO_KSWAPD,        &quot;GFP_NO_KSWAPD&quot;},       \</span>
<span class="quote">&gt; +       {(unsigned long)__GFP_DIRECT_RECLAIM,   &quot;GFP_DIRECT_RECLAIM&quot;},  \</span>
<span class="quote">&gt; +       {(unsigned long)__GFP_KSWAPD_RECLAIM,   &quot;GFP_KSWAPD_RECLAIM&quot;},  \</span>
<span class="quote">&gt;         {(unsigned long)__GFP_OTHER_NODE,       &quot;GFP_OTHER_NODE&quot;}       \</span>
<span class="quote">&gt;         ) : &quot;GFP_NOWAIT&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/kernel/audit.c b/kernel/audit.c</span>
<span class="quote">&gt; index f9e6065346db..6ab7a55dbdff 100644</span>
<span class="quote">&gt; --- a/kernel/audit.c</span>
<span class="quote">&gt; +++ b/kernel/audit.c</span>
<span class="quote">&gt; @@ -1357,16 +1357,16 @@ struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,</span>
<span class="quote">&gt;         if (unlikely(audit_filter_type(type)))</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +       if (gfp_mask &amp; __GFP_DIRECT_RECLAIM) {</span>
<span class="quote">&gt;                 if (audit_pid &amp;&amp; audit_pid == current-&gt;pid)</span>
<span class="quote">&gt; -                       gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +                       gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;                 else</span>
<span class="quote">&gt;                         reserve = 0;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         while (audit_backlog_limit</span>
<span class="quote">&gt;                &amp;&amp; skb_queue_len(&amp;audit_skb_queue) &gt; audit_backlog_limit + reserve) {</span>
<span class="quote">&gt; -               if (gfp_mask &amp; __GFP_WAIT &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt; +               if (gfp_mask &amp; __GFP_DIRECT_RECLAIM &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt;                         long sleep_time;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                         sleep_time = timeout_start + audit_backlog_wait_time - jiffies;</span>
<span class="quote">&gt; diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; index 8acfbf773e06..9aa39f20f593 100644</span>
<span class="quote">&gt; --- a/kernel/locking/lockdep.c</span>
<span class="quote">&gt; +++ b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; @@ -2738,7 +2738,7 @@ static void __lockdep_trace_alloc(gfp_t gfp_mask, unsigned long flags)</span>
<span class="quote">&gt;                 return;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* no reclaim without waiting on it */</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;                 return;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* this guy won&#39;t enter reclaim */</span>
<span class="quote">&gt; diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c</span>
<span class="quote">&gt; index 5235dd4e1e2f..3a970604308f 100644</span>
<span class="quote">&gt; --- a/kernel/power/snapshot.c</span>
<span class="quote">&gt; +++ b/kernel/power/snapshot.c</span>
<span class="quote">&gt; @@ -1779,7 +1779,7 @@ alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)</span>
<span class="quote">&gt;         while (to_alloc-- &gt; 0) {</span>
<span class="quote">&gt;                 struct page *page;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -               page = alloc_image_page(__GFP_HIGHMEM);</span>
<span class="quote">&gt; +               page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;                 memory_bm_set_bit(bm, page_to_pfn(page));</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;         return nr_highmem;</span>
<span class="quote">&gt; diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="quote">&gt; index 07854477c164..d903c02223af 100644</span>
<span class="quote">&gt; --- a/kernel/smp.c</span>
<span class="quote">&gt; +++ b/kernel/smp.c</span>
<span class="quote">&gt; @@ -669,7 +669,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
<span class="quote">&gt;         cpumask_var_t cpus;</span>
<span class="quote">&gt;         int cpu, ret;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {</span>
<span class="quote">&gt;                 preempt_disable();</span>
<span class="quote">&gt; diff --git a/lib/idr.c b/lib/idr.c</span>
<span class="quote">&gt; index 5335c43adf46..6098336df267 100644</span>
<span class="quote">&gt; --- a/lib/idr.c</span>
<span class="quote">&gt; +++ b/lib/idr.c</span>
<span class="quote">&gt; @@ -399,7 +399,7 @@ void idr_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;          * allocation guarantee.  Disallow usage from those contexts.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         WARN_ON_ONCE(in_interrupt());</span>
<span class="quote">&gt; -       might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         preempt_disable();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -453,7 +453,7 @@ int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)</span>
<span class="quote">&gt;         struct idr_layer *pa[MAX_IDR_LEVEL + 1];</span>
<span class="quote">&gt;         int id;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* sanity checks */</span>
<span class="quote">&gt;         if (WARN_ON_ONCE(start &lt; 0))</span>
<span class="quote">&gt; diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="quote">&gt; index f9ebe1c82060..c3775ee46cd6 100644</span>
<span class="quote">&gt; --- a/lib/radix-tree.c</span>
<span class="quote">&gt; +++ b/lib/radix-tree.c</span>
<span class="quote">&gt; @@ -188,7 +188,7 @@ radix_tree_node_alloc(struct radix_tree_root *root)</span>
<span class="quote">&gt;          * preloading in the interrupt anyway as all the allocations have to</span>
<span class="quote">&gt;          * be atomic. So just do normal allocation when in interrupt.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp_mask) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt;                 struct radix_tree_preload *rtp;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt; @@ -249,7 +249,7 @@ radix_tree_node_free(struct radix_tree_node *node)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -286,12 +286,12 @@ static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         /* Warn on non-sensical use... */</span>
<span class="quote">&gt; -       WARN_ON_ONCE(!(gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +       WARN_ON_ONCE(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;         return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt; @@ -303,7 +303,7 @@ EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_maybe_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;                 return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;         /* Preloading doesn&#39;t help anything with this gfp mask, skip it */</span>
<span class="quote">&gt;         preempt_disable();</span>
<span class="quote">&gt; diff --git a/mm/backing-dev.c b/mm/backing-dev.c</span>
<span class="quote">&gt; index dac5bf59309d..805ce70b72f3 100644</span>
<span class="quote">&gt; --- a/mm/backing-dev.c</span>
<span class="quote">&gt; +++ b/mm/backing-dev.c</span>
<span class="quote">&gt; @@ -632,7 +632,7 @@ struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct bdi_writeback *wb;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(gfp));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (!memcg_css-&gt;parent)</span>
<span class="quote">&gt;                 return &amp;bdi-&gt;wb;</span>
<span class="quote">&gt; diff --git a/mm/dmapool.c b/mm/dmapool.c</span>
<span class="quote">&gt; index fd5fe4342e93..84dac666fc0c 100644</span>
<span class="quote">&gt; --- a/mm/dmapool.c</span>
<span class="quote">&gt; +++ b/mm/dmapool.c</span>
<span class="quote">&gt; @@ -323,7 +323,7 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,</span>
<span class="quote">&gt;         size_t offset;</span>
<span class="quote">&gt;         void *retval;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(mem_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(mem_flags));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         spin_lock_irqsave(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;         list_for_each_entry(page, &amp;pool-&gt;page_list, page_list) {</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index acb93c554f6e..e34f6411da8c 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -2268,7 +2268,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;         if (unlikely(task_in_memcg_oom(current)))</span>
<span class="quote">&gt;                 goto nomem;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;                 goto nomem;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);</span>
<span class="quote">&gt; @@ -2327,7 +2327,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;         css_get_many(&amp;memcg-&gt;css, batch);</span>
<span class="quote">&gt;         if (batch &gt; nr_pages)</span>
<span class="quote">&gt;                 refill_stock(memcg, batch - nr_pages);</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;                 goto done;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * If the hierarchy is above the normal consumption range,</span>
<span class="quote">&gt; @@ -4696,8 +4696,8 @@ static int mem_cgroup_do_precharge(unsigned long count)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         int ret;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       /* Try a single bulk charge without reclaim first */</span>
<span class="quote">&gt; -       ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_WAIT, count);</span>
<span class="quote">&gt; +       /* Try a single bulk charge without reclaim first, kswapd may wake */</span>
<span class="quote">&gt; +       ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM, count);</span>
<span class="quote">&gt;         if (!ret) {</span>
<span class="quote">&gt;                 mc.precharge += count;</span>
<span class="quote">&gt;                 return ret;</span>
<span class="quote">&gt; diff --git a/mm/mempool.c b/mm/mempool.c</span>
<span class="quote">&gt; index 2cc08de8b1db..bfd2a0dd0e18 100644</span>
<span class="quote">&gt; --- a/mm/mempool.c</span>
<span class="quote">&gt; +++ b/mm/mempool.c</span>
<span class="quote">&gt; @@ -317,13 +317,13 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;         gfp_t gfp_temp;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         VM_WARN_ON_ONCE(gfp_mask &amp; __GFP_ZERO);</span>
<span class="quote">&gt; -       might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         gfp_mask |= __GFP_NOMEMALLOC;   /* don&#39;t allocate emergency reserves */</span>
<span class="quote">&gt;         gfp_mask |= __GFP_NORETRY;      /* don&#39;t loop in __alloc_pages */</span>
<span class="quote">&gt;         gfp_mask |= __GFP_NOWARN;       /* failures are OK */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       gfp_temp = gfp_mask &amp; ~(__GFP_WAIT|__GFP_IO);</span>
<span class="quote">&gt; +       gfp_temp = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM|__GFP_IO);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  repeat_alloc:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -346,7 +346,7 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; -        * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If</span>
<span class="quote">&gt; +        * We use gfp mask w/o direct reclaim or IO for the first round.  If</span>
<span class="quote">&gt;          * alloc failed with that and @pool was empty, retry immediately.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         if (gfp_temp != gfp_mask) {</span>
<span class="quote">&gt; @@ -355,8 +355,8 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;                 goto repeat_alloc;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       /* We must not sleep if !__GFP_WAIT */</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +       /* We must not sleep if !__GFP_DIRECT_RECLAIM */</span>
<span class="quote">&gt; +       if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;                 spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index eb4267107d1f..0e16c4047638 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1564,7 +1564,7 @@ static struct page *alloc_misplaced_dst_page(struct page *page,</span>
<span class="quote">&gt;                                          (GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt;                                           __GFP_THISNODE | __GFP_NOMEMALLOC |</span>
<span class="quote">&gt;                                           __GFP_NORETRY | __GFP_NOWARN) &amp;</span>
<span class="quote">&gt; -                                        ~GFP_IOFS, 0);</span>
<span class="quote">&gt; +                                        ~(__GFP_IO | __GFP_FS), 0);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return newpage;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 32d1cec124bc..68f961bdfdf8 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -151,12 +151,12 @@ void pm_restrict_gfp_mask(void)</span>
<span class="quote">&gt;         WARN_ON(!mutex_is_locked(&amp;pm_mutex));</span>
<span class="quote">&gt;         WARN_ON(saved_gfp_mask);</span>
<span class="quote">&gt;         saved_gfp_mask = gfp_allowed_mask;</span>
<span class="quote">&gt; -       gfp_allowed_mask &amp;= ~GFP_IOFS;</span>
<span class="quote">&gt; +       gfp_allowed_mask &amp;= ~(__GFP_IO | __GFP_FS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  bool pm_suspended_storage(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       if ((gfp_allowed_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +       if ((gfp_allowed_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt;         return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2158,7 +2158,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt;         if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt; -       if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; @@ -2660,7 +2660,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;                 if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt;                     (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt;                         filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; -       if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +       if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
<span class="quote">&gt;                 filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (fmt) {</span>
<span class="quote">&gt; @@ -2915,7 +2915,6 @@ static inline int</span>
<span class="quote">&gt;  gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;</span>
<span class="quote">&gt; -       const bool atomic = !(gfp_mask &amp; (__GFP_WAIT | __GFP_NO_KSWAPD));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* __GFP_HIGH is assumed to be the same as ALLOC_HIGH to save a branch. */</span>
<span class="quote">&gt;         BUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);</span>
<span class="quote">&gt; @@ -2924,11 +2923,11 @@ gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;          * The caller may dip into page reserves a bit more if the caller</span>
<span class="quote">&gt;          * cannot run direct reclaim, or if the caller has realtime scheduling</span>
<span class="quote">&gt;          * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will</span>
<span class="quote">&gt; -        * set both ALLOC_HARDER (atomic == true) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt; +        * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt;         alloc_flags |= (__force int) (gfp_mask &amp; __GFP_HIGH);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (atomic) {</span>
<span class="quote">&gt; +       if (gfp_mask &amp; __GFP_ATOMIC) {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * Not worth trying to allocate harder for __GFP_NOMEMALLOC even</span>
<span class="quote">&gt;                  * if it can&#39;t schedule.</span>
<span class="quote">&gt; @@ -2965,11 +2964,16 @@ bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
<span class="quote">&gt;         return !!(gfp_to_alloc_flags(gfp_mask) &amp; ALLOC_NO_WATERMARKS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +static inline bool is_thp_gfp_mask(gfp_t gfp_mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return (gfp_mask &amp; (GFP_TRANSHUGE | __GFP_KSWAPD_RECLAIM)) == GFP_TRANSHUGE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline struct page *</span>
<span class="quote">&gt;  __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;                                                 struct alloc_context *ac)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       const gfp_t wait = gfp_mask &amp; __GFP_WAIT;</span>
<span class="quote">&gt; +       bool can_direct_reclaim = gfp_mask &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;         struct page *page = NULL;</span>
<span class="quote">&gt;         int alloc_flags;</span>
<span class="quote">&gt;         unsigned long pages_reclaimed = 0;</span>
<span class="quote">&gt; @@ -2990,15 +2994,23 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; +        * We also sanity check to catch abuse of atomic reserves being used by</span>
<span class="quote">&gt; +        * callers that are not in atomic context.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (WARN_ON_ONCE((gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==</span>
<span class="quote">&gt; +                               (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt; +               gfp_mask &amp;= ~__GFP_ATOMIC;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt;          * If this allocation cannot block and it is for a specific node, then</span>
<span class="quote">&gt;          * fail early.  There&#39;s no need to wakeup kswapd or retry for a</span>
<span class="quote">&gt;          * speculative node-specific allocation.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; -       if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !wait)</span>
<span class="quote">&gt; +       if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !can_direct_reclaim)</span>
<span class="quote">&gt;                 goto nopage;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_NO_KSWAPD))</span>
<span class="quote">&gt; +       if (gfp_mask &amp; __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;                 wake_all_kswapds(order, ac);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; @@ -3041,8 +3053,8 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       /* Atomic allocations - we can&#39;t balance anything */</span>
<span class="quote">&gt; -       if (!wait) {</span>
<span class="quote">&gt; +       /* Caller is not willing to reclaim, we can&#39;t balance anything */</span>
<span class="quote">&gt; +       if (!can_direct_reclaim) {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * All existing users of the deprecated __GFP_NOFAIL are</span>
<span class="quote">&gt;                  * blockable, so warn of any new users that actually allow this</span>
<span class="quote">&gt; @@ -3072,7 +3084,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;                 goto got_pg;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* Checks for THP-specific high-order allocations */</span>
<span class="quote">&gt; -       if ((gfp_mask &amp; GFP_TRANSHUGE) == GFP_TRANSHUGE) {</span>
<span class="quote">&gt; +       if (is_thp_gfp_mask(gfp_mask)) {</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * If compaction is deferred for high-order allocations, it is</span>
<span class="quote">&gt;                  * because sync compaction recently failed. If this is the case</span>
<span class="quote">&gt; @@ -3107,8 +3119,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;          * fault, so use asynchronous memory compaction for THP unless it is</span>
<span class="quote">&gt;          * khugepaged trying to collapse.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; -       if ((gfp_mask &amp; GFP_TRANSHUGE) != GFP_TRANSHUGE ||</span>
<span class="quote">&gt; -                                               (current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt; +       if (!is_thp_gfp_mask(gfp_mask) || (current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt;                 migration_mode = MIGRATE_SYNC_LIGHT;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* Try direct reclaim and then allocating */</span>
<span class="quote">&gt; @@ -3179,7 +3190,7 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         lockdep_trace_alloc(gfp_mask);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (should_fail_alloc_page(gfp_mask, order))</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt; diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="quote">&gt; index 200e22412a16..f82bdb3eb1fc 100644</span>
<span class="quote">&gt; --- a/mm/slab.c</span>
<span class="quote">&gt; +++ b/mm/slab.c</span>
<span class="quote">&gt; @@ -1030,12 +1030,12 @@ static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Construct gfp mask to allocate from a specific node but do not invoke reclaim</span>
<span class="quote">&gt; - * or warn about failures.</span>
<span class="quote">&gt; + * Construct gfp mask to allocate from a specific node but do not direct reclaim</span>
<span class="quote">&gt; + * or warn about failures. kswapd may still wake to reclaim in the background.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline gfp_t gfp_exact_node(gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +       return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -2625,7 +2625,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         offset *= cachep-&gt;colour_off;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;                 local_irq_enable();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; @@ -2655,7 +2655,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         cache_init_objs(cachep, page);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;                 local_irq_disable();</span>
<span class="quote">&gt;         check_irq_off();</span>
<span class="quote">&gt;         spin_lock(&amp;n-&gt;list_lock);</span>
<span class="quote">&gt; @@ -2669,7 +2669,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  opps1:</span>
<span class="quote">&gt;         kmem_freepages(cachep, page);</span>
<span class="quote">&gt;  failed:</span>
<span class="quote">&gt; -       if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;                 local_irq_disable();</span>
<span class="quote">&gt;         return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2861,7 +2861,7 @@ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,</span>
<span class="quote">&gt;  static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,</span>
<span class="quote">&gt;                                                 gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;  #if DEBUG</span>
<span class="quote">&gt;         kmem_flagcheck(cachep, flags);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -3049,11 +3049,11 @@ static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt;                 struct page *page;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -               if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +               if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;                         local_irq_enable();</span>
<span class="quote">&gt;                 kmem_flagcheck(cache, flags);</span>
<span class="quote">&gt;                 page = kmem_getpages(cache, local_flags, numa_mem_id());</span>
<span class="quote">&gt; -               if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +               if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;                         local_irq_disable();</span>
<span class="quote">&gt;                 if (page) {</span>
<span class="quote">&gt;                         /*</span>
<span class="quote">&gt; diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="quote">&gt; index 816df0016555..a4661c59ff54 100644</span>
<span class="quote">&gt; --- a/mm/slub.c</span>
<span class="quote">&gt; +++ b/mm/slub.c</span>
<span class="quote">&gt; @@ -1263,7 +1263,7 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;         lockdep_trace_alloc(flags);</span>
<span class="quote">&gt; -       might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (should_failslab(s-&gt;object_size, flags, s-&gt;flags))</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt; @@ -1339,7 +1339,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;                 local_irq_enable();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         flags |= s-&gt;allocflags;</span>
<span class="quote">&gt; @@ -1380,7 +1380,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;                         kmemcheck_mark_unallocated_pages(page, pages);</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;                 local_irq_disable();</span>
<span class="quote">&gt;         if (!page)</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt; diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="quote">&gt; index 2faaa2976447..9ad4dcb0631c 100644</span>
<span class="quote">&gt; --- a/mm/vmalloc.c</span>
<span class="quote">&gt; +++ b/mm/vmalloc.c</span>
<span class="quote">&gt; @@ -1617,7 +1617,7 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,</span>
<span class="quote">&gt;                         goto fail;</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;                 area-&gt;pages[i] = page;</span>
<span class="quote">&gt; -               if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +               if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;                         cond_resched();</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index e950134c4b9a..837c440d60a9 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -1465,7 +1465,7 @@ static int too_many_isolated(struct zone *zone, int file,</span>
<span class="quote">&gt;          * won&#39;t get blocked by normal direct-reclaimers, forming a circular</span>
<span class="quote">&gt;          * deadlock.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; -       if ((sc-&gt;gfp_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +       if ((sc-&gt;gfp_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;                 inactive &gt;&gt;= 3;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         return isolated &gt; inactive;</span>
<span class="quote">&gt; @@ -3764,7 +3764,7 @@ int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Do not scan if the allocation should not be delayed.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; -       if (!(gfp_mask &amp; __GFP_WAIT) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt; +       if (!gfpflags_allow_blocking(gfp_mask) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt;                 return ZONE_RECLAIM_NOSCAN;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; diff --git a/mm/zswap.c b/mm/zswap.c</span>
<span class="quote">&gt; index 2d5727baed59..26104a68c972 100644</span>
<span class="quote">&gt; --- a/mm/zswap.c</span>
<span class="quote">&gt; +++ b/mm/zswap.c</span>
<span class="quote">&gt; @@ -684,7 +684,8 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* store */</span>
<span class="quote">&gt;         len = dlen + sizeof(struct zswap_header);</span>
<span class="quote">&gt; -       ret = zpool_malloc(zswap_pool, len, __GFP_NORETRY | __GFP_NOWARN,</span>
<span class="quote">&gt; +       ret = zpool_malloc(zswap_pool, len,</span>
<span class="quote">&gt; +               __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM,</span>
<span class="quote">&gt;                 &amp;handle);</span>
<span class="quote">&gt;         if (ret == -ENOSPC) {</span>
<span class="quote">&gt;                 zswap_reject_compress_poor++;</span>
<span class="quote">&gt; @@ -900,7 +901,7 @@ static void __exit zswap_debugfs_exit(void) { }</span>
<span class="quote">&gt;  **********************************/</span>
<span class="quote">&gt;  static int __init init_zswap(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt; +       gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         pr_info(&quot;loading zswap\n&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="quote">&gt; index b6a19ca0f99e..6f025e2544de 100644</span>
<span class="quote">&gt; --- a/net/core/skbuff.c</span>
<span class="quote">&gt; +++ b/net/core/skbuff.c</span>
<span class="quote">&gt; @@ -414,7 +414,7 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,</span>
<span class="quote">&gt;         len += NET_SKB_PAD;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -           (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +           (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;                 skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;                 if (!skb)</span>
<span class="quote">&gt;                         goto skb_fail;</span>
<span class="quote">&gt; @@ -481,7 +481,7 @@ struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,</span>
<span class="quote">&gt;         len += NET_SKB_PAD + NET_IP_ALIGN;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -           (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +           (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;                 skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;                 if (!skb)</span>
<span class="quote">&gt;                         goto skb_fail;</span>
<span class="quote">&gt; @@ -4452,7 +4452,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;                 return NULL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         gfp_head = gfp_mask;</span>
<span class="quote">&gt; -       if (gfp_head &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +       if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
<span class="quote">&gt;                 gfp_head |= __GFP_REPEAT;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         *errcode = -ENOBUFS;</span>
<span class="quote">&gt; @@ -4467,7 +4467,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 while (order) {</span>
<span class="quote">&gt;                         if (npages &gt;= 1 &lt;&lt; order) {</span>
<span class="quote">&gt; -                               page = alloc_pages((gfp_mask &amp; ~__GFP_WAIT) |</span>
<span class="quote">&gt; +                               page = alloc_pages((gfp_mask &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt;                                                    __GFP_COMP |</span>
<span class="quote">&gt;                                                    __GFP_NOWARN |</span>
<span class="quote">&gt;                                                    __GFP_NORETRY,</span>
<span class="quote">&gt; diff --git a/net/core/sock.c b/net/core/sock.c</span>
<span class="quote">&gt; index 193901d09757..02b705cc9eb3 100644</span>
<span class="quote">&gt; --- a/net/core/sock.c</span>
<span class="quote">&gt; +++ b/net/core/sock.c</span>
<span class="quote">&gt; @@ -1879,8 +1879,10 @@ bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         pfrag-&gt;offset = 0;</span>
<span class="quote">&gt;         if (SKB_FRAG_PAGE_ORDER) {</span>
<span class="quote">&gt; -               pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_WAIT) | __GFP_COMP |</span>
<span class="quote">&gt; -                                         __GFP_NOWARN | __GFP_NORETRY,</span>
<span class="quote">&gt; +               /* Avoid direct reclaim but allow kswapd to wake */</span>
<span class="quote">&gt; +               pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt; +                                         __GFP_COMP | __GFP_NOWARN |</span>
<span class="quote">&gt; +                                         __GFP_NORETRY,</span>
<span class="quote">&gt;                                           SKB_FRAG_PAGE_ORDER);</span>
<span class="quote">&gt;                 if (likely(pfrag-&gt;page)) {</span>
<span class="quote">&gt;                         pfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;</span>
<span class="quote">&gt; diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; index 67d210477863..8283d90dde74 100644</span>
<span class="quote">&gt; --- a/net/netlink/af_netlink.c</span>
<span class="quote">&gt; +++ b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; @@ -2066,7 +2066,7 @@ int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid</span>
<span class="quote">&gt;         consume_skb(info.skb2);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (info.delivered) {</span>
<span class="quote">&gt; -               if (info.congested &amp;&amp; (allocation &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +               if (info.congested &amp;&amp; gfpflags_allow_blocking(allocation))</span>
<span class="quote">&gt;                         yield();</span>
<span class="quote">&gt;                 return 0;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; diff --git a/net/rxrpc/ar-connection.c b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; index 6631f4f1e39b..3b5de4b86058 100644</span>
<span class="quote">&gt; --- a/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; +++ b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; @@ -500,7 +500,7 @@ int rxrpc_connect_call(struct rxrpc_sock *rx,</span>
<span class="quote">&gt;                 if (bundle-&gt;num_conns &gt;= 20) {</span>
<span class="quote">&gt;                         _debug(&quot;too many conns&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -                       if (!(gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +                       if (!gfpflags_allow_blocking(gfp)) {</span>
<span class="quote">&gt;                                 _leave(&quot; = -EAGAIN&quot;);</span>
<span class="quote">&gt;                                 return -EAGAIN;</span>
<span class="quote">&gt;                         }</span>
<span class="quote">&gt; diff --git a/net/sctp/associola.c b/net/sctp/associola.c</span>
<span class="quote">&gt; index 197c3f59ecbf..75369ae8de1e 100644</span>
<span class="quote">&gt; --- a/net/sctp/associola.c</span>
<span class="quote">&gt; +++ b/net/sctp/associola.c</span>
<span class="quote">&gt; @@ -1588,7 +1588,7 @@ int sctp_assoc_lookup_laddr(struct sctp_association *asoc,</span>
<span class="quote">&gt;  /* Set an association id for a given association */</span>
<span class="quote">&gt;  int sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       bool preload = !!(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +       bool preload = gfpflags_allow_blocking(gfp);</span>
<span class="quote">&gt;         int ret;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* If the id is already assigned, keep it. */</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.4.6</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Sept. 9, 2015, 12:22 p.m.</div>
<pre class="content">
On Tue, Sep 08, 2015 at 03:49:58PM +0900, Joonsoo Kim wrote:
<span class="quote">&gt; 2015-08-24 21:09 GMT+09:00 Mel Gorman &lt;mgorman@techsingularity.net&gt;:</span>
<span class="quote">&gt; &gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; &gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; &gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; &gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; &gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; &gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; &gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; &gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; &gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; &gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; &gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; &gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; &gt; background reclaim.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hello, Mel.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think that it is better to do one thing at one patch.</span>

This was a case where the incremental change felt unnecessary. The purpose
of the patch is to &quot;distinguish between being unable to sleep, unwilling
to sleep and avoiding waking kswapd&quot;. Splitting that up is possible but
I&#39;m not convinced it helps.
<span class="quote">
&gt; To distinguish real atomic, we just need to introduce __GFP_ATOMIC and</span>
<span class="quote">&gt; make GFP_ATOMIC to __GFP_ATOMIC | GFP_HARDER and change related</span>
<span class="quote">&gt; things. __GFP_WAIT changes isn&#39;t needed at all for this purpose. It can</span>
<span class="quote">&gt; reduce patch size and provides more good bisectability.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And, I don&#39;t think that introducing __GFP_KSWAPD_RECLAIM is good thing.</span>
<span class="quote">&gt; Basically, kswapd reclaim should be enforced.</span>

Several years ago, I would have agreed. Now there are callers that want
to control kswapd and I think it made more sense to clearly state whether
RECLAIM and KSWAPD are allowed instead of having RECLAIM and NO_KSWAPD
flags -- i.e. flags that consistently allow or consistently deny.
<span class="quote">
&gt; New flag makes user who manually</span>
<span class="quote">&gt; manipulate gfp flag more difficult. Without this change, your second hazard will</span>
<span class="quote">&gt; be disappeared although it is almost harmless.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And, I doubt that this big one shot change is preferable. AFAIK, even if changes</span>
<span class="quote">&gt; are one to one mapping and no functional difference, each one is made by</span>
<span class="quote">&gt; one patch and send it to correct maintainer. I guess there is some difficulty</span>
<span class="quote">&gt; in this patch to do like this, but, it could. Isn&#39;t it?</span>
<span class="quote">&gt; </span>

Splitting this into one patch per maintainer would be a review and bisection
nightmare. If I saw someone else doing that I would wonder if they were
just trying to increase their patch count for no reason.
<span class="quote">
&gt; Some nitpicks are below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; index 03e75fef15b8..86809bd2026d 100644</span>
<span class="quote">&gt; &gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; @@ -25,7 +25,7 @@</span>
<span class="quote">&gt; &gt;  unsigned long xen_get_swiotlb_free_pages(unsigned int order)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;         struct memblock_region *reg;</span>
<span class="quote">&gt; &gt; -       gfp_t flags = __GFP_NOWARN;</span>
<span class="quote">&gt; &gt; +       gfp_t flags = __GFP_NOWARN|___GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please use __XXX rather than ___XXX.</span>
<span class="quote">&gt; </span>

Fixed.
<span class="quote">
&gt; &gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; @@ -457,13 +457,13 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
<span class="quote">&gt; &gt;                  * We solve this, and guarantee forward progress, with a rescuer</span>
<span class="quote">&gt; &gt;                  * workqueue per bio_set. If we go to allocate and there are</span>
<span class="quote">&gt; &gt;                  * bios on current-&gt;bio_list, we first try the allocation</span>
<span class="quote">&gt; &gt; -                * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="quote">&gt; &gt; -                * would be blocking to the rescuer workqueue before we retry</span>
<span class="quote">&gt; &gt; -                * with the original gfp_flags.</span>
<span class="quote">&gt; &gt; +                * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="quote">&gt; &gt; +                * bios we would be blocking to the rescuer workqueue before</span>
<span class="quote">&gt; &gt; +                * we retry with the original gfp_flags.</span>
<span class="quote">&gt; &gt;                  */</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;                 if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="quote">&gt; &gt; -                       gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; &gt; +                       gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How about introduce helper function to mask out __GFP_DIRECT_RECLAIM?</span>
<span class="quote">&gt; It can be used many places.</span>
<span class="quote">&gt; </span>

In this case, the pattern for removing a single flag is easier to recognise
than a helper whose implementation must be examined.
<span class="quote">
&gt; &gt;                 p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);</span>
<span class="quote">&gt; &gt;                 if (!p &amp;&amp; gfp_mask != saved_gfp) {</span>
<span class="quote">&gt; &gt; diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="quote">&gt; &gt; index 627ed0c593fb..e3605acaaffc 100644</span>
<span class="quote">&gt; &gt; --- a/block/blk-core.c</span>
<span class="quote">&gt; &gt; +++ b/block/blk-core.c</span>
<span class="quote">&gt; &gt; @@ -1156,8 +1156,8 @@ static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
<span class="quote">&gt; &gt;   * @bio: bio to allocate request for (can be %NULL)</span>
<span class="quote">&gt; &gt;   * @gfp_mask: allocation mask</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; - * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="quote">&gt; &gt; - * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; &gt; + * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="quote">&gt; &gt; + * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt;   * Must be called with @q-&gt;queue_lock held and,</span>
<span class="quote">&gt; &gt;   * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.</span>
<span class="quote">&gt; &gt; @@ -1177,7 +1177,7 @@ static struct request *get_request(struct request_queue *q, int rw_flags,</span>
<span class="quote">&gt; &gt;         if (!IS_ERR(rq))</span>
<span class="quote">&gt; &gt;                 return rq;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -       if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; &gt; +       if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; &gt;                 blk_put_rl(rl);</span>
<span class="quote">&gt; &gt;                 return rq;</span>
<span class="quote">&gt; &gt;         }</span>
<span class="quote">&gt; &gt; @@ -1255,11 +1255,11 @@ EXPORT_SYMBOL(blk_get_request);</span>
<span class="quote">&gt; &gt;   * BUG.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt;   * WARNING: When allocating/cloning a bio-chain, careful consideration should be</span>
<span class="quote">&gt; &gt; - * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="quote">&gt; &gt; - * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="quote">&gt; &gt; - * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="quote">&gt; &gt; - * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="quote">&gt; &gt; - * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; &gt; + * given to how you allocate bios. In particular, you cannot use</span>
<span class="quote">&gt; &gt; + * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="quote">&gt; &gt; + * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="quote">&gt; &gt; + * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="quote">&gt; &gt; + * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; &gt;   * If possible a big IO should be split into smaller parts when allocation</span>
<span class="quote">&gt; &gt;   * fails. Partial allocation should not be an error, or you risk a live-lock.</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt; diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; index 1a27f45ec776..381cb50a673c 100644</span>
<span class="quote">&gt; &gt; --- a/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; +++ b/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; @@ -289,7 +289,7 @@ struct io_context *get_task_io_context(struct task_struct *task,</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;         struct io_context *ioc;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -       might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; &gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         do {</span>
<span class="quote">&gt; &gt;                 task_lock(task);</span>
<span class="quote">&gt; &gt; diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; index 9b6e28830b82..a8b46659ce4e 100644</span>
<span class="quote">&gt; &gt; --- a/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; +++ b/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; @@ -264,7 +264,7 @@ static int bt_get(struct blk_mq_alloc_data *data,</span>
<span class="quote">&gt; &gt;         if (tag != -1)</span>
<span class="quote">&gt; &gt;                 return tag;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -       if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt; +       if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
<span class="quote">&gt; &gt;                 return -1;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         bs = bt_wait_ptr(bt, hctx);</span>
<span class="quote">&gt; &gt; diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="quote">&gt; &gt; index 7d842db59699..7d80379d7a38 100644</span>
<span class="quote">&gt; &gt; --- a/block/blk-mq.c</span>
<span class="quote">&gt; &gt; +++ b/block/blk-mq.c</span>
<span class="quote">&gt; &gt; @@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
<span class="quote">&gt; &gt;                 if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))</span>
<span class="quote">&gt; &gt;                         return 0;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -               if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt; +               if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt; &gt;                         return -EBUSY;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;                 ret = wait_event_interruptible(q-&gt;mq_freeze_wq,</span>
<span class="quote">&gt; &gt; @@ -261,11 +261,11 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt; &gt;         hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt; &gt; -       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="quote">&gt; &gt; +       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
<span class="quote">&gt; &gt;                         reserved, ctx, hctx);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt; &gt; -       if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; &gt; +       if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt; &gt;                 __blk_mq_run_hw_queue(hctx);</span>
<span class="quote">&gt; &gt;                 blk_mq_put_ctx(ctx);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any reason not to use gfpflags_allow_nonblocking() here?</span>
<span class="quote">&gt; There are some places not using this helper and reason isn&#39;t</span>
<span class="quote">&gt; specified.</span>
<span class="quote">&gt; </span>

Strictly speaking the helper could be used. However, in cases where the
same function manipulates or examines the flag in any way, I did not use
the helper. It&#39;s in all those cases, I thought the final result was
easier to follow.
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt; + * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="quote">&gt; &gt; + * wake kswapd to reclaim pages in the background until the high</span>
<span class="quote">&gt; &gt; + * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="quote">&gt; &gt; + * avoid unnecessary delays when a fallback option is available but</span>
<span class="quote">&gt; &gt; + * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="quote">&gt; &gt; + * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="quote">&gt; &gt; + * disruption.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#define __GFP_WAIT (__GFP_DIRECT_RECLAIM|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Convention is that combination of gfp flags don&#39;t use __XXX.</span>
<span class="quote">&gt; </span>

I don&#39;t understand. GFP_MOVABLE_MASK, GFP_USER and a bunch of other
combinations use __XXX.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57221">Joonsoo Kim</a> - Sept. 18, 2015, 6:25 a.m.</div>
<pre class="content">
On Wed, Sep 09, 2015 at 01:22:03PM +0100, Mel Gorman wrote:
<span class="quote">&gt; On Tue, Sep 08, 2015 at 03:49:58PM +0900, Joonsoo Kim wrote:</span>
<span class="quote">&gt; &gt; 2015-08-24 21:09 GMT+09:00 Mel Gorman &lt;mgorman@techsingularity.net&gt;:</span>
<span class="quote">&gt; &gt; &gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; &gt; &gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; &gt; &gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; &gt; &gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; &gt; &gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; &gt; &gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; &gt; &gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; &gt; &gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; &gt; &gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; &gt; &gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; &gt; &gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; &gt; &gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; &gt; &gt; background reclaim.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hello, Mel.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think that it is better to do one thing at one patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This was a case where the incremental change felt unnecessary. The purpose</span>
<span class="quote">&gt; of the patch is to &quot;distinguish between being unable to sleep, unwilling</span>
<span class="quote">&gt; to sleep and avoiding waking kswapd&quot;. Splitting that up is possible but</span>
<span class="quote">&gt; I&#39;m not convinced it helps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; To distinguish real atomic, we just need to introduce __GFP_ATOMIC and</span>
<span class="quote">&gt; &gt; make GFP_ATOMIC to __GFP_ATOMIC | GFP_HARDER and change related</span>
<span class="quote">&gt; &gt; things. __GFP_WAIT changes isn&#39;t needed at all for this purpose. It can</span>
<span class="quote">&gt; &gt; reduce patch size and provides more good bisectability.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And, I don&#39;t think that introducing __GFP_KSWAPD_RECLAIM is good thing.</span>
<span class="quote">&gt; &gt; Basically, kswapd reclaim should be enforced.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Several years ago, I would have agreed. Now there are callers that want</span>
<span class="quote">&gt; to control kswapd and I think it made more sense to clearly state whether</span>
<span class="quote">&gt; RECLAIM and KSWAPD are allowed instead of having RECLAIM and NO_KSWAPD</span>
<span class="quote">&gt; flags -- i.e. flags that consistently allow or consistently deny.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; New flag makes user who manually</span>
<span class="quote">&gt; &gt; manipulate gfp flag more difficult. Without this change, your second hazard will</span>
<span class="quote">&gt; &gt; be disappeared although it is almost harmless.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And, I doubt that this big one shot change is preferable. AFAIK, even if changes</span>
<span class="quote">&gt; &gt; are one to one mapping and no functional difference, each one is made by</span>
<span class="quote">&gt; &gt; one patch and send it to correct maintainer. I guess there is some difficulty</span>
<span class="quote">&gt; &gt; in this patch to do like this, but, it could. Isn&#39;t it?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Splitting this into one patch per maintainer would be a review and bisection</span>
<span class="quote">&gt; nightmare. If I saw someone else doing that I would wonder if they were</span>
<span class="quote">&gt; just trying to increase their patch count for no reason.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Some nitpicks are below.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; &gt; index 03e75fef15b8..86809bd2026d 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; &gt; &gt; @@ -25,7 +25,7 @@</span>
<span class="quote">&gt; &gt; &gt;  unsigned long xen_get_swiotlb_free_pages(unsigned int order)</span>
<span class="quote">&gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt;         struct memblock_region *reg;</span>
<span class="quote">&gt; &gt; &gt; -       gfp_t flags = __GFP_NOWARN;</span>
<span class="quote">&gt; &gt; &gt; +       gfp_t flags = __GFP_NOWARN|___GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Please use __XXX rather than ___XXX.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fixed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; @@ -457,13 +457,13 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
<span class="quote">&gt; &gt; &gt;                  * We solve this, and guarantee forward progress, with a rescuer</span>
<span class="quote">&gt; &gt; &gt;                  * workqueue per bio_set. If we go to allocate and there are</span>
<span class="quote">&gt; &gt; &gt;                  * bios on current-&gt;bio_list, we first try the allocation</span>
<span class="quote">&gt; &gt; &gt; -                * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="quote">&gt; &gt; &gt; -                * would be blocking to the rescuer workqueue before we retry</span>
<span class="quote">&gt; &gt; &gt; -                * with the original gfp_flags.</span>
<span class="quote">&gt; &gt; &gt; +                * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="quote">&gt; &gt; &gt; +                * bios we would be blocking to the rescuer workqueue before</span>
<span class="quote">&gt; &gt; &gt; +                * we retry with the original gfp_flags.</span>
<span class="quote">&gt; &gt; &gt;                  */</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;                 if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="quote">&gt; &gt; &gt; -                       gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; &gt; &gt; +                       gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; How about introduce helper function to mask out __GFP_DIRECT_RECLAIM?</span>
<span class="quote">&gt; &gt; It can be used many places.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In this case, the pattern for removing a single flag is easier to recognise</span>
<span class="quote">&gt; than a helper whose implementation must be examined.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt;                 p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);</span>
<span class="quote">&gt; &gt; &gt;                 if (!p &amp;&amp; gfp_mask != saved_gfp) {</span>
<span class="quote">&gt; &gt; &gt; diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="quote">&gt; &gt; &gt; index 627ed0c593fb..e3605acaaffc 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/block/blk-core.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/block/blk-core.c</span>
<span class="quote">&gt; &gt; &gt; @@ -1156,8 +1156,8 @@ static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
<span class="quote">&gt; &gt; &gt;   * @bio: bio to allocate request for (can be %NULL)</span>
<span class="quote">&gt; &gt; &gt;   * @gfp_mask: allocation mask</span>
<span class="quote">&gt; &gt; &gt;   *</span>
<span class="quote">&gt; &gt; &gt; - * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="quote">&gt; &gt; &gt; - * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; &gt; &gt; + * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="quote">&gt; &gt; &gt; + * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; &gt; &gt;   *</span>
<span class="quote">&gt; &gt; &gt;   * Must be called with @q-&gt;queue_lock held and,</span>
<span class="quote">&gt; &gt; &gt;   * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.</span>
<span class="quote">&gt; &gt; &gt; @@ -1177,7 +1177,7 @@ static struct request *get_request(struct request_queue *q, int rw_flags,</span>
<span class="quote">&gt; &gt; &gt;         if (!IS_ERR(rq))</span>
<span class="quote">&gt; &gt; &gt;                 return rq;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; -       if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; &gt; &gt; +       if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; &gt; &gt;                 blk_put_rl(rl);</span>
<span class="quote">&gt; &gt; &gt;                 return rq;</span>
<span class="quote">&gt; &gt; &gt;         }</span>
<span class="quote">&gt; &gt; &gt; @@ -1255,11 +1255,11 @@ EXPORT_SYMBOL(blk_get_request);</span>
<span class="quote">&gt; &gt; &gt;   * BUG.</span>
<span class="quote">&gt; &gt; &gt;   *</span>
<span class="quote">&gt; &gt; &gt;   * WARNING: When allocating/cloning a bio-chain, careful consideration should be</span>
<span class="quote">&gt; &gt; &gt; - * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="quote">&gt; &gt; &gt; - * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="quote">&gt; &gt; &gt; - * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="quote">&gt; &gt; &gt; - * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="quote">&gt; &gt; &gt; - * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; &gt; &gt; + * given to how you allocate bios. In particular, you cannot use</span>
<span class="quote">&gt; &gt; &gt; + * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="quote">&gt; &gt; &gt; + * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="quote">&gt; &gt; &gt; + * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="quote">&gt; &gt; &gt; + * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; &gt; &gt;   * If possible a big IO should be split into smaller parts when allocation</span>
<span class="quote">&gt; &gt; &gt;   * fails. Partial allocation should not be an error, or you risk a live-lock.</span>
<span class="quote">&gt; &gt; &gt;   */</span>
<span class="quote">&gt; &gt; &gt; diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; &gt; index 1a27f45ec776..381cb50a673c 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/block/blk-ioc.c</span>
<span class="quote">&gt; &gt; &gt; @@ -289,7 +289,7 @@ struct io_context *get_task_io_context(struct task_struct *task,</span>
<span class="quote">&gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt;         struct io_context *ioc;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; -       might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; &gt; &gt; +       might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;         do {</span>
<span class="quote">&gt; &gt; &gt;                 task_lock(task);</span>
<span class="quote">&gt; &gt; &gt; diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; &gt; index 9b6e28830b82..a8b46659ce4e 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/block/blk-mq-tag.c</span>
<span class="quote">&gt; &gt; &gt; @@ -264,7 +264,7 @@ static int bt_get(struct blk_mq_alloc_data *data,</span>
<span class="quote">&gt; &gt; &gt;         if (tag != -1)</span>
<span class="quote">&gt; &gt; &gt;                 return tag;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; -       if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt; &gt; +       if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
<span class="quote">&gt; &gt; &gt;                 return -1;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;         bs = bt_wait_ptr(bt, hctx);</span>
<span class="quote">&gt; &gt; &gt; diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="quote">&gt; &gt; &gt; index 7d842db59699..7d80379d7a38 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/block/blk-mq.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/block/blk-mq.c</span>
<span class="quote">&gt; &gt; &gt; @@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
<span class="quote">&gt; &gt; &gt;                 if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))</span>
<span class="quote">&gt; &gt; &gt;                         return 0;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; -               if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; &gt; &gt; +               if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt; &gt; &gt;                         return -EBUSY;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;                 ret = wait_event_interruptible(q-&gt;mq_freeze_wq,</span>
<span class="quote">&gt; &gt; &gt; @@ -261,11 +261,11 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;         ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt; &gt; &gt;         hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt; &gt; &gt; -       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="quote">&gt; &gt; &gt; +       blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
<span class="quote">&gt; &gt; &gt;                         reserved, ctx, hctx);</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;         rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt; &gt; &gt; -       if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; &gt; &gt; +       if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt; &gt; &gt;                 __blk_mq_run_hw_queue(hctx);</span>
<span class="quote">&gt; &gt; &gt;                 blk_mq_put_ctx(ctx);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is there any reason not to use gfpflags_allow_nonblocking() here?</span>
<span class="quote">&gt; &gt; There are some places not using this helper and reason isn&#39;t</span>
<span class="quote">&gt; &gt; specified.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Strictly speaking the helper could be used. However, in cases where the</span>
<span class="quote">&gt; same function manipulates or examines the flag in any way, I did not use</span>
<span class="quote">&gt; the helper. It&#39;s in all those cases, I thought the final result was</span>
<span class="quote">&gt; easier to follow.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;  /*</span>
<span class="quote">&gt; &gt; &gt; + * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="quote">&gt; &gt; &gt; + * wake kswapd to reclaim pages in the background until the high</span>
<span class="quote">&gt; &gt; &gt; + * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="quote">&gt; &gt; &gt; + * avoid unnecessary delays when a fallback option is available but</span>
<span class="quote">&gt; &gt; &gt; + * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="quote">&gt; &gt; &gt; + * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="quote">&gt; &gt; &gt; + * disruption.</span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; +#define __GFP_WAIT (__GFP_DIRECT_RECLAIM|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Convention is that combination of gfp flags don&#39;t use __XXX.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t understand. GFP_MOVABLE_MASK, GFP_USER and a bunch of other</span>
<span class="quote">&gt; combinations use __XXX.</span>

Hello, Mel.
Sorry for late response.

Yes, GFP_XXX can consist of multiple __GFP_XXX.
But, __GFP_XXX doesn&#39;t consist of multiple __GFP_YYY.
Your __GFP_WAIT seems to be a first one.

Thanks.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/balance b/Documentation/vm/balance</span>
<span class="p_header">index c46e68cf9344..964595481af6 100644</span>
<span class="p_header">--- a/Documentation/vm/balance</span>
<span class="p_header">+++ b/Documentation/vm/balance</span>
<span class="p_chunk">@@ -1,12 +1,14 @@</span> <span class="p_context"></span>
 Started Jan 2000 by Kanoj Sarcar &lt;kanoj@sgi.com&gt;
 
<span class="p_del">-Memory balancing is needed for non __GFP_WAIT as well as for non</span>
<span class="p_del">-__GFP_IO allocations.</span>
<span class="p_add">+Memory balancing is needed for !__GFP_ATOMIC and !__GFP_KSWAPD_RECLAIM as</span>
<span class="p_add">+well as for non __GFP_IO allocations.</span>
 
<span class="p_del">-There are two reasons to be requesting non __GFP_WAIT allocations:</span>
<span class="p_del">-the caller can not sleep (typically intr context), or does not want</span>
<span class="p_del">-to incur cost overheads of page stealing and possible swap io for</span>
<span class="p_del">-whatever reasons.</span>
<span class="p_add">+The first reason why a caller may avoid reclaim is that the caller can not</span>
<span class="p_add">+sleep due to holding a spinlock or is in interrupt context. The second may</span>
<span class="p_add">+be that the caller is willing to fail the allocation without incurring the</span>
<span class="p_add">+overhead of page reclaim. This may happen for opportunistic high-order</span>
<span class="p_add">+allocation requests that have order-0 fallback options. In such cases,</span>
<span class="p_add">+the caller may also wish to avoid waking kswapd.</span>
 
 __GFP_IO allocation requests are made to prevent file system deadlocks.
 
<span class="p_header">diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">index cba12f34ff77..f999f0987a3e 100644</span>
<span class="p_header">--- a/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -650,7 +650,7 @@</span> <span class="p_context"> static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,</span>
 
 	if (is_coherent || nommu())
 		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);
<span class="p_del">-	else if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	else if (!gfpflags_allow_blocking(gfp))</span>
 		addr = __alloc_from_pool(size, &amp;page);
 	else if (!dev_get_cma_area(dev))
 		addr = __alloc_remap_buffer(dev, size, gfp, prot, &amp;page, caller, want_vaddr);
<span class="p_chunk">@@ -1369,7 +1369,7 @@</span> <span class="p_context"> static void *arm_iommu_alloc_attrs(struct device *dev, size_t size,</span>
 	*handle = DMA_ERROR_CODE;
 	size = PAGE_ALIGN(size);
 
<span class="p_del">-	if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp))</span>
 		return __iommu_alloc_atomic(dev, size, handle);
 
 	/*
<span class="p_header">diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="p_header">index 03e75fef15b8..86809bd2026d 100644</span>
<span class="p_header">--- a/arch/arm/xen/mm.c</span>
<span class="p_header">+++ b/arch/arm/xen/mm.c</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"></span>
 unsigned long xen_get_swiotlb_free_pages(unsigned int order)
 {
 	struct memblock_region *reg;
<span class="p_del">-	gfp_t flags = __GFP_NOWARN;</span>
<span class="p_add">+	gfp_t flags = __GFP_NOWARN|___GFP_KSWAPD_RECLAIM;</span>
 
 	for_each_memblock(memory, reg) {
 		if (reg-&gt;base &lt; (phys_addr_t)0xffffffff) {
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index d16a1cead23f..1f10b2503af8 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -100,7 +100,7 @@</span> <span class="p_context"> static void *__dma_alloc_coherent(struct device *dev, size_t size,</span>
 	if (IS_ENABLED(CONFIG_ZONE_DMA) &amp;&amp;
 	    dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(32))
 		flags |= GFP_DMA;
<span class="p_del">-	if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; (flags &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_DMA_CMA) &amp;&amp; gfpflags_allow_blocking(flags)) {</span>
 		struct page *page;
 		void *addr;
 
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> static void *__dma_alloc(struct device *dev, size_t size,</span>
 
 	size = PAGE_ALIGN(size);
 
<span class="p_del">-	if (!coherent &amp;&amp; !(flags &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {</span>
 		struct page *page = NULL;
 		void *addr = __alloc_from_pool(size, &amp;page, flags);
 
<span class="p_header">diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">index 353972c1946c..0310e73e6b57 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-dma.c</span>
<span class="p_chunk">@@ -101,7 +101,7 @@</span> <span class="p_context"> void *dma_generic_alloc_coherent(struct device *dev, size_t size,</span>
 again:
 	page = NULL;
 	/* CMA can be used only in the context which permits sleeping */
<span class="p_del">-	if (flag &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flag)) {</span>
 		page = dma_alloc_from_contiguous(dev, count, get_order(size));
 		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {
 			dma_release_from_contiguous(dev, page, count);
<span class="p_header">diff --git a/block/bio.c b/block/bio.c</span>
<span class="p_header">index d6e5ba3399f0..fbc558b50e67 100644</span>
<span class="p_header">--- a/block/bio.c</span>
<span class="p_header">+++ b/block/bio.c</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
 		bvl = mempool_alloc(pool, gfp_mask);
 	} else {
 		struct biovec_slab *bvs = bvec_slabs + *idx;
<span class="p_del">-		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_WAIT | __GFP_IO);</span>
<span class="p_add">+		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM | __GFP_IO);</span>
 
 		/*
 		 * Make this allocation restricted and don&#39;t dump info on
<span class="p_chunk">@@ -221,11 +221,11 @@</span> <span class="p_context"> struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
 		__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;
 
 		/*
<span class="p_del">-		 * Try a slab allocation. If this fails and __GFP_WAIT</span>
<span class="p_add">+		 * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM</span>
 		 * is set, retry with the 1-entry mempool
 		 */
 		bvl = kmem_cache_alloc(bvs-&gt;slab, __gfp_mask);
<span class="p_del">-		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_WAIT))) {</span>
<span class="p_add">+		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))) {</span>
 			*idx = BIOVEC_MAX_IDX;
 			goto fallback;
 		}
<span class="p_chunk">@@ -393,12 +393,12 @@</span> <span class="p_context"> static void punt_bios_to_rescuer(struct bio_set *bs)</span>
  *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is
  *   backed by the @bs&#39;s mempool.
  *
<span class="p_del">- *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be</span>
<span class="p_del">- *   able to allocate a bio. This is due to the mempool guarantees. To make this</span>
<span class="p_del">- *   work, callers must never allocate more than 1 bio at a time from this pool.</span>
<span class="p_del">- *   Callers that need to allocate more than 1 bio must always submit the</span>
<span class="p_del">- *   previously allocated bio for IO before attempting to allocate a new one.</span>
<span class="p_del">- *   Failure to do so can cause deadlocks under memory pressure.</span>
<span class="p_add">+ *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will</span>
<span class="p_add">+ *   always be able to allocate a bio. This is due to the mempool guarantees.</span>
<span class="p_add">+ *   To make this work, callers must never allocate more than 1 bio at a time</span>
<span class="p_add">+ *   from this pool. Callers that need to allocate more than 1 bio must always</span>
<span class="p_add">+ *   submit the previously allocated bio for IO before attempting to allocate</span>
<span class="p_add">+ *   a new one. Failure to do so can cause deadlocks under memory pressure.</span>
  *
  *   Note that when running under generic_make_request() (i.e. any block
  *   driver), bios are not submitted until after you return - see the code in
<span class="p_chunk">@@ -457,13 +457,13 @@</span> <span class="p_context"> struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
 		 * We solve this, and guarantee forward progress, with a rescuer
 		 * workqueue per bio_set. If we go to allocate and there are
 		 * bios on current-&gt;bio_list, we first try the allocation
<span class="p_del">-		 * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="p_del">-		 * would be blocking to the rescuer workqueue before we retry</span>
<span class="p_del">-		 * with the original gfp_flags.</span>
<span class="p_add">+		 * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="p_add">+		 * bios we would be blocking to the rescuer workqueue before</span>
<span class="p_add">+		 * we retry with the original gfp_flags.</span>
 		 */
 
 		if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))
<span class="p_del">-			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 
 		p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);
 		if (!p &amp;&amp; gfp_mask != saved_gfp) {
<span class="p_header">diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="p_header">index 627ed0c593fb..e3605acaaffc 100644</span>
<span class="p_header">--- a/block/blk-core.c</span>
<span class="p_header">+++ b/block/blk-core.c</span>
<span class="p_chunk">@@ -1156,8 +1156,8 @@</span> <span class="p_context"> static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
  * @bio: bio to allocate request for (can be %NULL)
  * @gfp_mask: allocation mask
  *
<span class="p_del">- * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="p_del">- * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="p_add">+ * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="p_add">+ * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
  *
  * Must be called with @q-&gt;queue_lock held and,
  * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.
<span class="p_chunk">@@ -1177,7 +1177,7 @@</span> <span class="p_context"> static struct request *get_request(struct request_queue *q, int rw_flags,</span>
 	if (!IS_ERR(rq))
 		return rq;
 
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
 		blk_put_rl(rl);
 		return rq;
 	}
<span class="p_chunk">@@ -1255,11 +1255,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(blk_get_request);</span>
  * BUG.
  *
  * WARNING: When allocating/cloning a bio-chain, careful consideration should be
<span class="p_del">- * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="p_del">- * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="p_del">- * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="p_del">- * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="p_del">- * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="p_add">+ * given to how you allocate bios. In particular, you cannot use</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="p_add">+ * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="p_add">+ * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="p_add">+ * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
  * If possible a big IO should be split into smaller parts when allocation
  * fails. Partial allocation should not be an error, or you risk a live-lock.
  */
<span class="p_header">diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="p_header">index 1a27f45ec776..381cb50a673c 100644</span>
<span class="p_header">--- a/block/blk-ioc.c</span>
<span class="p_header">+++ b/block/blk-ioc.c</span>
<span class="p_chunk">@@ -289,7 +289,7 @@</span> <span class="p_context"> struct io_context *get_task_io_context(struct task_struct *task,</span>
 {
 	struct io_context *ioc;
 
<span class="p_del">-	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
 
 	do {
 		task_lock(task);
<span class="p_header">diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="p_header">index 9b6e28830b82..a8b46659ce4e 100644</span>
<span class="p_header">--- a/block/blk-mq-tag.c</span>
<span class="p_header">+++ b/block/blk-mq-tag.c</span>
<span class="p_chunk">@@ -264,7 +264,7 @@</span> <span class="p_context"> static int bt_get(struct blk_mq_alloc_data *data,</span>
 	if (tag != -1)
 		return tag;
 
<span class="p_del">-	if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
 		return -1;
 
 	bs = bt_wait_ptr(bt, hctx);
<span class="p_header">diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="p_header">index 7d842db59699..7d80379d7a38 100644</span>
<span class="p_header">--- a/block/blk-mq.c</span>
<span class="p_header">+++ b/block/blk-mq.c</span>
<span class="p_chunk">@@ -85,7 +85,7 @@</span> <span class="p_context"> static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
 		if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))
 			return 0;
 
<span class="p_del">-		if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (!gfpflags_allow_blocking(gfp))</span>
 			return -EBUSY;
 
 		ret = wait_event_interruptible(q-&gt;mq_freeze_wq,
<span class="p_chunk">@@ -261,11 +261,11 @@</span> <span class="p_context"> struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
 
 	ctx = blk_mq_get_ctx(q);
 	hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);
<span class="p_del">-	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="p_add">+	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
 			reserved, ctx, hctx);
 
 	rq = __blk_mq_alloc_request(&amp;alloc_data, rw);
<span class="p_del">-	if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
 		__blk_mq_run_hw_queue(hctx);
 		blk_mq_put_ctx(ctx);
 
<span class="p_chunk">@@ -1221,7 +1221,7 @@</span> <span class="p_context"> static struct request *blk_mq_map_request(struct request_queue *q,</span>
 		ctx = blk_mq_get_ctx(q);
 		hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);
 		blk_mq_set_alloc_data(&amp;alloc_data, q,
<span class="p_del">-				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);</span>
<span class="p_add">+				__GFP_WAIT|__GFP_HIGH, false, ctx, hctx);</span>
 		rq = __blk_mq_alloc_request(&amp;alloc_data, rw);
 		ctx = alloc_data.ctx;
 		hctx = alloc_data.hctx;
<span class="p_header">diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c</span>
<span class="p_header">index c62bb2e650b8..ecd1d1b61382 100644</span>
<span class="p_header">--- a/block/cfq-iosched.c</span>
<span class="p_header">+++ b/block/cfq-iosched.c</span>
<span class="p_chunk">@@ -3674,7 +3674,7 @@</span> <span class="p_context"> cfq_find_alloc_queue(struct cfq_data *cfqd, bool is_sync, struct cfq_io_cq *cic,</span>
 		if (new_cfqq) {
 			cfqq = new_cfqq;
 			new_cfqq = NULL;
<span class="p_del">-		} else if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="p_add">+		} else if (gfpflags_allow_blocking(gfp_mask)) {</span>
 			rcu_read_unlock();
 			spin_unlock_irq(cfqd-&gt;queue-&gt;queue_lock);
 			new_cfqq = kmem_cache_alloc_node(cfq_pool,
<span class="p_chunk">@@ -4289,7 +4289,7 @@</span> <span class="p_context"> cfq_set_request(struct request_queue *q, struct request *rq, struct bio *bio,</span>
 	const bool is_sync = rq_is_sync(rq);
 	struct cfq_queue *cfqq;
 
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
 
 	spin_lock_irq(q-&gt;queue_lock);
 
<span class="p_header">diff --git a/drivers/block/drbd/drbd_receiver.c b/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_header">index c097909c589c..b4b5680ac6ad 100644</span>
<span class="p_header">--- a/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_header">+++ b/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_chunk">@@ -357,7 +357,8 @@</span> <span class="p_context"> drbd_alloc_peer_req(struct drbd_peer_device *peer_device, u64 id, sector_t secto</span>
 	}
 
 	if (has_payload &amp;&amp; data_size) {
<span class="p_del">-		page = drbd_alloc_pages(peer_device, nr_pages, (gfp_mask &amp; __GFP_WAIT));</span>
<span class="p_add">+		page = drbd_alloc_pages(peer_device, nr_pages,</span>
<span class="p_add">+					gfpflags_allow_blocking(gfp_mask));</span>
 		if (!page)
 			goto fail;
 	}
<span class="p_header">diff --git a/drivers/block/osdblk.c b/drivers/block/osdblk.c</span>
<span class="p_header">index e22942596207..1b709a4e3b5e 100644</span>
<span class="p_header">--- a/drivers/block/osdblk.c</span>
<span class="p_header">+++ b/drivers/block/osdblk.c</span>
<span class="p_chunk">@@ -271,7 +271,7 @@</span> <span class="p_context"> static struct bio *bio_chain_clone(struct bio *old_chain, gfp_t gfpmask)</span>
 			goto err_out;
 
 		tmp-&gt;bi_bdev = NULL;
<span class="p_del">-		gfpmask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+		gfpmask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 		tmp-&gt;bi_next = NULL;
 
 		if (!new_chain)
<span class="p_header">diff --git a/drivers/connector/connector.c b/drivers/connector/connector.c</span>
<span class="p_header">index 30f522848c73..d7373ca69c99 100644</span>
<span class="p_header">--- a/drivers/connector/connector.c</span>
<span class="p_header">+++ b/drivers/connector/connector.c</span>
<span class="p_chunk">@@ -124,7 +124,8 @@</span> <span class="p_context"> int cn_netlink_send_mult(struct cn_msg *msg, u16 len, u32 portid, u32 __group,</span>
 	if (group)
 		return netlink_broadcast(dev-&gt;nls, skb, portid, group,
 					 gfp_mask);
<span class="p_del">-	return netlink_unicast(dev-&gt;nls, skb, portid, !(gfp_mask&amp;__GFP_WAIT));</span>
<span class="p_add">+	return netlink_unicast(dev-&gt;nls, skb, portid,</span>
<span class="p_add">+			!gfpflags_allow_blocking(gfp_mask));</span>
 }
 EXPORT_SYMBOL_GPL(cn_netlink_send_mult);
 
<span class="p_header">diff --git a/drivers/firewire/core-cdev.c b/drivers/firewire/core-cdev.c</span>
<span class="p_header">index 2a3973a7c441..36a7c2d89a01 100644</span>
<span class="p_header">--- a/drivers/firewire/core-cdev.c</span>
<span class="p_header">+++ b/drivers/firewire/core-cdev.c</span>
<span class="p_chunk">@@ -486,7 +486,7 @@</span> <span class="p_context"> static int ioctl_get_info(struct client *client, union ioctl_arg *arg)</span>
 static int add_client_resource(struct client *client,
 			       struct client_resource *resource, gfp_t gfp_mask)
 {
<span class="p_del">-	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
 	unsigned long flags;
 	int ret;
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index 52b446b27b4d..c2b45081c5ab 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -2225,7 +2225,7 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	 */
 	mapping = file_inode(obj-&gt;base.filp)-&gt;i_mapping;
 	gfp = mapping_gfp_mask(mapping);
<span class="p_del">-	gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;</span>
<span class="p_add">+	gfp |= __GFP_NORETRY | __GFP_NOWARN;</span>
 	gfp &amp;= ~(__GFP_IO | __GFP_WAIT);
 	sg = st-&gt;sgl;
 	st-&gt;nents = 0;
<span class="p_header">diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c</span>
<span class="p_header">index ca919f429666..7474d79ffac0 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/sa_query.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/sa_query.c</span>
<span class="p_chunk">@@ -619,7 +619,7 @@</span> <span class="p_context"> static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)</span>
 
 static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)
 {
<span class="p_del">-	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
 	unsigned long flags;
 	int ret, id;
 
<span class="p_header">diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="p_header">index 658ee39e6569..95d4c70dc7b1 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu.c</span>
<span class="p_chunk">@@ -2755,7 +2755,7 @@</span> <span class="p_context"> static void *alloc_coherent(struct device *dev, size_t size,</span>
 
 	page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));
 	if (!page) {
<span class="p_del">-		if (!(flag &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (!gfpflags_allow_blocking(flag))</span>
 			return NULL;
 
 		page = dma_alloc_from_contiguous(dev, size &gt;&gt; PAGE_SHIFT,
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 0649b94f5958..f77becf3d8d8 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -3566,7 +3566,7 @@</span> <span class="p_context"> static void *intel_alloc_coherent(struct device *dev, size_t size,</span>
 			flags |= GFP_DMA32;
 	}
 
<span class="p_del">-	if (flags &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags)) {</span>
 		unsigned int count = size &gt;&gt; PAGE_SHIFT;
 
 		page = dma_alloc_from_contiguous(dev, count, order);
<span class="p_header">diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="p_header">index 0f48fed44a17..6dda08385309 100644</span>
<span class="p_header">--- a/drivers/md/dm-crypt.c</span>
<span class="p_header">+++ b/drivers/md/dm-crypt.c</span>
<span class="p_chunk">@@ -993,7 +993,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 	struct bio_vec *bvec;
 
 retry:
<span class="p_del">-	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		mutex_lock(&amp;cc-&gt;bio_alloc_lock);
 
 	clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, cc-&gt;bs);
<span class="p_chunk">@@ -1009,7 +1009,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 		if (!page) {
 			crypt_free_buffer_pages(cc, clone);
 			bio_put(clone);
<span class="p_del">-			gfp_mask |= __GFP_WAIT;</span>
<span class="p_add">+			gfp_mask |= __GFP_DIRECT_RECLAIM;</span>
 			goto retry;
 		}
 
<span class="p_chunk">@@ -1026,7 +1026,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 	}
 
 return_clone:
<span class="p_del">-	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		mutex_unlock(&amp;cc-&gt;bio_alloc_lock);
 
 	return clone;
<span class="p_header">diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c</span>
<span class="p_header">index 3a7cade5e27d..1452ed9aacb4 100644</span>
<span class="p_header">--- a/drivers/md/dm-kcopyd.c</span>
<span class="p_header">+++ b/drivers/md/dm-kcopyd.c</span>
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> static int kcopyd_get_pages(struct dm_kcopyd_client *kc,</span>
 	*pages = NULL;
 
 	do {
<span class="p_del">-		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY);</span>
<span class="p_add">+		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);</span>
 		if (unlikely(!pl)) {
 			/* Use reserved pages */
 			pl = kc-&gt;pages;
<span class="p_header">diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_header">index 53fff5425c13..fb2cb4bdc0c1 100644</span>
<span class="p_header">--- a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_header">+++ b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_chunk">@@ -1291,7 +1291,7 @@</span> <span class="p_context"> static struct solo_enc_dev *solo_enc_alloc(struct solo_dev *solo_dev,</span>
 	solo_enc-&gt;vidq.ops = &amp;solo_enc_video_qops;
 	solo_enc-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;
 	solo_enc-&gt;vidq.drv_priv = solo_enc;
<span class="p_del">-	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	solo_enc-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;
 	solo_enc-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);
 	solo_enc-&gt;vidq.lock = &amp;solo_enc-&gt;lock;
<span class="p_header">diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2.c b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_header">index 63ae8a61f603..bde77b22340c 100644</span>
<span class="p_header">--- a/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_header">+++ b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_chunk">@@ -675,7 +675,7 @@</span> <span class="p_context"> int solo_v4l2_init(struct solo_dev *solo_dev, unsigned nr)</span>
 	solo_dev-&gt;vidq.mem_ops = &amp;vb2_dma_contig_memops;
 	solo_dev-&gt;vidq.drv_priv = solo_dev;
 	solo_dev-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;
<span class="p_del">-	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	solo_dev-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);
 	solo_dev-&gt;vidq.lock = &amp;solo_dev-&gt;lock;
 	ret = vb2_queue_init(&amp;solo_dev-&gt;vidq);
<span class="p_header">diff --git a/drivers/media/pci/tw68/tw68-video.c b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_header">index 8355e55b4e8e..e556f989aaab 100644</span>
<span class="p_header">--- a/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_header">+++ b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_chunk">@@ -975,7 +975,7 @@</span> <span class="p_context"> int tw68_video_init2(struct tw68_dev *dev, int video_nr)</span>
 	dev-&gt;vidq.ops = &amp;tw68_video_qops;
 	dev-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;
 	dev-&gt;vidq.drv_priv = dev;
<span class="p_del">-	dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	dev-&gt;vidq.buf_struct_size = sizeof(struct tw68_buf);
 	dev-&gt;vidq.lock = &amp;dev-&gt;lock;
 	dev-&gt;vidq.min_buffers_needed = 2;
<span class="p_header">diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c</span>
<span class="p_header">index 8bbbb751bf45..2dfb291a47c6 100644</span>
<span class="p_header">--- a/drivers/mtd/mtdcore.c</span>
<span class="p_header">+++ b/drivers/mtd/mtdcore.c</span>
<span class="p_chunk">@@ -1188,8 +1188,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(mtd_writev);</span>
  */
 void *mtd_kmalloc_up_to(const struct mtd_info *mtd, size_t *size)
 {
<span class="p_del">-	gfp_t flags = __GFP_NOWARN | __GFP_WAIT |</span>
<span class="p_del">-		       __GFP_NORETRY | __GFP_NO_KSWAPD;</span>
<span class="p_add">+	gfp_t flags = __GFP_NOWARN | __GFP_DIRECT_RECLAIM | __GFP_NORETRY;</span>
 	size_t min_alloc = max_t(size_t, mtd-&gt;writesize, PAGE_SIZE);
 	void *kbuf;
 
<span class="p_header">diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_header">index f7fbdc9d1325..3a407e59acab 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_chunk">@@ -689,7 +689,7 @@</span> <span class="p_context"> static void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)</span>
 {
 	if (fp-&gt;rx_frag_size) {
 		/* GFP_KERNEL allocations are used only during initialization */
<span class="p_del">-		if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (unlikely(gfpflags_allow_blocking(gfp_mask)))</span>
 			return (void *)__get_free_page(gfp_mask);
 
 		return netdev_alloc_frag(fp-&gt;rx_frag_size);
<span class="p_header">diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_header">index da2a63c0a9ba..2615e0ae4f0a 100644</span>
<span class="p_header">--- a/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_header">+++ b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_chunk">@@ -27,7 +27,7 @@</span> <span class="p_context"></span>
 #include &quot;ion_priv.h&quot;
 
 static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |
<span class="p_del">-				     __GFP_NORETRY) &amp; ~__GFP_WAIT;</span>
<span class="p_add">+				     __GFP_NORETRY) &amp; ~__GFP_DIRECT_RECLAIM;</span>
 static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);
 static const unsigned int orders[] = {8, 4, 0};
 static const int num_orders = ARRAY_SIZE(orders);
<span class="p_header">diff --git a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_header">index ed37d26eb20d..5b0756cb6576 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_header">+++ b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> do {						\</span>
 do {									    \
 	LASSERT(!in_interrupt() ||					    \
 		((size) &lt;= LIBCFS_VMALLOC_SIZE &amp;&amp;			    \
<span class="p_del">-		 ((mask) &amp; __GFP_WAIT) == 0));				    \</span>
<span class="p_add">+		 !gfpflags_allow_blocking(mask)));			    \</span>
 } while (0)
 
 #define LIBCFS_ALLOC_POST(ptr, size)					    \
<span class="p_header">diff --git a/drivers/usb/host/u132-hcd.c b/drivers/usb/host/u132-hcd.c</span>
<span class="p_header">index d51687780b61..8d4c1806e32f 100644</span>
<span class="p_header">--- a/drivers/usb/host/u132-hcd.c</span>
<span class="p_header">+++ b/drivers/usb/host/u132-hcd.c</span>
<span class="p_chunk">@@ -2247,7 +2247,7 @@</span> <span class="p_context"> static int u132_urb_enqueue(struct usb_hcd *hcd, struct urb *urb,</span>
 {
 	struct u132 *u132 = hcd_to_u132(hcd);
 	if (irqs_disabled()) {
<span class="p_del">-		if (__GFP_WAIT &amp; mem_flags) {</span>
<span class="p_add">+		if (gfpflags_allow_blocking(mem_flags)) {</span>
 			printk(KERN_ERR &quot;invalid context for function that migh&quot;
 				&quot;t sleep\n&quot;);
 			return -EINVAL;
<span class="p_header">diff --git a/drivers/video/fbdev/vermilion/vermilion.c b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_header">index 6b70d7f62b2f..1c1e95a0b8fa 100644</span>
<span class="p_header">--- a/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_header">+++ b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_chunk">@@ -99,7 +99,7 @@</span> <span class="p_context"> static int vmlfb_alloc_vram_area(struct vram_area *va, unsigned max_order,</span>
 		 * below the first 16MB.
 		 */
 
<span class="p_del">-		flags = __GFP_DMA | __GFP_HIGH;</span>
<span class="p_add">+		flags = __GFP_DMA | __GFP_HIGH | __GFP_KSWAPD_RECLAIM;</span>
 		va-&gt;logical =
 			 __get_free_pages(flags, --max_order);
 	} while (va-&gt;logical == 0 &amp;&amp; max_order &gt; min_order);
<span class="p_header">diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c</span>
<span class="p_header">index f556c3732c2c..3dd4792b8099 100644</span>
<span class="p_header">--- a/fs/btrfs/disk-io.c</span>
<span class="p_header">+++ b/fs/btrfs/disk-io.c</span>
<span class="p_chunk">@@ -2566,7 +2566,7 @@</span> <span class="p_context"> int open_ctree(struct super_block *sb,</span>
 	fs_info-&gt;commit_interval = BTRFS_DEFAULT_COMMIT_INTERVAL;
 	fs_info-&gt;avg_delayed_ref_runtime = NSEC_PER_SEC &gt;&gt; 6; /* div by 64 */
 	/* readahead state */
<span class="p_del">-	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	spin_lock_init(&amp;fs_info-&gt;reada_lock);
 
 	fs_info-&gt;thread_pool_size = min_t(unsigned long,
<span class="p_header">diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c</span>
<span class="p_header">index 02d05817cbdf..c8a6cdcbef2b 100644</span>
<span class="p_header">--- a/fs/btrfs/extent_io.c</span>
<span class="p_header">+++ b/fs/btrfs/extent_io.c</span>
<span class="p_chunk">@@ -594,7 +594,7 @@</span> <span class="p_context"> int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (bits &amp; (EXTENT_IOBITS | EXTENT_BOUNDARY))
 		clear = 1;
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		/*
 		 * Don&#39;t care for allocation failure here because we might end
 		 * up not needing the pre-allocated extent state at all, which
<span class="p_chunk">@@ -718,7 +718,7 @@</span> <span class="p_context"> int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	goto again;
 }
<span class="p_chunk">@@ -850,7 +850,7 @@</span> <span class="p_context"> __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 
 	bits |= EXTENT_FIRST_DELALLOC;
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		prealloc = alloc_extent_state(mask);
 		BUG_ON(!prealloc);
 	}
<span class="p_chunk">@@ -1028,7 +1028,7 @@</span> <span class="p_context"> __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	goto again;
 }
<span class="p_chunk">@@ -1076,7 +1076,7 @@</span> <span class="p_context"> int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	btrfs_debug_check_extent_io_range(tree, start, end);
 
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		/*
 		 * Best effort, don&#39;t worry if extent state allocation fails
 		 * here for the first iteration. We might have a cached state
<span class="p_chunk">@@ -1253,7 +1253,7 @@</span> <span class="p_context"> int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	first_iteration = false;
 	goto again;
<span class="p_chunk">@@ -4265,7 +4265,7 @@</span> <span class="p_context"> int try_release_extent_mapping(struct extent_map_tree *map,</span>
 	u64 start = page_offset(page);
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 
<span class="p_del">-	if ((mask &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask) &amp;&amp;</span>
 	    page-&gt;mapping-&gt;host-&gt;i_size &gt; 16 * 1024 * 1024) {
 		u64 len;
 		while (start &lt;= end) {
<span class="p_header">diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c</span>
<span class="p_header">index fbe7c104531c..b1968f36a39b 100644</span>
<span class="p_header">--- a/fs/btrfs/volumes.c</span>
<span class="p_header">+++ b/fs/btrfs/volumes.c</span>
<span class="p_chunk">@@ -156,8 +156,8 @@</span> <span class="p_context"> static struct btrfs_device *__alloc_device(void)</span>
 	spin_lock_init(&amp;dev-&gt;reada_lock);
 	atomic_set(&amp;dev-&gt;reada_in_flight, 0);
 	atomic_set(&amp;dev-&gt;dev_stats_ccnt, 0);
<span class="p_del">-	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_del">-	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 
 	return dev;
 }
<span class="p_header">diff --git a/fs/ext3/super.c b/fs/ext3/super.c</span>
<span class="p_header">index 5ed0044fbb37..9004c786716f 100644</span>
<span class="p_header">--- a/fs/ext3/super.c</span>
<span class="p_header">+++ b/fs/ext3/super.c</span>
<span class="p_chunk">@@ -750,7 +750,7 @@</span> <span class="p_context"> static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
 		return 0;
 	if (journal)
 		return journal_try_to_free_buffers(journal, page, 
<span class="p_del">-						   wait &amp; ~__GFP_WAIT);</span>
<span class="p_add">+						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	return try_to_free_buffers(page);
 }
 
<span class="p_header">diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="p_header">index 58987b5c514b..abe76d41ef1e 100644</span>
<span class="p_header">--- a/fs/ext4/super.c</span>
<span class="p_header">+++ b/fs/ext4/super.c</span>
<span class="p_chunk">@@ -1045,7 +1045,7 @@</span> <span class="p_context"> static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
 		return 0;
 	if (journal)
 		return jbd2_journal_try_to_free_buffers(journal, page,
<span class="p_del">-							wait &amp; ~__GFP_WAIT);</span>
<span class="p_add">+						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	return try_to_free_buffers(page);
 }
 
<span class="p_header">diff --git a/fs/fscache/cookie.c b/fs/fscache/cookie.c</span>
<span class="p_header">index d403c69bee08..4304072161aa 100644</span>
<span class="p_header">--- a/fs/fscache/cookie.c</span>
<span class="p_header">+++ b/fs/fscache/cookie.c</span>
<span class="p_chunk">@@ -111,7 +111,7 @@</span> <span class="p_context"> struct fscache_cookie *__fscache_acquire_cookie(</span>
 
 	/* radix tree insertion won&#39;t use the preallocation pool unless it&#39;s
 	 * told it may not wait */
<span class="p_del">-	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 
 	switch (cookie-&gt;def-&gt;type) {
 	case FSCACHE_COOKIE_TYPE_INDEX:
<span class="p_header">diff --git a/fs/fscache/page.c b/fs/fscache/page.c</span>
<span class="p_header">index 483bbc613bf0..79483b3d8c6f 100644</span>
<span class="p_header">--- a/fs/fscache/page.c</span>
<span class="p_header">+++ b/fs/fscache/page.c</span>
<span class="p_chunk">@@ -58,7 +58,7 @@</span> <span class="p_context"> bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)</span>
 
 /*
  * decide whether a page can be released, possibly by cancelling a store to it
<span class="p_del">- * - we&#39;re allowed to sleep if __GFP_WAIT is flagged</span>
<span class="p_add">+ * - we&#39;re allowed to sleep if __GFP_DIRECT_RECLAIM is flagged</span>
  */
 bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 				  struct page *page,
<span class="p_chunk">@@ -122,7 +122,7 @@</span> <span class="p_context"> bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
 	 * allocator as the work threads writing to the cache may all end up
 	 * sleeping on memory allocation, so we may need to impose a timeout
 	 * too. */
<span class="p_del">-	if (!(gfp &amp; __GFP_WAIT) || !(gfp &amp; __GFP_FS)) {</span>
<span class="p_add">+	if (!(gfp &amp; __GFP_DIRECT_RECLAIM) || !(gfp &amp; __GFP_FS)) {</span>
 		fscache_stat(&amp;fscache_n_store_vmscan_busy);
 		return false;
 	}
<span class="p_chunk">@@ -132,7 +132,7 @@</span> <span class="p_context"> bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
 		_debug(&quot;fscache writeout timeout page: %p{%lx}&quot;,
 			page, page-&gt;index);
 
<span class="p_del">-	gfp &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+	gfp &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 	goto try_again;
 }
 EXPORT_SYMBOL(__fscache_maybe_release_page);
<span class="p_header">diff --git a/fs/jbd/transaction.c b/fs/jbd/transaction.c</span>
<span class="p_header">index 1695ba8334a2..f45b90ba7c5c 100644</span>
<span class="p_header">--- a/fs/jbd/transaction.c</span>
<span class="p_header">+++ b/fs/jbd/transaction.c</span>
<span class="p_chunk">@@ -1690,8 +1690,8 @@</span> <span class="p_context"> __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
  * @journal: journal for operation
  * @page: to try and free
  * @gfp_mask: we use the mask to detect how hard should we try to release
<span class="p_del">- * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="p_del">- * release the buffers.</span>
<span class="p_add">+ * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="p_add">+ * code to release the buffers.</span>
  *
  *
  * For all the buffers on this page,
<span class="p_header">diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c</span>
<span class="p_header">index f3d06174b051..06e18bcdb888 100644</span>
<span class="p_header">--- a/fs/jbd2/transaction.c</span>
<span class="p_header">+++ b/fs/jbd2/transaction.c</span>
<span class="p_chunk">@@ -1893,8 +1893,8 @@</span> <span class="p_context"> __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
  * @journal: journal for operation
  * @page: to try and free
  * @gfp_mask: we use the mask to detect how hard should we try to release
<span class="p_del">- * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="p_del">- * release the buffers.</span>
<span class="p_add">+ * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="p_add">+ * code to release the buffers.</span>
  *
  *
  * For all the buffers on this page,
<span class="p_header">diff --git a/fs/nfs/file.c b/fs/nfs/file.c</span>
<span class="p_header">index cc4fa1ed61fc..be6821967ec6 100644</span>
<span class="p_header">--- a/fs/nfs/file.c</span>
<span class="p_header">+++ b/fs/nfs/file.c</span>
<span class="p_chunk">@@ -480,8 +480,8 @@</span> <span class="p_context"> static int nfs_release_page(struct page *page, gfp_t gfp)</span>
 	dfprintk(PAGECACHE, &quot;NFS: release_page(%p)\n&quot;, page);
 
 	/* Always try to initiate a &#39;commit&#39; if relevant, but only
<span class="p_del">-	 * wait for it if __GFP_WAIT is set.  Even then, only wait 1</span>
<span class="p_del">-	 * second and only if the &#39;bdi&#39; is not congested.</span>
<span class="p_add">+	 * wait for it if the caller allows blocking.  Even then,</span>
<span class="p_add">+	 * only wait 1 second and only if the &#39;bdi&#39; is not congested.</span>
 	 * Waiting indefinitely can cause deadlocks when the NFS
 	 * server is on this machine, when a new TCP connection is
 	 * needed and in other rare cases.  There is no particular
<span class="p_chunk">@@ -491,7 +491,7 @@</span> <span class="p_context"> static int nfs_release_page(struct page *page, gfp_t gfp)</span>
 	if (mapping) {
 		struct nfs_server *nfss = NFS_SERVER(mapping-&gt;host);
 		nfs_commit_inode(mapping-&gt;host, 0);
<span class="p_del">-		if ((gfp &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="p_add">+		if (gfpflags_allow_blocking(gfp) &amp;&amp;</span>
 		    !bdi_write_congested(&amp;nfss-&gt;backing_dev_info)) {
 			wait_on_page_bit_killable_timeout(page, PG_private,
 							  HZ);
<span class="p_header">diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c</span>
<span class="p_header">index eac9549efd52..587174fd4f2c 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_qm.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_qm.c</span>
<span class="p_chunk">@@ -525,7 +525,7 @@</span> <span class="p_context"> xfs_qm_shrink_scan(</span>
 	unsigned long		freed;
 	int			error;
 
<span class="p_del">-	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT))</span>
<span class="p_add">+	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_DIRECT_RECLAIM)) != (__GFP_FS|__GFP_DIRECT_RECLAIM))</span>
 		return 0;
 
 	INIT_LIST_HEAD(&amp;isol.buffers);
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index a10347ca5053..bd1937977d84 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -29,12 +29,13 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define ___GFP_NOMEMALLOC	0x10000u
 #define ___GFP_HARDWALL		0x20000u
 #define ___GFP_THISNODE		0x40000u
<span class="p_del">-#define ___GFP_WAIT		0x80000u</span>
<span class="p_add">+#define ___GFP_ATOMIC		0x80000u</span>
 #define ___GFP_NOACCOUNT	0x100000u
 #define ___GFP_NOTRACK		0x200000u
<span class="p_del">-#define ___GFP_NO_KSWAPD	0x400000u</span>
<span class="p_add">+#define ___GFP_DIRECT_RECLAIM	0x400000u</span>
 #define ___GFP_OTHER_NODE	0x800000u
 #define ___GFP_WRITE		0x1000000u
<span class="p_add">+#define ___GFP_KSWAPD_RECLAIM	0x2000000u</span>
 /* If the above are modified, __GFP_BITS_SHIFT may need updating */
 
 /*
<span class="p_chunk">@@ -68,7 +69,7 @@</span> <span class="p_context"> struct vm_area_struct;</span>
  * __GFP_MOVABLE: Flag that this page will be movable by the page migration
  * mechanism or reclaimed
  */
<span class="p_del">-#define __GFP_WAIT	((__force gfp_t)___GFP_WAIT)	/* Can wait and reschedule? */</span>
<span class="p_add">+#define __GFP_ATOMIC	((__force gfp_t)___GFP_ATOMIC)  /* Caller cannot wait or reschedule */</span>
 #define __GFP_HIGH	((__force gfp_t)___GFP_HIGH)	/* Should access emergency pools? */
 #define __GFP_IO	((__force gfp_t)___GFP_IO)	/* Can start physical IO? */
 #define __GFP_FS	((__force gfp_t)___GFP_FS)	/* Can call down to low-level FS? */
<span class="p_chunk">@@ -91,23 +92,37 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define __GFP_NOACCOUNT	((__force gfp_t)___GFP_NOACCOUNT) /* Don&#39;t account to kmemcg */
 #define __GFP_NOTRACK	((__force gfp_t)___GFP_NOTRACK)  /* Don&#39;t track with kmemcheck */
 
<span class="p_del">-#define __GFP_NO_KSWAPD	((__force gfp_t)___GFP_NO_KSWAPD)</span>
 #define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */
 #define __GFP_WRITE	((__force gfp_t)___GFP_WRITE)	/* Allocator intends to dirty page */
 
 /*
<span class="p_add">+ * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="p_add">+ * wake kswapd to reclaim pages in the background until the high</span>
<span class="p_add">+ * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="p_add">+ * avoid unnecessary delays when a fallback option is available but</span>
<span class="p_add">+ * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="p_add">+ * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="p_add">+ * disruption.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __GFP_WAIT (__GFP_DIRECT_RECLAIM|__GFP_KSWAPD_RECLAIM)</span>
<span class="p_add">+#define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="p_add">+#define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * This may seem redundant, but it&#39;s a way of annotating false positives vs.
  * allocations that simply cannot be supported (e.g. page tables).
  */
 #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)
 
<span class="p_del">-#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */</span>
<span class="p_add">+#define __GFP_BITS_SHIFT 26	/* Room for N __GFP_FOO bits */</span>
 #define __GFP_BITS_MASK ((__force gfp_t)((1 &lt;&lt; __GFP_BITS_SHIFT) - 1))
 
<span class="p_del">-/* This equals 0, but use constants in case they ever change */</span>
<span class="p_del">-#define GFP_NOWAIT	(GFP_ATOMIC &amp; ~__GFP_HIGH)</span>
<span class="p_del">-/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */</span>
<span class="p_del">-#define GFP_ATOMIC	(__GFP_HIGH)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * GFP_ATOMIC callers can not sleep, need the allocation to succeed.</span>
<span class="p_add">+ * A lower watermark is applied to allow access to &quot;atomic reserves&quot;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define GFP_ATOMIC	(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)</span>
<span class="p_add">+#define GFP_NOWAIT	(__GFP_KSWAPD_RECLAIM)</span>
 #define GFP_NOIO	(__GFP_WAIT)
 #define GFP_NOFS	(__GFP_WAIT | __GFP_IO)
 #define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)
<span class="p_chunk">@@ -116,10 +131,10 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)
 #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)
 #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)
<span class="p_del">-#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="p_del">-#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="p_del">-			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="p_del">-			 __GFP_NO_KSWAPD)</span>
<span class="p_add">+#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="p_add">+#define GFP_TRANSHUGE	((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="p_add">+			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &amp; \</span>
<span class="p_add">+			 ~__GFP_KSWAPD_RECLAIM)</span>
 
 /* This mask makes up all the page movable related flags */
 #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)
<span class="p_chunk">@@ -161,6 +176,11 @@</span> <span class="p_context"> static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)</span>
 	return (gfp_flags &amp; GFP_MOVABLE_MASK) &gt;&gt; GFP_MOVABLE_SHIFT;
 }
 
<span class="p_add">+static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return gfp_flags &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HIGHMEM
 #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM
 #else
<span class="p_header">diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h</span>
<span class="p_header">index 22b6d9ca1654..55c4a9175801 100644</span>
<span class="p_header">--- a/include/linux/skbuff.h</span>
<span class="p_header">+++ b/include/linux/skbuff.h</span>
<span class="p_chunk">@@ -1109,7 +1109,7 @@</span> <span class="p_context"> static inline int skb_cloned(const struct sk_buff *skb)</span>
 
 static inline int skb_unclone(struct sk_buff *skb, gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 
 	if (skb_cloned(skb))
 		return pskb_expand_head(skb, 0, 0, pri);
<span class="p_chunk">@@ -1193,7 +1193,7 @@</span> <span class="p_context"> static inline int skb_shared(const struct sk_buff *skb)</span>
  */
 static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 	if (skb_shared(skb)) {
 		struct sk_buff *nskb = skb_clone(skb, pri);
 
<span class="p_chunk">@@ -1229,7 +1229,7 @@</span> <span class="p_context"> static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
 static inline struct sk_buff *skb_unshare(struct sk_buff *skb,
 					  gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 	if (skb_cloned(skb)) {
 		struct sk_buff *nskb = skb_copy(skb, pri);
 
<span class="p_header">diff --git a/include/net/sock.h b/include/net/sock.h</span>
<span class="p_header">index f21f0708ec59..cec0c4b634dc 100644</span>
<span class="p_header">--- a/include/net/sock.h</span>
<span class="p_header">+++ b/include/net/sock.h</span>
<span class="p_chunk">@@ -2035,7 +2035,7 @@</span> <span class="p_context"> struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,</span>
  */
 static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
<span class="p_del">-	if (sk-&gt;sk_allocation &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(sk-&gt;sk_allocation))</span>
 		return &amp;current-&gt;task_frag;
 
 	return &amp;sk-&gt;sk_frag;
<span class="p_header">diff --git a/include/trace/events/gfpflags.h b/include/trace/events/gfpflags.h</span>
<span class="p_header">index d6fd8e5b14b7..dde6bf092c8a 100644</span>
<span class="p_header">--- a/include/trace/events/gfpflags.h</span>
<span class="p_header">+++ b/include/trace/events/gfpflags.h</span>
<span class="p_chunk">@@ -20,7 +20,7 @@</span> <span class="p_context"></span>
 	{(unsigned long)GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\
 	{(unsigned long)GFP_NOIO,		&quot;GFP_NOIO&quot;},		\
 	{(unsigned long)__GFP_HIGH,		&quot;GFP_HIGH&quot;},		\
<span class="p_del">-	{(unsigned long)__GFP_WAIT,		&quot;GFP_WAIT&quot;},		\</span>
<span class="p_add">+	{(unsigned long)__GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
 	{(unsigned long)__GFP_IO,		&quot;GFP_IO&quot;},		\
 	{(unsigned long)__GFP_COLD,		&quot;GFP_COLD&quot;},		\
 	{(unsigned long)__GFP_NOWARN,		&quot;GFP_NOWARN&quot;},		\
<span class="p_chunk">@@ -36,7 +36,8 @@</span> <span class="p_context"></span>
 	{(unsigned long)__GFP_RECLAIMABLE,	&quot;GFP_RECLAIMABLE&quot;},	\
 	{(unsigned long)__GFP_MOVABLE,		&quot;GFP_MOVABLE&quot;},		\
 	{(unsigned long)__GFP_NOTRACK,		&quot;GFP_NOTRACK&quot;},		\
<span class="p_del">-	{(unsigned long)__GFP_NO_KSWAPD,	&quot;GFP_NO_KSWAPD&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_DIRECT_RECLAIM,	&quot;GFP_DIRECT_RECLAIM&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_KSWAPD_RECLAIM,	&quot;GFP_KSWAPD_RECLAIM&quot;},	\</span>
 	{(unsigned long)__GFP_OTHER_NODE,	&quot;GFP_OTHER_NODE&quot;}	\
 	) : &quot;GFP_NOWAIT&quot;
 
<span class="p_header">diff --git a/kernel/audit.c b/kernel/audit.c</span>
<span class="p_header">index f9e6065346db..6ab7a55dbdff 100644</span>
<span class="p_header">--- a/kernel/audit.c</span>
<span class="p_header">+++ b/kernel/audit.c</span>
<span class="p_chunk">@@ -1357,16 +1357,16 @@</span> <span class="p_context"> struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,</span>
 	if (unlikely(audit_filter_type(type)))
 		return NULL;
 
<span class="p_del">-	if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_DIRECT_RECLAIM) {</span>
 		if (audit_pid &amp;&amp; audit_pid == current-&gt;pid)
<span class="p_del">-			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 		else
 			reserve = 0;
 	}
 
 	while (audit_backlog_limit
 	       &amp;&amp; skb_queue_len(&amp;audit_skb_queue) &gt; audit_backlog_limit + reserve) {
<span class="p_del">-		if (gfp_mask &amp; __GFP_WAIT &amp;&amp; audit_backlog_wait_time) {</span>
<span class="p_add">+		if (gfp_mask &amp; __GFP_DIRECT_RECLAIM &amp;&amp; audit_backlog_wait_time) {</span>
 			long sleep_time;
 
 			sleep_time = timeout_start + audit_backlog_wait_time - jiffies;
<span class="p_header">diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="p_header">index 8acfbf773e06..9aa39f20f593 100644</span>
<span class="p_header">--- a/kernel/locking/lockdep.c</span>
<span class="p_header">+++ b/kernel/locking/lockdep.c</span>
<span class="p_chunk">@@ -2738,7 +2738,7 @@</span> <span class="p_context"> static void __lockdep_trace_alloc(gfp_t gfp_mask, unsigned long flags)</span>
 		return;
 
 	/* no reclaim without waiting on it */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		return;
 
 	/* this guy won&#39;t enter reclaim */
<span class="p_header">diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c</span>
<span class="p_header">index 5235dd4e1e2f..3a970604308f 100644</span>
<span class="p_header">--- a/kernel/power/snapshot.c</span>
<span class="p_header">+++ b/kernel/power/snapshot.c</span>
<span class="p_chunk">@@ -1779,7 +1779,7 @@</span> <span class="p_context"> alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)</span>
 	while (to_alloc-- &gt; 0) {
 		struct page *page;
 
<span class="p_del">-		page = alloc_image_page(__GFP_HIGHMEM);</span>
<span class="p_add">+		page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);</span>
 		memory_bm_set_bit(bm, page_to_pfn(page));
 	}
 	return nr_highmem;
<span class="p_header">diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="p_header">index 07854477c164..d903c02223af 100644</span>
<span class="p_header">--- a/kernel/smp.c</span>
<span class="p_header">+++ b/kernel/smp.c</span>
<span class="p_chunk">@@ -669,7 +669,7 @@</span> <span class="p_context"> void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
 	cpumask_var_t cpus;
 	int cpu, ret;
 
<span class="p_del">-	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
 
 	if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {
 		preempt_disable();
<span class="p_header">diff --git a/lib/idr.c b/lib/idr.c</span>
<span class="p_header">index 5335c43adf46..6098336df267 100644</span>
<span class="p_header">--- a/lib/idr.c</span>
<span class="p_header">+++ b/lib/idr.c</span>
<span class="p_chunk">@@ -399,7 +399,7 @@</span> <span class="p_context"> void idr_preload(gfp_t gfp_mask)</span>
 	 * allocation guarantee.  Disallow usage from those contexts.
 	 */
 	WARN_ON_ONCE(in_interrupt());
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
 
 	preempt_disable();
 
<span class="p_chunk">@@ -453,7 +453,7 @@</span> <span class="p_context"> int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)</span>
 	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
 	int id;
 
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
 
 	/* sanity checks */
 	if (WARN_ON_ONCE(start &lt; 0))
<span class="p_header">diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="p_header">index f9ebe1c82060..c3775ee46cd6 100644</span>
<span class="p_header">--- a/lib/radix-tree.c</span>
<span class="p_header">+++ b/lib/radix-tree.c</span>
<span class="p_chunk">@@ -188,7 +188,7 @@</span> <span class="p_context"> radix_tree_node_alloc(struct radix_tree_root *root)</span>
 	 * preloading in the interrupt anyway as all the allocations have to
 	 * be atomic. So just do normal allocation when in interrupt.
 	 */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) &amp;&amp; !in_interrupt()) {</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) &amp;&amp; !in_interrupt()) {</span>
 		struct radix_tree_preload *rtp;
 
 		/*
<span class="p_chunk">@@ -249,7 +249,7 @@</span> <span class="p_context"> radix_tree_node_free(struct radix_tree_node *node)</span>
  * with preemption not disabled.
  *
  * To make use of this facility, the radix tree must be initialised without
<span class="p_del">- * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
  */
 static int __radix_tree_preload(gfp_t gfp_mask)
 {
<span class="p_chunk">@@ -286,12 +286,12 @@</span> <span class="p_context"> static int __radix_tree_preload(gfp_t gfp_mask)</span>
  * with preemption not disabled.
  *
  * To make use of this facility, the radix tree must be initialised without
<span class="p_del">- * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
  */
 int radix_tree_preload(gfp_t gfp_mask)
 {
 	/* Warn on non-sensical use... */
<span class="p_del">-	WARN_ON_ONCE(!(gfp_mask &amp; __GFP_WAIT));</span>
<span class="p_add">+	WARN_ON_ONCE(gfpflags_allow_blocking(gfp_mask));</span>
 	return __radix_tree_preload(gfp_mask);
 }
 EXPORT_SYMBOL(radix_tree_preload);
<span class="p_chunk">@@ -303,7 +303,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(radix_tree_preload);</span>
  */
 int radix_tree_maybe_preload(gfp_t gfp_mask)
 {
<span class="p_del">-	if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(gfp_mask))</span>
 		return __radix_tree_preload(gfp_mask);
 	/* Preloading doesn&#39;t help anything with this gfp mask, skip it */
 	preempt_disable();
<span class="p_header">diff --git a/mm/backing-dev.c b/mm/backing-dev.c</span>
<span class="p_header">index dac5bf59309d..805ce70b72f3 100644</span>
<span class="p_header">--- a/mm/backing-dev.c</span>
<span class="p_header">+++ b/mm/backing-dev.c</span>
<span class="p_chunk">@@ -632,7 +632,7 @@</span> <span class="p_context"> struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span>
 {
 	struct bdi_writeback *wb;
 
<span class="p_del">-	might_sleep_if(gfp &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp));</span>
 
 	if (!memcg_css-&gt;parent)
 		return &amp;bdi-&gt;wb;
<span class="p_header">diff --git a/mm/dmapool.c b/mm/dmapool.c</span>
<span class="p_header">index fd5fe4342e93..84dac666fc0c 100644</span>
<span class="p_header">--- a/mm/dmapool.c</span>
<span class="p_header">+++ b/mm/dmapool.c</span>
<span class="p_chunk">@@ -323,7 +323,7 @@</span> <span class="p_context"> void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,</span>
 	size_t offset;
 	void *retval;
 
<span class="p_del">-	might_sleep_if(mem_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(mem_flags));</span>
 
 	spin_lock_irqsave(&amp;pool-&gt;lock, flags);
 	list_for_each_entry(page, &amp;pool-&gt;page_list, page_list) {
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index acb93c554f6e..e34f6411da8c 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -2268,7 +2268,7 @@</span> <span class="p_context"> static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 	if (unlikely(task_in_memcg_oom(current)))
 		goto nomem;
 
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask))</span>
 		goto nomem;
 
 	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);
<span class="p_chunk">@@ -2327,7 +2327,7 @@</span> <span class="p_context"> static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 	css_get_many(&amp;memcg-&gt;css, batch);
 	if (batch &gt; nr_pages)
 		refill_stock(memcg, batch - nr_pages);
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask))</span>
 		goto done;
 	/*
 	 * If the hierarchy is above the normal consumption range,
<span class="p_chunk">@@ -4696,8 +4696,8 @@</span> <span class="p_context"> static int mem_cgroup_do_precharge(unsigned long count)</span>
 {
 	int ret;
 
<span class="p_del">-	/* Try a single bulk charge without reclaim first */</span>
<span class="p_del">-	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_WAIT, count);</span>
<span class="p_add">+	/* Try a single bulk charge without reclaim first, kswapd may wake */</span>
<span class="p_add">+	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM, count);</span>
 	if (!ret) {
 		mc.precharge += count;
 		return ret;
<span class="p_header">diff --git a/mm/mempool.c b/mm/mempool.c</span>
<span class="p_header">index 2cc08de8b1db..bfd2a0dd0e18 100644</span>
<span class="p_header">--- a/mm/mempool.c</span>
<span class="p_header">+++ b/mm/mempool.c</span>
<span class="p_chunk">@@ -317,13 +317,13 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 	gfp_t gfp_temp;
 
 	VM_WARN_ON_ONCE(gfp_mask &amp; __GFP_ZERO);
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
 
 	gfp_mask |= __GFP_NOMEMALLOC;	/* don&#39;t allocate emergency reserves */
 	gfp_mask |= __GFP_NORETRY;	/* don&#39;t loop in __alloc_pages */
 	gfp_mask |= __GFP_NOWARN;	/* failures are OK */
 
<span class="p_del">-	gfp_temp = gfp_mask &amp; ~(__GFP_WAIT|__GFP_IO);</span>
<span class="p_add">+	gfp_temp = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM|__GFP_IO);</span>
 
 repeat_alloc:
 
<span class="p_chunk">@@ -346,7 +346,7 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 	}
 
 	/*
<span class="p_del">-	 * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If</span>
<span class="p_add">+	 * We use gfp mask w/o direct reclaim or IO for the first round.  If</span>
 	 * alloc failed with that and @pool was empty, retry immediately.
 	 */
 	if (gfp_temp != gfp_mask) {
<span class="p_chunk">@@ -355,8 +355,8 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 		goto repeat_alloc;
 	}
 
<span class="p_del">-	/* We must not sleep if !__GFP_WAIT */</span>
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	/* We must not sleep if !__GFP_DIRECT_RECLAIM */</span>
<span class="p_add">+	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM)) {</span>
 		spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);
 		return NULL;
 	}
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index eb4267107d1f..0e16c4047638 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1564,7 +1564,7 @@</span> <span class="p_context"> static struct page *alloc_misplaced_dst_page(struct page *page,</span>
 					 (GFP_HIGHUSER_MOVABLE |
 					  __GFP_THISNODE | __GFP_NOMEMALLOC |
 					  __GFP_NORETRY | __GFP_NOWARN) &amp;
<span class="p_del">-					 ~GFP_IOFS, 0);</span>
<span class="p_add">+					 ~(__GFP_IO | __GFP_FS), 0);</span>
 
 	return newpage;
 }
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 32d1cec124bc..68f961bdfdf8 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -151,12 +151,12 @@</span> <span class="p_context"> void pm_restrict_gfp_mask(void)</span>
 	WARN_ON(!mutex_is_locked(&amp;pm_mutex));
 	WARN_ON(saved_gfp_mask);
 	saved_gfp_mask = gfp_allowed_mask;
<span class="p_del">-	gfp_allowed_mask &amp;= ~GFP_IOFS;</span>
<span class="p_add">+	gfp_allowed_mask &amp;= ~(__GFP_IO | __GFP_FS);</span>
 }
 
 bool pm_suspended_storage(void)
 {
<span class="p_del">-	if ((gfp_allowed_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="p_add">+	if ((gfp_allowed_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
 		return false;
 	return true;
 }
<span class="p_chunk">@@ -2158,7 +2158,7 @@</span> <span class="p_context"> static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
 		return false;
 	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))
 		return false;
<span class="p_del">-	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
 		return false;
 
 	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);
<span class="p_chunk">@@ -2660,7 +2660,7 @@</span> <span class="p_context"> void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
 		if (test_thread_flag(TIF_MEMDIE) ||
 		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))
 			filter &amp;= ~SHOW_MEM_FILTER_NODES;
<span class="p_del">-	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT) || (gfp_mask &amp; __GFP_ATOMIC))</span>
 		filter &amp;= ~SHOW_MEM_FILTER_NODES;
 
 	if (fmt) {
<span class="p_chunk">@@ -2915,7 +2915,6 @@</span> <span class="p_context"> static inline int</span>
 gfp_to_alloc_flags(gfp_t gfp_mask)
 {
 	int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;
<span class="p_del">-	const bool atomic = !(gfp_mask &amp; (__GFP_WAIT | __GFP_NO_KSWAPD));</span>
 
 	/* __GFP_HIGH is assumed to be the same as ALLOC_HIGH to save a branch. */
 	BUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);
<span class="p_chunk">@@ -2924,11 +2923,11 @@</span> <span class="p_context"> gfp_to_alloc_flags(gfp_t gfp_mask)</span>
 	 * The caller may dip into page reserves a bit more if the caller
 	 * cannot run direct reclaim, or if the caller has realtime scheduling
 	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will
<span class="p_del">-	 * set both ALLOC_HARDER (atomic == true) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="p_add">+	 * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).</span>
 	 */
 	alloc_flags |= (__force int) (gfp_mask &amp; __GFP_HIGH);
 
<span class="p_del">-	if (atomic) {</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_ATOMIC) {</span>
 		/*
 		 * Not worth trying to allocate harder for __GFP_NOMEMALLOC even
 		 * if it can&#39;t schedule.
<span class="p_chunk">@@ -2965,11 +2964,16 @@</span> <span class="p_context"> bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
 	return !!(gfp_to_alloc_flags(gfp_mask) &amp; ALLOC_NO_WATERMARKS);
 }
 
<span class="p_add">+static inline bool is_thp_gfp_mask(gfp_t gfp_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (gfp_mask &amp; (GFP_TRANSHUGE | __GFP_KSWAPD_RECLAIM)) == GFP_TRANSHUGE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *
 __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 						struct alloc_context *ac)
 {
<span class="p_del">-	const gfp_t wait = gfp_mask &amp; __GFP_WAIT;</span>
<span class="p_add">+	bool can_direct_reclaim = gfp_mask &amp; __GFP_DIRECT_RECLAIM;</span>
 	struct page *page = NULL;
 	int alloc_flags;
 	unsigned long pages_reclaimed = 0;
<span class="p_chunk">@@ -2990,15 +2994,23 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	}
 
 	/*
<span class="p_add">+	 * We also sanity check to catch abuse of atomic reserves being used by</span>
<span class="p_add">+	 * callers that are not in atomic context.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (WARN_ON_ONCE((gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==</span>
<span class="p_add">+				(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="p_add">+		gfp_mask &amp;= ~__GFP_ATOMIC;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * If this allocation cannot block and it is for a specific node, then
 	 * fail early.  There&#39;s no need to wakeup kswapd or retry for a
 	 * speculative node-specific allocation.
 	 */
<span class="p_del">-	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !wait)</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !can_direct_reclaim)</span>
 		goto nopage;
 
 retry:
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_NO_KSWAPD))</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_KSWAPD_RECLAIM)</span>
 		wake_all_kswapds(order, ac);
 
 	/*
<span class="p_chunk">@@ -3041,8 +3053,8 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 		}
 	}
 
<span class="p_del">-	/* Atomic allocations - we can&#39;t balance anything */</span>
<span class="p_del">-	if (!wait) {</span>
<span class="p_add">+	/* Caller is not willing to reclaim, we can&#39;t balance anything */</span>
<span class="p_add">+	if (!can_direct_reclaim) {</span>
 		/*
 		 * All existing users of the deprecated __GFP_NOFAIL are
 		 * blockable, so warn of any new users that actually allow this
<span class="p_chunk">@@ -3072,7 +3084,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 		goto got_pg;
 
 	/* Checks for THP-specific high-order allocations */
<span class="p_del">-	if ((gfp_mask &amp; GFP_TRANSHUGE) == GFP_TRANSHUGE) {</span>
<span class="p_add">+	if (is_thp_gfp_mask(gfp_mask)) {</span>
 		/*
 		 * If compaction is deferred for high-order allocations, it is
 		 * because sync compaction recently failed. If this is the case
<span class="p_chunk">@@ -3107,8 +3119,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	 * fault, so use asynchronous memory compaction for THP unless it is
 	 * khugepaged trying to collapse.
 	 */
<span class="p_del">-	if ((gfp_mask &amp; GFP_TRANSHUGE) != GFP_TRANSHUGE ||</span>
<span class="p_del">-						(current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="p_add">+	if (!is_thp_gfp_mask(gfp_mask) || (current-&gt;flags &amp; PF_KTHREAD))</span>
 		migration_mode = MIGRATE_SYNC_LIGHT;
 
 	/* Try direct reclaim and then allocating */
<span class="p_chunk">@@ -3179,7 +3190,7 @@</span> <span class="p_context"> __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,</span>
 
 	lockdep_trace_alloc(gfp_mask);
 
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
 
 	if (should_fail_alloc_page(gfp_mask, order))
 		return NULL;
<span class="p_header">diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="p_header">index 200e22412a16..f82bdb3eb1fc 100644</span>
<span class="p_header">--- a/mm/slab.c</span>
<span class="p_header">+++ b/mm/slab.c</span>
<span class="p_chunk">@@ -1030,12 +1030,12 @@</span> <span class="p_context"> static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)</span>
 }
 
 /*
<span class="p_del">- * Construct gfp mask to allocate from a specific node but do not invoke reclaim</span>
<span class="p_del">- * or warn about failures.</span>
<span class="p_add">+ * Construct gfp mask to allocate from a specific node but do not direct reclaim</span>
<span class="p_add">+ * or warn about failures. kswapd may still wake to reclaim in the background.</span>
  */
 static inline gfp_t gfp_exact_node(gfp_t flags)
 {
<span class="p_del">-	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_WAIT;</span>
<span class="p_add">+	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_DIRECT_RECLAIM;</span>
 }
 #endif
 
<span class="p_chunk">@@ -2625,7 +2625,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 
 	offset *= cachep-&gt;colour_off;
 
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_enable();
 
 	/*
<span class="p_chunk">@@ -2655,7 +2655,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 
 	cache_init_objs(cachep, page);
 
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_disable();
 	check_irq_off();
 	spin_lock(&amp;n-&gt;list_lock);
<span class="p_chunk">@@ -2669,7 +2669,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 opps1:
 	kmem_freepages(cachep, page);
 failed:
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_disable();
 	return 0;
 }
<span class="p_chunk">@@ -2861,7 +2861,7 @@</span> <span class="p_context"> static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,</span>
 static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,
 						gfp_t flags)
 {
<span class="p_del">-	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(flags));</span>
 #if DEBUG
 	kmem_flagcheck(cachep, flags);
 #endif
<span class="p_chunk">@@ -3049,11 +3049,11 @@</span> <span class="p_context"> static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)</span>
 		 */
 		struct page *page;
 
<span class="p_del">-		if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(local_flags))</span>
 			local_irq_enable();
 		kmem_flagcheck(cache, flags);
 		page = kmem_getpages(cache, local_flags, numa_mem_id());
<span class="p_del">-		if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(local_flags))</span>
 			local_irq_disable();
 		if (page) {
 			/*
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index 816df0016555..a4661c59ff54 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -1263,7 +1263,7 @@</span> <span class="p_context"> static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,</span>
 {
 	flags &amp;= gfp_allowed_mask;
 	lockdep_trace_alloc(flags);
<span class="p_del">-	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(flags));</span>
 
 	if (should_failslab(s-&gt;object_size, flags, s-&gt;flags))
 		return NULL;
<span class="p_chunk">@@ -1339,7 +1339,7 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 
 	flags &amp;= gfp_allowed_mask;
 
<span class="p_del">-	if (flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags))</span>
 		local_irq_enable();
 
 	flags |= s-&gt;allocflags;
<span class="p_chunk">@@ -1380,7 +1380,7 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 			kmemcheck_mark_unallocated_pages(page, pages);
 	}
 
<span class="p_del">-	if (flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags))</span>
 		local_irq_disable();
 	if (!page)
 		return NULL;
<span class="p_header">diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="p_header">index 2faaa2976447..9ad4dcb0631c 100644</span>
<span class="p_header">--- a/mm/vmalloc.c</span>
<span class="p_header">+++ b/mm/vmalloc.c</span>
<span class="p_chunk">@@ -1617,7 +1617,7 @@</span> <span class="p_context"> static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,</span>
 			goto fail;
 		}
 		area-&gt;pages[i] = page;
<span class="p_del">-		if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(gfp_mask))</span>
 			cond_resched();
 	}
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index e950134c4b9a..837c440d60a9 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1465,7 +1465,7 @@</span> <span class="p_context"> static int too_many_isolated(struct zone *zone, int file,</span>
 	 * won&#39;t get blocked by normal direct-reclaimers, forming a circular
 	 * deadlock.
 	 */
<span class="p_del">-	if ((sc-&gt;gfp_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="p_add">+	if ((sc-&gt;gfp_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
 		inactive &gt;&gt;= 3;
 
 	return isolated &gt; inactive;
<span class="p_chunk">@@ -3764,7 +3764,7 @@</span> <span class="p_context"> int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)</span>
 	/*
 	 * Do not scan if the allocation should not be delayed.
 	 */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
 		return ZONE_RECLAIM_NOSCAN;
 
 	/*
<span class="p_header">diff --git a/mm/zswap.c b/mm/zswap.c</span>
<span class="p_header">index 2d5727baed59..26104a68c972 100644</span>
<span class="p_header">--- a/mm/zswap.c</span>
<span class="p_header">+++ b/mm/zswap.c</span>
<span class="p_chunk">@@ -684,7 +684,8 @@</span> <span class="p_context"> static int zswap_frontswap_store(unsigned type, pgoff_t offset,</span>
 
 	/* store */
 	len = dlen + sizeof(struct zswap_header);
<span class="p_del">-	ret = zpool_malloc(zswap_pool, len, __GFP_NORETRY | __GFP_NOWARN,</span>
<span class="p_add">+	ret = zpool_malloc(zswap_pool, len,</span>
<span class="p_add">+		__GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM,</span>
 		&amp;handle);
 	if (ret == -ENOSPC) {
 		zswap_reject_compress_poor++;
<span class="p_chunk">@@ -900,7 +901,7 @@</span> <span class="p_context"> static void __exit zswap_debugfs_exit(void) { }</span>
 **********************************/
 static int __init init_zswap(void)
 {
<span class="p_del">-	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="p_add">+	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;</span>
 
 	pr_info(&quot;loading zswap\n&quot;);
 
<span class="p_header">diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="p_header">index b6a19ca0f99e..6f025e2544de 100644</span>
<span class="p_header">--- a/net/core/skbuff.c</span>
<span class="p_header">+++ b/net/core/skbuff.c</span>
<span class="p_chunk">@@ -414,7 +414,7 @@</span> <span class="p_context"> struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,</span>
 	len += NET_SKB_PAD;
 
 	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||
<span class="p_del">-	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="p_add">+	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
 		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
 		if (!skb)
 			goto skb_fail;
<span class="p_chunk">@@ -481,7 +481,7 @@</span> <span class="p_context"> struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,</span>
 	len += NET_SKB_PAD + NET_IP_ALIGN;
 
 	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||
<span class="p_del">-	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="p_add">+	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
 		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
 		if (!skb)
 			goto skb_fail;
<span class="p_chunk">@@ -4452,7 +4452,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 		return NULL;
 
 	gfp_head = gfp_mask;
<span class="p_del">-	if (gfp_head &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
 		gfp_head |= __GFP_REPEAT;
 
 	*errcode = -ENOBUFS;
<span class="p_chunk">@@ -4467,7 +4467,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 
 		while (order) {
 			if (npages &gt;= 1 &lt;&lt; order) {
<span class="p_del">-				page = alloc_pages((gfp_mask &amp; ~__GFP_WAIT) |</span>
<span class="p_add">+				page = alloc_pages((gfp_mask &amp; ~__GFP_DIRECT_RECLAIM) |</span>
 						   __GFP_COMP |
 						   __GFP_NOWARN |
 						   __GFP_NORETRY,
<span class="p_header">diff --git a/net/core/sock.c b/net/core/sock.c</span>
<span class="p_header">index 193901d09757..02b705cc9eb3 100644</span>
<span class="p_header">--- a/net/core/sock.c</span>
<span class="p_header">+++ b/net/core/sock.c</span>
<span class="p_chunk">@@ -1879,8 +1879,10 @@</span> <span class="p_context"> bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)</span>
 
 	pfrag-&gt;offset = 0;
 	if (SKB_FRAG_PAGE_ORDER) {
<span class="p_del">-		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_WAIT) | __GFP_COMP |</span>
<span class="p_del">-					  __GFP_NOWARN | __GFP_NORETRY,</span>
<span class="p_add">+		/* Avoid direct reclaim but allow kswapd to wake */</span>
<span class="p_add">+		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="p_add">+					  __GFP_COMP | __GFP_NOWARN |</span>
<span class="p_add">+					  __GFP_NORETRY,</span>
 					  SKB_FRAG_PAGE_ORDER);
 		if (likely(pfrag-&gt;page)) {
 			pfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;
<span class="p_header">diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="p_header">index 67d210477863..8283d90dde74 100644</span>
<span class="p_header">--- a/net/netlink/af_netlink.c</span>
<span class="p_header">+++ b/net/netlink/af_netlink.c</span>
<span class="p_chunk">@@ -2066,7 +2066,7 @@</span> <span class="p_context"> int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid</span>
 	consume_skb(info.skb2);
 
 	if (info.delivered) {
<span class="p_del">-		if (info.congested &amp;&amp; (allocation &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (info.congested &amp;&amp; gfpflags_allow_blocking(allocation))</span>
 			yield();
 		return 0;
 	}
<span class="p_header">diff --git a/net/rxrpc/ar-connection.c b/net/rxrpc/ar-connection.c</span>
<span class="p_header">index 6631f4f1e39b..3b5de4b86058 100644</span>
<span class="p_header">--- a/net/rxrpc/ar-connection.c</span>
<span class="p_header">+++ b/net/rxrpc/ar-connection.c</span>
<span class="p_chunk">@@ -500,7 +500,7 @@</span> <span class="p_context"> int rxrpc_connect_call(struct rxrpc_sock *rx,</span>
 		if (bundle-&gt;num_conns &gt;= 20) {
 			_debug(&quot;too many conns&quot;);
 
<span class="p_del">-			if (!(gfp &amp; __GFP_WAIT)) {</span>
<span class="p_add">+			if (!gfpflags_allow_blocking(gfp)) {</span>
 				_leave(&quot; = -EAGAIN&quot;);
 				return -EAGAIN;
 			}
<span class="p_header">diff --git a/net/sctp/associola.c b/net/sctp/associola.c</span>
<span class="p_header">index 197c3f59ecbf..75369ae8de1e 100644</span>
<span class="p_header">--- a/net/sctp/associola.c</span>
<span class="p_header">+++ b/net/sctp/associola.c</span>
<span class="p_chunk">@@ -1588,7 +1588,7 @@</span> <span class="p_context"> int sctp_assoc_lookup_laddr(struct sctp_association *asoc,</span>
 /* Set an association id for a given association */
 int sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)
 {
<span class="p_del">-	bool preload = !!(gfp &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp);</span>
 	int ret;
 
 	/* If the id is already assigned, keep it. */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



