
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v7,05/10] mm: thp: enable thp migration in generic path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v7,05/10] mm: thp: enable thp migration in generic path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 20, 2017, 11:07 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170620230715.81590-6-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9800413/mbox/"
   >mbox</a>
|
   <a href="/patch/9800413/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9800413/">/patch/9800413/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	17D31600C5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Jun 2017 23:17:57 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F2D1D1FF40
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Jun 2017 23:17:56 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E7735283B4; Tue, 20 Jun 2017 23:17:56 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F38121FF40
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Jun 2017 23:17:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753066AbdFTXRk (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 20 Jun 2017 19:17:40 -0400
Received: from out3-smtp.messagingengine.com ([66.111.4.27]:40475 &quot;EHLO
	out3-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752425AbdFTXP5 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 20 Jun 2017 19:15:57 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by mailout.nyi.internal (Postfix) with ESMTP id BF483209FD;
	Tue, 20 Jun 2017 19:07:49 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute3.internal (MEProxy); Tue, 20 Jun 2017 19:07:49 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-me-sender:x-me-sender:x-sasl-enc:x-sasl-enc; s=fm1; bh=/mxTjw
	dHy13han1fsNdkqwJINpgWsfEnkXIcd7OYKr4=; b=SDUqd1Om6ztnWSFN0nynqN
	R5ng18zJsnOeBTi5rFdiUW4wzXXWt/18OCHPr+kegvyeshgl0h5Fmj4g5Bqu8L+b
	kryyXt1IZiOVLXIWXJQHP83JLInJD3OyZKlsz7VCAixPovqwBnqG0hFNpiEN2R31
	inx3Ynq9A06Kq+Eh5Qi7GkFTwozMQ01CbT9TqPPqUMsENUcxzdzpgKOdiSAfc+YV
	GN/tNOxVtQLDThRede8gFusXEoT6fY1ArLOdfmRTnFElQ7a9HMyGWB2MVyS1U4BJ
	L/bO7dxTGa+vQr9+avrguYiuSCRbW59mCuPsv+mlmOs2PiJ3CJ95FjcWhO4/ME7A
	==
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-me-sender:x-me-sender:x-sasl-enc
	:x-sasl-enc; s=fm1; bh=/mxTjwdHy13han1fsNdkqwJINpgWsfEnkXIcd7OYK
	r4=; b=gmwPYwFEV4TOUt07ThwX+tPQ2l+2bRnzNhoh07YdHWoGFvcPcvAlGCkuI
	JnMG1YrB1dnWMZuqOwcObJu5saxOiwI/1L+vY28GySHJQBDrNcZoU/vDvdOHzOLX
	+r28j2PyktTrTTmQjvlZrey/W1vBTLaq/HCbNuH3qVGXkBctf/8KIUaw9YGSZIo9
	GCEvMiB8iI6rU3kOuljYJmmEIg4UQghKRCRra/UbGjjGNOECZA6GbMC4JmoxCRI0
	RoD0YGoimpYIbm72Cu8QUN9CKdwgoPlUOWXwKguZjoODSyoA7W+K7FC4qjt3BwZo
	l8mMxZh5N+qT1QdfYaCaZ636/n6GQ==
X-ME-Sender: &lt;xms:xapJWYo7_qtm30tGiFGZqUMoCzKAezTz_gOmI2a1U5JWTtwdfCJDUQ&gt;
X-Sasl-enc: 5RmTMg2tq5/OhNcxgyu0KD5BR9vNKYFeMBXTNHRnGPlT 1498000069
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id 4127C7E6F1;
	Tue, 20 Jun 2017 19:07:49 -0400 (EDT)
From: Zi Yan &lt;zi.yan@sent.com&gt;
To: kirill.shutemov@linux.intel.com, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org
Cc: akpm@linux-foundation.org, minchan@kernel.org, vbabka@suse.cz,
	mgorman@techsingularity.net, mhocko@kernel.org,
	khandual@linux.vnet.ibm.com, zi.yan@cs.rutgers.edu,
	dnellans@nvidia.com, dave.hansen@intel.com, n-horiguchi@ah.jp.nec.com
Subject: [PATCH v7 05/10] mm: thp: enable thp migration in generic path
Date: Tue, 20 Jun 2017 19:07:10 -0400
Message-Id: &lt;20170620230715.81590-6-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170620230715.81590-1-zi.yan@sent.com&gt;
References: &lt;20170620230715.81590-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - June 20, 2017, 11:07 p.m.</div>
<pre class="content">
<span class="from">From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>

This patch adds thp migration&#39;s core code, including conversions
between a PMD entry and a swap entry, setting PMD migration entry,
removing PMD migration entry, and waiting on PMD migration entries.

This patch makes it possible to support thp migration.
If you fail to allocate a destination page as a thp, you just split
the source thp as we do now, and then enter the normal page migration.
If you succeed to allocate destination thp, you enter thp migration.
Subsequent patches actually enable thp migration for each caller of
page migration by allowing its get_new_page() callback to
allocate thps.

ChangeLog v1 -&gt; v2:
- support pte-mapped thp, doubly-mapped thp
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

ChangeLog v2 -&gt; v3:
- use page_vma_mapped_walk()
- use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in
  set_pmd_migration_entry()

ChangeLog v3 -&gt; v4:
- factor out the code of removing pte pgtable page in zap_huge_pmd()

ChangeLog v4 -&gt; v5:
- remove unnecessary PTE-mapped THP code in remove_migration_pmd()
  and set_pmd_migration_entry()
- restructure the code in zap_huge_pmd() to avoid factoring out
  the pte pgtable page code
- in zap_huge_pmd(), check that PMD swap entries are migration entries
- change author information

ChangeLog v5 -&gt; v7
- use macro to disable the code when thp migration is not enabled
<span class="signed-off-by">
Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;
---
 arch/x86/include/asm/pgtable_64.h |  2 +
 include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++-
 mm/huge_memory.c                  | 88 ++++++++++++++++++++++++++++++++++++---
 mm/migrate.c                      | 32 +++++++++++++-
 mm/page_vma_mapped.c              | 17 ++++++--
 mm/pgtable-generic.c              |  3 +-
 mm/rmap.c                         | 13 ++++++
 7 files changed, 212 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - June 21, 2017, 11:23 a.m.</div>
<pre class="content">
On Tue, Jun 20, 2017 at 07:07:10PM -0400, Zi Yan wrote:
<span class="quote">&gt; From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; allocate thps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in</span>
<span class="quote">&gt;   set_pmd_migration_entry()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v4 -&gt; v5:</span>
<span class="quote">&gt; - remove unnecessary PTE-mapped THP code in remove_migration_pmd()</span>
<span class="quote">&gt;   and set_pmd_migration_entry()</span>
<span class="quote">&gt; - restructure the code in zap_huge_pmd() to avoid factoring out</span>
<span class="quote">&gt;   the pte pgtable page code</span>
<span class="quote">&gt; - in zap_huge_pmd(), check that PMD swap entries are migration entries</span>
<span class="quote">&gt; - change author information</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v5 -&gt; v7</span>
<span class="quote">&gt; - use macro to disable the code when thp migration is not enabled</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h |  2 +</span>
<span class="quote">&gt;  include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  mm/huge_memory.c                  | 88 ++++++++++++++++++++++++++++++++++++---</span>
<span class="quote">&gt;  mm/migrate.c                      | 32 +++++++++++++-</span>
<span class="quote">&gt;  mm/page_vma_mapped.c              | 17 ++++++--</span>
<span class="quote">&gt;  mm/pgtable-generic.c              |  3 +-</span>
<span class="quote">&gt;  mm/rmap.c                         | 13 ++++++</span>
<span class="quote">&gt;  7 files changed, 212 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index 45b7a4094de0..eac7f8cf4ae0 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -208,7 +208,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index c5ff7b217ee6..ae0c5fc18788 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt;  	return p;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -163,6 +164,70 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 421631ff3aeb..d9405ba628f6 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1641,10 +1641,27 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +		struct page *page = NULL;</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (pmd_present(orig_pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>

Can we have IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) instead here and below?
<span class="quote">
&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +			migration = 1;</span>

I guess something like &#39;flush_needed&#39; instead would be more descriptive.
<span class="quote">
&gt; +#else</span>
<span class="quote">&gt; +			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		if (PageAnon(page)) {</span>
<span class="quote">&gt;  			zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;  			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; @@ -1653,8 +1670,10 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;  			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		if (!migration)</span>
<span class="quote">&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2694,3 +2713,62 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt; +			address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>

We don&#39;t hold mmap_sem for write here, right?

I *think* it means we can race with MADV_DONTNEED the same way as
described in ced108037c2a.

I guess pmdp_invalidate() approach is required.
<span class="quote">
&gt; +	if (pmd_dirty(pmdval))</span>
<span class="quote">&gt; +		set_page_dirty(page);</span>
<span class="quote">&gt; +	entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="quote">&gt; +	pmdval = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +	set_pmd_at(mm, address, pvmw-&gt;pmd, pmdval);</span>
<span class="quote">&gt; +	page_remove_rmap(page, true);</span>
<span class="quote">&gt; +	put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="quote">&gt; +			address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; +	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; +	pmd_t pmde;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +	get_page(new);</span>
<span class="quote">&gt; +	pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +	if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +		pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="quote">&gt; +	set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="quote">&gt; +	flush_tlb_range(vma, mmun_start, mmun_end);</span>

Why do we need flush here? We replace non-present pmd with a present one.

And we are under ptl, but flush IIRC can sleep.
<span class="quote">
&gt; +	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; +		mlock_vma_page(new);</span>
<span class="quote">&gt; +	update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 627671551873..cae5c3b3b491 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -215,6 +215,15 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;  				linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt; +		if (!pvmw.pte &amp;&amp; pvmw.page) {</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);</span>
<span class="quote">&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		get_page(new);</span>
<span class="quote">&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>
<span class="quote">&gt; @@ -329,6 +338,27 @@ void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	__migration_entry_wait(mm, pte, ptl);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (!is_pmd_migration_entry(*pmd))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="quote">&gt; +	if (!get_page_unless_zero(page))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	wait_on_page_locked(page);</span>
<span class="quote">&gt; +	put_page(page);</span>
<span class="quote">&gt; +	return;</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_BLOCK</span>
<span class="quote">&gt;  /* Returns true if all buffers are successfully locked */</span>
<span class="quote">&gt;  static bool buffer_migrate_lock_buffers(struct buffer_head *head,</span>
<span class="quote">&gt; @@ -1087,7 +1117,7 @@ static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (unlikely(PageTransHuge(page))) {</span>
<span class="quote">&gt; +	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
<span class="quote">&gt;  		lock_page(page);</span>
<span class="quote">&gt;  		rc = split_huge_page(page);</span>
<span class="quote">&gt;  		unlock_page(page);</span>
<span class="quote">&gt; diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; index 8ec6ba230bb9..ff5517e67788 100644</span>
<span class="quote">&gt; --- a/mm/page_vma_mapped.c</span>
<span class="quote">&gt; +++ b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; @@ -138,16 +138,27 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
<span class="quote">&gt;  	if (!pud_present(*pud))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;  		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);</span>
<span class="quote">&gt; -		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="quote">&gt; -			return not_found(pvmw);</span>
<span class="quote">&gt;  		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {</span>
<span class="quote">&gt;  			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			if (pmd_page(*pvmw-&gt;pmd) != page)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			return true;</span>
<span class="quote">&gt; +		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>

Shouldn&#39;t we check PVMW_MIGRATION here?
<span class="quote">
&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="quote">&gt; +				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				if (migration_entry_to_page(entry) != page)</span>
<span class="quote">&gt; +					return not_found(pvmw);</span>
<span class="quote">&gt; +				return true;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +			return not_found(pvmw);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			/* THP pmd was split under us: handle on pte level */</span>
<span class="quote">&gt;  			spin_unlock(pvmw-&gt;ptl);</span>
<span class="quote">&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt; index c99d9512a45b..1175f6a24fdb 100644</span>
<span class="quote">&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt; @@ -124,7 +124,8 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="quote">&gt; +	VM_BUG_ON((pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="quote">&gt; +			   !pmd_devmap(*pmdp)) || !pmd_present(*pmdp));</span>
<span class="quote">&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;  	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 91948fbbb0bb..b28f633cd569 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1302,6 +1302,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	bool ret = true;</span>
<span class="quote">&gt;  	enum ttu_flags flags = (enum ttu_flags)arg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt;  		return true;</span>
<span class="quote">&gt; @@ -1312,6 +1313,18 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (page_vma_mapped_walk(&amp;pvmw)) {</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt; +		if (flags &amp; TTU_MIGRATION) {</span>
<span class="quote">&gt; +			if (!pvmw.pte &amp;&amp; page) {</span>
<span class="quote">&gt; +				VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page),</span>
<span class="quote">&gt; +						page);</span>
<span class="quote">&gt; +				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If the page is mlock()d, we cannot swap it out.</span>
<span class="quote">&gt;  		 * If it&#39;s recently referenced (perhaps page_referenced</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - June 21, 2017, 2:37 p.m.</div>
<pre class="content">
On 21 Jun 2017, at 7:23, Kirill A. Shutemov wrote:
<span class="quote">
&gt; On Tue, Jun 20, 2017 at 07:07:10PM -0400, Zi Yan wrote:</span>
<span class="quote">&gt;&gt; From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt;&gt; allocate thps.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt;&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in</span>
<span class="quote">&gt;&gt;   set_pmd_migration_entry()</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt;&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v4 -&gt; v5:</span>
<span class="quote">&gt;&gt; - remove unnecessary PTE-mapped THP code in remove_migration_pmd()</span>
<span class="quote">&gt;&gt;   and set_pmd_migration_entry()</span>
<span class="quote">&gt;&gt; - restructure the code in zap_huge_pmd() to avoid factoring out</span>
<span class="quote">&gt;&gt;   the pte pgtable page code</span>
<span class="quote">&gt;&gt; - in zap_huge_pmd(), check that PMD swap entries are migration entries</span>
<span class="quote">&gt;&gt; - change author information</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v5 -&gt; v7</span>
<span class="quote">&gt;&gt; - use macro to disable the code when thp migration is not enabled</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt; Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/pgtable_64.h |  2 +</span>
<span class="quote">&gt;&gt;  include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;&gt;  mm/huge_memory.c                  | 88 ++++++++++++++++++++++++++++++++++++---</span>
<span class="quote">&gt;&gt;  mm/migrate.c                      | 32 +++++++++++++-</span>
<span class="quote">&gt;&gt;  mm/page_vma_mapped.c              | 17 ++++++--</span>
<span class="quote">&gt;&gt;  mm/pgtable-generic.c              |  3 +-</span>
<span class="quote">&gt;&gt;  mm/rmap.c                         | 13 ++++++</span>
<span class="quote">&gt;&gt;  7 files changed, 212 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; index 45b7a4094de0..eac7f8cf4ae0 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt; @@ -208,7 +208,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt;&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt;&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt;&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt;&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt;&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt;&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; index c5ff7b217ee6..ae0c5fc18788 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt;&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt;&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt;&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt;&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt;&gt;  	return p;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -163,6 +164,70 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *new);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt;&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt;&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *new)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; index 421631ff3aeb..d9405ba628f6 100644</span>
<span class="quote">&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; @@ -1641,10 +1641,27 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;  	} else {</span>
<span class="quote">&gt;&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt; +		struct page *page = NULL;</span>
<span class="quote">&gt;&gt; +		int migration = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (pmd_present(orig_pmd)) {</span>
<span class="quote">&gt;&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt; +		} else {</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can we have IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) instead here and below?</span>
<span class="quote">&gt;</span>

No. Both chunks have pmd_to_swp_entry(), which triggers BUILD_BUG()
when CONFIG_ARCH_ENABLE_THP_MIGRATION is not set. So we need this macro
to disable the code when THP migration is not enabled.
<span class="quote">
&gt;&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="quote">&gt;&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt;&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt;&gt; +			migration = 1;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I guess something like &#39;flush_needed&#39; instead would be more descriptive.</span>

Will change the name.
<span class="quote">
&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		if (PageAnon(page)) {</span>
<span class="quote">&gt;&gt;  			zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt;  			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt; @@ -1653,8 +1670,10 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt;&gt;  			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt; +		if (!migration)</span>
<span class="quote">&gt;&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  	return 1;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -2694,3 +2713,62 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt; +		struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt;&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt;&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt;&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="quote">&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt;&gt; +			address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt; +	pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We don&#39;t hold mmap_sem for write here, right?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I *think* it means we can race with MADV_DONTNEED the same way as</span>
<span class="quote">&gt; described in ced108037c2a.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I guess pmdp_invalidate() approach is required.</span>

Yes. You are right. I will use pmdp_invalidate() instead.
<span class="quote">

&gt;&gt; +	if (pmd_dirty(pmdval))</span>
<span class="quote">&gt;&gt; +		set_page_dirty(page);</span>
<span class="quote">&gt;&gt; +	entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="quote">&gt;&gt; +	pmdval = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt;&gt; +	set_pmd_at(mm, address, pvmw-&gt;pmd, pmdval);</span>
<span class="quote">&gt;&gt; +	page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt; +	put_page(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="quote">&gt;&gt; +			address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt;&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt;&gt; +	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;&gt; +	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt;&gt; +	pmd_t pmde;</span>
<span class="quote">&gt;&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="quote">&gt;&gt; +		return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt;&gt; +	get_page(new);</span>
<span class="quote">&gt;&gt; +	pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt;&gt; +	if (is_write_migration_entry(entry))</span>
<span class="quote">&gt;&gt; +		pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt; +	page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="quote">&gt;&gt; +	set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="quote">&gt;&gt; +	flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why do we need flush here? We replace non-present pmd with a present one.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And we are under ptl, but flush IIRC can sleep.</span>

Right. flush is not needed here. Thanks.
<span class="quote">
&gt;&gt; +	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt;&gt; +		mlock_vma_page(new);</span>
<span class="quote">&gt;&gt; +	update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt;&gt; index 627671551873..cae5c3b3b491 100644</span>
<span class="quote">&gt;&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt;&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt;&gt; @@ -215,6 +215,15 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  			new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;&gt;  				linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt;&gt; +		if (!pvmw.pte &amp;&amp; pvmw.page) {</span>
<span class="quote">&gt;&gt; +			VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);</span>
<span class="quote">&gt;&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		get_page(new);</span>
<span class="quote">&gt;&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>
<span class="quote">&gt;&gt; @@ -329,6 +338,27 @@ void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  	__migration_entry_wait(mm, pte, ptl);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt;&gt; +	struct page *page;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;&gt; +	if (!is_pmd_migration_entry(*pmd))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="quote">&gt;&gt; +	if (!get_page_unless_zero(page))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt;&gt; +	wait_on_page_locked(page);</span>
<span class="quote">&gt;&gt; +	put_page(page);</span>
<span class="quote">&gt;&gt; +	return;</span>
<span class="quote">&gt;&gt; +unlock:</span>
<span class="quote">&gt;&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  #ifdef CONFIG_BLOCK</span>
<span class="quote">&gt;&gt;  /* Returns true if all buffers are successfully locked */</span>
<span class="quote">&gt;&gt;  static bool buffer_migrate_lock_buffers(struct buffer_head *head,</span>
<span class="quote">&gt;&gt; @@ -1087,7 +1117,7 @@ static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
<span class="quote">&gt;&gt;  		goto out;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -	if (unlikely(PageTransHuge(page))) {</span>
<span class="quote">&gt;&gt; +	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
<span class="quote">&gt;&gt;  		lock_page(page);</span>
<span class="quote">&gt;&gt;  		rc = split_huge_page(page);</span>
<span class="quote">&gt;&gt;  		unlock_page(page);</span>
<span class="quote">&gt;&gt; diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="quote">&gt;&gt; index 8ec6ba230bb9..ff5517e67788 100644</span>
<span class="quote">&gt;&gt; --- a/mm/page_vma_mapped.c</span>
<span class="quote">&gt;&gt; +++ b/mm/page_vma_mapped.c</span>
<span class="quote">&gt;&gt; @@ -138,16 +138,27 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
<span class="quote">&gt;&gt;  	if (!pud_present(*pud))</span>
<span class="quote">&gt;&gt;  		return false;</span>
<span class="quote">&gt;&gt;  	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);</span>
<span class="quote">&gt;&gt; -	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;&gt; +	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;&gt;  		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);</span>
<span class="quote">&gt;&gt; -		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="quote">&gt;&gt; -			return not_found(pvmw);</span>
<span class="quote">&gt;&gt;  		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {</span>
<span class="quote">&gt;&gt;  			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)</span>
<span class="quote">&gt;&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;&gt;  			if (pmd_page(*pvmw-&gt;pmd) != page)</span>
<span class="quote">&gt;&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;&gt;  			return true;</span>
<span class="quote">&gt;&gt; +		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Shouldn&#39;t we check PVMW_MIGRATION here?</span>

Right. I will add:

if (!(pvmw-&gt;flags &amp; PVMW_MIGRATION))
    return not_found(pvmw);
<span class="quote">
&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt; +			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="quote">&gt;&gt; +				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +				if (migration_entry_to_page(entry) != page)</span>
<span class="quote">&gt;&gt; +					return not_found(pvmw);</span>
<span class="quote">&gt;&gt; +				return true;</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +			return not_found(pvmw);</span>
<span class="quote">&gt;&gt;  		} else {</span>
<span class="quote">&gt;&gt;  			/* THP pmd was split under us: handle on pte level */</span>
<span class="quote">&gt;&gt;  			spin_unlock(pvmw-&gt;ptl);</span>

Thanks for your review.

--
Best Regards
Yan Zi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - June 21, 2017, 2:50 p.m.</div>
<pre class="content">
On Wed, Jun 21, 2017 at 10:37:30AM -0400, Zi Yan wrote:
<span class="quote">&gt; On 21 Jun 2017, at 7:23, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Tue, Jun 20, 2017 at 07:07:10PM -0400, Zi Yan wrote:</span>
<span class="quote">&gt; &gt;&gt; From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; &gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; &gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; &gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; &gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; &gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; &gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; &gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; &gt;&gt; allocate thps.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; &gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; &gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt; &gt;&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in</span>
<span class="quote">&gt; &gt;&gt;   set_pmd_migration_entry()</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt; &gt;&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v4 -&gt; v5:</span>
<span class="quote">&gt; &gt;&gt; - remove unnecessary PTE-mapped THP code in remove_migration_pmd()</span>
<span class="quote">&gt; &gt;&gt;   and set_pmd_migration_entry()</span>
<span class="quote">&gt; &gt;&gt; - restructure the code in zap_huge_pmd() to avoid factoring out</span>
<span class="quote">&gt; &gt;&gt;   the pte pgtable page code</span>
<span class="quote">&gt; &gt;&gt; - in zap_huge_pmd(), check that PMD swap entries are migration entries</span>
<span class="quote">&gt; &gt;&gt; - change author information</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; ChangeLog v5 -&gt; v7</span>
<span class="quote">&gt; &gt;&gt; - use macro to disable the code when thp migration is not enabled</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; &gt;&gt; Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt;&gt;  arch/x86/include/asm/pgtable_64.h |  2 +</span>
<span class="quote">&gt; &gt;&gt;  include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++-</span>
<span class="quote">&gt; &gt;&gt;  mm/huge_memory.c                  | 88 ++++++++++++++++++++++++++++++++++++---</span>
<span class="quote">&gt; &gt;&gt;  mm/migrate.c                      | 32 +++++++++++++-</span>
<span class="quote">&gt; &gt;&gt;  mm/page_vma_mapped.c              | 17 ++++++--</span>
<span class="quote">&gt; &gt;&gt;  mm/pgtable-generic.c              |  3 +-</span>
<span class="quote">&gt; &gt;&gt;  mm/rmap.c                         | 13 ++++++</span>
<span class="quote">&gt; &gt;&gt;  7 files changed, 212 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; &gt;&gt; index 45b7a4094de0..eac7f8cf4ae0 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; &gt;&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; &gt;&gt; @@ -208,7 +208,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt; &gt;&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt; &gt;&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt; &gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt; &gt;&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt; &gt;&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt; &gt;&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt; &gt;&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt; &gt;&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; &gt;&gt; index c5ff7b217ee6..ae0c5fc18788 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; &gt;&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; &gt;&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt; &gt;&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt; &gt;&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt; &gt;&gt;  {</span>
<span class="quote">&gt; &gt;&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt; &gt;&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt; &gt;&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt; &gt;&gt;  }</span>
<span class="quote">&gt; &gt;&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; &gt;&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt; &gt;&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt; &gt;&gt;  	 */</span>
<span class="quote">&gt; &gt;&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt; &gt;&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt; &gt;&gt;  	return p;</span>
<span class="quote">&gt; &gt;&gt;  }</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; @@ -163,6 +164,70 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;  #endif</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; &gt;&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; &gt;&gt; +		struct page *page);</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; &gt;&gt; +		struct page *new);</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; &gt;&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt; &gt;&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +#else</span>
<span class="quote">&gt; &gt;&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; &gt;&gt; +		struct page *page)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; &gt;&gt; +		struct page *new)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; &gt;&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; &gt;&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	return 0;</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +#endif</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt; &gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; index 421631ff3aeb..d9405ba628f6 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; @@ -1641,10 +1641,27 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;&gt;  	} else {</span>
<span class="quote">&gt; &gt;&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt;&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt;&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;&gt; +		struct page *page = NULL;</span>
<span class="quote">&gt; &gt;&gt; +		int migration = 0;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +		if (pmd_present(orig_pmd)) {</span>
<span class="quote">&gt; &gt;&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt;&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt;&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;&gt; +		} else {</span>
<span class="quote">&gt; &gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Can we have IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) instead here and below?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No. Both chunks have pmd_to_swp_entry(), which triggers BUILD_BUG()</span>
<span class="quote">&gt; when CONFIG_ARCH_ENABLE_THP_MIGRATION is not set. So we need this macro</span>
<span class="quote">&gt; to disable the code when THP migration is not enabled.</span>

I would rather downgrade pmd_to_swp_entry() to nop than have this ifdefs.
But up to you.
<span class="quote">
&gt; &gt;&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="quote">&gt; &gt;&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; &gt;&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; &gt;&gt; +			migration = 1;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I guess something like &#39;flush_needed&#39; instead would be more descriptive.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will change the name.</span>

Don&#39;t forget to revert the logic :P
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - June 21, 2017, 2:59 p.m.</div>
<pre class="content">
On 21 Jun 2017, at 10:50, Kirill A. Shutemov wrote:
<span class="quote">
&gt; On Wed, Jun 21, 2017 at 10:37:30AM -0400, Zi Yan wrote:</span>
<span class="quote">&gt;&gt; On 21 Jun 2017, at 7:23, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jun 20, 2017 at 07:07:10PM -0400, Zi Yan wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; From: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt;&gt;&gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt;&gt;&gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt;&gt;&gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt;&gt;&gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt;&gt;&gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt;&gt;&gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt;&gt;&gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt;&gt;&gt;&gt; allocate thps.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt;&gt;&gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt;&gt;&gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt;&gt;&gt;&gt; - use pmdp_huge_clear_flush() instead of pmdp_huge_get_and_clear() in</span>
<span class="quote">&gt;&gt;&gt;&gt;   set_pmd_migration_entry()</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt;&gt;&gt;&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ChangeLog v4 -&gt; v5:</span>
<span class="quote">&gt;&gt;&gt;&gt; - remove unnecessary PTE-mapped THP code in remove_migration_pmd()</span>
<span class="quote">&gt;&gt;&gt;&gt;   and set_pmd_migration_entry()</span>
<span class="quote">&gt;&gt;&gt;&gt; - restructure the code in zap_huge_pmd() to avoid factoring out</span>
<span class="quote">&gt;&gt;&gt;&gt;   the pte pgtable page code</span>
<span class="quote">&gt;&gt;&gt;&gt; - in zap_huge_pmd(), check that PMD swap entries are migration entries</span>
<span class="quote">&gt;&gt;&gt;&gt; - change author information</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ChangeLog v5 -&gt; v7</span>
<span class="quote">&gt;&gt;&gt;&gt; - use macro to disable the code when thp migration is not enabled</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;&gt;  arch/x86/include/asm/pgtable_64.h |  2 +</span>
<span class="quote">&gt;&gt;&gt;&gt;  include/linux/swapops.h           | 69 +++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/huge_memory.c                  | 88 ++++++++++++++++++++++++++++++++++++---</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/migrate.c                      | 32 +++++++++++++-</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/page_vma_mapped.c              | 17 ++++++--</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/pgtable-generic.c              |  3 +-</span>
<span class="quote">&gt;&gt;&gt;&gt;  mm/rmap.c                         | 13 ++++++</span>
<span class="quote">&gt;&gt;&gt;&gt;  7 files changed, 212 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt;&gt;&gt; index 45b7a4094de0..eac7f8cf4ae0 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -208,7 +208,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt;&gt;&gt;&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt;&gt;&gt;&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt;&gt;&gt;&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt;&gt;&gt;&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt;&gt;&gt;&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt;&gt;&gt; index c5ff7b217ee6..ae0c5fc18788 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;&gt;&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;&gt;&gt;&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt;&gt;&gt;&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt;&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;&gt;&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt;&gt;&gt;&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt;&gt;&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt;&gt;&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt;&gt;&gt;&gt;  	return p;</span>
<span class="quote">&gt;&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -163,6 +164,70 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt;&gt;&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct page *page);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct page *new);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +#else</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct page *page)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct page *new)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUILD_BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	return 0;</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt;&gt;&gt; index 421631ff3aeb..d9405ba628f6 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -1641,10 +1641,27 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt;&gt;&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;  	} else {</span>
<span class="quote">&gt;&gt;&gt;&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt;&gt;&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt;&gt;&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct page *page = NULL;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		int migration = 0;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (pmd_present(orig_pmd)) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		} else {</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Can we have IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) instead here and below?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; No. Both chunks have pmd_to_swp_entry(), which triggers BUILD_BUG()</span>
<span class="quote">&gt;&gt; when CONFIG_ARCH_ENABLE_THP_MIGRATION is not set. So we need this macro</span>
<span class="quote">&gt;&gt; to disable the code when THP migration is not enabled.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I would rather downgrade pmd_to_swp_entry() to nop than have this ifdefs.</span>
<span class="quote">&gt; But up to you.</span>

OK. I will do that.
<span class="quote">
&gt;&gt;&gt;&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="quote">&gt;&gt;&gt;&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt;&gt;&gt;&gt; +			migration = 1;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I guess something like &#39;flush_needed&#39; instead would be more descriptive.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Will change the name.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Don&#39;t forget to revert the logic :P</span>

Nice catch before I send the patch. Thanks. ;)

--
Best Regards
Yan Zi
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 45b7a4094de0..eac7f8cf4ae0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -208,7 +208,9 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \
 					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
<span class="p_add">+#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
<span class="p_add">+#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
 
 extern int kern_addr_valid(unsigned long addr);
 extern void cleanup_highmap(void);
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index c5ff7b217ee6..ae0c5fc18788 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -103,7 +103,8 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_del">-	BUG_ON(!PageLocked(page));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="p_add">+</span>
 	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,
 			page_to_pfn(page));
 }
<span class="p_chunk">@@ -126,7 +127,7 @@</span> <span class="p_context"> static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
 	 * Any use of migration entries may only occur while the
 	 * corresponding page is locked
 	 */
<span class="p_del">-	BUG_ON(!PageLocked(p));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(p)));</span>
 	return p;
 }
 
<span class="p_chunk">@@ -163,6 +164,70 @@</span> <span class="p_context"> static inline int is_write_migration_entry(swp_entry_t entry)</span>
 
 #endif
 
<span class="p_add">+struct page_vma_mapped_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="p_add">+	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="p_add">+	return __swp_entry_to_pmd(arch_entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return (pmd_t){ 0 };</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MEMORY_FAILURE
 
 extern atomic_long_t num_poisoned_pages __read_mostly;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 421631ff3aeb..d9405ba628f6 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1641,10 +1641,27 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		struct page *page = NULL;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pmd_present(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			VM_BUG_ON(!is_pmd_migration_entry(orig_pmd));</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+			migration = 1;</span>
<span class="p_add">+#else</span>
<span class="p_add">+			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if (PageAnon(page)) {
 			zap_deposited_table(tlb-&gt;mm, pmd);
 			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);
<span class="p_chunk">@@ -1653,8 +1670,10 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 				zap_deposited_table(tlb-&gt;mm, pmd);
 			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);
 		}
<span class="p_add">+</span>
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -2694,3 +2713,62 @@</span> <span class="p_context"> static int __init split_huge_pages_debugfs(void)</span>
 }
 late_initcall(split_huge_pages_debugfs);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	pmd_t pmdval;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="p_add">+			address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+	if (pmd_dirty(pmdval))</span>
<span class="p_add">+		set_page_dirty(page);</span>
<span class="p_add">+	entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="p_add">+	pmdval = swp_entry_to_pmd(entry);</span>
<span class="p_add">+	set_pmd_at(mm, address, pvmw-&gt;pmd, pmdval);</span>
<span class="p_add">+	page_remove_rmap(page, true);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="p_add">+			address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="p_add">+	pmd_t pmde;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+	get_page(new);</span>
<span class="p_add">+	pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	if (is_write_migration_entry(entry))</span>
<span class="p_add">+		pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+	page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="p_add">+	set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="p_add">+	flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+		mlock_vma_page(new);</span>
<span class="p_add">+	update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 627671551873..cae5c3b3b491 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -215,6 +215,15 @@</span> <span class="p_context"> static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
 			new = page - pvmw.page-&gt;index +
 				linear_page_index(vma, pvmw.address);
 
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (!pvmw.pte &amp;&amp; pvmw.page) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page), page);</span>
<span class="p_add">+			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 		get_page(new);
 		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));
 		if (pte_swp_soft_dirty(*pvmw.pte))
<span class="p_chunk">@@ -329,6 +338,27 @@</span> <span class="p_context"> void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
 	__migration_entry_wait(mm, pte, ptl);
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pmd_lock(mm, pmd);</span>
<span class="p_add">+	if (!is_pmd_migration_entry(*pmd))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="p_add">+	if (!get_page_unless_zero(page))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	wait_on_page_locked(page);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLOCK
 /* Returns true if all buffers are successfully locked */
 static bool buffer_migrate_lock_buffers(struct buffer_head *head,
<span class="p_chunk">@@ -1087,7 +1117,7 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 		goto out;
 	}
 
<span class="p_del">-	if (unlikely(PageTransHuge(page))) {</span>
<span class="p_add">+	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
 		lock_page(page);
 		rc = split_huge_page(page);
 		unlock_page(page);
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index 8ec6ba230bb9..ff5517e67788 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -138,16 +138,27 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	if (!pud_present(*pud))
 		return false;
 	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);
<span class="p_del">-	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
 		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);
<span class="p_del">-		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="p_del">-			return not_found(pvmw);</span>
 		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {
 			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)
 				return not_found(pvmw);
 			if (pmd_page(*pvmw-&gt;pmd) != page)
 				return not_found(pvmw);
 			return true;
<span class="p_add">+		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="p_add">+				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+				if (migration_entry_to_page(entry) != page)</span>
<span class="p_add">+					return not_found(pvmw);</span>
<span class="p_add">+				return true;</span>
<span class="p_add">+			}</span>
<span class="p_add">+#else</span>
<span class="p_add">+			WARN_ONCE(1, &quot;Non present huge pmd without pmd migration enabled!&quot;);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			return not_found(pvmw);</span>
 		} else {
 			/* THP pmd was split under us: handle on pte level */
 			spin_unlock(pvmw-&gt;ptl);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index c99d9512a45b..1175f6a24fdb 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -124,7 +124,8 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 {
 	pmd_t pmd;
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="p_add">+	VM_BUG_ON((pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="p_add">+			   !pmd_devmap(*pmdp)) || !pmd_present(*pmdp));</span>
 	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return pmd;
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 91948fbbb0bb..b28f633cd569 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1302,6 +1302,7 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	bool ret = true;
 	enum ttu_flags flags = (enum ttu_flags)arg;
 
<span class="p_add">+</span>
 	/* munlock has nothing to gain from examining un-locked vmas */
 	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))
 		return true;
<span class="p_chunk">@@ -1312,6 +1313,18 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	}
 
 	while (page_vma_mapped_walk(&amp;pvmw)) {
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (flags &amp; TTU_MIGRATION) {</span>
<span class="p_add">+			if (!pvmw.pte &amp;&amp; page) {</span>
<span class="p_add">+				VM_BUG_ON_PAGE(PageHuge(page) || !PageTransCompound(page),</span>
<span class="p_add">+						page);</span>
<span class="p_add">+				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 		/*
 		 * If the page is mlock()d, we cannot swap it out.
 		 * If it&#39;s recently referenced (perhaps page_referenced

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



