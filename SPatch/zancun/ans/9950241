
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v8,11/18] RISC-V: Atomic and Locking Code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v8,11/18] RISC-V: Atomic and Locking Code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 12, 2017, 9:57 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170912215715.4186-12-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9950241/mbox/"
   >mbox</a>
|
   <a href="/patch/9950241/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9950241/">/patch/9950241/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7DD5D602C9 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Sep 2017 22:00:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6C7E129095
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Sep 2017 22:00:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5FC50290BC; Tue, 12 Sep 2017 22:00:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D657428C60
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Sep 2017 22:00:44 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751899AbdILWAl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 Sep 2017 18:00:41 -0400
Received: from mail-pg0-f68.google.com ([74.125.83.68]:38850 &quot;EHLO
	mail-pg0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751623AbdILV6A (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 Sep 2017 17:58:00 -0400
Received: by mail-pg0-f68.google.com with SMTP id m30so1775685pgn.5
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 12 Sep 2017 14:57:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=subject:date:message-id:in-reply-to:references:cc:from:to;
	bh=oXnwijCzmo6ScPG0eSybKDthwLqqPT+V1CEU+x72tSQ=;
	b=qgpO++CI0OWN9tPhnNRp+650bYj+nB5zRAhCwWp57i8iKqaNeAlwflurVQmqDVJGLo
	iZh89SCnsorVLXcxLo9LtLk4mAbYX6bKfmOtRfpYIkqXWhqh2x2ZEQKwb0Ty8NErCOiG
	GALAgFw/cz74LOWWptGb3BM4s5o+qV19Y+A72ss/o6Ke+IM6Le85zeyCS4B7WlSXh8Aa
	bEbn2kGVHzFUaMqsitJwV3iOMIztVAqiMHEa8uf9DGY9LfIZqdbjmeD5Mix24buODxYW
	3FVTJHZ60M5CUZ2U+ZmfQBptBrRlY9RGuCOUiEk/ElQTyXi7FTdeVl3dlybI4QH9lh3H
	XmWA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:subject:date:message-id:in-reply-to:references
	:cc:from:to;
	bh=oXnwijCzmo6ScPG0eSybKDthwLqqPT+V1CEU+x72tSQ=;
	b=aA7JeeOw6HaTbW7ugczrM7OWKw0D7uIlWnk+TkjIhBx8yiH5tiYSYVC7lxbC+4ubEV
	MdswWlxnUkIgmvXKzsFSHNT5MthrYIqUPb9io51dhBJH9ARRZqWys0Xf4G/k1/IF8Lgd
	duQs5XeL7s5BQV6emKfnst/nZNhu8lynWQwu5h9Uk49FSaGlBVeUCGSfwNAd/oksBV0Q
	43pXdy01JeyMQ5RMISTlTGrU6AEqILE5zGzJTRgKrKOvQQr183Z1y+7/JmQW7hyiZQ48
	usf6OrDbH71bFmt0+sZms+16SyKlPF4c0jZZvfpP5jto+XjUWD0aKg6dBircQEKg9dWW
	/QEQ==
X-Gm-Message-State: AHPjjUj45yNI9enCoCFWlWhp0DK5yoSg26jXKqBXnVGyTctcGVo7BCUg
	XZkgTVLOjfjkE1Ll
X-Google-Smtp-Source: ADKCNb6togspTfNCsT9j7S69ksWQQYyGqcibsqowQdyrST49uxRbpLQ7yXthOuY4/v6NiGVpcOceXg==
X-Received: by 10.84.216.78 with SMTP id f14mr11118014plj.394.1505253478828; 
	Tue, 12 Sep 2017 14:57:58 -0700 (PDT)
Received: from localhost ([12.206.222.5]) by smtp.gmail.com with ESMTPSA id
	a19sm21305517pfj.88.2017.09.12.14.57.57
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 12 Sep 2017 14:57:58 -0700 (PDT)
Subject: [PATCH v8 11/18] RISC-V: Atomic and Locking Code
Date: Tue, 12 Sep 2017 14:57:08 -0700
Message-Id: &lt;20170912215715.4186-12-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.5
In-Reply-To: &lt;20170912215715.4186-1-palmer@dabbelt.com&gt;
References: &lt;20170912215715.4186-1-palmer@dabbelt.com&gt;
Cc: yamada.masahiro@socionext.com, mmarek@suse.com, albert@sifive.com,
	will.deacon@arm.com, boqun.feng@gmail.com, oleg@redhat.com,
	mingo@redhat.com, daniel.lezcano@linaro.org,
	gregkh@linuxfoundation.org, jslaby@suse.com, davem@davemloft.net,
	mchehab@kernel.org, hverkuil@xs4all.nl, rdunlap@infradead.org,
	viro@zeniv.linux.org.uk, mhiramat@kernel.org, fweisbec@gmail.com,
	mcgrof@kernel.org, dledford@redhat.com, bart.vanassche@sandisk.com,
	sstabellini@kernel.org, mpe@ellerman.id.au,
	rmk+kernel@armlinux.org.uk, paul.gortmaker@windriver.com,
	nicolas.dichtel@6wind.com, linux@roeck-us.net,
	heiko.carstens@de.ibm.com, schwidefsky@de.ibm.com,
	geert@linux-m68k.org, akpm@linux-foundation.org,
	andriy.shevchenko@linux.intel.com, jiri@mellanox.com,
	vgupta@synopsys.com, airlied@redhat.com, jk@ozlabs.org,
	chris@chris-wilson.co.uk, Jason@zx2c4.com,
	paulmck@linux.vnet.ibm.com, ncardwell@google.com,
	linux-kernel@vger.kernel.org, linux-kbuild@vger.kernel.org,
	patches@groups.riscv.org, Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: peterz@infradead.org, tglx@linutronix.de, jason@lakedaemon.net,
	marc.zyngier@arm.com, Arnd Bergmann &lt;arnd@arndb.de&gt;, dmitriy@oss-tech.org
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - Sept. 12, 2017, 9:57 p.m.</div>
<pre class="content">
This contains all the code that directly interfaces with the RISC-V
memory model.  While this code corforms to the current RISC-V ISA
specifications (user 2.2 and priv 1.10), the memory model is somewhat
underspecified in those documents.  There is a working group that hopes
to produce a formal memory model by the end of the year, but my
understanding is that the basic definitions we&#39;re relying on here won&#39;t
change significantly.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/include/asm/atomic.h         | 375 ++++++++++++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h        |  68 ++++++
 arch/riscv/include/asm/bitops.h         | 218 +++++++++++++++++++
 arch/riscv/include/asm/cacheflush.h     |  39 ++++
 arch/riscv/include/asm/cmpxchg.h        | 134 ++++++++++++
 arch/riscv/include/asm/io.h             | 303 ++++++++++++++++++++++++++
 arch/riscv/include/asm/spinlock.h       | 165 ++++++++++++++
 arch/riscv/include/asm/spinlock_types.h |  33 +++
 arch/riscv/include/asm/tlb.h            |  24 ++
 arch/riscv/include/asm/tlbflush.h       |  64 ++++++
 10 files changed, 1423 insertions(+)
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e2e37c57cbeb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,375 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+# include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+# if (__riscv_xlen &lt; 64)</span>
<span class="p_add">+#  error &quot;64-bit atomics require XLEN to be at least 64&quot;</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+static __always_inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC64_INIT(i) { (i) }</span>
<span class="p_add">+static __always_inline long atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic64_set(atomic64_t *v, long i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * First, the atomic ops that have no ordering constraints and therefor don&#39;t</span>
<span class="p_add">+ * have the AQ or RL bits set.  These don&#39;t return anything, so there&#39;s only</span>
<span class="p_add">+ * one version to worry about.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, asm_op, c_op, I, asm_type, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(c_type i, atomic##prefix##_t *v)		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	__asm__ __volatile__ (									\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type &quot; zero, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)								\</span>
<span class="p_add">+		: &quot;r&quot; (I)									\</span>
<span class="p_add">+		: &quot;memory&quot;);									\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic ops that have ordered, relaxed, acquire, and relese variants.</span>
<span class="p_add">+ * There&#39;s two flavors of these: the arithmatic ops have both fetch and return</span>
<span class="p_add">+ * versions, while the logical ops only have fetch versions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_fetch_##op##c_or(c_type i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+	register c_type ret;										\</span>
<span class="p_add">+	__asm__ __volatile__ (										\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type #asm_or &quot; %1, %2, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)								\</span>
<span class="p_add">+		: &quot;r&quot; (I)										\</span>
<span class="p_add">+		: &quot;memory&quot;);										\</span>
<span class="p_add">+	return ret;											\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_##op##_return##c_or(c_type i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op##c_or(i, v) c_op I;						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, d, long, 64)	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )		\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * AMO-based operations above (aside from sub, which is easier to fit above).</span>
<span class="p_add">+ * These are required to perform a barrier, but they&#39;re OK this way because</span>
<span class="p_add">+ * atomic_*_return is also required to perform a barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(c_type i, atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(i, v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )		\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add_and_test, add, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(sub_and_test, sub, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(add_negative, add,  &lt;, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, c_op, I, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	atomic##prefix##_##func_op(I, v);					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, func_op, c_op, I, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_fetch_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{											\</span>
<span class="p_add">+	return atomic##prefix##_fetch_##func_op(I, v);					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_##op##_return(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{											\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op(v) c_op I;					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,  int,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I, long, 64)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, long, 64)				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(inc, add, +,  1)</span>
<span class="p_add">+ATOMIC_OPS(dec, add, +, -1)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0,   )</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0,   )</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0, 64)</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+/* This is required to provide a barrier on success. */</span>
<span class="p_add">+static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.w.aqrl  %[p],  %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;beq        %[p],  %[u], 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %[rc],  %[p], %[a]\n\t&quot;</span>
<span class="p_add">+		&quot;sc.w.aqrl %[rc], %[rc], %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %[rc], 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: [p]&quot;=&amp;r&quot; (prev), [rc]&quot;=&amp;r&quot; (rc), [c]&quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: [a]&quot;r&quot; (a), [u]&quot;r&quot; (u)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long __atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       long prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.d.aqrl  %[p],  %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;beq        %[p],  %[u], 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %[rc],  %[p], %[a]\n\t&quot;</span>
<span class="p_add">+		&quot;sc.d.aqrl %[rc], %[rc], %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %[rc], 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: [p]&quot;=&amp;r&quot; (prev), [rc]&quot;=&amp;r&quot; (rc), [c]&quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: [a]&quot;r&quot; (a), [u]&quot;r&quot; (u)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline int atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __atomic64_add_unless(v, a, u) != u;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * LR/SC-based operations above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int atomic_inc_not_zero(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return __atomic_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="p_add">+ * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="p_add">+ * use the other implementations directly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(c_t, prefix, c_or, size, asm_or)						\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_cmpxchg##c_or(atomic##prefix##_t *v, c_t o, c_t n) 	\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __cmpxchg(&amp;(v-&gt;counter), o, n, size, asm_or, asm_or);				\</span>
<span class="p_add">+}												\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_xchg##c_or(atomic##prefix##_t *v, c_t n) 		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __xchg(n, &amp;(v-&gt;counter), size, asm_or);						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)		\</span>
<span class="p_add">+	ATOMIC_OP(long, 64, c_or, 8, asm_or)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(        , .aqrl)</span>
<span class="p_add">+ATOMIC_OPS(_acquire,   .aq)</span>
<span class="p_add">+ATOMIC_OPS(_release,   .rl)</span>
<span class="p_add">+ATOMIC_OPS(_relaxed,      )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline int atomic_sub_if_positive(atomic_t *v, int offset)</span>
<span class="p_add">+{</span>
<span class="p_add">+       int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.w.aqrl  %[p],  %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;sub       %[rc],  %[p], %[o]\n\t&quot;</span>
<span class="p_add">+		&quot;bltz      %[rc],    1f\n\t&quot;</span>
<span class="p_add">+		&quot;sc.w.aqrl %[rc], %[rc], %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %[rc],    0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: [p]&quot;=&amp;r&quot; (prev), [rc]&quot;=&amp;r&quot; (rc), [c]&quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: [o]&quot;r&quot; (offset)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev - offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_dec_if_positive(v)	atomic_sub_if_positive(v, 1)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long atomic64_sub_if_positive(atomic64_t *v, int offset)</span>
<span class="p_add">+{</span>
<span class="p_add">+       long prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.d.aqrl  %[p],  %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;sub       %[rc],  %[p], %[o]\n\t&quot;</span>
<span class="p_add">+		&quot;bltz      %[rc],    1f\n\t&quot;</span>
<span class="p_add">+		&quot;sc.d.aqrl %[rc], %[rc], %[c]\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %[rc],    0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: [p]&quot;=&amp;r&quot; (prev), [rc]&quot;=&amp;r&quot; (rc), [c]&quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: [o]&quot;r&quot; (offset)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev - offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic64_dec_if_positive(v)	atomic64_sub_if_positive(v, 1)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..183534b7c39b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,68 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()		__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_FENCE(p, s) \</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;fence &quot; #p &quot;,&quot; #s : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barriers need to enforce ordering on both devices or memory. */</span>
<span class="p_add">+#define mb()		RISCV_FENCE(iorw,iorw)</span>
<span class="p_add">+#define rmb()		RISCV_FENCE(ir,ir)</span>
<span class="p_add">+#define wmb()		RISCV_FENCE(ow,ow)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barriers do not need to enforce ordering on devices, just memory. */</span>
<span class="p_add">+#define smp_mb()	RISCV_FENCE(rw,rw)</span>
<span class="p_add">+#define smp_rmb()	RISCV_FENCE(r,r)</span>
<span class="p_add">+#define smp_wmb()	RISCV_FENCE(w,w)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These fences exist to enforce ordering around the relaxed AMOs.  The</span>
<span class="p_add">+ * documentation defines that</span>
<span class="p_add">+ * &quot;</span>
<span class="p_add">+ *     atomic_fetch_add();</span>
<span class="p_add">+ *   is equivalent to:</span>
<span class="p_add">+ *     smp_mb__before_atomic();</span>
<span class="p_add">+ *     atomic_fetch_add_relaxed();</span>
<span class="p_add">+ *     smp_mb__after_atomic();</span>
<span class="p_add">+ * &quot;</span>
<span class="p_add">+ * So we emit full fences on both sides.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smb_mb__before_atomic()	smp_mb()</span>
<span class="p_add">+#define __smb_mb__after_atomic()	smp_mb()</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These barriers prevent accesses performed outside a spinlock from being moved</span>
<span class="p_add">+ * inside a spinlock.  Since RISC-V sets the aq/rl bits on our spinlock only</span>
<span class="p_add">+ * enforce release consistency, we need full fences here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smb_mb__before_spinlock()	smp_mb()</span>
<span class="p_add">+#define smb_mb__after_spinlock()	smp_mb()</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..7c281ef1d583</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,218 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit_ord(op, mod, nr, addr, ord)		\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) #ord &quot; %0, %2, %1&quot;			\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask))				\</span>
<span class="p_add">+		: &quot;memory&quot;);					\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit_ord(op, mod, nr, addr, ord)			\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) #ord &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr)))			\</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr) 			\</span>
<span class="p_add">+	__test_and_op_bit_ord(op, mod, nr, addr, )</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__op_bit_ord(op, mod, nr, addr, )</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation may be reordered on other architectures than x86.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation can be reordered on other architectures other than x86.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit()  may be reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit_ord(or, __NOP, nr, addr, .aq);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit_ord(and, __NOT, nr, addr, .rl);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On RISC-V systems there seems to be no benefit to taking advantage of the</span>
<span class="p_add">+ * non-atomic property here: it&#39;s a lot more instructions and we still have to</span>
<span class="p_add">+ * provide release semantics anyway.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit_unlock(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0595585013b0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..db249dbc7b97</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,134 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size, asm_or)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new)				\</span>
<span class="p_add">+			: &quot;memory&quot;);				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new)				\</span>
<span class="p_add">+			: &quot;memory&quot;);				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr)), .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg32(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg64(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size, lrb, scb)			\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __old = (old);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.w&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new)			\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.d&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new)			\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), .aqrl, .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), , ))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c1f32cfcc79b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,303 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * {read,write}{b,w,l,q} based on arch/arm64/include/asm/io.h</span>
<span class="p_add">+ *   which was based on arch/arm/include/io.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1996-2000 Russell King</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The RISC-V ISA doesn&#39;t yet specify how to query or modify PMAs, so we can&#39;t</span>
<span class="p_add">+ * change the properties of memory regions.  This should be fixed by the</span>
<span class="p_add">+ * upcoming platform spec.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Generic IO read/write.  These perform native-endian accesses. */</span>
<span class="p_add">+#define __raw_writeb __raw_writeb</span>
<span class="p_add">+static inline void __raw_writeb(u8 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sb %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writew __raw_writew</span>
<span class="p_add">+static inline void __raw_writew(u16 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sh %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writel __raw_writel</span>
<span class="p_add">+static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sw %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __raw_writeq __raw_writeq</span>
<span class="p_add">+static inline void __raw_writeq(u64 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sd %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readb __raw_readb</span>
<span class="p_add">+static inline u8 __raw_readb(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lb %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readw __raw_readw</span>
<span class="p_add">+static inline u16 __raw_readw(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lh %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readl __raw_readl</span>
<span class="p_add">+static inline u32 __raw_readl(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lw %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __raw_readq __raw_readq</span>
<span class="p_add">+static inline u64 __raw_readq(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;ld %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: I&#39;m flip-flopping on whether or not we should keep this or enforce</span>
<span class="p_add">+ * the ordering with I/O on spinlocks like PowerPC does.  The worry is that</span>
<span class="p_add">+ * drivers won&#39;t get this correct, but I also don&#39;t want to introduce a fence</span>
<span class="p_add">+ * into the lock code that otherwise only uses AMOs (and is essentially defined</span>
<span class="p_add">+ * by the ISA to be correct).   For now I&#39;m leaving this here: &quot;o,w&quot; is</span>
<span class="p_add">+ * sufficient to ensure that all writes to the device have completed before the</span>
<span class="p_add">+ * write to the spinlock is allowed to commit.  I surmised this from reading</span>
<span class="p_add">+ * &quot;ACQUIRES VS I/O ACCESSES&quot; in memory-barriers.txt.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define mmiowb()	__asm__ __volatile__ (&quot;fence o,w&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Unordered I/O memory access primitives.  These are even more relaxed than</span>
<span class="p_add">+ * the relaxed versions, as they don&#39;t even order accesses between successive</span>
<span class="p_add">+ * operations to the I/O regions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb_cpu(c)		({ u8  __r = __raw_readb(c); __r; })</span>
<span class="p_add">+#define readw_cpu(c)		({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="p_add">+#define readl_cpu(c)		({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_cpu(v,c)		((void)__raw_writeb((v),(c)))</span>
<span class="p_add">+#define writew_cpu(v,c)		((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="p_add">+#define writel_cpu(v,c)		((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define readq_cpu(c)		({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="p_add">+#define writeq_cpu(v,c)		((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="p_add">+ * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="p_add">+ * accesses.  These are defined to order the indicated access (either a read or</span>
<span class="p_add">+ * write) with all other I/O memory accesses. Since the platform specification</span>
<span class="p_add">+ * defines that all I/O regions are strongly ordered on channel 2, no explicit</span>
<span class="p_add">+ * fences are required to enforce this ordering.</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* FIXME: These are now the same as asm-generic */</span>
<span class="p_add">+#define __io_rbr()		do {} while (0)</span>
<span class="p_add">+#define __io_rar()		do {} while (0)</span>
<span class="p_add">+#define __io_rbw()		do {} while (0)</span>
<span class="p_add">+#define __io_raw()		do {} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb_relaxed(c)	({ u8  __v; __io_rbr(); __v = readb_cpu(c); __io_rar(); __v; })</span>
<span class="p_add">+#define readw_relaxed(c)	({ u16 __v; __io_rbr(); __v = readw_cpu(c); __io_rar(); __v; })</span>
<span class="p_add">+#define readl_relaxed(c)	({ u32 __v; __io_rbr(); __v = readl_cpu(c); __io_rar(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_relaxed(v,c)	({ __io_rbw(); writeb_cpu((v),(c)); __io_raw(); })</span>
<span class="p_add">+#define writew_relaxed(v,c)	({ __io_rbw(); writew_cpu((v),(c)); __io_raw(); })</span>
<span class="p_add">+#define writel_relaxed(v,c)	({ __io_rbw(); writel_cpu((v),(c)); __io_raw(); })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define readq_relaxed(c)	({ u64 __v; __io_rbr(); __v = readq_cpu(c); __io_rar(); __v; })</span>
<span class="p_add">+#define writeq_relaxed(v,c)	({ __io_rbw(); writeq_cpu((v),(c)); __io_raw(); })</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="p_add">+ * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="p_add">+ * Normal memory access.  The memory barriers here are necessary as RISC-V</span>
<span class="p_add">+ * doesn&#39;t define any ordering between the memory space and the I/O space.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __io_br()	do {} while (0)</span>
<span class="p_add">+#define __io_ar()	__asm__ __volatile__ (&quot;fence i,r&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __io_bw()	__asm__ __volatile__ (&quot;fence w,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __io_aw()	do {} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb(c)	({ u8  __v; __io_br(); __v = readb_cpu(c); __io_ar(); __v; })</span>
<span class="p_add">+#define readw(c)	({ u16 __v; __io_br(); __v = readw_cpu(c); __io_ar(); __v; })</span>
<span class="p_add">+#define readl(c)	({ u32 __v; __io_br(); __v = readl_cpu(c); __io_ar(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb(v,c)	({ __io_bw(); writeb_cpu((v),(c)); __io_aw(); })</span>
<span class="p_add">+#define writew(v,c)	({ __io_bw(); writew_cpu((v),(c)); __io_aw(); })</span>
<span class="p_add">+#define writel(v,c)	({ __io_bw(); writel_cpu((v),(c)); __io_aw(); })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define readq(c)	({ u64 __v; __io_br(); __v = readq_cpu(c); __io_ar(); __v; })</span>
<span class="p_add">+#define writeq(v,c)	({ __io_bw(); writeq_cpu((v),(c)); __io_aw(); })</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Emulation routines for the port-mapped IO space used by some PCI drivers.</span>
<span class="p_add">+ * These are defined as being &quot;fully synchronous&quot;, but also &quot;not guaranteed to</span>
<span class="p_add">+ * be fully ordered with respect to other memory and I/O operations&quot;.  We&#39;re</span>
<span class="p_add">+ * going to be on the safe side here and just make them:</span>
<span class="p_add">+ *  - Fully ordered WRT each other, by bracketing them with two fences.  The</span>
<span class="p_add">+ *    outer set contains both I/O so inX is ordered with outX, while the inner just</span>
<span class="p_add">+ *    needs the type of the access (I for inX and O for outX).</span>
<span class="p_add">+ *  - Ordered in the same manner as readX/writeX WRT memory by subsuming their</span>
<span class="p_add">+ *    fences.</span>
<span class="p_add">+ *  - Ordered WRT timer reads, so udelay and friends don&#39;t get elided by the</span>
<span class="p_add">+ *    implementation.</span>
<span class="p_add">+ * Note that there is no way to actually enforce that outX is a non-posted</span>
<span class="p_add">+ * operation on RISC-V, but hopefully the timer ordering constraint is</span>
<span class="p_add">+ * sufficient to ensure this works sanely on controllers that support I/O</span>
<span class="p_add">+ * writes.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __io_pbr()	__asm__ __volatile__ (&quot;fence io,i&quot;  : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __io_par()	__asm__ __volatile__ (&quot;fence i,ior&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __io_pbw()	__asm__ __volatile__ (&quot;fence iow,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __io_paw()	__asm__ __volatile__ (&quot;fence o,io&quot;  : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define inb(c)		({ u8  __v; __io_pbr(); __v = readb_cpu((void*)(PCI_IOBASE + (c))); __io_par(); __v; })</span>
<span class="p_add">+#define inw(c)		({ u16 __v; __io_pbr(); __v = readw_cpu((void*)(PCI_IOBASE + (c))); __io_par(); __v; })</span>
<span class="p_add">+#define inl(c)		({ u32 __v; __io_pbr(); __v = readl_cpu((void*)(PCI_IOBASE + (c))); __io_par(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define outb(v,c)	({ __io_pbw(); writeb_cpu((v),(void*)(PCI_IOBASE + (c))); __io_paw(); })</span>
<span class="p_add">+#define outw(v,c)	({ __io_pbw(); writew_cpu((v),(void*)(PCI_IOBASE + (c))); __io_paw(); })</span>
<span class="p_add">+#define outl(v,c)	({ __io_pbw(); writel_cpu((v),(void*)(PCI_IOBASE + (c))); __io_paw(); })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define inq(c)		({ u64 __v; __io_pbr(); __v = readq_cpu((void*)(c)); __io_par(); __v; })</span>
<span class="p_add">+#define outq(v,c)	({ __io_pbw(); writeq_cpu((v),(void*)(c)); __io_paw(); })</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Accesses from a single hart to a single I/O address must be ordered.  This</span>
<span class="p_add">+ * allows us to use the raw read macros, but we still need to fence before and</span>
<span class="p_add">+ * after the block to ensure ordering WRT other macros.  These are defined to</span>
<span class="p_add">+ * perform host-endian accesses so we use __raw instead of __cpu.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __io_reads_ins(port, ctype, len, bfence, afence)			\</span>
<span class="p_add">+	static inline void __ ## port ## len(const volatile void __iomem *addr,	\</span>
<span class="p_add">+					     void *buffer,			\</span>
<span class="p_add">+					     unsigned int count)		\</span>
<span class="p_add">+	{									\</span>
<span class="p_add">+		bfence;								\</span>
<span class="p_add">+		if (count) {							\</span>
<span class="p_add">+			ctype *buf = buffer;					\</span>
<span class="p_add">+										\</span>
<span class="p_add">+			do {							\</span>
<span class="p_add">+				ctype x = __raw_read ## len(addr);		\</span>
<span class="p_add">+				*buf++ = x;					\</span>
<span class="p_add">+			} while (--count);					\</span>
<span class="p_add">+		}								\</span>
<span class="p_add">+		afence;								\</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __io_writes_outs(port, ctype, len, bfence, afence)			\</span>
<span class="p_add">+	static inline void __ ## port ## len(volatile void __iomem *addr,	\</span>
<span class="p_add">+					     const void *buffer,		\</span>
<span class="p_add">+					     unsigned int count)		\</span>
<span class="p_add">+	{									\</span>
<span class="p_add">+		bfence;								\</span>
<span class="p_add">+		if (count) {							\</span>
<span class="p_add">+			const ctype *buf = buffer;				\</span>
<span class="p_add">+										\</span>
<span class="p_add">+			do {							\</span>
<span class="p_add">+				__raw_writeq(*buf++, addr);			\</span>
<span class="p_add">+			} while (--count);					\</span>
<span class="p_add">+		}								\</span>
<span class="p_add">+		afence;								\</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+__io_reads_ins(reads,  u8, b, __io_br(), __io_ar())</span>
<span class="p_add">+__io_reads_ins(reads, u16, w, __io_br(), __io_ar())</span>
<span class="p_add">+__io_reads_ins(reads, u32, l, __io_br(), __io_ar())</span>
<span class="p_add">+#define readsb(addr, buffer, count) __readsb(addr, buffer, count)</span>
<span class="p_add">+#define readsw(addr, buffer, count) __readsw(addr, buffer, count)</span>
<span class="p_add">+#define readsl(addr, buffer, count) __readsl(addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_reads_ins(ins,  u8, b, __io_pbr(), __io_par())</span>
<span class="p_add">+__io_reads_ins(ins, u16, w, __io_pbr(), __io_par())</span>
<span class="p_add">+__io_reads_ins(ins, u32, l, __io_pbr(), __io_par())</span>
<span class="p_add">+#define insb(addr, buffer, count) __insb((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+#define insw(addr, buffer, count) __insw((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+#define insl(addr, buffer, count) __insl((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_writes_outs(writes,  u8, b, __io_bw(), __io_aw())</span>
<span class="p_add">+__io_writes_outs(writes, u16, w, __io_bw(), __io_aw())</span>
<span class="p_add">+__io_writes_outs(writes, u32, l, __io_bw(), __io_aw())</span>
<span class="p_add">+#define writesb(addr, buffer, count) __writesb(addr, buffer, count)</span>
<span class="p_add">+#define writesw(addr, buffer, count) __writesw(addr, buffer, count)</span>
<span class="p_add">+#define writesl(addr, buffer, count) __writesl(addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_writes_outs(outs,  u8, b, __io_pbw(), __io_paw())</span>
<span class="p_add">+__io_writes_outs(outs, u16, w, __io_pbw(), __io_paw())</span>
<span class="p_add">+__io_writes_outs(outs, u32, l, __io_pbw(), __io_paw())</span>
<span class="p_add">+#define outsb(addr, buffer, count) __outsb((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+#define outsw(addr, buffer, count) __outsw((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+#define outsl(addr, buffer, count) __outsl((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+__io_reads_ins(reads, u64, q, __io_br(), __io_ar())</span>
<span class="p_add">+#define readsq(addr, buffer, count) __readsq(addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_reads_ins(ins, u64, q, __io_pbr(), __io_par())</span>
<span class="p_add">+#define insq(addr, buffer, count) __insq((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_writes_outs(writes, u64, q, __io_bw(), __io_aw())</span>
<span class="p_add">+#define writesq(addr, buffer, count) __writesq(addr, buffer, count)</span>
<span class="p_add">+</span>
<span class="p_add">+__io_writes_outs(outs, u64, q, __io_pbr(), __io_paw())</span>
<span class="p_add">+#define outsq(addr, buffer, count) __outsq((void __iomem *)addr, buffer, count)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b3b394ffaf7e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,165 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+	} while (arch_spin_is_locked(lock));</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83ac4ac9e2ac</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c229509288ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ee4ae370b5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



