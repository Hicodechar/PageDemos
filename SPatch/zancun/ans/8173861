
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,5/8] procfs: Add support for PUDs to smaps, clear_refs and pagemap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,5/8] procfs: Add support for PUDs to smaps, clear_refs and pagemap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 31, 2016, 12:09 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1454242175-16870-6-git-send-email-matthew.r.wilcox@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8173861/mbox/"
   >mbox</a>
|
   <a href="/patch/8173861/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8173861/">/patch/8173861/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 69EE99FC36
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 31 Jan 2016 12:10:24 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id A968C20253
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 31 Jan 2016 12:10:23 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C947420351
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 31 Jan 2016 12:10:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932297AbcAaMKR (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 31 Jan 2016 07:10:17 -0500
Received: from mga03.intel.com ([134.134.136.65]:60406 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1757412AbcAaMJv (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 31 Jan 2016 07:09:51 -0500
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by orsmga103.jf.intel.com with ESMTP; 31 Jan 2016 04:09:50 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.22,374,1449561600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;902231156&quot;
Received: from vsundhar-mobl.amr.corp.intel.com (HELO thog.int.wil.cx)
	([10.252.206.106])
	by orsmga002.jf.intel.com with SMTP; 31 Jan 2016 04:09:47 -0800
Received: by thog.int.wil.cx (Postfix, from userid 1000)
	id 43CDF61C97; Sun, 31 Jan 2016 07:09:42 -0500 (EST)
From: Matthew Wilcox &lt;matthew.r.wilcox@intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Matthew Wilcox &lt;willy@linux.intel.com&gt;, linux-mm@kvack.org,
	linux-nvdimm@lists.01.org, linux-fsdevel@vger.kernel.org,
	linux-kernel@vger.kernel.org, x86@kernel.org
Subject: [PATCH v4 5/8] procfs: Add support for PUDs to smaps,
	clear_refs and pagemap
Date: Sun, 31 Jan 2016 23:09:32 +1100
Message-Id: &lt;1454242175-16870-6-git-send-email-matthew.r.wilcox@intel.com&gt;
X-Mailer: git-send-email 2.7.0.rc3
In-Reply-To: &lt;1454242175-16870-1-git-send-email-matthew.r.wilcox@intel.com&gt;
References: &lt;1454242175-16870-1-git-send-email-matthew.r.wilcox@intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a> - Jan. 31, 2016, 12:09 p.m.</div>
<pre class="content">
<span class="from">From: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>

Because there&#39;s no &#39;struct page&#39; for DAX THPs, a lot of this code is
simpler than the PMD code it mimics.  Extra code would need to be added
to support PUDs of anonymous or page-cache THPs.
<span class="signed-off-by">
Signed-off-by: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>
---
 fs/proc/task_mmu.c | 109 +++++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 109 insertions(+)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 3ba3c64..ea20ce4 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -586,6 +586,33 @@</span> <span class="p_context"> static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,</span>
 }
 #endif
 
<span class="p_add">+static int smaps_pud_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="p_add">+	struct mem_size_stats *mss = walk-&gt;private;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_huge_zero_pud(*pud))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	mss-&gt;resident += HPAGE_PUD_SIZE;</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_SHARED) {</span>
<span class="p_add">+		if (pud_dirty(*pud))</span>
<span class="p_add">+			mss-&gt;shared_dirty += HPAGE_PUD_SIZE;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			mss-&gt;shared_clean += HPAGE_PUD_SIZE;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (pud_dirty(*pud))</span>
<span class="p_add">+			mss-&gt;private_dirty += HPAGE_PUD_SIZE;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			mss-&gt;private_clean += HPAGE_PUD_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
 			   struct mm_walk *walk)
 {
<span class="p_chunk">@@ -707,6 +734,7 @@</span> <span class="p_context"> static int show_smap(struct seq_file *m, void *v, int is_pid)</span>
 	struct vm_area_struct *vma = v;
 	struct mem_size_stats mss;
 	struct mm_walk smaps_walk = {
<span class="p_add">+		.pud_entry = smaps_pud_range,</span>
 		.pmd_entry = smaps_pte_range,
 #ifdef CONFIG_HUGETLB_PAGE
 		.hugetlb_entry = smaps_hugetlb_range,
<span class="p_chunk">@@ -889,13 +917,50 @@</span> <span class="p_context"> static inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,</span>
 
 	set_pmd_at(vma-&gt;vm_mm, addr, pmdp, pmd);
 }
<span class="p_add">+static inline void clear_soft_dirty_pud(struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long addr, pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+	pud_t pud = pudp_huge_get_and_clear(vma-&gt;vm_mm, addr, pudp);</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_wrprotect(pud);</span>
<span class="p_add">+	pud = pud_clear_soft_dirty(pud);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="p_add">+		vma-&gt;vm_flags &amp;= ~VM_SOFTDIRTY;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pud_at(vma-&gt;vm_mm, addr, pudp, pud);</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+}</span>
 #else
 static inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,
 		unsigned long addr, pmd_t *pmdp)
 {
 }
<span class="p_add">+static inline void clear_soft_dirty_pud(struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long addr, pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 #endif
 
<span class="p_add">+static int clear_refs_pud_range(pud_t *pud, unsigned long addr,</span>
<span class="p_add">+				unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+	struct clear_refs_private *cp = walk-&gt;private;</span>
<span class="p_add">+	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cp-&gt;type == CLEAR_REFS_SOFT_DIRTY) {</span>
<span class="p_add">+		clear_soft_dirty_pud(vma, addr, pud);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Clear accessed and referenced bits. */</span>
<span class="p_add">+		pudp_test_and_clear_young(vma, addr, pud);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,
 				unsigned long end, struct mm_walk *walk)
 {
<span class="p_chunk">@@ -1006,6 +1071,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 			.type = type,
 		};
 		struct mm_walk clear_refs_walk = {
<span class="p_add">+			.pud_entry = clear_refs_pud_range,</span>
 			.pmd_entry = clear_refs_pte_range,
 			.test_walk = clear_refs_test_walk,
 			.mm = mm,
<span class="p_chunk">@@ -1170,6 +1236,48 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 	return make_pme(frame, flags);
 }
 
<span class="p_add">+static int pagemap_pud_range(pud_t *pudp, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			     struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err = 0;</span>
<span class="p_add">+#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD</span>
<span class="p_add">+	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="p_add">+	struct pagemapread *pm = walk-&gt;private;</span>
<span class="p_add">+	u64 flags = 0, frame = 0;</span>
<span class="p_add">+	pud_t pud = *pudp;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pud_soft_dirty(pud))</span>
<span class="p_add">+		flags |= PM_SOFT_DIRTY;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Currently pud for thp is always present because thp</span>
<span class="p_add">+	 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="p_add">+	 * (split in such cases instead.)</span>
<span class="p_add">+	 * This if-check is just to prepare for future implementation.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pud_present(pud)) {</span>
<span class="p_add">+		flags |= PM_PRESENT;</span>
<span class="p_add">+		if (!(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+			flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pm-&gt;show_pfn)</span>
<span class="p_add">+			frame = pud_pfn(pud) +</span>
<span class="p_add">+					((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+		for (; addr != end; addr += PAGE_SIZE) {</span>
<span class="p_add">+			pagemap_entry_t pme = make_pme(frame, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+			err = add_to_pagemap(addr, &amp;pme, pm);</span>
<span class="p_add">+			if (err)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			if (pm-&gt;show_pfn &amp;&amp; (flags &amp; PM_PRESENT))</span>
<span class="p_add">+				frame++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif /* CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,
 			     struct mm_walk *walk)
 {
<span class="p_chunk">@@ -1349,6 +1457,7 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 	if (!pm.buffer)
 		goto out_mm;
 
<span class="p_add">+	pagemap_walk.pud_entry = pagemap_pud_range;</span>
 	pagemap_walk.pmd_entry = pagemap_pmd_range;
 	pagemap_walk.pte_hole = pagemap_pte_hole;
 #ifdef CONFIG_HUGETLB_PAGE

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



