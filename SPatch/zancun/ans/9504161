
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2] arm64: do not set dma masks that device connection can&#39;t handle - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2] arm64: do not set dma masks that device connection can&#39;t handle</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 9, 2017, 7:30 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1483947002-16410-1-git-send-email-nikita.yoush@cogentembedded.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9504161/mbox/"
   >mbox</a>
|
   <a href="/patch/9504161/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9504161/">/patch/9504161/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DDC0060710 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 07:30:36 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CDCF42582C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 07:30:36 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C28DF2840E; Mon,  9 Jan 2017 07:30:36 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8E32B2582C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 07:30:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1761851AbdAIHaX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 9 Jan 2017 02:30:23 -0500
Received: from mail-lf0-f41.google.com ([209.85.215.41]:34586 &quot;EHLO
	mail-lf0-f41.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1761837AbdAIHaM (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 9 Jan 2017 02:30:12 -0500
Received: by mail-lf0-f41.google.com with SMTP id v186so34706523lfa.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sun, 08 Jan 2017 23:30:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=cogentembedded-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:cc:subject:date:message-id;
	bh=kep6S/hqoBQ7MefqnlsBEu/hrZVMkS7b/2yvJUV00lY=;
	b=IOrhudrw2TMqDMHgBO+Kx6W4mD8zRDsmEe6QE809LeT/AdsPoZlqFRXL6n9KqUYK6u
	wIp0AiaOXpQCAWeoxt/2VnPqjvWyesLDD3iEk0K23r8jTPPw3N0aDmdcxbtnYk+3FUM5
	roi18ObR26AyVMDyVYiWZ/vZSRgnYDn6JwTN6Js8YfFeqdv+ZIO6aeXiRUqGYIfdYrAj
	5SGNTgx01qT/cSxtMq5/lVCRms58H7yBgerdjxV76VLo1I5bgDuDYfJB653YYygDtC1N
	8F2KbXLnqWEDeNH66AmY08GKm+gDidYzhUjjj9/72wDEzFAleSlal+9eZcQzjaVAD7C1
	rudg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id;
	bh=kep6S/hqoBQ7MefqnlsBEu/hrZVMkS7b/2yvJUV00lY=;
	b=LmGWi+Ee0O1AQbZ8igP6aszYSvJuNlb3z+mesU3AuEshjfHOFB7jOdauS6sMj7Oa97
	NqcgphsPBgXle6HtxwSfFz/GCC6B8lL9+TQ76FIJWAK7nQuabmzo3+JWltEPcuztzOt4
	Cklvr++wSdmxnGsuhuRdbaWn/pc0JC9WtIJMkC52Nr+LkZPQYWBDKoQxz8Nv5xbBWT8n
	fCH6oCpvwPhUNHzDYbRAdbXx2S3gdrKSE58BpTGpBrrVuDLw+7k6DxH47aSjbJG531xE
	zmFMoGo4raCy5ejM19B5K8Ic6KbyHXhbzwa75tmaPiNHWHzAbWIZ+IHpEPVkYqohExAv
	ABgg==
X-Gm-Message-State: AIkVDXKa1rHG5oLlAKfEn2jaY8vIWDhoTAX5hZkWsiJF7qJ5fxeT6rX61WuG1+xdDQ9Asw==
X-Received: by 10.25.40.211 with SMTP id o202mr32861007lfo.183.1483947010625;
	Sun, 08 Jan 2017 23:30:10 -0800 (PST)
Received: from hugenb.home (nikaet.starlink.ru. [94.141.168.29])
	by smtp.gmail.com with ESMTPSA id
	d79sm21421937lfd.46.2017.01.08.23.30.08
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Sun, 08 Jan 2017 23:30:09 -0800 (PST)
From: Nikita Yushchenko &lt;nikita.yoush@cogentembedded.com&gt;
To: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: linux-arm-kernel@lists.infradead.org, Will Deacon &lt;will.deacon@arm.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	linux-kernel@vger.kernel.org, linux-renesas-soc@vger.kernel.org,
	Simon Horman &lt;horms@verge.net.au&gt;, Bjorn Helgaas &lt;bhelgaas@google.com&gt;,
	artemi.ivanov@cogentembedded.com,
	Nikita Yushchenko &lt;nikita.yoush@cogentembedded.com&gt;
Subject: [PATCH v2] arm64: do not set dma masks that device connection can&#39;t
	handle
Date: Mon,  9 Jan 2017 10:30:02 +0300
Message-Id: &lt;1483947002-16410-1-git-send-email-nikita.yoush@cogentembedded.com&gt;
X-Mailer: git-send-email 2.1.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 9, 2017, 7:30 a.m.</div>
<pre class="content">
It is possible that device is capable of 64-bit DMA addresses, and
device driver tries to set wide DMA mask, but bridge or bus used to
connect device to the system can&#39;t handle wide addresses.

With swiotlb, memory above 4G still can be used by drivers for streaming
DMA, but *dev-&gt;mask and dev-&gt;dma_coherent_mask must still keep values
that hardware handles physically.

This patch enforces that. Based on original version by
Arnd Bergmann &lt;arnd@arndb.de&gt;, extended with coherent mask hadnling.
<span class="signed-off-by">
Signed-off-by: Nikita Yushchenko &lt;nikita.yoush@cogentembedded.com&gt;</span>
CC: Arnd Bergmann &lt;arnd@arndb.de&gt;
---
Changes since v1:
- fixed issues noted by Sergei Shtylyov &lt;sergei.shtylyov@cogentembedded.com&gt;
  - save mask, not size
  - remove doube empty line

 arch/arm64/Kconfig              |  3 +++
 arch/arm64/include/asm/device.h |  1 +
 arch/arm64/mm/dma-mapping.c     | 51 +++++++++++++++++++++++++++++++++++++++++
 3 files changed, 55 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Jan. 10, 2017, 11:51 a.m.</div>
<pre class="content">
On Mon, Jan 09, 2017 at 10:30:02AM +0300, Nikita Yushchenko wrote:
<span class="quote">&gt; It is possible that device is capable of 64-bit DMA addresses, and</span>
<span class="quote">&gt; device driver tries to set wide DMA mask, but bridge or bus used to</span>
<span class="quote">&gt; connect device to the system can&#39;t handle wide addresses.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With swiotlb, memory above 4G still can be used by drivers for streaming</span>
<span class="quote">&gt; DMA, but *dev-&gt;mask and dev-&gt;dma_coherent_mask must still keep values</span>
<span class="quote">&gt; that hardware handles physically.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch enforces that. Based on original version by</span>
<span class="quote">&gt; Arnd Bergmann &lt;arnd@arndb.de&gt;, extended with coherent mask hadnling.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Nikita Yushchenko &lt;nikita.yoush@cogentembedded.com&gt;</span>
<span class="quote">&gt; CC: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Changes since v1:</span>
<span class="quote">&gt; - fixed issues noted by Sergei Shtylyov &lt;sergei.shtylyov@cogentembedded.com&gt;</span>
<span class="quote">&gt;   - save mask, not size</span>
<span class="quote">&gt;   - remove doube empty line</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  arch/arm64/Kconfig              |  3 +++</span>
<span class="quote">&gt;  arch/arm64/include/asm/device.h |  1 +</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c     | 51 +++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  3 files changed, 55 insertions(+)</span>

I still don&#39;t think this patch is general enough. The problem you&#39;re seeing
with swiotlb seems to be exactly the same problem reported by Feng Kan over
at:

  http://lkml.kernel.org/r/CAL85gmA_SSCwM80TKdkZqEe+S1beWzDEvdki1kpkmUTDRmSP7g@mail.gmail.com

[read on; it was initially thought to be a hardware erratum, but it&#39;s
 actually the inability to restrict the DMA mask of the endpoint that&#39;s
 the problem]

The point here is that an IOMMU doesn&#39;t solve your issue, and the
IOMMU-backed DMA ops need the same treatment. In light of that, it really
feels to me like the DMA masks should be restricted in of_dma_configure
so that the parent mask is taken into account there, rather than hook
into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to
do something to stop dma_set_mask widening the mask if it was restricted
by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 10, 2017, 12:47 p.m.</div>
<pre class="content">
Hi
<span class="quote">
&gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt; into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to</span>
<span class="quote">&gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>

What issue &quot;IOMMU doesn&#39;t solve&quot;?

Issue I&#39;m trying to address is - inconsistency within swiotlb
dma_map_ops, where (1) any wide mask is silently accepted, but (2) then
mask is used to decide if bounce buffers are needed or not. This
inconsistency causes NVMe+R-Car cobmo not working (and breaking memory
instead).

I just can&#39;t think out what similar issue iommu can have.
Do you mean that in iommu case, mask also must not be set to whatever
wider than initial value? Why? What is the use of mask in iommu case? Is
there any real case when iommu can&#39;t address all memory existing in the
system?

NVMe maintainer has just stated that they expect
set_dma_mask(DMA_BIT_MASK(64)) to always succeed, and are going to error
out driver probe if that call fails.  They claim that architecture must
always be able to dma_map() whatever memory existing in the system - via
iommu or swiotlb or whatever. Their direction is to remove bounce
buffers from block and other layers.

With this direction, semantics of dma mask becomes even more
questionable. I&#39;d say dma_mask is candidate for removal (or to move to
swiotlb&#39;s or iommu&#39;s local area)

Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 10, 2017, 1:12 p.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 3:47:25 PM CET Nikita Yushchenko wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt; &gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt; &gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt; &gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt; &gt; into each set of DMA ops to intercept set_dma_mask.</span>

of_dma_configure() sets up a 32-bit mask, which is assumed to always work
in the kernel. We can&#39;t change it to a larger mask because that would
break drivers that have to restrict devices to 32-bit.

If the bus addressing is narrower than 32 bits however, the initial mask
should probably be limited to whatever the bus supports, but that is not
the problem we are trying to solve here.
<span class="quote">
&gt; &gt; We&#39;d still need to</span>
<span class="quote">&gt; &gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt; &gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt; instead).</span>

It&#39;s not just an inconsistency, it&#39;s a known bug that we really
need to fix.
<span class="quote">
&gt; I just can&#39;t think out what similar issue iommu can have.</span>
<span class="quote">&gt; Do you mean that in iommu case, mask also must not be set to whatever</span>
<span class="quote">&gt; wider than initial value? Why? What is the use of mask in iommu case? Is</span>
<span class="quote">&gt; there any real case when iommu can&#39;t address all memory existing in the</span>
<span class="quote">&gt; system?</span>

I think the problem that Will is referring to is when the IOMMU has
a virtual address space that is wider than the DMA mask of the device:
In this case, dma_map_single() might return a dma_addr_t that is not
reachable by the device.

I&#39;d consider that a separate bug that needs to be worked around in
the IOMMU code.
<span class="quote">
&gt; NVMe maintainer has just stated that they expect</span>
<span class="quote">&gt; set_dma_mask(DMA_BIT_MASK(64)) to always succeed, and are going to error</span>
<span class="quote">&gt; out driver probe if that call fails.  They claim that architecture must</span>
<span class="quote">&gt; always be able to dma_map() whatever memory existing in the system - via</span>
<span class="quote">&gt; iommu or swiotlb or whatever. Their direction is to remove bounce</span>
<span class="quote">&gt; buffers from block and other layers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With this direction, semantics of dma mask becomes even more</span>
<span class="quote">&gt; questionable. I&#39;d say dma_mask is candidate for removal (or to move to</span>
<span class="quote">&gt; swiotlb&#39;s or iommu&#39;s local area)</span>

Removing dma_mask is not realistic any time soon, there are too many things
in the kernel that use it for one thing or another, so any changes here
have to be done really carefully. We definitely need the mask to support
architectures without swiotlb.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - Jan. 10, 2017, 1:25 p.m.</div>
<pre class="content">
On 10/01/17 12:47, Nikita Yushchenko wrote:
<span class="quote">&gt; Hi</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt;&gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt;&gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt;&gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt;&gt; into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to</span>
<span class="quote">&gt;&gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt;&gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt; instead).</span>

The fundamental underlying problem is the &quot;any wide mask is silently
accepted&quot; part, and that applies equally to IOMMU ops as well.
<span class="quote">
&gt; I just can&#39;t think out what similar issue iommu can have.</span>
<span class="quote">&gt; Do you mean that in iommu case, mask also must not be set to whatever</span>
<span class="quote">&gt; wider than initial value? Why? What is the use of mask in iommu case? Is</span>
<span class="quote">&gt; there any real case when iommu can&#39;t address all memory existing in the</span>
<span class="quote">&gt; system?</span>

There&#39;s a very subtle misunderstanding there - the DMA mask does not
describe the memory a device can address, it describes the range of
addresses the device is capable of generating. Yes, in the non-IOMMU
case they are equivalent, but once you put an IOMMU in between, the
problem is merely shifted from &quot;what range of physical addresses can
this device access&quot; to &quot;what range of IOVAs is valid to give to this
device&quot; - the fact that those IOVAs can map to any underlying physical
address only obviates the need for any bouncing at the memory end; it
doesn&#39;t remove the fact that the device has a hardware addressing
limitation which needs to be accommodated.

The thread Will linked to describes that equivalent version of your
problem - the IOMMU gives the device 48-bit addresses which get
erroneously truncated because it doesn&#39;t know that only 42 bits are
actually wired up. That situation still requires the device&#39;s DMA mask
to correctly describe its addressing capability just as yours does.
<span class="quote">
&gt; NVMe maintainer has just stated that they expect</span>
<span class="quote">&gt; set_dma_mask(DMA_BIT_MASK(64)) to always succeed, and are going to error</span>
<span class="quote">&gt; out driver probe if that call fails.  They claim that architecture must</span>
<span class="quote">&gt; always be able to dma_map() whatever memory existing in the system - via</span>
<span class="quote">&gt; iommu or swiotlb or whatever. Their direction is to remove bounce</span>
<span class="quote">&gt; buffers from block and other layers.</span>

I have to say I&#39;m in full agreement with that - having subsystems do
their own bouncing is generally redundant, although there are some
edge-cases like MMC_BLOCK_BOUNCE (which is primarily about aligning and
coalescing buffers than working around DMA limitations per se).
<span class="quote">
&gt; With this direction, semantics of dma mask becomes even more</span>
<span class="quote">&gt; questionable. I&#39;d say dma_mask is candidate for removal (or to move to</span>
<span class="quote">&gt; swiotlb&#39;s or iommu&#39;s local area)</span>

We still need a way for drivers to communicate a device&#39;s probed
addressing capability to SWIOTLB, so there&#39;s always going to have to be
*some* sort of public interface. Personally, the change in semantics I&#39;d
like to see is to make dma_set_mask() only fail if DMA is entirely
disallowed - in the normal case it would always succeed, but the DMA API
implementation would be permitted to set a smaller mask than requested
(this is effectively what the x86 IOMMU ops do already). The significant
work that would require, though, is changing all the drivers currently
using this sort of pattern:

	if (!dma_set_mask(dev, DMA_BIT_MASK(64))
		/* put device into 64-bit mode */
	else if (!dma_set_mask(dev, DMA_BIT_MASK(32))
		/* put device into 32-bit mode */
	else
		/* error */

to something like this:

	if (!dma_set_mask(dev, DMA_BIT_MASK(64))
		/* error */
	if (dma_get_mask(dev) &gt; DMA_BIT_MASK(32))
		/* put device into 64-bit mode */
	else
		/* put device into 32-bit mode */

Which would be a pretty major job.

Robin.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 10, 2017, 1:42 p.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 1:25:12 PM CET Robin Murphy wrote:
<span class="quote">&gt; On 10/01/17 12:47, Nikita Yushchenko wrote:</span>
<span class="quote">&gt; &gt;&gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt; &gt;&gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt; &gt;&gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt; &gt;&gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt; &gt;&gt; into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to</span>
<span class="quote">&gt; &gt;&gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt; &gt;&gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt; &gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt; &gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt; &gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt; &gt; instead).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fundamental underlying problem is the &quot;any wide mask is silently</span>
<span class="quote">&gt; accepted&quot; part, and that applies equally to IOMMU ops as well.</span>

It&#39;s a much rarer problem for the IOMMU case though, because it only
impacts devices that are restricted to addressing of less than 32-bits.

If you have an IOMMU enabled, the dma-mapping interface does not care
if the device can do wider than 32 bit addressing, as it will never
hand out IOVAs above 0xffffffff.
<span class="quote">
&gt; &gt; I just can&#39;t think out what similar issue iommu can have.</span>
<span class="quote">&gt; &gt; Do you mean that in iommu case, mask also must not be set to whatever</span>
<span class="quote">&gt; &gt; wider than initial value? Why? What is the use of mask in iommu case? Is</span>
<span class="quote">&gt; &gt; there any real case when iommu can&#39;t address all memory existing in the</span>
<span class="quote">&gt; &gt; system?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There&#39;s a very subtle misunderstanding there - the DMA mask does not</span>
<span class="quote">&gt; describe the memory a device can address, it describes the range of</span>
<span class="quote">&gt; addresses the device is capable of generating. Yes, in the non-IOMMU</span>
<span class="quote">&gt; case they are equivalent, but once you put an IOMMU in between, the</span>
<span class="quote">&gt; problem is merely shifted from &quot;what range of physical addresses can</span>
<span class="quote">&gt; this device access&quot; to &quot;what range of IOVAs is valid to give to this</span>
<span class="quote">&gt; device&quot; - the fact that those IOVAs can map to any underlying physical</span>
<span class="quote">&gt; address only obviates the need for any bouncing at the memory end; it</span>
<span class="quote">&gt; doesn&#39;t remove the fact that the device has a hardware addressing</span>
<span class="quote">&gt; limitation which needs to be accommodated.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The thread Will linked to describes that equivalent version of your</span>
<span class="quote">&gt; problem - the IOMMU gives the device 48-bit addresses which get</span>
<span class="quote">&gt; erroneously truncated because it doesn&#39;t know that only 42 bits are</span>
<span class="quote">&gt; actually wired up. That situation still requires the device&#39;s DMA mask</span>
<span class="quote">&gt; to correctly describe its addressing capability just as yours does.</span>

That problem should only impact virtual machines which have a guest
bus address space covering more than 42 bits of physical RAM, whereas
the problem we have with swiotlb is for the dma-mapping interface.
<span class="quote">
&gt; &gt; With this direction, semantics of dma mask becomes even more</span>
<span class="quote">&gt; &gt; questionable. I&#39;d say dma_mask is candidate for removal (or to move to</span>
<span class="quote">&gt; &gt; swiotlb&#39;s or iommu&#39;s local area)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We still need a way for drivers to communicate a device&#39;s probed</span>
<span class="quote">&gt; addressing capability to SWIOTLB, so there&#39;s always going to have to be</span>
<span class="quote">&gt; *some* sort of public interface. Personally, the change in semantics I&#39;d</span>
<span class="quote">&gt; like to see is to make dma_set_mask() only fail if DMA is entirely</span>
<span class="quote">&gt; disallowed - in the normal case it would always succeed, but the DMA API</span>
<span class="quote">&gt; implementation would be permitted to set a smaller mask than requested</span>
<span class="quote">&gt; (this is effectively what the x86 IOMMU ops do already).</span>

With swiotlb enabled, it only needs to fail if the mask does not contain
the swiotlb bounce buffer area, either because the start of RAM is outside
of the mask, or the bounce area has been allocated at the end of ZONE_DMA
and the mask is smaller than ZONE_DMA.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 10, 2017, 2:01 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt;&gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt;&gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt;&gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt;&gt; instead).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fundamental underlying problem is the &quot;any wide mask is silently</span>
<span class="quote">&gt; accepted&quot; part, and that applies equally to IOMMU ops as well.</span>

Is just posted version better?

It should cover iommu case as well.

Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - Jan. 10, 2017, 2:16 p.m.</div>
<pre class="content">
On 10/01/17 13:42, Arnd Bergmann wrote:
<span class="quote">&gt; On Tuesday, January 10, 2017 1:25:12 PM CET Robin Murphy wrote:</span>
<span class="quote">&gt;&gt; On 10/01/17 12:47, Nikita Yushchenko wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt;&gt;&gt;&gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt;&gt;&gt;&gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt;&gt;&gt;&gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt;&gt;&gt;&gt; into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to</span>
<span class="quote">&gt;&gt;&gt;&gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt;&gt;&gt;&gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt;&gt;&gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt;&gt;&gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt;&gt;&gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt;&gt;&gt; instead).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The fundamental underlying problem is the &quot;any wide mask is silently</span>
<span class="quote">&gt;&gt; accepted&quot; part, and that applies equally to IOMMU ops as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s a much rarer problem for the IOMMU case though, because it only</span>
<span class="quote">&gt; impacts devices that are restricted to addressing of less than 32-bits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you have an IOMMU enabled, the dma-mapping interface does not care</span>
<span class="quote">&gt; if the device can do wider than 32 bit addressing, as it will never</span>
<span class="quote">&gt; hand out IOVAs above 0xffffffff.</span>

I can assure you that it will - we constrain allocations to the
intersection of the IOMMU domain aperture (normally the IOMMU&#39;s physical
input address width) and the given device&#39;s DMA mask. If both of those
are &gt;32 bits then &gt;32-bit IOVAs will fall out. For the arm64/common
implementation I have prototyped a copy of the x86 optimisation which
always first tries to get 32-bit IOVAs for PCI devices, but even then it
can start returning higher addresses if the 32-bit space fills up.
<span class="quote">
&gt;&gt;&gt; I just can&#39;t think out what similar issue iommu can have.</span>
<span class="quote">&gt;&gt;&gt; Do you mean that in iommu case, mask also must not be set to whatever</span>
<span class="quote">&gt;&gt;&gt; wider than initial value? Why? What is the use of mask in iommu case? Is</span>
<span class="quote">&gt;&gt;&gt; there any real case when iommu can&#39;t address all memory existing in the</span>
<span class="quote">&gt;&gt;&gt; system?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; There&#39;s a very subtle misunderstanding there - the DMA mask does not</span>
<span class="quote">&gt;&gt; describe the memory a device can address, it describes the range of</span>
<span class="quote">&gt;&gt; addresses the device is capable of generating. Yes, in the non-IOMMU</span>
<span class="quote">&gt;&gt; case they are equivalent, but once you put an IOMMU in between, the</span>
<span class="quote">&gt;&gt; problem is merely shifted from &quot;what range of physical addresses can</span>
<span class="quote">&gt;&gt; this device access&quot; to &quot;what range of IOVAs is valid to give to this</span>
<span class="quote">&gt;&gt; device&quot; - the fact that those IOVAs can map to any underlying physical</span>
<span class="quote">&gt;&gt; address only obviates the need for any bouncing at the memory end; it</span>
<span class="quote">&gt;&gt; doesn&#39;t remove the fact that the device has a hardware addressing</span>
<span class="quote">&gt;&gt; limitation which needs to be accommodated.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The thread Will linked to describes that equivalent version of your</span>
<span class="quote">&gt;&gt; problem - the IOMMU gives the device 48-bit addresses which get</span>
<span class="quote">&gt;&gt; erroneously truncated because it doesn&#39;t know that only 42 bits are</span>
<span class="quote">&gt;&gt; actually wired up. That situation still requires the device&#39;s DMA mask</span>
<span class="quote">&gt;&gt; to correctly describe its addressing capability just as yours does.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That problem should only impact virtual machines which have a guest</span>
<span class="quote">&gt; bus address space covering more than 42 bits of physical RAM, whereas</span>
<span class="quote">&gt; the problem we have with swiotlb is for the dma-mapping interface.</span>

As above, it impacts DMA API use for anything whose addressing
capability is narrower than the IOMMU&#39;s reported input size and whose
driver is able to blindly set a too-big DMA mask. It just happens to be
the case that the stars line up on most systems, and for 32-bit devices
who keep the default DMA mask.

I actually have a third variation of this problem involving a PCI root
complex which *could* drive full-width (40-bit) addresses, but won&#39;t,
due to the way its PCI&lt;-&gt;AXI interface is programmed. That would require
even more complicated dma-ranges handling to describe the windows of
valid physical addresses which it *will* pass, so I&#39;m not pressing the
issue - let&#39;s just get the basic DMA mask case fixed first.
<span class="quote">
&gt;&gt;&gt; With this direction, semantics of dma mask becomes even more</span>
<span class="quote">&gt;&gt;&gt; questionable. I&#39;d say dma_mask is candidate for removal (or to move to</span>
<span class="quote">&gt;&gt;&gt; swiotlb&#39;s or iommu&#39;s local area)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; We still need a way for drivers to communicate a device&#39;s probed</span>
<span class="quote">&gt;&gt; addressing capability to SWIOTLB, so there&#39;s always going to have to be</span>
<span class="quote">&gt;&gt; *some* sort of public interface. Personally, the change in semantics I&#39;d</span>
<span class="quote">&gt;&gt; like to see is to make dma_set_mask() only fail if DMA is entirely</span>
<span class="quote">&gt;&gt; disallowed - in the normal case it would always succeed, but the DMA API</span>
<span class="quote">&gt;&gt; implementation would be permitted to set a smaller mask than requested</span>
<span class="quote">&gt;&gt; (this is effectively what the x86 IOMMU ops do already).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With swiotlb enabled, it only needs to fail if the mask does not contain</span>
<span class="quote">&gt; the swiotlb bounce buffer area, either because the start of RAM is outside</span>
<span class="quote">&gt; of the mask, or the bounce area has been allocated at the end of ZONE_DMA</span>
<span class="quote">&gt; and the mask is smaller than ZONE_DMA.</span>

Agreed, I&#39;d managed to overlook that specific case, but I&#39;d be inclined
to consider &quot;impossible&quot; a subset of &quot;disallowed&quot; still :)

Robin.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 2:51 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 03:47:25PM +0300, Nikita Yushchenko wrote:
<span class="quote">&gt; With this direction, semantics of dma mask becomes even more</span>
<span class="quote">&gt; questionable. I&#39;d say dma_mask is candidate for removal (or to move to</span>
<span class="quote">&gt; swiotlb&#39;s or iommu&#39;s local area)</span>

We need the dma mask so that the device can advertise what addresses
the device supports.  Many old devices only support 32-bit DMA addressing,
and some less common ones just 24-bit or other weird ones.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 2:57 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 01:25:12PM +0000, Robin Murphy wrote:
<span class="quote">&gt; We still need a way for drivers to communicate a device&#39;s probed</span>
<span class="quote">&gt; addressing capability to SWIOTLB, so there&#39;s always going to have to be</span>
<span class="quote">&gt; *some* sort of public interface. Personally, the change in semantics I&#39;d</span>
<span class="quote">&gt; like to see is to make dma_set_mask() only fail if DMA is entirely</span>
<span class="quote">&gt; disallowed - in the normal case it would always succeed, but the DMA API</span>
<span class="quote">&gt; implementation would be permitted to set a smaller mask than requested</span>
<span class="quote">&gt; (this is effectively what the x86 IOMMU ops do already).</span>

Yes, this sounds reasonable.
<span class="quote">
&gt; The significant</span>
<span class="quote">&gt; work that would require, though, is changing all the drivers currently</span>
<span class="quote">&gt; using this sort of pattern:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (!dma_set_mask(dev, DMA_BIT_MASK(64))</span>
<span class="quote">&gt; 		/* put device into 64-bit mode */</span>
<span class="quote">&gt; 	else if (!dma_set_mask(dev, DMA_BIT_MASK(32))</span>
<span class="quote">&gt; 		/* put device into 32-bit mode */</span>
<span class="quote">&gt; 	else</span>
<span class="quote">&gt; 		/* error */</span>

While we have this pattern in a lot of places it&#39;s already rather
pointless on most architectures as the first dma_set_mask call
won&#39;t ever fail for the most common dma_ops implementations.
<span class="quote">
&gt; to something like this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (!dma_set_mask(dev, DMA_BIT_MASK(64))</span>
<span class="quote">&gt; 		/* error */</span>
<span class="quote">&gt; 	if (dma_get_mask(dev) &gt; DMA_BIT_MASK(32))</span>
<span class="quote">&gt; 		/* put device into 64-bit mode */</span>
<span class="quote">&gt; 	else</span>
<span class="quote">&gt; 		/* put device into 32-bit mode */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which would be a pretty major job.</span>

I don&#39;t think it&#39;s too bad.  Also for many modern devices there is no
need to put the device into a specific mode.  It&#39;s mostly a historic
issue from the PCI/PCI-X days with the less efficient DAC addressing
scheme.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 2:59 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 02:42:23PM +0100, Arnd Bergmann wrote:
<span class="quote">&gt; It&#39;s a much rarer problem for the IOMMU case though, because it only</span>
<span class="quote">&gt; impacts devices that are restricted to addressing of less than 32-bits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you have an IOMMU enabled, the dma-mapping interface does not care</span>
<span class="quote">&gt; if the device can do wider than 32 bit addressing, as it will never</span>
<span class="quote">&gt; hand out IOVAs above 0xffffffff.</span>

That&#39;s absolutely not the case.  IOMMUs can and do generate addresses
larger than 32-bit.  Also various platforms have modes where an IOMMU
can be used when &lt;= 32-bit addresses are used and bypassed if full 64-bit
addressing is supported and I/O isolation is not explicitly requested.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 10, 2017, 3:06 p.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 2:16:57 PM CET Robin Murphy wrote:
<span class="quote">&gt; On 10/01/17 13:42, Arnd Bergmann wrote:</span>
<span class="quote">&gt; &gt; On Tuesday, January 10, 2017 1:25:12 PM CET Robin Murphy wrote:</span>
<span class="quote">&gt; &gt;&gt; On 10/01/17 12:47, Nikita Yushchenko wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; The point here is that an IOMMU doesn&#39;t solve your issue, and the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; IOMMU-backed DMA ops need the same treatment. In light of that, it really</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; feels to me like the DMA masks should be restricted in of_dma_configure</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; so that the parent mask is taken into account there, rather than hook</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; into each set of DMA ops to intercept set_dma_mask. We&#39;d still need to</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; do something to stop dma_set_mask widening the mask if it was restricted</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; by of_dma_configure, but I think Robin (cc&#39;d) was playing with that.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; What issue &quot;IOMMU doesn&#39;t solve&quot;?</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Issue I&#39;m trying to address is - inconsistency within swiotlb</span>
<span class="quote">&gt; &gt;&gt;&gt; dma_map_ops, where (1) any wide mask is silently accepted, but (2) then</span>
<span class="quote">&gt; &gt;&gt;&gt; mask is used to decide if bounce buffers are needed or not. This</span>
<span class="quote">&gt; &gt;&gt;&gt; inconsistency causes NVMe+R-Car cobmo not working (and breaking memory</span>
<span class="quote">&gt; &gt;&gt;&gt; instead).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The fundamental underlying problem is the &quot;any wide mask is silently</span>
<span class="quote">&gt; &gt;&gt; accepted&quot; part, and that applies equally to IOMMU ops as well.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s a much rarer problem for the IOMMU case though, because it only</span>
<span class="quote">&gt; &gt; impacts devices that are restricted to addressing of less than 32-bits.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If you have an IOMMU enabled, the dma-mapping interface does not care</span>
<span class="quote">&gt; &gt; if the device can do wider than 32 bit addressing, as it will never</span>
<span class="quote">&gt; &gt; hand out IOVAs above 0xffffffff.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can assure you that it will - we constrain allocations to the</span>
<span class="quote">&gt; intersection of the IOMMU domain aperture (normally the IOMMU&#39;s physical</span>
<span class="quote">&gt; input address width) and the given device&#39;s DMA mask. If both of those</span>
<span class="quote">&gt; are &gt;32 bits then &gt;32-bit IOVAs will fall out. For the arm64/common</span>
<span class="quote">&gt; implementation I have prototyped a copy of the x86 optimisation which</span>
<span class="quote">&gt; always first tries to get 32-bit IOVAs for PCI devices, but even then it</span>
<span class="quote">&gt; can start returning higher addresses if the 32-bit space fills up.</span>

Ok, got it. I have to admit that most of my knowledge about the internals
of IOMMUs is from PowerPC of a few years ago, which couldn&#39;t do this at
all. I agree that we need to do the same thing on swiotlb and iommu then.
<span class="quote">
&gt; &gt;&gt; The thread Will linked to describes that equivalent version of your</span>
<span class="quote">&gt; &gt;&gt; problem - the IOMMU gives the device 48-bit addresses which get</span>
<span class="quote">&gt; &gt;&gt; erroneously truncated because it doesn&#39;t know that only 42 bits are</span>
<span class="quote">&gt; &gt;&gt; actually wired up. That situation still requires the device&#39;s DMA mask</span>
<span class="quote">&gt; &gt;&gt; to correctly describe its addressing capability just as yours does.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That problem should only impact virtual machines which have a guest</span>
<span class="quote">&gt; &gt; bus address space covering more than 42 bits of physical RAM, whereas</span>
<span class="quote">&gt; &gt; the problem we have with swiotlb is for the dma-mapping interface.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; I actually have a third variation of this problem involving a PCI root</span>
<span class="quote">&gt; complex which *could* drive full-width (40-bit) addresses, but won&#39;t,</span>
<span class="quote">&gt; due to the way its PCI&lt;-&gt;AXI interface is programmed. That would require</span>
<span class="quote">&gt; even more complicated dma-ranges handling to describe the windows of</span>
<span class="quote">&gt; valid physical addresses which it *will* pass, so I&#39;m not pressing the</span>
<span class="quote">&gt; issue - let&#39;s just get the basic DMA mask case fixed first.</span>

Can you describe this a little more? We should at least try to not
make it harder to solve the next problem while solving this one,
so I&#39;d like to understand the exact limitation you are hitting there.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 11, 2017, 12:37 p.m.</div>
<pre class="content">
<span class="quote">&gt; I actually have a third variation of this problem involving a PCI root</span>
<span class="quote">&gt; complex which *could* drive full-width (40-bit) addresses, but won&#39;t,</span>
<span class="quote">&gt; due to the way its PCI&lt;-&gt;AXI interface is programmed. That would require</span>
<span class="quote">&gt; even more complicated dma-ranges handling to describe the windows of</span>
<span class="quote">&gt; valid physical addresses which it *will* pass, so I&#39;m not pressing the</span>
<span class="quote">&gt; issue - let&#39;s just get the basic DMA mask case fixed first.</span>

R-Car + NVMe is actually not &quot;basic case&quot;.

It has PCI&lt;-&gt;AXI interface involved.
PCI addresses are 64-bit and controller does handle 64-bit addresses
there. Mapping between PCI addresses and AXI addresses is defined. But
AXI is 32-bit.

SoC has iommu that probably could be used between PCIe module and RAM.
Although AFAIK nobody made that working yet.

Board I work with has 4G of RAM, in 4 banks, located at different parts
of wide address space, and only one of them is below 4G. But if iommu is
capable of translating addresses such that 4 gigabyte banks map to first
4 gigabytes of address space, then all memory will become available for
DMA from PCIe device.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 11, 2017, 4:21 p.m.</div>
<pre class="content">
On Wednesday, January 11, 2017 3:37:22 PM CET Nikita Yushchenko wrote:
<span class="quote">&gt; &gt; I actually have a third variation of this problem involving a PCI root</span>
<span class="quote">&gt; &gt; complex which *could* drive full-width (40-bit) addresses, but won&#39;t,</span>
<span class="quote">&gt; &gt; due to the way its PCI&lt;-&gt;AXI interface is programmed. That would require</span>
<span class="quote">&gt; &gt; even more complicated dma-ranges handling to describe the windows of</span>
<span class="quote">&gt; &gt; valid physical addresses which it *will* pass, so I&#39;m not pressing the</span>
<span class="quote">&gt; &gt; issue - let&#39;s just get the basic DMA mask case fixed first.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; R-Car + NVMe is actually not &quot;basic case&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It has PCI&lt;-&gt;AXI interface involved.</span>
<span class="quote">&gt; PCI addresses are 64-bit and controller does handle 64-bit addresses</span>
<span class="quote">&gt; there. Mapping between PCI addresses and AXI addresses is defined. But</span>
<span class="quote">&gt; AXI is 32-bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; SoC has iommu that probably could be used between PCIe module and RAM.</span>
<span class="quote">&gt; Although AFAIK nobody made that working yet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Board I work with has 4G of RAM, in 4 banks, located at different parts</span>
<span class="quote">&gt; of wide address space, and only one of them is below 4G. But if iommu is</span>
<span class="quote">&gt; capable of translating addresses such that 4 gigabyte banks map to first</span>
<span class="quote">&gt; 4 gigabytes of address space, then all memory will become available for</span>
<span class="quote">&gt; DMA from PCIe device.</span>

You can in theory handle this by defining your own platform specific
dma_map_ops, as we used to do in the old days. Unfortunately, the modern
way of using the generic IOVA allocation can&#39;t handle really it, so it&#39;s
unclear if the work that would be necessary to support it (and the long
term maintenance cost) outweigh the benefits.

The more likely option here is to try harder to get the IOMMU working
(or show that it&#39;s impossible but make sure the next chip gets it right).

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - Jan. 11, 2017, 6:28 p.m.</div>
<pre class="content">
On 11/01/17 12:37, Nikita Yushchenko wrote:
<span class="quote">&gt;&gt; I actually have a third variation of this problem involving a PCI root</span>
<span class="quote">&gt;&gt; complex which *could* drive full-width (40-bit) addresses, but won&#39;t,</span>
<span class="quote">&gt;&gt; due to the way its PCI&lt;-&gt;AXI interface is programmed. That would require</span>
<span class="quote">&gt;&gt; even more complicated dma-ranges handling to describe the windows of</span>
<span class="quote">&gt;&gt; valid physical addresses which it *will* pass, so I&#39;m not pressing the</span>
<span class="quote">&gt;&gt; issue - let&#39;s just get the basic DMA mask case fixed first.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; R-Car + NVMe is actually not &quot;basic case&quot;.</span>

I meant &quot;basic&quot; in terms of what needs to be done in Linux - simply
preventing device drivers from overwriting the DT-configured DMA mask
will make everything work as well as well as it possibly can on R-Car,
both with or without the IOMMU, since apparently all you need is to
ensure a PCI device never gets given a DMA address above 4GB. The
situation where PCI devices *can* DMA to all of physical memory, but
can&#39;t use arbitrary addresses *outside* it - which only becomes a
problem with an IOMMU - is considerably trickier.
<span class="quote">
&gt; It has PCI&lt;-&gt;AXI interface involved.</span>
<span class="quote">&gt; PCI addresses are 64-bit and controller does handle 64-bit addresses</span>
<span class="quote">&gt; there. Mapping between PCI addresses and AXI addresses is defined. But</span>
<span class="quote">&gt; AXI is 32-bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; SoC has iommu that probably could be used between PCIe module and RAM.</span>
<span class="quote">&gt; Although AFAIK nobody made that working yet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Board I work with has 4G of RAM, in 4 banks, located at different parts</span>
<span class="quote">&gt; of wide address space, and only one of them is below 4G. But if iommu is</span>
<span class="quote">&gt; capable of translating addresses such that 4 gigabyte banks map to first</span>
<span class="quote">&gt; 4 gigabytes of address space, then all memory will become available for</span>
<span class="quote">&gt; DMA from PCIe device.</span>

The aforementioned situation on Juno is similar yet different - the PLDA
XR3 root complex uses an address-based lookup table to translate
outgoing PCI memory space transactions to AXI bus addresses with the
appropriate attributes, in power-of-two-sized regions. The firmware
configures 3 LUT entries - 2GB at 0x8000_0000 and 8GB at 0x8_8000_0000
with cache-coherent attributes to cover the DRAM areas, plus a small one
with device attributes covering the GICv2m MSI frame. The issue is that
there is no &quot;no match&quot; translation, so any transaction not within one of
those regions never makes it out of the root complex at all.

That&#39;s fine in normal operation, as there&#39;s nothing outside those
regions in the physical memory map a PCI device should be accessing
anyway, but turning on the SMMU is another matter - since the IOVA
allocator runs top-down, a PCI device with a 64-bit DMA mask will do a
dma_map or dma_alloc, get the physical page mapped to an IOVA up around
FF_FFFF_F000 (the SMMU will constrain things to the system bus width of
40 bits), then try to access that address and get a termination straight
back from the RC. Similarly, A KVM guest which wants to place its memory
at arbitrary locations and expect device passthrough to work is going to
have a bad time.

I don&#39;t know if it&#39;s feasible to have the firmware set the LUT up
differently, as that might lead to other problems when not using the
SMMU, and/or just require far more than the 8 available LUT entries
(assuming they have to be non-overlapping - I&#39;m not 100% sure and
documentation is sparse). Thus it seems appropriate to describe the
currently valid PCI-AXI translations with dma-ranges, but then we&#39;d have
multiple entries - last time I looked Linux simply ignores all but the
last one in that case - which can&#39;t be combined into a simple bitmask,
so I&#39;m not entirely sure where to go from there. Especially as so far it
seems to be a problem exclusive to one not-widely-available ageing
early-access development platform...

It happens that limiting all PCI DMA masks to 32 bits would bodge around
this problem thanks to the current IOVA allocator behaviour, but that&#39;s
pretty yuck, and would force unnecessary bouncing for the non-SMMU case.
My other hack to carve up IOVA domains to reserve all addresses not
matching memblocks is hardly any more realistic, hence why the SMMU is
in the Juno DT in a change-it-at-your-own-peril &quot;disabled&quot; state ;)

Robin.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index 1117421..afb2c08 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -216,6 +216,9 @@</span> <span class="p_context"> config NEED_DMA_MAP_STATE</span>
 config NEED_SG_DMA_LENGTH
 	def_bool y
 
<span class="p_add">+config ARCH_HAS_DMA_SET_COHERENT_MASK</span>
<span class="p_add">+	def_bool y</span>
<span class="p_add">+</span>
 config SMP
 	def_bool y
 
<span class="p_header">diff --git a/arch/arm64/include/asm/device.h b/arch/arm64/include/asm/device.h</span>
<span class="p_header">index 243ef25..a57e7bb 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/device.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/device.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"> struct dev_archdata {</span>
 	void *iommu;			/* private IOMMU data */
 #endif
 	bool dma_coherent;
<span class="p_add">+	u64 parent_dma_mask;</span>
 };
 
 struct pdev_archdata {
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index e040827..5ab15ce 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -352,6 +352,30 @@</span> <span class="p_context"> static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
 	return 1;
 }
 
<span class="p_add">+static int __swiotlb_set_dma_mask(struct device *dev, u64 mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* device is not DMA capable */</span>
<span class="p_add">+	if (!dev-&gt;dma_mask)</span>
<span class="p_add">+		return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* mask is below swiotlb bounce buffer, so fail */</span>
<span class="p_add">+	if (!swiotlb_dma_supported(dev, mask))</span>
<span class="p_add">+		return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * because of the swiotlb, we can return success for</span>
<span class="p_add">+	 * larger masks, but need to ensure that bounce buffers</span>
<span class="p_add">+	 * are used above parent_dma_mask, so set that as</span>
<span class="p_add">+	 * the effective mask.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mask &gt; dev-&gt;archdata.parent_dma_mask)</span>
<span class="p_add">+		mask = dev-&gt;archdata.parent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	*dev-&gt;dma_mask = mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct dma_map_ops swiotlb_dma_ops = {
 	.alloc = __dma_alloc,
 	.free = __dma_free,
<span class="p_chunk">@@ -367,8 +391,23 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
 	.dma_supported = __swiotlb_dma_supported,
 	.mapping_error = swiotlb_dma_mapping_error,
<span class="p_add">+	.set_dma_mask = __swiotlb_set_dma_mask,</span>
 };
 
<span class="p_add">+int dma_set_coherent_mask(struct device *dev, u64 mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_supported(dev, mask))</span>
<span class="p_add">+		return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (get_dma_ops(dev) == &amp;swiotlb_dma_ops &amp;&amp;</span>
<span class="p_add">+	    mask &gt; dev-&gt;archdata.parent_dma_mask)</span>
<span class="p_add">+		mask = dev-&gt;archdata.parent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	dev-&gt;coherent_dma_mask = mask;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dma_set_coherent_mask);</span>
<span class="p_add">+</span>
 static int __init atomic_pool_init(void)
 {
 	pgprot_t prot = __pgprot(PROT_NORMAL_NC);
<span class="p_chunk">@@ -958,6 +997,18 @@</span> <span class="p_context"> void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
 	if (!dev-&gt;archdata.dma_ops)
 		dev-&gt;archdata.dma_ops = &amp;swiotlb_dma_ops;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * we don&#39;t yet support buses that have a non-zero mapping.</span>
<span class="p_add">+	 *  Let&#39;s hope we won&#39;t need it</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(dma_base != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Whatever the parent bus can set. A device must not set</span>
<span class="p_add">+	 * a DMA mask larger than this.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	dev-&gt;archdata.parent_dma_mask = size - 1;</span>
<span class="p_add">+</span>
 	dev-&gt;archdata.dma_coherent = coherent;
 	__iommu_setup_dma_ops(dev, dma_base, size, iommu);
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



