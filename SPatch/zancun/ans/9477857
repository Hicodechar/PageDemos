
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>crash during oom reaper - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    crash during oom reaper</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 16, 2016, 12:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161216123555.GE27758@node&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9477857/mbox/"
   >mbox</a>
|
   <a href="/patch/9477857/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9477857/">/patch/9477857/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8EDC8601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 12:46:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8001828810
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 12:46:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7373428827; Fri, 16 Dec 2016 12:46:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8A03028810
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 12:46:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1759552AbcLPMqW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 16 Dec 2016 07:46:22 -0500
Received: from mail-wm0-f65.google.com ([74.125.82.65]:35748 &quot;EHLO
	mail-wm0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1757521AbcLPMqM (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 16 Dec 2016 07:46:12 -0500
Received: by mail-wm0-f65.google.com with SMTP id a20so5264917wme.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 16 Dec 2016 04:46:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=shutemov-name.20150623.gappssmtp.com; s=20150623;
	h=date:from:to:cc:subject:message-id:references:mime-version
	:content-disposition:in-reply-to:user-agent;
	bh=4TT5K6qbVvL/owSvwu4LBr7eM5yrxFUJ6IlJZvIsRfw=;
	b=YKB5KJkJ0XUIfuC5dOX3dKAEaDuD2Fg0BERpJ+4jmw1vSdn+GR1b0oGVAOtg1R9sQn
	d01578oiVxLY6aiGAnLpppBs/+OdRqHl17uYFhDMy2s0O72FTC3BRTdjHt1OcZVM5lqX
	RxCoU2nCWYRjSZweqB/Xp8tguWOTNhFPla927o9qSU7ZAsyRkAgHrcsKKp44E0GFPCBn
	rGlRfWK4iHyKhLCN8Npz/ObJhLRyTKRwhiqi0x3NJagwcQ0uHdecB0r0C/N+ms60JSlW
	CJPPI2KrYv0Pmbz65vaIaQadOaugobJHENXn1Nn0KQvC9gW352nPWaVjT0Y5PlwzCYFM
	oujA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:date:from:to:cc:subject:message-id:references
	:mime-version:content-disposition:in-reply-to:user-agent;
	bh=4TT5K6qbVvL/owSvwu4LBr7eM5yrxFUJ6IlJZvIsRfw=;
	b=PybbEz2RmbNVHj72iZhiUrtWVdC3kbwjVIPFrzIcJj2EK5jsfYzZl/Nfs6Ln8uuYTl
	zEpt9awner1TRIOovCk4Q67LP89GNBMTyJWeKl6CM1iV0kjspnyNZRiNPO+CTIe0LMYo
	aALRUPxW/F1GSV5N4TW6XSkZv6NEycG1csk7Fv9z4BZeJrtkVCqI0Xj/rpTvbp85g5ET
	YPXpasfiNfWF5qb4mJjburewzG8Md6zjjaun3Ocdd2I6xX65c/94bAJ2NKf6pOUlf9pb
	cTKTfDtCy7I2jpUsLMvS7+VVVPJLWqCSwZlF+ATE42RPAm9el3njnzMJLuwQeaRwNvQa
	+ccA==
X-Gm-Message-State: AIkVDXJiLNAbuvLygaCeB/HnyjStYNA8haICrjAlG3vuZz1OddoyAzuuNu2DIAlezjKMsw==
X-Received: by 10.28.67.69 with SMTP id q66mr2525231wma.22.1481891757030;
	Fri, 16 Dec 2016 04:35:57 -0800 (PST)
Received: from node.shutemov.name ([93.85.31.201])
	by smtp.gmail.com with ESMTPSA id
	q7sm6622874wjh.9.2016.12.16.04.35.56
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Fri, 16 Dec 2016 04:35:56 -0800 (PST)
Received: by node.shutemov.name (Postfix, from userid 1000)
	id 42739648C1A0; Fri, 16 Dec 2016 15:35:55 +0300 (+03)
Date: Fri, 16 Dec 2016 15:35:55 +0300
From: &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;
To: Michal Hocko &lt;mhocko@kernel.org&gt;
Cc: Vegard Nossum &lt;vegard.nossum@oracle.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, Rik van Riel &lt;riel@redhat.com&gt;,
	Matthew Wilcox &lt;mawilcox@microsoft.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Al Viro &lt;viro@zeniv.linux.org.uk&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Subject: Re: crash during oom reaper
Message-ID: &lt;20161216123555.GE27758@node&gt;
References: &lt;20161216082202.21044-1-vegard.nossum@oracle.com&gt;
	&lt;20161216082202.21044-4-vegard.nossum@oracle.com&gt;
	&lt;20161216090157.GA13940@dhcp22.suse.cz&gt;
	&lt;d944e3ca-07d4-c7d6-5025-dc101406b3a7@oracle.com&gt;
	&lt;20161216101113.GE13940@dhcp22.suse.cz&gt;
	&lt;20161216104438.GD27758@node&gt;
	&lt;20161216114243.GG13940@dhcp22.suse.cz&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20161216114243.GG13940@dhcp22.suse.cz&gt;
User-Agent: Mutt/1.5.23.1 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Dec. 16, 2016, 12:35 p.m.</div>
<pre class="content">
On Fri, Dec 16, 2016 at 12:42:43PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 16-12-16 13:44:38, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Fri, Dec 16, 2016 at 11:11:13AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri 16-12-16 10:43:52, Vegard Nossum wrote:</span>
<span class="quote">&gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; I don&#39;t think it&#39;s a bug in the OOM reaper itself, but either of the</span>
<span class="quote">&gt; &gt; &gt; &gt; following two patches will fix the problem (without my understand how or</span>
<span class="quote">&gt; &gt; &gt; &gt; why):</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; index ec9f11d4f094..37b14b2e2af4 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -485,7 +485,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk,</span>
<span class="quote">&gt; &gt; &gt; &gt; struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; &gt;  	mutex_lock(&amp;oom_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; -	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; __oom_reap_task_mm is basically the same thing as MADV_DONTNEED and that</span>
<span class="quote">&gt; &gt; &gt; doesn&#39;t require the exlusive mmap_sem. So this looks correct to me.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; BTW, shouldn&#39;t we filter out all VM_SPECIAL VMAs there? Or VM_PFNMAP at</span>
<span class="quote">&gt; &gt; least.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; MADV_DONTNEED doesn&#39;t touch VM_PFNMAP, but I don&#39;t see anything matching</span>
<span class="quote">&gt; &gt; on __oom_reap_task_mm() side.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess you are right and we should match the MADV_DONTNEED behavior</span>
<span class="quote">&gt; here. Care to send a patch?</span>

Below. Testing required.
<span class="quote">
&gt; &gt; Other difference is that you use unmap_page_range() witch doesn&#39;t touch</span>
<span class="quote">&gt; &gt; mmu_notifiers. MADV_DONTNEED goes via zap_page_range(), which invalidates</span>
<span class="quote">&gt; &gt; the range. Not sure if it can make any difference here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which mmu notifier would care about this? I am not really familiar with</span>
<span class="quote">&gt; those users so I might miss something easily.</span>

No idea either.

Is there any reason not to use zap_page_range here too?

Few more notes:

I propably miss something, but why do we need details-&gt;ignore_dirty?
It only appiled for non-anon pages, but since we filter out shared
mappings, how can we have pte_dirty() for !PageAnon()?

check_swap_entries is also sloppy: the behavior doesn&#39;t match the comment:
details == NULL makes it check swap entries. I removed it and restore
details-&gt;check_mapping test as we had before.

After the change no user of zap_page_range() wants non-NULL details, I&#39;ve
dropped the argument.

If it looks okay, I&#39;ll split it into several patches with proper commit
messages.

-----8&lt;-----
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Dec. 16, 2016, 12:56 p.m.</div>
<pre class="content">
On Fri 16-12-16 15:35:55, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Fri, Dec 16, 2016 at 12:42:43PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Fri 16-12-16 13:44:38, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri, Dec 16, 2016 at 11:11:13AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Fri 16-12-16 10:43:52, Vegard Nossum wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; I don&#39;t think it&#39;s a bug in the OOM reaper itself, but either of the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; following two patches will fix the problem (without my understand how or</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; why):</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index ec9f11d4f094..37b14b2e2af4 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -485,7 +485,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	mutex_lock(&amp;oom_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; __oom_reap_task_mm is basically the same thing as MADV_DONTNEED and that</span>
<span class="quote">&gt; &gt; &gt; &gt; doesn&#39;t require the exlusive mmap_sem. So this looks correct to me.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; BTW, shouldn&#39;t we filter out all VM_SPECIAL VMAs there? Or VM_PFNMAP at</span>
<span class="quote">&gt; &gt; &gt; least.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; MADV_DONTNEED doesn&#39;t touch VM_PFNMAP, but I don&#39;t see anything matching</span>
<span class="quote">&gt; &gt; &gt; on __oom_reap_task_mm() side.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I guess you are right and we should match the MADV_DONTNEED behavior</span>
<span class="quote">&gt; &gt; here. Care to send a patch?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Below. Testing required.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; Other difference is that you use unmap_page_range() witch doesn&#39;t touch</span>
<span class="quote">&gt; &gt; &gt; mmu_notifiers. MADV_DONTNEED goes via zap_page_range(), which invalidates</span>
<span class="quote">&gt; &gt; &gt; the range. Not sure if it can make any difference here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Which mmu notifier would care about this? I am not really familiar with</span>
<span class="quote">&gt; &gt; those users so I might miss something easily.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No idea either.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any reason not to use zap_page_range here too?</span>

Yes, zap_page_range is much more heavy and performs operations which
might lock AFAIR which I really would like to prevent from.
<span class="quote"> 
&gt; Few more notes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I propably miss something, but why do we need details-&gt;ignore_dirty?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It only appiled for non-anon pages, but since we filter out shared</span>
<span class="quote">&gt; mappings, how can we have pte_dirty() for !PageAnon()?</span>

Why couldn&#39;t we have dirty pages on the private file mappings? The
underlying page might be still in the page cache, right?
<span class="quote">
&gt; check_swap_entries is also sloppy: the behavior doesn&#39;t match the comment:</span>
<span class="quote">&gt; details == NULL makes it check swap entries. I removed it and restore</span>
<span class="quote">&gt; details-&gt;check_mapping test as we had before.</span>

the reason is unmap_mapping_range which didn&#39;t use to check swap entries
so I wanted to have it opt in AFAIR.
<span class="quote">
&gt; @@ -531,8 +519,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  		 * count elevated without a good reason.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; -			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt; -					 &amp;details);</span>
<span class="quote">&gt; +			madvise_dontneed(vma, &amp;vma, vma-&gt;vm_start, vma-&gt;vm_end);</span>

I would rather keep the unmap_page_range because it is the bare minumum
we have to do. Currently we are doing 

		if (is_vm_hugetlb_page(vma))
			continue;

so I would rather do something like
		if (!can_vma_madv_dontneed(vma))
			continue;
instead.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Dec. 16, 2016, 1:07 p.m.</div>
<pre class="content">
On Fri, Dec 16, 2016 at 01:56:50PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 16-12-16 15:35:55, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Fri, Dec 16, 2016 at 12:42:43PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri 16-12-16 13:44:38, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Fri, Dec 16, 2016 at 11:11:13AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; On Fri 16-12-16 10:43:52, Vegard Nossum wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t think it&#39;s a bug in the OOM reaper itself, but either of the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; following two patches will fix the problem (without my understand how or</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; why):</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index ec9f11d4f094..37b14b2e2af4 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -485,7 +485,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  	mutex_lock(&amp;oom_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +	if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; __oom_reap_task_mm is basically the same thing as MADV_DONTNEED and that</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; doesn&#39;t require the exlusive mmap_sem. So this looks correct to me.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; BTW, shouldn&#39;t we filter out all VM_SPECIAL VMAs there? Or VM_PFNMAP at</span>
<span class="quote">&gt; &gt; &gt; &gt; least.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; MADV_DONTNEED doesn&#39;t touch VM_PFNMAP, but I don&#39;t see anything matching</span>
<span class="quote">&gt; &gt; &gt; &gt; on __oom_reap_task_mm() side.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I guess you are right and we should match the MADV_DONTNEED behavior</span>
<span class="quote">&gt; &gt; &gt; here. Care to send a patch?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Below. Testing required.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Other difference is that you use unmap_page_range() witch doesn&#39;t touch</span>
<span class="quote">&gt; &gt; &gt; &gt; mmu_notifiers. MADV_DONTNEED goes via zap_page_range(), which invalidates</span>
<span class="quote">&gt; &gt; &gt; &gt; the range. Not sure if it can make any difference here.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Which mmu notifier would care about this? I am not really familiar with</span>
<span class="quote">&gt; &gt; &gt; those users so I might miss something easily.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No idea either.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is there any reason not to use zap_page_range here too?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, zap_page_range is much more heavy and performs operations which</span>
<span class="quote">&gt; might lock AFAIR which I really would like to prevent from.</span>

What exactly can block there? I don&#39;t see anything with that potential.
<span class="quote">
&gt; &gt; Few more notes:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I propably miss something, but why do we need details-&gt;ignore_dirty?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It only appiled for non-anon pages, but since we filter out shared</span>
<span class="quote">&gt; &gt; mappings, how can we have pte_dirty() for !PageAnon()?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why couldn&#39;t we have dirty pages on the private file mappings? The</span>
<span class="quote">&gt; underlying page might be still in the page cache, right?</span>

The check is about dirty PTE, not dirty page.
<span class="quote">
&gt; &gt; check_swap_entries is also sloppy: the behavior doesn&#39;t match the comment:</span>
<span class="quote">&gt; &gt; details == NULL makes it check swap entries. I removed it and restore</span>
<span class="quote">&gt; &gt; details-&gt;check_mapping test as we had before.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; the reason is unmap_mapping_range which didn&#39;t use to check swap entries</span>
<span class="quote">&gt; so I wanted to have it opt in AFAIR.</span>

details == NULL would give you it in both cases.
<span class="quote">
&gt; &gt; @@ -531,8 +519,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt; &gt;  		 * count elevated without a good reason.</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt;  		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; &gt; -			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt; &gt; -					 &amp;details);</span>
<span class="quote">&gt; &gt; +			madvise_dontneed(vma, &amp;vma, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would rather keep the unmap_page_range because it is the bare minumum</span>
<span class="quote">&gt; we have to do. Currently we are doing </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (is_vm_hugetlb_page(vma))</span>
<span class="quote">&gt; 			continue;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; so I would rather do something like</span>
<span class="quote">&gt; 		if (!can_vma_madv_dontneed(vma))</span>
<span class="quote">&gt; 			continue;</span>
<span class="quote">&gt; instead.</span>

We can do that.
But let&#39;s first understand why code should differ from madvise_dontneed().
It&#39;s not obvious to me.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Dec. 16, 2016, 1:14 p.m.</div>
<pre class="content">
On Fri 16-12-16 16:07:30, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Fri, Dec 16, 2016 at 01:56:50PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Fri 16-12-16 15:35:55, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri, Dec 16, 2016 at 12:42:43PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Fri 16-12-16 13:44:38, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; On Fri, Dec 16, 2016 at 11:11:13AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; On Fri 16-12-16 10:43:52, Vegard Nossum wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; I don&#39;t think it&#39;s a bug in the OOM reaper itself, but either of the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; following two patches will fix the problem (without my understand how or</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; why):</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; index ec9f11d4f094..37b14b2e2af4 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; @@ -485,7 +485,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;  	mutex_lock(&amp;oom_lock);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; -	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +	if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; __oom_reap_task_mm is basically the same thing as MADV_DONTNEED and that</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; doesn&#39;t require the exlusive mmap_sem. So this looks correct to me.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; BTW, shouldn&#39;t we filter out all VM_SPECIAL VMAs there? Or VM_PFNMAP at</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; least.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; MADV_DONTNEED doesn&#39;t touch VM_PFNMAP, but I don&#39;t see anything matching</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; on __oom_reap_task_mm() side.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I guess you are right and we should match the MADV_DONTNEED behavior</span>
<span class="quote">&gt; &gt; &gt; &gt; here. Care to send a patch?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Below. Testing required.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Other difference is that you use unmap_page_range() witch doesn&#39;t touch</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; mmu_notifiers. MADV_DONTNEED goes via zap_page_range(), which invalidates</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; the range. Not sure if it can make any difference here.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Which mmu notifier would care about this? I am not really familiar with</span>
<span class="quote">&gt; &gt; &gt; &gt; those users so I might miss something easily.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; No idea either.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Is there any reason not to use zap_page_range here too?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes, zap_page_range is much more heavy and performs operations which</span>
<span class="quote">&gt; &gt; might lock AFAIR which I really would like to prevent from.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What exactly can block there? I don&#39;t see anything with that potential.</span>

I would have to rememeber all the details. This is mostly off-topic for
this particular thread so I think it would be better if you could send a
full patch separatelly and we can discuss it there?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - Dec. 18, 2016, 1:47 p.m.</div>
<pre class="content">
On 2016/12/16 22:14, Michal Hocko wrote:
<span class="quote">&gt; On Fri 16-12-16 16:07:30, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt; On Fri, Dec 16, 2016 at 01:56:50PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt; On Fri 16-12-16 15:35:55, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Fri, Dec 16, 2016 at 12:42:43PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Fri 16-12-16 13:44:38, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On Fri, Dec 16, 2016 at 11:11:13AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Fri 16-12-16 10:43:52, Vegard Nossum wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; [...]</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I don&#39;t think it&#39;s a bug in the OOM reaper itself, but either of the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; following two patches will fix the problem (without my understand how or</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; why):</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; index ec9f11d4f094..37b14b2e2af4 100644</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -485,7 +485,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	mutex_lock(&amp;oom_lock);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; -	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (!down_write_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; __oom_reap_task_mm is basically the same thing as MADV_DONTNEED and that</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; doesn&#39;t require the exlusive mmap_sem. So this looks correct to me.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; BTW, shouldn&#39;t we filter out all VM_SPECIAL VMAs there? Or VM_PFNMAP at</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; least.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; MADV_DONTNEED doesn&#39;t touch VM_PFNMAP, but I don&#39;t see anything matching</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; on __oom_reap_task_mm() side.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I guess you are right and we should match the MADV_DONTNEED behavior</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; here. Care to send a patch?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Below. Testing required.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Other difference is that you use unmap_page_range() witch doesn&#39;t touch</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; mmu_notifiers. MADV_DONTNEED goes via zap_page_range(), which invalidates</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; the range. Not sure if it can make any difference here.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Which mmu notifier would care about this? I am not really familiar with</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; those users so I might miss something easily.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; No idea either.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Is there any reason not to use zap_page_range here too?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Yes, zap_page_range is much more heavy and performs operations which</span>
<span class="quote">&gt;&gt;&gt; might lock AFAIR which I really would like to prevent from.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What exactly can block there? I don&#39;t see anything with that potential.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would have to rememeber all the details. This is mostly off-topic for</span>
<span class="quote">&gt; this particular thread so I think it would be better if you could send a</span>
<span class="quote">&gt; full patch separatelly and we can discuss it there?</span>
<span class="quote">&gt; </span>

zap_page_range() calls mmu_notifier_invalidate_range_start().
mmu_notifier_invalidate_range_start() calls __mmu_notifier_invalidate_range_start().
__mmu_notifier_invalidate_range_start() calls srcu_read_lock()/srcu_read_unlock().
This means that zap_page_range() might sleep.

I don&#39;t know what individual notifier will do, but for example

  static const struct mmu_notifier_ops i915_gem_userptr_notifier = {
          .invalidate_range_start = i915_gem_userptr_mn_invalidate_range_start,
  };

i915_gem_userptr_mn_invalidate_range_start() calls flush_workqueue()
which means that we can OOM livelock if work item involves memory allocation.
Some of other notifiers call mutex_lock()/mutex_unlock().

Even if none of currently in-tree notifier users are blocked on memory
allocation, I think it is not guaranteed that future changes/users won&#39;t be
blocked on memory allocation.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Dec. 18, 2016, 4:06 p.m.</div>
<pre class="content">
On Sun 18-12-16 22:47:07, Tetsuo Handa wrote:
<span class="quote">&gt; On 2016/12/16 22:14, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; I would have to rememeber all the details. This is mostly off-topic for</span>
<span class="quote">&gt; &gt; this particular thread so I think it would be better if you could send a</span>
<span class="quote">&gt; &gt; full patch separatelly and we can discuss it there?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; zap_page_range() calls mmu_notifier_invalidate_range_start().</span>
<span class="quote">&gt; mmu_notifier_invalidate_range_start() calls __mmu_notifier_invalidate_range_start().</span>
<span class="quote">&gt; __mmu_notifier_invalidate_range_start() calls srcu_read_lock()/srcu_read_unlock().</span>
<span class="quote">&gt; This means that zap_page_range() might sleep.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t know what individual notifier will do, but for example</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   static const struct mmu_notifier_ops i915_gem_userptr_notifier = {</span>
<span class="quote">&gt;           .invalidate_range_start = i915_gem_userptr_mn_invalidate_range_start,</span>
<span class="quote">&gt;   };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; i915_gem_userptr_mn_invalidate_range_start() calls flush_workqueue()</span>
<span class="quote">&gt; which means that we can OOM livelock if work item involves memory allocation.</span>
<span class="quote">&gt; Some of other notifiers call mutex_lock()/mutex_unlock().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Even if none of currently in-tree notifier users are blocked on memory</span>
<span class="quote">&gt; allocation, I think it is not guaranteed that future changes/users won&#39;t be</span>
<span class="quote">&gt; blocked on memory allocation.</span>

Kirill has sent this as a separate patchset [1]. Could you follow up on
that there please?

http://lkml.kernel.org/r/20161216141556.75130-4-kirill.shutemov@linux.intel.com
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/s390/mm/gmap.c b/arch/s390/mm/gmap.c</span>
<span class="p_header">index ec1f0dedb948..59ac93714fa4 100644</span>
<span class="p_header">--- a/arch/s390/mm/gmap.c</span>
<span class="p_header">+++ b/arch/s390/mm/gmap.c</span>
<span class="p_chunk">@@ -687,7 +687,7 @@</span> <span class="p_context"> void gmap_discard(struct gmap *gmap, unsigned long from, unsigned long to)</span>
 		/* Find vma in the parent mm */
 		vma = find_vma(gmap-&gt;mm, vmaddr);
 		size = min(to - gaddr, PMD_SIZE - (gaddr &amp; ~PMD_MASK));
<span class="p_del">-		zap_page_range(vma, vmaddr, size, NULL);</span>
<span class="p_add">+		zap_page_range(vma, vmaddr, size);</span>
 	}
 	up_read(&amp;gmap-&gt;mm-&gt;mmap_sem);
 }
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index e4f800999b32..4bfb31e79d5d 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -796,7 +796,7 @@</span> <span class="p_context"> static noinline int zap_bt_entries_mapping(struct mm_struct *mm,</span>
 			return -EINVAL;
 
 		len = min(vma-&gt;vm_end, end) - addr;
<span class="p_del">-		zap_page_range(vma, addr, len, NULL);</span>
<span class="p_add">+		zap_page_range(vma, addr, len);</span>
 		trace_mpx_unmap_zap(addr, addr+len);
 
 		vma = vma-&gt;vm_next;
<span class="p_header">diff --git a/drivers/android/binder.c b/drivers/android/binder.c</span>
<span class="p_header">index 3c71b982bf2a..d97f6725cf8c 100644</span>
<span class="p_header">--- a/drivers/android/binder.c</span>
<span class="p_header">+++ b/drivers/android/binder.c</span>
<span class="p_chunk">@@ -629,7 +629,7 @@</span> <span class="p_context"> static int binder_update_page_range(struct binder_proc *proc, int allocate,</span>
 		page = &amp;proc-&gt;pages[(page_addr - proc-&gt;buffer) / PAGE_SIZE];
 		if (vma)
 			zap_page_range(vma, (uintptr_t)page_addr +
<span class="p_del">-				proc-&gt;user_buffer_offset, PAGE_SIZE, NULL);</span>
<span class="p_add">+				proc-&gt;user_buffer_offset, PAGE_SIZE);</span>
 err_vm_insert_page_failed:
 		unmap_kernel_range((unsigned long)page_addr, PAGE_SIZE);
 err_map_kernel_failed:
<span class="p_header">diff --git a/drivers/staging/android/ion/ion.c b/drivers/staging/android/ion/ion.c</span>
<span class="p_header">index b653451843c8..0fb0e28ace70 100644</span>
<span class="p_header">--- a/drivers/staging/android/ion/ion.c</span>
<span class="p_header">+++ b/drivers/staging/android/ion/ion.c</span>
<span class="p_chunk">@@ -865,8 +865,7 @@</span> <span class="p_context"> static void ion_buffer_sync_for_device(struct ion_buffer *buffer,</span>
 	list_for_each_entry(vma_list, &amp;buffer-&gt;vmas, list) {
 		struct vm_area_struct *vma = vma_list-&gt;vma;
 
<span class="p_del">-		zap_page_range(vma, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="p_del">-			       NULL);</span>
<span class="p_add">+		zap_page_range(vma, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start);</span>
 	}
 	mutex_unlock(&amp;buffer-&gt;lock);
 }
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 4424784ac374..92dcada8caaf 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1148,8 +1148,6 @@</span> <span class="p_context"> struct zap_details {</span>
 	struct address_space *check_mapping;	/* Check page-&gt;mapping if set */
 	pgoff_t	first_index;			/* Lowest page-&gt;index to unmap */
 	pgoff_t last_index;			/* Highest page-&gt;index to unmap */
<span class="p_del">-	bool ignore_dirty;			/* Ignore dirty pages */</span>
<span class="p_del">-	bool check_swap_entries;		/* Check also swap entries */</span>
 };
 
 struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
<span class="p_chunk">@@ -1160,7 +1158,7 @@</span> <span class="p_context"> struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,</span>
 int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
 		unsigned long size);
 void zap_page_range(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned long size, struct zap_details *);</span>
<span class="p_add">+		unsigned long size);</span>
 void unmap_vmas(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long start, unsigned long end);
 
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 44d68895a9b9..5c355855e4ad 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -41,10 +41,9 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf);</span>
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
 
<span class="p_del">-void unmap_page_range(struct mmu_gather *tlb,</span>
<span class="p_del">-			     struct vm_area_struct *vma,</span>
<span class="p_del">-			     unsigned long addr, unsigned long end,</span>
<span class="p_del">-			     struct zap_details *details);</span>
<span class="p_add">+long madvise_dontneed(struct vm_area_struct *vma,</span>
<span class="p_add">+			     struct vm_area_struct **prev,</span>
<span class="p_add">+			     unsigned long start, unsigned long end);</span>
 
 extern int __do_page_cache_readahead(struct address_space *mapping,
 		struct file *filp, pgoff_t offset, unsigned long nr_to_read,
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 0e3828eae9f8..8c9f19b62b4a 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -468,7 +468,7 @@</span> <span class="p_context"> static long madvise_free(struct vm_area_struct *vma,</span>
  * An interface that causes the system to free clean pages and flush
  * dirty pages is already available as msync(MS_INVALIDATE).
  */
<span class="p_del">-static long madvise_dontneed(struct vm_area_struct *vma,</span>
<span class="p_add">+long madvise_dontneed(struct vm_area_struct *vma,</span>
 			     struct vm_area_struct **prev,
 			     unsigned long start, unsigned long end)
 {
<span class="p_chunk">@@ -476,7 +476,7 @@</span> <span class="p_context"> static long madvise_dontneed(struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))
 		return -EINVAL;
 
<span class="p_del">-	zap_page_range(vma, start, end - start, NULL);</span>
<span class="p_add">+	zap_page_range(vma, start, end - start);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 455c3e628d52..f8836232a492 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1155,12 +1155,6 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 
 			if (!PageAnon(page)) {
 				if (pte_dirty(ptent)) {
<span class="p_del">-					/*</span>
<span class="p_del">-					 * oom_reaper cannot tear down dirty</span>
<span class="p_del">-					 * pages</span>
<span class="p_del">-					 */</span>
<span class="p_del">-					if (unlikely(details &amp;&amp; details-&gt;ignore_dirty))</span>
<span class="p_del">-						continue;</span>
 					force_flush = 1;
 					set_page_dirty(page);
 				}
<span class="p_chunk">@@ -1179,8 +1173,8 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 			}
 			continue;
 		}
<span class="p_del">-		/* only check swap_entries if explicitly asked for in details */</span>
<span class="p_del">-		if (unlikely(details &amp;&amp; !details-&gt;check_swap_entries))</span>
<span class="p_add">+		/* If details-&gt;check_mapping, we leave swap entries. */</span>
<span class="p_add">+		if (unlikely(details))</span>
 			continue;
 
 		entry = pte_to_swp_entry(ptent);
<span class="p_chunk">@@ -1277,7 +1271,7 @@</span> <span class="p_context"> static inline unsigned long zap_pud_range(struct mmu_gather *tlb,</span>
 	return addr;
 }
 
<span class="p_del">-void unmap_page_range(struct mmu_gather *tlb,</span>
<span class="p_add">+static void unmap_page_range(struct mmu_gather *tlb,</span>
 			     struct vm_area_struct *vma,
 			     unsigned long addr, unsigned long end,
 			     struct zap_details *details)
<span class="p_chunk">@@ -1381,7 +1375,7 @@</span> <span class="p_context"> void unmap_vmas(struct mmu_gather *tlb,</span>
  * Caller must protect the VMA list
  */
 void zap_page_range(struct vm_area_struct *vma, unsigned long start,
<span class="p_del">-		unsigned long size, struct zap_details *details)</span>
<span class="p_add">+		unsigned long size)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct mmu_gather tlb;
<span class="p_chunk">@@ -1392,7 +1386,7 @@</span> <span class="p_context"> void zap_page_range(struct vm_area_struct *vma, unsigned long start,</span>
 	update_hiwater_rss(mm);
 	mmu_notifier_invalidate_range_start(mm, start, end);
 	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next)
<span class="p_del">-		unmap_single_vma(&amp;tlb, vma, start, end, details);</span>
<span class="p_add">+		unmap_single_vma(&amp;tlb, vma, start, end, NULL);</span>
 	mmu_notifier_invalidate_range_end(mm, start, end);
 	tlb_finish_mmu(&amp;tlb, start, end);
 }
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index ec9f11d4f094..f6451eacb0aa 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -465,8 +465,6 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
<span class="p_del">-	struct zap_details details = {.check_swap_entries = true,</span>
<span class="p_del">-				      .ignore_dirty = true};</span>
 	bool ret = true;
 
 	/*
<span class="p_chunk">@@ -481,7 +479,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 *				out_of_memory
 	 *				  select_bad_process
 	 *				    # no TIF_MEMDIE task selects new victim
<span class="p_del">-	 *  unmap_page_range # frees some memory</span>
<span class="p_add">+	 *  madv_dontneed # frees some memory</span>
 	 */
 	mutex_lock(&amp;oom_lock);
 
<span class="p_chunk">@@ -510,16 +508,6 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 
 	tlb_gather_mmu(&amp;tlb, mm, 0, -1);
 	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {
<span class="p_del">-		if (is_vm_hugetlb_page(vma))</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * mlocked VMAs require explicit munlocking before unmap.</span>
<span class="p_del">-		 * Let&#39;s keep it simple here and skip such VMAs.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
 		/*
 		 * Only anonymous pages have a good chance to be dropped
 		 * without additional steps which we cannot afford as we
<span class="p_chunk">@@ -531,8 +519,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 		 * count elevated without a good reason.
 		 */
 		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))
<span class="p_del">-			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="p_del">-					 &amp;details);</span>
<span class="p_add">+			madvise_dontneed(vma, &amp;vma, vma-&gt;vm_start, vma-&gt;vm_end);</span>
 	}
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 	pr_info(&quot;oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



