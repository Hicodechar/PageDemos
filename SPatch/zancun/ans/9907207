
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,14/20] mm: Provide speculative fault infrastructure - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,14/20] mm: Provide speculative fault infrastructure</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 17, 2017, 10:05 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1503007519-26777-15-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9907207/mbox/"
   >mbox</a>
|
   <a href="/patch/9907207/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9907207/">/patch/9907207/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1B6DA60244 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 22:11:06 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 02BBF28C3A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 22:11:06 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F418328C23; Thu, 17 Aug 2017 22:11:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7FA8728C04
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 17 Aug 2017 22:10:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932131AbdHQWKv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 17 Aug 2017 18:10:51 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:34476 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1753324AbdHQWGY (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 17 Aug 2017 18:06:24 -0400
Received: from pps.filterd (m0098414.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	v7HM3tIu063696
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 17 Aug 2017 18:06:23 -0400
Received: from e06smtp14.uk.ibm.com (e06smtp14.uk.ibm.com [195.75.94.110])
	by mx0b-001b2d01.pphosted.com with ESMTP id 2cdd2pcus1-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Thu, 17 Aug 2017 18:06:23 -0400
Received: from localhost
	by e06smtp14.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Thu, 17 Aug 2017 23:06:21 +0100
Received: from b06cxnps4074.portsmouth.uk.ibm.com (9.149.109.196)
	by e06smtp14.uk.ibm.com (192.168.101.144) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Thu, 17 Aug 2017 23:06:15 +0100
Received: from d06av25.portsmouth.uk.ibm.com (d06av25.portsmouth.uk.ibm.com
	[9.149.105.61])
	by b06cxnps4074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v7HM6Euk20054258; Thu, 17 Aug 2017 22:06:14 GMT
Received: from d06av25.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 918EB11C04A;
	Thu, 17 Aug 2017 23:03:02 +0100 (BST)
Received: from d06av25.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id BF05511C04C;
	Thu, 17 Aug 2017 23:03:00 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.145.2.164])
	by d06av25.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Thu, 17 Aug 2017 23:03:00 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: paulmck@linux.vnet.ibm.com, peterz@infradead.org,
	akpm@linux-foundation.org, kirill@shutemov.name,
	ak@linux.intel.com, mhocko@kernel.org, dave@stgolabs.net,
	jack@suse.cz, Matthew Wilcox &lt;willy@infradead.org&gt;,
	benh@kernel.crashing.org, mpe@ellerman.id.au, paulus@samba.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, hpa@zytor.com,
	Will Deacon &lt;will.deacon@arm.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	haren@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	npiggin@gmail.com, bsingharora@gmail.com,
	Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org
Subject: [PATCH v2 14/20] mm: Provide speculative fault infrastructure
Date: Fri, 18 Aug 2017 00:05:13 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1503007519-26777-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
References: &lt;1503007519-26777-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17081722-0016-0000-0000-000004E487BF
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17081722-0017-0000-0000-0000281DAC88
Message-Id: &lt;1503007519-26777-15-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-08-17_12:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1707230000
	definitions=main-1708170357
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 17, 2017, 10:05 p.m.</div>
<pre class="content">
<span class="from">From: Peter Zijlstra &lt;peterz@infradead.org&gt;</span>

Provide infrastructure to do a speculative fault (not holding
mmap_sem).

The not holding of mmap_sem means we can race against VMA
change/removal and page-table destruction. We use the SRCU VMA freeing
to keep the VMA around. We use the VMA seqcount to detect change
(including umapping / page-table deletion) and we use gup_fast() style
page-table walking to deal with page-table races.

Once we&#39;ve obtained the page and are ready to update the PTE, we
validate if the state we started the fault with is still valid, if
not, we&#39;ll fail the fault with VM_FAULT_RETRY, otherwise we update the
PTE and we&#39;re done.
<span class="signed-off-by">
Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>

[Manage the newly introduced pte_spinlock() for speculative page
 fault to fail if the VMA is touched in our back]
[Rename vma_is_dead() to vma_has_changed() and declare it here]
[Call p4d_alloc() as it is safe since pgd is valid]
[Call pud_alloc() as it is safe since p4d is valid]
[Set fe.sequence in __handle_mm_fault()]
[Abort speculative path when handle_userfault() has to be called]
[Add additional VMA&#39;s flags checks in handle_speculative_fault()]
[Clear FAULT_FLAG_ALLOW_RETRY in handle_speculative_fault()]
[Don&#39;t set vmf-&gt;pte and vmf-&gt;ptl if pte_map_lock() failed]
[Remove warning comment about waiting for !seq&amp;1 since we don&#39;t want
 to wait]
[Remove warning about no huge page support, mention it explictly]
[Don&#39;t call do_fault() in the speculative path as __do_fault() calls
 vma-&gt;vm_ops-&gt;fault() which may want to release mmap_sem]
[Only vm_fault pointer argument for vma_has_changed()]
[Fix check against huge page, calling pmd_trans_huge()]
[Introduce __HAVE_ARCH_CALL_SPF to declare the SPF handler only when
 architecture is supporting it]
[Use READ_ONCE() when reading VMA&#39;s fields in the speculative path]
[Explicitly check for __HAVE_ARCH_PTE_SPECIAL as we can&#39;t support for
 processing done in vm_normal_page()]
[Check that vma-&gt;anon_vma is already set when starting the speculative
 path]
[Check for memory policy as we can&#39;t support MPOL_INTERLEAVE case due to
 the processing done in mpol_misplaced()]
[Don&#39;t support VMA growing up or down]
[Move check on vm_sequence just before calling handle_pte_fault()]
<span class="signed-off-by">Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 include/linux/hugetlb_inline.h |   2 +-
 include/linux/mm.h             |   5 +
 include/linux/pagemap.h        |   4 +-
 mm/internal.h                  |  14 +++
 mm/memory.c                    | 237 ++++++++++++++++++++++++++++++++++++++++-
 5 files changed, 254 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2990">Sergey Senozhatsky</a> - Aug. 20, 2017, 12:11 p.m.</div>
<pre class="content">
On (08/18/17 00:05), Laurent Dufour wrote:
[..]
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="quote">&gt; +	 * are not compatible with the speculative page fault processing.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pol = __get_vma_policy(vma, address);</span>
<span class="quote">&gt; +	if (!pol)</span>
<span class="quote">&gt; +		pol = get_task_policy(current);</span>
<span class="quote">&gt; +	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="quote">&gt; +		goto unlock;</span>

include/linux/mempolicy.h defines

struct mempolicy *get_task_policy(struct task_struct *p);
struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
		unsigned long addr);

only for CONFIG_NUMA configs.

	-ss
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 25, 2017, 8:52 a.m.</div>
<pre class="content">
On 20/08/2017 14:11, Sergey Senozhatsky wrote:
<span class="quote">&gt; On (08/18/17 00:05), Laurent Dufour wrote:</span>
<span class="quote">&gt; [..]</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="quote">&gt;&gt; +	 * are not compatible with the speculative page fault processing.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	pol = __get_vma_policy(vma, address);</span>
<span class="quote">&gt;&gt; +	if (!pol)</span>
<span class="quote">&gt;&gt; +		pol = get_task_policy(current);</span>
<span class="quote">&gt;&gt; +	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; include/linux/mempolicy.h defines</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; struct mempolicy *get_task_policy(struct task_struct *p);</span>
<span class="quote">&gt; struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,</span>
<span class="quote">&gt; 		unsigned long addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; only for CONFIG_NUMA configs.</span>

Thanks Sergey, I&#39;ll add #ifdef around this block.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 27, 2017, 12:18 a.m.</div>
<pre class="content">
On Fri, Aug 18, 2017 at 12:05:13AM +0200, Laurent Dufour wrote:
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * vm_normal_page() adds some processing which should be done while</span>
<span class="quote">&gt; + * hodling the mmap_sem.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int handle_speculative_fault(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt; +			     unsigned int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_fault vmf = {</span>
<span class="quote">&gt; +		.address = address,</span>
<span class="quote">&gt; +	};</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	p4d_t *p4d;</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +	int dead, seq, idx, ret = VM_FAULT_RETRY;</span>
<span class="quote">&gt; +	struct vm_area_struct *vma;</span>
<span class="quote">&gt; +	struct mempolicy *pol;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Clear flags that may lead to release the mmap_sem to retry */</span>
<span class="quote">&gt; +	flags &amp;= ~(FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_KILLABLE);</span>
<span class="quote">&gt; +	flags |= FAULT_FLAG_SPECULATIVE;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	idx = srcu_read_lock(&amp;vma_srcu);</span>
<span class="quote">&gt; +	vma = find_vma_srcu(mm, address);</span>
<span class="quote">&gt; +	if (!vma)</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Validate the VMA found by the lockless lookup.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	dead = RB_EMPTY_NODE(&amp;vma-&gt;vm_rb);</span>
<span class="quote">&gt; +	seq = raw_read_seqcount(&amp;vma-&gt;vm_sequence); /* rmb &lt;-&gt; seqlock,vma_rb_erase() */</span>
<span class="quote">&gt; +	if ((seq &amp; 1) || dead)</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Can&#39;t call vm_ops service has we don&#39;t know what they would do</span>
<span class="quote">&gt; +	 * with the VMA.</span>
<span class="quote">&gt; +	 * This include huge page from hugetlbfs.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (vma-&gt;vm_ops)</span>
<span class="quote">&gt; +		goto unlock;</span>

I think we need to have a way to white-list safe -&gt;vm_ops.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt; +		goto unlock;</span>

It deserves a comment.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	vmf.vma_flags = READ_ONCE(vma-&gt;vm_flags);</span>
<span class="quote">&gt; +	vmf.vma_page_prot = READ_ONCE(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Can&#39;t call userland page fault handler in the speculative path */</span>
<span class="quote">&gt; +	if (unlikely(vmf.vma_flags &amp; VM_UFFD_MISSING))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="quote">&gt; +	 * are not compatible with the speculative page fault processing.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pol = __get_vma_policy(vma, address);</span>
<span class="quote">&gt; +	if (!pol)</span>
<span class="quote">&gt; +		pol = get_task_policy(current);</span>
<span class="quote">&gt; +	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (vmf.vma_flags &amp; VM_GROWSDOWN || vmf.vma_flags &amp; VM_GROWSUP)</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * This could be detected by the check address against VMA&#39;s</span>
<span class="quote">&gt; +		 * boundaries but we want to trace it as not supported instead</span>
<span class="quote">&gt; +		 * of changed.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (address &lt; READ_ONCE(vma-&gt;vm_start)</span>
<span class="quote">&gt; +	    || READ_ONCE(vma-&gt;vm_end) &lt;= address)</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The three following checks are copied from access_error from</span>
<span class="quote">&gt; +	 * arch/x86/mm/fault.c</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="quote">&gt; +				       flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="quote">&gt; +				       flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* This is one is required to check that the VMA has write access set */</span>
<span class="quote">&gt; +	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="quote">&gt; +		if (unlikely(!(vmf.vma_flags &amp; VM_WRITE)))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (unlikely(!(vmf.vma_flags &amp; (VM_READ | VM_EXEC | VM_WRITE))))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Do a speculative lookup of the PTE entry.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	local_irq_disable();</span>
<span class="quote">&gt; +	pgd = pgd_offset(mm, address);</span>
<span class="quote">&gt; +	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))</span>
<span class="quote">&gt; +		goto out_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="quote">&gt; +	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="quote">&gt; +		goto out_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_alloc(mm, p4d, address);</span>
<span class="quote">&gt; +	if (pud_none(*pud) || unlikely(pud_bad(*pud)))</span>
<span class="quote">&gt; +		goto out_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; +	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))</span>
<span class="quote">&gt; +		goto out_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The above does not allocate/instantiate page-tables because doing so</span>
<span class="quote">&gt; +	 * would lead to the possibility of instantiating page-tables after</span>
<span class="quote">&gt; +	 * free_pgtables() -- and consequently leaking them.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * The result is that we take at least one !speculative fault per PMD</span>
<span class="quote">&gt; +	 * in order to instantiate it.</span>
<span class="quote">&gt; +	 */</span>


Doing all this job and just give up because we cannot allocate page tables
looks very wasteful to me.

Have you considered to look how we can hand over from speculative to
non-speculative path without starting from scratch (when possible)?
<span class="quote">
&gt; +	/* Transparent huge pages are not supported. */</span>
<span class="quote">&gt; +	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt; +		goto out_walk;</span>

That&#39;s looks like a blocker to me.

Is there any problem with making it supported (besides plain coding)?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	vmf.vma = vma;</span>
<span class="quote">&gt; +	vmf.pmd = pmd;</span>
<span class="quote">&gt; +	vmf.pgoff = linear_page_index(vma, address);</span>
<span class="quote">&gt; +	vmf.gfp_mask = __get_fault_gfp_mask(vma);</span>
<span class="quote">&gt; +	vmf.sequence = seq;</span>
<span class="quote">&gt; +	vmf.flags = flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local_irq_enable();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We need to re-validate the VMA after checking the bounds, otherwise</span>
<span class="quote">&gt; +	 * we might have a false positive on the bounds.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (read_seqcount_retry(&amp;vma-&gt;vm_sequence, seq))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ret = handle_pte_fault(&amp;vmf);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt; +	srcu_read_unlock(&amp;vma_srcu, idx);</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_walk:</span>
<span class="quote">&gt; +	local_irq_enable();</span>
<span class="quote">&gt; +	goto unlock;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* __HAVE_ARCH_CALL_SPF */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * By the time we get here, we already hold the mm semaphore</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.4</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 28, 2017, 9:37 a.m.</div>
<pre class="content">
On Sun, Aug 27, 2017 at 03:18:23AM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Fri, Aug 18, 2017 at 12:05:13AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Can&#39;t call vm_ops service has we don&#39;t know what they would do</span>
<span class="quote">&gt; &gt; +	 * with the VMA.</span>
<span class="quote">&gt; &gt; +	 * This include huge page from hugetlbfs.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (vma-&gt;vm_ops)</span>
<span class="quote">&gt; &gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we need to have a way to white-list safe -&gt;vm_ops.</span>

Either that, or simply teach all -&gt;fault() callbacks about speculative
faults. Shouldn&#39;t be too hard, just &#39;work&#39;.
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt; &gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It deserves a comment.</span>

Yes, that was very much not intended. It wrecks most of the fun. This
really _should_ work for file maps too.
<span class="quote">
&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Do a speculative lookup of the PTE entry.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	local_irq_disable();</span>
<span class="quote">&gt; &gt; +	pgd = pgd_offset(mm, address);</span>
<span class="quote">&gt; &gt; +	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))</span>
<span class="quote">&gt; &gt; +		goto out_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="quote">&gt; &gt; +	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="quote">&gt; &gt; +		goto out_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pud = pud_alloc(mm, p4d, address);</span>
<span class="quote">&gt; &gt; +	if (pud_none(*pud) || unlikely(pud_bad(*pud)))</span>
<span class="quote">&gt; &gt; +		goto out_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; &gt; +	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))</span>
<span class="quote">&gt; &gt; +		goto out_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * The above does not allocate/instantiate page-tables because doing so</span>
<span class="quote">&gt; &gt; +	 * would lead to the possibility of instantiating page-tables after</span>
<span class="quote">&gt; &gt; +	 * free_pgtables() -- and consequently leaking them.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * The result is that we take at least one !speculative fault per PMD</span>
<span class="quote">&gt; &gt; +	 * in order to instantiate it.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Doing all this job and just give up because we cannot allocate page tables</span>
<span class="quote">&gt; looks very wasteful to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Have you considered to look how we can hand over from speculative to</span>
<span class="quote">&gt; non-speculative path without starting from scratch (when possible)?</span>

So we _can_ in fact allocate and install page-tables, but we have to be
very careful about it. The interesting case is where we race with
free_pgtables() and install a page that was just taken out.

But since we already have the VMA I think we can do something like:

	if (p*g_none()) {
		p*d_t *new = p*d_alloc_one(mm, address);

		spin_lock(&amp;mm-&gt;page_table_lock);
		if (!vma_changed_or_dead(vma,seq)) {
			if (p*d_none())
				p*d_populate(mm, p*d, new);
			else
				p*d_free(new);

			new = NULL;
		}
		spin_unlock(&amp;mm-&gt;page_table_lock);

		if (new) {
			p*d_free(new);
			goto out_walk;
		}
	}

I just never bothered with that, figured we ought to get the basics
working before trying to be clever.
<span class="quote">
&gt; &gt; +	/* Transparent huge pages are not supported. */</span>
<span class="quote">&gt; &gt; +	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt; &gt; +		goto out_walk;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s looks like a blocker to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any problem with making it supported (besides plain coding)?</span>

Not that I can remember, but I never really looked at THP, I don&#39;t think
we even had that when I did the first versions.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=245">Benjamin Herrenschmidt</a> - Aug. 28, 2017, 9:14 p.m.</div>
<pre class="content">
On Mon, 2017-08-28 at 11:37 +0200, Peter Zijlstra wrote:
<span class="quote">&gt; &gt; Doing all this job and just give up because we cannot allocate page tables</span>
<span class="quote">&gt; &gt; looks very wasteful to me.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Have you considered to look how we can hand over from speculative to</span>
<span class="quote">&gt; &gt; non-speculative path without starting from scratch (when possible)?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So we _can_ in fact allocate and install page-tables, but we have to be</span>
<span class="quote">&gt; very careful about it. The interesting case is where we race with</span>
<span class="quote">&gt; free_pgtables() and install a page that was just taken out.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But since we already have the VMA I think we can do something like:</span>

That makes me extremely nervous... there could be all sort of
assumptions esp. in arch code about the fact that we never populate the
tree without the mm sem.

We&#39;d have to audit archs closely. Things like the page walk cache
flushing on power etc...

I don&#39;t mind the &quot;retry&quot; .. .we&#39;ve brought stuff in the L1 cache
already which I would expect to be the bulk of the overhead, and the
allocation case isn&#39;t that common. Do we have numbers to show how
destrimental this is today ?

Cheers,
Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123">Andi Kleen</a> - Aug. 28, 2017, 10:35 p.m.</div>
<pre class="content">
<span class="quote">&gt; That makes me extremely nervous... there could be all sort of</span>
<span class="quote">&gt; assumptions esp. in arch code about the fact that we never populate the</span>
<span class="quote">&gt; tree without the mm sem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;d have to audit archs closely. Things like the page walk cache</span>
<span class="quote">&gt; flushing on power etc...</span>

Yes the whole thing is quite risky. Probably will need some
kind of per architecture opt-in scheme?

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 29, 2017, 7:59 a.m.</div>
<pre class="content">
On 27/08/2017 02:18, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Fri, Aug 18, 2017 at 12:05:13AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * vm_normal_page() adds some processing which should be done while</span>
<span class="quote">&gt;&gt; + * hodling the mmap_sem.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +int handle_speculative_fault(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;&gt; +			     unsigned int flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vm_fault vmf = {</span>
<span class="quote">&gt;&gt; +		.address = address,</span>
<span class="quote">&gt;&gt; +	};</span>
<span class="quote">&gt;&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt;&gt; +	p4d_t *p4d;</span>
<span class="quote">&gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt;&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt;&gt; +	int dead, seq, idx, ret = VM_FAULT_RETRY;</span>
<span class="quote">&gt;&gt; +	struct vm_area_struct *vma;</span>
<span class="quote">&gt;&gt; +	struct mempolicy *pol;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Clear flags that may lead to release the mmap_sem to retry */</span>
<span class="quote">&gt;&gt; +	flags &amp;= ~(FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_KILLABLE);</span>
<span class="quote">&gt;&gt; +	flags |= FAULT_FLAG_SPECULATIVE;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	idx = srcu_read_lock(&amp;vma_srcu);</span>
<span class="quote">&gt;&gt; +	vma = find_vma_srcu(mm, address);</span>
<span class="quote">&gt;&gt; +	if (!vma)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Validate the VMA found by the lockless lookup.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	dead = RB_EMPTY_NODE(&amp;vma-&gt;vm_rb);</span>
<span class="quote">&gt;&gt; +	seq = raw_read_seqcount(&amp;vma-&gt;vm_sequence); /* rmb &lt;-&gt; seqlock,vma_rb_erase() */</span>
<span class="quote">&gt;&gt; +	if ((seq &amp; 1) || dead)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Can&#39;t call vm_ops service has we don&#39;t know what they would do</span>
<span class="quote">&gt;&gt; +	 * with the VMA.</span>
<span class="quote">&gt;&gt; +	 * This include huge page from hugetlbfs.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (vma-&gt;vm_ops)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we need to have a way to white-list safe -&gt;vm_ops.</span>

Hi Kirill,
Yes this would be a good optimization done in a next step.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It deserves a comment.</span>

You&#39;re right I&#39;ll add it in the next version.
For the record, the root cause is that __anon_vma_prepare() requires the
mmap_sem to be held because vm_next and vm_prev must be safe.
<span class="quote">

&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vmf.vma_flags = READ_ONCE(vma-&gt;vm_flags);</span>
<span class="quote">&gt;&gt; +	vmf.vma_page_prot = READ_ONCE(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Can&#39;t call userland page fault handler in the speculative path */</span>
<span class="quote">&gt;&gt; +	if (unlikely(vmf.vma_flags &amp; VM_UFFD_MISSING))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="quote">&gt;&gt; +	 * are not compatible with the speculative page fault processing.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	pol = __get_vma_policy(vma, address);</span>
<span class="quote">&gt;&gt; +	if (!pol)</span>
<span class="quote">&gt;&gt; +		pol = get_task_policy(current);</span>
<span class="quote">&gt;&gt; +	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (vmf.vma_flags &amp; VM_GROWSDOWN || vmf.vma_flags &amp; VM_GROWSUP)</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * This could be detected by the check address against VMA&#39;s</span>
<span class="quote">&gt;&gt; +		 * boundaries but we want to trace it as not supported instead</span>
<span class="quote">&gt;&gt; +		 * of changed.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (address &lt; READ_ONCE(vma-&gt;vm_start)</span>
<span class="quote">&gt;&gt; +	    || READ_ONCE(vma-&gt;vm_end) &lt;= address)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The three following checks are copied from access_error from</span>
<span class="quote">&gt;&gt; +	 * arch/x86/mm/fault.c</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="quote">&gt;&gt; +				       flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="quote">&gt;&gt; +				       flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* This is one is required to check that the VMA has write access set */</span>
<span class="quote">&gt;&gt; +	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="quote">&gt;&gt; +		if (unlikely(!(vmf.vma_flags &amp; VM_WRITE)))</span>
<span class="quote">&gt;&gt; +			goto unlock;</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		if (unlikely(!(vmf.vma_flags &amp; (VM_READ | VM_EXEC | VM_WRITE))))</span>
<span class="quote">&gt;&gt; +			goto unlock;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Do a speculative lookup of the PTE entry.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	local_irq_disable();</span>
<span class="quote">&gt;&gt; +	pgd = pgd_offset(mm, address);</span>
<span class="quote">&gt;&gt; +	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="quote">&gt;&gt; +	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pud = pud_alloc(mm, p4d, address);</span>
<span class="quote">&gt;&gt; +	if (pud_none(*pud) || unlikely(pud_bad(*pud)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt;&gt; +	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The above does not allocate/instantiate page-tables because doing so</span>
<span class="quote">&gt;&gt; +	 * would lead to the possibility of instantiating page-tables after</span>
<span class="quote">&gt;&gt; +	 * free_pgtables() -- and consequently leaking them.</span>
<span class="quote">&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt; +	 * The result is that we take at least one !speculative fault per PMD</span>
<span class="quote">&gt;&gt; +	 * in order to instantiate it.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Doing all this job and just give up because we cannot allocate page tables</span>
<span class="quote">&gt; looks very wasteful to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Have you considered to look how we can hand over from speculative to</span>
<span class="quote">&gt; non-speculative path without starting from scratch (when possible)?</span>

Not really, but as mentioned by Benjamin and Andy, this will require care
from the architecture code.
This may be a future optimization, but it will require guarantee from the
architecture code as well.
<span class="quote">
&gt;&gt; +	/* Transparent huge pages are not supported. */</span>
<span class="quote">&gt;&gt; +	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s looks like a blocker to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any problem with making it supported (besides plain coding)?</span>

To be honest, I can&#39;t remember why I added such a check, may be for safety
reason, but I need to double check that again. I&#39;ll do so and come back
later with a statement.

Thanks,
Laurent.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vmf.vma = vma;</span>
<span class="quote">&gt;&gt; +	vmf.pmd = pmd;</span>
<span class="quote">&gt;&gt; +	vmf.pgoff = linear_page_index(vma, address);</span>
<span class="quote">&gt;&gt; +	vmf.gfp_mask = __get_fault_gfp_mask(vma);</span>
<span class="quote">&gt;&gt; +	vmf.sequence = seq;</span>
<span class="quote">&gt;&gt; +	vmf.flags = flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	local_irq_enable();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * We need to re-validate the VMA after checking the bounds, otherwise</span>
<span class="quote">&gt;&gt; +	 * we might have a false positive on the bounds.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (read_seqcount_retry(&amp;vma-&gt;vm_sequence, seq))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ret = handle_pte_fault(&amp;vmf);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +unlock:</span>
<span class="quote">&gt;&gt; +	srcu_read_unlock(&amp;vma_srcu, idx);</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +out_walk:</span>
<span class="quote">&gt;&gt; +	local_irq_enable();</span>
<span class="quote">&gt;&gt; +	goto unlock;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif /* __HAVE_ARCH_CALL_SPF */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * By the time we get here, we already hold the mm semaphore</span>
<span class="quote">&gt;&gt;   *</span>
<span class="quote">&gt;&gt; -- </span>
<span class="quote">&gt;&gt; 2.7.4</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 29, 2017, 8:15 a.m.</div>
<pre class="content">
On Mon, Aug 28, 2017 at 03:35:11PM -0700, Andi Kleen wrote:
<span class="quote">&gt; Yes the whole thing is quite risky. Probably will need some</span>
<span class="quote">&gt; kind of per architecture opt-in scheme?</span>

See patch 19/20, that not enough for you?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 29, 2017, 8:33 a.m.</div>
<pre class="content">
On Tue, Aug 29, 2017 at 07:14:37AM +1000, Benjamin Herrenschmidt wrote:
<span class="quote">&gt; On Mon, 2017-08-28 at 11:37 +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; &gt; Doing all this job and just give up because we cannot allocate page tables</span>
<span class="quote">&gt; &gt; &gt; looks very wasteful to me.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Have you considered to look how we can hand over from speculative to</span>
<span class="quote">&gt; &gt; &gt; non-speculative path without starting from scratch (when possible)?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So we _can_ in fact allocate and install page-tables, but we have to be</span>
<span class="quote">&gt; &gt; very careful about it. The interesting case is where we race with</span>
<span class="quote">&gt; &gt; free_pgtables() and install a page that was just taken out.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But since we already have the VMA I think we can do something like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That makes me extremely nervous... there could be all sort of</span>
<span class="quote">&gt; assumptions esp. in arch code about the fact that we never populate the</span>
<span class="quote">&gt; tree without the mm sem.</span>

That _would_ be somewhat dodgy, because that means it needs to rely on
taking mmap_sem for _writing_ to undo things and arch/powerpc/ doesn&#39;t
have many down_write.*mmap_sem:

$ git grep &quot;down_write.*mmap_sem&quot; arch/powerpc/
arch/powerpc/kernel/vdso.c:     if (down_write_killable(&amp;mm-&gt;mmap_sem))
arch/powerpc/kvm/book3s_64_vio.c:       down_write(&amp;current-&gt;mm-&gt;mmap_sem);
arch/powerpc/mm/mmu_context_iommu.c:    down_write(&amp;mm-&gt;mmap_sem);
arch/powerpc/mm/subpage-prot.c: down_write(&amp;mm-&gt;mmap_sem);
arch/powerpc/mm/subpage-prot.c: down_write(&amp;mm-&gt;mmap_sem);
arch/powerpc/mm/subpage-prot.c:         down_write(&amp;mm-&gt;mmap_sem);

Then again, I suppose it could be relying on the implicit down_write
from things like munmap() and the like..

And things _ought_ to be ordered by the various PTLs
(mm-&gt;page_table_lock and pmd-&gt;lock) which of course doesn&#39;t mean
something accidentally snuck through.
<span class="quote">
&gt; We&#39;d have to audit archs closely. Things like the page walk cache</span>
<span class="quote">&gt; flushing on power etc...</span>

If you point me where to look, I&#39;ll have a poke around. I&#39;m not
quite sure what you mean with pagewalk cache flushing. Your hash thing
flushes everything inside the PTL IIRC and the radix code appears fairly
&#39;normal&#39;.
<span class="quote">
&gt; I don&#39;t mind the &quot;retry&quot; .. .we&#39;ve brought stuff in the L1 cache</span>
<span class="quote">&gt; already which I would expect to be the bulk of the overhead, and the</span>
<span class="quote">&gt; allocation case isn&#39;t that common. Do we have numbers to show how</span>
<span class="quote">&gt; destrimental this is today ?</span>

No numbers, afaik. And like I said, I didn&#39;t consider this an actual
problem when I did these patches. But since Kirill asked ;-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 29, 2017, 11:27 a.m.</div>
<pre class="content">
On Tue, Aug 29, 2017 at 10:33:52AM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Aug 29, 2017 at 07:14:37AM +1000, Benjamin Herrenschmidt wrote:</span>
<span class="quote">&gt; &gt; We&#39;d have to audit archs closely. Things like the page walk cache</span>
<span class="quote">&gt; &gt; flushing on power etc...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you point me where to look, I&#39;ll have a poke around. I&#39;m not</span>
<span class="quote">&gt; quite sure what you mean with pagewalk cache flushing. Your hash thing</span>
<span class="quote">&gt; flushes everything inside the PTL IIRC and the radix code appears fairly</span>
<span class="quote">&gt; &#39;normal&#39;.</span>

mpe helped me out and explained that is the PWC hint to TBLIE.

So, you set need_flush_all when you unhook pud/pmd/pte which you then
use to set PWC. So free_pgtables() will do the PWC when it unhooks
higher level pages.

But you&#39;re right that there&#39;s some issues, free_pgtables() itself
doesn&#39;t seem to use mm-&gt;page_table_lock,pmd-&gt;lock _AT_ALL_ to unhook the
pages.

If it were to do that, things should work fine since those locks would
then serialize against the speculative faults, we would never install a
page if the VMA would be under tear-down and it would thus not be
visible to your caches either.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 29, 2017, 12:04 p.m.</div>
<pre class="content">
On Tue, Aug 29, 2017 at 09:59:30AM +0200, Laurent Dufour wrote:
<span class="quote">&gt; On 27/08/2017 02:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt; &gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It deserves a comment.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You&#39;re right I&#39;ll add it in the next version.</span>
<span class="quote">&gt; For the record, the root cause is that __anon_vma_prepare() requires the</span>
<span class="quote">&gt; mmap_sem to be held because vm_next and vm_prev must be safe.</span>

But should that test not be:

	if (unlikely(vma_is_anonymous(vma) &amp;&amp; !vma-&gt;anon_vma))
		goto unlock;

Because !anon vmas will never have -&gt;anon_vma set and you don&#39;t want to
exclude those.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 29, 2017, 1:18 p.m.</div>
<pre class="content">
On 29/08/2017 14:04, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Aug 29, 2017 at 09:59:30AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; On 27/08/2017 02:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt;&gt;&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It deserves a comment.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; You&#39;re right I&#39;ll add it in the next version.</span>
<span class="quote">&gt;&gt; For the record, the root cause is that __anon_vma_prepare() requires the</span>
<span class="quote">&gt;&gt; mmap_sem to be held because vm_next and vm_prev must be safe.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But should that test not be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (unlikely(vma_is_anonymous(vma) &amp;&amp; !vma-&gt;anon_vma))</span>
<span class="quote">&gt; 		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because !anon vmas will never have -&gt;anon_vma set and you don&#39;t want to</span>
<span class="quote">&gt; exclude those.</span>

Yes in the case we later allow non anonymous vmas to be handled.
Currently only anonymous vmas are supported so the check is good enough,
isn&#39;t it ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 29, 2017, 1:45 p.m.</div>
<pre class="content">
On Tue, Aug 29, 2017 at 03:18:25PM +0200, Laurent Dufour wrote:
<span class="quote">&gt; On 29/08/2017 14:04, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; On Tue, Aug 29, 2017 at 09:59:30AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt;&gt; On 27/08/2017 02:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; It deserves a comment.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; You&#39;re right I&#39;ll add it in the next version.</span>
<span class="quote">&gt; &gt;&gt; For the record, the root cause is that __anon_vma_prepare() requires the</span>
<span class="quote">&gt; &gt;&gt; mmap_sem to be held because vm_next and vm_prev must be safe.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But should that test not be:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (unlikely(vma_is_anonymous(vma) &amp;&amp; !vma-&gt;anon_vma))</span>
<span class="quote">&gt; &gt; 		goto unlock;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because !anon vmas will never have -&gt;anon_vma set and you don&#39;t want to</span>
<span class="quote">&gt; &gt; exclude those.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes in the case we later allow non anonymous vmas to be handled.</span>
<span class="quote">&gt; Currently only anonymous vmas are supported so the check is good enough,</span>
<span class="quote">&gt; isn&#39;t it ?</span>

That wasn&#39;t at all clear from reading the code. This makes it clear
-&gt;anon_vma is only ever looked at for anonymous.

And like Kirill says, we _really_ should start allowing some (if not
all) vm_ops. Large file based mappings aren&#39;t particularly rare.

I&#39;m not sure we want to introduce a white-list or just bite the bullet
and audit all -&gt;fault() implementations. But either works and isn&#39;t
terribly difficult, auditing all is more work though.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=245">Benjamin Herrenschmidt</a> - Aug. 29, 2017, 9:19 p.m.</div>
<pre class="content">
On Tue, 2017-08-29 at 13:27 +0200, Peter Zijlstra wrote:
<span class="quote">&gt; mpe helped me out and explained that is the PWC hint to TBLIE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, you set need_flush_all when you unhook pud/pmd/pte which you then</span>
<span class="quote">&gt; use to set PWC. So free_pgtables() will do the PWC when it unhooks</span>
<span class="quote">&gt; higher level pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But you&#39;re right that there&#39;s some issues, free_pgtables() itself</span>
<span class="quote">&gt; doesn&#39;t seem to use mm-&gt;page_table_lock,pmd-&gt;lock _AT_ALL_ to unhook the</span>
<span class="quote">&gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If it were to do that, things should work fine since those locks would</span>
<span class="quote">&gt; then serialize against the speculative faults, we would never install a</span>
<span class="quote">&gt; page if the VMA would be under tear-down and it would thus not be</span>
<span class="quote">&gt; visible to your caches either.</span>

That&#39;s one case. I don&#39;t remember of *all* the cases to be honest, but
I do remember several times over the past few years thinking &quot;ah we are
fine because the mm sem taken for writing protects us from any
concurrent tree structure change&quot; :-)

Cheers,
Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Aug. 30, 2017, 3:48 a.m.</div>
<pre class="content">
On 08/29/2017 05:34 PM, Peter Zijlstra wrote:
<span class="quote">&gt; On Tue, Aug 29, 2017 at 09:59:30AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; On 27/08/2017 02:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt;&gt;&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt;&gt; It deserves a comment.</span>
<span class="quote">&gt;&gt; You&#39;re right I&#39;ll add it in the next version.</span>
<span class="quote">&gt;&gt; For the record, the root cause is that __anon_vma_prepare() requires the</span>
<span class="quote">&gt;&gt; mmap_sem to be held because vm_next and vm_prev must be safe.</span>
<span class="quote">&gt; But should that test not be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (unlikely(vma_is_anonymous(vma) &amp;&amp; !vma-&gt;anon_vma))</span>
<span class="quote">&gt; 		goto unlock;</span>

This makes more sense. We are backing off from speculative path
because struct anon_vma has not been created for this anonymous
vma and we cannot do that without holding mmap_sem. This should
have nothing to do with vma-&gt;vm_ops availability.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Aug. 30, 2017, 5:25 a.m.</div>
<pre class="content">
On 08/27/2017 05:48 AM, Kirill A. Shutemov wrote:
<span class="quote">&gt;&gt; +	/* Transparent huge pages are not supported. */</span>
<span class="quote">&gt;&gt; +	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt; That&#39;s looks like a blocker to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any problem with making it supported (besides plain coding)?</span>

IIUC we would have to reattempt once for each PMD level fault because
of the lack of a page table entry there. Besides do we want to support
huge pages in general as part of speculative page fault path ? The
number of faults will be very less (256 times lower on POWER and 512
times lower on X86). So is it worth it ? BTW calling hugetlb_fault()
after figuring out the VMA, works correctly inside handle_speculative
_fault() last time I checked.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - Aug. 30, 2017, 6:13 a.m.</div>
<pre class="content">
On Wed, Aug 30, 2017 at 07:19:30AM +1000, Benjamin Herrenschmidt wrote:
<span class="quote">&gt; On Tue, 2017-08-29 at 13:27 +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; mpe helped me out and explained that is the PWC hint to TBLIE.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So, you set need_flush_all when you unhook pud/pmd/pte which you then</span>
<span class="quote">&gt; &gt; use to set PWC. So free_pgtables() will do the PWC when it unhooks</span>
<span class="quote">&gt; &gt; higher level pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But you&#39;re right that there&#39;s some issues, free_pgtables() itself</span>
<span class="quote">&gt; &gt; doesn&#39;t seem to use mm-&gt;page_table_lock,pmd-&gt;lock _AT_ALL_ to unhook the</span>
<span class="quote">&gt; &gt; pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If it were to do that, things should work fine since those locks would</span>
<span class="quote">&gt; &gt; then serialize against the speculative faults, we would never install a</span>
<span class="quote">&gt; &gt; page if the VMA would be under tear-down and it would thus not be</span>
<span class="quote">&gt; &gt; visible to your caches either.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s one case. I don&#39;t remember of *all* the cases to be honest, but</span>
<span class="quote">&gt; I do remember several times over the past few years thinking &quot;ah we are</span>
<span class="quote">&gt; fine because the mm sem taken for writing protects us from any</span>
<span class="quote">&gt; concurrent tree structure change&quot; :-)</span>

Well, installing always seems to use the locks (it needs to, because its
always done with down_read()), that only leaves removal, and the only
place I know that removes stuff is free_pgtables().

But I think I found another fun place, copy_page_range(). While it
(pointlessly) takes all the PTLs on the dst mm it walks the src page
tables without any PTLs.

This means that if we have a multi-threaded process doing fork() a
thread of the src mm could instantiate page-tables that will not be
copied over.

Of course, this is highly dubious behaviour to begin with, and I don&#39;t
think there&#39;s anything fundamentally wrong with missing those pages but
we should document this stuff.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 30, 2017, 8:56 a.m.</div>
<pre class="content">
On 27/08/2017 02:18, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Fri, Aug 18, 2017 at 12:05:13AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * vm_normal_page() adds some processing which should be done while</span>
<span class="quote">&gt;&gt; + * hodling the mmap_sem.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +int handle_speculative_fault(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;&gt; +			     unsigned int flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vm_fault vmf = {</span>
<span class="quote">&gt;&gt; +		.address = address,</span>
<span class="quote">&gt;&gt; +	};</span>
<span class="quote">&gt;&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt;&gt; +	p4d_t *p4d;</span>
<span class="quote">&gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt;&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt;&gt; +	int dead, seq, idx, ret = VM_FAULT_RETRY;</span>
<span class="quote">&gt;&gt; +	struct vm_area_struct *vma;</span>
<span class="quote">&gt;&gt; +	struct mempolicy *pol;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Clear flags that may lead to release the mmap_sem to retry */</span>
<span class="quote">&gt;&gt; +	flags &amp;= ~(FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_KILLABLE);</span>
<span class="quote">&gt;&gt; +	flags |= FAULT_FLAG_SPECULATIVE;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	idx = srcu_read_lock(&amp;vma_srcu);</span>
<span class="quote">&gt;&gt; +	vma = find_vma_srcu(mm, address);</span>
<span class="quote">&gt;&gt; +	if (!vma)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Validate the VMA found by the lockless lookup.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	dead = RB_EMPTY_NODE(&amp;vma-&gt;vm_rb);</span>
<span class="quote">&gt;&gt; +	seq = raw_read_seqcount(&amp;vma-&gt;vm_sequence); /* rmb &lt;-&gt; seqlock,vma_rb_erase() */</span>
<span class="quote">&gt;&gt; +	if ((seq &amp; 1) || dead)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Can&#39;t call vm_ops service has we don&#39;t know what they would do</span>
<span class="quote">&gt;&gt; +	 * with the VMA.</span>
<span class="quote">&gt;&gt; +	 * This include huge page from hugetlbfs.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (vma-&gt;vm_ops)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we need to have a way to white-list safe -&gt;vm_ops.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It deserves a comment.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vmf.vma_flags = READ_ONCE(vma-&gt;vm_flags);</span>
<span class="quote">&gt;&gt; +	vmf.vma_page_prot = READ_ONCE(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Can&#39;t call userland page fault handler in the speculative path */</span>
<span class="quote">&gt;&gt; +	if (unlikely(vmf.vma_flags &amp; VM_UFFD_MISSING))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="quote">&gt;&gt; +	 * are not compatible with the speculative page fault processing.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	pol = __get_vma_policy(vma, address);</span>
<span class="quote">&gt;&gt; +	if (!pol)</span>
<span class="quote">&gt;&gt; +		pol = get_task_policy(current);</span>
<span class="quote">&gt;&gt; +	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (vmf.vma_flags &amp; VM_GROWSDOWN || vmf.vma_flags &amp; VM_GROWSUP)</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * This could be detected by the check address against VMA&#39;s</span>
<span class="quote">&gt;&gt; +		 * boundaries but we want to trace it as not supported instead</span>
<span class="quote">&gt;&gt; +		 * of changed.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (address &lt; READ_ONCE(vma-&gt;vm_start)</span>
<span class="quote">&gt;&gt; +	    || READ_ONCE(vma-&gt;vm_end) &lt;= address)</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The three following checks are copied from access_error from</span>
<span class="quote">&gt;&gt; +	 * arch/x86/mm/fault.c</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="quote">&gt;&gt; +				       flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="quote">&gt;&gt; +				       flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* This is one is required to check that the VMA has write access set */</span>
<span class="quote">&gt;&gt; +	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="quote">&gt;&gt; +		if (unlikely(!(vmf.vma_flags &amp; VM_WRITE)))</span>
<span class="quote">&gt;&gt; +			goto unlock;</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		if (unlikely(!(vmf.vma_flags &amp; (VM_READ | VM_EXEC | VM_WRITE))))</span>
<span class="quote">&gt;&gt; +			goto unlock;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Do a speculative lookup of the PTE entry.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	local_irq_disable();</span>
<span class="quote">&gt;&gt; +	pgd = pgd_offset(mm, address);</span>
<span class="quote">&gt;&gt; +	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="quote">&gt;&gt; +	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pud = pud_alloc(mm, p4d, address);</span>
<span class="quote">&gt;&gt; +	if (pud_none(*pud) || unlikely(pud_bad(*pud)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt;&gt; +	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * The above does not allocate/instantiate page-tables because doing so</span>
<span class="quote">&gt;&gt; +	 * would lead to the possibility of instantiating page-tables after</span>
<span class="quote">&gt;&gt; +	 * free_pgtables() -- and consequently leaking them.</span>
<span class="quote">&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt; +	 * The result is that we take at least one !speculative fault per PMD</span>
<span class="quote">&gt;&gt; +	 * in order to instantiate it.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Doing all this job and just give up because we cannot allocate page tables</span>
<span class="quote">&gt; looks very wasteful to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Have you considered to look how we can hand over from speculative to</span>
<span class="quote">&gt; non-speculative path without starting from scratch (when possible)?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +	/* Transparent huge pages are not supported. */</span>
<span class="quote">&gt;&gt; +	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt;&gt; +		goto out_walk;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s looks like a blocker to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there any problem with making it supported (besides plain coding)?</span>

This is not straight forward, as the THP are mainly handled in
__handle_mm_fault() and it is not called during the speculative path.
Having THP handled in the speculative path sounds doable but I&#39;d have to
double check all the callees deeper, and this will required either
redesigning __handle_mm_fault() or doing the job in a dedicated way in
handle_speculative_fault() .
Furthermore, we should handle both PUD and PMD&#39;s level huge pages.

This being said, I can&#39;t see any blocking issue at this time except plain
coding but I&#39;d prefer to get it done in a next step, as an optimization,
since huge page&#39;s faults are far less frequent per design.

Having _standard_ page&#39;s fault handled in a speculative way is already
providing good performance improvement, we should consider having it
upstreamed and then adding support for THP as well as other compatible
vm_ops like hugetlb, isn&#39;t it ?

Cheers,
Laurent.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	vmf.vma = vma;</span>
<span class="quote">&gt;&gt; +	vmf.pmd = pmd;</span>
<span class="quote">&gt;&gt; +	vmf.pgoff = linear_page_index(vma, address);</span>
<span class="quote">&gt;&gt; +	vmf.gfp_mask = __get_fault_gfp_mask(vma);</span>
<span class="quote">&gt;&gt; +	vmf.sequence = seq;</span>
<span class="quote">&gt;&gt; +	vmf.flags = flags;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	local_irq_enable();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * We need to re-validate the VMA after checking the bounds, otherwise</span>
<span class="quote">&gt;&gt; +	 * we might have a false positive on the bounds.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	if (read_seqcount_retry(&amp;vma-&gt;vm_sequence, seq))</span>
<span class="quote">&gt;&gt; +		goto unlock;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ret = handle_pte_fault(&amp;vmf);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +unlock:</span>
<span class="quote">&gt;&gt; +	srcu_read_unlock(&amp;vma_srcu, idx);</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +out_walk:</span>
<span class="quote">&gt;&gt; +	local_irq_enable();</span>
<span class="quote">&gt;&gt; +	goto unlock;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif /* __HAVE_ARCH_CALL_SPF */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * By the time we get here, we already hold the mm semaphore</span>
<span class="quote">&gt;&gt;   *</span>
<span class="quote">&gt;&gt; -- </span>
<span class="quote">&gt;&gt; 2.7.4</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb_inline.h b/include/linux/hugetlb_inline.h</span>
<span class="p_header">index a4e7ca0f3585..6cfdfca4cc2a 100644</span>
<span class="p_header">--- a/include/linux/hugetlb_inline.h</span>
<span class="p_header">+++ b/include/linux/hugetlb_inline.h</span>
<span class="p_chunk">@@ -7,7 +7,7 @@</span> <span class="p_context"></span>
 
 static inline bool is_vm_hugetlb_page(struct vm_area_struct *vma)
 {
<span class="p_del">-	return !!(vma-&gt;vm_flags &amp; VM_HUGETLB);</span>
<span class="p_add">+	return !!(READ_ONCE(vma-&gt;vm_flags) &amp; VM_HUGETLB);</span>
 }
 
 #else
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 0f4ddd72b172..0fe0811d304f 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -315,6 +315,7 @@</span> <span class="p_context"> struct vm_fault {</span>
 	gfp_t gfp_mask;			/* gfp mask to be used for allocations */
 	pgoff_t pgoff;			/* Logical page offset based on vma */
 	unsigned long address;		/* Faulting virtual address */
<span class="p_add">+	unsigned int sequence;</span>
 	pmd_t *pmd;			/* Pointer to pmd entry matching
 					 * the &#39;address&#39; */
 	pud_t *pud;			/* Pointer to pud entry matching
<span class="p_chunk">@@ -1297,6 +1298,10 @@</span> <span class="p_context"> int invalidate_inode_page(struct page *page);</span>
 #ifdef CONFIG_MMU
 extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		unsigned int flags);
<span class="p_add">+#ifdef __HAVE_ARCH_CALL_SPF</span>
<span class="p_add">+extern int handle_speculative_fault(struct mm_struct *mm,</span>
<span class="p_add">+				    unsigned long address, unsigned int flags);</span>
<span class="p_add">+#endif /* __HAVE_ARCH_CALL_SPF */</span>
 extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
<span class="p_header">diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h</span>
<span class="p_header">index 79b36f57c3ba..3a9735dfa6b6 100644</span>
<span class="p_header">--- a/include/linux/pagemap.h</span>
<span class="p_header">+++ b/include/linux/pagemap.h</span>
<span class="p_chunk">@@ -443,8 +443,8 @@</span> <span class="p_context"> static inline pgoff_t linear_page_index(struct vm_area_struct *vma,</span>
 	pgoff_t pgoff;
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		return linear_hugepage_index(vma, address);
<span class="p_del">-	pgoff = (address - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_del">-	pgoff += vma-&gt;vm_pgoff;</span>
<span class="p_add">+	pgoff = (address - READ_ONCE(vma-&gt;vm_start)) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	pgoff += READ_ONCE(vma-&gt;vm_pgoff);</span>
 	return pgoff;
 }
 
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 736540f15936..9d6347e35747 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -45,6 +45,20 @@</span> <span class="p_context"> extern struct srcu_struct vma_srcu;</span>
 extern struct vm_area_struct *find_vma_srcu(struct mm_struct *mm,
 					    unsigned long addr);
 
<span class="p_add">+static inline bool vma_has_changed(struct vm_fault *vmf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = RB_EMPTY_NODE(&amp;vmf-&gt;vma-&gt;vm_rb);</span>
<span class="p_add">+	unsigned seq = ACCESS_ONCE(vmf-&gt;vma-&gt;vm_sequence.sequence);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Matches both the wmb in write_seqlock_{begin,end}() and</span>
<span class="p_add">+	 * the wmb in vma_rb_erase().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret || seq != vmf-&gt;sequence;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 51bc8315281e..0ba14a5797b2 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -760,7 +760,8 @@</span> <span class="p_context"> static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,</span>
 	if (page)
 		dump_page(page, &quot;bad pte&quot;);
 	pr_alert(&quot;addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%lx\n&quot;,
<span class="p_del">-		 (void *)addr, vma-&gt;vm_flags, vma-&gt;anon_vma, mapping, index);</span>
<span class="p_add">+		 (void *)addr, READ_ONCE(vma-&gt;vm_flags), vma-&gt;anon_vma,</span>
<span class="p_add">+		 mapping, index);</span>
 	/*
 	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
 	 */
<span class="p_chunk">@@ -2285,15 +2286,69 @@</span> <span class="p_context"> static inline void wp_page_reuse(struct vm_fault *vmf)</span>
 
 static bool pte_spinlock(struct vm_fault *vmf)
 {
<span class="p_add">+	bool ret = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if vma is still valid */</span>
<span class="p_add">+	if (!(vmf-&gt;flags &amp; FAULT_FLAG_SPECULATIVE)) {</span>
<span class="p_add">+		vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="p_add">+		spin_lock(vmf-&gt;ptl);</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	if (vma_has_changed(vmf))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);
 	spin_lock(vmf-&gt;ptl);
<span class="p_del">-	return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma_has_changed(vmf)) {</span>
<span class="p_add">+		spin_unlock(vmf-&gt;ptl);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = true;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+	return ret;</span>
 }
 
 static bool pte_map_lock(struct vm_fault *vmf)
 {
<span class="p_del">-	vmf-&gt;pte = pte_offset_map_lock(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd, vmf-&gt;address, &amp;vmf-&gt;ptl);</span>
<span class="p_del">-	return true;</span>
<span class="p_add">+	bool ret = false;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(vmf-&gt;flags &amp; FAULT_FLAG_SPECULATIVE)) {</span>
<span class="p_add">+		vmf-&gt;pte = pte_offset_map_lock(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd,</span>
<span class="p_add">+					       vmf-&gt;address, &amp;vmf-&gt;ptl);</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The first vma_has_changed() guarantees the page-tables are still</span>
<span class="p_add">+	 * valid, having IRQs disabled ensures they stay around, hence the</span>
<span class="p_add">+	 * second vma_has_changed() to make sure they are still valid once</span>
<span class="p_add">+	 * we&#39;ve got the lock. After that a concurrent zap_pte_range() will</span>
<span class="p_add">+	 * block on the PTL and thus we&#39;re safe.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	if (vma_has_changed(vmf))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_map_lock(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd,</span>
<span class="p_add">+				  vmf-&gt;address, &amp;ptl);</span>
<span class="p_add">+	if (vma_has_changed(vmf)) {</span>
<span class="p_add">+		pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	vmf-&gt;pte = pte;</span>
<span class="p_add">+	vmf-&gt;ptl = ptl;</span>
<span class="p_add">+	ret = true;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+	return ret;</span>
 }
 
 /*
<span class="p_chunk">@@ -2939,6 +2994,14 @@</span> <span class="p_context"> static int do_anonymous_page(struct vm_fault *vmf)</span>
 			return VM_FAULT_RETRY;
 		if (!pte_none(*vmf-&gt;pte))
 			goto unlock;
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Don&#39;t call the userfaultfd during the speculative path.</span>
<span class="p_add">+		 * We already checked for the VMA to not be managed through</span>
<span class="p_add">+		 * userfaultfd, but it may be set in our back once we have lock</span>
<span class="p_add">+		 * the pte. In such a case we can ignore it this time.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (vmf-&gt;flags &amp; FAULT_FLAG_SPECULATIVE)</span>
<span class="p_add">+			goto setpte;</span>
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf-&gt;pte, vmf-&gt;ptl);
<span class="p_chunk">@@ -2977,7 +3040,7 @@</span> <span class="p_context"> static int do_anonymous_page(struct vm_fault *vmf)</span>
 		goto release;
 
 	/* Deliver the page fault to userland, check inside PT lock */
<span class="p_del">-	if (userfaultfd_missing(vma)) {</span>
<span class="p_add">+	if (!(vmf-&gt;flags &amp; FAULT_FLAG_SPECULATIVE) &amp;&amp; userfaultfd_missing(vma)) {</span>
 		pte_unmap_unlock(vmf-&gt;pte, vmf-&gt;ptl);
 		mem_cgroup_cancel_charge(page, memcg, false);
 		put_page(page);
<span class="p_chunk">@@ -3748,6 +3811,8 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
 	if (!vmf-&gt;pte) {
 		if (vma_is_anonymous(vmf-&gt;vma))
 			return do_anonymous_page(vmf);
<span class="p_add">+		else if (vmf-&gt;flags &amp; FAULT_FLAG_SPECULATIVE)</span>
<span class="p_add">+			return VM_FAULT_RETRY;</span>
 		else
 			return do_fault(vmf);
 	}
<span class="p_chunk">@@ -3845,6 +3910,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
 	if (!vmf.pmd)
 		return VM_FAULT_OOM;
<span class="p_add">+	vmf.sequence = raw_read_seqcount(&amp;vma-&gt;vm_sequence);</span>
 	if (pmd_none(*vmf.pmd) &amp;&amp; transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pmd(&amp;vmf);
 		if (!(ret &amp; VM_FAULT_FALLBACK))
<span class="p_chunk">@@ -3872,6 +3938,167 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	return handle_pte_fault(&amp;vmf);
 }
 
<span class="p_add">+#ifdef __HAVE_ARCH_CALL_SPF</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __HAVE_ARCH_PTE_SPECIAL</span>
<span class="p_add">+/* This is required by vm_normal_page() */</span>
<span class="p_add">+#error &quot;Speculative page fault handler requires __HAVE_ARCH_PTE_SPECIAL&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * vm_normal_page() adds some processing which should be done while</span>
<span class="p_add">+ * hodling the mmap_sem.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int handle_speculative_fault(struct mm_struct *mm, unsigned long address,</span>
<span class="p_add">+			     unsigned int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_fault vmf = {</span>
<span class="p_add">+		.address = address,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	int dead, seq, idx, ret = VM_FAULT_RETRY;</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct mempolicy *pol;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Clear flags that may lead to release the mmap_sem to retry */</span>
<span class="p_add">+	flags &amp;= ~(FAULT_FLAG_ALLOW_RETRY|FAULT_FLAG_KILLABLE);</span>
<span class="p_add">+	flags |= FAULT_FLAG_SPECULATIVE;</span>
<span class="p_add">+</span>
<span class="p_add">+	idx = srcu_read_lock(&amp;vma_srcu);</span>
<span class="p_add">+	vma = find_vma_srcu(mm, address);</span>
<span class="p_add">+	if (!vma)</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Validate the VMA found by the lockless lookup.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	dead = RB_EMPTY_NODE(&amp;vma-&gt;vm_rb);</span>
<span class="p_add">+	seq = raw_read_seqcount(&amp;vma-&gt;vm_sequence); /* rmb &lt;-&gt; seqlock,vma_rb_erase() */</span>
<span class="p_add">+	if ((seq &amp; 1) || dead)</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Can&#39;t call vm_ops service has we don&#39;t know what they would do</span>
<span class="p_add">+	 * with the VMA.</span>
<span class="p_add">+	 * This include huge page from hugetlbfs.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (vma-&gt;vm_ops)</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(!vma-&gt;anon_vma))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	vmf.vma_flags = READ_ONCE(vma-&gt;vm_flags);</span>
<span class="p_add">+	vmf.vma_page_prot = READ_ONCE(vma-&gt;vm_page_prot);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Can&#39;t call userland page fault handler in the speculative path */</span>
<span class="p_add">+	if (unlikely(vmf.vma_flags &amp; VM_UFFD_MISSING))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * MPOL_INTERLEAVE implies additional check in mpol_misplaced() which</span>
<span class="p_add">+	 * are not compatible with the speculative page fault processing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pol = __get_vma_policy(vma, address);</span>
<span class="p_add">+	if (!pol)</span>
<span class="p_add">+		pol = get_task_policy(current);</span>
<span class="p_add">+	if (pol &amp;&amp; pol-&gt;mode == MPOL_INTERLEAVE)</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vmf.vma_flags &amp; VM_GROWSDOWN || vmf.vma_flags &amp; VM_GROWSUP)</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This could be detected by the check address against VMA&#39;s</span>
<span class="p_add">+		 * boundaries but we want to trace it as not supported instead</span>
<span class="p_add">+		 * of changed.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (address &lt; READ_ONCE(vma-&gt;vm_start)</span>
<span class="p_add">+	    || READ_ONCE(vma-&gt;vm_end) &lt;= address)</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The three following checks are copied from access_error from</span>
<span class="p_add">+	 * arch/x86/mm/fault.c</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!arch_vma_access_permitted(vma, flags &amp; FAULT_FLAG_WRITE,</span>
<span class="p_add">+				       flags &amp; FAULT_FLAG_INSTRUCTION,</span>
<span class="p_add">+				       flags &amp; FAULT_FLAG_REMOTE))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This is one is required to check that the VMA has write access set */</span>
<span class="p_add">+	if (flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="p_add">+		if (unlikely(!(vmf.vma_flags &amp; VM_WRITE)))</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (unlikely(!(vmf.vma_flags &amp; (VM_READ | VM_EXEC | VM_WRITE))))</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do a speculative lookup of the PTE entry.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	pgd = pgd_offset(mm, address);</span>
<span class="p_add">+	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))</span>
<span class="p_add">+		goto out_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d)))</span>
<span class="p_add">+		goto out_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, address);</span>
<span class="p_add">+	if (pud_none(*pud) || unlikely(pud_bad(*pud)))</span>
<span class="p_add">+		goto out_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))</span>
<span class="p_add">+		goto out_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The above does not allocate/instantiate page-tables because doing so</span>
<span class="p_add">+	 * would lead to the possibility of instantiating page-tables after</span>
<span class="p_add">+	 * free_pgtables() -- and consequently leaking them.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The result is that we take at least one !speculative fault per PMD</span>
<span class="p_add">+	 * in order to instantiate it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Transparent huge pages are not supported. */</span>
<span class="p_add">+	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="p_add">+		goto out_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	vmf.vma = vma;</span>
<span class="p_add">+	vmf.pmd = pmd;</span>
<span class="p_add">+	vmf.pgoff = linear_page_index(vma, address);</span>
<span class="p_add">+	vmf.gfp_mask = __get_fault_gfp_mask(vma);</span>
<span class="p_add">+	vmf.sequence = seq;</span>
<span class="p_add">+	vmf.flags = flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We need to re-validate the VMA after checking the bounds, otherwise</span>
<span class="p_add">+	 * we might have a false positive on the bounds.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (read_seqcount_retry(&amp;vma-&gt;vm_sequence, seq))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = handle_pte_fault(&amp;vmf);</span>
<span class="p_add">+</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	srcu_read_unlock(&amp;vma_srcu, idx);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+out_walk:</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+	goto unlock;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __HAVE_ARCH_CALL_SPF */</span>
<span class="p_add">+</span>
 /*
  * By the time we get here, we already hold the mm semaphore
  *

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



