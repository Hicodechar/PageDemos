
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[25/64] mm: use mm locking wrappers - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [25/64] mm: use mm locking wrappers</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=109101">Davidlohr Bueso</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 5, 2018, 1:27 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180205012754.23615-26-dbueso@wotan.suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10199757/mbox/"
   >mbox</a>
|
   <a href="/patch/10199757/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10199757/">/patch/10199757/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5EEF960247 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:38:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 499B6283ED
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:38:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3DB2E285C4; Mon,  5 Feb 2018 01:38:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 583D5283ED
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:38:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753020AbeBEBiZ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 4 Feb 2018 20:38:25 -0500
Received: from mx2.suse.de ([195.135.220.15]:44141 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752728AbeBEB2b (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 4 Feb 2018 20:28:31 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 93743ADFA;
	Mon,  5 Feb 2018 01:28:00 +0000 (UTC)
From: Davidlohr Bueso &lt;dbueso@suse.de&gt;
To: akpm@linux-foundation.org, mingo@kernel.org
Cc: peterz@infradead.org, ldufour@linux.vnet.ibm.com, jack@suse.cz,
	mhocko@kernel.org, kirill.shutemov@linux.intel.com,
	mawilcox@microsoft.com, mgorman@techsingularity.net,
	dave@stgolabs.net, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, Davidlohr Bueso &lt;dbueso@suse.de&gt;
Subject: [PATCH 25/64] mm: use mm locking wrappers
Date: Mon,  5 Feb 2018 02:27:15 +0100
Message-Id: &lt;20180205012754.23615-26-dbueso@wotan.suse.de&gt;
X-Mailer: git-send-email 2.12.3
In-Reply-To: &lt;20180205012754.23615-1-dbueso@wotan.suse.de&gt;
References: &lt;20180205012754.23615-1-dbueso@wotan.suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=109101">Davidlohr Bueso</a> - Feb. 5, 2018, 1:27 a.m.</div>
<pre class="content">
<span class="from">From: Davidlohr Bueso &lt;dave@stgolabs.net&gt;</span>

Most of the mmap_sem users are already aware of mmrange,
making the conversion straightforward. Those who don&#39;t,
simply use the mmap_sem within the same function context.
No change in semantics.
<span class="signed-off-by">
Signed-off-by: Davidlohr Bueso &lt;dbueso@suse.de&gt;</span>
---
 mm/filemap.c           |  4 ++--
 mm/frame_vector.c      |  4 ++--
 mm/gup.c               | 16 ++++++++--------
 mm/khugepaged.c        | 35 +++++++++++++++++++----------------
 mm/memcontrol.c        | 10 +++++-----
 mm/memory.c            |  9 +++++----
 mm/mempolicy.c         | 21 +++++++++++----------
 mm/migrate.c           | 10 ++++++----
 mm/mincore.c           |  4 ++--
 mm/mmap.c              | 30 +++++++++++++++++-------------
 mm/mprotect.c          | 14 ++++++++------
 mm/mremap.c            |  4 ++--
 mm/msync.c             |  9 +++++----
 mm/nommu.c             | 23 +++++++++++++----------
 mm/oom_kill.c          |  8 ++++----
 mm/pagewalk.c          |  4 ++--
 mm/process_vm_access.c |  4 ++--
 mm/shmem.c             |  2 +-
 mm/swapfile.c          |  7 ++++---
 mm/userfaultfd.c       | 24 ++++++++++++++----------
 mm/util.c              |  9 +++++----
 21 files changed, 137 insertions(+), 114 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 6124ede79a4d..b56f93e14992 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -1303,7 +1303,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 		if (flags &amp; FAULT_FLAG_RETRY_NOWAIT)
 			return 0;
 
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, mmrange);</span>
 		if (flags &amp; FAULT_FLAG_KILLABLE)
 			wait_on_page_locked_killable(page);
 		else
<span class="p_chunk">@@ -1315,7 +1315,7 @@</span> <span class="p_context"> int __lock_page_or_retry(struct page *page, struct mm_struct *mm,</span>
 
 			ret = __lock_page_killable(page);
 			if (ret) {
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(mm, mmrange);</span>
 				return 0;
 			}
 		} else
<span class="p_header">diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="p_header">index d3dccd80c6ee..2074f6c4d6e9 100644</span>
<span class="p_header">--- a/mm/frame_vector.c</span>
<span class="p_header">+++ b/mm/frame_vector.c</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	if (WARN_ON_ONCE(nr_frames &gt; vec-&gt;nr_allocated))
 		nr_frames = vec-&gt;nr_allocated;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	locked = 1;
 	vma = find_vma_intersection(mm, start, start + 1);
 	if (!vma) {
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	} while (vma &amp;&amp; vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP));
 out:
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 	if (!ret)
 		ret = -EFAULT;
 	if (ret &gt; 0)
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 3d1b6dd11616..08d7c17e9f06 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -827,7 +827,7 @@</span> <span class="p_context"> int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
 	}
 
 	if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, mmrange);</span>
 		if (!(fault_flags &amp; FAULT_FLAG_TRIED)) {
 			*unlocked = true;
 			fault_flags &amp;= ~FAULT_FLAG_ALLOW_RETRY;
<span class="p_chunk">@@ -911,7 +911,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 */
 		*locked = 1;
 		lock_dropped = true;
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, mmrange);</span>
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
 				       pages, NULL, NULL, mmrange);
 		if (ret != 1) {
<span class="p_chunk">@@ -932,7 +932,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		 * We must let the caller know we temporarily dropped the lock
 		 * and so the critical section protected by it was lost.
 		 */
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, mmrange);</span>
 		*locked = 0;
 	}
 	return pages_done;
<span class="p_chunk">@@ -992,11 +992,11 @@</span> <span class="p_context"> long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,</span>
 	long ret;
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	ret = __get_user_pages_locked(current, mm, start, nr_pages, pages, NULL,
 				      &amp;locked, gup_flags | FOLL_TOUCH, &amp;mmrange);
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 	return ret;
 }
 EXPORT_SYMBOL(get_user_pages_unlocked);
<span class="p_chunk">@@ -1184,7 +1184,7 @@</span> <span class="p_context"> long populate_vma_page_range(struct vm_area_struct *vma,</span>
 	VM_BUG_ON(end   &amp; ~PAGE_MASK);
 	VM_BUG_ON_VMA(start &lt; vma-&gt;vm_start, vma);
 	VM_BUG_ON_VMA(end   &gt; vma-&gt;vm_end, vma);
<span class="p_del">-	VM_BUG_ON_MM(!rwsem_is_locked(&amp;mm-&gt;mmap_sem), mm);</span>
<span class="p_add">+	VM_BUG_ON_MM(!mm_is_locked(mm, mmrange), mm);</span>
 
 	gup_flags = FOLL_TOUCH | FOLL_POPULATE | FOLL_MLOCK;
 	if (vma-&gt;vm_flags &amp; VM_LOCKONFAULT)
<span class="p_chunk">@@ -1239,7 +1239,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 */
 		if (!locked) {
 			locked = 1;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, &amp;mmrange);</span>
 			vma = find_vma(mm, nstart);
 		} else if (nstart &gt;= vma-&gt;vm_end)
 			vma = vma-&gt;vm_next;
<span class="p_chunk">@@ -1271,7 +1271,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		ret = 0;
 	}
 	if (locked)
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 	return ret;	/* 0 or negative error code */
 }
 
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index 0b91ce730160..9076d26d162a 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -469,6 +469,8 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 		free_mm_slot(mm_slot);
 		mmdrop(mm);
 	} else if (mm_slot) {
<span class="p_add">+		DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
 		/*
 		 * This is required to serialize against
 		 * khugepaged_test_exit() (which is guaranteed to run
<span class="p_chunk">@@ -477,8 +479,8 @@</span> <span class="p_context"> void __khugepaged_exit(struct mm_struct *mm)</span>
 		 * khugepaged has finished working on the pagetables
 		 * under the mmap_sem.
 		 */
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;mmrange);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;mmrange);</span>
 	}
 }
 
<span class="p_chunk">@@ -902,7 +904,7 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 
 		/* do_swap_page returns VM_FAULT_RETRY with released mmap_sem */
 		if (ret &amp; VM_FAULT_RETRY) {
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		        mm_read_lock(mm, mmrange);</span>
 			if (hugepage_vma_revalidate(mm, address, &amp;vmf.vma)) {
 				/* vma is no longer available, don&#39;t continue to swapin */
 				trace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, 0);
<span class="p_chunk">@@ -956,7 +958,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * sync compaction, and we do not need to hold the mmap_sem during
 	 * that. We will recheck the vma after taking it again in write mode.
 	 */
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+        mm_read_unlock(mm, mmrange);</span>
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
 	if (!new_page) {
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
<span class="p_chunk">@@ -968,11 +970,11 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+        mm_read_lock(mm, mmrange);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, mmrange);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -980,7 +982,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (!pmd) {
 		result = SCAN_PMD_NULL;
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, mmrange);</span>
 		goto out_nolock;
 	}
 
<span class="p_chunk">@@ -991,17 +993,17 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 */
 	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced, mmrange)) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, mmrange);</span>
 		goto out_nolock;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+        mm_read_unlock(mm, mmrange);</span>
 	/*
 	 * Prevent all access to pagetables with the exception of
 	 * gup_fast later handled by the ptep_clear_flush and the VM
 	 * handled by the anon_vma lock + PG_lock.
 	 */
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, mmrange);</span>
 	result = hugepage_vma_revalidate(mm, address, &amp;vma);
 	if (result)
 		goto out;
<span class="p_chunk">@@ -1084,7 +1086,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	khugepaged_pages_collapsed++;
 	result = SCAN_SUCCEED;
 out_up_write:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, mmrange);</span>
 out_nolock:
 	trace_mm_collapse_huge_page(mm, isolated, result);
 	return;
<span class="p_chunk">@@ -1249,6 +1251,7 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 	struct vm_area_struct *vma;
 	unsigned long addr;
 	pmd_t *pmd, _pmd;
<span class="p_add">+ 	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	i_mmap_lock_write(mapping);
 	vma_interval_tree_foreach(vma, &amp;mapping-&gt;i_mmap, pgoff, pgoff) {
<span class="p_chunk">@@ -1269,12 +1272,12 @@</span> <span class="p_context"> static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)</span>
 		 * re-fault. Not ideal, but it&#39;s more important to not disturb
 		 * the system too much.
 		 */
<span class="p_del">-		if (down_write_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem)) {</span>
<span class="p_add">+		if (mm_write_trylock(vma-&gt;vm_mm, &amp;mmrange)) {</span>
 			spinlock_t *ptl = pmd_lock(vma-&gt;vm_mm, pmd);
 			/* assume page table is clear */
 			_pmd = pmdp_collapse_flush(vma, addr, pmd);
 			spin_unlock(ptl);
<span class="p_del">-			up_write(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+		        mm_write_unlock(vma-&gt;vm_mm, &amp;mmrange);</span>
 			mm_dec_nr_ptes(vma-&gt;vm_mm);
 			pte_free(vma-&gt;vm_mm, pmd_pgtable(_pmd));
 		}
<span class="p_chunk">@@ -1684,7 +1687,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 	 * the next mm on the list.
 	 */
 	vma = NULL;
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mm-&gt;mmap_sem)))</span>
<span class="p_add">+	if (unlikely(!mm_read_trylock(mm, &amp;mmrange)))</span>
 		goto breakouterloop_mmap_sem;
 	if (likely(!khugepaged_test_exit(mm)))
 		vma = find_vma(mm, khugepaged_scan.address);
<span class="p_chunk">@@ -1729,7 +1732,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 				if (!shmem_huge_enabled(vma))
 					goto skip;
 				file = get_file(vma-&gt;vm_file);
<span class="p_del">-				up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(mm, &amp;mmrange);</span>
 				ret = 1;
 				khugepaged_scan_shmem(mm, file-&gt;f_mapping,
 						pgoff, hpage);
<span class="p_chunk">@@ -1750,7 +1753,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 		}
 	}
 breakouterloop:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem); /* exit_mmap will destroy ptes after this */</span>
<span class="p_add">+        mm_read_unlock(mm, &amp;mmrange); /* exit_mmap will destroy ptes after this */</span>
 breakouterloop_mmap_sem:
 
 	spin_lock(&amp;khugepaged_mm_lock);
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index a7ac5a14b22e..699d35ffee1a 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4916,16 +4916,16 @@</span> <span class="p_context"> static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
 static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)
 {
 	unsigned long precharge;
<span class="p_del">-	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	struct mm_walk mem_cgroup_count_precharge_walk = {
 		.pmd_entry = mem_cgroup_count_precharge_pte_range,
 		.mm = mm,
 	};
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	walk_page_range(0, mm-&gt;highest_vm_end,
 			&amp;mem_cgroup_count_precharge_walk, &amp;mmrange);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 
 	precharge = mc.precharge;
 	mc.precharge = 0;
<span class="p_chunk">@@ -5211,7 +5211,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	atomic_inc(&amp;mc.from-&gt;moving_account);
 	synchronize_rcu();
 retry:
<span class="p_del">-	if (unlikely(!down_read_trylock(&amp;mc.mm-&gt;mmap_sem))) {</span>
<span class="p_add">+	if (unlikely(!mm_read_trylock(mc.mm, &amp;mmrange))) {</span>
 		/*
 		 * Someone who are holding the mmap_sem might be waiting in
 		 * waitq. So we cancel all extra charges, wake up all waiters,
<span class="p_chunk">@@ -5230,7 +5230,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk,
 			&amp;mmrange);
 
<span class="p_del">-	up_read(&amp;mc.mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mc.mm, &amp;mmrange);</span>
 	atomic_dec(&amp;mc.from-&gt;moving_account);
 }
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 598a8c69e3d3..e3bf2879f7c3 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -4425,7 +4425,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 	int write = gup_flags &amp; FOLL_WRITE;
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
<span class="p_chunk">@@ -4474,7 +4474,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		buf += bytes;
 		addr += bytes;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 
 	return buf - old_buf;
 }
<span class="p_chunk">@@ -4525,11 +4525,12 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * we might be running from an atomic context so we cannot sleep
 	 */
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;mmrange))</span>
 		return;
 
 	vma = find_vma(mm, ip);
<span class="p_chunk">@@ -4548,7 +4549,7 @@</span> <span class="p_context"> void print_vma_addr(char *prefix, unsigned long ip)</span>
 			free_page((unsigned long)buf);
 		}
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 }
 
 #if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 001dc176abc1..93b69c603e8d 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -378,11 +378,12 @@</span> <span class="p_context"> void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)</span>
 void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)
 {
 	struct vm_area_struct *vma;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;mmrange);</span>
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)
 		mpol_rebind_policy(vma-&gt;vm_policy, new);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 }
 
 static const struct mempolicy_operations mpol_ops[MPOL_MAX] = {
<span class="p_chunk">@@ -842,10 +843,10 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 		 * vma/shared policy at addr is NULL.  We
 		 * want to return MPOL_DEFAULT in this case.
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;mmrange);</span>
 		vma = find_vma_intersection(mm, addr, addr+1);
 		if (!vma) {
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;mmrange);</span>
 			return -EFAULT;
 		}
 		if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;get_policy)
<span class="p_chunk">@@ -895,7 +896,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
  out:
 	mpol_cond_put(pol);
 	if (vma)
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;mmrange);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -992,7 +993,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 	if (err)
 		return err;
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 
 	/*
 	 * Find a &#39;source&#39; bit set in &#39;tmp&#39; whose corresponding &#39;dest&#39;
<span class="p_chunk">@@ -1073,7 +1074,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 		if (err &lt; 0)
 			break;
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 	if (err &lt; 0)
 		return err;
 	return busy;
<span class="p_chunk">@@ -1195,12 +1196,12 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	{
 		NODEMASK_SCRATCH(scratch);
 		if (scratch) {
<span class="p_del">-			down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_write_lock(mm, &amp;mmrange);</span>
 			task_lock(current);
 			err = mpol_set_nodemask(new, nmask, scratch);
 			task_unlock(current);
 			if (err)
<span class="p_del">-				up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_write_unlock(mm, &amp;mmrange);</span>
 		} else
 			err = -ENOMEM;
 		NODEMASK_SCRATCH_FREE(scratch);
<span class="p_chunk">@@ -1229,7 +1230,7 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	} else
 		putback_movable_pages(&amp;pagelist);
 
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
  mpol_out:
 	mpol_put(new);
 	return err;
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 7a6afc34dd54..e905d2aef7fa 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1486,8 +1486,9 @@</span> <span class="p_context"> static int add_page_for_migration(struct mm_struct *mm, unsigned long addr,</span>
 	struct page *page;
 	unsigned int follflags;
 	int err;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	err = -EFAULT;
 	vma = find_vma(mm, addr);
 	if (!vma || addr &lt; vma-&gt;vm_start || !vma_migratable(vma))
<span class="p_chunk">@@ -1540,7 +1541,7 @@</span> <span class="p_context"> static int add_page_for_migration(struct mm_struct *mm, unsigned long addr,</span>
 	 */
 	put_page(page);
 out:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 	return err;
 }
 
<span class="p_chunk">@@ -1638,8 +1639,9 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 				const void __user **pages, int *status)
 {
 	unsigned long i;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 
 	for (i = 0; i &lt; nr_pages; i++) {
 		unsigned long addr = (unsigned long)(*pages);
<span class="p_chunk">@@ -1666,7 +1668,7 @@</span> <span class="p_context"> static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,</span>
 		status++;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="p_header">index a6875a34aac0..1255098449b8 100644</span>
<span class="p_header">--- a/mm/mincore.c</span>
<span class="p_header">+++ b/mm/mincore.c</span>
<span class="p_chunk">@@ -259,9 +259,9 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 		 * Do at most PAGE_SIZE entries per iteration, due to
 		 * the temporary buffer size.
 		 */
<span class="p_del">-		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(current-&gt;mm, &amp;mmrange);</span>
 		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp, &amp;mmrange);
<span class="p_del">-		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(current-&gt;mm, &amp;mmrange);</span>
 
 		if (retval &lt;= 0)
 			break;
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 8f0eb88a5d5e..e10d005f7e2f 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -191,7 +191,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	LIST_HEAD(uf);
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;mmrange))</span>
 		return -EINTR;
 
 #ifdef CONFIG_COMPAT_BRK
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 set_brk:
 	mm-&gt;brk = brk;
 	populate = newbrk &gt; oldbrk &amp;&amp; (mm-&gt;def_flags &amp; VM_LOCKED) != 0;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate)
 		mm_populate(oldbrk, newbrk - oldbrk);
<span class="p_chunk">@@ -252,7 +252,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 
 out:
 	retval = mm-&gt;brk;
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -2762,11 +2762,11 @@</span> <span class="p_context"> int vm_munmap(unsigned long start, size_t len)</span>
 	LIST_HEAD(uf);
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;mmrange))</span>
 		return -EINTR;
 
 	ret = do_munmap(mm, start, len, &amp;uf, &amp;mmrange);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	return ret;
 }
<span class="p_chunk">@@ -2808,7 +2808,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	if (pgoff + (size &gt;&gt; PAGE_SHIFT) &lt; pgoff)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;mmrange))</span>
 		return -EINTR;
 
 	vma = find_vma(mm, start);
<span class="p_chunk">@@ -2871,7 +2871,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 			    prot, flags, pgoff, &amp;populate, NULL, &amp;mmrange);
 	fput(file);
 out:
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	if (populate)
 		mm_populate(ret, populate);
 	if (!IS_ERR_VALUE(ret))
<span class="p_chunk">@@ -2882,9 +2882,11 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 static inline void verify_mm_writelocked(struct mm_struct *mm)
 {
 #ifdef CONFIG_DEBUG_VM
<span class="p_del">-	if (unlikely(down_read_trylock(&amp;mm-&gt;mmap_sem))) {</span>
<span class="p_add">+ 	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(mm_read_trylock(mm, &amp;mmrange))) {</span>
 		WARN_ON(1);
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 	}
 #endif
 }
<span class="p_chunk">@@ -2996,12 +2998,12 @@</span> <span class="p_context"> int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
 	LIST_HEAD(uf);
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(mm, &amp;mmrange))</span>
 		return -EINTR;
 
 	ret = do_brk_flags(addr, len, flags, &amp;uf, &amp;mmrange);
 	populate = ((mm-&gt;def_flags &amp; VM_LOCKED) != 0);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	if (populate &amp;&amp; !ret)
 		mm_populate(addr, len);
<span class="p_chunk">@@ -3048,6 +3050,8 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
 	if (unlikely(mm_is_oom_victim(mm))) {
<span class="p_add">+		DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
 		/*
 		 * Wait for oom_reap_task() to stop working on this
 		 * mm. Because MMF_OOM_SKIP is already set before
<span class="p_chunk">@@ -3061,8 +3065,8 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 		 * is found not NULL while holding the task_lock.
 		 */
 		set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags);
<span class="p_del">-		down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(mm, &amp;mmrange);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;mmrange);</span>
 	}
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index b84a70720319..2f39450ae959 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -424,7 +424,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 
 	reqprot = prot;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;mmrange))</span>
 		return -EINTR;
 
 	/*
<span class="p_chunk">@@ -514,7 +514,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 		prot = reqprot;
 	}
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 	return error;
 }
 
<span class="p_chunk">@@ -536,6 +536,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 {
 	int pkey;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* No flags supported yet. */
 	if (flags)
<span class="p_chunk">@@ -544,7 +545,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	if (init_val &amp; ~PKEY_ACCESS_MASK)
 		return -EINVAL;
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;mmrange);</span>
 	pkey = mm_pkey_alloc(current-&gt;mm);
 
 	ret = -ENOSPC;
<span class="p_chunk">@@ -558,17 +559,18 @@</span> <span class="p_context"> SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)</span>
 	}
 	ret = pkey;
 out:
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 	return ret;
 }
 
 SYSCALL_DEFINE1(pkey_free, int, pkey)
 {
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;mmrange);</span>
 	ret = mm_pkey_free(current-&gt;mm, pkey);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 
 	/*
 	 * We could provie warnings or errors if any VMA still
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 21a9e2a2baa2..cc56d13e5e67 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -557,7 +557,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	if (!new_len)
 		return ret;
 
<span class="p_del">-	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="p_add">+	if (mm_write_lock_killable(current-&gt;mm, &amp;mmrange))</span>
 		return -EINTR;
 
 	if (flags &amp; MREMAP_FIXED) {
<span class="p_chunk">@@ -641,7 +641,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		vm_unacct_memory(charged);
 		locked = 0;
 	}
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 	if (locked &amp;&amp; new_len &gt; old_len)
 		mm_populate(new_addr + old_len, new_len - old_len);
 	userfaultfd_unmap_complete(mm, &amp;uf_unmap_early);
<span class="p_header">diff --git a/mm/msync.c b/mm/msync.c</span>
<span class="p_header">index ef30a429623a..2524b4708e78 100644</span>
<span class="p_header">--- a/mm/msync.c</span>
<span class="p_header">+++ b/mm/msync.c</span>
<span class="p_chunk">@@ -36,6 +36,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	struct vm_area_struct *vma;
 	int unmapped_error = 0;
 	int error = -EINVAL;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (flags &amp; ~(MS_ASYNC | MS_INVALIDATE | MS_SYNC))
 		goto out;
<span class="p_chunk">@@ -55,7 +56,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 	 * If the interval [start,end) covers some unmapped address ranges,
 	 * just ignore them, but return -ENOMEM at the end.
 	 */
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	vma = find_vma(mm, start);
 	for (;;) {
 		struct file *file;
<span class="p_chunk">@@ -86,12 +87,12 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		if ((flags &amp; MS_SYNC) &amp;&amp; file &amp;&amp;
 				(vma-&gt;vm_flags &amp; VM_SHARED)) {
 			get_file(file);
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;mmrange);</span>
 			error = vfs_fsync_range(file, fstart, fend, 1);
 			fput(file);
 			if (error || start &gt;= end)
 				goto out;
<span class="p_del">-			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(mm, &amp;mmrange);</span>
 			vma = find_vma(mm, start);
 		} else {
 			if (start &gt;= end) {
<span class="p_chunk">@@ -102,7 +103,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		}
 	}
 out_unlock:
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 out:
 	return error ? : unmapped_error;
 }
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index 1805f0a788b3..575525e86961 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -187,10 +187,10 @@</span> <span class="p_context"> static long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 	long ret;
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	ret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,
 			       NULL, NULL, &amp;mmrange);
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -253,12 +253,13 @@</span> <span class="p_context"> void *vmalloc_user(unsigned long size)</span>
 	ret = __vmalloc(size, GFP_KERNEL | __GFP_ZERO, PAGE_KERNEL);
 	if (ret) {
 		struct vm_area_struct *vma;
<span class="p_add">+		DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-		down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_lock(current-&gt;mm, &amp;mmrange);</span>
 		vma = find_vma(current-&gt;mm, (unsigned long)ret);
 		if (vma)
 			vma-&gt;vm_flags |= VM_USERMAP;
<span class="p_del">-		up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 	}
 
 	return ret;
<span class="p_chunk">@@ -1651,9 +1652,9 @@</span> <span class="p_context"> int vm_munmap(unsigned long addr, size_t len)</span>
 	int ret;
 	DEFINE_RANGE_LOCK_FULL(mmrange);
 
<span class="p_del">-	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(mm, &amp;mmrange);</span>
 	ret = do_munmap(mm, addr, len, NULL, &amp;mmrange);
<span class="p_del">-	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(mm, &amp;mmrange);</span>
 	return ret;
 }
 EXPORT_SYMBOL(vm_munmap);
<span class="p_chunk">@@ -1739,10 +1740,11 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		unsigned long, new_addr)
 {
 	unsigned long ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_lock(current-&gt;mm, &amp;mmrange);</span>
 	ret = do_mremap(addr, old_len, new_len, flags, new_addr);
<span class="p_del">-	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_write_unlock(current-&gt;mm, &amp;mmrange);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1815,8 +1817,9 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 {
 	struct vm_area_struct *vma;
 	int write = gup_flags &amp; FOLL_WRITE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 
 	/* the access must start within one of the target process&#39;s mappings */
 	vma = find_vma(mm, addr);
<span class="p_chunk">@@ -1838,7 +1841,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		len = 0;
 	}
 
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 
 	return len;
 }
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 2288e1cb1bc9..6bf9cb38bfe1 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -508,7 +508,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 */
 	mutex_lock(&amp;oom_lock);
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;mmrange)) {</span>
 		ret = false;
 		trace_skip_task_reaping(tsk-&gt;pid);
 		goto unlock_oom;
<span class="p_chunk">@@ -521,7 +521,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * notifiers cannot block for unbounded amount of time
 	 */
 	if (mm_has_blockable_invalidate_notifiers(mm, &amp;mmrange)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 		schedule_timeout_idle(HZ);
 		goto unlock_oom;
 	}
<span class="p_chunk">@@ -533,7 +533,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * down_write();up_write() cycle in exit_mmap().
 	 */
 	if (test_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags)) {
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(mm, &amp;mmrange);</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
 		goto unlock_oom;
 	}
<span class="p_chunk">@@ -578,7 +578,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 
 	trace_finish_task_reaping(tsk-&gt;pid);
 unlock_oom:
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 44a2507c94fd..55a4dcc519cd 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -301,7 +301,7 @@</span> <span class="p_context"> int walk_page_range(unsigned long start, unsigned long end,</span>
 	if (!walk-&gt;mm)
 		return -EINVAL;
 
<span class="p_del">-	VM_BUG_ON_MM(!rwsem_is_locked(&amp;walk-&gt;mm-&gt;mmap_sem), walk-&gt;mm);</span>
<span class="p_add">+	VM_BUG_ON_MM(!mm_is_locked(walk-&gt;mm, mmrange), walk-&gt;mm);</span>
 
 	vma = find_vma(walk-&gt;mm, start);
 	do {
<span class="p_chunk">@@ -345,7 +345,7 @@</span> <span class="p_context"> int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk,</span>
 	if (!walk-&gt;mm)
 		return -EINVAL;
 
<span class="p_del">-	VM_BUG_ON(!rwsem_is_locked(&amp;walk-&gt;mm-&gt;mmap_sem));</span>
<span class="p_add">+	VM_BUG_ON(!mm_is_locked(walk-&gt;mm, mmrange));</span>
 	VM_BUG_ON(!vma);
 	walk-&gt;vma = vma;
 	err = walk_page_test(vma-&gt;vm_start, vma-&gt;vm_end, walk, mmrange);
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index ff6772b86195..aaccb8972f83 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -110,12 +110,12 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 		 * access remotely because task/mm might not
 		 * current/current-&gt;mm
 		 */
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;mmrange);</span>
 		pages = get_user_pages_remote(task, mm, pa, pages, flags,
 					      process_pages, NULL, &amp;locked,
 					      &amp;mmrange);
 		if (locked)
<span class="p_del">-			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(mm, &amp;mmrange);</span>
 		if (pages &lt;= 0)
 			return -EFAULT;
 
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index 1907688b75ee..8a99281bf502 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -1961,7 +1961,7 @@</span> <span class="p_context"> static int shmem_fault(struct vm_fault *vmf)</span>
 			if ((vmf-&gt;flags &amp; FAULT_FLAG_ALLOW_RETRY) &amp;&amp;
 			   !(vmf-&gt;flags &amp; FAULT_FLAG_RETRY_NOWAIT)) {
 				/* It&#39;s polite to up mmap_sem if we can */
<span class="p_del">-				up_read(&amp;vma-&gt;vm_mm-&gt;mmap_sem);</span>
<span class="p_add">+				mm_read_unlock(vma-&gt;vm_mm, vmf-&gt;lockrange);</span>
 				ret = VM_FAULT_RETRY;
 			}
 
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 006047b16814..d9c6ca32b94f 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1958,15 +1958,16 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 {
 	struct vm_area_struct *vma;
 	int ret = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
<span class="p_del">-	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {</span>
<span class="p_add">+	if (!mm_read_trylock(mm, &amp;mmrange)) {</span>
 		/*
 		 * Activate page so shrink_inactive_list is unlikely to unmap
 		 * its ptes while lock is dropped, so swapoff can make progress.
 		 */
 		activate_page(page);
 		unlock_page(page);
<span class="p_del">-		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_lock(mm, &amp;mmrange);</span>
 		lock_page(page);
 	}
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_chunk">@@ -1974,7 +1975,7 @@</span> <span class="p_context"> static int unuse_mm(struct mm_struct *mm,</span>
 			break;
 		cond_resched();
 	}
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 	return (ret &lt; 0)? ret: 0;
 }
 
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index 39791b81ede7..8ad13bea799d 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -155,7 +155,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 					      unsigned long dst_start,
 					      unsigned long src_start,
 					      unsigned long len,
<span class="p_del">-					      bool zeropage)</span>
<span class="p_add">+					      bool zeropage,</span>
<span class="p_add">+					      struct range_lock *mmrange)</span>
 {
 	int vm_alloc_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;
 	int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;
<span class="p_chunk">@@ -177,7 +178,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	 * feature is not supported.
 	 */
 	if (zeropage) {
<span class="p_del">-		up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_read_unlock(dst_mm, mmrange);</span>
 		return -EINVAL;
 	}
 
<span class="p_chunk">@@ -275,7 +276,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 		cond_resched();
 
 		if (unlikely(err == -EFAULT)) {
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(dst_mm, mmrange);</span>
 			BUG_ON(!page);
 
 			err = copy_huge_page_from_user(page,
<span class="p_chunk">@@ -285,7 +286,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 				err = -EFAULT;
 				goto out;
 			}
<span class="p_del">-			down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_lock(dst_mm, mmrange);</span>
 
 			dst_vma = NULL;
 			goto retry;
<span class="p_chunk">@@ -305,7 +306,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(dst_mm, mmrange);</span>
 out:
 	if (page) {
 		/*
<span class="p_chunk">@@ -367,7 +368,8 @@</span> <span class="p_context"> extern ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 				      unsigned long dst_start,
 				      unsigned long src_start,
 				      unsigned long len,
<span class="p_del">-				      bool zeropage);</span>
<span class="p_add">+				      bool zeropage,</span>
<span class="p_add">+				      struct range_lock *mmrange);</span>
 #endif /* CONFIG_HUGETLB_PAGE */
 
 static __always_inline ssize_t mfill_atomic_pte(struct mm_struct *dst_mm,
<span class="p_chunk">@@ -412,6 +414,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	unsigned long src_addr, dst_addr;
 	long copied;
 	struct page *page;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * Sanitize the command parameters:
<span class="p_chunk">@@ -428,7 +431,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	copied = 0;
 	page = NULL;
 retry:
<span class="p_del">-	down_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(dst_mm, &amp;mmrange);</span>
 
 	/*
 	 * Make sure the vma is not shared, that the dst range is
<span class="p_chunk">@@ -468,7 +471,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	 */
 	if (is_vm_hugetlb_page(dst_vma))
 		return  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,
<span class="p_del">-						src_start, len, zeropage);</span>
<span class="p_add">+					       src_start, len, zeropage,</span>
<span class="p_add">+					       &amp;mmrange);</span>
 
 	if (!vma_is_anonymous(dst_vma) &amp;&amp; !vma_is_shmem(dst_vma))
 		goto out_unlock;
<span class="p_chunk">@@ -523,7 +527,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 		if (unlikely(err == -EFAULT)) {
 			void *page_kaddr;
 
<span class="p_del">-			up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+			mm_read_unlock(dst_mm, &amp;mmrange);</span>
 			BUG_ON(!page);
 
 			page_kaddr = kmap(page);
<span class="p_chunk">@@ -552,7 +556,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	}
 
 out_unlock:
<span class="p_del">-	up_read(&amp;dst_mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(dst_mm, &amp;mmrange);</span>
 out:
 	if (page)
 		put_page(page);
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index b0ec1d88bb71..e17c6c74cc23 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -351,11 +351,11 @@</span> <span class="p_context"> unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,</span>
 
 	ret = security_mmap_file(file, prot, flag);
 	if (!ret) {
<span class="p_del">-		if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="p_add">+		if (mm_write_lock_killable(mm, &amp;mmrange))</span>
 			return -EINTR;
 		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
 				    &amp;populate, &amp;uf, &amp;mmrange);
<span class="p_del">-		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mm_write_unlock(mm, &amp;mmrange);</span>
 		userfaultfd_unmap_complete(mm, &amp;uf);
 		if (populate)
 			mm_populate(ret, populate);
<span class="p_chunk">@@ -715,18 +715,19 @@</span> <span class="p_context"> int get_cmdline(struct task_struct *task, char *buffer, int buflen)</span>
 	int res = 0;
 	unsigned int len;
 	struct mm_struct *mm = get_task_mm(task);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 	unsigned long arg_start, arg_end, env_start, env_end;
 	if (!mm)
 		goto out;
 	if (!mm-&gt;arg_end)
 		goto out_mm;	/* Shh! No looking before we&#39;re done */
 
<span class="p_del">-	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_lock(mm, &amp;mmrange);</span>
 	arg_start = mm-&gt;arg_start;
 	arg_end = mm-&gt;arg_end;
 	env_start = mm-&gt;env_start;
 	env_end = mm-&gt;env_end;
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	mm_read_unlock(mm, &amp;mmrange);</span>
 
 	len = arg_end - arg_start;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



