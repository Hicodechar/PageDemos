
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,1/4] Add additional range parameter to GUP() and handle_page_fault() - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,1/4] Add additional range parameter to GUP() and handle_page_fault()</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 19, 2017, 12:18 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;c538ce923eeb55020a439e0cda243bf465816b47.1492595897.git.ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9687367/mbox/"
   >mbox</a>
|
   <a href="/patch/9687367/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9687367/">/patch/9687367/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8364F60375 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 12:19:52 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 72AE72841F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 12:19:52 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6748D28427; Wed, 19 Apr 2017 12:19:52 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 373422841F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 12:19:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1763018AbdDSMTl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 19 Apr 2017 08:19:41 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:50683 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1762917AbdDSMSj (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 19 Apr 2017 08:18:39 -0400
Received: from pps.filterd (m0098413.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v3JCIcq6056566
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 19 Apr 2017 08:18:39 -0400
Received: from e06smtp10.uk.ibm.com (e06smtp10.uk.ibm.com [195.75.94.106])
	by mx0b-001b2d01.pphosted.com with ESMTP id 29x744t4h4-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 19 Apr 2017 08:18:38 -0400
Received: from localhost
	by e06smtp10.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Wed, 19 Apr 2017 13:18:36 +0100
Received: from b06cxnps4074.portsmouth.uk.ibm.com (9.149.109.196)
	by e06smtp10.uk.ibm.com (192.168.101.140) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Wed, 19 Apr 2017 13:18:33 +0100
Received: from d06av24.portsmouth.uk.ibm.com (d06av24.portsmouth.uk.ibm.com
	[9.149.105.60])
	by b06cxnps4074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v3JCIWnZ22544394; Wed, 19 Apr 2017 12:18:32 GMT
Received: from d06av24.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 5AF0B42049;
	Wed, 19 Apr 2017 13:17:32 +0100 (BST)
Received: from d06av24.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 3E98B42041;
	Wed, 19 Apr 2017 13:17:31 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.145.173.127])
	by d06av24.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Wed, 19 Apr 2017 13:17:31 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: linux-mm@kvack.org
Cc: Davidlohr Bueso &lt;dave@stgolabs.net&gt;, akpm@linux-foundation.org,
	Jan Kara &lt;jack@suse.cz&gt;, &quot;Kirill A . Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	haren@linux.vnet.ibm.com, aneesh.kumar@linux.vnet.ibm.com,
	khandual@linux.vnet.ibm.com, Paul.McKenney@us.ibm.com,
	linux-kernel@vger.kernel.org
Subject: [RFC 1/4] Add additional range parameter to GUP() and
	handle_page_fault()
Date: Wed, 19 Apr 2017 14:18:24 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
References: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
In-Reply-To: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
References: &lt;cover.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17041912-0040-0000-0000-0000036D7766
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17041912-0041-0000-0000-00002503DAF9
Message-Id: &lt;c538ce923eeb55020a439e0cda243bf465816b47.1492595897.git.ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-04-19_10:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=3
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1704190108
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - April 19, 2017, 12:18 p.m.</div>
<pre class="content">
Some functions which are called with the mmap_sem held by the caller
may want to release it. Since mmap_sem will become a range lock a
range_rwlock parameter must be added to allow the lock to freed
correctly.

This patch add the additional parameter in the caller and the callee
and also in the vm_fault structure.

Despite this additional parameter which is not used, there is no
functional change in the patch.
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/mm/copro_fault.c               |  2 +-
 arch/powerpc/mm/fault.c                     |  2 +-
 arch/x86/mm/fault.c                         |  2 +-
 arch/x86/mm/mpx.c                           |  2 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c     |  2 +-
 drivers/gpu/drm/etnaviv/etnaviv_gem.c       |  3 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c     |  2 +-
 drivers/gpu/drm/radeon/radeon_ttm.c         |  2 +-
 drivers/infiniband/core/umem.c              |  2 +-
 drivers/infiniband/core/umem_odp.c          |  2 +-
 drivers/infiniband/hw/mthca/mthca_memfree.c |  3 +-
 drivers/infiniband/hw/qib/qib_user_pages.c  |  2 +-
 drivers/infiniband/hw/usnic/usnic_uiom.c    |  2 +-
 drivers/iommu/intel-svm.c                   |  2 +-
 drivers/media/v4l2-core/videobuf-dma-sg.c   |  2 +-
 drivers/misc/mic/scif/scif_rma.c            |  2 +-
 fs/exec.c                                   |  2 +-
 fs/userfaultfd.c                            |  3 +-
 include/linux/hugetlb.h                     |  4 +--
 include/linux/mm.h                          | 21 ++++++++-----
 include/linux/pagemap.h                     |  8 +++--
 include/linux/userfaultfd_k.h               |  6 ++--
 kernel/events/uprobes.c                     |  4 +--
 kernel/futex.c                              |  2 +-
 mm/filemap.c                                |  5 ++--
 mm/frame_vector.c                           |  2 +-
 mm/gup.c                                    | 46 ++++++++++++++++-------------
 mm/hugetlb.c                                |  3 +-
 mm/internal.h                               |  3 +-
 mm/khugepaged.c                             | 17 +++++++----
 mm/ksm.c                                    |  3 +-
 mm/madvise.c                                | 14 +++++----
 mm/memory.c                                 | 12 ++++----
 mm/mempolicy.c                              |  2 +-
 mm/mmap.c                                   |  4 +--
 mm/mprotect.c                               |  2 +-
 mm/process_vm_access.c                      |  3 +-
 mm/userfaultfd.c                            |  8 +++--
 security/tomoyo/domain.c                    |  2 +-
 virt/kvm/kvm_main.c                         | 12 ++++----
 40 files changed, 130 insertions(+), 92 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">index 697b70ad1195..81fbf79d2e97 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_chunk">@@ -77,7 +77,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	}
 
 	ret = 0;
<span class="p_del">-	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0);</span>
<span class="p_add">+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL);</span>
 	if (unlikely(*flt &amp; VM_FAULT_ERROR)) {
 		if (*flt &amp; VM_FAULT_OOM) {
 			ret = -ENOMEM;
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index fd6484fc2fa9..20f470486177 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -446,7 +446,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, NULL);</span>
 
 	/*
 	 * Handle the retry right now, the mmap_sem has been released in that
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 428e31763cb9..d81cd399544a 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -1394,7 +1394,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * the fault.  Since we never set FAULT_FLAG_RETRY_NOWAIT, if
 	 * we get VM_FAULT_RETRY back, the mmap_sem has been unlocked.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, NULL);</span>
 	major |= fault &amp; VM_FAULT_MAJOR;
 
 	/*
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index cd44ae727df7..864a47193b6c 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -547,7 +547,7 @@</span> <span class="p_context"> static int mpx_resolve_fault(long __user *addr, int write)</span>
 	int nr_pages = 1;
 
 	gup_ret = get_user_pages((unsigned long)addr, nr_pages,
<span class="p_del">-			write ? FOLL_WRITE : 0,	NULL, NULL);</span>
<span class="p_add">+			write ? FOLL_WRITE : 0,	NULL, NULL, NULL);</span>
 	/*
 	 * get_user_pages() returns number of pages gotten.
 	 * 0 means we failed to fault in and get anything,
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_header">index 4c6094eefc51..14e02d3a6984 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_chunk">@@ -627,7 +627,7 @@</span> <span class="p_context"> int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)</span>
 		list_add(&amp;guptask.list, &amp;gtt-&gt;guptasks);
 		spin_unlock(&amp;gtt-&gt;guptasklock);
 
<span class="p_del">-		r = get_user_pages(userptr, num_pages, flags, p, NULL);</span>
<span class="p_add">+		r = get_user_pages(userptr, num_pages, flags, p, NULL, NULL);</span>
 
 		spin_lock(&amp;gtt-&gt;guptasklock);
 		list_del(&amp;guptask.list);
<span class="p_header">diff --git a/drivers/gpu/drm/etnaviv/etnaviv_gem.c b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">index fd56f92f3469..75ca18aaa34e 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/etnaviv/etnaviv_gem.c</span>
<span class="p_chunk">@@ -761,7 +761,8 @@</span> <span class="p_context"> static struct page **etnaviv_gem_userptr_do_get_pages(</span>
 	down_read(&amp;mm-&gt;mmap_sem);
 	while (pinned &lt; npages) {
 		ret = get_user_pages_remote(task, mm, ptr, npages - pinned,
<span class="p_del">-					    flags, pvec + pinned, NULL, NULL);</span>
<span class="p_add">+					    flags, pvec + pinned, NULL, NULL,</span>
<span class="p_add">+					    NULL);</span>
 		if (ret &lt; 0)
 			break;
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 22b46398831e..1f8e8eecb6df 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -516,7 +516,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 					 obj-&gt;userptr.ptr + pinned * PAGE_SIZE,
 					 npages - pinned,
 					 flags,
<span class="p_del">-					 pvec + pinned, NULL, NULL);</span>
<span class="p_add">+					 pvec + pinned, NULL, NULL, NULL);</span>
 				if (ret &lt; 0)
 					break;
 
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_ttm.c b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">index aaa3e80fecb4..86ced4d0092c 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_chunk">@@ -569,7 +569,7 @@</span> <span class="p_context"> static int radeon_ttm_tt_pin_userptr(struct ttm_tt *ttm)</span>
 		struct page **pages = ttm-&gt;pages + pinned;
 
 		r = get_user_pages(userptr, num_pages, write ? FOLL_WRITE : 0,
<span class="p_del">-				   pages, NULL);</span>
<span class="p_add">+				   pages, NULL, NULL);</span>
 		if (r &lt; 0)
 			goto release_pages;
 
<span class="p_header">diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c</span>
<span class="p_header">index 27f155d2df8d..0fe3bfb6839d 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 		ret = get_user_pages(cur_base,
 				     min_t(unsigned long, npages,
 					   PAGE_SIZE / sizeof (struct page *)),
<span class="p_del">-				     gup_flags, page_list, vma_list);</span>
<span class="p_add">+				     gup_flags, page_list, vma_list, NULL);</span>
 
 		if (ret &lt; 0)
 			goto out;
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index cb2742b548bb..0ac3c739a986 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -649,7 +649,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 		 */
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
<span class="p_del">-				flags, local_page_list, NULL, NULL);</span>
<span class="p_add">+				flags, local_page_list, NULL, NULL, NULL);</span>
 		up_read(&amp;owning_mm-&gt;mmap_sem);
 
 		if (npages &lt; 0)
<span class="p_header">diff --git a/drivers/infiniband/hw/mthca/mthca_memfree.c b/drivers/infiniband/hw/mthca/mthca_memfree.c</span>
<span class="p_header">index c6fe89d79248..9024f956669a 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mthca/mthca_memfree.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mthca/mthca_memfree.c</span>
<span class="p_chunk">@@ -472,7 +472,8 @@</span> <span class="p_context"> int mthca_map_user_db(struct mthca_dev *dev, struct mthca_uar *uar,</span>
 		goto out;
 	}
 
<span class="p_del">-	ret = get_user_pages(uaddr &amp; PAGE_MASK, 1, FOLL_WRITE, pages, NULL);</span>
<span class="p_add">+	ret = get_user_pages(uaddr &amp; PAGE_MASK, 1, FOLL_WRITE, pages, NULL,</span>
<span class="p_add">+			     NULL);</span>
 	if (ret &lt; 0)
 		goto out;
 
<span class="p_header">diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">index ce83ba9a12ef..c1cf13f2722a 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
 		ret = get_user_pages(start_page + got * PAGE_SIZE,
 				     num_pages - got,
 				     FOLL_WRITE | FOLL_FORCE,
<span class="p_del">-				     p + got, NULL);</span>
<span class="p_add">+				     p + got, NULL, NULL);</span>
 		if (ret &lt; 0)
 			goto bail_release;
 	}
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">index c49db7c33979..1591d0e78bfa 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_chunk">@@ -146,7 +146,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 		ret = get_user_pages(cur_base,
 					min_t(unsigned long, npages,
 					PAGE_SIZE / sizeof(struct page *)),
<span class="p_del">-					gup_flags, page_list, NULL);</span>
<span class="p_add">+					gup_flags, page_list, NULL, NULL);</span>
 
 		if (ret &lt; 0)
 			goto out;
<span class="p_header">diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c</span>
<span class="p_header">index 23c427602c55..4ba770b9cfbb 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-svm.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-svm.c</span>
<span class="p_chunk">@@ -591,7 +591,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 			goto invalid;
 
 		ret = handle_mm_fault(vma, address,
<span class="p_del">-				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0);</span>
<span class="p_add">+				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0, NULL);</span>
 		if (ret &amp; VM_FAULT_ERROR)
 			goto invalid;
 
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">index 0b5c43f7e020..b789070047df 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_chunk">@@ -186,7 +186,7 @@</span> <span class="p_context"> static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,</span>
 		data, size, dma-&gt;nr_pages);
 
 	err = get_user_pages(data &amp; PAGE_MASK, dma-&gt;nr_pages,
<span class="p_del">-			     flags, dma-&gt;pages, NULL);</span>
<span class="p_add">+			     flags, dma-&gt;pages, NULL, NULL);</span>
 
 	if (err != dma-&gt;nr_pages) {
 		dma-&gt;nr_pages = (err &gt;= 0) ? err : 0;
<span class="p_header">diff --git a/drivers/misc/mic/scif/scif_rma.c b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">index 329727e00e97..30e3c524216d 100644</span>
<span class="p_header">--- a/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">+++ b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_chunk">@@ -1401,7 +1401,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 				nr_pages,
 				(prot &amp; SCIF_PROT_WRITE) ? FOLL_WRITE : 0,
 				pinned_pages-&gt;pages,
<span class="p_del">-				NULL);</span>
<span class="p_add">+				NULL, NULL);</span>
 		up_write(&amp;mm-&gt;mmap_sem);
 		if (nr_pages != pinned_pages-&gt;nr_pages) {
 			if (try_upgrade) {
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 65145a3df065..49a3a19816f0 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -214,7 +214,7 @@</span> <span class="p_context"> static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,</span>
 	 * doing the exec and bprm-&gt;mm is the new process&#39;s mm.
 	 */
 	ret = get_user_pages_remote(current, bprm-&gt;mm, pos, 1, gup_flags,
<span class="p_del">-			&amp;page, NULL, NULL);</span>
<span class="p_add">+				    &amp;page, NULL, NULL, NULL);</span>
 	if (ret &lt;= 0)
 		return NULL;
 
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index f7555fc25877..b83117741b11 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -698,7 +698,8 @@</span> <span class="p_context"> void mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,</span>
 }
 
 bool userfaultfd_remove(struct vm_area_struct *vma,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			struct range_rwlock *range)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct userfaultfd_ctx *ctx;
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index b857fc8cc2ec..c586f0d40995 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -66,7 +66,7 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_ar</span>
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 struct page **, struct vm_area_struct **,
 			 unsigned long *, unsigned long *, long, unsigned int,
<span class="p_del">-			 int *);</span>
<span class="p_add">+			 int *, struct range_rwlock *);</span>
 void unmap_hugepage_range(struct vm_area_struct *,
 			  unsigned long, unsigned long, struct page *);
 void __unmap_hugepage_range_final(struct mmu_gather *tlb,
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> static inline unsigned long hugetlb_total_pages(void)</span>
 	return 0;
 }
 
<span class="p_del">-#define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n)	({ BUG(); 0; })</span>
<span class="p_add">+#define follow_hugetlb_page(m,v,p,vs,a,b,i,w,n,r)	({ BUG(); 0; })</span>
 #define follow_huge_addr(mm, addr, write)	ERR_PTR(-EINVAL)
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 static inline void hugetlb_report_meminfo(struct seq_file *m)
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 00a8fa7e366a..dbd77258baae 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/page_ext.h&gt;
 #include &lt;linux/err.h&gt;
 #include &lt;linux/page_ref.h&gt;
<span class="p_add">+#include &lt;linux/range_rwlock.h&gt;</span>
 
 struct mempolicy;
 struct anon_vma;
<span class="p_chunk">@@ -344,6 +345,7 @@</span> <span class="p_context"> struct vm_fault {</span>
 					 * page table to avoid allocation from
 					 * atomic context.
 					 */
<span class="p_add">+	struct range_rwlock *lockrange;	/* RW range lock interval */</span>
 };
 
 /* page entry size for vm-&gt;huge_fault() */
<span class="p_chunk">@@ -1272,13 +1274,14 @@</span> <span class="p_context"> int invalidate_inode_page(struct page *page);</span>
 
 #ifdef CONFIG_MMU
 extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags);</span>
<span class="p_add">+		unsigned int flags, struct range_rwlock *range);</span>
 extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
<span class="p_del">-			    bool *unlocked);</span>
<span class="p_add">+			    bool *unlocked, struct range_rwlock *range);</span>
 #else
 static inline int handle_mm_fault(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long address, unsigned int flags)</span>
<span class="p_add">+		unsigned long address, unsigned int flags,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	/* should never happen if there&#39;s no MMU */
 	BUG();
<span class="p_chunk">@@ -1286,7 +1289,8 @@</span> <span class="p_context"> static inline int handle_mm_fault(struct vm_area_struct *vma,</span>
 }
 static inline int fixup_user_fault(struct task_struct *tsk,
 		struct mm_struct *mm, unsigned long address,
<span class="p_del">-		unsigned int fault_flags, bool *unlocked)</span>
<span class="p_add">+		unsigned int fault_flags, bool *unlocked,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	/* should never happen if there&#39;s no MMU */
 	BUG();
<span class="p_chunk">@@ -1304,12 +1308,15 @@</span> <span class="p_context"> extern int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long start, unsigned long nr_pages,
 			    unsigned int gup_flags, struct page **pages,
<span class="p_del">-			    struct vm_area_struct **vmas, int *locked);</span>
<span class="p_add">+			    struct vm_area_struct **vmas, int *locked,</span>
<span class="p_add">+			    struct range_rwlock *range);</span>
 long get_user_pages(unsigned long start, unsigned long nr_pages,
 			    unsigned int gup_flags, struct page **pages,
<span class="p_del">-			    struct vm_area_struct **vmas);</span>
<span class="p_add">+			    struct vm_area_struct **vmas,</span>
<span class="p_add">+			    struct range_rwlock *range);</span>
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
<span class="p_del">-		    unsigned int gup_flags, struct page **pages, int *locked);</span>
<span class="p_add">+		    unsigned int gup_flags, struct page **pages, int *locked,</span>
<span class="p_add">+		    struct range_rwlock *range);</span>
 long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 		    struct page **pages, unsigned int gup_flags);
 int get_user_pages_fast(unsigned long start, int nr_pages, int write,
<span class="p_header">diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h</span>
<span class="p_header">index 84943e8057ef..a4c0d2d36d37 100644</span>
<span class="p_header">--- a/include/linux/pagemap.h</span>
<span class="p_header">+++ b/include/linux/pagemap.h</span>
<span class="p_chunk">@@ -434,7 +434,7 @@</span> <span class="p_context"> static inline pgoff_t linear_page_index(struct vm_area_struct *vma,</span>
 extern void __lock_page(struct page *page);
 extern int __lock_page_killable(struct page *page);
 extern int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
<span class="p_del">-				unsigned int flags);</span>
<span class="p_add">+				unsigned int flags, struct range_rwlock *range);</span>
 extern void unlock_page(struct page *page);
 
 static inline int trylock_page(struct page *page)
<span class="p_chunk">@@ -474,10 +474,12 @@</span> <span class="p_context"> static inline int lock_page_killable(struct page *page)</span>
  * __lock_page_or_retry().
  */
 static inline int lock_page_or_retry(struct page *page, struct mm_struct *mm,
<span class="p_del">-				     unsigned int flags)</span>
<span class="p_add">+				     unsigned int flags,</span>
<span class="p_add">+				     struct range_rwlock *range)</span>
 {
 	might_sleep();
<span class="p_del">-	return trylock_page(page) || __lock_page_or_retry(page, mm, flags);</span>
<span class="p_add">+	return trylock_page(page) || __lock_page_or_retry(page, mm, flags,</span>
<span class="p_add">+							  range);</span>
 }
 
 /*
<span class="p_header">diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h</span>
<span class="p_header">index 48a3483dccb1..9c73362cf4f6 100644</span>
<span class="p_header">--- a/include/linux/userfaultfd_k.h</span>
<span class="p_header">+++ b/include/linux/userfaultfd_k.h</span>
<span class="p_chunk">@@ -63,7 +63,8 @@</span> <span class="p_context"> extern void mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *,</span>
 
 extern bool userfaultfd_remove(struct vm_area_struct *vma,
 			       unsigned long start,
<span class="p_del">-			       unsigned long end);</span>
<span class="p_add">+			       unsigned long end,</span>
<span class="p_add">+			       struct range_rwlock *range);</span>
 
 extern int userfaultfd_unmap_prep(struct vm_area_struct *vma,
 				  unsigned long start, unsigned long end,
<span class="p_chunk">@@ -119,7 +120,8 @@</span> <span class="p_context"> static inline void mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *ctx,</span>
 
 static inline bool userfaultfd_remove(struct vm_area_struct *vma,
 				      unsigned long start,
<span class="p_del">-				      unsigned long end)</span>
<span class="p_add">+				      unsigned long end,</span>
<span class="p_add">+				      struct range_rwlock *range)</span>
 {
 	return true;
 }
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index 0e137f98a50c..dc2e5f7a8bb8 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -309,7 +309,7 @@</span> <span class="p_context"> int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
 retry:
 	/* Read the page with vaddr into memory */
 	ret = get_user_pages_remote(NULL, mm, vaddr, 1,
<span class="p_del">-			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL);</span>
<span class="p_add">+			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL, NULL);</span>
 	if (ret &lt;= 0)
 		return ret;
 
<span class="p_chunk">@@ -1720,7 +1720,7 @@</span> <span class="p_context"> static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)</span>
 	 * essentially a kernel access to the memory.
 	 */
 	result = get_user_pages_remote(NULL, mm, vaddr, 1, FOLL_FORCE, &amp;page,
<span class="p_del">-			NULL, NULL);</span>
<span class="p_add">+			NULL, NULL, NULL);</span>
 	if (result &lt; 0)
 		return result;
 
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 45858ec73941..4dd1bba09831 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -727,7 +727,7 @@</span> <span class="p_context"> static int fault_in_user_writeable(u32 __user *uaddr)</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	ret = fixup_user_fault(current, mm, (unsigned long)uaddr,
<span class="p_del">-			       FAULT_FLAG_WRITE, NULL);</span>
<span class="p_add">+			       FAULT_FLAG_WRITE, NULL, NULL);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 
 	return ret &lt; 0 ? ret : 0;
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 1694623a6289..3a5945f2fd3c 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -1053,7 +1053,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(__lock_page_killable);</span>
  * with the page locked and the mmap_sem unperturbed.
  */
 int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
<span class="p_del">-			 unsigned int flags)</span>
<span class="p_add">+			 unsigned int flags, struct range_rwlock *range)</span>
 {
 	if (flags &amp; FAULT_FLAG_ALLOW_RETRY) {
 		/*
<span class="p_chunk">@@ -2232,7 +2232,8 @@</span> <span class="p_context"> int filemap_fault(struct vm_fault *vmf)</span>
 			goto no_cached_page;
 	}
 
<span class="p_del">-	if (!lock_page_or_retry(page, vmf-&gt;vma-&gt;vm_mm, vmf-&gt;flags)) {</span>
<span class="p_add">+	if (!lock_page_or_retry(page, vmf-&gt;vma-&gt;vm_mm, vmf-&gt;flags,</span>
<span class="p_add">+				vmf-&gt;lockrange)) {</span>
 		put_page(page);
 		return ret | VM_FAULT_RETRY;
 	}
<span class="p_header">diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="p_header">index db77dcb38afd..579d1cbe039c 100644</span>
<span class="p_header">--- a/mm/frame_vector.c</span>
<span class="p_header">+++ b/mm/frame_vector.c</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 		vec-&gt;got_ref = true;
 		vec-&gt;is_pfns = false;
 		ret = get_user_pages_locked(start, nr_frames,
<span class="p_del">-			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked);</span>
<span class="p_add">+			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked, NULL);</span>
 		goto out;
 	}
 
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 04aa405350dc..b83b47804c6e 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -379,7 +379,8 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
  */
 static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
<span class="p_del">-		unsigned long address, unsigned int *flags, int *nonblocking)</span>
<span class="p_add">+		unsigned long address, unsigned int *flags, int *nonblocking,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	unsigned int fault_flags = 0;
 	int ret;
<span class="p_chunk">@@ -405,7 +406,7 @@</span> <span class="p_context"> static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,</span>
 		fault_flags |= FAULT_FLAG_TRIED;
 	}
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, fault_flags, range);</span>
 	if (ret &amp; VM_FAULT_ERROR) {
 		if (ret &amp; VM_FAULT_OOM)
 			return -ENOMEM;
<span class="p_chunk">@@ -546,7 +547,8 @@</span> <span class="p_context"> static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)</span>
 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas, int *nonblocking)</span>
<span class="p_add">+		struct vm_area_struct **vmas, int *nonblocking,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	long i = 0;
 	unsigned int page_mask;
<span class="p_chunk">@@ -589,7 +591,7 @@</span> <span class="p_context"> static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 			if (is_vm_hugetlb_page(vma)) {
 				i = follow_hugetlb_page(mm, vma, pages, vmas,
 						&amp;start, &amp;nr_pages, i,
<span class="p_del">-						gup_flags, nonblocking);</span>
<span class="p_add">+						gup_flags, nonblocking, range);</span>
 				continue;
 			}
 		}
<span class="p_chunk">@@ -605,7 +607,7 @@</span> <span class="p_context"> static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 		if (!page) {
 			int ret;
 			ret = faultin_page(tsk, vma, start, &amp;foll_flags,
<span class="p_del">-					nonblocking);</span>
<span class="p_add">+					nonblocking, range);</span>
 			switch (ret) {
 			case 0:
 				goto retry;
<span class="p_chunk">@@ -704,7 +706,7 @@</span> <span class="p_context"> static bool vma_permits_fault(struct vm_area_struct *vma,</span>
  */
 int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 		     unsigned long address, unsigned int fault_flags,
<span class="p_del">-		     bool *unlocked)</span>
<span class="p_add">+		     bool *unlocked, struct range_rwlock *range)</span>
 {
 	struct vm_area_struct *vma;
 	int ret, major = 0;
<span class="p_chunk">@@ -720,7 +722,7 @@</span> <span class="p_context"> int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
 	if (!vma_permits_fault(vma, fault_flags))
 		return -EFAULT;
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, fault_flags, range);</span>
 	major |= ret &amp; VM_FAULT_MAJOR;
 	if (ret &amp; VM_FAULT_ERROR) {
 		if (ret &amp; VM_FAULT_OOM)
<span class="p_chunk">@@ -759,6 +761,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 						struct page **pages,
 						struct vm_area_struct **vmas,
 						int *locked, bool notify_drop,
<span class="p_add">+						struct range_rwlock *range,</span>
 						unsigned int flags)
 {
 	long ret, pages_done;
<span class="p_chunk">@@ -778,7 +781,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 	lock_dropped = false;
 	for (;;) {
 		ret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,
<span class="p_del">-				       vmas, locked);</span>
<span class="p_add">+				       vmas, locked, range);</span>
 		if (!locked)
 			/* VM_FAULT_RETRY couldn&#39;t trigger, bypass */
 			return ret;
<span class="p_chunk">@@ -818,7 +821,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		lock_dropped = true;
 		down_read(&amp;mm-&gt;mmap_sem);
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
<span class="p_del">-				       pages, NULL, NULL);</span>
<span class="p_add">+				       pages, NULL, NULL, range);</span>
 		if (ret != 1) {
 			BUG_ON(ret &gt; 1);
 			if (!pages_done)
<span class="p_chunk">@@ -866,10 +869,10 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
  */
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 			   unsigned int gup_flags, struct page **pages,
<span class="p_del">-			   int *locked)</span>
<span class="p_add">+			   int *locked, struct range_rwlock *range)</span>
 {
 	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,
<span class="p_del">-				       pages, NULL, locked, true,</span>
<span class="p_add">+				       pages, NULL, locked, true, range,</span>
 				       gup_flags | FOLL_TOUCH);
 }
 EXPORT_SYMBOL(get_user_pages_locked);
<span class="p_chunk">@@ -892,7 +895,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	ret = __get_user_pages_locked(tsk, mm, start, nr_pages, pages, NULL,
<span class="p_del">-				      &amp;locked, false, gup_flags);</span>
<span class="p_add">+				      &amp;locked, false, NULL, gup_flags);</span>
 	if (locked)
 		up_read(&amp;mm-&gt;mmap_sem);
 	return ret;
<span class="p_chunk">@@ -980,10 +983,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_unlocked);</span>
 long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas, int *locked)</span>
<span class="p_add">+		struct vm_area_struct **vmas, int *locked,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
<span class="p_del">-				       locked, true,</span>
<span class="p_add">+				       locked, true, range,</span>
 				       gup_flags | FOLL_TOUCH | FOLL_REMOTE);
 }
 EXPORT_SYMBOL(get_user_pages_remote);
<span class="p_chunk">@@ -997,10 +1001,10 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_remote);</span>
  */
 long get_user_pages(unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas)</span>
<span class="p_add">+		struct vm_area_struct **vmas, struct range_rwlock *range)</span>
 {
 	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,
<span class="p_del">-				       pages, vmas, NULL, false,</span>
<span class="p_add">+				       pages, vmas, NULL, false, range,</span>
 				       gup_flags | FOLL_TOUCH);
 }
 EXPORT_SYMBOL(get_user_pages);
<span class="p_chunk">@@ -1025,7 +1029,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages);</span>
  * released.  If it&#39;s released, *@nonblocking will be set to 0.
  */
 long populate_vma_page_range(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end, int *nonblocking)</span>
<span class="p_add">+		unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="p_add">+		struct range_rwlock *range)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	unsigned long nr_pages = (end - start) / PAGE_SIZE;
<span class="p_chunk">@@ -1060,7 +1065,7 @@</span> <span class="p_context"> long populate_vma_page_range(struct vm_area_struct *vma,</span>
 	 * not result in a stack expansion that recurses back here.
 	 */
 	return __get_user_pages(current, mm, start, nr_pages, gup_flags,
<span class="p_del">-				NULL, NULL, nonblocking);</span>
<span class="p_add">+				NULL, NULL, nonblocking, range);</span>
 }
 
 /*
<span class="p_chunk">@@ -1109,7 +1114,8 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 * double checks the vma flags, so that it won&#39;t mlock pages
 		 * if the vma was already munlocked.
 		 */
<span class="p_del">-		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked);</span>
<span class="p_add">+		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked,</span>
<span class="p_add">+					      NULL);</span>
 		if (ret &lt; 0) {
 			if (ignore_errors) {
 				ret = 0;
<span class="p_chunk">@@ -1147,7 +1153,7 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
 
 	if (__get_user_pages(current, current-&gt;mm, addr, 1,
 			     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &amp;page, &amp;vma,
<span class="p_del">-			     NULL) &lt; 1)</span>
<span class="p_add">+			     NULL, NULL) &lt; 1)</span>
 		return NULL;
 	flush_cache_page(vma, addr, page_to_pfn(page));
 	return page;
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index e5828875f7bb..f63e50975017 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4089,7 +4089,8 @@</span> <span class="p_context"> int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			 struct page **pages, struct vm_area_struct **vmas,
 			 unsigned long *position, unsigned long *nr_pages,
<span class="p_del">-			 long i, unsigned int flags, int *nonblocking)</span>
<span class="p_add">+			 long i, unsigned int flags, int *nonblocking,</span>
<span class="p_add">+			 struct range_rwlock *range)</span>
 {
 	unsigned long pfn_offset;
 	unsigned long vaddr = *position;
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 266efaeaa370..dddb86c1a43b 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -278,7 +278,8 @@</span> <span class="p_context"> void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 #ifdef CONFIG_MMU
 extern long populate_vma_page_range(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end, int *nonblocking);</span>
<span class="p_add">+		unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="p_add">+		struct range_rwlock *range);</span>
 extern void munlock_vma_pages_range(struct vm_area_struct *vma,
 			unsigned long start, unsigned long end);
 static inline void munlock_vma_pages_all(struct vm_area_struct *vma)
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index ba40b7f673f4..d2b2a06f7853 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -875,7 +875,8 @@</span> <span class="p_context"> static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,</span>
 static bool __collapse_huge_page_swapin(struct mm_struct *mm,
 					struct vm_area_struct *vma,
 					unsigned long address, pmd_t *pmd,
<span class="p_del">-					int referenced)</span>
<span class="p_add">+					int referenced,</span>
<span class="p_add">+					struct range_rwlock *range)</span>
 {
 	int swapped_in = 0, ret = 0;
 	struct vm_fault vmf = {
<span class="p_chunk">@@ -884,6 +885,7 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 		.flags = FAULT_FLAG_ALLOW_RETRY,
 		.pmd = pmd,
 		.pgoff = linear_page_index(vma, address),
<span class="p_add">+		.lockrange = range,</span>
 	};
 
 	/* we only decide to swapin, if there is enough young ptes */
<span class="p_chunk">@@ -928,7 +930,8 @@</span> <span class="p_context"> static bool __collapse_huge_page_swapin(struct mm_struct *mm,</span>
 static void collapse_huge_page(struct mm_struct *mm,
 				   unsigned long address,
 				   struct page **hpage,
<span class="p_del">-				   int node, int referenced)</span>
<span class="p_add">+				   int node, int referenced,</span>
<span class="p_add">+				   struct range_rwlock *range)</span>
 {
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
<span class="p_chunk">@@ -986,7 +989,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 * If it fails, we release mmap_sem and jump out_nolock.
 	 * Continuing to collapse causes inconsistency.
 	 */
<span class="p_del">-	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced)) {</span>
<span class="p_add">+	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced,</span>
<span class="p_add">+					 range)) {</span>
 		mem_cgroup_cancel_charge(new_page, memcg, true);
 		up_read(&amp;mm-&gt;mmap_sem);
 		goto out_nolock;
<span class="p_chunk">@@ -1093,7 +1097,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 static int khugepaged_scan_pmd(struct mm_struct *mm,
 			       struct vm_area_struct *vma,
 			       unsigned long address,
<span class="p_del">-			       struct page **hpage)</span>
<span class="p_add">+			       struct page **hpage,</span>
<span class="p_add">+			       struct range_rwlock *range)</span>
 {
 	pmd_t *pmd;
 	pte_t *pte, *_pte;
<span class="p_chunk">@@ -1207,7 +1212,7 @@</span> <span class="p_context"> static int khugepaged_scan_pmd(struct mm_struct *mm,</span>
 	if (ret) {
 		node = khugepaged_find_target_node();
 		/* collapse_huge_page will return with the mmap_sem released */
<span class="p_del">-		collapse_huge_page(mm, address, hpage, node, referenced);</span>
<span class="p_add">+		collapse_huge_page(mm, address, hpage, node, referenced, range);</span>
 	}
 out:
 	trace_mm_khugepaged_scan_pmd(mm, page, writable, referenced,
<span class="p_chunk">@@ -1728,7 +1733,7 @@</span> <span class="p_context"> static unsigned int khugepaged_scan_mm_slot(unsigned int pages,</span>
 			} else {
 				ret = khugepaged_scan_pmd(mm, vma,
 						khugepaged_scan.address,
<span class="p_del">-						hpage);</span>
<span class="p_add">+						hpage, NULL);</span>
 			}
 			/* move to next address */
 			khugepaged_scan.address += HPAGE_PMD_SIZE;
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 19b4f2dea7a5..c419f53912ba 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -391,7 +391,8 @@</span> <span class="p_context"> static int break_ksm(struct vm_area_struct *vma, unsigned long addr)</span>
 			break;
 		if (PageKsm(page))
 			ret = handle_mm_fault(vma, addr,
<span class="p_del">-					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE);</span>
<span class="p_add">+					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,</span>
<span class="p_add">+					NULL);</span>
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 7a2abf0127ae..7eb62e3995ca 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -507,13 +507,14 @@</span> <span class="p_context"> static long madvise_free(struct vm_area_struct *vma,</span>
  */
 static long madvise_dontneed(struct vm_area_struct *vma,
 			     struct vm_area_struct **prev,
<span class="p_del">-			     unsigned long start, unsigned long end)</span>
<span class="p_add">+			     unsigned long start, unsigned long end,</span>
<span class="p_add">+			     struct range_rwlock *range)</span>
 {
 	*prev = vma;
 	if (!can_madv_dontneed_vma(vma))
 		return -EINVAL;
 
<span class="p_del">-	if (!userfaultfd_remove(vma, start, end)) {</span>
<span class="p_add">+	if (!userfaultfd_remove(vma, start, end, range)) {</span>
 		*prev = NULL; /* mmap_sem has been dropped, prev is stale */
 
 		down_read(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -590,7 +591,7 @@</span> <span class="p_context"> static long madvise_remove(struct vm_area_struct *vma,</span>
 	 * mmap_sem.
 	 */
 	get_file(f);
<span class="p_del">-	if (userfaultfd_remove(vma, start, end)) {</span>
<span class="p_add">+	if (userfaultfd_remove(vma, start, end, NULL) {</span>
 		/* mmap_sem was not released by userfaultfd_remove() */
 		up_read(&amp;current-&gt;mm-&gt;mmap_sem);
 	}
<span class="p_chunk">@@ -643,7 +644,8 @@</span> <span class="p_context"> static int madvise_hwpoison(int bhv, unsigned long start, unsigned long end)</span>
 
 static long
 madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
<span class="p_del">-		unsigned long start, unsigned long end, int behavior)</span>
<span class="p_add">+	    unsigned long start, unsigned long end, int behavior,</span>
<span class="p_add">+	    struct range_rwlock *range)</span>
 {
 	switch (behavior) {
 	case MADV_REMOVE:
<span class="p_chunk">@@ -659,7 +661,7 @@</span> <span class="p_context"> madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 			return madvise_free(vma, prev, start, end);
 		/* passthrough */
 	case MADV_DONTNEED:
<span class="p_del">-		return madvise_dontneed(vma, prev, start, end);</span>
<span class="p_add">+		return madvise_dontneed(vma, prev, start, end, range);</span>
 	default:
 		return madvise_behavior(vma, prev, start, end, behavior);
 	}
<span class="p_chunk">@@ -822,7 +824,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 			tmp = end;
 
 		/* Here vma-&gt;vm_start &lt;= start &lt; tmp &lt;= (end|vma-&gt;vm_end). */
<span class="p_del">-		error = madvise_vma(vma, &amp;prev, start, tmp, behavior);</span>
<span class="p_add">+		error = madvise_vma(vma, &amp;prev, start, tmp, behavior, NULL);</span>
 		if (error)
 			goto out;
 		start = tmp;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 235ba51b2fbf..745acb75b3b4 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2732,7 +2732,8 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 	}
 
 	swapcache = page;
<span class="p_del">-	locked = lock_page_or_retry(page, vma-&gt;vm_mm, vmf-&gt;flags);</span>
<span class="p_add">+	locked = lock_page_or_retry(page, vma-&gt;vm_mm, vmf-&gt;flags,</span>
<span class="p_add">+				    vmf-&gt;lockrange);</span>
 
 	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
 	if (!locked) {
<span class="p_chunk">@@ -3765,7 +3766,7 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+		unsigned int flags, struct range_rwlock *range)</span>
 {
 	struct vm_fault vmf = {
 		.vma = vma,
<span class="p_chunk">@@ -3773,6 +3774,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		.flags = flags,
 		.pgoff = linear_page_index(vma, address),
 		.gfp_mask = __get_fault_gfp_mask(vma),
<span class="p_add">+		.lockrange = range,</span>
 	};
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	pgd_t *pgd;
<span class="p_chunk">@@ -3848,7 +3850,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+		unsigned int flags, struct range_rwlock *range)</span>
 {
 	int ret;
 
<span class="p_chunk">@@ -3875,7 +3877,7 @@</span> <span class="p_context"> int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);
 	else
<span class="p_del">-		ret = __handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+		ret = __handle_mm_fault(vma, address, flags, range);</span>
 
 	if (flags &amp; FAULT_FLAG_USER) {
 		mem_cgroup_oom_disable();
<span class="p_chunk">@@ -4169,7 +4171,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		struct page *page = NULL;
 
 		ret = get_user_pages_remote(tsk, mm, addr, 1,
<span class="p_del">-				gup_flags, &amp;page, &amp;vma, NULL);</span>
<span class="p_add">+					    gup_flags, &amp;page, &amp;vma, NULL, NULL);</span>
 		if (ret &lt;= 0) {
 #ifndef CONFIG_HAVE_IOREMAP_PROT
 			break;
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 37d0b334bfe9..0658c7240e54 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -855,7 +855,7 @@</span> <span class="p_context"> static int lookup_node(unsigned long addr)</span>
 	struct page *p;
 	int err;
 
<span class="p_del">-	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL);</span>
<span class="p_add">+	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL, NULL);</span>
 	if (err &gt;= 0) {
 		err = page_to_nid(p);
 		put_page(p);
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index bfbe8856d134..cd8fa7e74784 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2380,7 +2380,7 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 	if (!prev || expand_stack(prev, addr))
 		return NULL;
 	if (prev-&gt;vm_flags &amp; VM_LOCKED)
<span class="p_del">-		populate_vma_page_range(prev, addr, prev-&gt;vm_end, NULL);</span>
<span class="p_add">+		populate_vma_page_range(prev, addr, prev-&gt;vm_end, NULL, NULL);</span>
 	return prev;
 }
 #else
<span class="p_chunk">@@ -2415,7 +2415,7 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 	if (expand_stack(vma, addr))
 		return NULL;
 	if (vma-&gt;vm_flags &amp; VM_LOCKED)
<span class="p_del">-		populate_vma_page_range(vma, addr, start, NULL);</span>
<span class="p_add">+		populate_vma_page_range(vma, addr, start, NULL, NULL);</span>
 	return vma;
 }
 #endif
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 8edd0d576254..fef798619b06 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -358,7 +358,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	 */
 	if ((oldflags &amp; (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &amp;&amp;
 			(newflags &amp; VM_WRITE)) {
<span class="p_del">-		populate_vma_page_range(vma, start, end, NULL);</span>
<span class="p_add">+		populate_vma_page_range(vma, start, end, NULL, NULL);</span>
 	}
 
 	vm_stat_account(mm, oldflags, -nrpages);
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index 8973cd231ece..fb4f2b96d488 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -111,7 +111,8 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 		 */
 		down_read(&amp;mm-&gt;mmap_sem);
 		pages = get_user_pages_remote(task, mm, pa, pages, flags,
<span class="p_del">-					      process_pages, NULL, &amp;locked);</span>
<span class="p_add">+					      process_pages, NULL, &amp;locked,</span>
<span class="p_add">+					      NULL);</span>
 		if (locked)
 			up_read(&amp;mm-&gt;mmap_sem);
 		if (pages &lt;= 0)
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index 8bcb501bce60..923a1ef22bc2 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -156,7 +156,8 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 					      unsigned long dst_start,
 					      unsigned long src_start,
 					      unsigned long len,
<span class="p_del">-					      bool zeropage)</span>
<span class="p_add">+					      bool zeropage,</span>
<span class="p_add">+					      struct range_rwlock *range)</span>
 {
 	int vm_alloc_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;
 	int vm_shared = dst_vma-&gt;vm_flags &amp; VM_SHARED;
<span class="p_chunk">@@ -368,7 +369,8 @@</span> <span class="p_context"> extern ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,</span>
 				      unsigned long dst_start,
 				      unsigned long src_start,
 				      unsigned long len,
<span class="p_del">-				      bool zeropage);</span>
<span class="p_add">+				      bool zeropage,</span>
<span class="p_add">+				      struct range_rwlock *range);</span>
 #endif /* CONFIG_HUGETLB_PAGE */
 
 static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,
<span class="p_chunk">@@ -439,7 +441,7 @@</span> <span class="p_context"> static __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,</span>
 	 */
 	if (is_vm_hugetlb_page(dst_vma))
 		return  __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,
<span class="p_del">-						src_start, len, zeropage);</span>
<span class="p_add">+					       src_start, len, zeropage, NULL);</span>
 
 	if (!vma_is_anonymous(dst_vma) &amp;&amp; !vma_is_shmem(dst_vma))
 		goto out_unlock;
<span class="p_header">diff --git a/security/tomoyo/domain.c b/security/tomoyo/domain.c</span>
<span class="p_header">index 00d223e9fb37..d2ef438ee887 100644</span>
<span class="p_header">--- a/security/tomoyo/domain.c</span>
<span class="p_header">+++ b/security/tomoyo/domain.c</span>
<span class="p_chunk">@@ -883,7 +883,7 @@</span> <span class="p_context"> bool tomoyo_dump_page(struct linux_binprm *bprm, unsigned long pos,</span>
 	 * the execve().
 	 */
 	if (get_user_pages_remote(current, bprm-&gt;mm, pos, 1,
<span class="p_del">-				FOLL_FORCE, &amp;page, NULL, NULL) &lt;= 0)</span>
<span class="p_add">+				  FOLL_FORCE, &amp;page, NULL, NULL, NULL) &lt;= 0)</span>
 		return false;
 #else
 	page = bprm-&gt;page[pos / PAGE_SIZE];
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 88257b311cb5..43b8a01ac131 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -1354,14 +1354,14 @@</span> <span class="p_context"> static int get_user_page_nowait(unsigned long start, int write,</span>
 	if (write)
 		flags |= FOLL_WRITE;
 
<span class="p_del">-	return get_user_pages(start, 1, flags, page, NULL);</span>
<span class="p_add">+	return get_user_pages(start, 1, flags, page, NULL, NULL);</span>
 }
 
 static inline int check_user_page_hwpoison(unsigned long addr)
 {
 	int rc, flags = FOLL_HWPOISON | FOLL_WRITE;
 
<span class="p_del">-	rc = get_user_pages(addr, 1, flags, NULL, NULL);</span>
<span class="p_add">+	rc = get_user_pages(addr, 1, flags, NULL, NULL, NULL);</span>
 	return rc == -EHWPOISON;
 }
 
<span class="p_chunk">@@ -1458,7 +1458,8 @@</span> <span class="p_context"> static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
 
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
<span class="p_del">-			       bool write_fault, kvm_pfn_t *p_pfn)</span>
<span class="p_add">+			       bool write_fault, kvm_pfn_t *p_pfn,</span>
<span class="p_add">+			       struct range_rwlock *range)</span>
 {
 	unsigned long pfn;
 	int r;
<span class="p_chunk">@@ -1472,7 +1473,7 @@</span> <span class="p_context"> static int hva_to_pfn_remapped(struct vm_area_struct *vma,</span>
 		bool unlocked = false;
 		r = fixup_user_fault(current, current-&gt;mm, addr,
 				     (write_fault ? FAULT_FLAG_WRITE : 0),
<span class="p_del">-				     &amp;unlocked);</span>
<span class="p_add">+				     &amp;unlocked, range);</span>
 		if (unlocked)
 			return -EAGAIN;
 		if (r)
<span class="p_chunk">@@ -1549,7 +1550,8 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	if (vma == NULL)
 		pfn = KVM_PFN_ERR_FAULT;
 	else if (vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP)) {
<span class="p_del">-		r = hva_to_pfn_remapped(vma, addr, async, write_fault, &amp;pfn);</span>
<span class="p_add">+		r = hva_to_pfn_remapped(vma, addr, async, write_fault, &amp;pfn,</span>
<span class="p_add">+					NULL);</span>
 		if (r == -EAGAIN)
 			goto retry;
 		if (r &lt; 0)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



