
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at PGD level</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 19, 2017, 3:27 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170419032759.29700-1-khandual@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9686791/mbox/"
   >mbox</a>
|
   <a href="/patch/9686791/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9686791/">/patch/9686791/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	966656037E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 03:29:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 86F1520223
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 03:29:11 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7BD9B283F1; Wed, 19 Apr 2017 03:29:11 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 08F1720223
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 19 Apr 2017 03:29:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1759403AbdDSD25 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 18 Apr 2017 23:28:57 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:39901 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1758837AbdDSD2z (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 18 Apr 2017 23:28:55 -0400
Received: from pps.filterd (m0098409.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v3J3SdPW007703
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 18 Apr 2017 23:28:54 -0400
Received: from e23smtp02.au.ibm.com (e23smtp02.au.ibm.com [202.81.31.144])
	by mx0a-001b2d01.pphosted.com with ESMTP id 29wqpt9ukt-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 18 Apr 2017 23:28:54 -0400
Received: from localhost
	by e23smtp02.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;khandual@linux.vnet.ibm.com&gt;;
	Wed, 19 Apr 2017 13:28:51 +1000
Received: from d23relay06.au.ibm.com (202.81.31.225)
	by e23smtp02.au.ibm.com (202.81.31.208) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Wed, 19 Apr 2017 13:28:51 +1000
Received: from d23av01.au.ibm.com (d23av01.au.ibm.com [9.190.234.96])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	v3J3Sg0i65929352
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 19 Apr 2017 13:28:50 +1000
Received: from d23av01.au.ibm.com (localhost [127.0.0.1])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	v3J3SHxE031205
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 19 Apr 2017 13:28:17 +1000
Received: from dhcp-9-202-26-78.in.ibm.com ([9.202.26.78])
	by d23av01.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	v3J3SFMR030971; Wed, 19 Apr 2017 13:28:15 +1000
From: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: n-horiguchi@ah.jp.nec.com, akpm@linux-foundation.org,
	aneesh.kumar@linux.vnet.ibm.com
Subject: [RFC] mm/madvise: Enable (soft|hard) offline of HugeTLB pages at
	PGD level
Date: Wed, 19 Apr 2017 08:57:59 +0530
X-Mailer: git-send-email 2.9.3
X-TM-AS-MML: disable
x-cbid: 17041903-0004-0000-0000-000001F632CE
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17041903-0005-0000-0000-000009EA5015
Message-Id: &lt;20170419032759.29700-1-khandual@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-04-19_02:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=10
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1704190032
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 19, 2017, 3:27 a.m.</div>
<pre class="content">
Though migrating gigantic HugeTLB pages does not sound much like real
world use case, they can be affected by memory errors. Hence migration
at the PGD level HugeTLB pages should be supported just to enable soft
and hard offline use cases.

While allocating the new gigantic HugeTLB page, it should not matter
whether new page comes from the same node or not. There would be very
few gigantic pages on the system afterall, we should not be bothered
about node locality when trying to save a big page from crashing.

This introduces a new HugeTLB allocator called alloc_gigantic_page()
which will scan over all online nodes on the system and allocate a
single HugeTLB page.
<span class="signed-off-by">
Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
---
Tested on a POWER8 machine with 16GB pages along with Aneesh&#39;s
recent HugeTLB enablement patch series on powerpc which can
be found here.

https://lkml.org/lkml/2017/4/17/225

Here, we directly call alloc_gigantic_page() which ignores node
locality. But we can also first call normal alloc_huge_page()
with the node number and if that fails to allocate then call
alloc_gigantic_page() as a fallback option.

 include/linux/hugetlb.h |  8 +++++++-
 mm/hugetlb.c            | 17 +++++++++++++++++
 mm/memory-failure.c     |  8 ++++++--
 3 files changed, 30 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - April 19, 2017, 6:20 a.m.</div>
<pre class="content">
Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt; writes:
<span class="quote">
&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt; and hard offline use cases.</span>

In that case do we want to isolated the entire 16GB range ? Should we
just dequeue the page from hugepage pool convert them to regular 64K
pages and then isolate the 64K that had memory error ?
<span class="quote">
&gt;</span>
<span class="quote">&gt; While allocating the new gigantic HugeTLB page, it should not matter</span>
<span class="quote">&gt; whether new page comes from the same node or not. There would be very</span>
<span class="quote">&gt; few gigantic pages on the system afterall, we should not be bothered</span>
<span class="quote">&gt; about node locality when trying to save a big page from crashing.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This introduces a new HugeTLB allocator called alloc_gigantic_page()</span>
<span class="quote">&gt; which will scan over all online nodes on the system and allocate a</span>
<span class="quote">&gt; single HugeTLB page.</span>
<span class="quote">&gt;</span>


-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 19, 2017, 6:42 a.m.</div>
<pre class="content">
On 04/19/2017 11:50 AM, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt;&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt;&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt;&gt; and hard offline use cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In that case do we want to isolated the entire 16GB range ? Should we</span>
<span class="quote">&gt; just dequeue the page from hugepage pool convert them to regular 64K</span>
<span class="quote">&gt; pages and then isolate the 64K that had memory error ?</span>

Though its a better thing to do, assuming that we can actually dequeue
the huge page and push it to the buddy allocator as normal 64K pages
(need to check on this as the original allocation happened from the
memblock instead of the buddy allocator, guess it should be possible
given that we do similar stuff during memory hot plug). In that case
we will also have to consider the same for the PMD based HugeTLB pages
as well or it should be only for these gigantic huge pages ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 20, 2017, 5:05 a.m.</div>
<pre class="content">
On 04/19/2017 12:12 PM, Anshuman Khandual wrote:
<span class="quote">&gt; On 04/19/2017 11:50 AM, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Though migrating gigantic HugeTLB pages does not sound much like real</span>
<span class="quote">&gt;&gt;&gt; world use case, they can be affected by memory errors. Hence migration</span>
<span class="quote">&gt;&gt;&gt; at the PGD level HugeTLB pages should be supported just to enable soft</span>
<span class="quote">&gt;&gt;&gt; and hard offline use cases.</span>
<span class="quote">&gt;&gt; In that case do we want to isolated the entire 16GB range ? Should we</span>
<span class="quote">&gt;&gt; just dequeue the page from hugepage pool convert them to regular 64K</span>
<span class="quote">&gt;&gt; pages and then isolate the 64K that had memory error ?</span>
<span class="quote">&gt; Though its a better thing to do, assuming that we can actually dequeue</span>
<span class="quote">&gt; the huge page and push it to the buddy allocator as normal 64K pages</span>
<span class="quote">&gt; (need to check on this as the original allocation happened from the</span>
<span class="quote">&gt; memblock instead of the buddy allocator, guess it should be possible</span>
<span class="quote">&gt; given that we do similar stuff during memory hot plug). In that case</span>
<span class="quote">&gt; we will also have to consider the same for the PMD based HugeTLB pages</span>
<span class="quote">&gt; as well or it should be only for these gigantic huge pages ?</span>

If we look at the code inside the function soft_offline_huge_page(),
if the source huge page has been freed to the active_freelist then
we mark the *entire* hugepage as poisoned but if the huge page has
been released back to the buddy allocator then only the page in
question is marked poisoned not the entire huge page. This was
part was added with the commit a49ecbcd7 (&quot;mm/memory-failure.c:
recheck PageHuge() after hugetlb page migrate successfully&quot;). But
when I look at the migrate_pages() handling of huge pages, it always
calls putback_active_hugepage() after successful migration to release
the huge page back the active list not to the buddy allocator. I am 
not sure if the second half of &#39;if&#39; block is ever getting executed
at all.

I am starting to wonder whats the point of releasing the huge page
to the active list in migrate_pages() when we will go and mark the
entire huge page as *poisoned*, put it in a dangling state (page-&gt;lru
pointing to itself) which can not be allocated anyway.

After migrate_pages() is successful and the source huge page is
release to the active list. We just mark the single normal page
has poisoned, get the source page from the active list and free
it to the buddy allocator. This should just take care both PMD
and PGD based huge pages.

----------------------------------------------------------------------
ret = migrate_pages(&amp;pagelist, new_page, NULL, MPOL_MF_MOVE_ALL,
				MIGRATE_SYNC, MR_MEMORY_FAILURE);
if (ret) {
	pr_info(&quot;soft offline: %#lx: migration failed %d, type %lx\n&quot;,
		pfn, ret, page-&gt;flags);
	/*
	 * We know that soft_offline_huge_page() tries to migrate
	 * only one hugepage pointed to by hpage, so we need not
	 * run through the pagelist here.
	 */
	putback_active_hugepage(hpage);
	if (ret &gt; 0)
		ret = -EIO;
} else {
	/* overcommit hugetlb page will be freed to buddy */
	if (PageHuge(page)) {
		set_page_hwpoison_huge_page(hpage);
		dequeue_hwpoisoned_huge_page(hpage);
		num_poisoned_pages_add(1 &lt;&lt; compound_order(hpage));
	} else {
		SetPageHWPoison(page);
		num_poisoned_pages_inc();
	}
}
----------------------------------------------------------------------
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 04b73a9c8b4b..ee75197e6ed8 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -347,6 +347,7 @@</span> <span class="p_context"> struct huge_bootmem_page {</span>
 
 struct page *alloc_huge_page(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_add">+struct page *alloc_gigantic_page(struct hstate *h);</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_chunk">@@ -473,7 +474,11 @@</span> <span class="p_context"> extern int dissolve_free_huge_pages(unsigned long start_pfn,</span>
 static inline bool hugepage_migration_supported(struct hstate *h)
 {
 #ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION
<span class="p_del">-	return huge_page_shift(h) == PMD_SHIFT;</span>
<span class="p_add">+	if ((huge_page_shift(h) == PMD_SHIFT) ||</span>
<span class="p_add">+		(huge_page_shift(h) == PGDIR_SHIFT))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return false;</span>
 #else
 	return false;
 #endif
<span class="p_chunk">@@ -511,6 +516,7 @@</span> <span class="p_context"> static inline void hugetlb_count_sub(long l, struct mm_struct *mm)</span>
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
<span class="p_add">+#define alloc_gigantic_page(h) NULL</span>
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 97a44db06850..f2b31dddb1bc 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1669,6 +1669,23 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);
 }
 
<span class="p_add">+struct page *alloc_gigantic_page(struct hstate *h)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	int nid = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;hugetlb_lock);</span>
<span class="p_add">+	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="p_add">+		for_each_online_node(nid) {</span>
<span class="p_add">+			page = dequeue_huge_page_node(h, nid);</span>
<span class="p_add">+			if (page)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(&amp;hugetlb_lock);</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * This allocation function is useful in the context where vma is irrelevant.
  * E.g. soft-offlining uses this function because it only cares physical
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index fe64d7729a8e..619650969fe5 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -1481,11 +1481,15 @@</span> <span class="p_context"> EXPORT_SYMBOL(unpoison_memory);</span>
 static struct page *new_page(struct page *p, unsigned long private, int **x)
 {
 	int nid = page_to_nid(p);
<span class="p_del">-	if (PageHuge(p))</span>
<span class="p_add">+	if (PageHuge(p)) {</span>
<span class="p_add">+		if (hstate_is_gigantic(page_hstate(compound_head(p))))</span>
<span class="p_add">+			return alloc_gigantic_page(page_hstate(compound_head(p)));</span>
<span class="p_add">+</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 						   nid);
<span class="p_del">-	else</span>
<span class="p_add">+	} else {</span>
 		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);
<span class="p_add">+	}</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



