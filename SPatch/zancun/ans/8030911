
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 fixes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 fixes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 14, 2016, 10:16 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160114101615.GA16926@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8030911/mbox/"
   >mbox</a>
|
   <a href="/patch/8030911/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8030911/">/patch/8030911/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 6A1189F3F6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 Jan 2016 10:16:48 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4347120497
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 Jan 2016 10:16:45 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0A49D2045A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 Jan 2016 10:16:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753544AbcANKQb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 14 Jan 2016 05:16:31 -0500
Received: from mail-wm0-f65.google.com ([74.125.82.65]:34670 &quot;EHLO
	mail-wm0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753523AbcANKQU (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 14 Jan 2016 05:16:20 -0500
Received: by mail-wm0-f65.google.com with SMTP id b14so42435177wmb.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 14 Jan 2016 02:16:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version:content-type
	:content-disposition:user-agent;
	bh=N45FHfXEXS5T+qRf2btnZWfQHsgyYuPSvVpP4fct4lA=;
	b=BYFWZ7rWtteZ7HvyXKmtPfP+gNXq25ZDDQMWC+YjL+PlAnesDi4wkuxvy2sii0dq3W
	PIg+OpNavlGCqySxuVefooYphC3oKFh7bXd0PBpEbCnH0PFPS2VgBt3rTPcinSLvQv3G
	sex7BIDJjSlZ6/mUGTXK0z6jJQcPJkefddmGHPzFhSL/xKojP6PExEaoODp+Str3Lx66
	EK8wuFK0HwuOj0aZHsr9oWPuP4ZV4GiWeGgC41t5LG7HZg/G86WMZ3XX0OeCLtkBRGcN
	SEudxpCwsSrtufPjQvkHuI6wUsKgCqGQRwp3kctS0LbL/Q6X2KYd+ftDcebZv7m4bnd6
	Zc3Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-type:content-disposition:user-agent;
	bh=N45FHfXEXS5T+qRf2btnZWfQHsgyYuPSvVpP4fct4lA=;
	b=OBzKUYRwNEZSFWleZFx45rH4GhXNb63TGOuuCzBQ9SJnZHKqcsCCUucNmSE5hSGnxX
	hF/B1EuvxkJ3Cm8H01qwR0+ynwgEYHqBLb6PRqgHuKiGlqv2tGgCKP9oeDLxSri90ywM
	LxsyzNpUtB65y1HGaak77YwJv1aBVMCovyrmGipxsCRxDcf7ZhCAUQrp0qTzZNSGdhn0
	ly/sCRkktrTgzZKJsEh6TnMyKB1b6YOoxcthfjm5H/wqu19Dx2nVvnkdqiDDp4ywcNvN
	iCT6BtUhxXezw4ZQvYDMy05S0+CufCcewRymynJ4uc0AeYLX62RUj+AyngivqfZ1Wcsv
	QrUQ==
X-Gm-Message-State: ALoCoQni/YKHO1etGPoLKL5uVbd8axJuQFkOcEmsdoP8Zs+e0MILbjR1uuZupmSoAODdH8XWWK+9lrLBaNtQjIEuwcjlJ6konw==
X-Received: by 10.28.136.148 with SMTP id k142mr32288240wmd.41.1452766578889;
	Thu, 14 Jan 2016 02:16:18 -0800 (PST)
Received: from gmail.com (54033495.catv.pool.telekom.hu. [84.3.52.149])
	by smtp.gmail.com with ESMTPSA id
	bg1sm1063763wjc.27.2016.01.14.02.16.17
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 14 Jan 2016 02:16:17 -0800 (PST)
Date: Thu, 14 Jan 2016 11:16:15 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86 fixes
Message-ID: &lt;20160114101615.GA16926@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 14, 2016, 10:16 a.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-urgent-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-urgent-for-linus

   # HEAD: 7030a7e9321166eef44c811fe4af4d460360d424 x86/cpu/amd: Remove an unneeded condition in srat_detect_node()

Misc changes:

 - fix lguest bug
 - fix /proc/meminfo output on certain configs
 - fix pvclock bug
 - fix reboot on certain iMacs by adding new reboot quirk
 - fix bootup crash
 - fix FPU boot line option parsing
 - add more x86 self-tests
 - small cleanups, documentation improvements, etc.

  out-of-topic modifications in x86-urgent-for-linus:
  -----------------------------------------------------
  drivers/lguest/core.c              # e27d90e8be08: lguest: Map switcher text R/
  tools/testing/selftests/x86/Makefile# 65cacec1ba90: selftests/x86: Test __kernel
                                   # 0f672809f91a: selftests/x86: Disable the l
  tools/testing/selftests/x86/vdso_restorer.c# 65cacec1ba90: selftests/x86: Test __kernel

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (5):
      x86/mm: Add barriers and document switch_mm()-vs-flush synchronization
      selftests/x86: Disable the ldt_gdt_64 test for now
      selftests/x86: Test __kernel_sigreturn and __kernel_rt_sigreturn
      x86/mm: Improve switch_mm() barrier comments
      x86/vdso/pvclock: Protect STABLE check with the seqcount

Borislav Petkov (1):
      x86/boot: Hide local labels in verify_cpu()

Dan Carpenter (1):
      x86/cpu/amd: Remove an unneeded condition in srat_detect_node()

Dave Jones (1):
      x86/mm/pat: Make split_page_count() check for empty levels to fix /proc/meminfo output

H.J. Lu (1):
      x86/boot: Double BOOT_HEAP_SIZE to 64KB

Kefeng Wang (1):
      x86/mm: Use PAGE_ALIGNED instead of IS_ALIGNED

Mario Kleiner (1):
      x86/reboot/quirks: Add iMac10,1 to pci_reboot_dmi_table[]

Rusty Russell (1):
      lguest: Map switcher text R/O

yu-cheng yu (4):
      x86/fpu: Fix early FPU command-line parsing
      x86/fpu: Disable XGETBV1 when no XSAVE
      x86/fpu: Disable MPX when eagerfpu is off
      x86/fpu: Disable AVX when eagerfpu is off


 arch/x86/entry/vdso/vclock_gettime.c        |  12 +--
 arch/x86/include/asm/boot.h                 |   2 +-
 arch/x86/include/asm/fpu/internal.h         |   1 +
 arch/x86/include/asm/fpu/xstate.h           |  11 +-
 arch/x86/include/asm/lguest.h               |   4 +-
 arch/x86/include/asm/mmu_context.h          |  34 +++++-
 arch/x86/kernel/cpu/amd.c                   |   3 +-
 arch/x86/kernel/fpu/init.c                  | 161 ++++++++++++++--------------
 arch/x86/kernel/fpu/xstate.c                |   4 +-
 arch/x86/kernel/reboot.c                    |   8 ++
 arch/x86/kernel/verify_cpu.S                |  50 ++++-----
 arch/x86/mm/init_64.c                       |   3 +-
 arch/x86/mm/pageattr.c                      |   3 +
 arch/x86/mm/tlb.c                           |  29 ++++-
 drivers/lguest/core.c                       |  74 +++++++++----
 tools/testing/selftests/x86/Makefile        |   6 +-
 tools/testing/selftests/x86/vdso_restorer.c |  88 +++++++++++++++
 17 files changed, 342 insertions(+), 151 deletions(-)
 create mode 100644 tools/testing/selftests/x86/vdso_restorer.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/vdso/vclock_gettime.c b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">index 8602f06c759f..1a50e09c945b 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_chunk">@@ -126,23 +126,23 @@</span> <span class="p_context"> static notrace cycle_t vread_pvclock(int *mode)</span>
 	 *
 	 * On Xen, we don&#39;t appear to have that guarantee, but Xen still
 	 * supplies a valid seqlock using the version field.
<span class="p_del">-</span>
<span class="p_add">+	 *</span>
 	 * We only do pvclock vdso timing at all if
 	 * PVCLOCK_TSC_STABLE_BIT is set, and we interpret that bit to
 	 * mean that all vCPUs have matching pvti and that the TSC is
 	 * synced, so we can just look at vCPU 0&#39;s pvti.
 	 */
 
<span class="p_del">-	if (unlikely(!(pvti-&gt;flags &amp; PVCLOCK_TSC_STABLE_BIT))) {</span>
<span class="p_del">-		*mode = VCLOCK_NONE;</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	do {
 		version = pvti-&gt;version;
 
 		smp_rmb();
 
<span class="p_add">+		if (unlikely(!(pvti-&gt;flags &amp; PVCLOCK_TSC_STABLE_BIT))) {</span>
<span class="p_add">+			*mode = VCLOCK_NONE;</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		tsc = rdtsc_ordered();
 		pvti_tsc_to_system_mul = pvti-&gt;tsc_to_system_mul;
 		pvti_tsc_shift = pvti-&gt;tsc_shift;
<span class="p_header">diff --git a/arch/x86/include/asm/boot.h b/arch/x86/include/asm/boot.h</span>
<span class="p_header">index 4fa687a47a62..6b8d6e8cd449 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/boot.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/boot.h</span>
<span class="p_chunk">@@ -27,7 +27,7 @@</span> <span class="p_context"></span>
 #define BOOT_HEAP_SIZE             0x400000
 #else /* !CONFIG_KERNEL_BZIP2 */
 
<span class="p_del">-#define BOOT_HEAP_SIZE	0x8000</span>
<span class="p_add">+#define BOOT_HEAP_SIZE	0x10000</span>
 
 #endif /* !CONFIG_KERNEL_BZIP2 */
 
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_header">index eadcdd5bb946..0fd440df63f1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/internal.h</span>
<span class="p_chunk">@@ -42,6 +42,7 @@</span> <span class="p_context"> extern void fpu__init_cpu_xstate(void);</span>
 extern void fpu__init_system(struct cpuinfo_x86 *c);
 extern void fpu__init_check_bugs(void);
 extern void fpu__resume_cpu(void);
<span class="p_add">+extern u64 fpu__get_supported_xfeatures_mask(void);</span>
 
 /*
  * Debugging facility:
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">index 3a6c89b70307..af30fdeb140d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_chunk">@@ -20,15 +20,16 @@</span> <span class="p_context"></span>
 
 /* Supported features which support lazy state saving */
 #define XFEATURE_MASK_LAZY	(XFEATURE_MASK_FP | \
<span class="p_del">-				 XFEATURE_MASK_SSE | \</span>
<span class="p_add">+				 XFEATURE_MASK_SSE)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Supported features which require eager state saving */</span>
<span class="p_add">+#define XFEATURE_MASK_EAGER	(XFEATURE_MASK_BNDREGS | \</span>
<span class="p_add">+				 XFEATURE_MASK_BNDCSR | \</span>
 				 XFEATURE_MASK_YMM | \
<span class="p_del">-				 XFEATURE_MASK_OPMASK |	\</span>
<span class="p_add">+				 XFEATURE_MASK_OPMASK | \</span>
 				 XFEATURE_MASK_ZMM_Hi256 | \
 				 XFEATURE_MASK_Hi16_ZMM)
 
<span class="p_del">-/* Supported features which require eager state saving */</span>
<span class="p_del">-#define XFEATURE_MASK_EAGER	(XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR)</span>
<span class="p_del">-</span>
 /* All currently supported features */
 #define XCNTXT_MASK	(XFEATURE_MASK_LAZY | XFEATURE_MASK_EAGER)
 
<span class="p_header">diff --git a/arch/x86/include/asm/lguest.h b/arch/x86/include/asm/lguest.h</span>
<span class="p_header">index 3bbc07a57a31..73d0c9b92087 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/lguest.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/lguest.h</span>
<span class="p_chunk">@@ -12,7 +12,9 @@</span> <span class="p_context"></span>
 #define GUEST_PL 1
 
 /* Page for Switcher text itself, then two pages per cpu */
<span class="p_del">-#define TOTAL_SWITCHER_PAGES (1 + 2 * nr_cpu_ids)</span>
<span class="p_add">+#define SWITCHER_TEXT_PAGES (1)</span>
<span class="p_add">+#define SWITCHER_STACK_PAGES (2 * nr_cpu_ids)</span>
<span class="p_add">+#define TOTAL_SWITCHER_PAGES (SWITCHER_TEXT_PAGES + SWITCHER_STACK_PAGES)</span>
 
 /* Where we map the Switcher, in both Host and Guest. */
 extern unsigned long switcher_addr;
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 379cd3658799..bfd9b2a35a0b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -116,8 +116,36 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 #endif
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 
<span class="p_del">-		/* Re-load page tables */</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Re-load page tables.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * This logic has an ordering constraint:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 *  CPU 0: Write to a PTE for &#39;next&#39;</span>
<span class="p_add">+		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.</span>
<span class="p_add">+		 *  CPU 1: set bit 1 in next&#39;s mm_cpumask</span>
<span class="p_add">+		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We need to prevent an outcome in which CPU 1 observes</span>
<span class="p_add">+		 * the new PTE value and CPU 0 observes bit 1 clear in</span>
<span class="p_add">+		 * mm_cpumask.  (If that occurs, then the IPI will never</span>
<span class="p_add">+		 * be sent, and CPU 0&#39;s TLB will contain a stale entry.)</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * The bad outcome can occur if either CPU&#39;s load is</span>
<span class="p_add">+		 * reordered before that CPU&#39;s store, so both CPUs must</span>
<span class="p_add">+		 * execute full barriers to prevent this from happening.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Thus, switch_mm needs a full barrier between the</span>
<span class="p_add">+		 * store to mm_cpumask and any operation that could load</span>
<span class="p_add">+		 * from next-&gt;pgd.  TLB fills are special and can happen</span>
<span class="p_add">+		 * due to instruction fetches or for no reason at all,</span>
<span class="p_add">+		 * and neither LOCK nor MFENCE orders them.</span>
<span class="p_add">+		 * Fortunately, load_cr3() is serializing and gives the</span>
<span class="p_add">+		 * ordering guarantee we need.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 */</span>
 		load_cr3(next-&gt;pgd);
<span class="p_add">+</span>
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 
 		/* Stop flush ipis for the previous mm */
<span class="p_chunk">@@ -156,10 +184,14 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 			 * schedule, protecting us from simultaneous changes.
 			 */
 			cpumask_set_cpu(cpu, mm_cpumask(next));
<span class="p_add">+</span>
 			/*
 			 * We were in lazy tlb mode and leave_mm disabled
 			 * tlb flush IPI delivery. We must reload CR3
 			 * to make sure to use no freed page tables.
<span class="p_add">+			 *</span>
<span class="p_add">+			 * As above, load_cr3() is serializing and orders TLB</span>
<span class="p_add">+			 * fills with respect to the mm_cpumask write.</span>
 			 */
 			load_cr3(next-&gt;pgd);
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index e678ddeed030..a07956a08936 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -434,8 +434,7 @@</span> <span class="p_context"> static void srat_detect_node(struct cpuinfo_x86 *c)</span>
 		 */
 		int ht_nodeid = c-&gt;initial_apicid;
 
<span class="p_del">-		if (ht_nodeid &gt;= 0 &amp;&amp;</span>
<span class="p_del">-		    __apicid_to_node[ht_nodeid] != NUMA_NO_NODE)</span>
<span class="p_add">+		if (__apicid_to_node[ht_nodeid] != NUMA_NO_NODE)</span>
 			node = __apicid_to_node[ht_nodeid];
 		/* Pick a nearby node */
 		if (!node_online(node))
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 7b2978ab30df..6d9f0a7ef4c8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -3,8 +3,11 @@</span> <span class="p_context"></span>
  */
 #include &lt;asm/fpu/internal.h&gt;
 #include &lt;asm/tlbflush.h&gt;
<span class="p_add">+#include &lt;asm/setup.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
 
 #include &lt;linux/sched.h&gt;
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
 
 /*
  * Initialize the TS bit in CR0 according to the style of context-switches
<span class="p_chunk">@@ -270,20 +273,52 @@</span> <span class="p_context"> static void __init fpu__init_system_xstate_size_legacy(void)</span>
  */
 static enum { AUTO, ENABLE, DISABLE } eagerfpu = AUTO;
 
<span class="p_del">-static int __init eager_fpu_setup(char *s)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Find supported xfeatures based on cpu features and command-line input.</span>
<span class="p_add">+ * This must be called after fpu__init_parse_early_param() is called and</span>
<span class="p_add">+ * xfeatures_mask is enumerated.</span>
<span class="p_add">+ */</span>
<span class="p_add">+u64 __init fpu__get_supported_xfeatures_mask(void)</span>
 {
<span class="p_del">-	if (!strcmp(s, &quot;on&quot;))</span>
<span class="p_del">-		eagerfpu = ENABLE;</span>
<span class="p_del">-	else if (!strcmp(s, &quot;off&quot;))</span>
<span class="p_del">-		eagerfpu = DISABLE;</span>
<span class="p_del">-	else if (!strcmp(s, &quot;auto&quot;))</span>
<span class="p_del">-		eagerfpu = AUTO;</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	/* Support all xfeatures known to us */</span>
<span class="p_add">+	if (eagerfpu != DISABLE)</span>
<span class="p_add">+		return XCNTXT_MASK;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Warning of xfeatures being disabled for no eagerfpu mode */</span>
<span class="p_add">+	if (xfeatures_mask &amp; XFEATURE_MASK_EAGER) {</span>
<span class="p_add">+		pr_err(&quot;x86/fpu: eagerfpu switching disabled, disabling the following xstate features: 0x%llx.\n&quot;,</span>
<span class="p_add">+			xfeatures_mask &amp; XFEATURE_MASK_EAGER);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Return a mask that masks out all features requiring eagerfpu mode */</span>
<span class="p_add">+	return ~XFEATURE_MASK_EAGER;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Disable features dependent on eagerfpu.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init fpu__clear_eager_fpu_features(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_MPX);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX2);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX512F);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX512PF);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX512ER);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_AVX512CD);</span>
 }
<span class="p_del">-__setup(&quot;eagerfpu=&quot;, eager_fpu_setup);</span>
 
 /*
  * Pick the FPU context switching strategy:
<span class="p_add">+ *</span>
<span class="p_add">+ * When eagerfpu is AUTO or ENABLE, we ensure it is ENABLE if either of</span>
<span class="p_add">+ * the following is true:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * (1) the cpu has xsaveopt, as it has the optimization and doing eager</span>
<span class="p_add">+ *     FPU switching has a relatively low cost compared to a plain xsave;</span>
<span class="p_add">+ * (2) the cpu has xsave features (e.g. MPX) that depend on eager FPU</span>
<span class="p_add">+ *     switching. Should the kernel boot with noxsaveopt, we support MPX</span>
<span class="p_add">+ *     with eager FPU switching at a higher cost.</span>
  */
 static void __init fpu__init_system_ctx_switch(void)
 {
<span class="p_chunk">@@ -295,19 +330,11 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
 	WARN_ON_FPU(current-&gt;thread.fpu.fpstate_active);
 	current_thread_info()-&gt;status = 0;
 
<span class="p_del">-	/* Auto enable eagerfpu for xsaveopt */</span>
 	if (boot_cpu_has(X86_FEATURE_XSAVEOPT) &amp;&amp; eagerfpu != DISABLE)
 		eagerfpu = ENABLE;
 
<span class="p_del">-	if (xfeatures_mask &amp; XFEATURE_MASK_EAGER) {</span>
<span class="p_del">-		if (eagerfpu == DISABLE) {</span>
<span class="p_del">-			pr_err(&quot;x86/fpu: eagerfpu switching disabled, disabling the following xstate features: 0x%llx.\n&quot;,</span>
<span class="p_del">-			       xfeatures_mask &amp; XFEATURE_MASK_EAGER);</span>
<span class="p_del">-			xfeatures_mask &amp;= ~XFEATURE_MASK_EAGER;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			eagerfpu = ENABLE;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (xfeatures_mask &amp; XFEATURE_MASK_EAGER)</span>
<span class="p_add">+		eagerfpu = ENABLE;</span>
 
 	if (eagerfpu == ENABLE)
 		setup_force_cpu_cap(X86_FEATURE_EAGER_FPU);
<span class="p_chunk">@@ -316,11 +343,48 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
 }
 
 /*
<span class="p_add">+ * We parse fpu parameters early because fpu__init_system() is executed</span>
<span class="p_add">+ * before parse_early_param().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init fpu__init_parse_early_param(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No need to check &quot;eagerfpu=auto&quot; again, since it is the</span>
<span class="p_add">+	 * initial default.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;eagerfpu=off&quot;)) {</span>
<span class="p_add">+		eagerfpu = DISABLE;</span>
<span class="p_add">+		fpu__clear_eager_fpu_features();</span>
<span class="p_add">+	} else if (cmdline_find_option_bool(boot_command_line, &quot;eagerfpu=on&quot;)) {</span>
<span class="p_add">+		eagerfpu = ENABLE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;no387&quot;))</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_FPU);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nofxsr&quot;)) {</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_FXSR);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_FXSR_OPT);</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_XMM);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;noxsave&quot;))</span>
<span class="p_add">+		fpu__xstate_clear_all_cpu_caps();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;noxsaveopt&quot;))</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;noxsaves&quot;))</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_XSAVES);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Called on the boot CPU once per system bootup, to set up the initial
  * FPU state that is later cloned into all processes:
  */
 void __init fpu__init_system(struct cpuinfo_x86 *c)
 {
<span class="p_add">+	fpu__init_parse_early_param();</span>
 	fpu__init_system_early_generic(c);
 
 	/*
<span class="p_chunk">@@ -344,62 +408,3 @@</span> <span class="p_context"> void __init fpu__init_system(struct cpuinfo_x86 *c)</span>
 
 	fpu__init_system_ctx_switch();
 }
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Boot parameter to turn off FPU support and fall back to math-emu:</span>
<span class="p_del">- */</span>
<span class="p_del">-static int __init no_387(char *s)</span>
<span class="p_del">-{</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_FPU);</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;no387&quot;, no_387);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Disable all xstate CPU features:</span>
<span class="p_del">- */</span>
<span class="p_del">-static int __init x86_noxsave_setup(char *s)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (strlen(s))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	fpu__xstate_clear_all_cpu_caps();</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;noxsave&quot;, x86_noxsave_setup);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Disable the XSAVEOPT instruction specifically:</span>
<span class="p_del">- */</span>
<span class="p_del">-static int __init x86_noxsaveopt_setup(char *s)</span>
<span class="p_del">-{</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XSAVEOPT);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;noxsaveopt&quot;, x86_noxsaveopt_setup);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Disable the XSAVES instruction:</span>
<span class="p_del">- */</span>
<span class="p_del">-static int __init x86_noxsaves_setup(char *s)</span>
<span class="p_del">-{</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XSAVES);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;noxsaves&quot;, x86_noxsaves_setup);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Disable FX save/restore and SSE support:</span>
<span class="p_del">- */</span>
<span class="p_del">-static int __init x86_nofxsr_setup(char *s)</span>
<span class="p_del">-{</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_FXSR);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_FXSR_OPT);</span>
<span class="p_del">-	setup_clear_cpu_cap(X86_FEATURE_XMM);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-__setup(&quot;nofxsr&quot;, x86_nofxsr_setup);</span>
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index 40f100285984..d425cda5ae6d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -52,6 +52,7 @@</span> <span class="p_context"> void fpu__xstate_clear_all_cpu_caps(void)</span>
 	setup_clear_cpu_cap(X86_FEATURE_AVX512ER);
 	setup_clear_cpu_cap(X86_FEATURE_AVX512CD);
 	setup_clear_cpu_cap(X86_FEATURE_MPX);
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_XGETBV1);</span>
 }
 
 /*
<span class="p_chunk">@@ -632,8 +633,7 @@</span> <span class="p_context"> void __init fpu__init_system_xstate(void)</span>
 		BUG();
 	}
 
<span class="p_del">-	/* Support only the state known to the OS: */</span>
<span class="p_del">-	xfeatures_mask = xfeatures_mask &amp; XCNTXT_MASK;</span>
<span class="p_add">+	xfeatures_mask &amp;= fpu__get_supported_xfeatures_mask();</span>
 
 	/* Enable xstate instructions to be able to continue with initialization: */
 	fpu__init_cpu_xstate();
<span class="p_header">diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c</span>
<span class="p_header">index d64889aa2d46..ab0adc0fa5db 100644</span>
<span class="p_header">--- a/arch/x86/kernel/reboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/reboot.c</span>
<span class="p_chunk">@@ -182,6 +182,14 @@</span> <span class="p_context"> static struct dmi_system_id __initdata reboot_dmi_table[] = {</span>
 			DMI_MATCH(DMI_PRODUCT_NAME, &quot;iMac9,1&quot;),
 		},
 	},
<span class="p_add">+	{	/* Handle problems with rebooting on the iMac10,1. */</span>
<span class="p_add">+		.callback = set_pci_reboot,</span>
<span class="p_add">+		.ident = &quot;Apple iMac10,1&quot;,</span>
<span class="p_add">+		.matches = {</span>
<span class="p_add">+		    DMI_MATCH(DMI_SYS_VENDOR, &quot;Apple Inc.&quot;),</span>
<span class="p_add">+		    DMI_MATCH(DMI_PRODUCT_NAME, &quot;iMac10,1&quot;),</span>
<span class="p_add">+		},</span>
<span class="p_add">+	},</span>
 
 	/* ASRock */
 	{	/* Handle problems with rebooting on ASRock Q1900DC-ITX */
<span class="p_header">diff --git a/arch/x86/kernel/verify_cpu.S b/arch/x86/kernel/verify_cpu.S</span>
<span class="p_header">index 4cf401f581e7..07efb35ee4bc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/verify_cpu.S</span>
<span class="p_header">+++ b/arch/x86/kernel/verify_cpu.S</span>
<span class="p_chunk">@@ -48,31 +48,31 @@</span> <span class="p_context"></span>
 	pushfl
 	popl	%eax
 	cmpl	%eax,%ebx
<span class="p_del">-	jz	verify_cpu_no_longmode	# cpu has no cpuid</span>
<span class="p_add">+	jz	.Lverify_cpu_no_longmode	# cpu has no cpuid</span>
 #endif
 
 	movl	$0x0,%eax		# See if cpuid 1 is implemented
 	cpuid
 	cmpl	$0x1,%eax
<span class="p_del">-	jb	verify_cpu_no_longmode	# no cpuid 1</span>
<span class="p_add">+	jb	.Lverify_cpu_no_longmode	# no cpuid 1</span>
 
 	xor	%di,%di
 	cmpl	$0x68747541,%ebx	# AuthenticAMD
<span class="p_del">-	jnz	verify_cpu_noamd</span>
<span class="p_add">+	jnz	.Lverify_cpu_noamd</span>
 	cmpl	$0x69746e65,%edx
<span class="p_del">-	jnz	verify_cpu_noamd</span>
<span class="p_add">+	jnz	.Lverify_cpu_noamd</span>
 	cmpl	$0x444d4163,%ecx
<span class="p_del">-	jnz	verify_cpu_noamd</span>
<span class="p_add">+	jnz	.Lverify_cpu_noamd</span>
 	mov	$1,%di			# cpu is from AMD
<span class="p_del">-	jmp	verify_cpu_check</span>
<span class="p_add">+	jmp	.Lverify_cpu_check</span>
 
<span class="p_del">-verify_cpu_noamd:</span>
<span class="p_add">+.Lverify_cpu_noamd:</span>
 	cmpl	$0x756e6547,%ebx        # GenuineIntel?
<span class="p_del">-	jnz	verify_cpu_check</span>
<span class="p_add">+	jnz	.Lverify_cpu_check</span>
 	cmpl	$0x49656e69,%edx
<span class="p_del">-	jnz	verify_cpu_check</span>
<span class="p_add">+	jnz	.Lverify_cpu_check</span>
 	cmpl	$0x6c65746e,%ecx
<span class="p_del">-	jnz	verify_cpu_check</span>
<span class="p_add">+	jnz	.Lverify_cpu_check</span>
 
 	# only call IA32_MISC_ENABLE when:
 	# family &gt; 6 || (family == 6 &amp;&amp; model &gt;= 0xd)
<span class="p_chunk">@@ -83,59 +83,59 @@</span> <span class="p_context"></span>
 	andl	$0x0ff00f00, %eax	# mask family and extended family
 	shrl	$8, %eax
 	cmpl	$6, %eax
<span class="p_del">-	ja	verify_cpu_clear_xd	# family &gt; 6, ok</span>
<span class="p_del">-	jb	verify_cpu_check	# family &lt; 6, skip</span>
<span class="p_add">+	ja	.Lverify_cpu_clear_xd	# family &gt; 6, ok</span>
<span class="p_add">+	jb	.Lverify_cpu_check	# family &lt; 6, skip</span>
 
 	andl	$0x000f00f0, %ecx	# mask model and extended model
 	shrl	$4, %ecx
 	cmpl	$0xd, %ecx
<span class="p_del">-	jb	verify_cpu_check	# family == 6, model &lt; 0xd, skip</span>
<span class="p_add">+	jb	.Lverify_cpu_check	# family == 6, model &lt; 0xd, skip</span>
 
<span class="p_del">-verify_cpu_clear_xd:</span>
<span class="p_add">+.Lverify_cpu_clear_xd:</span>
 	movl	$MSR_IA32_MISC_ENABLE, %ecx
 	rdmsr
 	btrl	$2, %edx		# clear MSR_IA32_MISC_ENABLE_XD_DISABLE
<span class="p_del">-	jnc	verify_cpu_check	# only write MSR if bit was changed</span>
<span class="p_add">+	jnc	.Lverify_cpu_check	# only write MSR if bit was changed</span>
 	wrmsr
 
<span class="p_del">-verify_cpu_check:</span>
<span class="p_add">+.Lverify_cpu_check:</span>
 	movl    $0x1,%eax		# Does the cpu have what it takes
 	cpuid
 	andl	$REQUIRED_MASK0,%edx
 	xorl	$REQUIRED_MASK0,%edx
<span class="p_del">-	jnz	verify_cpu_no_longmode</span>
<span class="p_add">+	jnz	.Lverify_cpu_no_longmode</span>
 
 	movl    $0x80000000,%eax	# See if extended cpuid is implemented
 	cpuid
 	cmpl    $0x80000001,%eax
<span class="p_del">-	jb      verify_cpu_no_longmode	# no extended cpuid</span>
<span class="p_add">+	jb      .Lverify_cpu_no_longmode	# no extended cpuid</span>
 
 	movl    $0x80000001,%eax	# Does the cpu have what it takes
 	cpuid
 	andl    $REQUIRED_MASK1,%edx
 	xorl    $REQUIRED_MASK1,%edx
<span class="p_del">-	jnz     verify_cpu_no_longmode</span>
<span class="p_add">+	jnz     .Lverify_cpu_no_longmode</span>
 
<span class="p_del">-verify_cpu_sse_test:</span>
<span class="p_add">+.Lverify_cpu_sse_test:</span>
 	movl	$1,%eax
 	cpuid
 	andl	$SSE_MASK,%edx
 	cmpl	$SSE_MASK,%edx
<span class="p_del">-	je	verify_cpu_sse_ok</span>
<span class="p_add">+	je	.Lverify_cpu_sse_ok</span>
 	test	%di,%di
<span class="p_del">-	jz	verify_cpu_no_longmode	# only try to force SSE on AMD</span>
<span class="p_add">+	jz	.Lverify_cpu_no_longmode	# only try to force SSE on AMD</span>
 	movl	$MSR_K7_HWCR,%ecx
 	rdmsr
 	btr	$15,%eax		# enable SSE
 	wrmsr
 	xor	%di,%di			# don&#39;t loop
<span class="p_del">-	jmp	verify_cpu_sse_test	# try again</span>
<span class="p_add">+	jmp	.Lverify_cpu_sse_test	# try again</span>
 
<span class="p_del">-verify_cpu_no_longmode:</span>
<span class="p_add">+.Lverify_cpu_no_longmode:</span>
 	popf				# Restore caller passed flags
 	movl $1,%eax
 	ret
<span class="p_del">-verify_cpu_sse_ok:</span>
<span class="p_add">+.Lverify_cpu_sse_ok:</span>
 	popf				# Restore caller passed flags
 	xorl %eax, %eax
 	ret
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index ec081fe0ce2c..8829482d69ec 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -814,8 +814,7 @@</span> <span class="p_context"> remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,</span>
 		if (phys_addr &lt; (phys_addr_t)0x40000000)
 			return;
 
<span class="p_del">-		if (IS_ALIGNED(addr, PAGE_SIZE) &amp;&amp;</span>
<span class="p_del">-		    IS_ALIGNED(next, PAGE_SIZE)) {</span>
<span class="p_add">+		if (PAGE_ALIGNED(addr) &amp;&amp; PAGE_ALIGNED(next)) {</span>
 			/*
 			 * Do not free direct mapping pages since they were
 			 * freed when offlining, or simplely not in use.
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 6000ad7f560c..fc6a4c8f6e2a 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -66,6 +66,9 @@</span> <span class="p_context"> void update_page_count(int level, unsigned long pages)</span>
 
 static void split_page_count(int level)
 {
<span class="p_add">+	if (direct_pages_count[level] == 0)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	direct_pages_count[level]--;
 	direct_pages_count[level - 1] += PTRS_PER_PTE;
 }
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 8ddb5d0d66fb..8f4cc3dfac32 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -161,7 +161,10 @@</span> <span class="p_context"> void flush_tlb_current_task(void)</span>
 	preempt_disable();
 
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
<span class="p_add">+</span>
<span class="p_add">+	/* This is an implicit full barrier that synchronizes with switch_mm. */</span>
 	local_flush_tlb();
<span class="p_add">+</span>
 	trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);
 	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)
 		flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);
<span class="p_chunk">@@ -188,17 +191,29 @@</span> <span class="p_context"> void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,</span>
 	unsigned long base_pages_to_flush = TLB_FLUSH_ALL;
 
 	preempt_disable();
<span class="p_del">-	if (current-&gt;active_mm != mm)</span>
<span class="p_add">+	if (current-&gt;active_mm != mm) {</span>
<span class="p_add">+		/* Synchronize with switch_mm. */</span>
<span class="p_add">+		smp_mb();</span>
<span class="p_add">+</span>
 		goto out;
<span class="p_add">+	}</span>
 
 	if (!current-&gt;mm) {
 		leave_mm(smp_processor_id());
<span class="p_add">+</span>
<span class="p_add">+		/* Synchronize with switch_mm. */</span>
<span class="p_add">+		smp_mb();</span>
<span class="p_add">+</span>
 		goto out;
 	}
 
 	if ((end != TLB_FLUSH_ALL) &amp;&amp; !(vmflag &amp; VM_HUGETLB))
 		base_pages_to_flush = (end - start) &gt;&gt; PAGE_SHIFT;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Both branches below are implicit full barriers (MOV to CR or</span>
<span class="p_add">+	 * INVLPG) that synchronize with switch_mm.</span>
<span class="p_add">+	 */</span>
 	if (base_pages_to_flush &gt; tlb_single_page_flush_ceiling) {
 		base_pages_to_flush = TLB_FLUSH_ALL;
 		count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
<span class="p_chunk">@@ -228,10 +243,18 @@</span> <span class="p_context"> void flush_tlb_page(struct vm_area_struct *vma, unsigned long start)</span>
 	preempt_disable();
 
 	if (current-&gt;active_mm == mm) {
<span class="p_del">-		if (current-&gt;mm)</span>
<span class="p_add">+		if (current-&gt;mm) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Implicit full barrier (INVLPG) that synchronizes</span>
<span class="p_add">+			 * with switch_mm.</span>
<span class="p_add">+			 */</span>
 			__flush_tlb_one(start);
<span class="p_del">-		else</span>
<span class="p_add">+		} else {</span>
 			leave_mm(smp_processor_id());
<span class="p_add">+</span>
<span class="p_add">+			/* Synchronize with switch_mm. */</span>
<span class="p_add">+			smp_mb();</span>
<span class="p_add">+		}</span>
 	}
 
 	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids)
<span class="p_header">diff --git a/drivers/lguest/core.c b/drivers/lguest/core.c</span>
<span class="p_header">index 312ffd3d0017..9e385b38debf 100644</span>
<span class="p_header">--- a/drivers/lguest/core.c</span>
<span class="p_header">+++ b/drivers/lguest/core.c</span>
<span class="p_chunk">@@ -22,7 +22,8 @@</span> <span class="p_context"></span>
 
 unsigned long switcher_addr;
 struct page **lg_switcher_pages;
<span class="p_del">-static struct vm_struct *switcher_vma;</span>
<span class="p_add">+static struct vm_struct *switcher_text_vma;</span>
<span class="p_add">+static struct vm_struct *switcher_stacks_vma;</span>
 
 /* This One Big lock protects all inter-guest data structures. */
 DEFINE_MUTEX(lguest_lock);
<span class="p_chunk">@@ -83,54 +84,80 @@</span> <span class="p_context"> static __init int map_switcher(void)</span>
 	}
 
 	/*
<span class="p_add">+	 * Copy in the compiled-in Switcher code (from x86/switcher_32.S).</span>
<span class="p_add">+	 * It goes in the first page, which we map in momentarily.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	memcpy(kmap(lg_switcher_pages[0]), start_switcher_text,</span>
<span class="p_add">+	       end_switcher_text - start_switcher_text);</span>
<span class="p_add">+	kunmap(lg_switcher_pages[0]);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * We place the Switcher underneath the fixmap area, which is the
 	 * highest virtual address we can get.  This is important, since we
 	 * tell the Guest it can&#39;t access this memory, so we want its ceiling
 	 * as high as possible.
 	 */
<span class="p_del">-	switcher_addr = FIXADDR_START - (TOTAL_SWITCHER_PAGES+1)*PAGE_SIZE;</span>
<span class="p_add">+	switcher_addr = FIXADDR_START - TOTAL_SWITCHER_PAGES*PAGE_SIZE;</span>
 
 	/*
<span class="p_del">-	 * Now we reserve the &quot;virtual memory area&quot; we want.  We might</span>
<span class="p_del">-	 * not get it in theory, but in practice it&#39;s worked so far.</span>
<span class="p_del">-	 * The end address needs +1 because __get_vm_area allocates an</span>
<span class="p_del">-	 * extra guard page, so we need space for that.</span>
<span class="p_add">+	 * Now we reserve the &quot;virtual memory area&quot;s we want.  We might</span>
<span class="p_add">+	 * not get them in theory, but in practice it&#39;s worked so far.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We want the switcher text to be read-only and executable, and</span>
<span class="p_add">+	 * the stacks to be read-write and non-executable.</span>
 	 */
<span class="p_del">-	switcher_vma = __get_vm_area(TOTAL_SWITCHER_PAGES * PAGE_SIZE,</span>
<span class="p_del">-				     VM_ALLOC, switcher_addr, switcher_addr</span>
<span class="p_del">-				     + (TOTAL_SWITCHER_PAGES+1) * PAGE_SIZE);</span>
<span class="p_del">-	if (!switcher_vma) {</span>
<span class="p_add">+	switcher_text_vma = __get_vm_area(PAGE_SIZE, VM_ALLOC|VM_NO_GUARD,</span>
<span class="p_add">+					  switcher_addr,</span>
<span class="p_add">+					  switcher_addr + PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!switcher_text_vma) {</span>
 		err = -ENOMEM;
 		printk(&quot;lguest: could not map switcher pages high\n&quot;);
 		goto free_pages;
 	}
 
<span class="p_add">+	switcher_stacks_vma = __get_vm_area(SWITCHER_STACK_PAGES * PAGE_SIZE,</span>
<span class="p_add">+					    VM_ALLOC|VM_NO_GUARD,</span>
<span class="p_add">+					    switcher_addr + PAGE_SIZE,</span>
<span class="p_add">+					    switcher_addr + TOTAL_SWITCHER_PAGES * PAGE_SIZE);</span>
<span class="p_add">+	if (!switcher_stacks_vma) {</span>
<span class="p_add">+		err = -ENOMEM;</span>
<span class="p_add">+		printk(&quot;lguest: could not map switcher pages high\n&quot;);</span>
<span class="p_add">+		goto free_text_vma;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * This code actually sets up the pages we&#39;ve allocated to appear at
 	 * switcher_addr.  map_vm_area() takes the vma we allocated above, the
<span class="p_del">-	 * kind of pages we&#39;re mapping (kernel pages), and a pointer to our</span>
<span class="p_del">-	 * array of struct pages.</span>
<span class="p_add">+	 * kind of pages we&#39;re mapping (kernel text pages and kernel writable</span>
<span class="p_add">+	 * pages respectively), and a pointer to our array of struct pages.</span>
 	 */
<span class="p_del">-	err = map_vm_area(switcher_vma, PAGE_KERNEL_EXEC, lg_switcher_pages);</span>
<span class="p_add">+	err = map_vm_area(switcher_text_vma, PAGE_KERNEL_RX, lg_switcher_pages);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		printk(&quot;lguest: text map_vm_area failed: %i\n&quot;, err);</span>
<span class="p_add">+		goto free_vmas;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	err = map_vm_area(switcher_stacks_vma, PAGE_KERNEL,</span>
<span class="p_add">+			  lg_switcher_pages + SWITCHER_TEXT_PAGES);</span>
 	if (err) {
<span class="p_del">-		printk(&quot;lguest: map_vm_area failed: %i\n&quot;, err);</span>
<span class="p_del">-		goto free_vma;</span>
<span class="p_add">+		printk(&quot;lguest: stacks map_vm_area failed: %i\n&quot;, err);</span>
<span class="p_add">+		goto free_vmas;</span>
 	}
 
 	/*
 	 * Now the Switcher is mapped at the right address, we can&#39;t fail!
<span class="p_del">-	 * Copy in the compiled-in Switcher code (from x86/switcher_32.S).</span>
 	 */
<span class="p_del">-	memcpy(switcher_vma-&gt;addr, start_switcher_text,</span>
<span class="p_del">-	       end_switcher_text - start_switcher_text);</span>
<span class="p_del">-</span>
 	printk(KERN_INFO &quot;lguest: mapped switcher at %p\n&quot;,
<span class="p_del">-	       switcher_vma-&gt;addr);</span>
<span class="p_add">+	       switcher_text_vma-&gt;addr);</span>
 	/* And we succeeded... */
 	return 0;
 
<span class="p_del">-free_vma:</span>
<span class="p_del">-	vunmap(switcher_vma-&gt;addr);</span>
<span class="p_add">+free_vmas:</span>
<span class="p_add">+	/* Undoes map_vm_area and __get_vm_area */</span>
<span class="p_add">+	vunmap(switcher_stacks_vma-&gt;addr);</span>
<span class="p_add">+free_text_vma:</span>
<span class="p_add">+	vunmap(switcher_text_vma-&gt;addr);</span>
 free_pages:
 	i = TOTAL_SWITCHER_PAGES;
 free_some_pages:
<span class="p_chunk">@@ -148,7 +175,8 @@</span> <span class="p_context"> static void unmap_switcher(void)</span>
 	unsigned int i;
 
 	/* vunmap() undoes *both* map_vm_area() and __get_vm_area(). */
<span class="p_del">-	vunmap(switcher_vma-&gt;addr);</span>
<span class="p_add">+	vunmap(switcher_text_vma-&gt;addr);</span>
<span class="p_add">+	vunmap(switcher_stacks_vma-&gt;addr);</span>
 	/* Now we just need to free the pages we copied the switcher into */
 	for (i = 0; i &lt; TOTAL_SWITCHER_PAGES; i++)
 		__free_pages(lg_switcher_pages[i], 0);
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index eabcff411984..d0c473f65850 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -4,9 +4,11 @@</span> <span class="p_context"> include ../lib.mk</span>
 
 .PHONY: all all_32 all_64 warn_32bit_failure clean
 
<span class="p_del">-TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs ldt_gdt syscall_nt ptrace_syscall</span>
<span class="p_add">+TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall</span>
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault sigreturn test_syscall_vdso unwind_vdso \
<span class="p_del">-			test_FCMOV test_FCOMI test_FISTTP</span>
<span class="p_add">+			test_FCMOV test_FCOMI test_FISTTP \</span>
<span class="p_add">+			ldt_gdt \</span>
<span class="p_add">+			vdso_restorer</span>
 
 TARGETS_C_32BIT_ALL := $(TARGETS_C_BOTHBITS) $(TARGETS_C_32BIT_ONLY)
 BINARIES_32 := $(TARGETS_C_32BIT_ALL:%=%_32)
<span class="p_header">diff --git a/tools/testing/selftests/x86/vdso_restorer.c b/tools/testing/selftests/x86/vdso_restorer.c</span>
new file mode 100644
<span class="p_header">index 000000000000..cb038424a403</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/vdso_restorer.c</span>
<span class="p_chunk">@@ -0,0 +1,88 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * vdso_restorer.c - tests vDSO-based signal restore</span>
<span class="p_add">+ * Copyright (c) 2015 Andrew Lutomirski</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms and conditions of the GNU General Public License,</span>
<span class="p_add">+ * version 2, as published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This makes sure that sa_restorer == NULL keeps working on 32-bit</span>
<span class="p_add">+ * configurations.  Modern glibc doesn&#39;t use it under any circumstances,</span>
<span class="p_add">+ * so it&#39;s easy to overlook breakage.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * 64-bit userspace has never supported sa_restorer == NULL, so this is</span>
<span class="p_add">+ * 32-bit only.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;err.h&gt;</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;signal.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;syscall.h&gt;</span>
<span class="p_add">+#include &lt;sys/syscall.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Open-code this -- the headers are too messy to easily use them. */</span>
<span class="p_add">+struct real_sigaction {</span>
<span class="p_add">+	void *handler;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	void *restorer;</span>
<span class="p_add">+	unsigned int mask[2];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static volatile sig_atomic_t handler_called;</span>
<span class="p_add">+</span>
<span class="p_add">+static void handler_with_siginfo(int sig, siginfo_t *info, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	handler_called = 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void handler_without_siginfo(int sig)</span>
<span class="p_add">+{</span>
<span class="p_add">+	handler_called = 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int main()</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	struct real_sigaction sa;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(&amp;sa, 0, sizeof(sa));</span>
<span class="p_add">+	sa.handler = handler_with_siginfo;</span>
<span class="p_add">+	sa.flags = SA_SIGINFO;</span>
<span class="p_add">+	sa.restorer = NULL;	/* request kernel-provided restorer */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (syscall(SYS_rt_sigaction, SIGUSR1, &amp;sa, NULL, 8) != 0)</span>
<span class="p_add">+		err(1, &quot;raw rt_sigaction syscall&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	raise(SIGUSR1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (handler_called) {</span>
<span class="p_add">+		printf(&quot;[OK]\tSA_SIGINFO handler returned successfully\n&quot;);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tSA_SIGINFO handler was not called\n&quot;);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	sa.flags = 0;</span>
<span class="p_add">+	sa.handler = handler_without_siginfo;</span>
<span class="p_add">+	if (syscall(SYS_sigaction, SIGUSR1, &amp;sa, 0) != 0)</span>
<span class="p_add">+		err(1, &quot;raw sigaction syscall&quot;);</span>
<span class="p_add">+	handler_called = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	raise(SIGUSR1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (handler_called) {</span>
<span class="p_add">+		printf(&quot;[OK]\t!SA_SIGINFO handler returned successfully\n&quot;);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[FAIL]\t!SA_SIGINFO handler was not called\n&quot;);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



