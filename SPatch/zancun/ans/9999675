
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,07/22] mm: Protect VMA modifications using VMA sequence count - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,07/22] mm: Protect VMA modifications using VMA sequence count</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 11, 2017, 1:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1507729966-10660-8-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9999675/mbox/"
   >mbox</a>
|
   <a href="/patch/9999675/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9999675/">/patch/9999675/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5DAD86037F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Oct 2017 13:53:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4F73228A4A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Oct 2017 13:53:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4346928A50; Wed, 11 Oct 2017 13:53:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 695BE28A4A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Oct 2017 13:53:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757471AbdJKNxZ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 11 Oct 2017 09:53:25 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:55668 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1752475AbdJKNxT (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 11 Oct 2017 09:53:19 -0400
Received: from pps.filterd (m0098420.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	v9BDnLTn091177
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 11 Oct 2017 09:53:19 -0400
Received: from e06smtp12.uk.ibm.com (e06smtp12.uk.ibm.com [195.75.94.108])
	by mx0b-001b2d01.pphosted.com with ESMTP id 2dhk0hpmc0-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 11 Oct 2017 09:53:19 -0400
Received: from localhost
	by e06smtp12.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Wed, 11 Oct 2017 14:53:17 +0100
Received: from b06cxnps4074.portsmouth.uk.ibm.com (9.149.109.196)
	by e06smtp12.uk.ibm.com (192.168.101.142) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Wed, 11 Oct 2017 14:53:10 +0100
Received: from d06av22.portsmouth.uk.ibm.com (d06av22.portsmouth.uk.ibm.com
	[9.149.105.58])
	by b06cxnps4074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v9BDr9xO7995606; Wed, 11 Oct 2017 13:53:09 GMT
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 225C54C046;
	Wed, 11 Oct 2017 14:49:06 +0100 (BST)
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 65E3F4C04A;
	Wed, 11 Oct 2017 14:49:04 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.145.30.240])
	by d06av22.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Wed, 11 Oct 2017 14:49:04 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: paulmck@linux.vnet.ibm.com, peterz@infradead.org,
	akpm@linux-foundation.org, kirill@shutemov.name,
	ak@linux.intel.com, mhocko@kernel.org, dave@stgolabs.net,
	jack@suse.cz, Matthew Wilcox &lt;willy@infradead.org&gt;,
	benh@kernel.crashing.org, mpe@ellerman.id.au, paulus@samba.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, hpa@zytor.com,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Alexei Starovoitov &lt;alexei.starovoitov@gmail.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	haren@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	npiggin@gmail.com, bsingharora@gmail.com,
	Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org
Subject: [PATCH v5 07/22] mm: Protect VMA modifications using VMA sequence
	count
Date: Wed, 11 Oct 2017 15:52:31 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1507729966-10660-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
References: &lt;1507729966-10660-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17101113-0008-0000-0000-0000049EC80D
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17101113-0009-0000-0000-00001E30D56D
Message-Id: &lt;1507729966-10660-8-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-10-11_05:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1707230000
	definitions=main-1710110191
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Oct. 11, 2017, 1:52 p.m.</div>
<pre class="content">
The VMA sequence count has been introduced to allow fast detection of
VMA modification when running a page fault handler without holding
the mmap_sem.

This patch provides protection against the VMA modification done in :
	- madvise()
	- mremap()
	- mpol_rebind_policy()
	- vma_replace_policy()
	- change_prot_numa()
	- mlock(), munlock()
	- mprotect()
	- mmap_region()
	- collapse_huge_page()
	- userfaultd registering services

In addition, VMA fields which will be read during the speculative fault
path needs to be written using WRITE_ONCE to prevent write to be split
and intermediate values to be pushed to other CPUs.
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 fs/proc/task_mmu.c |  5 ++++-
 fs/userfaultfd.c   | 17 +++++++++++++----
 mm/khugepaged.c    |  3 +++
 mm/madvise.c       |  6 +++++-
 mm/mempolicy.c     | 51 ++++++++++++++++++++++++++++++++++-----------------
 mm/mlock.c         | 13 ++++++++-----
 mm/mmap.c          | 17 ++++++++++-------
 mm/mprotect.c      |  4 +++-
 mm/mremap.c        |  6 ++++++
 9 files changed, 86 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Oct. 26, 2017, 10:18 a.m.</div>
<pre class="content">
Hello Laurent,

Message-ID: &lt;7ca80231-fe02-a3a7-84bc-ce81690ea051@intel.com&gt; shows
significant slowdown even for brk/malloc ops both single and
multi threaded.

The single threaded case I think is the most important because it has
zero chance of getting back any benefit later during page faults.

Could you check if:

1. it&#39;s possible change vm_write_begin to be a noop if mm-&gt;mm_count is
   &lt;= 1? Hint: clone() will run single threaded so there&#39;s no way it can run
   in the middle of a being/end critical section (clone could set an
   MMF flag to possibly keep the sequence counter activated if a child
   thread exits and mm_count drops to 1 while the other cpu is in the
   middle of a critical section in the other thread).

2. Same thing with RCU freeing of vmas. Wouldn&#39;t it be nicer if RCU
   freeing happened only once a MMF flag is set? That will at least
   reduce the risk of temporary memory waste until the next RCU grace
   period. The read of the MMF will scale fine. Of course to allow
   point 1 and 2 then the page fault should also take the mmap_sem
   until the MMF flag is set.

Could you also investigate a much bigger change: I wonder if it&#39;s
possible to drop the sequence number entirely from the vma and stop
using sequence numbers entirely (which is likely the source of the
single threaded regression in point 1 that may explain the report in
the above message-id), and just call the vma rbtree lookup once again
and check that everything is still the same in the vma and the PT lock
obtained is still a match to finish the anon page fault and fill the
pte?

Then of course we also need to add a method to the read-write
semaphore so it tells us if there&#39;s already one user holding the read
mmap_sem and we&#39;re the second one.  If we&#39;re the second one (or more
than second) only then we should skip taking the down_read mmap_sem.
Even a multithreaded app won&#39;t ever skip taking the mmap_sem until
there&#39;s sign of runtime contention, and it won&#39;t have to run the way
more expensive sequence number-less revalidation during page faults,
unless we get an immediate scalability payoff because we already know
the mmap_sem is already contended and there are multiple nested
threads in the page fault handler of the same mm.

Perhaps we&#39;d need something more advanced than a
down_read_trylock_if_not_hold() (which has to guaranteed not to write
to any cacheline) and we&#39;ll have to count the per-thread exponential
backoff of mmap_sem frequency, but starting with
down_read_trylock_if_not_hold() would be good I think.

This is not how the current patch works, the current patch uses a
sequence number because it pretends to go lockless always and in turn
has to slow down all vma updates fast paths or the revalidation
slowsdown performance for page fault too much (as it always
revalidates).

I think it would be much better to go speculative only when there&#39;s
&quot;detected&quot; runtime contention on the mmap_sem with
down_read_trylock_if_not_hold() and that will make the revalidation
cost not an issue to worry about because normally we won&#39;t have to
revalidate the vma at all during page fault. In turn by making the
revalidation more expensive by starting a vma rbtree lookup from
scratch, we can drop the sequence number entirely and that should
simplify the patch tremendously because all vm_write_begin/end would
disappear from the patch and in turn the mmap/brk slowdown measured by
the message-id above, should disappear as well.
   
Thanks,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Nov. 2, 2017, 3:16 p.m.</div>
<pre class="content">
Hi Andrea,

Thanks for reviewing this series, and sorry for the late answer, I took few
days off...

On 26/10/2017 12:18, Andrea Arcangeli wrote:
<span class="quote">&gt; Hello Laurent,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Message-ID: &lt;7ca80231-fe02-a3a7-84bc-ce81690ea051@intel.com&gt; shows</span>
<span class="quote">&gt; significant slowdown even for brk/malloc ops both single and</span>
<span class="quote">&gt; multi threaded.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The single threaded case I think is the most important because it has</span>
<span class="quote">&gt; zero chance of getting back any benefit later during page faults.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you check if:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1. it&#39;s possible change vm_write_begin to be a noop if mm-&gt;mm_count is</span>
<span class="quote">&gt;    &lt;= 1? Hint: clone() will run single threaded so there&#39;s no way it can run</span>
<span class="quote">&gt;    in the middle of a being/end critical section (clone could set an</span>
<span class="quote">&gt;    MMF flag to possibly keep the sequence counter activated if a child</span>
<span class="quote">&gt;    thread exits and mm_count drops to 1 while the other cpu is in the</span>
<span class="quote">&gt;    middle of a critical section in the other thread).</span>

This sounds to be a good idea, I&#39;ll dig on that.
The major risk here is to have a thread calling vm_*_begin() with
mm-&gt;mm_count &gt; 1 and later calling vm_*_end() with mm-&gt;mm_count &lt;= 1, but
as you mentioned we should find a way to work around this.
<span class="quote">
&gt; </span>
<span class="quote">&gt; 2. Same thing with RCU freeing of vmas. Wouldn&#39;t it be nicer if RCU</span>
<span class="quote">&gt;    freeing happened only once a MMF flag is set? That will at least</span>
<span class="quote">&gt;    reduce the risk of temporary memory waste until the next RCU grace</span>
<span class="quote">&gt;    period. The read of the MMF will scale fine. Of course to allow</span>
<span class="quote">&gt;    point 1 and 2 then the page fault should also take the mmap_sem</span>
<span class="quote">&gt;    until the MMF flag is set.</span>
<span class="quote">&gt; </span>

I think we could also deal with the mm-&gt;mm_count value here, if there is
only one thread, no need to postpone the VMA&#39;s free operation. Isn&#39;t it ?
Also, if mm-&gt;mm_count &lt;= 1, there is no need to try the speculative path.
<span class="quote">
&gt; Could you also investigate a much bigger change: I wonder if it&#39;s</span>
<span class="quote">&gt; possible to drop the sequence number entirely from the vma and stop</span>
<span class="quote">&gt; using sequence numbers entirely (which is likely the source of the</span>
<span class="quote">&gt; single threaded regression in point 1 that may explain the report in</span>
<span class="quote">&gt; the above message-id), and just call the vma rbtree lookup once again</span>
<span class="quote">&gt; and check that everything is still the same in the vma and the PT lock</span>
<span class="quote">&gt; obtained is still a match to finish the anon page fault and fill the</span>
<span class="quote">&gt; pte?</span>

That&#39;s an interesting idea. The big deal here would be to detect that the
VMA has been touched in our back, but there are not so much VMA&#39;s fields
involved in the speculative path so that sounds reasonable. The other point
is to identify the impact of the vma rbtree lookup, it&#39;s also a known
order, but there is the vma_srcu&#39;s lock involved.
<span class="quote">&gt; </span>
<span class="quote">&gt; Then of course we also need to add a method to the read-write</span>
<span class="quote">&gt; semaphore so it tells us if there&#39;s already one user holding the read</span>
<span class="quote">&gt; mmap_sem and we&#39;re the second one.  If we&#39;re the second one (or more</span>
<span class="quote">&gt; than second) only then we should skip taking the down_read mmap_sem.</span>
<span class="quote">&gt; Even a multithreaded app won&#39;t ever skip taking the mmap_sem until</span>
<span class="quote">&gt; there&#39;s sign of runtime contention, and it won&#39;t have to run the way</span>
<span class="quote">&gt; more expensive sequence number-less revalidation during page faults,</span>
<span class="quote">&gt; unless we get an immediate scalability payoff because we already know</span>
<span class="quote">&gt; the mmap_sem is already contended and there are multiple nested</span>
<span class="quote">&gt; threads in the page fault handler of the same mm.</span>

The problem is that we may have a thread entering the page fault path,
seeing that the mmap_sem is free, grab it and continue processing the page
fault. Then another thread is entering mprotect or any other mm service
which grab the mmap_sem and it will be blocked until the page fault is
done. The idea with the speculative page fault is also to not block the
other thread which may need to grab the mmap_sem.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Perhaps we&#39;d need something more advanced than a</span>
<span class="quote">&gt; down_read_trylock_if_not_hold() (which has to guaranteed not to write</span>
<span class="quote">&gt; to any cacheline) and we&#39;ll have to count the per-thread exponential</span>
<span class="quote">&gt; backoff of mmap_sem frequency, but starting with</span>
<span class="quote">&gt; down_read_trylock_if_not_hold() would be good I think.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is not how the current patch works, the current patch uses a</span>
<span class="quote">&gt; sequence number because it pretends to go lockless always and in turn</span>
<span class="quote">&gt; has to slow down all vma updates fast paths or the revalidation</span>
<span class="quote">&gt; slowsdown performance for page fault too much (as it always</span>
<span class="quote">&gt; revalidates).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it would be much better to go speculative only when there&#39;s</span>
<span class="quote">&gt; &quot;detected&quot; runtime contention on the mmap_sem with</span>
<span class="quote">&gt; down_read_trylock_if_not_hold() and that will make the revalidation</span>
<span class="quote">&gt; cost not an issue to worry about because normally we won&#39;t have to</span>
<span class="quote">&gt; revalidate the vma at all during page fault. In turn by making the</span>
<span class="quote">&gt; revalidation more expensive by starting a vma rbtree lookup from</span>
<span class="quote">&gt; scratch, we can drop the sequence number entirely and that should</span>
<span class="quote">&gt; simplify the patch tremendously because all vm_write_begin/end would</span>
<span class="quote">&gt; disappear from the patch and in turn the mmap/brk slowdown measured by</span>
<span class="quote">&gt; the message-id above, should disappear as well.</span>

As I mentioned above, I&#39;m not sure about checking the lock contention when
entering the page fault path, checking for the mm-&gt;mm_count or a dedicated
mm flags should be enough, but removing the sequence lock would be a very
good simplification. I&#39;ll dig further here, and come back soon.

Thanks a lot,
Laurent.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Nov. 2, 2017, 5:25 p.m.</div>
<pre class="content">
On 02/11/2017 16:16, Laurent Dufour wrote:
<span class="quote">&gt; Hi Andrea,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for reviewing this series, and sorry for the late answer, I took few</span>
<span class="quote">&gt; days off...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 26/10/2017 12:18, Andrea Arcangeli wrote:</span>
<span class="quote">&gt;&gt; Hello Laurent,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Message-ID: &lt;7ca80231-fe02-a3a7-84bc-ce81690ea051@intel.com&gt; shows</span>
<span class="quote">&gt;&gt; significant slowdown even for brk/malloc ops both single and</span>
<span class="quote">&gt;&gt; multi threaded.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The single threaded case I think is the most important because it has</span>
<span class="quote">&gt;&gt; zero chance of getting back any benefit later during page faults.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Could you check if:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 1. it&#39;s possible change vm_write_begin to be a noop if mm-&gt;mm_count is</span>
<span class="quote">&gt;&gt;    &lt;= 1? Hint: clone() will run single threaded so there&#39;s no way it can run</span>
<span class="quote">&gt;&gt;    in the middle of a being/end critical section (clone could set an</span>
<span class="quote">&gt;&gt;    MMF flag to possibly keep the sequence counter activated if a child</span>
<span class="quote">&gt;&gt;    thread exits and mm_count drops to 1 while the other cpu is in the</span>
<span class="quote">&gt;&gt;    middle of a critical section in the other thread).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This sounds to be a good idea, I&#39;ll dig on that.</span>
<span class="quote">&gt; The major risk here is to have a thread calling vm_*_begin() with</span>
<span class="quote">&gt; mm-&gt;mm_count &gt; 1 and later calling vm_*_end() with mm-&gt;mm_count &lt;= 1, but</span>
<span class="quote">&gt; as you mentioned we should find a way to work around this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 2. Same thing with RCU freeing of vmas. Wouldn&#39;t it be nicer if RCU</span>
<span class="quote">&gt;&gt;    freeing happened only once a MMF flag is set? That will at least</span>
<span class="quote">&gt;&gt;    reduce the risk of temporary memory waste until the next RCU grace</span>
<span class="quote">&gt;&gt;    period. The read of the MMF will scale fine. Of course to allow</span>
<span class="quote">&gt;&gt;    point 1 and 2 then the page fault should also take the mmap_sem</span>
<span class="quote">&gt;&gt;    until the MMF flag is set.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we could also deal with the mm-&gt;mm_count value here, if there is</span>
<span class="quote">&gt; only one thread, no need to postpone the VMA&#39;s free operation. Isn&#39;t it ?</span>
<span class="quote">&gt; Also, if mm-&gt;mm_count &lt;= 1, there is no need to try the speculative path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Could you also investigate a much bigger change: I wonder if it&#39;s</span>
<span class="quote">&gt;&gt; possible to drop the sequence number entirely from the vma and stop</span>
<span class="quote">&gt;&gt; using sequence numbers entirely (which is likely the source of the</span>
<span class="quote">&gt;&gt; single threaded regression in point 1 that may explain the report in</span>
<span class="quote">&gt;&gt; the above message-id), and just call the vma rbtree lookup once again</span>
<span class="quote">&gt;&gt; and check that everything is still the same in the vma and the PT lock</span>
<span class="quote">&gt;&gt; obtained is still a match to finish the anon page fault and fill the</span>
<span class="quote">&gt;&gt; pte?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s an interesting idea. The big deal here would be to detect that the</span>
<span class="quote">&gt; VMA has been touched in our back, but there are not so much VMA&#39;s fields</span>
<span class="quote">&gt; involved in the speculative path so that sounds reasonable. The other point</span>
<span class="quote">&gt; is to identify the impact of the vma rbtree lookup, it&#39;s also a known</span>
<span class="quote">&gt; order, but there is the vma_srcu&#39;s lock involved.</span>

I think there is some memory barrier missing when the VMA is modified so
currently the modifications done in the VMA structure may not be written
down at the time the pte is locked. So doing that change will also requires
to call smp_wmb() before locking the page tables. In the current patch this
is ensured by the call to write_seqcount_end().
Doing so will still require to have a memory barrier when touching the VMA.
Not sure we get far better performance compared to the sequence count
change. But I&#39;ll give it a try anyway ;)
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt; Then of course we also need to add a method to the read-write</span>
<span class="quote">&gt;&gt; semaphore so it tells us if there&#39;s already one user holding the read</span>
<span class="quote">&gt;&gt; mmap_sem and we&#39;re the second one.  If we&#39;re the second one (or more</span>
<span class="quote">&gt;&gt; than second) only then we should skip taking the down_read mmap_sem.</span>
<span class="quote">&gt;&gt; Even a multithreaded app won&#39;t ever skip taking the mmap_sem until</span>
<span class="quote">&gt;&gt; there&#39;s sign of runtime contention, and it won&#39;t have to run the way</span>
<span class="quote">&gt;&gt; more expensive sequence number-less revalidation during page faults,</span>
<span class="quote">&gt;&gt; unless we get an immediate scalability payoff because we already know</span>
<span class="quote">&gt;&gt; the mmap_sem is already contended and there are multiple nested</span>
<span class="quote">&gt;&gt; threads in the page fault handler of the same mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The problem is that we may have a thread entering the page fault path,</span>
<span class="quote">&gt; seeing that the mmap_sem is free, grab it and continue processing the page</span>
<span class="quote">&gt; fault. Then another thread is entering mprotect or any other mm service</span>
<span class="quote">&gt; which grab the mmap_sem and it will be blocked until the page fault is</span>
<span class="quote">&gt; done. The idea with the speculative page fault is also to not block the</span>
<span class="quote">&gt; other thread which may need to grab the mmap_sem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Perhaps we&#39;d need something more advanced than a</span>
<span class="quote">&gt;&gt; down_read_trylock_if_not_hold() (which has to guaranteed not to write</span>
<span class="quote">&gt;&gt; to any cacheline) and we&#39;ll have to count the per-thread exponential</span>
<span class="quote">&gt;&gt; backoff of mmap_sem frequency, but starting with</span>
<span class="quote">&gt;&gt; down_read_trylock_if_not_hold() would be good I think.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is not how the current patch works, the current patch uses a</span>
<span class="quote">&gt;&gt; sequence number because it pretends to go lockless always and in turn</span>
<span class="quote">&gt;&gt; has to slow down all vma updates fast paths or the revalidation</span>
<span class="quote">&gt;&gt; slowsdown performance for page fault too much (as it always</span>
<span class="quote">&gt;&gt; revalidates).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think it would be much better to go speculative only when there&#39;s</span>
<span class="quote">&gt;&gt; &quot;detected&quot; runtime contention on the mmap_sem with</span>
<span class="quote">&gt;&gt; down_read_trylock_if_not_hold() and that will make the revalidation</span>
<span class="quote">&gt;&gt; cost not an issue to worry about because normally we won&#39;t have to</span>
<span class="quote">&gt;&gt; revalidate the vma at all during page fault. In turn by making the</span>
<span class="quote">&gt;&gt; revalidation more expensive by starting a vma rbtree lookup from</span>
<span class="quote">&gt;&gt; scratch, we can drop the sequence number entirely and that should</span>
<span class="quote">&gt;&gt; simplify the patch tremendously because all vm_write_begin/end would</span>
<span class="quote">&gt;&gt; disappear from the patch and in turn the mmap/brk slowdown measured by</span>
<span class="quote">&gt;&gt; the message-id above, should disappear as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As I mentioned above, I&#39;m not sure about checking the lock contention when</span>
<span class="quote">&gt; entering the page fault path, checking for the mm-&gt;mm_count or a dedicated</span>
<span class="quote">&gt; mm flags should be enough, but removing the sequence lock would be a very</span>
<span class="quote">&gt; good simplification. I&#39;ll dig further here, and come back soon.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks a lot,</span>
<span class="quote">&gt; Laurent.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - Nov. 2, 2017, 8:08 p.m.</div>
<pre class="content">
On Thu, Nov 02, 2017 at 06:25:11PM +0100, Laurent Dufour wrote:
<span class="quote">&gt; I think there is some memory barrier missing when the VMA is modified so</span>
<span class="quote">&gt; currently the modifications done in the VMA structure may not be written</span>
<span class="quote">&gt; down at the time the pte is locked. So doing that change will also requires</span>
<span class="quote">&gt; to call smp_wmb() before locking the page tables. In the current patch this</span>
<span class="quote">&gt; is ensured by the call to write_seqcount_end().</span>
<span class="quote">&gt; Doing so will still require to have a memory barrier when touching the VMA.</span>
<span class="quote">&gt; Not sure we get far better performance compared to the sequence count</span>
<span class="quote">&gt; change. But I&#39;ll give it a try anyway ;)</span>

Luckily smp_wmb is a noop on x86. I would suggest to ignore the above
issue completely if you give it a try, and then if this performs, we
can just embed a smp_wmb() before spin_lock() somewhere in
pte_offset_map_lock/pte_lockptr/spin_lock_nested for those archs whose
spin_lock isn&#39;t a smp_wmb() equivalent. I would focus at flushing
writes before every pagetable spin_lock for non-x86 archs, rather than
after all vma modifications. That should be easier to keep under
control and it&#39;s going to be more efficient too as if something there
are fewer spin locks than vma modifications.

For non-x86 archs we may then need a smp_wmb__before_spin_lock. That
looks more self contained than surrounding all vma modifications and
it&#39;s a noop on x86 anyway.

I thought about the contention detection logic too yesterday: to
detect contention we could have a mm-&gt;mmap_sem_contention_jiffies and
if down_read_trylock_exclusive() [same as down_read_if_not_hold in
prev mail] fails (and it&#39;ll fail if either read or write mmap_sem is
hold, so also convering mremap/mprotect etc..) we set
mm-&gt;mmap_sem_contention_jiffies = jiffies and then to know if you must
not touch the mmap_sem at all, you compare jiffies against
mmap_sem_contention_jiffies, if it&#39;s equal we go speculative. If
that&#39;s not enough we can just keep going speculative for a few more
jiffies with time_before(). The srcu lock is non concerning because the
inc/dec of the fast path is in per-cpu cacheline of course, no false
sharing possible there or it wouldn&#39;t be any better than a normal lock.

The vma revalidation is already done by khugepaged and mm/userfaultfd,
both need to drop the mmap_sem and continue working on the pagetables,
so we already know it&#39;s workable and not too slow.

Summarizing.. by using a runtime contention triggered speculative
design that goes speculative only when contention is runtime-detected
using the above logic (or equivalent), and by having to revalidate the
vma by hand with find_vma without knowing instantly if the vma become
stale, we will run with a substantially slower speculative page fault
than with your current speculative always-on design, but the slower
speculative page fault runtime will still scale 100% in SMP so it
should still be faster on large SMP systems. The pros is that it won&#39;t
regress the mmap/brk vma modifications. The whole complexity of
tracking the vma modifications should also go away and the resulting
code should be more maintainable and less risky to break in subtle
ways impossible to reproduce.

Thanks!
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Nov. 6, 2017, 9:47 a.m.</div>
<pre class="content">
Hi Andrea,

On 02/11/2017 21:08, Andrea Arcangeli wrote:
<span class="quote">&gt; On Thu, Nov 02, 2017 at 06:25:11PM +0100, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; I think there is some memory barrier missing when the VMA is modified so</span>
<span class="quote">&gt;&gt; currently the modifications done in the VMA structure may not be written</span>
<span class="quote">&gt;&gt; down at the time the pte is locked. So doing that change will also requires</span>
<span class="quote">&gt;&gt; to call smp_wmb() before locking the page tables. In the current patch this</span>
<span class="quote">&gt;&gt; is ensured by the call to write_seqcount_end().</span>
<span class="quote">&gt;&gt; Doing so will still require to have a memory barrier when touching the VMA.</span>
<span class="quote">&gt;&gt; Not sure we get far better performance compared to the sequence count</span>
<span class="quote">&gt;&gt; change. But I&#39;ll give it a try anyway ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Luckily smp_wmb is a noop on x86. I would suggest to ignore the above</span>
<span class="quote">&gt; issue completely if you give it a try, and then if this performs, we</span>
<span class="quote">&gt; can just embed a smp_wmb() before spin_lock() somewhere in</span>
<span class="quote">&gt; pte_offset_map_lock/pte_lockptr/spin_lock_nested for those archs whose</span>
<span class="quote">&gt; spin_lock isn&#39;t a smp_wmb() equivalent. I would focus at flushing</span>
<span class="quote">&gt; writes before every pagetable spin_lock for non-x86 archs, rather than</span>
<span class="quote">&gt; after all vma modifications. That should be easier to keep under</span>
<span class="quote">&gt; control and it&#39;s going to be more efficient too as if something there</span>
<span class="quote">&gt; are fewer spin locks than vma modifications.</span>

I do agree that would simplify the patch series a lot.
I&#39;ll double check that pte lock is not done in a loop other wise having
smp_wmb() there will be bad.

Another point I&#39;m trying to double check is that we may have inconsistency
while reading the vma&#39;s flags in the page fault path until the memory
barrier got it in the VMA&#39;s changing path. Especially we may have vm_flags
and vm_page_prot not matching at all, which couldn&#39;t happen when checking
for the vm_sequence count.
<span class="quote">
&gt; </span>
<span class="quote">&gt; For non-x86 archs we may then need a smp_wmb__before_spin_lock. That</span>
<span class="quote">&gt; looks more self contained than surrounding all vma modifications and</span>
<span class="quote">&gt; it&#39;s a noop on x86 anyway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought about the contention detection logic too yesterday: to</span>
<span class="quote">&gt; detect contention we could have a mm-&gt;mmap_sem_contention_jiffies and</span>
<span class="quote">&gt; if down_read_trylock_exclusive() [same as down_read_if_not_hold in</span>
<span class="quote">&gt; prev mail] fails (and it&#39;ll fail if either read or write mmap_sem is</span>
<span class="quote">&gt; hold, so also convering mremap/mprotect etc..) we set</span>
<span class="quote">&gt; mm-&gt;mmap_sem_contention_jiffies = jiffies and then to know if you must</span>
<span class="quote">&gt; not touch the mmap_sem at all, you compare jiffies against</span>
<span class="quote">&gt; mmap_sem_contention_jiffies, if it&#39;s equal we go speculative. If</span>
<span class="quote">&gt; that&#39;s not enough we can just keep going speculative for a few more</span>
<span class="quote">&gt; jiffies with time_before(). The srcu lock is non concerning because the</span>
<span class="quote">&gt; inc/dec of the fast path is in per-cpu cacheline of course, no false</span>
<span class="quote">&gt; sharing possible there or it wouldn&#39;t be any better than a normal lock.</span>

I&#39;m sorry, I should have missed something here. I can&#39;t see how this would
help fixing the case where a thread is entering the page fault handler
seeing that no one else has the mmap_sem and then grab it. While it is
processing the page fault another thread is entering mprotect for instance
and thus will wait for the mmap_sem to be released by the thread processing
the page fault.

Cheers,
Laurent.
<span class="quote">
&gt; The vma revalidation is already done by khugepaged and mm/userfaultfd,</span>
<span class="quote">&gt; both need to drop the mmap_sem and continue working on the pagetables,</span>
<span class="quote">&gt; so we already know it&#39;s workable and not too slow.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Summarizing.. by using a runtime contention triggered speculative</span>
<span class="quote">&gt; design that goes speculative only when contention is runtime-detected</span>
<span class="quote">&gt; using the above logic (or equivalent), and by having to revalidate the</span>
<span class="quote">&gt; vma by hand with find_vma without knowing instantly if the vma become</span>
<span class="quote">&gt; stale, we will run with a substantially slower speculative page fault</span>
<span class="quote">&gt; than with your current speculative always-on design, but the slower</span>
<span class="quote">&gt; speculative page fault runtime will still scale 100% in SMP so it</span>
<span class="quote">&gt; should still be faster on large SMP systems. The pros is that it won&#39;t</span>
<span class="quote">&gt; regress the mmap/brk vma modifications. The whole complexity of</span>
<span class="quote">&gt; tracking the vma modifications should also go away and the resulting</span>
<span class="quote">&gt; code should be more maintainable and less risky to break in subtle</span>
<span class="quote">&gt; ways impossible to reproduce.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks!</span>
<span class="quote">&gt; Andrea</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 0bf9e423aa99..4fc37f88437c 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -1155,8 +1155,11 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 					goto out_mm;
 				}
 				for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_del">-					vma-&gt;vm_flags &amp;= ~VM_SOFTDIRTY;</span>
<span class="p_add">+					vm_write_begin(vma);</span>
<span class="p_add">+					WRITE_ONCE(vma-&gt;vm_flags,</span>
<span class="p_add">+						   vma-&gt;vm_flags &amp; ~VM_SOFTDIRTY);</span>
 					vma_set_page_prot(vma);
<span class="p_add">+					vm_write_end(vma);</span>
 				}
 				downgrade_write(&amp;mm-&gt;mmap_sem);
 				break;
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 1c713fd5b3e6..426af4fd407c 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -640,8 +640,11 @@</span> <span class="p_context"> int dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)</span>
 
 	octx = vma-&gt;vm_userfaultfd_ctx.ctx;
 	if (!octx || !(octx-&gt;features &amp; UFFD_FEATURE_EVENT_FORK)) {
<span class="p_add">+		vm_write_begin(vma);</span>
 		vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
<span class="p_del">-		vma-&gt;vm_flags &amp;= ~(VM_UFFD_WP | VM_UFFD_MISSING);</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_flags,</span>
<span class="p_add">+			   vma-&gt;vm_flags &amp; ~(VM_UFFD_WP | VM_UFFD_MISSING));</span>
<span class="p_add">+		vm_write_end(vma);</span>
 		return 0;
 	}
 
<span class="p_chunk">@@ -866,8 +869,10 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 			vma = prev;
 		else
 			prev = vma;
<span class="p_del">-		vma-&gt;vm_flags = new_flags;</span>
<span class="p_add">+		vm_write_begin(vma);</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_flags, new_flags);</span>
 		vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
<span class="p_add">+		vm_write_end(vma);</span>
 	}
 	up_write(&amp;mm-&gt;mmap_sem);
 	mmput(mm);
<span class="p_chunk">@@ -1425,8 +1430,10 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 		 * the next vma was merged into the current one and
 		 * the current one has not been updated yet.
 		 */
<span class="p_del">-		vma-&gt;vm_flags = new_flags;</span>
<span class="p_add">+		vm_write_begin(vma);</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_flags, new_flags);</span>
 		vma-&gt;vm_userfaultfd_ctx.ctx = ctx;
<span class="p_add">+		vm_write_end(vma);</span>
 
 	skip:
 		prev = vma;
<span class="p_chunk">@@ -1583,8 +1590,10 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 		 * the next vma was merged into the current one and
 		 * the current one has not been updated yet.
 		 */
<span class="p_del">-		vma-&gt;vm_flags = new_flags;</span>
<span class="p_add">+		vm_write_begin(vma);</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_flags, new_flags);</span>
 		vma-&gt;vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
<span class="p_add">+		vm_write_end(vma);</span>
 
 	skip:
 		prev = vma;
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index c01f177a1120..f723d42140db 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -1005,6 +1005,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (mm_find_pmd(mm, address) != pmd)
 		goto out;
 
<span class="p_add">+	vm_write_begin(vma);</span>
 	anon_vma_lock_write(vma-&gt;anon_vma);
 
 	pte = pte_offset_map(pmd, address);
<span class="p_chunk">@@ -1040,6 +1041,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 		pmd_populate(mm, pmd, pmd_pgtable(_pmd));
 		spin_unlock(pmd_ptl);
 		anon_vma_unlock_write(vma-&gt;anon_vma);
<span class="p_add">+		vm_write_end(vma);</span>
 		result = SCAN_FAIL;
 		goto out;
 	}
<span class="p_chunk">@@ -1074,6 +1076,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	set_pmd_at(mm, address, pmd, _pmd);
 	update_mmu_cache_pmd(vma, address, pmd);
 	spin_unlock(pmd_ptl);
<span class="p_add">+	vm_write_end(vma);</span>
 
 	*hpage = NULL;
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index acdd39fb1f5d..707e0657b33f 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -183,7 +183,9 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 	/*
 	 * vm_flags is protected by the mmap_sem held in write mode.
 	 */
<span class="p_del">-	vma-&gt;vm_flags = new_flags;</span>
<span class="p_add">+	vm_write_begin(vma);</span>
<span class="p_add">+	WRITE_ONCE(vma-&gt;vm_flags, new_flags);</span>
<span class="p_add">+	vm_write_end(vma);</span>
 out:
 	return error;
 }
<span class="p_chunk">@@ -451,9 +453,11 @@</span> <span class="p_context"> static void madvise_free_page_range(struct mmu_gather *tlb,</span>
 		.private = tlb,
 	};
 
<span class="p_add">+	vm_write_begin(vma);</span>
 	tlb_start_vma(tlb, vma);
 	walk_page_range(addr, end, &amp;free_walk);
 	tlb_end_vma(tlb, vma);
<span class="p_add">+	vm_write_end(vma);</span>
 }
 
 static int madvise_free_single_vma(struct vm_area_struct *vma,
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index a2af6d58a68f..72645928daa0 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -379,8 +379,11 @@</span> <span class="p_context"> void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)</span>
 	struct vm_area_struct *vma;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)</span>
<span class="p_add">+	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		vm_write_begin(vma);</span>
 		mpol_rebind_policy(vma-&gt;vm_policy, new);
<span class="p_add">+		vm_write_end(vma);</span>
<span class="p_add">+	}</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 }
 
<span class="p_chunk">@@ -578,9 +581,11 @@</span> <span class="p_context"> unsigned long change_prot_numa(struct vm_area_struct *vma,</span>
 {
 	int nr_updated;
 
<span class="p_add">+	vm_write_begin(vma);</span>
 	nr_updated = change_protection(vma, addr, end, PAGE_NONE, 0, 1);
 	if (nr_updated)
 		count_vm_numa_events(NUMA_PTE_UPDATES, nr_updated);
<span class="p_add">+	vm_write_end(vma);</span>
 
 	return nr_updated;
 }
<span class="p_chunk">@@ -681,6 +686,7 @@</span> <span class="p_context"> static int vma_replace_policy(struct vm_area_struct *vma,</span>
 	if (IS_ERR(new))
 		return PTR_ERR(new);
 
<span class="p_add">+	vm_write_begin(vma);</span>
 	if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;set_policy) {
 		err = vma-&gt;vm_ops-&gt;set_policy(vma, new);
 		if (err)
<span class="p_chunk">@@ -688,11 +694,17 @@</span> <span class="p_context"> static int vma_replace_policy(struct vm_area_struct *vma,</span>
 	}
 
 	old = vma-&gt;vm_policy;
<span class="p_del">-	vma-&gt;vm_policy = new; /* protected by mmap_sem */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The speculative page fault handler access this field without</span>
<span class="p_add">+	 * hodling the mmap_sem.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WRITE_ONCE(vma-&gt;vm_policy,  new);</span>
<span class="p_add">+	vm_write_end(vma);</span>
 	mpol_put(old);
 
 	return 0;
  err_out:
<span class="p_add">+	vm_write_end(vma);</span>
 	mpol_put(new);
 	return err;
 }
<span class="p_chunk">@@ -1562,23 +1574,28 @@</span> <span class="p_context"> COMPAT_SYSCALL_DEFINE6(mbind, compat_ulong_t, start, compat_ulong_t, len,</span>
 struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
 						unsigned long addr)
 {
<span class="p_del">-	struct mempolicy *pol = NULL;</span>
<span class="p_add">+	struct mempolicy *pol;</span>
 
<span class="p_del">-	if (vma) {</span>
<span class="p_del">-		if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;get_policy) {</span>
<span class="p_del">-			pol = vma-&gt;vm_ops-&gt;get_policy(vma, addr);</span>
<span class="p_del">-		} else if (vma-&gt;vm_policy) {</span>
<span class="p_del">-			pol = vma-&gt;vm_policy;</span>
<span class="p_add">+	if (!vma)</span>
<span class="p_add">+		return NULL;</span>
 
<span class="p_del">-			/*</span>
<span class="p_del">-			 * shmem_alloc_page() passes MPOL_F_SHARED policy with</span>
<span class="p_del">-			 * a pseudo vma whose vma-&gt;vm_ops=NULL. Take a reference</span>
<span class="p_del">-			 * count on these policies which will be dropped by</span>
<span class="p_del">-			 * mpol_cond_put() later</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			if (mpol_needs_cond_ref(pol))</span>
<span class="p_del">-				mpol_get(pol);</span>
<span class="p_del">-		}</span>
<span class="p_add">+	if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;get_policy)</span>
<span class="p_add">+		return vma-&gt;vm_ops-&gt;get_policy(vma, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This could be called without holding the mmap_sem in the</span>
<span class="p_add">+	 * speculative page fault handler&#39;s path.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pol = READ_ONCE(vma-&gt;vm_policy);</span>
<span class="p_add">+	if (pol) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * shmem_alloc_page() passes MPOL_F_SHARED policy with</span>
<span class="p_add">+		 * a pseudo vma whose vma-&gt;vm_ops=NULL. Take a reference</span>
<span class="p_add">+		 * count on these policies which will be dropped by</span>
<span class="p_add">+		 * mpol_cond_put() later</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mpol_needs_cond_ref(pol))</span>
<span class="p_add">+			mpol_get(pol);</span>
 	}
 
 	return pol;
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index 4d009350893f..83e49f99ad38 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -438,7 +438,9 @@</span> <span class="p_context"> static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,</span>
 void munlock_vma_pages_range(struct vm_area_struct *vma,
 			     unsigned long start, unsigned long end)
 {
<span class="p_del">-	vma-&gt;vm_flags &amp;= VM_LOCKED_CLEAR_MASK;</span>
<span class="p_add">+	vm_write_begin(vma);</span>
<span class="p_add">+	WRITE_ONCE(vma-&gt;vm_flags, vma-&gt;vm_flags &amp; VM_LOCKED_CLEAR_MASK);</span>
<span class="p_add">+	vm_write_end(vma);</span>
 
 	while (start &lt; end) {
 		struct page *page;
<span class="p_chunk">@@ -562,10 +564,11 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 	 * It&#39;s okay if try_to_unmap_one unmaps a page just after we
 	 * set VM_LOCKED, populate_vma_page_range will bring it back.
 	 */
<span class="p_del">-</span>
<span class="p_del">-	if (lock)</span>
<span class="p_del">-		vma-&gt;vm_flags = newflags;</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (lock) {</span>
<span class="p_add">+		vm_write_begin(vma);</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_flags, newflags);</span>
<span class="p_add">+		vm_write_end(vma);</span>
<span class="p_add">+	} else</span>
 		munlock_vma_pages_range(vma, start, end);
 
 out:
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 0e90b469fd97..e28136cd3275 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -847,17 +847,18 @@</span> <span class="p_context"> int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
 	}
 
 	if (start != vma-&gt;vm_start) {
<span class="p_del">-		vma-&gt;vm_start = start;</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_start, start);</span>
 		start_changed = true;
 	}
 	if (end != vma-&gt;vm_end) {
<span class="p_del">-		vma-&gt;vm_end = end;</span>
<span class="p_add">+		WRITE_ONCE(vma-&gt;vm_end, end);</span>
 		end_changed = true;
 	}
<span class="p_del">-	vma-&gt;vm_pgoff = pgoff;</span>
<span class="p_add">+	WRITE_ONCE(vma-&gt;vm_pgoff, pgoff);</span>
 	if (adjust_next) {
<span class="p_del">-		next-&gt;vm_start += adjust_next &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-		next-&gt;vm_pgoff += adjust_next;</span>
<span class="p_add">+		WRITE_ONCE(next-&gt;vm_start,</span>
<span class="p_add">+			   next-&gt;vm_start + (adjust_next &lt;&lt; PAGE_SHIFT));</span>
<span class="p_add">+		WRITE_ONCE(next-&gt;vm_pgoff, next-&gt;vm_pgoff + adjust_next);</span>
 	}
 
 	if (root) {
<span class="p_chunk">@@ -1754,6 +1755,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 out:
 	perf_event_mmap(vma);
 
<span class="p_add">+	vm_write_begin(vma);</span>
 	vm_stat_account(mm, vm_flags, len &gt;&gt; PAGE_SHIFT);
 	if (vm_flags &amp; VM_LOCKED) {
 		if (!((vm_flags &amp; VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
<span class="p_chunk">@@ -1777,6 +1779,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 	vma-&gt;vm_flags |= VM_SOFTDIRTY;
 
 	vma_set_page_prot(vma);
<span class="p_add">+	vm_write_end(vma);</span>
 
 	return addr;
 
<span class="p_chunk">@@ -2405,8 +2408,8 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 					mm-&gt;locked_vm += grow;
 				vm_stat_account(mm, vma-&gt;vm_flags, grow);
 				anon_vma_interval_tree_pre_update_vma(vma);
<span class="p_del">-				vma-&gt;vm_start = address;</span>
<span class="p_del">-				vma-&gt;vm_pgoff -= grow;</span>
<span class="p_add">+				WRITE_ONCE(vma-&gt;vm_start, address);</span>
<span class="p_add">+				WRITE_ONCE(vma-&gt;vm_pgoff, vma-&gt;vm_pgoff - grow);</span>
 				anon_vma_interval_tree_post_update_vma(vma);
 				vma_gap_update(vma);
 				spin_unlock(&amp;mm-&gt;page_table_lock);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 6d3e2f082290..ce93c4f6af70 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -358,7 +358,8 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	 * vm_flags and vm_page_prot are protected by the mmap_sem
 	 * held in write mode.
 	 */
<span class="p_del">-	vma-&gt;vm_flags = newflags;</span>
<span class="p_add">+	vm_write_begin(vma);</span>
<span class="p_add">+	WRITE_ONCE(vma-&gt;vm_flags, newflags);</span>
 	dirty_accountable = vma_wants_writenotify(vma, vma-&gt;vm_page_prot);
 	vma_set_page_prot(vma);
 
<span class="p_chunk">@@ -373,6 +374,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 			(newflags &amp; VM_WRITE)) {
 		populate_vma_page_range(vma, start, end, NULL);
 	}
<span class="p_add">+	vm_write_end(vma);</span>
 
 	vm_stat_account(mm, oldflags, -nrpages);
 	vm_stat_account(mm, newflags, nrpages);
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index cfec004c4ff9..ca0b5cb6ed4d 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -301,6 +301,9 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 	if (!new_vma)
 		return -ENOMEM;
 
<span class="p_add">+	vm_write_begin(vma);</span>
<span class="p_add">+	vm_write_begin_nested(new_vma, SINGLE_DEPTH_NESTING);</span>
<span class="p_add">+</span>
 	moved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,
 				     need_rmap_locks);
 	if (moved_len &lt; old_len) {
<span class="p_chunk">@@ -317,6 +320,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		 */
 		move_page_tables(new_vma, new_addr, vma, old_addr, moved_len,
 				 true);
<span class="p_add">+		vm_write_end(vma);</span>
 		vma = new_vma;
 		old_len = new_len;
 		old_addr = new_addr;
<span class="p_chunk">@@ -325,7 +329,9 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		mremap_userfaultfd_prep(new_vma, uf);
 		arch_remap(mm, old_addr, old_addr + old_len,
 			   new_addr, new_addr + new_len);
<span class="p_add">+		vm_write_end(vma);</span>
 	}
<span class="p_add">+	vm_write_end(new_vma);</span>
 
 	/* Conceal VM_ACCOUNT so old reservation is not undone */
 	if (vm_flags &amp; VM_ACCOUNT) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



