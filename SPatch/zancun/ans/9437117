
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v13,16/18] mm/hmm/migrate: new memory migration helper for use with device memory - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v13,16/18] mm/hmm/migrate: new memory migration helper for use with device memory</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 18, 2016, 6:18 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1479493107-982-17-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9437117/mbox/"
   >mbox</a>
|
   <a href="/patch/9437117/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9437117/">/patch/9437117/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B043D60755 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:19:16 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9CFF4294D3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:19:16 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 91A9C296C3; Fri, 18 Nov 2016 17:19:16 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2243F294D3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:19:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754402AbcKRRTE (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 18 Nov 2016 12:19:04 -0500
Received: from mx1.redhat.com ([209.132.183.28]:46226 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932399AbcKRRR5 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 18 Nov 2016 12:17:57 -0500
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 951988E675;
	Fri, 18 Nov 2016 17:17:57 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id uAIHHd9C016767; Fri, 18 Nov 2016 12:17:56 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Jatin Kumar &lt;jakumar@nvidia.com&gt;, Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM v13 16/18] mm/hmm/migrate: new memory migration helper for use
	with device memory
Date: Fri, 18 Nov 2016 13:18:25 -0500
Message-Id: &lt;1479493107-982-17-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1479493107-982-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1479493107-982-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.25]);
	Fri, 18 Nov 2016 17:17:57 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 18, 2016, 6:18 p.m.</div>
<pre class="content">
This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory
(which can be allocated through special allocator). It differs from
numa migration by working on a range of virtual address and thus by
doing migration in chunk that can be large enough to use DMA engine
or special copy offloading engine.

Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...). As
an example IBM platform with CAPI bus can make use of this feature
to migrate between regular memory and CAPI device memory. New CPU
architecture with a pool of high performance memory not manage as
cache but presented as regular memory (while being faster and with
lower latency than DDR) will also be prime user of this patch.

Migration to private device memory will be usefull for device that
have large pool of such like GPU, NVidia plans to use HMM for that.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 include/linux/hmm.h |  54 ++++-
 mm/migrate.c        | 584 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 635 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Nov. 18, 2016, 7:57 p.m.</div>
<pre class="content">
Jérôme Glisse &lt;jglisse@redhat.com&gt; writes:
<span class="quote">
&gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; or special copy offloading engine.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; lower latency than DDR) will also be prime user of this patch.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Migration to private device memory will be usefull for device that</span>
<span class="quote">&gt; have large pool of such like GPU, NVidia plans to use HMM for that.</span>
<span class="quote">&gt;</span>



..............
<span class="quote">

&gt;+</span>
<span class="quote">&gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; +				unsigned long start,</span>
<span class="quote">&gt; +				unsigned long end,</span>
<span class="quote">&gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; +	int pages = 0;</span>
<span class="quote">&gt; +	pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; +		goto again;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; +		unsigned long pfn;</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; +		bool write;</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="quote">&gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; +		get_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			pages++;</span>
<span class="quote">&gt; +		}</span>

Can you explain this. What does a failure to lock means here. Also why
convert the pte to migration entries here ? We do that in try_to_unmap right ?
<span class="quote">

&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; +	if (pages)</span>
<span class="quote">&gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 18, 2016, 8:15 p.m.</div>
<pre class="content">
On Sat, Nov 19, 2016 at 01:27:28AM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Jérôme Glisse &lt;jglisse@redhat.com&gt; writes:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &gt;+</span>
<span class="quote">&gt; &gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; &gt; +				unsigned long start,</span>
<span class="quote">&gt; &gt; +				unsigned long end,</span>
<span class="quote">&gt; &gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; &gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; &gt; +	int pages = 0;</span>
<span class="quote">&gt; &gt; +	pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +again:</span>
<span class="quote">&gt; &gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; &gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; &gt; +		goto again;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; &gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; &gt; +		unsigned long pfn;</span>
<span class="quote">&gt; &gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +		struct page *page;</span>
<span class="quote">&gt; &gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; &gt; +		bool write;</span>
<span class="quote">&gt; &gt; +		pte_t pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; &gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; &gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; &gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="quote">&gt; &gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; &gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; &gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; &gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; &gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; &gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; &gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; &gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; &gt; +		get_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; &gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; &gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			pages++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you explain this. What does a failure to lock means here. Also why</span>
<span class="quote">&gt; convert the pte to migration entries here ? We do that in try_to_unmap right ?</span>

This an optimization for the usual case where the memory is only use in one
process and that no concurrent migration/memory event is happening. Basicly
if we can lock the page without waiting then we unmap it and the later call
to try_to_unmap() is a no op.

This is purely to optimize this common case. In short it is doing try_to_unmap()
work ahead of time.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Nov. 19, 2016, 2:32 p.m.</div>
<pre class="content">
Jérôme Glisse &lt;jglisse@redhat.com&gt; writes:
<span class="quote">
&gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; or special copy offloading engine.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; lower latency than DDR) will also be prime user of this patch.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Migration to private device memory will be usefull for device that</span>
<span class="quote">&gt; have large pool of such like GPU, NVidia plans to use HMM for that.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hmm.h |  54 ++++-</span>
<span class="quote">&gt;  mm/migrate.c        | 584 ++++++++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  2 files changed, 635 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; index c79abfc..9777309 100644</span>
<span class="quote">&gt; --- a/include/linux/hmm.h</span>
<span class="quote">&gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; @@ -101,10 +101,13 @@ struct hmm;</span>
<span class="quote">&gt;   * HMM_PFN_EMPTY: corresponding CPU page table entry is none (pte_none() true)</span>
<span class="quote">&gt;   * HMM_PFN_FAULT: use by hmm_vma_fault() to signify which address need faulting</span>
<span class="quote">&gt;   * HMM_PFN_DEVICE: this is device memory (ie a ZONE_DEVICE page)</span>
<span class="quote">&gt; + * HMM_PFN_LOCKED: underlying struct page is lock</span>
<span class="quote">&gt;   * HMM_PFN_SPECIAL: corresponding CPU page table entry is special ie result of</span>
<span class="quote">&gt;   *      vm_insert_pfn() or vm_insert_page() and thus should not be mirror by a</span>
<span class="quote">&gt;   *      device (the entry will never have HMM_PFN_VALID set and the pfn value</span>
<span class="quote">&gt;   *      is undefine)</span>
<span class="quote">&gt; + * HMM_PFN_MIGRATE: use by hmm_vma_migrate() to signify which address can be</span>
<span class="quote">&gt; + *      migrated</span>
<span class="quote">&gt;   * HMM_PFN_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; @@ -116,9 +119,11 @@ typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt;  #define HMM_PFN_EMPTY (1 &lt;&lt; 4)</span>
<span class="quote">&gt;  #define HMM_PFN_FAULT (1 &lt;&lt; 5)</span>
<span class="quote">&gt;  #define HMM_PFN_DEVICE (1 &lt;&lt; 6)</span>
<span class="quote">&gt; -#define HMM_PFN_SPECIAL (1 &lt;&lt; 7)</span>
<span class="quote">&gt; -#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 8)</span>
<span class="quote">&gt; -#define HMM_PFN_SHIFT 9</span>
<span class="quote">&gt; +#define HMM_PFN_LOCKED (1 &lt;&lt; 7)</span>
<span class="quote">&gt; +#define HMM_PFN_SPECIAL (1 &lt;&lt; 8)</span>
<span class="quote">&gt; +#define HMM_PFN_MIGRATE (1 &lt;&lt; 9)</span>
<span class="quote">&gt; +#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 10)</span>
<span class="quote">&gt; +#define HMM_PFN_SHIFT 11</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -323,6 +328,49 @@ bool hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		   hmm_pfn_t *pfns);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * struct hmm_migrate_ops - migrate operation callback</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="quote">&gt; + * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The new HMM migrate helper hmm_vma_migrate() allow memory migration to use</span>
<span class="quote">&gt; + * device DMA engine to perform copy from source to destination memory it also</span>
<span class="quote">&gt; + * allow caller to use its own memory allocator for destination memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="quote">&gt; + * the entry, for those it must clear the HMM_PFN_MIGRATE flag. The destination</span>
<span class="quote">&gt; + * page must lock and the corresponding hmm_pfn_t value in the array updated</span>
<span class="quote">&gt; + * with the HMM_PFN_MIGRATE and HMM_PFN_LOCKED flag set (and of course be a</span>
<span class="quote">&gt; + * valid entry). It is expected that the page allocated will have an elevated</span>
<span class="quote">&gt; + * refcount and that a put_page() will free the page. Device driver might want</span>
<span class="quote">&gt; + * to allocate with an extra-refcount if they want to control deallocation of</span>
<span class="quote">&gt; + * failed migration inside the finalize_and_map() callback.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Inside finalize_and_map() device driver must use the HMM_PFN_MIGRATE flag to</span>
<span class="quote">&gt; + * determine which page have been successfully migrated.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct hmm_migrate_ops {</span>
<span class="quote">&gt; +	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +			       unsigned long start,</span>
<span class="quote">&gt; +			       unsigned long end,</span>
<span class="quote">&gt; +			       hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +			       void *private);</span>
<span class="quote">&gt; +	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				 unsigned long start,</span>
<span class="quote">&gt; +				 unsigned long end,</span>
<span class="quote">&gt; +				 hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +				 void *private);</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		    unsigned long start,</span>
<span class="quote">&gt; +		    unsigned long end,</span>
<span class="quote">&gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +		    void *private);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt;  void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index d9ce8db..393d592 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -41,6 +41,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -421,6 +422,14 @@ int migrate_page_move_mapping(struct address_space *mapping,</span>
<span class="quote">&gt;  	int expected_count = 1 + extra_count;</span>
<span class="quote">&gt;  	void **pslot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="quote">&gt; +	 * the MEMORY_MOVABLE flag set (see include/linux/memory_hotplug.h).</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	expected_count += is_zone_device_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (!mapping) {</span>
<span class="quote">&gt;  		/* Anonymous page without mapping */</span>
<span class="quote">&gt;  		if (page_count(page) != expected_count)</span>
<span class="quote">&gt; @@ -2087,3 +2096,578 @@ out_unlock:</span>
<span class="quote">&gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* CONFIG_NUMA */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if defined(CONFIG_HMM)</span>
<span class="quote">&gt; +struct hmm_migrate {</span>
<span class="quote">&gt; +	struct vm_area_struct	*vma;</span>
<span class="quote">&gt; +	unsigned long		start;</span>
<span class="quote">&gt; +	unsigned long		end;</span>
<span class="quote">&gt; +	unsigned long		npages;</span>
<span class="quote">&gt; +	hmm_pfn_t		*pfns;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; +				unsigned long start,</span>
<span class="quote">&gt; +				unsigned long end,</span>
<span class="quote">&gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; +	int pages = 0;</span>
<span class="quote">&gt; +	pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; +		goto again;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; +		unsigned long pfn;</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; +		bool write;</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="quote">&gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>

Will that is_zone_device_page() be ever true ?, The pte is present in
the else patch can the struct page backing that come from zone device ?
<span class="quote">

&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; +			continue;</span>

What about page cache pages. Do we support that ? If not may be skip
that here ?
<span class="quote">


&gt; +</span>
<span class="quote">&gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; +		get_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			pages++;</span>
<span class="quote">&gt; +		}</span>

If this is an optimization, can we get that as a seperate patch with
addtional comments. ? How does take a successful page lock implies it is
not a shared mapping ?
<span class="quote">

&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; +	if (pages)</span>
<span class="quote">&gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>


So without the optimization the above function is suppose to raise the
refcount and collect all possible pfns tha we can migrate in the array ?
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mm_walk mm_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="quote">&gt; +	mm_walk.pte_entry = NULL;</span>
<span class="quote">&gt; +	mm_walk.pte_hole = NULL;</span>
<span class="quote">&gt; +	mm_walk.hugetlb_entry = NULL;</span>
<span class="quote">&gt; +	mm_walk.test_walk = NULL;</span>
<span class="quote">&gt; +	mm_walk.vma = migrate-&gt;vma;</span>
<span class="quote">&gt; +	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	mm_walk.private = migrate;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="quote">&gt; +					    migrate-&gt;start,</span>
<span class="quote">&gt; +					    migrate-&gt;end);</span>
<span class="quote">&gt; +	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="quote">&gt; +					  migrate-&gt;start,</span>
<span class="quote">&gt; +					  migrate-&gt;end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool hmm_migrate_page_check(struct page *page, int extra)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="quote">&gt; +	 * check them then regular page because they can be map with a pmd or</span>
<span class="quote">&gt; +	 * with a pte (split pte mapping).</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (PageCompound(page))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt; +		extra++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt; +	bool allow_drain = true;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	lru_add_drain();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt; +			lock_page(page);</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; +		}</span>

What does taking a page_lock protect against ? Can we document that ?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt; +			goto check;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt; +			lru_add_drain_all();</span>
<span class="quote">&gt; +			allow_drain = false;</span>
<span class="quote">&gt; +			goto again;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		} else</span>
<span class="quote">&gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +check:</span>
<span class="quote">&gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!restore)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (addr = migrate-&gt;start, i = 0; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +		unsigned long next, restart;</span>
<span class="quote">&gt; +		spinlock_t *ptl;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE)) {</span>
<span class="quote">&gt; +			addr += PAGE_SIZE;</span>
<span class="quote">&gt; +			i++;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		restart = addr;</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; +			addr = next;</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +			bool write;</span>
<span class="quote">&gt; +			pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			write = migrate-&gt;pfns[i] &amp; HMM_PFN_WRITE;</span>
<span class="quote">&gt; +			write &amp;= (vma-&gt;vm_flags &amp; VM_WRITE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Here it means pte must be a valid migration entry */</span>
<span class="quote">&gt; +			pte = ptep_get_and_clear(mm, addr, ptep);</span>


It is already a non present pte, so why use ptep_get_and_clear ? why not
*ptep ? Some archs does lot of additional stuff in get_and_clear. 
<span class="quote">
&gt; +			if (pte_none(pte) || pte_present(pte))</span>
<span class="quote">&gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page) &amp;&amp;</span>
<span class="quote">&gt; +			    !is_addressable_page(page)) {</span>
<span class="quote">&gt; +				entry = make_device_entry(page, write);</span>
<span class="quote">&gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; +				pte = pte_mkold(pte);</span>
<span class="quote">&gt; +				if (write)</span>
<span class="quote">&gt; +					pte = pte_mkwrite(pte);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			if (pte_swp_soft_dirty(*ptep))</span>
<span class="quote">&gt; +				pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +			if (PageAnon(page))</span>
<span class="quote">&gt; +				page_add_anon_rmap(page, vma, addr, false);</span>
<span class="quote">&gt; +			else</span>
<span class="quote">&gt; +				page_add_file_rmap(page, false);</span>

Do we support pagecache already ? Or is thise just a place holder ? if
so may be we can drop it and add it later when we add page cache
support. ?
<span class="quote">

&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		addr = restart;</span>
<span class="quote">&gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +			unlock_page(page);</span>
<span class="quote">&gt; +			restore--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; +				put_page(page);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			putback_lru_page(page);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!restore)</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +	}</span>


All the above restore won&#39;t be needed if we didn&#39;t do that migration
entry setup in the first function right ? We just need to drop the
refcount for pages that we failed to isolated ? No need to walk the page
table etc ?
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		try_to_unmap(page, flags);</span>
<span class="quote">&gt; +		if (page_mapped(page) || !hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		remove_migration_ptes(page, page, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +		unlock_page(page);</span>
<span class="quote">&gt; +		restore--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		putback_lru_page(page);</span>

May be 

     } else 
	putback_lru_page(page);
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		unsigned long next;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; +			addr = next;</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			struct address_space *mapping;</span>
<span class="quote">&gt; +			struct page *newpage, *page;</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +			int r;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!newpage || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			if (pte_none(*ptep) || pte_present(*ptep)) {</span>
<span class="quote">&gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +				put_page(newpage);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry)) {</span>
<span class="quote">&gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +				put_page(newpage);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +			mapping = page_mapping(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * For now only support private anonymous when migrating</span>
<span class="quote">&gt; +			 * to un-addressable device memory.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="quote">&gt; +			    !is_addressable_page(newpage)) {</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			r = migrate_page(mapping, newpage, page,</span>
<span class="quote">&gt; +					 MIGRATE_SYNC, false);</span>
<span class="quote">&gt; +			if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>


Why are we walking the page table multiple times ? Is it that after
alloc_copy the content of migrate-&gt;pfns pfn array is now the new pfns ?
It is confusing that each of these functions walk one page table
multiple times (even when page can be shared). I was expecting us to
walk the page table once to collect the pfns/pages and then use that
in rest of the calls. Any specific reason you choose to implement it
this way ?
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		unsigned long next;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			struct page *page, *newpage;</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (pte_none(*ptep) || pte_present(*ptep))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!newpage)</span>
<span class="quote">&gt; +				newpage = page;</span>
<span class="quote">&gt; +			remove_migration_ptes(page, newpage, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +			unlock_page(page);</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page))</span>
<span class="quote">&gt; +				put_page(page);</span>
<span class="quote">&gt; +			else</span>
<span class="quote">&gt; +				putback_lru_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (newpage != page) {</span>
<span class="quote">&gt; +				unlock_page(newpage);</span>
<span class="quote">&gt; +				if (is_zone_device_page(newpage))</span>
<span class="quote">&gt; +					put_page(newpage);</span>
<span class="quote">&gt; +				else</span>
<span class="quote">&gt; +					putback_lru_page(newpage);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @ops: migration callback for allocating destination memory and copying</span>
<span class="quote">&gt; + * @vma: virtual memory area containing the range to be migrated</span>
<span class="quote">&gt; + * @start: start address of the range to migrate (inclusive)</span>
<span class="quote">&gt; + * @end: end address of the range to migrate (exclusive)</span>
<span class="quote">&gt; + * @pfns: array of hmm_pfn_t first containing source pfns then destination</span>
<span class="quote">&gt; + * @private: pointer passed back to each of the callback</span>
<span class="quote">&gt; + * Returns: 0 on success, error code otherwise</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This will try to migrate a range of memory using callback to allocate and</span>
<span class="quote">&gt; + * copy memory from source to destination. This function will first collect,</span>
<span class="quote">&gt; + * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="quote">&gt; + * for device driver to allocate destination memory and copy from source.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="quote">&gt; + * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="quote">&gt; + * table it will call finalize_and_map() callback so that device driver can</span>
<span class="quote">&gt; + * inspect what have been successfully migrated and update its own page table</span>
<span class="quote">&gt; + * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="quote">&gt; + * this API).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Finaly the function update CPU page table and unlock the pages before</span>
<span class="quote">&gt; + * returning 0.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * It will return an error code only if one of the argument is invalid.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		    unsigned long start,</span>
<span class="quote">&gt; +		    unsigned long end,</span>
<span class="quote">&gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +		    void *private)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_migrate migrate;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Sanity check the arguments */</span>
<span class="quote">&gt; +	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; +	end &amp;= PAGE_MASK;</span>
<span class="quote">&gt; +	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (!vma || !ops || !pfns || start &gt;= end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	migrate.start = start;</span>
<span class="quote">&gt; +	migrate.pfns = pfns;</span>
<span class="quote">&gt; +	migrate.npages = 0;</span>
<span class="quote">&gt; +	migrate.end = end;</span>
<span class="quote">&gt; +	migrate.vma = vma;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Collect, and try to unmap source pages */</span>
<span class="quote">&gt; +	hmm_migrate_collect(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Lock and isolate page */</span>
<span class="quote">&gt; +	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Unmap pages */</span>
<span class="quote">&gt; +	hmm_migrate_unmap(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="quote">&gt; +	 * content and can safely be copied to destination memory that is</span>
<span class="quote">&gt; +	 * allocated by the callback.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="quote">&gt; +	 * individual page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	ops-&gt;alloc_and_copy(vma, start, end, pfns, private);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* This does the real migration of struct page */</span>
<span class="quote">&gt; +	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ops-&gt;finalize_and_map(vma, start, end, pfns, private);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Unlock and remap pages */</span>
<span class="quote">&gt; +	hmm_migrate_remove_migration_pte(&amp;migrate);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(hmm_vma_migrate);</span>
<span class="quote">&gt; +#endif /* CONFIG_HMM */</span>

IMHO If we can get each of the above functions documented properly it will
help with code review. Also if we can avoid that multiple page table
walk, it will make it closer to the existing migration logic.

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 19, 2016, 5:17 p.m.</div>
<pre class="content">
On Sat, Nov 19, 2016 at 08:02:26PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Jérôme Glisse &lt;jglisse@redhat.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt; &gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; &gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; &gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; &gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; &gt; or special copy offloading engine.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; &gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; &gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; &gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; &gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; &gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; &gt; lower latency than DDR) will also be prime user of this patch.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Migration to private device memory will be usefull for device that</span>
<span class="quote">&gt; &gt; have large pool of such like GPU, NVidia plans to use HMM for that.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/hmm.h |  54 ++++-</span>
<span class="quote">&gt; &gt;  mm/migrate.c        | 584 ++++++++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  2 files changed, 635 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; &gt; index c79abfc..9777309 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/hmm.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; &gt; @@ -101,10 +101,13 @@ struct hmm;</span>
<span class="quote">&gt; &gt;   * HMM_PFN_EMPTY: corresponding CPU page table entry is none (pte_none() true)</span>
<span class="quote">&gt; &gt;   * HMM_PFN_FAULT: use by hmm_vma_fault() to signify which address need faulting</span>
<span class="quote">&gt; &gt;   * HMM_PFN_DEVICE: this is device memory (ie a ZONE_DEVICE page)</span>
<span class="quote">&gt; &gt; + * HMM_PFN_LOCKED: underlying struct page is lock</span>
<span class="quote">&gt; &gt;   * HMM_PFN_SPECIAL: corresponding CPU page table entry is special ie result of</span>
<span class="quote">&gt; &gt;   *      vm_insert_pfn() or vm_insert_page() and thus should not be mirror by a</span>
<span class="quote">&gt; &gt;   *      device (the entry will never have HMM_PFN_VALID set and the pfn value</span>
<span class="quote">&gt; &gt;   *      is undefine)</span>
<span class="quote">&gt; &gt; + * HMM_PFN_MIGRATE: use by hmm_vma_migrate() to signify which address can be</span>
<span class="quote">&gt; &gt; + *      migrated</span>
<span class="quote">&gt; &gt;   * HMM_PFN_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; &gt; @@ -116,9 +119,11 @@ typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; &gt;  #define HMM_PFN_EMPTY (1 &lt;&lt; 4)</span>
<span class="quote">&gt; &gt;  #define HMM_PFN_FAULT (1 &lt;&lt; 5)</span>
<span class="quote">&gt; &gt;  #define HMM_PFN_DEVICE (1 &lt;&lt; 6)</span>
<span class="quote">&gt; &gt; -#define HMM_PFN_SPECIAL (1 &lt;&lt; 7)</span>
<span class="quote">&gt; &gt; -#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 8)</span>
<span class="quote">&gt; &gt; -#define HMM_PFN_SHIFT 9</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_LOCKED (1 &lt;&lt; 7)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_SPECIAL (1 &lt;&lt; 8)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_MIGRATE (1 &lt;&lt; 9)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 10)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_SHIFT 11</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; @@ -323,6 +328,49 @@ bool hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		   hmm_pfn_t *pfns);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * struct hmm_migrate_ops - migrate operation callback</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="quote">&gt; &gt; + * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * The new HMM migrate helper hmm_vma_migrate() allow memory migration to use</span>
<span class="quote">&gt; &gt; + * device DMA engine to perform copy from source to destination memory it also</span>
<span class="quote">&gt; &gt; + * allow caller to use its own memory allocator for destination memory.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="quote">&gt; &gt; + * the entry, for those it must clear the HMM_PFN_MIGRATE flag. The destination</span>
<span class="quote">&gt; &gt; + * page must lock and the corresponding hmm_pfn_t value in the array updated</span>
<span class="quote">&gt; &gt; + * with the HMM_PFN_MIGRATE and HMM_PFN_LOCKED flag set (and of course be a</span>
<span class="quote">&gt; &gt; + * valid entry). It is expected that the page allocated will have an elevated</span>
<span class="quote">&gt; &gt; + * refcount and that a put_page() will free the page. Device driver might want</span>
<span class="quote">&gt; &gt; + * to allocate with an extra-refcount if they want to control deallocation of</span>
<span class="quote">&gt; &gt; + * failed migration inside the finalize_and_map() callback.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Inside finalize_and_map() device driver must use the HMM_PFN_MIGRATE flag to</span>
<span class="quote">&gt; &gt; + * determine which page have been successfully migrated.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +struct hmm_migrate_ops {</span>
<span class="quote">&gt; &gt; +	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +			       unsigned long start,</span>
<span class="quote">&gt; &gt; +			       unsigned long end,</span>
<span class="quote">&gt; &gt; +			       hmm_pfn_t *pfns,</span>
<span class="quote">&gt; &gt; +			       void *private);</span>
<span class="quote">&gt; &gt; +	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +				 unsigned long start,</span>
<span class="quote">&gt; &gt; +				 unsigned long end,</span>
<span class="quote">&gt; &gt; +				 hmm_pfn_t *pfns,</span>
<span class="quote">&gt; &gt; +				 void *private);</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; &gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +		    unsigned long start,</span>
<span class="quote">&gt; &gt; +		    unsigned long end,</span>
<span class="quote">&gt; &gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; &gt; +		    void *private);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  /* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt; &gt;  void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; &gt; index d9ce8db..393d592 100644</span>
<span class="quote">&gt; &gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; &gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; &gt; @@ -41,6 +41,7 @@</span>
<span class="quote">&gt; &gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -421,6 +422,14 @@ int migrate_page_move_mapping(struct address_space *mapping,</span>
<span class="quote">&gt; &gt;  	int expected_count = 1 + extra_count;</span>
<span class="quote">&gt; &gt;  	void **pslot;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="quote">&gt; &gt; +	 * the MEMORY_MOVABLE flag set (see include/linux/memory_hotplug.h).</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	expected_count += is_zone_device_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	if (!mapping) {</span>
<span class="quote">&gt; &gt;  		/* Anonymous page without mapping */</span>
<span class="quote">&gt; &gt;  		if (page_count(page) != expected_count)</span>
<span class="quote">&gt; &gt; @@ -2087,3 +2096,578 @@ out_unlock:</span>
<span class="quote">&gt; &gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #endif /* CONFIG_NUMA */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#if defined(CONFIG_HMM)</span>
<span class="quote">&gt; &gt; +struct hmm_migrate {</span>
<span class="quote">&gt; &gt; +	struct vm_area_struct	*vma;</span>
<span class="quote">&gt; &gt; +	unsigned long		start;</span>
<span class="quote">&gt; &gt; +	unsigned long		end;</span>
<span class="quote">&gt; &gt; +	unsigned long		npages;</span>
<span class="quote">&gt; &gt; +	hmm_pfn_t		*pfns;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; &gt; +				unsigned long start,</span>
<span class="quote">&gt; &gt; +				unsigned long end,</span>
<span class="quote">&gt; &gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; &gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; &gt; +	int pages = 0;</span>
<span class="quote">&gt; &gt; +	pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +again:</span>
<span class="quote">&gt; &gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; &gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; &gt; +		goto again;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; &gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; &gt; +		unsigned long pfn;</span>
<span class="quote">&gt; &gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +		struct page *page;</span>
<span class="quote">&gt; &gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; &gt; +		bool write;</span>
<span class="quote">&gt; &gt; +		pte_t pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; &gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; &gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; &gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="quote">&gt; &gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; &gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; &gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; &gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; &gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will that is_zone_device_page() be ever true ?, The pte is present in</span>
<span class="quote">&gt; the else patch can the struct page backing that come from zone device ?</span>

Yes, for ZONE_DEVICE on architecture with CAPI like bus you can have ZONE_DEVICE
memory map as device memory is accessible.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; &gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about page cache pages. Do we support that ? If not may be skip</span>
<span class="quote">&gt; that here ?</span>

No, page cache is supported, it will fail latter in the process if it try to
migrate to un-accessible memory which need special handling from address_space
point of view (handling read/write and writeback).
<span class="quote">

&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; &gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; &gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; &gt; +		get_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; &gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; &gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			pages++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If this is an optimization, can we get that as a seperate patch with</span>
<span class="quote">&gt; addtional comments. ? How does take a successful page lock implies it is</span>
<span class="quote">&gt; not a shared mapping ?</span>

It can be a share mapping and that&#39;s fine, migration only fail if page is
pin.
<span class="quote"> 

&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; &gt; +	if (pages)</span>
<span class="quote">&gt; &gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So without the optimization the above function is suppose to raise the</span>
<span class="quote">&gt; refcount and collect all possible pfns tha we can migrate in the array ?</span>

Yes correct, this function collect all page we can migrate in the range.
<span class="quote"> 

&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mm_walk mm_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="quote">&gt; &gt; +	mm_walk.pte_entry = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.pte_hole = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.hugetlb_entry = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.test_walk = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.vma = migrate-&gt;vma;</span>
<span class="quote">&gt; &gt; +	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	mm_walk.private = migrate;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="quote">&gt; &gt; +					    migrate-&gt;start,</span>
<span class="quote">&gt; &gt; +					    migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="quote">&gt; &gt; +	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="quote">&gt; &gt; +					  migrate-&gt;start,</span>
<span class="quote">&gt; &gt; +					  migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline bool hmm_migrate_page_check(struct page *page, int extra)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="quote">&gt; &gt; +	 * check them then regular page because they can be map with a pmd or</span>
<span class="quote">&gt; &gt; +	 * with a pte (split pte mapping).</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (PageCompound(page))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +		extra++;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return true;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt; &gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt; &gt; +	bool allow_drain = true;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	lru_add_drain();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +again:</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page)</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt; &gt; +			lock_page(page);</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What does taking a page_lock protect against ? Can we document that ?</span>

This usual page migration process like existing code, page lock protect against
anyone trying to map the page inside another process or at different address. It
also block few fs operations. I don&#39;t think there is a comprehensive list anywhere
but i can try to make one.
<span class="quote"> 
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt; &gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +			goto check;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt; &gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt; &gt; +			lru_add_drain_all();</span>
<span class="quote">&gt; &gt; +			allow_drain = false;</span>
<span class="quote">&gt; &gt; +			goto again;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		} else</span>
<span class="quote">&gt; &gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +check:</span>
<span class="quote">&gt; &gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!restore)</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (addr = migrate-&gt;start, i = 0; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +		unsigned long next, restart;</span>
<span class="quote">&gt; &gt; +		spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE)) {</span>
<span class="quote">&gt; &gt; +			addr += PAGE_SIZE;</span>
<span class="quote">&gt; &gt; +			i++;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		restart = addr;</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; &gt; +			addr = next;</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +			bool write;</span>
<span class="quote">&gt; &gt; +			pte_t pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			write = migrate-&gt;pfns[i] &amp; HMM_PFN_WRITE;</span>
<span class="quote">&gt; &gt; +			write &amp;= (vma-&gt;vm_flags &amp; VM_WRITE);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			/* Here it means pte must be a valid migration entry */</span>
<span class="quote">&gt; &gt; +			pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is already a non present pte, so why use ptep_get_and_clear ? why not</span>
<span class="quote">&gt; *ptep ? Some archs does lot of additional stuff in get_and_clear. </span>

Yes i can switch to *ptep, if memory serve this most likely because it was
cut and paste from collect function so there is no motivation behind the use
of get_and_clear
<span class="quote">
&gt; &gt; +			if (pte_none(pte) || pte_present(pte))</span>
<span class="quote">&gt; &gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; &gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page) &amp;&amp;</span>
<span class="quote">&gt; &gt; +			    !is_addressable_page(page)) {</span>
<span class="quote">&gt; &gt; +				entry = make_device_entry(page, write);</span>
<span class="quote">&gt; &gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; +				pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; &gt; +				pte = pte_mkold(pte);</span>
<span class="quote">&gt; &gt; +				if (write)</span>
<span class="quote">&gt; &gt; +					pte = pte_mkwrite(pte);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +			if (pte_swp_soft_dirty(*ptep))</span>
<span class="quote">&gt; &gt; +				pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			get_page(page);</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +			if (PageAnon(page))</span>
<span class="quote">&gt; &gt; +				page_add_anon_rmap(page, vma, addr, false);</span>
<span class="quote">&gt; &gt; +			else</span>
<span class="quote">&gt; &gt; +				page_add_file_rmap(page, false);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we support pagecache already ? Or is thise just a place holder ? if</span>
<span class="quote">&gt; so may be we can drop it and add it later when we add page cache</span>
<span class="quote">&gt; support. ?</span>

It support pagecache already. It does not support page cache migration to
un-addressable memory. So in CAPI case pagecache works but in x86/PCIE it
does not.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		addr = restart;</span>
<span class="quote">&gt; &gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt; &gt; +			restore--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			putback_lru_page(page);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!restore)</span>
<span class="quote">&gt; &gt; +			break;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; All the above restore won&#39;t be needed if we didn&#39;t do that migration</span>
<span class="quote">&gt; entry setup in the first function right ? We just need to drop the</span>
<span class="quote">&gt; refcount for pages that we failed to isolated ? No need to walk the page</span>
<span class="quote">&gt; table etc ?</span>

Well the migration entry setup is important so that no concurrent migration
can race with each other, the one that set the migration entry first is the
one that win in respect of migration. Also the CPU page table entry need to
be clear so that page content is stable and DMA copy does not miss any data
left over in some cache.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		try_to_unmap(page, flags);</span>
<span class="quote">&gt; &gt; +		if (page_mapped(page) || !hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		remove_migration_ptes(page, page, false);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +		unlock_page(page);</span>
<span class="quote">&gt; &gt; +		restore--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		putback_lru_page(page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; May be </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      } else </span>
<span class="quote">&gt; 	putback_lru_page(page);</span>

Yes it probably clearer to use an else branch there.
<span class="quote">
&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		unsigned long next;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; &gt; +			addr = next;</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			struct address_space *mapping;</span>
<span class="quote">&gt; &gt; +			struct page *newpage, *page;</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +			int r;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!newpage || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			if (pte_none(*ptep) || pte_present(*ptep)) {</span>
<span class="quote">&gt; &gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +				put_page(newpage);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +				put_page(newpage);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			mapping = page_mapping(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * For now only support private anonymous when migrating</span>
<span class="quote">&gt; &gt; +			 * to un-addressable device memory.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="quote">&gt; &gt; +			    !is_addressable_page(newpage)) {</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			r = migrate_page(mapping, newpage, page,</span>
<span class="quote">&gt; &gt; +					 MIGRATE_SYNC, false);</span>
<span class="quote">&gt; &gt; +			if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why are we walking the page table multiple times ? Is it that after</span>
<span class="quote">&gt; alloc_copy the content of migrate-&gt;pfns pfn array is now the new pfns ?</span>
<span class="quote">&gt; It is confusing that each of these functions walk one page table</span>
<span class="quote">&gt; multiple times (even when page can be shared). I was expecting us to</span>
<span class="quote">&gt; walk the page table once to collect the pfns/pages and then use that</span>
<span class="quote">&gt; in rest of the calls. Any specific reason you choose to implement it</span>
<span class="quote">&gt; this way ?</span>

Well you need to know the source and destination page, so either i have
2 arrays one for source page and one for destination pages and then i do
not need to walk page table multiple time. But needing 2 arrays might be
problematic as here we want to migrate reasonable chunk ie few megabyte
hence there is a need for vmalloc.

My advice to device driver was to pre-allocate this array once (maybe
preallocate couple of them). If you really prefer avoiding walking the
CPU page table over and over then i can switch to 2 arrays solutions.
<span class="quote"> 

&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		unsigned long next;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			struct page *page, *newpage;</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (pte_none(*ptep) || pte_present(*ptep))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!newpage)</span>
<span class="quote">&gt; &gt; +				newpage = page;</span>
<span class="quote">&gt; &gt; +			remove_migration_ptes(page, newpage, false);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt; &gt; +			else</span>
<span class="quote">&gt; &gt; +				putback_lru_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (newpage != page) {</span>
<span class="quote">&gt; &gt; +				unlock_page(newpage);</span>
<span class="quote">&gt; &gt; +				if (is_zone_device_page(newpage))</span>
<span class="quote">&gt; &gt; +					put_page(newpage);</span>
<span class="quote">&gt; &gt; +				else</span>
<span class="quote">&gt; &gt; +					putback_lru_page(newpage);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @ops: migration callback for allocating destination memory and copying</span>
<span class="quote">&gt; &gt; + * @vma: virtual memory area containing the range to be migrated</span>
<span class="quote">&gt; &gt; + * @start: start address of the range to migrate (inclusive)</span>
<span class="quote">&gt; &gt; + * @end: end address of the range to migrate (exclusive)</span>
<span class="quote">&gt; &gt; + * @pfns: array of hmm_pfn_t first containing source pfns then destination</span>
<span class="quote">&gt; &gt; + * @private: pointer passed back to each of the callback</span>
<span class="quote">&gt; &gt; + * Returns: 0 on success, error code otherwise</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This will try to migrate a range of memory using callback to allocate and</span>
<span class="quote">&gt; &gt; + * copy memory from source to destination. This function will first collect,</span>
<span class="quote">&gt; &gt; + * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="quote">&gt; &gt; + * for device driver to allocate destination memory and copy from source.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="quote">&gt; &gt; + * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="quote">&gt; &gt; + * table it will call finalize_and_map() callback so that device driver can</span>
<span class="quote">&gt; &gt; + * inspect what have been successfully migrated and update its own page table</span>
<span class="quote">&gt; &gt; + * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="quote">&gt; &gt; + * this API).</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Finaly the function update CPU page table and unlock the pages before</span>
<span class="quote">&gt; &gt; + * returning 0.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * It will return an error code only if one of the argument is invalid.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; &gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +		    unsigned long start,</span>
<span class="quote">&gt; &gt; +		    unsigned long end,</span>
<span class="quote">&gt; &gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; &gt; +		    void *private)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_migrate migrate;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Sanity check the arguments */</span>
<span class="quote">&gt; &gt; +	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; &gt; +	end &amp;= PAGE_MASK;</span>
<span class="quote">&gt; &gt; +	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (!vma || !ops || !pfns || start &gt;= end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	migrate.start = start;</span>
<span class="quote">&gt; &gt; +	migrate.pfns = pfns;</span>
<span class="quote">&gt; &gt; +	migrate.npages = 0;</span>
<span class="quote">&gt; &gt; +	migrate.end = end;</span>
<span class="quote">&gt; &gt; +	migrate.vma = vma;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Collect, and try to unmap source pages */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_collect(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Lock and isolate page */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Unmap pages */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_unmap(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="quote">&gt; &gt; +	 * content and can safely be copied to destination memory that is</span>
<span class="quote">&gt; &gt; +	 * allocated by the callback.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="quote">&gt; &gt; +	 * individual page.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	ops-&gt;alloc_and_copy(vma, start, end, pfns, private);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* This does the real migration of struct page */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	ops-&gt;finalize_and_map(vma, start, end, pfns, private);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Unlock and remap pages */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_remove_migration_pte(&amp;migrate);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +EXPORT_SYMBOL(hmm_vma_migrate);</span>
<span class="quote">&gt; &gt; +#endif /* CONFIG_HMM */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMHO If we can get each of the above functions documented properly it will</span>
<span class="quote">&gt; help with code review. Also if we can avoid that multiple page table</span>
<span class="quote">&gt; walk, it will make it closer to the existing migration logic.</span>
<span class="quote">&gt; </span>

What kind of documentation are you looking for ? I thought the high level overview
was enough as none of the function do anything out of the ordinary. Do you want
more inline documation ? Or a more verbose highlevel overview ?

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Nov. 20, 2016, 6:21 p.m.</div>
<pre class="content">
Jerome Glisse &lt;jglisse@redhat.com&gt; writes:

.....
<span class="quote">
&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt;&gt; &gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt;&gt; &gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt;&gt; &gt; +		get_page(page);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt;&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt;&gt; &gt; +		} else {</span>
<span class="quote">&gt;&gt; &gt; +			pte_t swp_pte;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt;&gt; &gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt;&gt; &gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt;&gt; &gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;&gt; &gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt;&gt; &gt; +			pages++;</span>
<span class="quote">&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; If this is an optimization, can we get that as a seperate patch with</span>
<span class="quote">&gt;&gt; addtional comments. ? How does take a successful page lock implies it is</span>
<span class="quote">&gt;&gt; not a shared mapping ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It can be a share mapping and that&#39;s fine, migration only fail if page is</span>
<span class="quote">&gt; pin.</span>
<span class="quote">&gt;</span>

In the previous mail you replied above trylock_page() usage is an
optimization for the usual case where the memory is only use in one
process and that no concurrent migration/memory event is happening. 

How did we know that it is only in use by one process. I got the part
that if we can lock, and since we lock the page early, it avoid
concurrent migration. But I am not sure about the use by one process
part. 
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; &gt; +	}</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt;&gt; &gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt;&gt; &gt; +	if (pages)</span>
<span class="quote">&gt;&gt; &gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +	return 0;</span>
<span class="quote">&gt;&gt; &gt; +}</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So without the optimization the above function is suppose to raise the</span>
<span class="quote">&gt;&gt; refcount and collect all possible pfns tha we can migrate in the array ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes correct, this function collect all page we can migrate in the range.</span>
<span class="quote">&gt;</span>

.....
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; &gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt;&gt; &gt; +{</span>
<span class="quote">&gt;&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt;&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt;&gt; &gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt;&gt; &gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt;&gt; &gt; +	bool allow_drain = true;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +	lru_add_drain();</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +again:</span>
<span class="quote">&gt;&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt;&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (!page)</span>
<span class="quote">&gt;&gt; &gt; +			continue;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt;&gt; &gt; +			lock_page(page);</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; What does taking a page_lock protect against ? Can we document that ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This usual page migration process like existing code, page lock protect against</span>
<span class="quote">&gt; anyone trying to map the page inside another process or at different address. It</span>
<span class="quote">&gt; also block few fs operations. I don&#39;t think there is a comprehensive list anywhere</span>
<span class="quote">&gt; but i can try to make one.</span>


I was comparing it against the trylock_page() usage above. But I guess
documenting the page lock can be another patch. 
<span class="quote">

&gt;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt;&gt; &gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt;&gt; &gt; +			goto check;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt;&gt; &gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt;&gt; &gt; +			lru_add_drain_all();</span>
<span class="quote">&gt;&gt; &gt; +			allow_drain = false;</span>
<span class="quote">&gt;&gt; &gt; +			goto again;</span>
<span class="quote">&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt;&gt; &gt; +			restore++;</span>
<span class="quote">&gt;&gt; &gt; +		} else</span>
<span class="quote">&gt;&gt; &gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +check:</span>
<span class="quote">&gt;&gt; &gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt;&gt; &gt; +			restore++;</span>
<span class="quote">&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; &gt; +	}</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt; </span>

.....
<span class="quote">
&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; &gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		addr = restart;</span>
<span class="quote">&gt;&gt; &gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt; &gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt;&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt;&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt;&gt; &gt; +				continue;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt;&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt;&gt; &gt; +			restore--;</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt;&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt;&gt; &gt; +				continue;</span>
<span class="quote">&gt;&gt; &gt; +			}</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +			putback_lru_page(page);</span>
<span class="quote">&gt;&gt; &gt; +		}</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +		if (!restore)</span>
<span class="quote">&gt;&gt; &gt; +			break;</span>
<span class="quote">&gt;&gt; &gt; +	}</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; All the above restore won&#39;t be needed if we didn&#39;t do that migration</span>
<span class="quote">&gt;&gt; entry setup in the first function right ? We just need to drop the</span>
<span class="quote">&gt;&gt; refcount for pages that we failed to isolated ? No need to walk the page</span>
<span class="quote">&gt;&gt; table etc ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Well the migration entry setup is important so that no concurrent migration</span>
<span class="quote">&gt; can race with each other, the one that set the migration entry first is the</span>
<span class="quote">&gt; one that win in respect of migration. Also the CPU page table entry need to</span>
<span class="quote">&gt; be clear so that page content is stable and DMA copy does not miss any data</span>
<span class="quote">&gt; left over in some cache.</span>

This is the part i am still tryint to understand. 
hmm_collect_walk_pmd(), did migration entry setup only in one process
page table. So how can it prevent concurrent migration because one could
initiate a migration using the va/mapping of another process.

Isn&#39;t that page lock that is prevent concurrent migration ?

........
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; Why are we walking the page table multiple times ? Is it that after</span>
<span class="quote">&gt;&gt; alloc_copy the content of migrate-&gt;pfns pfn array is now the new pfns ?</span>
<span class="quote">&gt;&gt; It is confusing that each of these functions walk one page table</span>
<span class="quote">&gt;&gt; multiple times (even when page can be shared). I was expecting us to</span>
<span class="quote">&gt;&gt; walk the page table once to collect the pfns/pages and then use that</span>
<span class="quote">&gt;&gt; in rest of the calls. Any specific reason you choose to implement it</span>
<span class="quote">&gt;&gt; this way ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Well you need to know the source and destination page, so either i have</span>
<span class="quote">&gt; 2 arrays one for source page and one for destination pages and then i do</span>
<span class="quote">&gt; not need to walk page table multiple time. But needing 2 arrays might be</span>
<span class="quote">&gt; problematic as here we want to migrate reasonable chunk ie few megabyte</span>
<span class="quote">&gt; hence there is a need for vmalloc.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; My advice to device driver was to pre-allocate this array once (maybe</span>
<span class="quote">&gt; preallocate couple of them). If you really prefer avoiding walking the</span>
<span class="quote">&gt; CPU page table over and over then i can switch to 2 arrays solutions.</span>
<span class="quote">&gt;</span>

Having two array makes it easy to follow the code. But otherwise I guess
documenting the above usage of page table above the function will also
help.

.....
<span class="quote">
&gt;&gt; IMHO If we can get each of the above functions documented properly it will</span>
<span class="quote">&gt;&gt; help with code review. Also if we can avoid that multiple page table</span>
<span class="quote">&gt;&gt; walk, it will make it closer to the existing migration logic.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What kind of documentation are you looking for ? I thought the high level overview</span>
<span class="quote">&gt; was enough as none of the function do anything out of the ordinary. Do you want</span>
<span class="quote">&gt; more inline documation ? Or a more verbose highlevel overview ?</span>


Inline documentation for functions will be useful. Also if you can split
the hmm_collect_walk_pmd() optimization we discussed above into a
separate patch I guess this will be lot easy to follow.

I still haven&#39;t understood why we setup that migration entry early and
that too only on one process page table. If we can explain that as a
separate patch may be it will much easy to follow.

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 20, 2016, 8:06 p.m.</div>
<pre class="content">
On Sun, Nov 20, 2016 at 11:51:48PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Jerome Glisse &lt;jglisse@redhat.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		get_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; &gt;&gt; &gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			pages++;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; If this is an optimization, can we get that as a seperate patch with</span>
<span class="quote">&gt; &gt;&gt; addtional comments. ? How does take a successful page lock implies it is</span>
<span class="quote">&gt; &gt;&gt; not a shared mapping ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It can be a share mapping and that&#39;s fine, migration only fail if page is</span>
<span class="quote">&gt; &gt; pin.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the previous mail you replied above trylock_page() usage is an</span>
<span class="quote">&gt; optimization for the usual case where the memory is only use in one</span>
<span class="quote">&gt; process and that no concurrent migration/memory event is happening. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How did we know that it is only in use by one process. I got the part</span>
<span class="quote">&gt; that if we can lock, and since we lock the page early, it avoid</span>
<span class="quote">&gt; concurrent migration. But I am not sure about the use by one process</span>
<span class="quote">&gt; part. </span>
<span class="quote">&gt; </span>

The mapcount will reflect that and it is handled latter inside unmap
function. The refcount will be check for pin too.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	}</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt;&gt; &gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; &gt;&gt; &gt; +	if (pages)</span>
<span class="quote">&gt; &gt;&gt; &gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt;&gt; &gt; +}</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; So without the optimization the above function is suppose to raise the</span>
<span class="quote">&gt; &gt;&gt; refcount and collect all possible pfns tha we can migrate in the array ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yes correct, this function collect all page we can migrate in the range.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt;&gt; &gt; +{</span>
<span class="quote">&gt; &gt;&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	bool allow_drain = true;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +	lru_add_drain();</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +again:</span>
<span class="quote">&gt; &gt;&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!page)</span>
<span class="quote">&gt; &gt;&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			lock_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; What does taking a page_lock protect against ? Can we document that ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This usual page migration process like existing code, page lock protect against</span>
<span class="quote">&gt; &gt; anyone trying to map the page inside another process or at different address. It</span>
<span class="quote">&gt; &gt; also block few fs operations. I don&#39;t think there is a comprehensive list anywhere</span>
<span class="quote">&gt; &gt; but i can try to make one.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I was comparing it against the trylock_page() usage above. But I guess</span>
<span class="quote">&gt; documenting the page lock can be another patch. </span>

Well trylock_page() in collect function happen under a spinlock (page table spinlock)
hence we can&#39;t sleep and don&#39;t want to spin either.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt;&gt; &gt; +			goto check;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt; &gt;&gt; &gt; +			lru_add_drain_all();</span>
<span class="quote">&gt; &gt;&gt; &gt; +			allow_drain = false;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			goto again;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		} else</span>
<span class="quote">&gt; &gt;&gt; &gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt; &gt;&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +check:</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; &gt; +	}</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; &gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		addr = restart;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt;&gt; &gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt;&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +			restore--;</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt;&gt; &gt; +			}</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +			putback_lru_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +		if (!restore)</span>
<span class="quote">&gt; &gt;&gt; &gt; +			break;</span>
<span class="quote">&gt; &gt;&gt; &gt; +	}</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; All the above restore won&#39;t be needed if we didn&#39;t do that migration</span>
<span class="quote">&gt; &gt;&gt; entry setup in the first function right ? We just need to drop the</span>
<span class="quote">&gt; &gt;&gt; refcount for pages that we failed to isolated ? No need to walk the page</span>
<span class="quote">&gt; &gt;&gt; table etc ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Well the migration entry setup is important so that no concurrent migration</span>
<span class="quote">&gt; &gt; can race with each other, the one that set the migration entry first is the</span>
<span class="quote">&gt; &gt; one that win in respect of migration. Also the CPU page table entry need to</span>
<span class="quote">&gt; &gt; be clear so that page content is stable and DMA copy does not miss any data</span>
<span class="quote">&gt; &gt; left over in some cache.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is the part i am still tryint to understand. </span>
<span class="quote">&gt; hmm_collect_walk_pmd(), did migration entry setup only in one process</span>
<span class="quote">&gt; page table. So how can it prevent concurrent migration because one could</span>
<span class="quote">&gt; initiate a migration using the va/mapping of another process.</span>
<span class="quote">&gt; </span>

Well hmm_migrate_unmap() will unmap the page in all the process, so before we
call alloc_and_copy(). When alloc_and_copy() is call the page is unmap ie the
mapcount is zero and there is no pin either. Because the page is lock it can
no new mapping to it can be spawn from under us.

This is exactly like existing migration code, the difference is that existing
migration code do not do collect or lock. It expect to get the page locked and
then unmap before trying to migrate.

So ignoring the collect pass and the optimization where i unmap page in the
current process, my logic for migration is otherwise exactly the same as the
existing one.
<span class="quote">

&gt; Isn&#39;t that page lock that is prevent concurrent migration ?</span>

Page lock do prevent concurrent migration yes. But for the collect pass of
my code having the special migration entry is also a important hint that it
is pointless to migrate that page. Moreover the special migration entry do
exist for a reason. It is an indicator in couple place that is important to
have.
<span class="quote">
&gt; </span>
<span class="quote">&gt; ........</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; Why are we walking the page table multiple times ? Is it that after</span>
<span class="quote">&gt; &gt;&gt; alloc_copy the content of migrate-&gt;pfns pfn array is now the new pfns ?</span>
<span class="quote">&gt; &gt;&gt; It is confusing that each of these functions walk one page table</span>
<span class="quote">&gt; &gt;&gt; multiple times (even when page can be shared). I was expecting us to</span>
<span class="quote">&gt; &gt;&gt; walk the page table once to collect the pfns/pages and then use that</span>
<span class="quote">&gt; &gt;&gt; in rest of the calls. Any specific reason you choose to implement it</span>
<span class="quote">&gt; &gt;&gt; this way ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Well you need to know the source and destination page, so either i have</span>
<span class="quote">&gt; &gt; 2 arrays one for source page and one for destination pages and then i do</span>
<span class="quote">&gt; &gt; not need to walk page table multiple time. But needing 2 arrays might be</span>
<span class="quote">&gt; &gt; problematic as here we want to migrate reasonable chunk ie few megabyte</span>
<span class="quote">&gt; &gt; hence there is a need for vmalloc.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; My advice to device driver was to pre-allocate this array once (maybe</span>
<span class="quote">&gt; &gt; preallocate couple of them). If you really prefer avoiding walking the</span>
<span class="quote">&gt; &gt; CPU page table over and over then i can switch to 2 arrays solutions.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having two array makes it easy to follow the code. But otherwise I guess</span>
<span class="quote">&gt; documenting the above usage of page table above the function will also</span>
<span class="quote">&gt; help.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; IMHO If we can get each of the above functions documented properly it will</span>
<span class="quote">&gt; &gt;&gt; help with code review. Also if we can avoid that multiple page table</span>
<span class="quote">&gt; &gt;&gt; walk, it will make it closer to the existing migration logic.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; What kind of documentation are you looking for ? I thought the high level overview</span>
<span class="quote">&gt; &gt; was enough as none of the function do anything out of the ordinary. Do you want</span>
<span class="quote">&gt; &gt; more inline documation ? Or a more verbose highlevel overview ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Inline documentation for functions will be useful. Also if you can split</span>
<span class="quote">&gt; the hmm_collect_walk_pmd() optimization we discussed above into a</span>
<span class="quote">&gt; separate patch I guess this will be lot easy to follow.</span>

Ok will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt; I still haven&#39;t understood why we setup that migration entry early and</span>
<span class="quote">&gt; that too only on one process page table. If we can explain that as a</span>
<span class="quote">&gt; separate patch may be it will much easy to follow.</span>

Well the one process and early is the optimization, i do setup the special
migration entry in all process inside hmm_migrate_unmap(). So my migration
works exactly as existing one except that i optimize the common case where
the page we are interested in is only map in the process we are doing the
migration against.

I will split the optimization as its own patch.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Nov. 21, 2016, 3:30 a.m.</div>
<pre class="content">
On 19/11/16 05:18, Jérôme Glisse wrote:
<span class="quote">&gt; This patch add a new memory migration helpers, which migrate memory</span>
             adds                       helper         migrates
<span class="quote">&gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; or special copy offloading engine.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; lower latency than DDR) will also be prime user of this patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Migration to private device memory will be usefull for device that</span>
<span class="quote">&gt; have large pool of such like GPU, NVidia plans to use HMM for that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hmm.h |  54 ++++-</span>
<span class="quote">&gt;  mm/migrate.c        | 584 ++++++++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  2 files changed, 635 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; index c79abfc..9777309 100644</span>
<span class="quote">&gt; --- a/include/linux/hmm.h</span>
<span class="quote">&gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; @@ -101,10 +101,13 @@ struct hmm;</span>
<span class="quote">&gt;   * HMM_PFN_EMPTY: corresponding CPU page table entry is none (pte_none() true)</span>
<span class="quote">&gt;   * HMM_PFN_FAULT: use by hmm_vma_fault() to signify which address need faulting</span>
<span class="quote">&gt;   * HMM_PFN_DEVICE: this is device memory (ie a ZONE_DEVICE page)</span>
<span class="quote">&gt; + * HMM_PFN_LOCKED: underlying struct page is lock</span>
<span class="quote">&gt;   * HMM_PFN_SPECIAL: corresponding CPU page table entry is special ie result of</span>
<span class="quote">&gt;   *      vm_insert_pfn() or vm_insert_page() and thus should not be mirror by a</span>
<span class="quote">&gt;   *      device (the entry will never have HMM_PFN_VALID set and the pfn value</span>
<span class="quote">&gt;   *      is undefine)</span>
<span class="quote">&gt; + * HMM_PFN_MIGRATE: use by hmm_vma_migrate() to signify which address can be</span>
<span class="quote">&gt; + *      migrated</span>
<span class="quote">&gt;   * HMM_PFN_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; @@ -116,9 +119,11 @@ typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt;  #define HMM_PFN_EMPTY (1 &lt;&lt; 4)</span>
<span class="quote">&gt;  #define HMM_PFN_FAULT (1 &lt;&lt; 5)</span>
<span class="quote">&gt;  #define HMM_PFN_DEVICE (1 &lt;&lt; 6)</span>
<span class="quote">&gt; -#define HMM_PFN_SPECIAL (1 &lt;&lt; 7)</span>
<span class="quote">&gt; -#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 8)</span>
<span class="quote">&gt; -#define HMM_PFN_SHIFT 9</span>
<span class="quote">&gt; +#define HMM_PFN_LOCKED (1 &lt;&lt; 7)</span>
<span class="quote">&gt; +#define HMM_PFN_SPECIAL (1 &lt;&lt; 8)</span>
<span class="quote">&gt; +#define HMM_PFN_MIGRATE (1 &lt;&lt; 9)</span>
<span class="quote">&gt; +#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 10)</span>
<span class="quote">&gt; +#define HMM_PFN_SHIFT 11</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -323,6 +328,49 @@ bool hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		   hmm_pfn_t *pfns);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * struct hmm_migrate_ops - migrate operation callback</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="quote">&gt; + * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The new HMM migrate helper hmm_vma_migrate() allow memory migration to use</span>
<span class="quote">&gt; + * device DMA engine to perform copy from source to destination memory it also</span>
<span class="quote">&gt; + * allow caller to use its own memory allocator for destination memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="quote">&gt; + * the entry, for those it must clear the HMM_PFN_MIGRATE flag. The destination</span>
<span class="quote">&gt; + * page must lock and the corresponding hmm_pfn_t value in the array updated</span>
<span class="quote">&gt; + * with the HMM_PFN_MIGRATE and HMM_PFN_LOCKED flag set (and of course be a</span>
<span class="quote">&gt; + * valid entry). It is expected that the page allocated will have an elevated</span>
<span class="quote">&gt; + * refcount and that a put_page() will free the page. Device driver might want</span>
<span class="quote">&gt; + * to allocate with an extra-refcount if they want to control deallocation of</span>
<span class="quote">&gt; + * failed migration inside the finalize_and_map() callback.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Inside finalize_and_map() device driver must use the HMM_PFN_MIGRATE flag to</span>
<span class="quote">&gt; + * determine which page have been successfully migrated.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct hmm_migrate_ops {</span>
<span class="quote">&gt; +	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +			       unsigned long start,</span>
<span class="quote">&gt; +			       unsigned long end,</span>
<span class="quote">&gt; +			       hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +			       void *private);</span>
<span class="quote">&gt; +	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				 unsigned long start,</span>
<span class="quote">&gt; +				 unsigned long end,</span>
<span class="quote">&gt; +				 hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +				 void *private);</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		    unsigned long start,</span>
<span class="quote">&gt; +		    unsigned long end,</span>
<span class="quote">&gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +		    void *private);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt;  void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index d9ce8db..393d592 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -41,6 +41,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/page_idle.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/page_owner.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -421,6 +422,14 @@ int migrate_page_move_mapping(struct address_space *mapping,</span>
<span class="quote">&gt;  	int expected_count = 1 + extra_count;</span>
<span class="quote">&gt;  	void **pslot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="quote">&gt; +	 * the MEMORY_MOVABLE flag set (see include/linux/memory_hotplug.h).</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	expected_count += is_zone_device_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (!mapping) {</span>
<span class="quote">&gt;  		/* Anonymous page without mapping */</span>
<span class="quote">&gt;  		if (page_count(page) != expected_count)</span>
<span class="quote">&gt; @@ -2087,3 +2096,578 @@ out_unlock:</span>
<span class="quote">&gt;  #endif /* CONFIG_NUMA_BALANCING */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* CONFIG_NUMA */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if defined(CONFIG_HMM)</span>
<span class="quote">&gt; +struct hmm_migrate {</span>
<span class="quote">&gt; +	struct vm_area_struct	*vma;</span>
<span class="quote">&gt; +	unsigned long		start;</span>
<span class="quote">&gt; +	unsigned long		end;</span>
<span class="quote">&gt; +	unsigned long		npages;</span>
<span class="quote">&gt; +	hmm_pfn_t		*pfns;</span>

I presume the destination is pfns[] or is the source?
<span class="quote">
&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; +				unsigned long start,</span>
<span class="quote">&gt; +				unsigned long end,</span>
<span class="quote">&gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; +	int pages = 0;</span>
<span class="quote">&gt; +	pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; +		goto again;</span>
<span class="quote">&gt; +</span>

OK., so we always split THP before migration
<span class="quote">

&gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; +		unsigned long pfn;</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; +		bool write;</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>

Why hard code this, in general the ability to migrate a VMA
start/end range seems like a useful API.
<span class="quote">
&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>

Currently UNADDRESSABLE?
<span class="quote">
&gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; +			continue;</span>

Didn&#39;t we split the THP above?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; +		get_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, pte);</span>

put_page()?
<span class="quote">
&gt; +		} else {</span>
<span class="quote">&gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			pages++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; +	if (pages)</span>
<span class="quote">&gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mm_walk mm_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="quote">&gt; +	mm_walk.pte_entry = NULL;</span>
<span class="quote">&gt; +	mm_walk.pte_hole = NULL;</span>
<span class="quote">&gt; +	mm_walk.hugetlb_entry = NULL;</span>
<span class="quote">&gt; +	mm_walk.test_walk = NULL;</span>
<span class="quote">&gt; +	mm_walk.vma = migrate-&gt;vma;</span>
<span class="quote">&gt; +	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	mm_walk.private = migrate;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="quote">&gt; +					    migrate-&gt;start,</span>
<span class="quote">&gt; +					    migrate-&gt;end);</span>
<span class="quote">&gt; +	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="quote">&gt; +					  migrate-&gt;start,</span>
<span class="quote">&gt; +					  migrate-&gt;end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool hmm_migrate_page_check(struct page *page, int extra)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="quote">&gt; +	 * check them then regular page because they can be map with a pmd or</span>
<span class="quote">&gt; +	 * with a pte (split pte mapping).</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (PageCompound(page))</span>
<span class="quote">&gt; +		return false;</span>

PageTransCompound()?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt; +		extra++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt; +	bool allow_drain = true;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	lru_add_drain();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt; +			lock_page(page);</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt; +			goto check;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt; +			lru_add_drain_all();</span>
<span class="quote">&gt; +			allow_drain = false;</span>
<span class="quote">&gt; +			goto again;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		} else</span>
<span class="quote">&gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +check:</span>
<span class="quote">&gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!restore)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (addr = migrate-&gt;start, i = 0; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +		unsigned long next, restart;</span>
<span class="quote">&gt; +		spinlock_t *ptl;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE)) {</span>
<span class="quote">&gt; +			addr += PAGE_SIZE;</span>
<span class="quote">&gt; +			i++;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		restart = addr;</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; +			addr = next;</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +			bool write;</span>
<span class="quote">&gt; +			pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			write = migrate-&gt;pfns[i] &amp; HMM_PFN_WRITE;</span>
<span class="quote">&gt; +			write &amp;= (vma-&gt;vm_flags &amp; VM_WRITE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/* Here it means pte must be a valid migration entry */</span>
<span class="quote">&gt; +			pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; +			if (pte_none(pte) || pte_present(pte))</span>
<span class="quote">&gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page) &amp;&amp;</span>
<span class="quote">&gt; +			    !is_addressable_page(page)) {</span>
<span class="quote">&gt; +				entry = make_device_entry(page, write);</span>
<span class="quote">&gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; +				pte = pte_mkold(pte);</span>
<span class="quote">&gt; +				if (write)</span>
<span class="quote">&gt; +					pte = pte_mkwrite(pte);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			if (pte_swp_soft_dirty(*ptep))</span>
<span class="quote">&gt; +				pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; +			if (PageAnon(page))</span>
<span class="quote">&gt; +				page_add_anon_rmap(page, vma, addr, false);</span>
<span class="quote">&gt; +			else</span>
<span class="quote">&gt; +				page_add_file_rmap(page, false);</span>

Why do we do the rmap bits here?
<span class="quote">

&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		addr = restart;</span>
<span class="quote">&gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +			unlock_page(page);</span>
<span class="quote">&gt; +			restore--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; +				put_page(page);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			putback_lru_page(page);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!restore)</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		try_to_unmap(page, flags);</span>
<span class="quote">&gt; +		if (page_mapped(page) || !hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +			restore++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		remove_migration_ptes(page, page, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +		unlock_page(page);</span>
<span class="quote">&gt; +		restore--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		putback_lru_page(page);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		unsigned long next;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; +			addr = next;</span>
<span class="quote">&gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			struct address_space *mapping;</span>
<span class="quote">&gt; +			struct page *newpage, *page;</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +			int r;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!newpage || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			if (pte_none(*ptep) || pte_present(*ptep)) {</span>
<span class="quote">&gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +				put_page(newpage);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry)) {</span>
<span class="quote">&gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +				put_page(newpage);</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +			mapping = page_mapping(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * For now only support private anonymous when migrating</span>
<span class="quote">&gt; +			 * to un-addressable device memory.</span>

I thought HMM supported page cache migration as well.
<span class="quote">
&gt; +			 */</span>
<span class="quote">&gt; +			if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="quote">&gt; +			    !is_addressable_page(newpage)) {</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			r = migrate_page(mapping, newpage, page,</span>
<span class="quote">&gt; +					 MIGRATE_SYNC, false);</span>
<span class="quote">&gt; +			if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="quote">&gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; +		unsigned long next;</span>
<span class="quote">&gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; +		pud_t *pudp;</span>
<span class="quote">&gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; +		pte_t *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; +			struct page *page, *newpage;</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (pte_none(*ptep) || pte_present(*ptep))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; +			if (!newpage)</span>
<span class="quote">&gt; +				newpage = page;</span>
<span class="quote">&gt; +			remove_migration_ptes(page, newpage, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; +			unlock_page(page);</span>
<span class="quote">&gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_zone_device_page(page))</span>
<span class="quote">&gt; +				put_page(page);</span>
<span class="quote">&gt; +			else</span>
<span class="quote">&gt; +				putback_lru_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (newpage != page) {</span>
<span class="quote">&gt; +				unlock_page(newpage);</span>
<span class="quote">&gt; +				if (is_zone_device_page(newpage))</span>
<span class="quote">&gt; +					put_page(newpage);</span>
<span class="quote">&gt; +				else</span>
<span class="quote">&gt; +					putback_lru_page(newpage);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @ops: migration callback for allocating destination memory and copying</span>
<span class="quote">&gt; + * @vma: virtual memory area containing the range to be migrated</span>
<span class="quote">&gt; + * @start: start address of the range to migrate (inclusive)</span>
<span class="quote">&gt; + * @end: end address of the range to migrate (exclusive)</span>
<span class="quote">&gt; + * @pfns: array of hmm_pfn_t first containing source pfns then destination</span>
<span class="quote">&gt; + * @private: pointer passed back to each of the callback</span>
<span class="quote">&gt; + * Returns: 0 on success, error code otherwise</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This will try to migrate a range of memory using callback to allocate and</span>
<span class="quote">&gt; + * copy memory from source to destination. This function will first collect,</span>
<span class="quote">&gt; + * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="quote">&gt; + * for device driver to allocate destination memory and copy from source.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="quote">&gt; + * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="quote">&gt; + * table it will call finalize_and_map() callback so that device driver can</span>
<span class="quote">&gt; + * inspect what have been successfully migrated and update its own page table</span>
<span class="quote">&gt; + * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="quote">&gt; + * this API).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Finaly the function update CPU page table and unlock the pages before</span>
<span class="quote">&gt; + * returning 0.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * It will return an error code only if one of the argument is invalid.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		    unsigned long start,</span>
<span class="quote">&gt; +		    unsigned long end,</span>
<span class="quote">&gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +		    void *private)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_migrate migrate;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Sanity check the arguments */</span>
<span class="quote">&gt; +	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; +	end &amp;= PAGE_MASK;</span>
<span class="quote">&gt; +	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (!vma || !ops || !pfns || start &gt;= end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	migrate.start = start;</span>
<span class="quote">&gt; +	migrate.pfns = pfns;</span>
<span class="quote">&gt; +	migrate.npages = 0;</span>
<span class="quote">&gt; +	migrate.end = end;</span>
<span class="quote">&gt; +	migrate.vma = vma;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Collect, and try to unmap source pages */</span>
<span class="quote">&gt; +	hmm_migrate_collect(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Lock and isolate page */</span>
<span class="quote">&gt; +	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Unmap pages */</span>
<span class="quote">&gt; +	hmm_migrate_unmap(&amp;migrate);</span>
<span class="quote">&gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="quote">&gt; +	 * content and can safely be copied to destination memory that is</span>
<span class="quote">&gt; +	 * allocated by the callback.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="quote">&gt; +	 * individual page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	ops-&gt;alloc_and_copy(vma, start, end, pfns, private);</span>

What is the expectation from alloc_and_copy()? Can it fail?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/* This does the real migration of struct page */</span>
<span class="quote">&gt; +	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ops-&gt;finalize_and_map(vma, start, end, pfns, private);</span>

Is this just notification to the driver or more?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/* Unlock and remap pages */</span>
<span class="quote">&gt; +	hmm_migrate_remove_migration_pte(&amp;migrate);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(hmm_vma_migrate);</span>
<span class="quote">&gt; +#endif /* CONFIG_HMM */</span>
<span class="quote">&gt; </span>

Balbir Singh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 21, 2016, 5:31 a.m.</div>
<pre class="content">
On Mon, Nov 21, 2016 at 02:30:46PM +1100, Balbir Singh wrote:
<span class="quote">&gt; On 19/11/16 05:18, Jérôme Glisse wrote:</span>

[...]
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#if defined(CONFIG_HMM)</span>
<span class="quote">&gt; &gt; +struct hmm_migrate {</span>
<span class="quote">&gt; &gt; +	struct vm_area_struct	*vma;</span>
<span class="quote">&gt; &gt; +	unsigned long		start;</span>
<span class="quote">&gt; &gt; +	unsigned long		end;</span>
<span class="quote">&gt; &gt; +	unsigned long		npages;</span>
<span class="quote">&gt; &gt; +	hmm_pfn_t		*pfns;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I presume the destination is pfns[] or is the source?</span>

Both when alloca_and_copy() is call it is fill with source memory, but once
that callback returns it must have set the destination memory inside that
array. This is what i discussed with Aneesh in this thread.
<span class="quote">
&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; &gt; +				unsigned long start,</span>
<span class="quote">&gt; &gt; +				unsigned long end,</span>
<span class="quote">&gt; &gt; +				struct mm_walk *walk)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	unsigned long addr = start;</span>
<span class="quote">&gt; &gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +	hmm_pfn_t *pfns;</span>
<span class="quote">&gt; &gt; +	int pages = 0;</span>
<span class="quote">&gt; &gt; +	pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +again:</span>
<span class="quote">&gt; &gt; +	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="quote">&gt; &gt; +	if (pmd_trans_unstable(pmdp))</span>
<span class="quote">&gt; &gt; +		goto again;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK., so we always split THP before migration</span>

Yes because i need special swap entry and those does not exist for pmd.
<span class="quote">
&gt; &gt; +	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="quote">&gt; &gt; +	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +	arch_enter_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="quote">&gt; &gt; +		unsigned long pfn;</span>
<span class="quote">&gt; &gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +		struct page *page;</span>
<span class="quote">&gt; &gt; +		hmm_pfn_t flags;</span>
<span class="quote">&gt; &gt; +		bool write;</span>
<span class="quote">&gt; &gt; +		pte_t pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; &gt; +		if (!pte_present(pte)) {</span>
<span class="quote">&gt; &gt; +			if (pte_none(pte))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; &gt; +			if (!is_device_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why hard code this, in general the ability to migrate a VMA</span>
<span class="quote">&gt; start/end range seems like a useful API.</span>

Some memory can not be migrated, can not migrate something that is already
being migrated or something that is swap or something that is bad memory
... I only try to migrate valid memory.
<span class="quote">
&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently UNADDRESSABLE?</span>

Yes, this is a special device swap entry and those it is unaddressable memory.
The destination memory might also be unaddressable (migrating from one device
to another device).
<span class="quote">

&gt; &gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			write = is_write_device_entry(entry);</span>
<span class="quote">&gt; &gt; +			pfn = page_to_pfn(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pfn = pte_pfn(pte);</span>
<span class="quote">&gt; &gt; +			page = pfn_to_page(pfn);</span>
<span class="quote">&gt; &gt; +			write = pte_write(pte);</span>
<span class="quote">&gt; &gt; +			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="quote">&gt; &gt; +		if (PageTransCompound(page))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Didn&#39;t we split the THP above?</span>

We splited huge pmd not huge page. Intention is to support huge page but i wanted
to keep patch simple and THP need special handling when it comes to refcount to
check for pin (either on huge page or on one of its tail page).
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="quote">&gt; &gt; +		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; &gt; +		migrate-&gt;npages++;</span>
<span class="quote">&gt; &gt; +		get_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; put_page()?</span>

No, we will try latter to lock the page and thus we want to keep a ref on the page.
<span class="quote">
&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			pte_t swp_pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			*pfns |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = make_migration_entry(page, write);</span>
<span class="quote">&gt; &gt; +			swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt; +			if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; &gt; +				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			pages++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	arch_leave_lazy_mmu_mode();</span>
<span class="quote">&gt; &gt; +	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Only flush the TLB if we actually modified any entries */</span>
<span class="quote">&gt; &gt; +	if (pages)</span>
<span class="quote">&gt; &gt; +		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mm_walk mm_walk;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="quote">&gt; &gt; +	mm_walk.pte_entry = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.pte_hole = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.hugetlb_entry = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.test_walk = NULL;</span>
<span class="quote">&gt; &gt; +	mm_walk.vma = migrate-&gt;vma;</span>
<span class="quote">&gt; &gt; +	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	mm_walk.private = migrate;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="quote">&gt; &gt; +					    migrate-&gt;start,</span>
<span class="quote">&gt; &gt; +					    migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="quote">&gt; &gt; +	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="quote">&gt; &gt; +					  migrate-&gt;start,</span>
<span class="quote">&gt; &gt; +					  migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline bool hmm_migrate_page_check(struct page *page, int extra)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="quote">&gt; &gt; +	 * check them then regular page because they can be map with a pmd or</span>
<span class="quote">&gt; &gt; +	 * with a pte (split pte mapping).</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (PageCompound(page))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; PageTransCompound()?</span>

Yes, right now i think on all arch it is equivalent.
<span class="quote">

&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +		extra++;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return true;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="quote">&gt; &gt; +	unsigned long restore = 0;</span>
<span class="quote">&gt; &gt; +	bool allow_drain = true;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	lru_add_drain();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +again:</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page)</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="quote">&gt; &gt; +			lock_page(page);</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* ZONE_DEVICE page are not on LRU */</span>
<span class="quote">&gt; &gt; +		if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +			goto check;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="quote">&gt; &gt; +			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="quote">&gt; &gt; +			lru_add_drain_all();</span>
<span class="quote">&gt; &gt; +			allow_drain = false;</span>
<span class="quote">&gt; &gt; +			goto again;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (isolate_lru_page(page)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		} else</span>
<span class="quote">&gt; &gt; +			/* Drop the reference we took in collect */</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +check:</span>
<span class="quote">&gt; &gt; +		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!restore)</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (addr = migrate-&gt;start, i = 0; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +		unsigned long next, restart;</span>
<span class="quote">&gt; &gt; +		spinlock_t *ptl;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE)) {</span>
<span class="quote">&gt; &gt; +			addr += PAGE_SIZE;</span>
<span class="quote">&gt; &gt; +			i++;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		restart = addr;</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; &gt; +			addr = next;</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +			bool write;</span>
<span class="quote">&gt; &gt; +			pte_t pte;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			write = migrate-&gt;pfns[i] &amp; HMM_PFN_WRITE;</span>
<span class="quote">&gt; &gt; +			write &amp;= (vma-&gt;vm_flags &amp; VM_WRITE);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			/* Here it means pte must be a valid migration entry */</span>
<span class="quote">&gt; &gt; +			pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="quote">&gt; &gt; +			if (pte_none(pte) || pte_present(pte))</span>
<span class="quote">&gt; &gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; &gt; +				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page) &amp;&amp;</span>
<span class="quote">&gt; &gt; +			    !is_addressable_page(page)) {</span>
<span class="quote">&gt; &gt; +				entry = make_device_entry(page, write);</span>
<span class="quote">&gt; &gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; +				pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; &gt; +				pte = pte_mkold(pte);</span>
<span class="quote">&gt; &gt; +				if (write)</span>
<span class="quote">&gt; &gt; +					pte = pte_mkwrite(pte);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +			if (pte_swp_soft_dirty(*ptep))</span>
<span class="quote">&gt; &gt; +				pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			get_page(page);</span>
<span class="quote">&gt; &gt; +			set_pte_at(mm, addr, ptep, pte);</span>
<span class="quote">&gt; &gt; +			if (PageAnon(page))</span>
<span class="quote">&gt; &gt; +				page_add_anon_rmap(page, vma, addr, false);</span>
<span class="quote">&gt; &gt; +			else</span>
<span class="quote">&gt; &gt; +				page_add_file_rmap(page, false);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why do we do the rmap bits here?</span>

Because we did page_remove_rmap() in hmm_migrate_collect() so we need to restore
rmap.
<span class="quote">

&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		addr = restart;</span>
<span class="quote">&gt; &gt; +		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="quote">&gt; &gt; +			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt; &gt; +			restore--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			putback_lru_page(page);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!restore)</span>
<span class="quote">&gt; &gt; +			break;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		try_to_unmap(page, flags);</span>
<span class="quote">&gt; &gt; +		if (page_mapped(page) || !hmm_migrate_page_check(page, 1)) {</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +			restore++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="quote">&gt; &gt; +		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		remove_migration_ptes(page, page, false);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +		unlock_page(page);</span>
<span class="quote">&gt; &gt; +		restore--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (is_zone_device_page(page)) {</span>
<span class="quote">&gt; &gt; +			put_page(page);</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		putback_lru_page(page);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		unsigned long next;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		if (!pudp || pud_none(*pudp)) {</span>
<span class="quote">&gt; &gt; +			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt; &gt; +			addr = next;</span>
<span class="quote">&gt; &gt; +			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			struct address_space *mapping;</span>
<span class="quote">&gt; &gt; +			struct page *newpage, *page;</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +			int r;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!newpage || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			if (pte_none(*ptep) || pte_present(*ptep)) {</span>
<span class="quote">&gt; &gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +				put_page(newpage);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				/* This should not happen but be nice */</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +				put_page(newpage);</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			mapping = page_mapping(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * For now only support private anonymous when migrating</span>
<span class="quote">&gt; &gt; +			 * to un-addressable device memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought HMM supported page cache migration as well.</span>

Not for un-addressable memory. Un-addressable memory need more change to filesystem
to handle read/write and writeback. This will be part of a separate patchset.
<span class="quote">

&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt; +			if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="quote">&gt; &gt; +			    !is_addressable_page(newpage)) {</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			r = migrate_page(mapping, newpage, page,</span>
<span class="quote">&gt; &gt; +					 MIGRATE_SYNC, false);</span>
<span class="quote">&gt; &gt; +			if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="quote">&gt; &gt; +				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="quote">&gt; &gt; +	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="quote">&gt; &gt; +		unsigned long next;</span>
<span class="quote">&gt; &gt; +		pgd_t *pgdp;</span>
<span class="quote">&gt; &gt; +		pud_t *pudp;</span>
<span class="quote">&gt; &gt; +		pmd_t *pmdp;</span>
<span class="quote">&gt; &gt; +		pte_t *ptep;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		pgdp = pgd_offset(mm, addr);</span>
<span class="quote">&gt; &gt; +		pudp = pud_offset(pgdp, addr);</span>
<span class="quote">&gt; &gt; +		pmdp = pmd_offset(pudp, addr);</span>
<span class="quote">&gt; &gt; +		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		/* No need to lock nothing can change from under us */</span>
<span class="quote">&gt; &gt; +		ptep = pte_offset_map(pmdp, addr);</span>
<span class="quote">&gt; &gt; +		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="quote">&gt; &gt; +			struct page *page, *newpage;</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (pte_none(*ptep) || pte_present(*ptep))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +			entry = pte_to_swp_entry(*ptep);</span>
<span class="quote">&gt; &gt; +			if (!is_migration_entry(entry))</span>
<span class="quote">&gt; &gt; +				continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="quote">&gt; &gt; +			if (!newpage)</span>
<span class="quote">&gt; &gt; +				newpage = page;</span>
<span class="quote">&gt; &gt; +			remove_migration_ptes(page, newpage, false);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			migrate-&gt;pfns[i] = 0;</span>
<span class="quote">&gt; &gt; +			unlock_page(page);</span>
<span class="quote">&gt; &gt; +			migrate-&gt;npages--;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_zone_device_page(page))</span>
<span class="quote">&gt; &gt; +				put_page(page);</span>
<span class="quote">&gt; &gt; +			else</span>
<span class="quote">&gt; &gt; +				putback_lru_page(page);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (newpage != page) {</span>
<span class="quote">&gt; &gt; +				unlock_page(newpage);</span>
<span class="quote">&gt; &gt; +				if (is_zone_device_page(newpage))</span>
<span class="quote">&gt; &gt; +					put_page(newpage);</span>
<span class="quote">&gt; &gt; +				else</span>
<span class="quote">&gt; &gt; +					putback_lru_page(newpage);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		pte_unmap(ptep - 1);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @ops: migration callback for allocating destination memory and copying</span>
<span class="quote">&gt; &gt; + * @vma: virtual memory area containing the range to be migrated</span>
<span class="quote">&gt; &gt; + * @start: start address of the range to migrate (inclusive)</span>
<span class="quote">&gt; &gt; + * @end: end address of the range to migrate (exclusive)</span>
<span class="quote">&gt; &gt; + * @pfns: array of hmm_pfn_t first containing source pfns then destination</span>
<span class="quote">&gt; &gt; + * @private: pointer passed back to each of the callback</span>
<span class="quote">&gt; &gt; + * Returns: 0 on success, error code otherwise</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This will try to migrate a range of memory using callback to allocate and</span>
<span class="quote">&gt; &gt; + * copy memory from source to destination. This function will first collect,</span>
<span class="quote">&gt; &gt; + * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="quote">&gt; &gt; + * for device driver to allocate destination memory and copy from source.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="quote">&gt; &gt; + * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="quote">&gt; &gt; + * table it will call finalize_and_map() callback so that device driver can</span>
<span class="quote">&gt; &gt; + * inspect what have been successfully migrated and update its own page table</span>
<span class="quote">&gt; &gt; + * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="quote">&gt; &gt; + * this API).</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Finaly the function update CPU page table and unlock the pages before</span>
<span class="quote">&gt; &gt; + * returning 0.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * It will return an error code only if one of the argument is invalid.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="quote">&gt; &gt; +		    struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +		    unsigned long start,</span>
<span class="quote">&gt; &gt; +		    unsigned long end,</span>
<span class="quote">&gt; &gt; +		    hmm_pfn_t *pfns,</span>
<span class="quote">&gt; &gt; +		    void *private)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_migrate migrate;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Sanity check the arguments */</span>
<span class="quote">&gt; &gt; +	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; &gt; +	end &amp;= PAGE_MASK;</span>
<span class="quote">&gt; &gt; +	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (!vma || !ops || !pfns || start &gt;= end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	migrate.start = start;</span>
<span class="quote">&gt; &gt; +	migrate.pfns = pfns;</span>
<span class="quote">&gt; &gt; +	migrate.npages = 0;</span>
<span class="quote">&gt; &gt; +	migrate.end = end;</span>
<span class="quote">&gt; &gt; +	migrate.vma = vma;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Collect, and try to unmap source pages */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_collect(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Lock and isolate page */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Unmap pages */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_unmap(&amp;migrate);</span>
<span class="quote">&gt; &gt; +	if (!migrate.npages)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="quote">&gt; &gt; +	 * content and can safely be copied to destination memory that is</span>
<span class="quote">&gt; &gt; +	 * allocated by the callback.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="quote">&gt; &gt; +	 * individual page.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	ops-&gt;alloc_and_copy(vma, start, end, pfns, private);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What is the expectation from alloc_and_copy()? Can it fail?</span>

It can fail, there is no global status it is all handled on individual page
basis. So for instance if a device can only allocate its device memory as 64
chunk than it  can migrate any chunk that match this constraint and fail for
anything smaller than that.
<span class="quote">

&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* This does the real migration of struct page */</span>
<span class="quote">&gt; &gt; +	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	ops-&gt;finalize_and_map(vma, start, end, pfns, private);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this just notification to the driver or more?</span>

Just a notification to driver.

Cheers,
Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="p_header">index c79abfc..9777309 100644</span>
<span class="p_header">--- a/include/linux/hmm.h</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -101,10 +101,13 @@</span> <span class="p_context"> struct hmm;</span>
  * HMM_PFN_EMPTY: corresponding CPU page table entry is none (pte_none() true)
  * HMM_PFN_FAULT: use by hmm_vma_fault() to signify which address need faulting
  * HMM_PFN_DEVICE: this is device memory (ie a ZONE_DEVICE page)
<span class="p_add">+ * HMM_PFN_LOCKED: underlying struct page is lock</span>
  * HMM_PFN_SPECIAL: corresponding CPU page table entry is special ie result of
  *      vm_insert_pfn() or vm_insert_page() and thus should not be mirror by a
  *      device (the entry will never have HMM_PFN_VALID set and the pfn value
  *      is undefine)
<span class="p_add">+ * HMM_PFN_MIGRATE: use by hmm_vma_migrate() to signify which address can be</span>
<span class="p_add">+ *      migrated</span>
  * HMM_PFN_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)
  */
 typedef unsigned long hmm_pfn_t;
<span class="p_chunk">@@ -116,9 +119,11 @@</span> <span class="p_context"> typedef unsigned long hmm_pfn_t;</span>
 #define HMM_PFN_EMPTY (1 &lt;&lt; 4)
 #define HMM_PFN_FAULT (1 &lt;&lt; 5)
 #define HMM_PFN_DEVICE (1 &lt;&lt; 6)
<span class="p_del">-#define HMM_PFN_SPECIAL (1 &lt;&lt; 7)</span>
<span class="p_del">-#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 8)</span>
<span class="p_del">-#define HMM_PFN_SHIFT 9</span>
<span class="p_add">+#define HMM_PFN_LOCKED (1 &lt;&lt; 7)</span>
<span class="p_add">+#define HMM_PFN_SPECIAL (1 &lt;&lt; 8)</span>
<span class="p_add">+#define HMM_PFN_MIGRATE (1 &lt;&lt; 9)</span>
<span class="p_add">+#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 10)</span>
<span class="p_add">+#define HMM_PFN_SHIFT 11</span>
 
 static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)
 {
<span class="p_chunk">@@ -323,6 +328,49 @@</span> <span class="p_context"> bool hmm_vma_fault(struct vm_area_struct *vma,</span>
 		   hmm_pfn_t *pfns);
 
 
<span class="p_add">+/*</span>
<span class="p_add">+ * struct hmm_migrate_ops - migrate operation callback</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="p_add">+ * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The new HMM migrate helper hmm_vma_migrate() allow memory migration to use</span>
<span class="p_add">+ * device DMA engine to perform copy from source to destination memory it also</span>
<span class="p_add">+ * allow caller to use its own memory allocator for destination memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="p_add">+ * the entry, for those it must clear the HMM_PFN_MIGRATE flag. The destination</span>
<span class="p_add">+ * page must lock and the corresponding hmm_pfn_t value in the array updated</span>
<span class="p_add">+ * with the HMM_PFN_MIGRATE and HMM_PFN_LOCKED flag set (and of course be a</span>
<span class="p_add">+ * valid entry). It is expected that the page allocated will have an elevated</span>
<span class="p_add">+ * refcount and that a put_page() will free the page. Device driver might want</span>
<span class="p_add">+ * to allocate with an extra-refcount if they want to control deallocation of</span>
<span class="p_add">+ * failed migration inside the finalize_and_map() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Inside finalize_and_map() device driver must use the HMM_PFN_MIGRATE flag to</span>
<span class="p_add">+ * determine which page have been successfully migrated.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_migrate_ops {</span>
<span class="p_add">+	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="p_add">+			       unsigned long start,</span>
<span class="p_add">+			       unsigned long end,</span>
<span class="p_add">+			       hmm_pfn_t *pfns,</span>
<span class="p_add">+			       void *private);</span>
<span class="p_add">+	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long start,</span>
<span class="p_add">+				 unsigned long end,</span>
<span class="p_add">+				 hmm_pfn_t *pfns,</span>
<span class="p_add">+				 void *private);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="p_add">+		    struct vm_area_struct *vma,</span>
<span class="p_add">+		    unsigned long start,</span>
<span class="p_add">+		    unsigned long end,</span>
<span class="p_add">+		    hmm_pfn_t *pfns,</span>
<span class="p_add">+		    void *private);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 /* Below are for HMM internal use only ! Not to be use by device driver ! */
 void hmm_mm_destroy(struct mm_struct *mm);
 
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index d9ce8db..393d592 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/page_idle.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/memremap.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_chunk">@@ -421,6 +422,14 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	int expected_count = 1 + extra_count;
 	void **pslot;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="p_add">+	 * the MEMORY_MOVABLE flag set (see include/linux/memory_hotplug.h).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	expected_count += is_zone_device_page(page);</span>
<span class="p_add">+</span>
 	if (!mapping) {
 		/* Anonymous page without mapping */
 		if (page_count(page) != expected_count)
<span class="p_chunk">@@ -2087,3 +2096,578 @@</span> <span class="p_context"> out_unlock:</span>
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* CONFIG_NUMA */
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_HMM)</span>
<span class="p_add">+struct hmm_migrate {</span>
<span class="p_add">+	struct vm_area_struct	*vma;</span>
<span class="p_add">+	unsigned long		start;</span>
<span class="p_add">+	unsigned long		end;</span>
<span class="p_add">+	unsigned long		npages;</span>
<span class="p_add">+	hmm_pfn_t		*pfns;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end,</span>
<span class="p_add">+				struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long addr = start;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	hmm_pfn_t *pfns;</span>
<span class="p_add">+	int pages = 0;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+again:</span>
<span class="p_add">+	if (pmd_none(*pmdp))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="p_add">+	if (pmd_trans_unstable(pmdp))</span>
<span class="p_add">+		goto again;</span>
<span class="p_add">+</span>
<span class="p_add">+	pfns = &amp;migrate-&gt;pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="p_add">+	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+	arch_enter_lazy_mmu_mode();</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr += PAGE_SIZE, pfns++, ptep++) {</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		hmm_pfn_t flags;</span>
<span class="p_add">+		bool write;</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+		if (!pte_present(pte)) {</span>
<span class="p_add">+			if (pte_none(pte))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pte_to_swp_entry(pte);</span>
<span class="p_add">+			if (!is_device_entry(entry)) {</span>
<span class="p_add">+				set_pte_at(mm, addr, ptep, pte);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
<span class="p_add">+			write = is_write_device_entry(entry);</span>
<span class="p_add">+			pfn = page_to_pfn(page);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!(page-&gt;pgmap-&gt;flags &amp; MEMORY_MOVABLE)) {</span>
<span class="p_add">+				set_pte_at(mm, addr, ptep, pte);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pfn = pte_pfn(pte);</span>
<span class="p_add">+			page = pfn_to_page(pfn);</span>
<span class="p_add">+			write = pte_write(pte);</span>
<span class="p_add">+			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="p_add">+		if (PageTransCompound(page))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		*pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="p_add">+		*pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="p_add">+		migrate-&gt;npages++;</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!trylock_page(page)) {</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, pte);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pte_t swp_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+			*pfns |= HMM_PFN_LOCKED;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = make_migration_entry(page, write);</span>
<span class="p_add">+			swp_pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+			if (pte_soft_dirty(pte))</span>
<span class="p_add">+				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, swp_pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			pages++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_leave_lazy_mmu_mode();</span>
<span class="p_add">+	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Only flush the TLB if we actually modified any entries */</span>
<span class="p_add">+	if (pages)</span>
<span class="p_add">+		flush_tlb_range(walk-&gt;vma, start, end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_walk mm_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="p_add">+	mm_walk.pte_entry = NULL;</span>
<span class="p_add">+	mm_walk.pte_hole = NULL;</span>
<span class="p_add">+	mm_walk.hugetlb_entry = NULL;</span>
<span class="p_add">+	mm_walk.test_walk = NULL;</span>
<span class="p_add">+	mm_walk.vma = migrate-&gt;vma;</span>
<span class="p_add">+	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	mm_walk.private = migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="p_add">+					    migrate-&gt;start,</span>
<span class="p_add">+					    migrate-&gt;end);</span>
<span class="p_add">+	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="p_add">+					  migrate-&gt;start,</span>
<span class="p_add">+					  migrate-&gt;end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool hmm_migrate_page_check(struct page *page, int extra)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="p_add">+	 * check them then regular page because they can be map with a pmd or</span>
<span class="p_add">+	 * with a pte (split pte mapping).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PageCompound(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_zone_device_page(page))</span>
<span class="p_add">+		extra++;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	struct vm_area_struct *vma = migrate-&gt;vma;</span>
<span class="p_add">+	unsigned long restore = 0;</span>
<span class="p_add">+	bool allow_drain = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+</span>
<span class="p_add">+again:</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!(migrate-&gt;pfns[i] &amp; HMM_PFN_LOCKED)) {</span>
<span class="p_add">+			lock_page(page);</span>
<span class="p_add">+			migrate-&gt;pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* ZONE_DEVICE page are not on LRU */</span>
<span class="p_add">+		if (is_zone_device_page(page))</span>
<span class="p_add">+			goto check;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="p_add">+			/* Drain CPU&#39;s pagevec so page can be isolated */</span>
<span class="p_add">+			lru_add_drain_all();</span>
<span class="p_add">+			allow_drain = false;</span>
<span class="p_add">+			goto again;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (isolate_lru_page(page)) {</span>
<span class="p_add">+			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			/* Drop the reference we took in collect */</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+check:</span>
<span class="p_add">+		if (!hmm_migrate_page_check(page, 1)) {</span>
<span class="p_add">+			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!restore)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = migrate-&gt;start, i = 0; addr &lt; migrate-&gt;end;) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+		unsigned long next, restart;</span>
<span class="p_add">+		spinlock_t *ptl;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE)) {</span>
<span class="p_add">+			addr += PAGE_SIZE;</span>
<span class="p_add">+			i++;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		restart = addr;</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="p_add">+			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pudp = pud_offset(pgdp, addr);</span>
<span class="p_add">+		if (!pudp || pud_none(*pudp)) {</span>
<span class="p_add">+			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pmdp = pmd_offset(pudp, addr);</span>
<span class="p_add">+		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="p_add">+			addr = next;</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+			bool write;</span>
<span class="p_add">+			pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			write = migrate-&gt;pfns[i] &amp; HMM_PFN_WRITE;</span>
<span class="p_add">+			write &amp;= (vma-&gt;vm_flags &amp; VM_WRITE);</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Here it means pte must be a valid migration entry */</span>
<span class="p_add">+			pte = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+			if (pte_none(pte) || pte_present(pte))</span>
<span class="p_add">+				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			entry = pte_to_swp_entry(pte);</span>
<span class="p_add">+			if (!is_migration_entry(entry))</span>
<span class="p_add">+				/* SOMETHING BAD IS GOING ON ! */</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_zone_device_page(page) &amp;&amp;</span>
<span class="p_add">+			    !is_addressable_page(page)) {</span>
<span class="p_add">+				entry = make_device_entry(page, write);</span>
<span class="p_add">+				pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				pte = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="p_add">+				pte = pte_mkold(pte);</span>
<span class="p_add">+				if (write)</span>
<span class="p_add">+					pte = pte_mkwrite(pte);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			if (pte_swp_soft_dirty(*ptep))</span>
<span class="p_add">+				pte = pte_mksoft_dirty(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, pte);</span>
<span class="p_add">+			if (PageAnon(page))</span>
<span class="p_add">+				page_add_anon_rmap(page, vma, addr, false);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				page_add_file_rmap(page, false);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+		addr = restart;</span>
<span class="p_add">+		i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		for (; addr &lt; next &amp;&amp; restore; addr += PAGE_SHIFT, i++) {</span>
<span class="p_add">+			page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+			if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			migrate-&gt;pfns[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_zone_device_page(page)) {</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!restore)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		try_to_unmap(page, flags);</span>
<span class="p_add">+		if (page_mapped(page) || !hmm_migrate_page_check(page, 1)) {</span>
<span class="p_add">+			migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;pfns[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_zone_device_page(page)) {</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="p_add">+		unsigned long next;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		if (!pgdp || pgd_none_or_clear_bad(pgdp)) {</span>
<span class="p_add">+			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pudp = pud_offset(pgdp, addr);</span>
<span class="p_add">+		if (!pudp || pud_none(*pudp)) {</span>
<span class="p_add">+			addr = pgd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pmdp = pmd_offset(pudp, addr);</span>
<span class="p_add">+		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+		if (!pmdp || pmd_none(*pmdp) || pmd_trans_huge(*pmdp)) {</span>
<span class="p_add">+			addr = next;</span>
<span class="p_add">+			i = (addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* No need to lock nothing can change from under us */</span>
<span class="p_add">+		ptep = pte_offset_map(pmdp, addr);</span>
<span class="p_add">+		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="p_add">+			struct address_space *mapping;</span>
<span class="p_add">+			struct page *newpage, *page;</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+			int r;</span>
<span class="p_add">+</span>
<span class="p_add">+			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+			if (!newpage || !(migrate-&gt;pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			if (pte_none(*ptep) || pte_present(*ptep)) {</span>
<span class="p_add">+				/* This should not happen but be nice */</span>
<span class="p_add">+				migrate-&gt;pfns[i] = 0;</span>
<span class="p_add">+				put_page(newpage);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			entry = pte_to_swp_entry(*ptep);</span>
<span class="p_add">+			if (!is_migration_entry(entry)) {</span>
<span class="p_add">+				/* This should not happen but be nice */</span>
<span class="p_add">+				migrate-&gt;pfns[i] = 0;</span>
<span class="p_add">+				put_page(newpage);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+			mapping = page_mapping(page);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For now only support private anonymous when migrating</span>
<span class="p_add">+			 * to un-addressable device memory.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="p_add">+			    !is_addressable_page(newpage)) {</span>
<span class="p_add">+				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			r = migrate_page(mapping, newpage, page,</span>
<span class="p_add">+					 MIGRATE_SYNC, false);</span>
<span class="p_add">+			if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="p_add">+				migrate-&gt;pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap(ptep - 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+	struct mm_struct *mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end;) {</span>
<span class="p_add">+		unsigned long next;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		pudp = pud_offset(pgdp, addr);</span>
<span class="p_add">+		pmdp = pmd_offset(pudp, addr);</span>
<span class="p_add">+		next = pmd_addr_end(addr, migrate-&gt;end);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* No need to lock nothing can change from under us */</span>
<span class="p_add">+		ptep = pte_offset_map(pmdp, addr);</span>
<span class="p_add">+		for (; addr &lt; next; addr += PAGE_SIZE, i++, ptep++) {</span>
<span class="p_add">+			struct page *page, *newpage;</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pte_none(*ptep) || pte_present(*ptep))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			entry = pte_to_swp_entry(*ptep);</span>
<span class="p_add">+			if (!is_migration_entry(entry))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+			newpage = hmm_pfn_to_page(migrate-&gt;pfns[i]);</span>
<span class="p_add">+			if (!newpage)</span>
<span class="p_add">+				newpage = page;</span>
<span class="p_add">+			remove_migration_ptes(page, newpage, false);</span>
<span class="p_add">+</span>
<span class="p_add">+			migrate-&gt;pfns[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_zone_device_page(page))</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (newpage != page) {</span>
<span class="p_add">+				unlock_page(newpage);</span>
<span class="p_add">+				if (is_zone_device_page(newpage))</span>
<span class="p_add">+					put_page(newpage);</span>
<span class="p_add">+				else</span>
<span class="p_add">+					putback_lru_page(newpage);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap(ptep - 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_add">+ * @vma: virtual memory area containing the range to be migrated</span>
<span class="p_add">+ * @start: start address of the range to migrate (inclusive)</span>
<span class="p_add">+ * @end: end address of the range to migrate (exclusive)</span>
<span class="p_add">+ * @pfns: array of hmm_pfn_t first containing source pfns then destination</span>
<span class="p_add">+ * @private: pointer passed back to each of the callback</span>
<span class="p_add">+ * Returns: 0 on success, error code otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will try to migrate a range of memory using callback to allocate and</span>
<span class="p_add">+ * copy memory from source to destination. This function will first collect,</span>
<span class="p_add">+ * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="p_add">+ * for device driver to allocate destination memory and copy from source.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="p_add">+ * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="p_add">+ * table it will call finalize_and_map() callback so that device driver can</span>
<span class="p_add">+ * inspect what have been successfully migrated and update its own page table</span>
<span class="p_add">+ * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="p_add">+ * this API).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Finaly the function update CPU page table and unlock the pages before</span>
<span class="p_add">+ * returning 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It will return an error code only if one of the argument is invalid.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="p_add">+		    struct vm_area_struct *vma,</span>
<span class="p_add">+		    unsigned long start,</span>
<span class="p_add">+		    unsigned long end,</span>
<span class="p_add">+		    hmm_pfn_t *pfns,</span>
<span class="p_add">+		    void *private)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_migrate migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity check the arguments */</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	end &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (!vma || !ops || !pfns || start &gt;= end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	migrate.start = start;</span>
<span class="p_add">+	migrate.pfns = pfns;</span>
<span class="p_add">+	migrate.npages = 0;</span>
<span class="p_add">+	migrate.end = end;</span>
<span class="p_add">+	migrate.vma = vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Collect, and try to unmap source pages */</span>
<span class="p_add">+	hmm_migrate_collect(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Lock and isolate page */</span>
<span class="p_add">+	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unmap pages */</span>
<span class="p_add">+	hmm_migrate_unmap(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="p_add">+	 * content and can safely be copied to destination memory that is</span>
<span class="p_add">+	 * allocated by the callback.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="p_add">+	 * individual page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ops-&gt;alloc_and_copy(vma, start, end, pfns, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This does the real migration of struct page */</span>
<span class="p_add">+	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	ops-&gt;finalize_and_map(vma, start, end, pfns, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unlock and remap pages */</span>
<span class="p_add">+	hmm_migrate_remove_migration_pte(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_vma_migrate);</span>
<span class="p_add">+#endif /* CONFIG_HMM */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



