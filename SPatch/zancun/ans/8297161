
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>kernel: fs: drop_caches: add dds drop_caches_count - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    kernel: fs: drop_caches: add dds drop_caches_count</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=144021">Daniel Walker</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 12, 2016, 8:14 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1455308080-27238-1-git-send-email-danielwa@cisco.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8297161/mbox/"
   >mbox</a>
|
   <a href="/patch/8297161/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8297161/">/patch/8297161/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 39245C02AA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Feb 2016 20:14:50 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6262220435
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Feb 2016 20:14:49 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A886D20460
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Feb 2016 20:14:47 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751153AbcBLUOo (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 12 Feb 2016 15:14:44 -0500
Received: from alln-iport-7.cisco.com ([173.37.142.94]:9277 &quot;EHLO
	alln-iport-7.cisco.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1750714AbcBLUOm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 12 Feb 2016 15:14:42 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple;
	d=cisco.com; i=@cisco.com; l=6536; q=dns/txt; s=iport;
	t=1455308082; x=1456517682;
	h=from:to:cc:subject:date:message-id;
	bh=+kVN3cfAhegvL94PUDsooLjQwTWPFsx1bzqYpT4WuVk=;
	b=Pk7nOfqQoePEXtYY/88wI5B3E/ePm+MT/eqRLDEtfy5fdU1392mVpeiv
	EWvNgaEpY8wqKu5MbIjP2v8kRdVfXQjsMm26Lm9ne2+7k8cnku3Mf23xQ
	AOKzT/GPKjzLMB5pdnUFG/nHEGT8bK/8mVXrTdA1L0AYm1+3Xcy/TDsjk 0=;
X-IronPort-AV: E=Sophos;i=&quot;5.22,437,1449532800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;237881918&quot;
Received: from alln-core-7.cisco.com ([173.36.13.140])
	by alln-iport-7.cisco.com with ESMTP/TLS/DHE-RSA-AES256-SHA;
	12 Feb 2016 20:14:41 +0000
Received: from zorba.cisco.com ([10.21.172.23])
	by alln-core-7.cisco.com (8.14.5/8.14.5) with ESMTP id u1CKEeIB028979;
	Fri, 12 Feb 2016 20:14:40 GMT
From: Daniel Walker &lt;danielwa@cisco.com&gt;
To: Alexander Viro &lt;viro@zeniv.linux.org.uk&gt;
Cc: Khalid Mughal &lt;khalidm@cisco.com&gt;, xe-kernel@external.cisco.com,
	dave.hansen@intel.com, hannes@cmpxchg.org, riel@redhat.com,
	Jonathan Corbet &lt;corbet@lwn.net&gt;, linux-doc@vger.kernel.org,
	linux-kernel@vger.kernel.org, linux-fsdevel@vger.kernel.org,
	linux-mm@kvack.org
Subject: [PATCH] kernel: fs: drop_caches: add dds drop_caches_count
Date: Fri, 12 Feb 2016 12:14:39 -0800
Message-Id: &lt;1455308080-27238-1-git-send-email-danielwa@cisco.com&gt;
X-Mailer: git-send-email 2.5.0
X-Auto-Response-Suppress: DR, OOF, AutoReply
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-14.6 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI, RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY, 
	USER_IN_DEF_DKIM_WL autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=144021">Daniel Walker</a> - Feb. 12, 2016, 8:14 p.m.</div>
<pre class="content">
<span class="from">From: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>

Currently there is no way to figure out the droppable pagecache size
from the meminfo output. The MemFree size can shrink during normal
system operation, when some of the memory pages get cached and is
reflected in &quot;Cached&quot; field. Similarly for file operations some of
the buffer memory gets cached and it is reflected in &quot;Buffers&quot; field.
The kernel automatically reclaims all this cached &amp; buffered memory,
when it is needed elsewhere on the system. The only way to manually
reclaim this memory is by writing 1 to /proc/sys/vm/drop_caches. But
this can have performance impact. Since it discards cached objects,
it may cause high CPU &amp; I/O utilization to recreate the dropped
objects during heavy system load.
This patch computes the droppable pagecache count, using same
algorithm as &quot;vm/drop_caches&quot;. It is non-destructive and does not
drop any pages. Therefore it does not have any impact on system
performance. The computation does not include the size of
reclaimable slab.

Cc: xe-kernel@external.cisco.com
Cc: dave.hansen@intel.com
Cc: hannes@cmpxchg.org
Cc: riel@redhat.com
<span class="signed-off-by">Signed-off-by: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Daniel Walker &lt;danielwa@cisco.com&gt;</span>
---
 Documentation/sysctl/vm.txt | 12 +++++++
 fs/drop_caches.c            | 80 +++++++++++++++++++++++++++++++++++++++++++--
 include/linux/mm.h          |  3 ++
 kernel/sysctl.c             |  7 ++++
 4 files changed, 100 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 14, 2016, 9:18 p.m.</div>
<pre class="content">
On Fri, Feb 12, 2016 at 12:14:39PM -0800, Daniel Walker wrote:
<span class="quote">&gt; From: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently there is no way to figure out the droppable pagecache size</span>
<span class="quote">&gt; from the meminfo output. The MemFree size can shrink during normal</span>
<span class="quote">&gt; system operation, when some of the memory pages get cached and is</span>
<span class="quote">&gt; reflected in &quot;Cached&quot; field. Similarly for file operations some of</span>
<span class="quote">&gt; the buffer memory gets cached and it is reflected in &quot;Buffers&quot; field.</span>
<span class="quote">&gt; The kernel automatically reclaims all this cached &amp; buffered memory,</span>
<span class="quote">&gt; when it is needed elsewhere on the system. The only way to manually</span>
<span class="quote">&gt; reclaim this memory is by writing 1 to /proc/sys/vm/drop_caches. But</span>
<span class="quote">&gt; this can have performance impact. Since it discards cached objects,</span>
<span class="quote">&gt; it may cause high CPU &amp; I/O utilization to recreate the dropped</span>
<span class="quote">&gt; objects during heavy system load.</span>
<span class="quote">&gt; This patch computes the droppable pagecache count, using same</span>
<span class="quote">&gt; algorithm as &quot;vm/drop_caches&quot;. It is non-destructive and does not</span>
<span class="quote">&gt; drop any pages. Therefore it does not have any impact on system</span>
<span class="quote">&gt; performance. The computation does not include the size of</span>
<span class="quote">&gt; reclaimable slab.</span>

Why, exactly, do you need this? You&#39;ve described what the patch
does (i.e. redundant, because we can read the code), and described
that the kernel already accounts this reclaimable memory elsewhere
and you can already read that and infer the amount of reclaimable
memory from it. So why isn&#39;t that accounting sufficient?

As to the code, I think it is a horrible hack - the calculation
does not come for free. Forcing iteration all the inodes in the
inode cache is not something we should allow users to do - what&#39;s to
stop someone just doing this 100 times in parallel and DOSing the
machine?

Or what happens when someone does &#39;grep &quot;&quot; /proc/sys/vm/*&quot; to see
what all the VM settings are on a machine with a couple of TB of
page cache spread across a couple of hundred million cached inodes?
It a) takes a long time, b) adds sustained load to an already
contended lock (sb-&gt;s_inode_list_lock), and c) isn&#39;t configuration
information.

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=144021">Daniel Walker</a> - Feb. 15, 2016, 6:19 p.m.</div>
<pre class="content">
On 02/14/2016 01:18 PM, Dave Chinner wrote:
<span class="quote">&gt; On Fri, Feb 12, 2016 at 12:14:39PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt;&gt; From: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Currently there is no way to figure out the droppable pagecache size</span>
<span class="quote">&gt;&gt; from the meminfo output. The MemFree size can shrink during normal</span>
<span class="quote">&gt;&gt; system operation, when some of the memory pages get cached and is</span>
<span class="quote">&gt;&gt; reflected in &quot;Cached&quot; field. Similarly for file operations some of</span>
<span class="quote">&gt;&gt; the buffer memory gets cached and it is reflected in &quot;Buffers&quot; field.</span>
<span class="quote">&gt;&gt; The kernel automatically reclaims all this cached &amp; buffered memory,</span>
<span class="quote">&gt;&gt; when it is needed elsewhere on the system. The only way to manually</span>
<span class="quote">&gt;&gt; reclaim this memory is by writing 1 to /proc/sys/vm/drop_caches. But</span>
<span class="quote">&gt;&gt; this can have performance impact. Since it discards cached objects,</span>
<span class="quote">&gt;&gt; it may cause high CPU &amp; I/O utilization to recreate the dropped</span>
<span class="quote">&gt;&gt; objects during heavy system load.</span>
<span class="quote">&gt;&gt; This patch computes the droppable pagecache count, using same</span>
<span class="quote">&gt;&gt; algorithm as &quot;vm/drop_caches&quot;. It is non-destructive and does not</span>
<span class="quote">&gt;&gt; drop any pages. Therefore it does not have any impact on system</span>
<span class="quote">&gt;&gt; performance. The computation does not include the size of</span>
<span class="quote">&gt;&gt; reclaimable slab.</span>
<span class="quote">&gt; Why, exactly, do you need this? You&#39;ve described what the patch</span>
<span class="quote">&gt; does (i.e. redundant, because we can read the code), and described</span>
<span class="quote">&gt; that the kernel already accounts this reclaimable memory elsewhere</span>
<span class="quote">&gt; and you can already read that and infer the amount of reclaimable</span>
<span class="quote">&gt; memory from it. So why isn&#39;t that accounting sufficient?</span>

We need it to determine accurately what the free memory in the system 
is. If you know where we can get this information already please tell, 
we aren&#39;t aware of it. For instance /proc/meminfo isn&#39;t accurate enough.
<span class="quote">
&gt; As to the code, I think it is a horrible hack - the calculation</span>
<span class="quote">&gt; does not come for free. Forcing iteration all the inodes in the</span>
<span class="quote">&gt; inode cache is not something we should allow users to do - what&#39;s to</span>
<span class="quote">&gt; stop someone just doing this 100 times in parallel and DOSing the</span>
<span class="quote">&gt; machine?</span>

Yes it is costly.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Or what happens when someone does &#39;grep &quot;&quot; /proc/sys/vm/*&quot; to see</span>
<span class="quote">&gt; what all the VM settings are on a machine with a couple of TB of</span>
<span class="quote">&gt; page cache spread across a couple of hundred million cached inodes?</span>
<span class="quote">&gt; It a) takes a long time, b) adds sustained load to an already</span>
<span class="quote">&gt; contended lock (sb-&gt;s_inode_list_lock), and c) isn&#39;t configuration</span>
<span class="quote">&gt; information.</span>
<span class="quote">&gt;</span>

We could make it &quot;echo 4 &gt; /proc/sys/vm/drop_cache&quot; then you &quot;cat 
/proc/sys/vm/drop_cache_count&quot; that would make the person executing the 
command responsible for the latency. So grep wouldn&#39;t trigger it.

Daniel
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 15, 2016, 11:05 p.m.</div>
<pre class="content">
On Mon, Feb 15, 2016 at 10:19:54AM -0800, Daniel Walker wrote:
<span class="quote">&gt; On 02/14/2016 01:18 PM, Dave Chinner wrote:</span>
<span class="quote">&gt; &gt;On Fri, Feb 12, 2016 at 12:14:39PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt; &gt;&gt;From: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;Currently there is no way to figure out the droppable pagecache size</span>
<span class="quote">&gt; &gt;&gt;from the meminfo output. The MemFree size can shrink during normal</span>
<span class="quote">&gt; &gt;&gt;system operation, when some of the memory pages get cached and is</span>
<span class="quote">&gt; &gt;&gt;reflected in &quot;Cached&quot; field. Similarly for file operations some of</span>
<span class="quote">&gt; &gt;&gt;the buffer memory gets cached and it is reflected in &quot;Buffers&quot; field.</span>
<span class="quote">&gt; &gt;&gt;The kernel automatically reclaims all this cached &amp; buffered memory,</span>
<span class="quote">&gt; &gt;&gt;when it is needed elsewhere on the system. The only way to manually</span>
<span class="quote">&gt; &gt;&gt;reclaim this memory is by writing 1 to /proc/sys/vm/drop_caches. But</span>
<span class="quote">&gt; &gt;&gt;this can have performance impact. Since it discards cached objects,</span>
<span class="quote">&gt; &gt;&gt;it may cause high CPU &amp; I/O utilization to recreate the dropped</span>
<span class="quote">&gt; &gt;&gt;objects during heavy system load.</span>
<span class="quote">&gt; &gt;&gt;This patch computes the droppable pagecache count, using same</span>
<span class="quote">&gt; &gt;&gt;algorithm as &quot;vm/drop_caches&quot;. It is non-destructive and does not</span>
<span class="quote">&gt; &gt;&gt;drop any pages. Therefore it does not have any impact on system</span>
<span class="quote">&gt; &gt;&gt;performance. The computation does not include the size of</span>
<span class="quote">&gt; &gt;&gt;reclaimable slab.</span>
<span class="quote">&gt; &gt;Why, exactly, do you need this? You&#39;ve described what the patch</span>
<span class="quote">&gt; &gt;does (i.e. redundant, because we can read the code), and described</span>
<span class="quote">&gt; &gt;that the kernel already accounts this reclaimable memory elsewhere</span>
<span class="quote">&gt; &gt;and you can already read that and infer the amount of reclaimable</span>
<span class="quote">&gt; &gt;memory from it. So why isn&#39;t that accounting sufficient?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We need it to determine accurately what the free memory in the</span>
<span class="quote">&gt; system is. If you know where we can get this information already</span>
<span class="quote">&gt; please tell, we aren&#39;t aware of it. For instance /proc/meminfo isn&#39;t</span>
<span class="quote">&gt; accurate enough.</span>

What you are proposing isn&#39;t accurate, either, because it will be
stale by the time the inode cache traversal is completed and the
count returned to userspace. e.g. pages that have already been
accounted as droppable can be reclaimed or marked dirty and hence
&quot;unreclaimable&quot;.

IOWs, the best you are going to get is an approximate point-in-time
indication of how much memory is available for immediate reclaim.
We&#39;re never going to get an accurate measure in userspace unless we
accurately account for it in the kernel itself. Which, I think it
has already been pointed out, is prohibitively expensive so isn&#39;t
done.

As for a replacement, looking at what pages you consider &quot;droppable&quot;
is really only file pages that are not under dirty or under
writeback. i.e. from /proc/meminfo:

Active(file):     220128 kB
Inactive(file):    60232 kB
Dirty:                 0 kB
Writeback:             0 kB

i.e. reclaimable file cache = Active + inactive - dirty - writeback.

And while you are there, when you drop slab caches:

SReclaimable:      66632 kB

some amount of that may be freed. No guarantees can be made about
the amount, though.

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=144021">Daniel Walker</a> - Feb. 15, 2016, 11:52 p.m.</div>
<pre class="content">
On 02/15/2016 03:05 PM, Dave Chinner wrote:
<span class="quote">&gt; On Mon, Feb 15, 2016 at 10:19:54AM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt;&gt; On 02/14/2016 01:18 PM, Dave Chinner wrote:</span>
<span class="quote">&gt;&gt;&gt; On Fri, Feb 12, 2016 at 12:14:39PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; From: Khalid Mughal &lt;khalidm@cisco.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Currently there is no way to figure out the droppable pagecache size</span>
<span class="quote">&gt;&gt;&gt; &gt;from the meminfo output. The MemFree size can shrink during normal</span>
<span class="quote">&gt;&gt;&gt;&gt; system operation, when some of the memory pages get cached and is</span>
<span class="quote">&gt;&gt;&gt;&gt; reflected in &quot;Cached&quot; field. Similarly for file operations some of</span>
<span class="quote">&gt;&gt;&gt;&gt; the buffer memory gets cached and it is reflected in &quot;Buffers&quot; field.</span>
<span class="quote">&gt;&gt;&gt;&gt; The kernel automatically reclaims all this cached &amp; buffered memory,</span>
<span class="quote">&gt;&gt;&gt;&gt; when it is needed elsewhere on the system. The only way to manually</span>
<span class="quote">&gt;&gt;&gt;&gt; reclaim this memory is by writing 1 to /proc/sys/vm/drop_caches. But</span>
<span class="quote">&gt;&gt;&gt;&gt; this can have performance impact. Since it discards cached objects,</span>
<span class="quote">&gt;&gt;&gt;&gt; it may cause high CPU &amp; I/O utilization to recreate the dropped</span>
<span class="quote">&gt;&gt;&gt;&gt; objects during heavy system load.</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch computes the droppable pagecache count, using same</span>
<span class="quote">&gt;&gt;&gt;&gt; algorithm as &quot;vm/drop_caches&quot;. It is non-destructive and does not</span>
<span class="quote">&gt;&gt;&gt;&gt; drop any pages. Therefore it does not have any impact on system</span>
<span class="quote">&gt;&gt;&gt;&gt; performance. The computation does not include the size of</span>
<span class="quote">&gt;&gt;&gt;&gt; reclaimable slab.</span>
<span class="quote">&gt;&gt;&gt; Why, exactly, do you need this? You&#39;ve described what the patch</span>
<span class="quote">&gt;&gt;&gt; does (i.e. redundant, because we can read the code), and described</span>
<span class="quote">&gt;&gt;&gt; that the kernel already accounts this reclaimable memory elsewhere</span>
<span class="quote">&gt;&gt;&gt; and you can already read that and infer the amount of reclaimable</span>
<span class="quote">&gt;&gt;&gt; memory from it. So why isn&#39;t that accounting sufficient?</span>
<span class="quote">&gt;&gt; We need it to determine accurately what the free memory in the</span>
<span class="quote">&gt;&gt; system is. If you know where we can get this information already</span>
<span class="quote">&gt;&gt; please tell, we aren&#39;t aware of it. For instance /proc/meminfo isn&#39;t</span>
<span class="quote">&gt;&gt; accurate enough.</span>
<span class="quote">&gt; What you are proposing isn&#39;t accurate, either, because it will be</span>
<span class="quote">&gt; stale by the time the inode cache traversal is completed and the</span>
<span class="quote">&gt; count returned to userspace. e.g. pages that have already been</span>
<span class="quote">&gt; accounted as droppable can be reclaimed or marked dirty and hence</span>
<span class="quote">&gt; &quot;unreclaimable&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; IOWs, the best you are going to get is an approximate point-in-time</span>
<span class="quote">&gt; indication of how much memory is available for immediate reclaim.</span>
<span class="quote">&gt; We&#39;re never going to get an accurate measure in userspace unless we</span>
<span class="quote">&gt; accurately account for it in the kernel itself. Which, I think it</span>
<span class="quote">&gt; has already been pointed out, is prohibitively expensive so isn&#39;t</span>
<span class="quote">&gt; done.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As for a replacement, looking at what pages you consider &quot;droppable&quot;</span>
<span class="quote">&gt; is really only file pages that are not under dirty or under</span>
<span class="quote">&gt; writeback. i.e. from /proc/meminfo:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Active(file):     220128 kB</span>
<span class="quote">&gt; Inactive(file):    60232 kB</span>
<span class="quote">&gt; Dirty:                 0 kB</span>
<span class="quote">&gt; Writeback:             0 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; i.e. reclaimable file cache = Active + inactive - dirty - writeback.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And while you are there, when you drop slab caches:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; SReclaimable:      66632 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; some amount of that may be freed. No guarantees can be made about</span>
<span class="quote">&gt; the amount, though.</span>

I got this response from another engineer here at Cisco (Nag he&#39;s CC&#39;d 
also),

&quot;

Approximate point-in-time indication is an accurate characterization of what we are doing. This is good enough for us. NO matter what we do, we are never going to be able to address the &quot;time of check to time of use” window.  But, this approximation works reasonably well for our use case.

As to his other suggestion of estimating the droppable cache, I have considered it but found it unusable. The problem is the inactive file pages count a whole lot pages more than the droppable pages.

See the value of these, before and [after] dropping reclaimable pages.

Before:

Active(file):     183488 kB
Inactive(file):   180504 kB

After (the drop caches):
Active(file):      89468 kB
Inactive(file):    32016 kB

The dirty and the write back are mostly 0KB under our workload as we are 
mostly dealing with the readonly file pages of binaries 
(programs/libraries)..
&quot;
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=44">Theodore Tso</a> - Feb. 16, 2016, 12:45 a.m.</div>
<pre class="content">
On Mon, Feb 15, 2016 at 03:52:31PM -0800, Daniel Walker wrote:
<span class="quote">&gt; &gt;&gt;We need it to determine accurately what the free memory in the</span>
<span class="quote">&gt; &gt;&gt;system is. If you know where we can get this information already</span>
<span class="quote">&gt; &gt;&gt;please tell, we aren&#39;t aware of it. For instance /proc/meminfo isn&#39;t</span>
<span class="quote">&gt; &gt;&gt;accurate enough.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Approximate point-in-time indication is an accurate characterization</span>
<span class="quote">&gt; of what we are doing. This is good enough for us. NO matter what we</span>
<span class="quote">&gt; do, we are never going to be able to address the &quot;time of check to</span>
<span class="quote">&gt; time of use” window.  But, this approximation works reasonably well</span>
<span class="quote">&gt; for our use case.</span>

Why do you need such accuracy, and what do you consider &quot;good enough&quot;.
Having something which iterates over all of the inodes in the system
is something that really shouldn&#39;t be in a general production kernel
At the very least it should only be accessible by root (so now only a
careless system administrator can DOS attack the system) but the
Dave&#39;s original question still stands.  Why do you need a certain
level of accuracy regarding how much memory is available after
dropping all of the caches?  What problem are you trying to
solve/avoid?

It may be that you are going about things completely the wrong way,
which is why understanding the higher order problem you are trying to
solve might be helpful in finding something which is safer,
architecturally cleaner, and something that could go into the upstream
kernel.

Cheers,

						- Ted
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=155301">Nag Avadhanam (nag)</a> - Feb. 16, 2016, 2:58 a.m.</div>
<pre class="content">
We have a class of platforms that are essentially swap-less embedded
systems that have limited memory resources (2GB and less).

There is a need to implement early alerts (before the OOM killer kicks in)
based on the current memory usage so admins can take appropriate steps (do
not initiate provisioning operations but support existing services,
de-provision certain services, etc. based on the extent of memory usage in
the system) . 

There is also a general need to let end users know the available memory so
they can determine if they can enable new services (helps in planning).

These two depend upon knowing approximate (accurate within few 10s of MB)
memory usage within the system. We want to alert admins before system
exhibits any thrashing behaviors.

We find the source of accounting anomalies to be the page cache
accounting. Anonymous page accounting is fine. Page cache usage on our
system can be attributed to these ­ file system cache, shared memory store
(non-reclaimable) and the in-memory file systems (non-reclaimable). We
know the sizes of the shared memory stores and the in memory file system
sizes.

If we can determine the amount of reclaimable file system cache (+/- few
10s of MB), we can improve the serviceability of these systems.
 
Total - (# of bytes of anon pages + # of bytes of shared memory/tmpfs
pages + # of bytes of non-reclaimable file system cache pages) gives us a
measure of the available memory.


Its the calculation of the # of bytes of non-reclaimable file system cache
pages that has been troubling us. We do not want to count inactive file
pages (of programs/binaries) that were once mapped by any process in the
system as reclaimable because that might lead to thrashing under memory
pressure (we want to alert admins before system starts dropping text
pages).

From our experiments, we determined running a VM scan looking for
droppable pages came close to establishing that number. If there are
cheaper ways of determining this stat, please let us know.

Thanks,
nag 


On 2/15/16, 4:45 PM, &quot;Theodore Ts&#39;o&quot; &lt;tytso@mit.edu&gt; wrote:
<span class="quote">
&gt;On Mon, Feb 15, 2016 at 03:52:31PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt;We need it to determine accurately what the free memory in the</span>
<span class="quote">&gt;&gt; &gt;&gt;system is. If you know where we can get this information already</span>
<span class="quote">&gt;&gt; &gt;&gt;please tell, we aren&#39;t aware of it. For instance /proc/meminfo isn&#39;t</span>
<span class="quote">&gt;&gt; &gt;&gt;accurate enough.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Approximate point-in-time indication is an accurate characterization</span>
<span class="quote">&gt;&gt; of what we are doing. This is good enough for us. NO matter what we</span>
<span class="quote">&gt;&gt; do, we are never going to be able to address the &quot;time of check to</span>
<span class="quote">&gt;&gt; time of use² window.  But, this approximation works reasonably well</span>
<span class="quote">&gt;&gt; for our use case.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Why do you need such accuracy, and what do you consider &quot;good enough&quot;.</span>
<span class="quote">&gt;Having something which iterates over all of the inodes in the system</span>
<span class="quote">&gt;is something that really shouldn&#39;t be in a general production kernel</span>
<span class="quote">&gt;At the very least it should only be accessible by root (so now only a</span>
<span class="quote">&gt;careless system administrator can DOS attack the system) but the</span>
<span class="quote">&gt;Dave&#39;s original question still stands.  Why do you need a certain</span>
<span class="quote">&gt;level of accuracy regarding how much memory is available after</span>
<span class="quote">&gt;dropping all of the caches?  What problem are you trying to</span>
<span class="quote">&gt;solve/avoid?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;It may be that you are going about things completely the wrong way,</span>
<span class="quote">&gt;which is why understanding the higher order problem you are trying to</span>
<span class="quote">&gt;solve might be helpful in finding something which is safer,</span>
<span class="quote">&gt;architecturally cleaner, and something that could go into the upstream</span>
<span class="quote">&gt;kernel.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Cheers,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;						- Ted</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 16, 2016, 5:28 a.m.</div>
<pre class="content">
On Mon, Feb 15, 2016 at 03:52:31PM -0800, Daniel Walker wrote:
<span class="quote">&gt; On 02/15/2016 03:05 PM, Dave Chinner wrote:</span>
<span class="quote">&gt; &gt;What you are proposing isn&#39;t accurate, either, because it will be</span>
<span class="quote">&gt; &gt;stale by the time the inode cache traversal is completed and the</span>
<span class="quote">&gt; &gt;count returned to userspace. e.g. pages that have already been</span>
<span class="quote">&gt; &gt;accounted as droppable can be reclaimed or marked dirty and hence</span>
<span class="quote">&gt; &gt;&quot;unreclaimable&quot;.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;IOWs, the best you are going to get is an approximate point-in-time</span>
<span class="quote">&gt; &gt;indication of how much memory is available for immediate reclaim.</span>
<span class="quote">&gt; &gt;We&#39;re never going to get an accurate measure in userspace unless we</span>
<span class="quote">&gt; &gt;accurately account for it in the kernel itself. Which, I think it</span>
<span class="quote">&gt; &gt;has already been pointed out, is prohibitively expensive so isn&#39;t</span>
<span class="quote">&gt; &gt;done.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;As for a replacement, looking at what pages you consider &quot;droppable&quot;</span>
<span class="quote">&gt; &gt;is really only file pages that are not under dirty or under</span>
<span class="quote">&gt; &gt;writeback. i.e. from /proc/meminfo:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Active(file):     220128 kB</span>
<span class="quote">&gt; &gt;Inactive(file):    60232 kB</span>
<span class="quote">&gt; &gt;Dirty:                 0 kB</span>
<span class="quote">&gt; &gt;Writeback:             0 kB</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;i.e. reclaimable file cache = Active + inactive - dirty - writeback.</span>
.....
<span class="quote">
&gt; Approximate point-in-time indication is an accurate</span>
<span class="quote">&gt; characterization of what we are doing. This is good enough for us.</span>
<span class="quote">&gt; NO matter what we do, we are never going to be able to address the</span>
<span class="quote">&gt; &quot;time of check to time of use” window.  But, this</span>
<span class="quote">&gt; approximation works reasonably well for our use case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As to his other suggestion of estimating the droppable cache, I</span>
<span class="quote">&gt; have considered it but found it unusable. The problem is the</span>
<span class="quote">&gt; inactive file pages count a whole lot pages more than the</span>
<span class="quote">&gt; droppable pages.</span>

inactive file pages are supposed to be exactly that - inactive. i.e.
the have not been referenced recently, and are unlikely to be dirty.
They should be immediately reclaimable.
<span class="quote">
&gt; See the value of these, before and [after] dropping reclaimable</span>
<span class="quote">&gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Before:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Active(file):     183488 kB</span>
<span class="quote">&gt; Inactive(file):   180504 kB</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After (the drop caches):</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Active(file):      89468 kB</span>
<span class="quote">&gt; Inactive(file):    32016 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The dirty and the write back are mostly 0KB under our workload as</span>
<span class="quote">&gt; we are mostly dealing with the readonly file pages of binaries</span>
<span class="quote">&gt; (programs/libraries)..  &quot;</span>

if the pages are read-only, then they are clean. If they are on the
LRUs, then they should be immediately reclaimable.

Let&#39;s go back to your counting criteria of all those file pages:

+static int is_page_droppable(struct page *page)
+{
+       struct address_space *mapping = page_mapping(page);
+
+       if (!mapping)
+               return 0;

invalidated page, should be none.

+       if (PageDirty(page))
+               return 0;

Dirty get ignored, in /proc/meminfo.

+       if (PageWriteback(page))
+               return 0;

Writeback ignored, in /proc/meminfo.

+       if (page_mapped(page))
+               return 0;

Clean page, mapped into userspace get ignored, in /proc/meminfo.

+       if (page-&gt;mapping != mapping)
+               return 0;

Invalidation race, should be none.

+       if (page_has_private(page))
+               return 0;

That&#39;s simply wrong. For XFs inodes, that will skip *every page on
every inode* because it attachs bufferheads to every page, even on
read. ext4 behaviour will depend on mount options and whether the
page has been dirtied or not. IOWs, this turns the number of
reclaimable pages in the inode cache into garbage because it counts
clean, reclaimable pages with attached bufferheads as non-reclaimable.

But let&#39;s ignore that by assuming you have read-only pages without
bufferheads (e.g. ext4, blocksize = pagesize, nobh mode on read-only
pages). That means the only thing that makes a difference to the
count returned is mapped pages, a count of which is also in
/proc/meminfo.

So, to pick a random active server here:

		before		after
Active(file):   12103200 kB	24060 kB
Inactive(file):  5976676 kB	 1380 kB
Mapped:            31308 kB	31308 kB

How much was not reclaimed? Roughly the same number of pages as the
Mapped count, and that&#39;s exactly what we&#39;d expect to see from the
above page walk counting code. Hence a slightly better approximation
of the pages that dropping caches will reclaim is:

reclaimable pages = active + inactive - dirty - writeback - mapped

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 16, 2016, 5:38 a.m.</div>
<pre class="content">
On Tue, Feb 16, 2016 at 02:58:04AM +0000, Nag Avadhanam (nag) wrote:
<span class="quote">&gt; Its the calculation of the # of bytes of non-reclaimable file system cache</span>
<span class="quote">&gt; pages that has been troubling us. We do not want to count inactive file</span>
<span class="quote">&gt; pages (of programs/binaries) that were once mapped by any process in the</span>
<span class="quote">&gt; system as reclaimable because that might lead to thrashing under memory</span>
<span class="quote">&gt; pressure (we want to alert admins before system starts dropping text</span>
<span class="quote">&gt; pages).</span>

The code presented does not match your requirements. It only counts
pages that are currently mapped into ptes. hence it will tell you
that once-used and now unmapped binary pages are reclaimable, and
drop caches will reclaim them. hence they&#39;ll need to be fetched from
disk again if they are faulted in again after a drop_caches run.

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=155301">Nag Avadhanam (nag)</a> - Feb. 16, 2016, 5:57 a.m.</div>
<pre class="content">
On Mon, 15 Feb 2016, Dave Chinner wrote:
<span class="quote">
&gt; On Mon, Feb 15, 2016 at 03:52:31PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt;&gt; On 02/15/2016 03:05 PM, Dave Chinner wrote:</span>
<span class="quote">&gt;&gt;&gt; What you are proposing isn&#39;t accurate, either, because it will be</span>
<span class="quote">&gt;&gt;&gt; stale by the time the inode cache traversal is completed and the</span>
<span class="quote">&gt;&gt;&gt; count returned to userspace. e.g. pages that have already been</span>
<span class="quote">&gt;&gt;&gt; accounted as droppable can be reclaimed or marked dirty and hence</span>
<span class="quote">&gt;&gt;&gt; &quot;unreclaimable&quot;.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; IOWs, the best you are going to get is an approximate point-in-time</span>
<span class="quote">&gt;&gt;&gt; indication of how much memory is available for immediate reclaim.</span>
<span class="quote">&gt;&gt;&gt; We&#39;re never going to get an accurate measure in userspace unless we</span>
<span class="quote">&gt;&gt;&gt; accurately account for it in the kernel itself. Which, I think it</span>
<span class="quote">&gt;&gt;&gt; has already been pointed out, is prohibitively expensive so isn&#39;t</span>
<span class="quote">&gt;&gt;&gt; done.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; As for a replacement, looking at what pages you consider &quot;droppable&quot;</span>
<span class="quote">&gt;&gt;&gt; is really only file pages that are not under dirty or under</span>
<span class="quote">&gt;&gt;&gt; writeback. i.e. from /proc/meminfo:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Active(file):     220128 kB</span>
<span class="quote">&gt;&gt;&gt; Inactive(file):    60232 kB</span>
<span class="quote">&gt;&gt;&gt; Dirty:                 0 kB</span>
<span class="quote">&gt;&gt;&gt; Writeback:             0 kB</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; i.e. reclaimable file cache = Active + inactive - dirty - writeback.</span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Approximate point-in-time indication is an accurate</span>
<span class="quote">&gt;&gt; characterization of what we are doing. This is good enough for us.</span>
<span class="quote">&gt;&gt; NO matter what we do, we are never going to be able to address the</span>
<span class="quote">&gt;&gt; &quot;time of check to time of use” window.  But, this</span>
<span class="quote">&gt;&gt; approximation works reasonably well for our use case.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; As to his other suggestion of estimating the droppable cache, I</span>
<span class="quote">&gt;&gt; have considered it but found it unusable. The problem is the</span>
<span class="quote">&gt;&gt; inactive file pages count a whole lot pages more than the</span>
<span class="quote">&gt;&gt; droppable pages.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; inactive file pages are supposed to be exactly that - inactive. i.e.</span>
<span class="quote">&gt; the have not been referenced recently, and are unlikely to be dirty.</span>
<span class="quote">&gt; They should be immediately reclaimable.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; See the value of these, before and [after] dropping reclaimable</span>
<span class="quote">&gt;&gt; pages.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Before:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Active(file):     183488 kB</span>
<span class="quote">&gt;&gt; Inactive(file):   180504 kB</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; After (the drop caches):</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Active(file):      89468 kB</span>
<span class="quote">&gt;&gt; Inactive(file):    32016 kB</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The dirty and the write back are mostly 0KB under our workload as</span>
<span class="quote">&gt;&gt; we are mostly dealing with the readonly file pages of binaries</span>
<span class="quote">&gt;&gt; (programs/libraries)..  &quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; if the pages are read-only, then they are clean. If they are on the</span>
<span class="quote">&gt; LRUs, then they should be immediately reclaimable.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Let&#39;s go back to your counting criteria of all those file pages:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +static int is_page_droppable(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct address_space *mapping = page_mapping(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!mapping)</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; invalidated page, should be none.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (PageDirty(page))</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Dirty get ignored, in /proc/meminfo.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (PageWriteback(page))</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Writeback ignored, in /proc/meminfo.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (page_mapped(page))</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Clean page, mapped into userspace get ignored, in /proc/meminfo.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (page-&gt;mapping != mapping)</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Invalidation race, should be none.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (page_has_private(page))</span>
<span class="quote">&gt; +               return 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s simply wrong. For XFs inodes, that will skip *every page on</span>
<span class="quote">&gt; every inode* because it attachs bufferheads to every page, even on</span>
<span class="quote">&gt; read. ext4 behaviour will depend on mount options and whether the</span>
<span class="quote">&gt; page has been dirtied or not. IOWs, this turns the number of</span>
<span class="quote">&gt; reclaimable pages in the inode cache into garbage because it counts</span>
<span class="quote">&gt; clean, reclaimable pages with attached bufferheads as non-reclaimable.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But let&#39;s ignore that by assuming you have read-only pages without</span>
<span class="quote">&gt; bufferheads (e.g. ext4, blocksize = pagesize, nobh mode on read-only</span>
<span class="quote">&gt; pages). That means the only thing that makes a difference to the</span>
<span class="quote">&gt; count returned is mapped pages, a count of which is also in</span>
<span class="quote">&gt; /proc/meminfo.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, to pick a random active server here:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 		before		after</span>
<span class="quote">&gt; Active(file):   12103200 kB	24060 kB</span>
<span class="quote">&gt; Inactive(file):  5976676 kB	 1380 kB</span>
<span class="quote">&gt; Mapped:            31308 kB	31308 kB</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; How much was not reclaimed? Roughly the same number of pages as the</span>
<span class="quote">&gt; Mapped count, and that&#39;s exactly what we&#39;d expect to see from the</span>
<span class="quote">&gt; above page walk counting code. Hence a slightly better approximation</span>
<span class="quote">&gt; of the pages that dropping caches will reclaim is:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; reclaimable pages = active + inactive - dirty - writeback - mapped</span>

Thanks Dave. I considered that, but see this.

Mapped page count below is much higher than the 
(active(file) + inactive (file)).

Mapped seems to include all page cache pages mapped into the process memory, 
including the shared memory pages, file pages and few other type
mappings.

I suppose the above can be rewritten as (mapped is still high):

reclaimable pages = active + inactive + shmem - dirty - writeback - mapped

What about kernel pages mapped into user address space? Does &quot;Mapped&quot;
include those pages as well? How do we exclude them? What about device 
mappings? Are these excluded in the &quot;Mapped&quot; pages calculation?

MemTotal:        1025444 kB
MemFree:          264712 kB
Buffers:            1220 kB
Cached:           212736 kB
SwapCached:            0 kB
Active:           398232 kB
Inactive:         240892 kB
Active(anon):     307588 kB
Inactive(anon):   204860 kB
Active(file):      90644 kB
Inactive(file):    36032 kB
Unevictable:       22672 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:                24 kB
Writeback:             0 kB
AnonPages:        447848 kB
Mapped:           202624 kB
Shmem:             64608 kB
Slab:              29632 kB
SReclaimable:      10996 kB
SUnreclaim:        18636 kB
KernelStack:        2528 kB
PageTables:         7936 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:      512720 kB
Committed_AS:     973504 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      140060 kB
VmallocChunk:   34359595388 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       10240 kB
DirectMap2M:     1042432 kB

thanks,
nag
<span class="quote">&gt;</span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Dave.</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; Dave Chinner</span>
<span class="quote">&gt; david@fromorbit.com</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=155301">Nag Avadhanam (nag)</a> - Feb. 16, 2016, 7:14 a.m.</div>
<pre class="content">
On Mon, 15 Feb 2016, Dave Chinner wrote:
<span class="quote">
&gt; On Tue, Feb 16, 2016 at 02:58:04AM +0000, Nag Avadhanam (nag) wrote:</span>
<span class="quote">&gt;&gt; Its the calculation of the # of bytes of non-reclaimable file system cache</span>
<span class="quote">&gt;&gt; pages that has been troubling us. We do not want to count inactive file</span>
<span class="quote">&gt;&gt; pages (of programs/binaries) that were once mapped by any process in the</span>
<span class="quote">&gt;&gt; system as reclaimable because that might lead to thrashing under memory</span>
<span class="quote">&gt;&gt; pressure (we want to alert admins before system starts dropping text</span>
<span class="quote">&gt;&gt; pages).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The code presented does not match your requirements. It only counts</span>
<span class="quote">&gt; pages that are currently mapped into ptes. hence it will tell you</span>
<span class="quote">&gt; that once-used and now unmapped binary pages are reclaimable, and</span>
<span class="quote">&gt; drop caches will reclaim them. hence they&#39;ll need to be fetched from</span>
<span class="quote">&gt; disk again if they are faulted in again after a drop_caches run.</span>

Will the inactive binary pages be automatically unmapped even if the process
into whose address space they are mapped is still around? I thought they
are left mapped until such time there is memory pressure.

We only care for binary pages (active and inactive) mapped into the address 
spaces of live processes. Its okay to aggressively reclaim inactive
pages once mapped into processes that are no longer around.

thanks,
nag
<span class="quote">
&gt;</span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Dave.</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; Dave Chinner</span>
<span class="quote">&gt; david@fromorbit.com</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 16, 2016, 8:22 a.m.</div>
<pre class="content">
On Mon, Feb 15, 2016 at 09:57:42PM -0800, Nag Avadhanam wrote:
<span class="quote">&gt; On Mon, 15 Feb 2016, Dave Chinner wrote:</span>
<span class="quote">&gt; &gt;So, to pick a random active server here:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;		before		after</span>
<span class="quote">&gt; &gt;Active(file):   12103200 kB	24060 kB</span>
<span class="quote">&gt; &gt;Inactive(file):  5976676 kB	 1380 kB</span>
<span class="quote">&gt; &gt;Mapped:            31308 kB	31308 kB</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;How much was not reclaimed? Roughly the same number of pages as the</span>
<span class="quote">&gt; &gt;Mapped count, and that&#39;s exactly what we&#39;d expect to see from the</span>
<span class="quote">&gt; &gt;above page walk counting code. Hence a slightly better approximation</span>
<span class="quote">&gt; &gt;of the pages that dropping caches will reclaim is:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;reclaimable pages = active + inactive - dirty - writeback - mapped</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks Dave. I considered that, but see this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Mapped page count below is much higher than the (active(file) +</span>
<span class="quote">&gt; inactive (file)).</span>

Yes. it&#39;s all unreclaimable from drop caches, though.
<span class="quote">
&gt; Mapped seems to include all page cache pages mapped into the process</span>
<span class="quote">&gt; memory, including the shared memory pages, file pages and few other</span>
<span class="quote">&gt; type</span>
<span class="quote">&gt; mappings.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I suppose the above can be rewritten as (mapped is still high):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; reclaimable pages = active + inactive + shmem - dirty - writeback - mapped</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about kernel pages mapped into user address space? Does &quot;Mapped&quot;</span>
<span class="quote">&gt; include those pages as well? How do we exclude them? What about</span>
<span class="quote">&gt; device mappings? Are these excluded in the &quot;Mapped&quot; pages</span>
<span class="quote">&gt; calculation?</span>

/me shrugs.

I have no idea - I really don&#39;t care about what pages are accounted
as mapped. I assumed that the patch proposed addressed your
requirements and so I suggested an alternative that provided almost
exactly the same information but erred on the side of
underestimation and hence solves your problem of drop_caches not
freeing as much memory as you expected....

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160">Dave Chinner</a> - Feb. 16, 2016, 8:35 a.m.</div>
<pre class="content">
On Mon, Feb 15, 2016 at 11:14:13PM -0800, Nag Avadhanam wrote:
<span class="quote">&gt; On Mon, 15 Feb 2016, Dave Chinner wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;On Tue, Feb 16, 2016 at 02:58:04AM +0000, Nag Avadhanam (nag) wrote:</span>
<span class="quote">&gt; &gt;&gt;Its the calculation of the # of bytes of non-reclaimable file system cache</span>
<span class="quote">&gt; &gt;&gt;pages that has been troubling us. We do not want to count inactive file</span>
<span class="quote">&gt; &gt;&gt;pages (of programs/binaries) that were once mapped by any process in the</span>
<span class="quote">&gt; &gt;&gt;system as reclaimable because that might lead to thrashing under memory</span>
<span class="quote">&gt; &gt;&gt;pressure (we want to alert admins before system starts dropping text</span>
<span class="quote">&gt; &gt;&gt;pages).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;The code presented does not match your requirements. It only counts</span>
<span class="quote">&gt; &gt;pages that are currently mapped into ptes. hence it will tell you</span>
<span class="quote">&gt; &gt;that once-used and now unmapped binary pages are reclaimable, and</span>
<span class="quote">&gt; &gt;drop caches will reclaim them. hence they&#39;ll need to be fetched from</span>
<span class="quote">&gt; &gt;disk again if they are faulted in again after a drop_caches run.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will the inactive binary pages be automatically unmapped even if the process</span>
<span class="quote">&gt; into whose address space they are mapped is still around? I thought they</span>
<span class="quote">&gt; are left mapped until such time there is memory pressure.</span>

Right, page reclaim via memory pressure can unmap mapped pages in
order to reclaim them. Drop caches will skip them.
<span class="quote">
&gt; We only care for binary pages (active and inactive) mapped into the</span>
<span class="quote">&gt; address spaces of live processes. Its okay to aggressively reclaim</span>
<span class="quote">&gt; inactive</span>
<span class="quote">&gt; pages once mapped into processes that are no longer around.</span>

Ok, if you&#39;re only concerned about live processes then drop caches
should behave as you want.

Cheers,

Dave.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143531">Vladimir Davydov</a> - Feb. 16, 2016, 8:43 a.m.</div>
<pre class="content">
On Tue, Feb 16, 2016 at 02:58:04AM +0000, Nag Avadhanam (nag) wrote:
<span class="quote">&gt; We have a class of platforms that are essentially swap-less embedded</span>
<span class="quote">&gt; systems that have limited memory resources (2GB and less).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There is a need to implement early alerts (before the OOM killer kicks in)</span>
<span class="quote">&gt; based on the current memory usage so admins can take appropriate steps (do</span>
<span class="quote">&gt; not initiate provisioning operations but support existing services,</span>
<span class="quote">&gt; de-provision certain services, etc. based on the extent of memory usage in</span>
<span class="quote">&gt; the system) . </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There is also a general need to let end users know the available memory so</span>
<span class="quote">&gt; they can determine if they can enable new services (helps in planning).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These two depend upon knowing approximate (accurate within few 10s of MB)</span>
<span class="quote">&gt; memory usage within the system. We want to alert admins before system</span>
<span class="quote">&gt; exhibits any thrashing behaviors.</span>

Have you considered using /proc/kpageflags for counting such pages? It
should already export all information about memory pages you might need,
e.g. which pages are mapped, which are anonymous, which are inactive,
basically all page flags and even more. Moreover, you can even determine
the set of pages that are really read/written by processes - see
/sys/kernel/mm/page_idle/bitmap. On such a small machine scanning the
whole pfn range should be pretty cheap, so you might find this API
acceptable.

Thanks,
Vladimir
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=181">Rik van Riel</a> - Feb. 16, 2016, 4:12 p.m.</div>
<pre class="content">
On Tue, 2016-02-16 at 16:28 +1100, Dave Chinner wrote:
<span class="quote">&gt; On Mon, Feb 15, 2016 at 03:52:31PM -0800, Daniel Walker wrote:</span>
<span class="quote">&gt; &gt; On 02/15/2016 03:05 PM, Dave Chinner wrote:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; As for a replacement, looking at what pages you consider</span>
<span class="quote">&gt; &gt; &gt; &quot;droppable&quot;</span>
<span class="quote">&gt; &gt; &gt; is really only file pages that are not under dirty or under</span>
<span class="quote">&gt; &gt; &gt; writeback. i.e. from /proc/meminfo:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Active(file):     220128 kB</span>
<span class="quote">&gt; &gt; &gt; Inactive(file):    60232 kB</span>
<span class="quote">&gt; &gt; &gt; Dirty:                 0 kB</span>
<span class="quote">&gt; &gt; &gt; Writeback:             0 kB</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; i.e. reclaimable file cache = Active + inactive - dirty -</span>
<span class="quote">&gt; &gt; &gt; writeback.</span>
<span class="quote">&gt; .....</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; As to his other suggestion of estimating the droppable cache, I</span>
<span class="quote">&gt; &gt; have considered it but found it unusable. The problem is the</span>
<span class="quote">&gt; &gt; inactive file pages count a whole lot pages more than the</span>
<span class="quote">&gt; &gt; droppable pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; inactive file pages are supposed to be exactly that - inactive. i.e.</span>
<span class="quote">&gt; the have not been referenced recently, and are unlikely to be dirty.</span>
<span class="quote">&gt; They should be immediately reclaimable.</span>

Inactive file pages can still be mapped by
processes.

The reason we do not unmap file pages when
moving them to the inactive list is that
some workloads fill essentially all of memory
with mmapped file pages.

Given that the inactive list is generally a
considerable fraction of file memory, unmapping
pages that get deactivated could create a lot
of churn and unnecessary page faults for that
kind of workload.

-- 
All rights reversed
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=155301">Nag Avadhanam (nag)</a> - Feb. 16, 2016, 6:37 p.m.</div>
<pre class="content">
On Tue, 16 Feb 2016, Vladimir Davydov wrote:
<span class="quote">
&gt; On Tue, Feb 16, 2016 at 02:58:04AM +0000, Nag Avadhanam (nag) wrote:</span>
<span class="quote">&gt;&gt; We have a class of platforms that are essentially swap-less embedded</span>
<span class="quote">&gt;&gt; systems that have limited memory resources (2GB and less).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; There is a need to implement early alerts (before the OOM killer kicks in)</span>
<span class="quote">&gt;&gt; based on the current memory usage so admins can take appropriate steps (do</span>
<span class="quote">&gt;&gt; not initiate provisioning operations but support existing services,</span>
<span class="quote">&gt;&gt; de-provision certain services, etc. based on the extent of memory usage in</span>
<span class="quote">&gt;&gt; the system) .</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; There is also a general need to let end users know the available memory so</span>
<span class="quote">&gt;&gt; they can determine if they can enable new services (helps in planning).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; These two depend upon knowing approximate (accurate within few 10s of MB)</span>
<span class="quote">&gt;&gt; memory usage within the system. We want to alert admins before system</span>
<span class="quote">&gt;&gt; exhibits any thrashing behaviors.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Have you considered using /proc/kpageflags for counting such pages? It</span>
<span class="quote">&gt; should already export all information about memory pages you might need,</span>
<span class="quote">&gt; e.g. which pages are mapped, which are anonymous, which are inactive,</span>
<span class="quote">&gt; basically all page flags and even more. Moreover, you can even determine</span>
<span class="quote">&gt; the set of pages that are really read/written by processes - see</span>
<span class="quote">&gt; /sys/kernel/mm/page_idle/bitmap. On such a small machine scanning the</span>
<span class="quote">&gt; whole pfn range should be pretty cheap, so you might find this API</span>
<span class="quote">&gt; acceptable.</span>

Thanks Vladimir. I came across the pagmemap interface sometime ago. I
was not sure if its mainstream. I think this should allow userspace 
VM scan (scans might take a bit longer). Will try it.

We could avoid the scans altogether.

The need plainly put is, inform the admins of these swapless embedded systems 
of the available memory.

If we can reliably and efficiently maintain counts of file pages 
(inactive and active) mapped into the address spaces of active user space 
processes, this need can be met. &quot;Mapped&quot; of /proc/meminfo does not seem 
to be a direct fit for this purpose (I need to understand this better). 
If I know for sure &quot;Mapped&quot; does not count device and the kernel pages 
mapped into the user space, then I can employ it gainfully for this need.

(Cached - Shmem - &lt;mapped file/binary pages of active processes&gt;) gives me
reclaimable file pages. If I can determine that then I can add that to MemFree 
and determine the available memory.

Thanks,
nag
<span class="quote">
&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Vladimir</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="p_header">index 89a887c..13a501c 100644</span>
<span class="p_header">--- a/Documentation/sysctl/vm.txt</span>
<span class="p_header">+++ b/Documentation/sysctl/vm.txt</span>
<span class="p_chunk">@@ -29,6 +29,7 @@</span> <span class="p_context"> Currently, these files are in /proc/sys/vm:</span>
 - dirty_ratio
 - dirty_writeback_centisecs
 - drop_caches
<span class="p_add">+- drop_caches_count</span>
 - extfrag_threshold
 - hugepages_treat_as_movable
 - hugetlb_shm_group
<span class="p_chunk">@@ -224,6 +225,17 @@</span> <span class="p_context"> with your system.  To disable them, echo 4 (bit 3) into drop_caches.</span>
 
 ==============================================================
 
<span class="p_add">+drop_caches_count</span>
<span class="p_add">+</span>
<span class="p_add">+The amount of droppable pagecache (in kilobytes). Reading this file</span>
<span class="p_add">+performs same calculation as writing 1 to /proc/sys/vm/drop_caches.</span>
<span class="p_add">+The actual pages are not dropped during computation of this value.</span>
<span class="p_add">+</span>
<span class="p_add">+To read the value:</span>
<span class="p_add">+	cat /proc/sys/vm/drop_caches_count</span>
<span class="p_add">+</span>
<span class="p_add">+==============================================================</span>
<span class="p_add">+</span>
 extfrag_threshold
 
 This parameter affects whether the kernel will compact memory or direct
<span class="p_header">diff --git a/fs/drop_caches.c b/fs/drop_caches.c</span>
<span class="p_header">index d72d52b..0cb2186 100644</span>
<span class="p_header">--- a/fs/drop_caches.c</span>
<span class="p_header">+++ b/fs/drop_caches.c</span>
<span class="p_chunk">@@ -8,12 +8,73 @@</span> <span class="p_context"></span>
 #include &lt;linux/writeback.h&gt;
 #include &lt;linux/sysctl.h&gt;
 #include &lt;linux/gfp.h&gt;
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagevec.h&gt;</span>
<span class="p_add">+#include &lt;linux/proc_fs.h&gt;</span>
<span class="p_add">+#include &lt;linux/seq_file.h&gt;</span>
<span class="p_add">+#include &lt;linux/vmstat.h&gt;</span>
<span class="p_add">+#include &lt;linux/blkdev.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;internal.h&quot;
 
 /* A global variable is a bit ugly, but it keeps the code simple */
<span class="p_add">+</span>
 int sysctl_drop_caches;
<span class="p_add">+unsigned int sysctl_drop_caches_count;</span>
<span class="p_add">+</span>
<span class="p_add">+static int is_page_droppable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mapping)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (PageDirty(page))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (PageWriteback(page))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (page_mapped(page))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (page-&gt;mapping != mapping)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (page_has_private(page))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long count_unlocked_pages(struct address_space *mapping)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pagevec pvec;</span>
<span class="p_add">+	pgoff_t start = 0;</span>
<span class="p_add">+	pgoff_t end = -1;</span>
<span class="p_add">+	unsigned long count = 0;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	int rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagevec_init(&amp;pvec, 0);</span>
<span class="p_add">+	while (start &lt;= end &amp;&amp; pagevec_lookup(&amp;pvec, mapping, start,</span>
<span class="p_add">+		min(end - start, (pgoff_t)PAGEVEC_SIZE - 1) + 1)) {</span>
<span class="p_add">+		for (i = 0; i &lt; pagevec_count(&amp;pvec); i++) {</span>
<span class="p_add">+			struct page *page = pvec.pages[i];</span>
<span class="p_add">+			start = page-&gt;index;</span>
<span class="p_add">+			if (start &gt; end)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			if (!trylock_page(page))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			WARN_ON(page-&gt;index != start);</span>
<span class="p_add">+			rc = is_page_droppable(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			count += rc;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pagevec_release(&amp;pvec);</span>
<span class="p_add">+		cond_resched();</span>
<span class="p_add">+		start++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return count;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-static void drop_pagecache_sb(struct super_block *sb, void *unused)</span>
<span class="p_add">+static void drop_pagecache_sb(struct super_block *sb, void *count)</span>
 {
 	struct inode *inode, *toput_inode = NULL;
 
<span class="p_chunk">@@ -29,7 +90,11 @@</span> <span class="p_context"> static void drop_pagecache_sb(struct super_block *sb, void *unused)</span>
 		spin_unlock(&amp;inode-&gt;i_lock);
 		spin_unlock(&amp;sb-&gt;s_inode_list_lock);
 
<span class="p_del">-		invalidate_mapping_pages(inode-&gt;i_mapping, 0, -1);</span>
<span class="p_add">+		if (count)</span>
<span class="p_add">+			sysctl_drop_caches_count += count_unlocked_pages(inode-&gt;i_mapping);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			invalidate_mapping_pages(inode-&gt;i_mapping, 0, -1);</span>
<span class="p_add">+</span>
 		iput(toput_inode);
 		toput_inode = inode;
 
<span class="p_chunk">@@ -67,3 +132,14 @@</span> <span class="p_context"> int drop_caches_sysctl_handler(struct ctl_table *table, int write,</span>
 	}
 	return 0;
 }
<span class="p_add">+</span>
<span class="p_add">+int drop_caches_count_sysctl_handler(struct ctl_table *table, int write,</span>
<span class="p_add">+	void __user *buffer, size_t *length, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+	sysctl_drop_caches_count = nr_blockdev_pages();</span>
<span class="p_add">+	iterate_supers(drop_pagecache_sb, &amp;sysctl_drop_caches_count);</span>
<span class="p_add">+	sysctl_drop_caches_count &lt;&lt;= (PAGE_SHIFT - 10); /* count in KBytes */</span>
<span class="p_add">+	ret = proc_dointvec_minmax(table, write, buffer, length, ppos);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index f1cd22f..02ebd41 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2220,8 +2220,11 @@</span> <span class="p_context"> static inline int in_gate_area(struct mm_struct *mm, unsigned long addr)</span>
 
 #ifdef CONFIG_SYSCTL
 extern int sysctl_drop_caches;
<span class="p_add">+extern unsigned int sysctl_drop_caches_count;</span>
 int drop_caches_sysctl_handler(struct ctl_table *, int,
 					void __user *, size_t *, loff_t *);
<span class="p_add">+int drop_caches_count_sysctl_handler(struct ctl_table *, int,</span>
<span class="p_add">+					void __user *, size_t *, loff_t *);</span>
 #endif
 
 void drop_slab(void);
<span class="p_header">diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="p_header">index 97715fd..c043175 100644</span>
<span class="p_header">--- a/kernel/sysctl.c</span>
<span class="p_header">+++ b/kernel/sysctl.c</span>
<span class="p_chunk">@@ -1356,6 +1356,13 @@</span> <span class="p_context"> static struct ctl_table vm_table[] = {</span>
 		.extra1		= &amp;one,
 		.extra2		= &amp;four,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		.procname	= &quot;drop_caches_count&quot;,</span>
<span class="p_add">+		.data		= &amp;sysctl_drop_caches_count,</span>
<span class="p_add">+		.maxlen		= sizeof(unsigned int),</span>
<span class="p_add">+		.mode		= 0444,</span>
<span class="p_add">+		.proc_handler	= drop_caches_count_sysctl_handler,</span>
<span class="p_add">+	},</span>
 #ifdef CONFIG_COMPACTION
 	{
 		.procname	= &quot;compact_memory&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



