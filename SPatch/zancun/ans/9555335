
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V2,2/7] mm: move MADV_FREE pages into LRU_INACTIVE_FILE list - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V2,2/7] mm: move MADV_FREE pages into LRU_INACTIVE_FILE list</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 3, 2017, 11:33 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;3914c9f53c343357c39cb891210da31aa30ad3a9.1486163864.git.shli@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9555335/mbox/"
   >mbox</a>
|
   <a href="/patch/9555335/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9555335/">/patch/9555335/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1A049602B7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Feb 2017 23:34:00 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0A72E200DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Feb 2017 23:34:00 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F0146208C2; Fri,  3 Feb 2017 23:33:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3DC10200DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Feb 2017 23:33:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753287AbdBCXds (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 3 Feb 2017 18:33:48 -0500
Received: from mx0a-00082601.pphosted.com ([67.231.145.42]:52256 &quot;EHLO
	mx0a-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752867AbdBCXdo (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 3 Feb 2017 18:33:44 -0500
Received: from pps.filterd (m0109334.ppops.net [127.0.0.1])
	by mx0a-00082601.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v13NX7l9005194
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 3 Feb 2017 15:33:44 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : in-reply-to : references : mime-version :
	content-type; s=facebook;
	bh=8o1Q6fWkLgpEm1zmP0eXeLMr7tR7fXKZL7vwvtNj088=; 
	b=rcWMo1LA5X4niYLCK/fIRN/6pJRDRkCClSIhyb9EbfA4QKqD04592DqWO+o102YDXNAs
	A47NW3H6vwIU+IbI7dwNf6tra2gEJ4ctrfnuTSMNyuJMig7fLkmdfEEXmFv9sDaAwuSo
	S3aqBSBGpvjHeqfXzGLggU1CjGvec0krINQ= 
Received: from mail.thefacebook.com ([199.201.64.23])
	by mx0a-00082601.pphosted.com with ESMTP id 28d272r8r4-10
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 03 Feb 2017 15:33:44 -0800
Received: from mx-out.facebook.com (192.168.52.123) by
	PRN-CHUB06.TheFacebook.com (192.168.16.16) with Microsoft SMTP Server
	(TLS) id 14.3.294.0; Fri, 3 Feb 2017 15:33:25 -0800
Received: from facebook.com (2401:db00:21:603d:face:0:19:0)     by
	mx-out.facebook.com (10.223.100.97) with ESMTP id
	27ffcc34ea6911e682f624be0593f280-fd9f5a50 for
	&lt;linux-kernel@vger.kernel.org&gt;; Fri, 03 Feb 2017 15:33:24 -0800
Received: by devbig638.prn2.facebook.com (Postfix, from userid 11222)   id
	7A41D42C1803; Fri,  3 Feb 2017 15:33:23 -0800 (PST)
Smtp-Origin-Hostprefix: devbig
From: Shaohua Li &lt;shli@fb.com&gt;
Smtp-Origin-Hostname: devbig638.prn2.facebook.com
To: &lt;linux-kernel@vger.kernel.org&gt;, &lt;linux-mm@kvack.org&gt;
CC: &lt;Kernel-team@fb.com&gt;, &lt;danielmicay@gmail.com&gt;, &lt;mhocko@suse.com&gt;,
	&lt;minchan@kernel.org&gt;, &lt;hughd@google.com&gt;, &lt;hannes@cmpxchg.org&gt;,
	&lt;riel@redhat.com&gt;, &lt;mgorman@techsingularity.net&gt;,
	&lt;akpm@linux-foundation.org&gt;
Smtp-Origin-Cluster: prn2c22
Subject: [PATCH V2 2/7] mm: move MADV_FREE pages into LRU_INACTIVE_FILE list
Date: Fri, 3 Feb 2017 15:33:18 -0800
Message-ID: &lt;3914c9f53c343357c39cb891210da31aa30ad3a9.1486163864.git.shli@fb.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;cover.1486163864.git.shli@fb.com&gt;
References: &lt;cover.1486163864.git.shli@fb.com&gt;
X-FB-Internal: Safe
MIME-Version: 1.0
Content-Type: text/plain
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-02-03_16:, , signatures=0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 3, 2017, 11:33 p.m.</div>
<pre class="content">
Userspace indicates MADV_FREE pages could be freed without pageout, so
it pretty much likes used once file pages. For such pages, we&#39;d like to
reclaim them once there is memory pressure. Also it might be unfair
reclaiming MADV_FREE pages always before used once file pages and we
definitively want to reclaim the pages before other anonymous and file
pages.

To speed up MADV_FREE pages reclaim, we put the pages into
LRU_INACTIVE_FILE list. The rationale is LRU_INACTIVE_FILE list is tiny
nowadays and should be full of used once file pages. Reclaiming
MADV_FREE pages will not have much interfere of anonymous and active
file pages. And the inactive file pages and MADV_FREE pages will be
reclaimed according to their age, so we don&#39;t reclaim too many MADV_FREE
pages too. Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also
means we can reclaim the pages without swap support. This idea is
suggested by Johannes.

We also clear the pages SwapBacked flag to indicate they are MADV_FREE
pages.

Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
<span class="signed-off-by">Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
---
 include/linux/mm_inline.h     |  5 +++++
 include/linux/swap.h          |  2 +-
 include/linux/vm_event_item.h |  2 +-
 mm/huge_memory.c              |  5 ++---
 mm/madvise.c                  |  3 +--
 mm/swap.c                     | 50 ++++++++++++++++++++++++-------------------
 mm/vmstat.c                   |  1 +
 7 files changed, 39 insertions(+), 29 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Feb. 4, 2017, 6:38 a.m.</div>
<pre class="content">
On February 04, 2017 7:33 AM Shaohua Li wrote: 
<span class="quote">&gt; @@ -1404,6 +1401,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	ret = true;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>

&lt;snipped&gt;
<span class="quote">
&gt; -void deactivate_page(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; -		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +void mark_page_lazyfree(struct page *page)</span>
<span class="quote">&gt; + {</span>
<span class="quote">&gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; +		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		get_page(page);</span>
<span class="quote">&gt;  		if (!pagevec_add(pvec, page) || PageCompound(page))</span>
<span class="quote">&gt; -			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; -		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt; +		put_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>

You are not adding it but would you please try to fix or avoid flipping
preempt count with page table lock hold?

thanks
Hillf
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Feb. 9, 2017, 6:33 a.m.</div>
<pre class="content">
On February 04, 2017 2:38 PM Hillf Danton wrote: 
<span class="quote">&gt; </span>
<span class="quote">&gt; On February 04, 2017 7:33 AM Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; @@ -1404,6 +1401,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt; &gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt; &gt;  	ret = true;</span>
<span class="quote">&gt; &gt;  out:</span>
<span class="quote">&gt; &gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &lt;snipped&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; -void deactivate_page(struct page *page)</span>
<span class="quote">&gt; &gt; -{</span>
<span class="quote">&gt; &gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; &gt; -		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; &gt; +void mark_page_lazyfree(struct page *page)</span>
<span class="quote">&gt; &gt; + {</span>
<span class="quote">&gt; &gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; &gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; &gt; +		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  		get_page(page);</span>
<span class="quote">&gt; &gt;  		if (!pagevec_add(pvec, page) || PageCompound(page))</span>
<span class="quote">&gt; &gt; -			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; &gt; -		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; &gt; +			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt; &gt; +		put_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You are not adding it but would you please try to fix or avoid flipping</span>
<span class="quote">&gt; preempt count with page table lock hold?</span>
<span class="quote">&gt; </span>
preempt_en/disable are embedded in spin_lock/unlock, so please
ignore my noise.

thanks
Hillf
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 10, 2017, 6:50 a.m.</div>
<pre class="content">
Hi Shaohua,

On Fri, Feb 03, 2017 at 03:33:18PM -0800, Shaohua Li wrote:
<span class="quote">&gt; Userspace indicates MADV_FREE pages could be freed without pageout, so</span>
<span class="quote">&gt; it pretty much likes used once file pages. For such pages, we&#39;d like to</span>
<span class="quote">&gt; reclaim them once there is memory pressure. Also it might be unfair</span>
<span class="quote">&gt; reclaiming MADV_FREE pages always before used once file pages and we</span>
<span class="quote">&gt; definitively want to reclaim the pages before other anonymous and file</span>
<span class="quote">&gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To speed up MADV_FREE pages reclaim, we put the pages into</span>
<span class="quote">&gt; LRU_INACTIVE_FILE list. The rationale is LRU_INACTIVE_FILE list is tiny</span>
<span class="quote">&gt; nowadays and should be full of used once file pages. Reclaiming</span>
<span class="quote">&gt; MADV_FREE pages will not have much interfere of anonymous and active</span>
<span class="quote">&gt; file pages. And the inactive file pages and MADV_FREE pages will be</span>
<span class="quote">&gt; reclaimed according to their age, so we don&#39;t reclaim too many MADV_FREE</span>
<span class="quote">&gt; pages too. Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also</span>
<span class="quote">&gt; means we can reclaim the pages without swap support. This idea is</span>
<span class="quote">&gt; suggested by Johannes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We also clear the pages SwapBacked flag to indicate they are MADV_FREE</span>
<span class="quote">&gt; pages.</span>

I think this patch should be merged with 3/7. Otherwise, MADV_FREE will
be broken during the bisect.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/mm_inline.h     |  5 +++++</span>
<span class="quote">&gt;  include/linux/swap.h          |  2 +-</span>
<span class="quote">&gt;  include/linux/vm_event_item.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c              |  5 ++---</span>
<span class="quote">&gt;  mm/madvise.c                  |  3 +--</span>
<span class="quote">&gt;  mm/swap.c                     | 50 ++++++++++++++++++++++++-------------------</span>
<span class="quote">&gt;  mm/vmstat.c                   |  1 +</span>
<span class="quote">&gt;  7 files changed, 39 insertions(+), 29 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="quote">&gt; index e030a68..fdded06 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_inline.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_inline.h</span>
<span class="quote">&gt; @@ -22,6 +22,11 @@ static inline int page_is_file_cache(struct page *page)</span>
<span class="quote">&gt;  	return !PageSwapBacked(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool page_is_lazyfree(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return PageAnon(page) &amp;&amp; !PageSwapBacked(page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

trivial:

How about using PageLazyFree for consistency with other PageXXX?
As well, use SetPageLazyFree/ClearPageLazyFree rather than using
raw {Set,Clear}PageSwapBacked.
<span class="quote">
&gt;  static __always_inline void __update_lru_size(struct lruvec *lruvec,</span>
<span class="quote">&gt;  				enum lru_list lru, enum zone_type zid,</span>
<span class="quote">&gt;  				int nr_pages)</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 45e91dd..486494e 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -279,7 +279,7 @@ extern void lru_add_drain_cpu(int cpu);</span>
<span class="quote">&gt;  extern void lru_add_drain_all(void);</span>
<span class="quote">&gt;  extern void rotate_reclaimable_page(struct page *page);</span>
<span class="quote">&gt;  extern void deactivate_file_page(struct page *page);</span>
<span class="quote">&gt; -extern void deactivate_page(struct page *page);</span>
<span class="quote">&gt; +extern void mark_page_lazyfree(struct page *page);</span>

trivial:

How about &quot;deactivate_lazyfree_page&quot;? IMO, it would show intention
clear that move the lazy free page to inactive list.

It&#39;s just matter of preference so I&#39;m not strong against.
<span class="quote">
&gt;  extern void swap_setup(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void add_page_to_unevictable_list(struct page *page);</span>
<span class="quote">&gt; diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; index 6aa1b6c..94e58da 100644</span>
<span class="quote">&gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(PGALLOC),</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(ALLOCSTALL),</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(PGSCAN_SKIP),</span>
<span class="quote">&gt; -		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="quote">&gt; +		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,</span>
<span class="quote">&gt;  		PGFAULT, PGMAJFAULT,</span>
<span class="quote">&gt;  		PGLAZYFREED,</span>
<span class="quote">&gt;  		PGREFILL,</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index ecf569d..ddb9a94 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1391,9 +1391,6 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		ClearPageDirty(page);</span>
<span class="quote">&gt;  	unlock_page(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageActive(page))</span>
<span class="quote">&gt; -		deactivate_page(page);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {</span>
<span class="quote">&gt;  		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,</span>
<span class="quote">&gt;  			tlb-&gt;fullmm);</span>
<span class="quote">&gt; @@ -1404,6 +1401,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	ret = true;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index c867d88..c24549e 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -378,10 +378,9 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt;  			ptent = pte_wrprotect(ptent);</span>
<span class="quote">&gt;  			set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt; -			if (PageActive(page))</span>
<span class="quote">&gt; -				deactivate_page(page);</span>
<span class="quote">&gt;  			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	if (nr_swap) {</span>
<span class="quote">&gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; index c4910f1..69a7e9d 100644</span>
<span class="quote">&gt; --- a/mm/swap.c</span>
<span class="quote">&gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; @@ -46,7 +46,7 @@ int page_cluster;</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);</span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);</span>
<span class="quote">&gt; +static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -268,6 +268,11 @@ static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  		int lru = page_lru_base_type(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		del_page_from_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; +		if (page_is_lazyfree(page)) {</span>
<span class="quote">&gt; +			SetPageSwapBacked(page);</span>
<span class="quote">&gt; +			file = 0;</span>

I don&#39;t see why you set file with 0. Could you explain the rationale?
<span class="quote">
&gt; +			lru = LRU_INACTIVE_ANON;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		SetPageActive(page);</span>
<span class="quote">&gt;  		lru += LRU_ACTIVE;</span>
<span class="quote">&gt;  		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; @@ -561,20 +566,21 @@ static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt; +static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  			    void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; -		int file = page_is_file_cache(page);</span>
<span class="quote">&gt; -		int lru = page_lru_base_type(page);</span>
<span class="quote">&gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; +		bool active = PageActive(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);</span>
<span class="quote">&gt; +		del_page_from_lru_list(page, lruvec, LRU_INACTIVE_ANON + active);</span>
<span class="quote">&gt;  		ClearPageActive(page);</span>
<span class="quote">&gt;  		ClearPageReferenced(page);</span>
<span class="quote">&gt; -		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; +		ClearPageSwapBacked(page);</span>
<span class="quote">&gt; +		add_page_to_lru_list(page, lruvec, LRU_INACTIVE_FILE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		__count_vm_event(PGDEACTIVATE);</span>
<span class="quote">&gt; -		update_page_reclaim_stat(lruvec, file, 0);</span>
<span class="quote">&gt; +		update_page_reclaim_stat(lruvec, 1, 0);</span>
<span class="quote">&gt; +		count_vm_events(PGLAZYFREE, hpage_nr_pages(page));</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -604,9 +610,9 @@ void lru_add_drain_cpu(int cpu)</span>
<span class="quote">&gt;  	if (pagevec_count(pvec))</span>
<span class="quote">&gt;  		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pvec = &amp;per_cpu(lru_deactivate_pvecs, cpu);</span>
<span class="quote">&gt; +	pvec = &amp;per_cpu(lru_lazyfree_pvecs, cpu);</span>
<span class="quote">&gt;  	if (pagevec_count(pvec))</span>
<span class="quote">&gt; -		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; +		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	activate_page_drain(cpu);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -638,22 +644,22 @@ void deactivate_file_page(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; - * deactivate_page - deactivate a page</span>
<span class="quote">&gt; + * mark_page_lazyfree - make an anon page lazyfree</span>
<span class="quote">&gt;   * @page: page to deactivate</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * deactivate_page() moves @page to the inactive list if @page was on the active</span>
<span class="quote">&gt; - * list and was not an unevictable page.  This is done to accelerate the reclaim</span>
<span class="quote">&gt; - * of @page.</span>
<span class="quote">&gt; + * mark_page_lazyfree() moves @page to the inactive file list.</span>
<span class="quote">&gt; + * This is done to accelerate the reclaim of @page.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -void deactivate_page(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; -		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +void mark_page_lazyfree(struct page *page)</span>
<span class="quote">&gt; + {</span>
<span class="quote">&gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; +		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		get_page(page);</span>
<span class="quote">&gt;  		if (!pagevec_add(pvec, page) || PageCompound(page))</span>
<span class="quote">&gt; -			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; -		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt; +		put_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -704,7 +710,7 @@ void lru_add_drain_all(void)</span>
<span class="quote">&gt;  		if (pagevec_count(&amp;per_cpu(lru_add_pvec, cpu)) ||</span>
<span class="quote">&gt;  		    pagevec_count(&amp;per_cpu(lru_rotate_pvecs, cpu)) ||</span>
<span class="quote">&gt;  		    pagevec_count(&amp;per_cpu(lru_deactivate_file_pvecs, cpu)) ||</span>
<span class="quote">&gt; -		    pagevec_count(&amp;per_cpu(lru_deactivate_pvecs, cpu)) ||</span>
<span class="quote">&gt; +		    pagevec_count(&amp;per_cpu(lru_lazyfree_pvecs, cpu)) ||</span>
<span class="quote">&gt;  		    need_activate_page_drain(cpu)) {</span>
<span class="quote">&gt;  			INIT_WORK(work, lru_add_drain_per_cpu);</span>
<span class="quote">&gt;  			queue_work_on(cpu, lru_add_drain_wq, work);</span>
<span class="quote">&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 69f9aff..7774196 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -992,6 +992,7 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt;  	&quot;pgfree&quot;,</span>
<span class="quote">&gt;  	&quot;pgactivate&quot;,</span>
<span class="quote">&gt;  	&quot;pgdeactivate&quot;,</span>
<span class="quote">&gt; +	&quot;pglazyfree&quot;,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	&quot;pgfault&quot;,</span>
<span class="quote">&gt;  	&quot;pgmajfault&quot;,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.9.3</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Feb. 10, 2017, 1:02 p.m.</div>
<pre class="content">
On Fri 03-02-17 15:33:18, Shaohua Li wrote:
<span class="quote">&gt; Userspace indicates MADV_FREE pages could be freed without pageout, so</span>
<span class="quote">&gt; it pretty much likes used once file pages. For such pages, we&#39;d like to</span>
<span class="quote">&gt; reclaim them once there is memory pressure. Also it might be unfair</span>
<span class="quote">&gt; reclaiming MADV_FREE pages always before used once file pages and we</span>
<span class="quote">&gt; definitively want to reclaim the pages before other anonymous and file</span>
<span class="quote">&gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To speed up MADV_FREE pages reclaim, we put the pages into</span>
<span class="quote">&gt; LRU_INACTIVE_FILE list. The rationale is LRU_INACTIVE_FILE list is tiny</span>
<span class="quote">&gt; nowadays and should be full of used once file pages. Reclaiming</span>
<span class="quote">&gt; MADV_FREE pages will not have much interfere of anonymous and active</span>
<span class="quote">&gt; file pages. And the inactive file pages and MADV_FREE pages will be</span>
<span class="quote">&gt; reclaimed according to their age, so we don&#39;t reclaim too many MADV_FREE</span>
<span class="quote">&gt; pages too. Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also</span>
<span class="quote">&gt; means we can reclaim the pages without swap support. This idea is</span>
<span class="quote">&gt; suggested by Johannes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We also clear the pages SwapBacked flag to indicate they are MADV_FREE</span>
<span class="quote">&gt; pages.</span>

I like this. I have expected this to be more convoluted but it looks
quite straightforward. I didn&#39;t get to do a really deep review and add
my acked-by but from a quick look there do not seem to be any surprises.
I was worried about vmstat accounting. There are some places which
isolate page from LRU and account based on the LRU and later use
page_is_file_cache to tell which LRU this was. This should work fine,
though, because you never touch pages which are off-lru.

That being said I do not see any major issues. There might be some minor
things and this will need a lot of testing but it is definitely a move
into right direction. I hope to do the deeper review after I get back
from vacation (20th Feb).
<span class="quote">
&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>

I guess
Suggested-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;

would be appropriate.
<span class="quote">
&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/mm_inline.h     |  5 +++++</span>
<span class="quote">&gt;  include/linux/swap.h          |  2 +-</span>
<span class="quote">&gt;  include/linux/vm_event_item.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c              |  5 ++---</span>
<span class="quote">&gt;  mm/madvise.c                  |  3 +--</span>
<span class="quote">&gt;  mm/swap.c                     | 50 ++++++++++++++++++++++++-------------------</span>
<span class="quote">&gt;  mm/vmstat.c                   |  1 +</span>
<span class="quote">&gt;  7 files changed, 39 insertions(+), 29 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="quote">&gt; index e030a68..fdded06 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_inline.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_inline.h</span>
<span class="quote">&gt; @@ -22,6 +22,11 @@ static inline int page_is_file_cache(struct page *page)</span>
<span class="quote">&gt;  	return !PageSwapBacked(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool page_is_lazyfree(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return PageAnon(page) &amp;&amp; !PageSwapBacked(page);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static __always_inline void __update_lru_size(struct lruvec *lruvec,</span>
<span class="quote">&gt;  				enum lru_list lru, enum zone_type zid,</span>
<span class="quote">&gt;  				int nr_pages)</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 45e91dd..486494e 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -279,7 +279,7 @@ extern void lru_add_drain_cpu(int cpu);</span>
<span class="quote">&gt;  extern void lru_add_drain_all(void);</span>
<span class="quote">&gt;  extern void rotate_reclaimable_page(struct page *page);</span>
<span class="quote">&gt;  extern void deactivate_file_page(struct page *page);</span>
<span class="quote">&gt; -extern void deactivate_page(struct page *page);</span>
<span class="quote">&gt; +extern void mark_page_lazyfree(struct page *page);</span>
<span class="quote">&gt;  extern void swap_setup(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern void add_page_to_unevictable_list(struct page *page);</span>
<span class="quote">&gt; diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; index 6aa1b6c..94e58da 100644</span>
<span class="quote">&gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(PGALLOC),</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(ALLOCSTALL),</span>
<span class="quote">&gt;  		FOR_ALL_ZONES(PGSCAN_SKIP),</span>
<span class="quote">&gt; -		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="quote">&gt; +		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,</span>
<span class="quote">&gt;  		PGFAULT, PGMAJFAULT,</span>
<span class="quote">&gt;  		PGLAZYFREED,</span>
<span class="quote">&gt;  		PGREFILL,</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index ecf569d..ddb9a94 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1391,9 +1391,6 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		ClearPageDirty(page);</span>
<span class="quote">&gt;  	unlock_page(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageActive(page))</span>
<span class="quote">&gt; -		deactivate_page(page);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {</span>
<span class="quote">&gt;  		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,</span>
<span class="quote">&gt;  			tlb-&gt;fullmm);</span>
<span class="quote">&gt; @@ -1404,6 +1401,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	ret = true;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index c867d88..c24549e 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -378,10 +378,9 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt;  			ptent = pte_wrprotect(ptent);</span>
<span class="quote">&gt;  			set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt; -			if (PageActive(page))</span>
<span class="quote">&gt; -				deactivate_page(page);</span>
<span class="quote">&gt;  			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	if (nr_swap) {</span>
<span class="quote">&gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; index c4910f1..69a7e9d 100644</span>
<span class="quote">&gt; --- a/mm/swap.c</span>
<span class="quote">&gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; @@ -46,7 +46,7 @@ int page_cluster;</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);</span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);</span>
<span class="quote">&gt; +static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -268,6 +268,11 @@ static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  		int lru = page_lru_base_type(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		del_page_from_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; +		if (page_is_lazyfree(page)) {</span>
<span class="quote">&gt; +			SetPageSwapBacked(page);</span>
<span class="quote">&gt; +			file = 0;</span>
<span class="quote">&gt; +			lru = LRU_INACTIVE_ANON;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		SetPageActive(page);</span>
<span class="quote">&gt;  		lru += LRU_ACTIVE;</span>
<span class="quote">&gt;  		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; @@ -561,20 +566,21 @@ static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt; +static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt;  			    void *arg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; -		int file = page_is_file_cache(page);</span>
<span class="quote">&gt; -		int lru = page_lru_base_type(page);</span>
<span class="quote">&gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; +		bool active = PageActive(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);</span>
<span class="quote">&gt; +		del_page_from_lru_list(page, lruvec, LRU_INACTIVE_ANON + active);</span>
<span class="quote">&gt;  		ClearPageActive(page);</span>
<span class="quote">&gt;  		ClearPageReferenced(page);</span>
<span class="quote">&gt; -		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; +		ClearPageSwapBacked(page);</span>
<span class="quote">&gt; +		add_page_to_lru_list(page, lruvec, LRU_INACTIVE_FILE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		__count_vm_event(PGDEACTIVATE);</span>
<span class="quote">&gt; -		update_page_reclaim_stat(lruvec, file, 0);</span>
<span class="quote">&gt; +		update_page_reclaim_stat(lruvec, 1, 0);</span>
<span class="quote">&gt; +		count_vm_events(PGLAZYFREE, hpage_nr_pages(page));</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -604,9 +610,9 @@ void lru_add_drain_cpu(int cpu)</span>
<span class="quote">&gt;  	if (pagevec_count(pvec))</span>
<span class="quote">&gt;  		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pvec = &amp;per_cpu(lru_deactivate_pvecs, cpu);</span>
<span class="quote">&gt; +	pvec = &amp;per_cpu(lru_lazyfree_pvecs, cpu);</span>
<span class="quote">&gt;  	if (pagevec_count(pvec))</span>
<span class="quote">&gt; -		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; +		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	activate_page_drain(cpu);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -638,22 +644,22 @@ void deactivate_file_page(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; - * deactivate_page - deactivate a page</span>
<span class="quote">&gt; + * mark_page_lazyfree - make an anon page lazyfree</span>
<span class="quote">&gt;   * @page: page to deactivate</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * deactivate_page() moves @page to the inactive list if @page was on the active</span>
<span class="quote">&gt; - * list and was not an unevictable page.  This is done to accelerate the reclaim</span>
<span class="quote">&gt; - * of @page.</span>
<span class="quote">&gt; + * mark_page_lazyfree() moves @page to the inactive file list.</span>
<span class="quote">&gt; + * This is done to accelerate the reclaim of @page.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -void deactivate_page(struct page *page)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="quote">&gt; -		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +void mark_page_lazyfree(struct page *page)</span>
<span class="quote">&gt; + {</span>
<span class="quote">&gt; +	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +	    !PageUnevictable(page)) {</span>
<span class="quote">&gt; +		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		get_page(page);</span>
<span class="quote">&gt;  		if (!pagevec_add(pvec, page) || PageCompound(page))</span>
<span class="quote">&gt; -			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="quote">&gt; -		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="quote">&gt; +			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="quote">&gt; +		put_cpu_var(lru_lazyfree_pvecs);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -704,7 +710,7 @@ void lru_add_drain_all(void)</span>
<span class="quote">&gt;  		if (pagevec_count(&amp;per_cpu(lru_add_pvec, cpu)) ||</span>
<span class="quote">&gt;  		    pagevec_count(&amp;per_cpu(lru_rotate_pvecs, cpu)) ||</span>
<span class="quote">&gt;  		    pagevec_count(&amp;per_cpu(lru_deactivate_file_pvecs, cpu)) ||</span>
<span class="quote">&gt; -		    pagevec_count(&amp;per_cpu(lru_deactivate_pvecs, cpu)) ||</span>
<span class="quote">&gt; +		    pagevec_count(&amp;per_cpu(lru_lazyfree_pvecs, cpu)) ||</span>
<span class="quote">&gt;  		    need_activate_page_drain(cpu)) {</span>
<span class="quote">&gt;  			INIT_WORK(work, lru_add_drain_per_cpu);</span>
<span class="quote">&gt;  			queue_work_on(cpu, lru_add_drain_wq, work);</span>
<span class="quote">&gt; diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="quote">&gt; index 69f9aff..7774196 100644</span>
<span class="quote">&gt; --- a/mm/vmstat.c</span>
<span class="quote">&gt; +++ b/mm/vmstat.c</span>
<span class="quote">&gt; @@ -992,6 +992,7 @@ const char * const vmstat_text[] = {</span>
<span class="quote">&gt;  	&quot;pgfree&quot;,</span>
<span class="quote">&gt;  	&quot;pgactivate&quot;,</span>
<span class="quote">&gt;  	&quot;pgdeactivate&quot;,</span>
<span class="quote">&gt; +	&quot;pglazyfree&quot;,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	&quot;pgfault&quot;,</span>
<span class="quote">&gt;  	&quot;pgmajfault&quot;,</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.9.3</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 10, 2017, 5:30 p.m.</div>
<pre class="content">
On Fri, Feb 10, 2017 at 03:50:22PM +0900, Minchan Kim wrote:
<span class="quote">&gt; Hi Shaohua,</span>

Thanks for your time!
<span class="quote"> 
&gt; On Fri, Feb 03, 2017 at 03:33:18PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; Userspace indicates MADV_FREE pages could be freed without pageout, so</span>
<span class="quote">&gt; &gt; it pretty much likes used once file pages. For such pages, we&#39;d like to</span>
<span class="quote">&gt; &gt; reclaim them once there is memory pressure. Also it might be unfair</span>
<span class="quote">&gt; &gt; reclaiming MADV_FREE pages always before used once file pages and we</span>
<span class="quote">&gt; &gt; definitively want to reclaim the pages before other anonymous and file</span>
<span class="quote">&gt; &gt; pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To speed up MADV_FREE pages reclaim, we put the pages into</span>
<span class="quote">&gt; &gt; LRU_INACTIVE_FILE list. The rationale is LRU_INACTIVE_FILE list is tiny</span>
<span class="quote">&gt; &gt; nowadays and should be full of used once file pages. Reclaiming</span>
<span class="quote">&gt; &gt; MADV_FREE pages will not have much interfere of anonymous and active</span>
<span class="quote">&gt; &gt; file pages. And the inactive file pages and MADV_FREE pages will be</span>
<span class="quote">&gt; &gt; reclaimed according to their age, so we don&#39;t reclaim too many MADV_FREE</span>
<span class="quote">&gt; &gt; pages too. Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also</span>
<span class="quote">&gt; &gt; means we can reclaim the pages without swap support. This idea is</span>
<span class="quote">&gt; &gt; suggested by Johannes.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We also clear the pages SwapBacked flag to indicate they are MADV_FREE</span>
<span class="quote">&gt; &gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this patch should be merged with 3/7. Otherwise, MADV_FREE will</span>
<span class="quote">&gt; be broken during the bisect.</span>

Maybe I should move the patch 3 ahead, then we won&#39;t break bisect and still
make the patches clear.
<span class="quote">
&gt; &gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/mm_inline.h     |  5 +++++</span>
<span class="quote">&gt; &gt;  include/linux/swap.h          |  2 +-</span>
<span class="quote">&gt; &gt;  include/linux/vm_event_item.h |  2 +-</span>
<span class="quote">&gt; &gt;  mm/huge_memory.c              |  5 ++---</span>
<span class="quote">&gt; &gt;  mm/madvise.c                  |  3 +--</span>
<span class="quote">&gt; &gt;  mm/swap.c                     | 50 ++++++++++++++++++++++++-------------------</span>
<span class="quote">&gt; &gt;  mm/vmstat.c                   |  1 +</span>
<span class="quote">&gt; &gt;  7 files changed, 39 insertions(+), 29 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="quote">&gt; &gt; index e030a68..fdded06 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/mm_inline.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/mm_inline.h</span>
<span class="quote">&gt; &gt; @@ -22,6 +22,11 @@ static inline int page_is_file_cache(struct page *page)</span>
<span class="quote">&gt; &gt;  	return !PageSwapBacked(page);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static inline bool page_is_lazyfree(struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return PageAnon(page) &amp;&amp; !PageSwapBacked(page);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; trivial:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How about using PageLazyFree for consistency with other PageXXX?</span>
<span class="quote">&gt; As well, use SetPageLazyFree/ClearPageLazyFree rather than using</span>
<span class="quote">&gt; raw {Set,Clear}PageSwapBacked.</span>

So SetPageLazyFree == ClearPageSwapBacked, that would be weird. I personally
prefer directly using {Set, Clear}PageSwapBacked, because reader can
immediately know what&#39;s happening. If using the PageLazyFree, people always
need to refer the code and check the relationship between PageLazyFree and
PageSwapBacked.
<span class="quote"> 
&gt; &gt;  static __always_inline void __update_lru_size(struct lruvec *lruvec,</span>
<span class="quote">&gt; &gt;  				enum lru_list lru, enum zone_type zid,</span>
<span class="quote">&gt; &gt;  				int nr_pages)</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; index 45e91dd..486494e 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; @@ -279,7 +279,7 @@ extern void lru_add_drain_cpu(int cpu);</span>
<span class="quote">&gt; &gt;  extern void lru_add_drain_all(void);</span>
<span class="quote">&gt; &gt;  extern void rotate_reclaimable_page(struct page *page);</span>
<span class="quote">&gt; &gt;  extern void deactivate_file_page(struct page *page);</span>
<span class="quote">&gt; &gt; -extern void deactivate_page(struct page *page);</span>
<span class="quote">&gt; &gt; +extern void mark_page_lazyfree(struct page *page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; trivial:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How about &quot;deactivate_lazyfree_page&quot;? IMO, it would show intention</span>
<span class="quote">&gt; clear that move the lazy free page to inactive list.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s just matter of preference so I&#39;m not strong against.</span>

Yes, I thought about the name a little bit. Don&#39;t think we should use
deactivate, because it sounds that only works for active page, while the
function works for both active/inactive pages. I&#39;m open to any suggestions.
<span class="quote">
&gt; &gt;  extern void swap_setup(void);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  extern void add_page_to_unevictable_list(struct page *page);</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; index 6aa1b6c..94e58da 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/vm_event_item.h</span>
<span class="quote">&gt; &gt; @@ -25,7 +25,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
<span class="quote">&gt; &gt;  		FOR_ALL_ZONES(PGALLOC),</span>
<span class="quote">&gt; &gt;  		FOR_ALL_ZONES(ALLOCSTALL),</span>
<span class="quote">&gt; &gt;  		FOR_ALL_ZONES(PGSCAN_SKIP),</span>
<span class="quote">&gt; &gt; -		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="quote">&gt; &gt; +		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,</span>
<span class="quote">&gt; &gt;  		PGFAULT, PGMAJFAULT,</span>
<span class="quote">&gt; &gt;  		PGLAZYFREED,</span>
<span class="quote">&gt; &gt;  		PGREFILL,</span>
<span class="quote">&gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; index ecf569d..ddb9a94 100644</span>
<span class="quote">&gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; @@ -1391,9 +1391,6 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		ClearPageDirty(page);</span>
<span class="quote">&gt; &gt;  	unlock_page(page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	if (PageActive(page))</span>
<span class="quote">&gt; &gt; -		deactivate_page(page);</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt;  	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {</span>
<span class="quote">&gt; &gt;  		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,</span>
<span class="quote">&gt; &gt;  			tlb-&gt;fullmm);</span>
<span class="quote">&gt; &gt; @@ -1404,6 +1401,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt; &gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt; &gt;  	ret = true;</span>
<span class="quote">&gt; &gt;  out:</span>
<span class="quote">&gt; &gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; &gt; index c867d88..c24549e 100644</span>
<span class="quote">&gt; &gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; &gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; &gt; @@ -378,10 +378,9 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt;  			ptent = pte_mkclean(ptent);</span>
<span class="quote">&gt; &gt;  			ptent = pte_wrprotect(ptent);</span>
<span class="quote">&gt; &gt;  			set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt; &gt; -			if (PageActive(page))</span>
<span class="quote">&gt; &gt; -				deactivate_page(page);</span>
<span class="quote">&gt; &gt;  			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt; +		mark_page_lazyfree(page);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  out:</span>
<span class="quote">&gt; &gt;  	if (nr_swap) {</span>
<span class="quote">&gt; &gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; &gt; index c4910f1..69a7e9d 100644</span>
<span class="quote">&gt; &gt; --- a/mm/swap.c</span>
<span class="quote">&gt; &gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; &gt; @@ -46,7 +46,7 @@ int page_cluster;</span>
<span class="quote">&gt; &gt;  static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);</span>
<span class="quote">&gt; &gt;  static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);</span>
<span class="quote">&gt; &gt;  static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);</span>
<span class="quote">&gt; &gt; -static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);</span>
<span class="quote">&gt; &gt; +static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt; &gt;  static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; @@ -268,6 +268,11 @@ static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
<span class="quote">&gt; &gt;  		int lru = page_lru_base_type(page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		del_page_from_lru_list(page, lruvec, lru);</span>
<span class="quote">&gt; &gt; +		if (page_is_lazyfree(page)) {</span>
<span class="quote">&gt; &gt; +			SetPageSwapBacked(page);</span>
<span class="quote">&gt; &gt; +			file = 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t see why you set file with 0. Could you explain the rationale?</span>

We are moving the page back to active anonymous list, so I&#39;d like to charge the
recent_scanned and recent_rotated to anonymous.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 10, 2017, 5:33 p.m.</div>
<pre class="content">
On Fri, Feb 10, 2017 at 02:02:36PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Fri 03-02-17 15:33:18, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; Userspace indicates MADV_FREE pages could be freed without pageout, so</span>
<span class="quote">&gt; &gt; it pretty much likes used once file pages. For such pages, we&#39;d like to</span>
<span class="quote">&gt; &gt; reclaim them once there is memory pressure. Also it might be unfair</span>
<span class="quote">&gt; &gt; reclaiming MADV_FREE pages always before used once file pages and we</span>
<span class="quote">&gt; &gt; definitively want to reclaim the pages before other anonymous and file</span>
<span class="quote">&gt; &gt; pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To speed up MADV_FREE pages reclaim, we put the pages into</span>
<span class="quote">&gt; &gt; LRU_INACTIVE_FILE list. The rationale is LRU_INACTIVE_FILE list is tiny</span>
<span class="quote">&gt; &gt; nowadays and should be full of used once file pages. Reclaiming</span>
<span class="quote">&gt; &gt; MADV_FREE pages will not have much interfere of anonymous and active</span>
<span class="quote">&gt; &gt; file pages. And the inactive file pages and MADV_FREE pages will be</span>
<span class="quote">&gt; &gt; reclaimed according to their age, so we don&#39;t reclaim too many MADV_FREE</span>
<span class="quote">&gt; &gt; pages too. Putting the MADV_FREE pages into LRU_INACTIVE_FILE_LIST also</span>
<span class="quote">&gt; &gt; means we can reclaim the pages without swap support. This idea is</span>
<span class="quote">&gt; &gt; suggested by Johannes.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We also clear the pages SwapBacked flag to indicate they are MADV_FREE</span>
<span class="quote">&gt; &gt; pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I like this. I have expected this to be more convoluted but it looks</span>
<span class="quote">&gt; quite straightforward. I didn&#39;t get to do a really deep review and add</span>
<span class="quote">&gt; my acked-by but from a quick look there do not seem to be any surprises.</span>
<span class="quote">&gt; I was worried about vmstat accounting. There are some places which</span>
<span class="quote">&gt; isolate page from LRU and account based on the LRU and later use</span>
<span class="quote">&gt; page_is_file_cache to tell which LRU this was. This should work fine,</span>
<span class="quote">&gt; though, because you never touch pages which are off-lru.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That being said I do not see any major issues. There might be some minor</span>
<span class="quote">&gt; things and this will need a lot of testing but it is definitely a move</span>
<span class="quote">&gt; into right direction. I hope to do the deeper review after I get back</span>
<span class="quote">&gt; from vacation (20th Feb).</span>

Sweat! Thanks for your time! 
<span class="quote">&gt; &gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess</span>
<span class="quote">&gt; Suggested-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; would be appropriate.</span>

Sure, will add in next post and will add &#39;the patches are based on Minchan&#39;s
patches&#39; too.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 13, 2017, 4:57 a.m.</div>
<pre class="content">
Hi Shaohua,

On Fri, Feb 10, 2017 at 09:30:09AM -0800, Shaohua Li wrote:

&lt; snip &gt;
<span class="quote">
&gt; &gt; &gt; +static inline bool page_is_lazyfree(struct page *page)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	return PageAnon(page) &amp;&amp; !PageSwapBacked(page);</span>
<span class="quote">&gt; &gt; &gt; +}</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; trivial:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; How about using PageLazyFree for consistency with other PageXXX?</span>
<span class="quote">&gt; &gt; As well, use SetPageLazyFree/ClearPageLazyFree rather than using</span>
<span class="quote">&gt; &gt; raw {Set,Clear}PageSwapBacked.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So SetPageLazyFree == ClearPageSwapBacked, that would be weird. I personally</span>
<span class="quote">&gt; prefer directly using {Set, Clear}PageSwapBacked, because reader can</span>
<span class="quote">&gt; immediately know what&#39;s happening. If using the PageLazyFree, people always</span>
<span class="quote">&gt; need to refer the code and check the relationship between PageLazyFree and</span>
<span class="quote">&gt; PageSwapBacked.</span>

I was not against so I was about to sending &quot;No problem&quot; now but I found your
patch 5 which accounts lazyfreeable pages in zone/node stat and handle them
in lru list management. Hmm, I think now we don&#39;t handle lazyfree pages with
separate LRU list so it&#39;s awkward to me although it may work. So, my idea is
we can handle it through wrapper regardless of LRU management.

For instance,

void SetLazyFreePage(struct page *page)
{
	if (!TestSetPageSwapBacked(page))
		inc_zone_page_state(page, NR_ZONE_LAZYFREE);
}


void ClearLazyFreePage(struct page *page)
{
	if (TestClearPageSwapBacked(page))
		dec_zone_page_state(page, NR_ZONE_LAZYFREE);
}

madvise_free_pte_range:
	SetLageFreePage(page);

activate_page,shrink_page_list:
	ClearLazyFreePage(page);

free_pages_prepare:
	if (PageMappingFlags(page)) {
		if (PageLazyFreePage(page))
			dec_zone_page_state(page, NR_ZONE_LAZYFREE);
		page-&gt;mapping = NULL;
	}

Surely, it&#39;s orthgonal issue regardless of using wrapper but it might
nudge you to use wrapper.
<span class="quote">
&gt;  </span>
<span class="quote">&gt; &gt; &gt;  static __always_inline void __update_lru_size(struct lruvec *lruvec,</span>
<span class="quote">&gt; &gt; &gt;  				enum lru_list lru, enum zone_type zid,</span>
<span class="quote">&gt; &gt; &gt;  				int nr_pages)</span>
<span class="quote">&gt; &gt; &gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; &gt; index 45e91dd..486494e 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; &gt; @@ -279,7 +279,7 @@ extern void lru_add_drain_cpu(int cpu);</span>
<span class="quote">&gt; &gt; &gt;  extern void lru_add_drain_all(void);</span>
<span class="quote">&gt; &gt; &gt;  extern void rotate_reclaimable_page(struct page *page);</span>
<span class="quote">&gt; &gt; &gt;  extern void deactivate_file_page(struct page *page);</span>
<span class="quote">&gt; &gt; &gt; -extern void deactivate_page(struct page *page);</span>
<span class="quote">&gt; &gt; &gt; +extern void mark_page_lazyfree(struct page *page);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; trivial:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; How about &quot;deactivate_lazyfree_page&quot;? IMO, it would show intention</span>
<span class="quote">&gt; &gt; clear that move the lazy free page to inactive list.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s just matter of preference so I&#39;m not strong against.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, I thought about the name a little bit. Don&#39;t think we should use</span>
<span class="quote">&gt; deactivate, because it sounds that only works for active page, while the</span>
<span class="quote">&gt; function works for both active/inactive pages. I&#39;m open to any suggestions.</span>

Indeed.

I don&#39;t have better idea, either so my last suggestion is &quot;demote_lazyfree_page&quot;.
It seems there are several papers/wikipedia to use *demote* in LRU managment.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="p_header">index e030a68..fdded06 100644</span>
<span class="p_header">--- a/include/linux/mm_inline.h</span>
<span class="p_header">+++ b/include/linux/mm_inline.h</span>
<span class="p_chunk">@@ -22,6 +22,11 @@</span> <span class="p_context"> static inline int page_is_file_cache(struct page *page)</span>
 	return !PageSwapBacked(page);
 }
 
<span class="p_add">+static inline bool page_is_lazyfree(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return PageAnon(page) &amp;&amp; !PageSwapBacked(page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static __always_inline void __update_lru_size(struct lruvec *lruvec,
 				enum lru_list lru, enum zone_type zid,
 				int nr_pages)
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 45e91dd..486494e 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -279,7 +279,7 @@</span> <span class="p_context"> extern void lru_add_drain_cpu(int cpu);</span>
 extern void lru_add_drain_all(void);
 extern void rotate_reclaimable_page(struct page *page);
 extern void deactivate_file_page(struct page *page);
<span class="p_del">-extern void deactivate_page(struct page *page);</span>
<span class="p_add">+extern void mark_page_lazyfree(struct page *page);</span>
 extern void swap_setup(void);
 
 extern void add_page_to_unevictable_list(struct page *page);
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 6aa1b6c..94e58da 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
 		FOR_ALL_ZONES(PGALLOC),
 		FOR_ALL_ZONES(ALLOCSTALL),
 		FOR_ALL_ZONES(PGSCAN_SKIP),
<span class="p_del">-		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="p_add">+		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLAZYFREE,</span>
 		PGFAULT, PGMAJFAULT,
 		PGLAZYFREED,
 		PGREFILL,
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index ecf569d..ddb9a94 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1391,9 +1391,6 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		ClearPageDirty(page);
 	unlock_page(page);
 
<span class="p_del">-	if (PageActive(page))</span>
<span class="p_del">-		deactivate_page(page);</span>
<span class="p_del">-</span>
 	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {
 		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,
 			tlb-&gt;fullmm);
<span class="p_chunk">@@ -1404,6 +1401,8 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		set_pmd_at(mm, addr, pmd, orig_pmd);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 	}
<span class="p_add">+</span>
<span class="p_add">+	mark_page_lazyfree(page);</span>
 	ret = true;
 out:
 	spin_unlock(ptl);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index c867d88..c24549e 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -378,10 +378,9 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			ptent = pte_mkclean(ptent);
 			ptent = pte_wrprotect(ptent);
 			set_pte_at(mm, addr, pte, ptent);
<span class="p_del">-			if (PageActive(page))</span>
<span class="p_del">-				deactivate_page(page);</span>
 			tlb_remove_tlb_entry(tlb, pte, addr);
 		}
<span class="p_add">+		mark_page_lazyfree(page);</span>
 	}
 out:
 	if (nr_swap) {
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index c4910f1..69a7e9d 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -46,7 +46,7 @@</span> <span class="p_context"> int page_cluster;</span>
 static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
 static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
 static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);
<span class="p_del">-static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);</span>
<span class="p_add">+static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);</span>
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);
 #endif
<span class="p_chunk">@@ -268,6 +268,11 @@</span> <span class="p_context"> static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
 		int lru = page_lru_base_type(page);
 
 		del_page_from_lru_list(page, lruvec, lru);
<span class="p_add">+		if (page_is_lazyfree(page)) {</span>
<span class="p_add">+			SetPageSwapBacked(page);</span>
<span class="p_add">+			file = 0;</span>
<span class="p_add">+			lru = LRU_INACTIVE_ANON;</span>
<span class="p_add">+		}</span>
 		SetPageActive(page);
 		lru += LRU_ACTIVE;
 		add_page_to_lru_list(page, lruvec, lru);
<span class="p_chunk">@@ -561,20 +566,21 @@</span> <span class="p_context"> static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,</span>
 }
 
 
<span class="p_del">-static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="p_add">+static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,</span>
 			    void *arg)
 {
<span class="p_del">-	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_del">-		int file = page_is_file_cache(page);</span>
<span class="p_del">-		int lru = page_lru_base_type(page);</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="p_add">+	    !PageUnevictable(page)) {</span>
<span class="p_add">+		bool active = PageActive(page);</span>
 
<span class="p_del">-		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);</span>
<span class="p_add">+		del_page_from_lru_list(page, lruvec, LRU_INACTIVE_ANON + active);</span>
 		ClearPageActive(page);
 		ClearPageReferenced(page);
<span class="p_del">-		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="p_add">+		ClearPageSwapBacked(page);</span>
<span class="p_add">+		add_page_to_lru_list(page, lruvec, LRU_INACTIVE_FILE);</span>
 
<span class="p_del">-		__count_vm_event(PGDEACTIVATE);</span>
<span class="p_del">-		update_page_reclaim_stat(lruvec, file, 0);</span>
<span class="p_add">+		update_page_reclaim_stat(lruvec, 1, 0);</span>
<span class="p_add">+		count_vm_events(PGLAZYFREE, hpage_nr_pages(page));</span>
 	}
 }
 
<span class="p_chunk">@@ -604,9 +610,9 @@</span> <span class="p_context"> void lru_add_drain_cpu(int cpu)</span>
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
 
<span class="p_del">-	pvec = &amp;per_cpu(lru_deactivate_pvecs, cpu);</span>
<span class="p_add">+	pvec = &amp;per_cpu(lru_lazyfree_pvecs, cpu);</span>
 	if (pagevec_count(pvec))
<span class="p_del">-		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="p_add">+		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
 
 	activate_page_drain(cpu);
 }
<span class="p_chunk">@@ -638,22 +644,22 @@</span> <span class="p_context"> void deactivate_file_page(struct page *page)</span>
 }
 
 /**
<span class="p_del">- * deactivate_page - deactivate a page</span>
<span class="p_add">+ * mark_page_lazyfree - make an anon page lazyfree</span>
  * @page: page to deactivate
  *
<span class="p_del">- * deactivate_page() moves @page to the inactive list if @page was on the active</span>
<span class="p_del">- * list and was not an unevictable page.  This is done to accelerate the reclaim</span>
<span class="p_del">- * of @page.</span>
<span class="p_add">+ * mark_page_lazyfree() moves @page to the inactive file list.</span>
<span class="p_add">+ * This is done to accelerate the reclaim of @page.</span>
  */
<span class="p_del">-void deactivate_page(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_del">-		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="p_add">+void mark_page_lazyfree(struct page *page)</span>
<span class="p_add">+ {</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="p_add">+	    !PageUnevictable(page)) {</span>
<span class="p_add">+		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
 
 		get_page(page);
 		if (!pagevec_add(pvec, page) || PageCompound(page))
<span class="p_del">-			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="p_del">-		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="p_add">+			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="p_add">+		put_cpu_var(lru_lazyfree_pvecs);</span>
 	}
 }
 
<span class="p_chunk">@@ -704,7 +710,7 @@</span> <span class="p_context"> void lru_add_drain_all(void)</span>
 		if (pagevec_count(&amp;per_cpu(lru_add_pvec, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_rotate_pvecs, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_deactivate_file_pvecs, cpu)) ||
<span class="p_del">-		    pagevec_count(&amp;per_cpu(lru_deactivate_pvecs, cpu)) ||</span>
<span class="p_add">+		    pagevec_count(&amp;per_cpu(lru_lazyfree_pvecs, cpu)) ||</span>
 		    need_activate_page_drain(cpu)) {
 			INIT_WORK(work, lru_add_drain_per_cpu);
 			queue_work_on(cpu, lru_add_drain_wq, work);
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 69f9aff..7774196 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -992,6 +992,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;pgfree&quot;,
 	&quot;pgactivate&quot;,
 	&quot;pgdeactivate&quot;,
<span class="p_add">+	&quot;pglazyfree&quot;,</span>
 
 	&quot;pgfault&quot;,
 	&quot;pgmajfault&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



