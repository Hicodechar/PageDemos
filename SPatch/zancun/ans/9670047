
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Documentation: vm, add hugetlbfs reservation overview - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Documentation: vm, add hugetlbfs reservation overview</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 7, 2017, 5:43 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1491586995-13085-1-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9670047/mbox/"
   >mbox</a>
|
   <a href="/patch/9670047/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9670047/">/patch/9670047/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	4CD51602A0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 17:44:35 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3EAF52860B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 17:44:35 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 31E3228622; Fri,  7 Apr 2017 17:44:35 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 43F882860B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  7 Apr 2017 17:44:33 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934262AbdDGRoP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 7 Apr 2017 13:44:15 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:17059 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933848AbdDGRoD (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 7 Apr 2017 13:44:03 -0400
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id v37HhouH031778
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 7 Apr 2017 17:43:50 GMT
Received: from aserv0121.oracle.com (aserv0121.oracle.com [141.146.126.235])
	by aserv0021.oracle.com (8.13.8/8.14.4) with ESMTP id
	v37HhnfR021647
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 7 Apr 2017 17:43:49 GMT
Received: from abhmp0002.oracle.com (abhmp0002.oracle.com [141.146.116.8])
	by aserv0121.oracle.com (8.13.8/8.13.8) with ESMTP id v37HhlB2017685; 
	Fri, 7 Apr 2017 17:43:48 GMT
Received: from monkey.oracle.com (/50.188.161.229)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Fri, 07 Apr 2017 10:43:47 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-doc@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org
Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [PATCH] Documentation: vm, add hugetlbfs reservation overview
Date: Fri,  7 Apr 2017 10:43:15 -0700
Message-Id: &lt;1491586995-13085-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.7.4
X-Source-IP: aserv0021.oracle.com [141.146.126.233]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - April 7, 2017, 5:43 p.m.</div>
<pre class="content">
Adding a brief overview of hugetlbfs reservation design and implementation
as an aid to those making code modifications in this area.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 Documentation/vm/00-INDEX             |   2 +
 Documentation/vm/hugetlbfs_reserv.txt | 529 ++++++++++++++++++++++++++++++++++
 2 files changed, 531 insertions(+)
 create mode 100644 Documentation/vm/hugetlbfs_reserv.txt
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - April 10, 2017, 3:11 a.m.</div>
<pre class="content">
On April 08, 2017 1:43 AM Mike Kravetz wrote: 
<span class="quote">&gt; </span>
<span class="quote">&gt; Adding a brief overview of hugetlbfs reservation design and implementation</span>
<span class="quote">&gt; as an aid to those making code modifications in this area.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
You are doing more than I can double thank you, Mike:)
<span class="acked-by">
Acked-by: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/00-INDEX b/Documentation/vm/00-INDEX</span>
<span class="p_header">index 6a5e2a1..11d3d8d 100644</span>
<span class="p_header">--- a/Documentation/vm/00-INDEX</span>
<span class="p_header">+++ b/Documentation/vm/00-INDEX</span>
<span class="p_chunk">@@ -12,6 +12,8 @@</span> <span class="p_context"> highmem.txt</span>
 	- Outline of highmem and common issues.
 hugetlbpage.txt
 	- a brief summary of hugetlbpage support in the Linux kernel.
<span class="p_add">+hugetlbfs_reserv.txt</span>
<span class="p_add">+	- A brief overview of hugetlbfs reservation design/implementation.</span>
 hwpoison.txt
 	- explains what hwpoison is
 idle_page_tracking.txt
<span class="p_header">diff --git a/Documentation/vm/hugetlbfs_reserv.txt b/Documentation/vm/hugetlbfs_reserv.txt</span>
new file mode 100644
<span class="p_header">index 0000000..9aca09a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/vm/hugetlbfs_reserv.txt</span>
<span class="p_chunk">@@ -0,0 +1,529 @@</span> <span class="p_context"></span>
<span class="p_add">+Hugetlbfs Reservation Overview</span>
<span class="p_add">+------------------------------</span>
<span class="p_add">+Huge pages as described at &#39;Documentation/vm/hugetlbpage.txt&#39; are typically</span>
<span class="p_add">+preallocated for application use.  These huge pages are instantiated in a</span>
<span class="p_add">+task&#39;s address space at page fault time if the VMA indicates huge pages are</span>
<span class="p_add">+to be used.  If no huge page exists at page fault time, the task is sent</span>
<span class="p_add">+a SIGBUS and often dies an unhappy death.  Shortly after huge page support</span>
<span class="p_add">+was added, it was determined that it would be better to detect a shortage</span>
<span class="p_add">+of huge pages at mmap() time.  The idea is that if there were not enough</span>
<span class="p_add">+huge pages to cover the mapping, the mmap() would fail.  This was first</span>
<span class="p_add">+done with a simple check in the code at mmap() time to determine if there</span>
<span class="p_add">+were enough free huge pages to cover the mapping.  Like most things in the</span>
<span class="p_add">+kernel, the code has evolved over time.  However, the basic idea was to</span>
<span class="p_add">+&#39;reserve&#39; huge pages at mmap() time to ensure that huge pages would be</span>
<span class="p_add">+available for page faults in that mapping.  The description below attempts to</span>
<span class="p_add">+describe how huge page reserve processing is done in the v4.10 kernel.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Audience</span>
<span class="p_add">+--------</span>
<span class="p_add">+This description is primarily targeted at kernel developers who are modifying</span>
<span class="p_add">+hugetlbfs code.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+The Data Structures</span>
<span class="p_add">+-------------------</span>
<span class="p_add">+resv_huge_pages</span>
<span class="p_add">+	This is a global (per-hstate) count of reserved huge pages.  Reserved</span>
<span class="p_add">+	huge pages are only available to the task which reserved them.</span>
<span class="p_add">+	Therefore, the number of huge pages generally available is computed</span>
<span class="p_add">+	as (free_huge_pages - resv_huge_pages).</span>
<span class="p_add">+Reserve Map</span>
<span class="p_add">+	A reserve map is described by the structure:</span>
<span class="p_add">+	struct resv_map {</span>
<span class="p_add">+		struct kref refs;</span>
<span class="p_add">+		spinlock_t lock;</span>
<span class="p_add">+		struct list_head regions;</span>
<span class="p_add">+		long adds_in_progress;</span>
<span class="p_add">+		struct list_head region_cache;</span>
<span class="p_add">+		long region_cache_count;</span>
<span class="p_add">+	};</span>
<span class="p_add">+	There is one reserve map for each huge page mapping in the system.</span>
<span class="p_add">+	The regions list within the resv_map describes the regions within</span>
<span class="p_add">+	the mapping.  A region is described as:</span>
<span class="p_add">+	struct file_region {</span>
<span class="p_add">+		struct list_head link;</span>
<span class="p_add">+		long from;</span>
<span class="p_add">+		long to;</span>
<span class="p_add">+	};</span>
<span class="p_add">+	The &#39;from&#39; and &#39;to&#39; fields of the file region structure are huge page</span>
<span class="p_add">+	indices into the mapping.  Depending on the type of mapping, a</span>
<span class="p_add">+	region in the reserv_map may indicate reservations exist for the</span>
<span class="p_add">+	range, or reservations do not exist.</span>
<span class="p_add">+Flags for MAP_PRIVATE Reservations</span>
<span class="p_add">+	These are stored in the bottom bits of the reservation map pointer.</span>
<span class="p_add">+	#define HPAGE_RESV_OWNER    (1UL &lt;&lt; 0) Indicates this task is the</span>
<span class="p_add">+		owner of the reservations associated with the mapping.</span>
<span class="p_add">+	#define HPAGE_RESV_UNMAPPED (1UL &lt;&lt; 1) Indicates task originally</span>
<span class="p_add">+		mapping this range (and creating reserves) has unmapped a</span>
<span class="p_add">+		page from this task (the child) due to a failed COW.</span>
<span class="p_add">+Page Flags</span>
<span class="p_add">+	The PagePrivate page flag is used to indicate that a huge page</span>
<span class="p_add">+	reservation must be restored when the huge page is freed.  More</span>
<span class="p_add">+	details will be discussed in the &quot;Freeing huge pages&quot; section.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Reservation Map Location (Private or Shared)</span>
<span class="p_add">+--------------------------------------------</span>
<span class="p_add">+A huge page mapping or segment is either private or shared.  If private,</span>
<span class="p_add">+it is typically only available to a single address space (task).  If shared,</span>
<span class="p_add">+it can be mapped into multiple address spaces (tasks).  The location and</span>
<span class="p_add">+semantics of the reservation map is significantly different for two types</span>
<span class="p_add">+of mappings.  Location differences are:</span>
<span class="p_add">+- For private mappings, the reservation map hangs off the the VMA structure.</span>
<span class="p_add">+  Specifically, vma-&gt;vm_private_data.  This reserve map is created at the</span>
<span class="p_add">+  time the mapping (mmap(MAP_PRIVATE)) is created.</span>
<span class="p_add">+- For shared mappings, the reservation map hangs off the inode.  Specifically,</span>
<span class="p_add">+  inode-&gt;i_mapping-&gt;private_data.  Since shared mappings are always backed</span>
<span class="p_add">+  by files in the hugetlbfs filesystem, the hugetlbfs code ensures each inode</span>
<span class="p_add">+  contains a reservation map.  As a result, the reservation map is allocated</span>
<span class="p_add">+  when the inode is created.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Creating Reservations</span>
<span class="p_add">+---------------------</span>
<span class="p_add">+Reservations are created when a huge page backed shared memory segment is</span>
<span class="p_add">+created (shmget(SHM_HUGETLB)) or a mapping is created via mmap(MAP_HUGETLB).</span>
<span class="p_add">+These operations result in a call to the routine hugetlb_reserve_pages()</span>
<span class="p_add">+</span>
<span class="p_add">+int hugetlb_reserve_pages(struct inode *inode,</span>
<span class="p_add">+					long from, long to,</span>
<span class="p_add">+					struct vm_area_struct *vma,</span>
<span class="p_add">+					vm_flags_t vm_flags)</span>
<span class="p_add">+</span>
<span class="p_add">+The first thing hugetlb_reserve_pages() does is check for the NORESERVE</span>
<span class="p_add">+flag was specified in either the shmget() or mmap() call.  If NORESERVE</span>
<span class="p_add">+was specified, then this routine returns immediately as no reservation</span>
<span class="p_add">+are desired.</span>
<span class="p_add">+</span>
<span class="p_add">+The arguments &#39;from&#39; and &#39;to&#39; are huge page indices into the mapping or</span>
<span class="p_add">+underlying file.  For shmget(), &#39;from&#39; is always 0 and &#39;to&#39; corresponds to</span>
<span class="p_add">+the length of the segment/mapping.  For mmap(), the offset argument could</span>
<span class="p_add">+be used to specify the offset into the underlying file.  In such a case</span>
<span class="p_add">+the &#39;from&#39; and &#39;to&#39; arguments have been adjusted by this offset.</span>
<span class="p_add">+</span>
<span class="p_add">+One of the big differences between PRIVATE and SHARED mappings is the way</span>
<span class="p_add">+in which reservations are represented in the reservation map.</span>
<span class="p_add">+- For shared mappings, an entry in the reservation map indicates a reservation</span>
<span class="p_add">+  exists or did exist for the corresponding page.  As reservations are</span>
<span class="p_add">+  consumed, the reservation map is not modified.</span>
<span class="p_add">+- For private mappings, the lack of an entry in the reservation map indicates</span>
<span class="p_add">+  a reservation exists for the corresponding page.  As reservations are</span>
<span class="p_add">+  consumed, entries are added to the reservation map.  Therefore, the</span>
<span class="p_add">+  reservation map can also be used to determine which reservations have</span>
<span class="p_add">+  been consumed.</span>
<span class="p_add">+</span>
<span class="p_add">+For private mappings, hugetlb_reserve_pages() creates the reservation map and</span>
<span class="p_add">+hangs it off the VMA structure.  In addition, the HPAGE_RESV_OWNER flag is set</span>
<span class="p_add">+to indicate this VMA owns the reservations.</span>
<span class="p_add">+</span>
<span class="p_add">+The reservation map is consulted to determine how many huge page reservations</span>
<span class="p_add">+are needed for the current mapping/segment.  For private mappings, this is</span>
<span class="p_add">+always the value (to - from).  However, for shared mappings it is possible that some reservations may already exist within the range (to - from).  See the</span>
<span class="p_add">+section &quot;Reservation Map Modifications&quot; for details on how this is accomplished.</span>
<span class="p_add">+</span>
<span class="p_add">+The mapping may be associated with a subpool.  If so, the subpool is consulted</span>
<span class="p_add">+to ensure there is sufficient space for the mapping.  It is possible that the</span>
<span class="p_add">+subpool has set aside reservations that can be used for the mapping.  See the</span>
<span class="p_add">+section &quot;Subpool Reservations&quot; for more details.</span>
<span class="p_add">+</span>
<span class="p_add">+After consulting the reservation map and subpool, the number of needed new</span>
<span class="p_add">+reservations is known.  The routine hugetlb_acct_memory() is called to check</span>
<span class="p_add">+for and take the requested number of reservations.  hugetlb_acct_memory()</span>
<span class="p_add">+calls into routines that potentially allocate and adjust surplus page counts.</span>
<span class="p_add">+However, within those routines the code is simply checking to ensure there</span>
<span class="p_add">+are enough free huge pages to accommodate the reservation.  If there are,</span>
<span class="p_add">+the global reservation count resv_huge_pages is adjusted something like the</span>
<span class="p_add">+following.</span>
<span class="p_add">+	if (resv_needed &lt;= (resv_huge_pages - free_huge_pages))</span>
<span class="p_add">+		resv_huge_pages += resv_needed;</span>
<span class="p_add">+Note that the global lock hugetlb_lock is held when checking and adjusting</span>
<span class="p_add">+these counters.</span>
<span class="p_add">+</span>
<span class="p_add">+If there were enough free huge pages and the global count resv_huge_pages</span>
<span class="p_add">+was adjusted, then the reservation map associated with the mapping is</span>
<span class="p_add">+modified to reflect the reservations.  In the case of a shared mapping, a</span>
<span class="p_add">+file_region will exist that includes the range &#39;from&#39; &#39;to&#39;.  For private</span>
<span class="p_add">+mappings, no modifications are made to the reservation map as lack of an</span>
<span class="p_add">+entry indicates a reservation exists.</span>
<span class="p_add">+</span>
<span class="p_add">+If hugetlb_reserve_pages() was successful, the global reservation count and</span>
<span class="p_add">+reservation map associated with the mapping will be modified as required to</span>
<span class="p_add">+ensure reservations exist for the range &#39;from&#39; - &#39;to&#39;.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Consuming Reservations/Allocating a Huge Page</span>
<span class="p_add">+---------------------------------------------</span>
<span class="p_add">+Reservations are consumed when huge pages associated with the reservations</span>
<span class="p_add">+are allocated and instantiated in the corresponding mapping.  The allocation</span>
<span class="p_add">+is performed within the routine alloc_huge_page().</span>
<span class="p_add">+struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="p_add">+                                    unsigned long addr, int avoid_reserve)</span>
<span class="p_add">+alloc_huge_page is passed a VMA pointer and a virtual address, so it can</span>
<span class="p_add">+consult the reservation map to determine if a reservation exists.  In addition,</span>
<span class="p_add">+alloc_huge_page takes the argument avoid_reserve which indicates reserves</span>
<span class="p_add">+should not be used even if it appears they have been set aside for the</span>
<span class="p_add">+specified address.  The avoid_reserve argument is most often used in the case</span>
<span class="p_add">+of Copy on Write and Page Migration where additional copies of an existing</span>
<span class="p_add">+page are being allocated.</span>
<span class="p_add">+</span>
<span class="p_add">+The helper routine vma_needs_reservation() is called to determine if a</span>
<span class="p_add">+reservation exists for the address within the mapping(vma).  See the section</span>
<span class="p_add">+&quot;Reservation Map Helper Routines&quot; for detailed information on what this</span>
<span class="p_add">+routine does.  The value returned from vma_needs_reservation() is generally</span>
<span class="p_add">+0 or 1.  0 if a reservation exists for the address, 1 if no reservation exists.</span>
<span class="p_add">+If a reservation does not exist, and there is a subpool associated with the</span>
<span class="p_add">+mapping the subpool is consulted to determine if it contains reservations.</span>
<span class="p_add">+If the subpool contains reservations, one can be used for this allocation.</span>
<span class="p_add">+However, in every case the avoid_reserve argument overrides the use of</span>
<span class="p_add">+a reservation for the allocation.  After determining whether a reservation</span>
<span class="p_add">+exists and can be used for the allocation, the routine dequeue_huge_page_vma()</span>
<span class="p_add">+is called.  This routine takes two arguments related to reservations:</span>
<span class="p_add">+- avoid_reserve, this is the same value/argument passed to alloc_huge_page()</span>
<span class="p_add">+- chg, even though this argument is of type long only the values 0 or 1 are</span>
<span class="p_add">+  passed to dequeue_huge_page_vma.  If the value is 0, it indicates a</span>
<span class="p_add">+  reservation exists (see the section &quot;Memory Policy and Reservations&quot; for</span>
<span class="p_add">+  possible issues).  If the value is 1, it indicates a reservation does not</span>
<span class="p_add">+  exist and the page must be taken from the global free pool if possible.</span>
<span class="p_add">+The free lists associated with the memory policy of the VMA are searched for</span>
<span class="p_add">+a free page.  If a page is found, the value free_huge_pages is decremented</span>
<span class="p_add">+when the page is removed from the free list.  If there was a reservation</span>
<span class="p_add">+associated with the page, the following adjustments are made:</span>
<span class="p_add">+	SetPagePrivate(page);	/* Indicates allocating this page consumed</span>
<span class="p_add">+				 * a reservation, and if an error is</span>
<span class="p_add">+				 * encountered such that the page must be</span>
<span class="p_add">+				 * freed, the reservation will be restored. */</span>
<span class="p_add">+	resv_huge_pages--;	/* Decrement the global reservation count */</span>
<span class="p_add">+Note, if no huge page can be found that satisfies the VMA&#39;s memory policy</span>
<span class="p_add">+an attempt will be made to allocate one using the buddy allocator.  This</span>
<span class="p_add">+brings up the issue of surplus huge pages and overcommit which is beyond</span>
<span class="p_add">+the scope reservations.  Even if a surplus page is allocated, the same</span>
<span class="p_add">+reservation based adjustments as above will be made: SetPagePrivate(page) and</span>
<span class="p_add">+resv_huge_pages--.</span>
<span class="p_add">+</span>
<span class="p_add">+After obtaining a new huge page, (page)-&gt;private is set to the value of</span>
<span class="p_add">+the subpool associated with the page if it exists.  This will be used for</span>
<span class="p_add">+subpool accounting when the page is freed.</span>
<span class="p_add">+</span>
<span class="p_add">+The routine vma_commit_reservation() is then called to adjust the reserve</span>
<span class="p_add">+map based on the consumption of the reservation.  In general, this involves</span>
<span class="p_add">+ensuring the page is represented within a file_region structure of the region</span>
<span class="p_add">+map.  For shared mappings where the the reservation was present, an entry</span>
<span class="p_add">+in the reserve map already existed so no change is made.  However, if there</span>
<span class="p_add">+was no reservation in a shared mapping or this was a private mapping a new</span>
<span class="p_add">+entry must be created.</span>
<span class="p_add">+</span>
<span class="p_add">+It is possible that the reserve map could have been changed between the call</span>
<span class="p_add">+to vma_needs_reservation() at the beginning of alloc_huge_page() and the</span>
<span class="p_add">+call to vma_commit_reservation() after the page was allocated.  This would</span>
<span class="p_add">+be possible if hugetlb_reserve_pages was called for the same page in a shared</span>
<span class="p_add">+mapping.  In such cases, the reservation count and subpool free page count</span>
<span class="p_add">+will be off by one.  This rare condition can be identified by comparing the</span>
<span class="p_add">+return value from vma_needs_reservation and vma_commit_reservation.  If such</span>
<span class="p_add">+a race is detected, the subpool and global reserve counts are adjusted to</span>
<span class="p_add">+compensate.  See the section &quot;Reservation Map Helper Routines&quot; for more</span>
<span class="p_add">+information on these routines.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Instantiate Huge Pages</span>
<span class="p_add">+----------------------</span>
<span class="p_add">+After huge page allocation, the page is typically added to the page tables</span>
<span class="p_add">+of the allocating task.  Before this, pages in a shared mapping are added</span>
<span class="p_add">+to the page cache and pages in private mappings are added to an anonymous</span>
<span class="p_add">+reverse mapping.  In both cases, the PagePrivate flag is cleared.  Therefore,</span>
<span class="p_add">+when a huge page that has been instantiated is freed no adjustment is made</span>
<span class="p_add">+to the global reservation count (resv_huge_pages).</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Freeing Huge Pages</span>
<span class="p_add">+------------------</span>
<span class="p_add">+Huge page freeing is performed by the routine free_huge_page().  This routine</span>
<span class="p_add">+is the destructor for hugetlbfs compound pages.  As a result, it is only</span>
<span class="p_add">+passed a pointer to the page struct.  When a huge page is freed, reservation</span>
<span class="p_add">+accounting may need to be performed.  This would be the case if the page was</span>
<span class="p_add">+associated with a subpool that contained reserves, or the page is being freed</span>
<span class="p_add">+on an error path where a global reserve count must be restored.</span>
<span class="p_add">+</span>
<span class="p_add">+The page-&gt;private field points to any subpool associated with the page.</span>
<span class="p_add">+If the PagePrivate flag is set, it indicates the global reserve count should</span>
<span class="p_add">+be adjusted (see the section &quot;Consuming Reservations/Allocating a Huge Page&quot;</span>
<span class="p_add">+for information on how these are set).</span>
<span class="p_add">+</span>
<span class="p_add">+The routine first calls hugepage_subpool_put_pages() for the page.  If this</span>
<span class="p_add">+routine returns a value of 0 (which does not equal the value passed 1) it</span>
<span class="p_add">+indicates reserves are associated with the subpool, and this newly free page</span>
<span class="p_add">+must be used to keep the number of subpool reserves above the minimum size.</span>
<span class="p_add">+Therefore, the global resv_huge_pages counter is incremented in this case.</span>
<span class="p_add">+</span>
<span class="p_add">+If the PagePrivate flag was set in the page, the global resv_huge_pages counter</span>
<span class="p_add">+will always be incremented.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Subpool Reservations</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+There is a struct hstate associated with each huge page size.  The hstate</span>
<span class="p_add">+tracks all huge pages of the specified size.  A subpool represents a subset</span>
<span class="p_add">+of pages within a hstate that is associated with a mounted hugetlbfs</span>
<span class="p_add">+filesystem.</span>
<span class="p_add">+</span>
<span class="p_add">+When a hugetlbfs filesystem is mounted a min_size option can be specified</span>
<span class="p_add">+which indicates the minimum number of huge pages required by the filesystem.</span>
<span class="p_add">+If this option is specified, the number of huge pages corresponding to</span>
<span class="p_add">+min_size are reserved for use by the filesystem.  This number is tracked in</span>
<span class="p_add">+the min_hpages field of a struct hugepage_subpool.  At mount time,</span>
<span class="p_add">+hugetlb_acct_memory(min_hpages) is called to reserve the specified number of</span>
<span class="p_add">+huge pages.  If they can not be reserved, the mount fails.</span>
<span class="p_add">+</span>
<span class="p_add">+The routines hugepage_subpool_get/put_pages() are called when pages are</span>
<span class="p_add">+obtained from or released back to a subpool.  They perform all subpool</span>
<span class="p_add">+accounting, and track any reservations associated with the subpool.</span>
<span class="p_add">+hugepage_subpool_get/put_pages are passed the number of huge pages by which</span>
<span class="p_add">+to adjust the subpool &#39;used page&#39; count (down for get, up for put).  Normally,</span>
<span class="p_add">+they return the same value that was passed or an error if not enough pages</span>
<span class="p_add">+exist in the subpool.</span>
<span class="p_add">+</span>
<span class="p_add">+However, if reserves are associated with the subpool a return value less</span>
<span class="p_add">+than the passed value may be returned.  This return value indicates the</span>
<span class="p_add">+number of additional global pool adjustments which must be made.  For example,</span>
<span class="p_add">+suppose a subpool contains 3 reserved huge pages and someone asks for 5.</span>
<span class="p_add">+The 3 reserved pages associated with the subpool can be used to satisfy part</span>
<span class="p_add">+of the request.  But, 2 pages must be obtained from the global pools.  To</span>
<span class="p_add">+relay this information to the caller, the value 2 is returned.  The caller</span>
<span class="p_add">+is then responsible for attempting to obtain the additional two pages from</span>
<span class="p_add">+the global pools.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+COW and Reservations</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+Since shared mappings all point to and use the same underlying pages, the</span>
<span class="p_add">+biggest reservation concern for COW is private mappings.  In this case,</span>
<span class="p_add">+two tasks can be pointing at the same previously allocated page.  One task</span>
<span class="p_add">+attempts to write to the page, so a new page must be allocated so that each</span>
<span class="p_add">+task points to its own page.</span>
<span class="p_add">+</span>
<span class="p_add">+When the page was originally allocated, the reservation for that page was</span>
<span class="p_add">+consumed.  When an attempt to allocate a new page is made as a result of</span>
<span class="p_add">+COW, it is possible that no free huge pages are free and the allocation</span>
<span class="p_add">+will fail.</span>
<span class="p_add">+</span>
<span class="p_add">+When the private mapping was originally created, the owner of the mapping</span>
<span class="p_add">+was noted by setting the HPAGE_RESV_OWNER bit in the pointer to the reservation</span>
<span class="p_add">+map of the owner.  Since the owner created the mapping, the owner owns all</span>
<span class="p_add">+the reservations associated with the mapping.  Therefore, when a write fault</span>
<span class="p_add">+occurs and there is no page available, different action is taken for the owner</span>
<span class="p_add">+and non-owner of the reservation.</span>
<span class="p_add">+</span>
<span class="p_add">+In the case where the faulting task is not the owner, the fault will fail and</span>
<span class="p_add">+the task will typically receive a SIGBUS.</span>
<span class="p_add">+</span>
<span class="p_add">+If the owner is the faulting task, we want it to succeed since it owned the</span>
<span class="p_add">+original reservation.  To accomplish this, the page is unmapped from the</span>
<span class="p_add">+non-owning task.  In this way, the only reference is from the owning task.</span>
<span class="p_add">+In addition, the HPAGE_RESV_UNMAPPED bit is set in the reservation map pointer</span>
<span class="p_add">+of the non-owning task.  The non-owning task may receive a SIGBUS if it later</span>
<span class="p_add">+faults on a non-present page.  But, the original owner of the</span>
<span class="p_add">+mapping/reservation will behave as expected.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Reservation Map Modifications</span>
<span class="p_add">+-----------------------------</span>
<span class="p_add">+The following low level routines are used to make modifications to a</span>
<span class="p_add">+reservation map.  Typically, these routines are not called directly.  Rather,</span>
<span class="p_add">+a reservation map helper routine is called which calls one of these low level</span>
<span class="p_add">+routines.  These low level routines are fairly well documented in the source</span>
<span class="p_add">+code (mm/hugetlb.c).  These routines are:</span>
<span class="p_add">+long region_chg(struct resv_map *resv, long f, long t);</span>
<span class="p_add">+long region_add(struct resv_map *resv, long f, long t);</span>
<span class="p_add">+void region_abort(struct resv_map *resv, long f, long t);</span>
<span class="p_add">+long region_count(struct resv_map *resv, long f, long t);</span>
<span class="p_add">+</span>
<span class="p_add">+Operations on the reservation map typically involve two operations:</span>
<span class="p_add">+1) region_chg() is called to examine the reserve map and determine how</span>
<span class="p_add">+   many pages in the specified range [f, t) are NOT currently represented.</span>
<span class="p_add">+</span>
<span class="p_add">+   The calling code performs global checks and allocations to determine if</span>
<span class="p_add">+   there are enough huge pages for the operation to succeed.</span>
<span class="p_add">+</span>
<span class="p_add">+2a) If the operation can succeed, region_add() is called to actually modify</span>
<span class="p_add">+    the reservation map for the same range [f, t) previously passed to</span>
<span class="p_add">+    region_chg().</span>
<span class="p_add">+2b) If the operation can not succeed, region_abort is called for the same range</span>
<span class="p_add">+    [f, t) to abort the operation.</span>
<span class="p_add">+</span>
<span class="p_add">+Note that this is a two step process where region_add() and region_abort()</span>
<span class="p_add">+are guaranteed to succeed after a prior call to region_chg() for the same</span>
<span class="p_add">+range.  region_chg() is responsible for pre-allocating any data structures</span>
<span class="p_add">+necessary to ensure the subsequent operations (specifically region_add()))</span>
<span class="p_add">+will succeed.</span>
<span class="p_add">+</span>
<span class="p_add">+As mentioned above, region_chg() determines the number of pages in the range</span>
<span class="p_add">+which are NOT currently represented in the map.  This number is returned to</span>
<span class="p_add">+the caller.  region_add() returns the number of pages in the range added to</span>
<span class="p_add">+the map.  In most cases, the return value of region_add() is the same as the</span>
<span class="p_add">+return value of region_chg().  However, in the case of shared mappings it is</span>
<span class="p_add">+possible for changes to the reservation map to be made between the calls to</span>
<span class="p_add">+region_chg() and region_add().  In this case, the return value of region_add()</span>
<span class="p_add">+will not match the return value of region_chg().  It is likely that in such</span>
<span class="p_add">+cases global counts and subpool accounting will be incorrect and in need of</span>
<span class="p_add">+adjustment.  It is the responsibility of the caller to check for this condition</span>
<span class="p_add">+and make the appropriate adjustments.</span>
<span class="p_add">+</span>
<span class="p_add">+The routine region_del() is called to remove regions from a reservation map.</span>
<span class="p_add">+It is typically called in the following situations:</span>
<span class="p_add">+- When a file in the hugetlbfs filesystem is being removed, the inode will</span>
<span class="p_add">+  be released and the reservation map freed.  Before freeing the reservation</span>
<span class="p_add">+  map, all the individual file_region structures must be freed.  In this case</span>
<span class="p_add">+  region_del is passed the range [0, LONG_MAX).</span>
<span class="p_add">+- When a hugetlbfs file is being truncated.  In this case, all allocated pages</span>
<span class="p_add">+  after the new file size must be freed.  In addition, any file_region entries</span>
<span class="p_add">+  in the reservation map past the new end of file must be deleted.  In this</span>
<span class="p_add">+  case, region_del is passed the range [new_end_of_file, LONG_MAX).</span>
<span class="p_add">+- When a hole is being punched in a hugetlbfs file.  In this case, huge pages</span>
<span class="p_add">+  are removed from the middle of the file one at a time.  As the pages are</span>
<span class="p_add">+  removed, region_del() is called to remove the corresponding entry from the</span>
<span class="p_add">+  reservation map.  In this case, region_del is passed the range</span>
<span class="p_add">+  [page_idx, page_idx + 1).</span>
<span class="p_add">+In every case, region_del() will return the number of pages removed from the</span>
<span class="p_add">+reservation map.  In VERY rare cases, region_del() can fail.  This can only</span>
<span class="p_add">+happen in the hole punch case where it has to split an existing file_region</span>
<span class="p_add">+entry and can not allocate a new structure.  In this error case, region_del()</span>
<span class="p_add">+will return -ENOMEM.  The problem here is that the reservation map will</span>
<span class="p_add">+indicate that there is a reservation for the page.  However, the subpool and</span>
<span class="p_add">+global reservation counts will not reflect the reservation.  To handle this</span>
<span class="p_add">+situation, the routine hugetlb_fix_reserve_counts() is called to adjust the</span>
<span class="p_add">+counters so that they correspond with the reservation map entry that could</span>
<span class="p_add">+not be deleted.</span>
<span class="p_add">+</span>
<span class="p_add">+region_count() is called when unmapping a private huge page mapping.  In</span>
<span class="p_add">+private mappings, the lack of a entry in the reservation map indicates that</span>
<span class="p_add">+a reservation exists.  Therefore, by counting the number of entries in the</span>
<span class="p_add">+reservation map we know how many reservations were consumed and how many are</span>
<span class="p_add">+outstanding (outstanding = (end - start) - region_count(resv, start, end)).</span>
<span class="p_add">+Since the mapping is going away, the subpool and global reservation counts</span>
<span class="p_add">+are decremented by the number of outstanding reservations.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Reservation Map Helper Routines</span>
<span class="p_add">+-------------------------------</span>
<span class="p_add">+Several helper routines exist to query and modify the reservation maps.</span>
<span class="p_add">+These routines are only interested with reservations for a specific huge</span>
<span class="p_add">+page, so they just pass in an address instead of a range.  In addition,</span>
<span class="p_add">+they pass in the associated VMA.  From the VMA, the type of mapping (private</span>
<span class="p_add">+or shared) and the location of the reservation map (inode or VMA) can be</span>
<span class="p_add">+determined.  These routines simply call the underlying routines described</span>
<span class="p_add">+in the section &quot;Reservation Map Modifications&quot;.  However, they do take into</span>
<span class="p_add">+account the &#39;opposite&#39; meaning of reservation map entries for private and</span>
<span class="p_add">+shared mappings and hide this detail from the caller.</span>
<span class="p_add">+</span>
<span class="p_add">+long vma_needs_reservation(struct hstate *h,</span>
<span class="p_add">+				struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+This routine calls region_chg() for the specified page.  If no reservation</span>
<span class="p_add">+exists, 1 is returned.  If a reservation exists, 0 is returned.</span>
<span class="p_add">+</span>
<span class="p_add">+long vma_commit_reservation(struct hstate *h,</span>
<span class="p_add">+				struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+This calls region_add() for the specified page.  As in the case of region_chg</span>
<span class="p_add">+and region_add, this routine is to be called after a previous call to</span>
<span class="p_add">+vma_needs_reservation.  It will add a reservation entry for the page.  It</span>
<span class="p_add">+returns 1 if the reservation was added and 0 if not.  The return value should</span>
<span class="p_add">+be compared with the return value of the previous call to</span>
<span class="p_add">+vma_needs_reservation.  An unexpected difference indicates the reservation</span>
<span class="p_add">+map was modified between calls.</span>
<span class="p_add">+</span>
<span class="p_add">+void vma_end_reservation(struct hstate *h,</span>
<span class="p_add">+				struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+This calls region_abort() for the specified page.  As in the case of region_chg</span>
<span class="p_add">+and region_abort, this routine is to be called after a previous call to</span>
<span class="p_add">+vma_needs_reservation.  It will abort/end the in progress reservation add</span>
<span class="p_add">+operation.</span>
<span class="p_add">+</span>
<span class="p_add">+long vma_add_reservation(struct hstate *h,</span>
<span class="p_add">+				struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+This is a special wrapper routine to help facilitate reservation cleanup</span>
<span class="p_add">+on error paths.  It is only called from the routine restore_reserve_on_error().</span>
<span class="p_add">+This routine is used in conjunction with vma_needs_reservation in an attempt</span>
<span class="p_add">+to add a reservation to the reservation map.  It takes into account the</span>
<span class="p_add">+different reservation map semantics for private and shared mappings.  Hence,</span>
<span class="p_add">+region_add is called for shared mappings (as an entry present in the map</span>
<span class="p_add">+indicates a reservation), and region_del is called for private mappings (as</span>
<span class="p_add">+the absence of an entry in the map indicates a reservation).  See the section</span>
<span class="p_add">+&quot;Reservation cleanup in error paths&quot; for more information on what needs to</span>
<span class="p_add">+be done on error paths.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Reservation Cleanup in Error Paths</span>
<span class="p_add">+----------------------------------</span>
<span class="p_add">+As mentioned in the section &quot;Reservation Map Helper Routines&quot;, reservation</span>
<span class="p_add">+map modifications are performed in two steps.  First vma_needs_reservation</span>
<span class="p_add">+is called before a page is allocated.  If the allocation is successful,</span>
<span class="p_add">+then vma_commit_reservation is called.  If not, vma_end_reservation is called.</span>
<span class="p_add">+Global and subpool reservation counts are adjusted based on success or failure</span>
<span class="p_add">+of the operation and all is well.</span>
<span class="p_add">+</span>
<span class="p_add">+Additionally, after a huge page is instantiated the PagePrivate flag is</span>
<span class="p_add">+cleared so that accounting when the page is ultimately freed is correct.</span>
<span class="p_add">+</span>
<span class="p_add">+However, there are several instances where errors are encountered after a huge</span>
<span class="p_add">+page is allocated but before it is instantiated.  In this case, the page</span>
<span class="p_add">+allocation has consumed the reservation and made the appropriate subpool,</span>
<span class="p_add">+reservation map and global count adjustments.  If the page is freed at this</span>
<span class="p_add">+time (before instantiation and clearing of PagePrivate), then free_huge_page</span>
<span class="p_add">+will increment the global reservation count.  However, the reservation map</span>
<span class="p_add">+indicates the reservation was consumed.  This resulting inconsistent state</span>
<span class="p_add">+will cause the &#39;leak&#39; of a reserved huge page.  The global reserve count will</span>
<span class="p_add">+be  higher than it should and prevent allocation of a pre-allocated page.</span>
<span class="p_add">+</span>
<span class="p_add">+The routine restore_reserve_on_error() attempts to handle this situation.  It</span>
<span class="p_add">+is fairly well documented.  The intention of this routine is to restore</span>
<span class="p_add">+the reservation map to the way it was before the page allocation.   In this</span>
<span class="p_add">+way, the state of the reservation map will correspond to the global reservation</span>
<span class="p_add">+count after the page is freed.</span>
<span class="p_add">+</span>
<span class="p_add">+The routine restore_reserve_on_error itself may encounter errors while</span>
<span class="p_add">+attempting to restore the reservation map entry.  In this case, it will</span>
<span class="p_add">+simply clear the PagePrivate flag of the page.  In this way, the global</span>
<span class="p_add">+reserve count will not be incremented when the page is freed.  However, the</span>
<span class="p_add">+reservation map will continue to look as though the reservation was consumed.</span>
<span class="p_add">+A page can still be allocated for the address, but it will not use a reserved</span>
<span class="p_add">+page as originally intended.</span>
<span class="p_add">+</span>
<span class="p_add">+There is some code (most notably userfaultfd) which can not call</span>
<span class="p_add">+restore_reserve_on_error.  In this case, it simply modifies the PagePrivate</span>
<span class="p_add">+so that a reservation will not be leaked when the huge page is freed.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Reservations and Memory Policy</span>
<span class="p_add">+------------------------------</span>
<span class="p_add">+Per-node huge page lists existed in struct hstate when git was first used</span>
<span class="p_add">+to manage Linux code.  The concept of reservations was added some time later.</span>
<span class="p_add">+When reservations were added, no attempt was made to take memory policy</span>
<span class="p_add">+into account.  While cpusets are not exactly the same as memory policy, this</span>
<span class="p_add">+comment in hugetlb_acct_memory sums up the interaction between reservations</span>
<span class="p_add">+and cpusets/memory policy.</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * When cpuset is configured, it breaks the strict hugetlb page</span>
<span class="p_add">+	 * reservation as the accounting is done on a global variable. Such</span>
<span class="p_add">+	 * reservation is completely rubbish in the presence of cpuset because</span>
<span class="p_add">+	 * the reservation is not checked against page availability for the</span>
<span class="p_add">+	 * current cpuset. Application can still potentially OOM&#39;ed by kernel</span>
<span class="p_add">+	 * with lack of free htlb page in cpuset that the task is in.</span>
<span class="p_add">+	 * Attempt to enforce strict accounting with cpuset is almost</span>
<span class="p_add">+	 * impossible (or too ugly) because cpuset is too fluid that</span>
<span class="p_add">+	 * task or memory node can be dynamically moved between cpusets.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The change of semantics for shared hugetlb mapping with cpuset is</span>
<span class="p_add">+	 * undesirable. However, in order to preserve some of the semantics,</span>
<span class="p_add">+	 * we fall back to check against current free page availability as</span>
<span class="p_add">+	 * a best attempt and hopefully to minimize the impact of changing</span>
<span class="p_add">+	 * semantics that cpuset has.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+Huge page reservations were added to prevent unexpected page allocation</span>
<span class="p_add">+failures (OOM) at page fault time.  However, if an application makes use</span>
<span class="p_add">+of cpusets or memory policy there is no guarantee that huge pages will be</span>
<span class="p_add">+available on the required nodes.  This is true even if there are a sufficient</span>
<span class="p_add">+number of global reservations.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Mike Kravetz, 7 April 2017</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



