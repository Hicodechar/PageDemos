
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.14.14 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.14.14</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 17, 2018, 9:44 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180117094432.GB18242@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10168883/mbox/"
   >mbox</a>
|
   <a href="/patch/10168883/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10168883/">/patch/10168883/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	193D0603ED for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 09:45:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C530327FA8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 09:45:08 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B50A72838B; Wed, 17 Jan 2018 09:45:08 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 79F2327FA8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 09:44:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752794AbeAQJo5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 17 Jan 2018 04:44:57 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:48156 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752687AbeAQJor (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 17 Jan 2018 04:44:47 -0500
Received: from localhost (unknown [185.156.173.27])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id C0ABE1108;
	Wed, 17 Jan 2018 09:44:35 +0000 (UTC)
Date: Wed, 17 Jan 2018 10:44:32 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.14.14
Message-ID: &lt;20180117094432.GB18242@kroah.com&gt;
References: &lt;20180117094417.GA18242@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: &lt;20180117094417.GA18242@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 17, 2018, 9:44 a.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/ABI/testing/sysfs-devices-system-cpu b/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_header">index f3d5817c4ef0..258902db14bf 100644</span>
<span class="p_header">--- a/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_header">+++ b/Documentation/ABI/testing/sysfs-devices-system-cpu</span>
<span class="p_chunk">@@ -373,3 +373,19 @@</span> <span class="p_context"> Contact:	Linux kernel mailing list &lt;linux-kernel@vger.kernel.org&gt;</span>
 Description:	information about CPUs heterogeneity.
 
 		cpu_capacity: capacity of cpu#.
<span class="p_add">+</span>
<span class="p_add">+What:		/sys/devices/system/cpu/vulnerabilities</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/meltdown</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/spectre_v1</span>
<span class="p_add">+		/sys/devices/system/cpu/vulnerabilities/spectre_v2</span>
<span class="p_add">+Date:		January 2018</span>
<span class="p_add">+Contact:	Linux kernel mailing list &lt;linux-kernel@vger.kernel.org&gt;</span>
<span class="p_add">+Description:	Information about CPU vulnerabilities</span>
<span class="p_add">+</span>
<span class="p_add">+		The files are named after the code names of CPU</span>
<span class="p_add">+		vulnerabilities. The output of those files reflects the</span>
<span class="p_add">+		state of the CPUs in the system. Possible output values:</span>
<span class="p_add">+</span>
<span class="p_add">+		&quot;Not affected&quot;	  CPU is not affected by the vulnerability</span>
<span class="p_add">+		&quot;Vulnerable&quot;	  CPU is affected and no mitigation in effect</span>
<span class="p_add">+		&quot;Mitigation: $M&quot;  CPU is affected and mitigation $M is in effect</span>
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index 520fdec15bbb..8122b5f98ea1 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2599,6 +2599,11 @@</span> <span class="p_context"></span>
 	nosmt		[KNL,S390] Disable symmetric multithreading (SMT).
 			Equivalent to smt=1.
 
<span class="p_add">+	nospectre_v2	[X86] Disable all mitigations for the Spectre variant 2</span>
<span class="p_add">+			(indirect branch prediction) vulnerability. System may</span>
<span class="p_add">+			allow data leaks with this option, which is equivalent</span>
<span class="p_add">+			to spectre_v2=off.</span>
<span class="p_add">+</span>
 	noxsave		[BUGS=X86] Disables x86 extended register state save
 			and restore using xsave. The kernel will fallback to
 			enabling legacy floating-point and sse state.
<span class="p_chunk">@@ -2685,8 +2690,6 @@</span> <span class="p_context"></span>
 			steal time is computed, but won&#39;t influence scheduler
 			behaviour
 
<span class="p_del">-	nopti		[X86-64] Disable kernel page table isolation</span>
<span class="p_del">-</span>
 	nolapic		[X86-32,APIC] Do not enable or use the local APIC.
 
 	nolapic_timer	[X86-32,APIC] Do not use the local APIC timer.
<span class="p_chunk">@@ -3255,11 +3258,20 @@</span> <span class="p_context"></span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_del">-	pti=		[X86_64]</span>
<span class="p_del">-			Control user/kernel address space isolation:</span>
<span class="p_del">-			on - enable</span>
<span class="p_del">-			off - disable</span>
<span class="p_del">-			auto - default setting</span>
<span class="p_add">+	pti=		[X86_64] Control Page Table Isolation of user and</span>
<span class="p_add">+			kernel address spaces.  Disabling this feature</span>
<span class="p_add">+			removes hardening, but improves performance of</span>
<span class="p_add">+			system calls and interrupts.</span>
<span class="p_add">+</span>
<span class="p_add">+			on   - unconditionally enable</span>
<span class="p_add">+			off  - unconditionally disable</span>
<span class="p_add">+			auto - kernel detects whether your CPU model is</span>
<span class="p_add">+			       vulnerable to issues that PTI mitigates</span>
<span class="p_add">+</span>
<span class="p_add">+			Not specifying this option is equivalent to pti=auto.</span>
<span class="p_add">+</span>
<span class="p_add">+	nopti		[X86_64]</span>
<span class="p_add">+			Equivalent to pti=off</span>
 
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
<span class="p_chunk">@@ -3901,6 +3913,29 @@</span> <span class="p_context"></span>
 	sonypi.*=	[HW] Sony Programmable I/O Control Device driver
 			See Documentation/laptops/sonypi.txt
 
<span class="p_add">+	spectre_v2=	[X86] Control mitigation of Spectre variant 2</span>
<span class="p_add">+			(indirect branch speculation) vulnerability.</span>
<span class="p_add">+</span>
<span class="p_add">+			on   - unconditionally enable</span>
<span class="p_add">+			off  - unconditionally disable</span>
<span class="p_add">+			auto - kernel detects whether your CPU model is</span>
<span class="p_add">+			       vulnerable</span>
<span class="p_add">+</span>
<span class="p_add">+			Selecting &#39;on&#39; will, and &#39;auto&#39; may, choose a</span>
<span class="p_add">+			mitigation method at run time according to the</span>
<span class="p_add">+			CPU, the available microcode, the setting of the</span>
<span class="p_add">+			CONFIG_RETPOLINE configuration option, and the</span>
<span class="p_add">+			compiler with which the kernel was built.</span>
<span class="p_add">+</span>
<span class="p_add">+			Specific mitigations can also be selected manually:</span>
<span class="p_add">+</span>
<span class="p_add">+			retpoline	  - replace indirect branches</span>
<span class="p_add">+			retpoline,generic - google&#39;s original retpoline</span>
<span class="p_add">+			retpoline,amd     - AMD-specific minimal thunk</span>
<span class="p_add">+</span>
<span class="p_add">+			Not specifying this option is equivalent to</span>
<span class="p_add">+			spectre_v2=auto.</span>
<span class="p_add">+</span>
 	spia_io_base=	[HW,MTD]
 	spia_fio_base=
 	spia_pedr=
<span class="p_header">diff --git a/Documentation/x86/pti.txt b/Documentation/x86/pti.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..d11eff61fc9a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/x86/pti.txt</span>
<span class="p_chunk">@@ -0,0 +1,186 @@</span> <span class="p_context"></span>
<span class="p_add">+Overview</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+Page Table Isolation (pti, previously known as KAISER[1]) is a</span>
<span class="p_add">+countermeasure against attacks on the shared user/kernel address</span>
<span class="p_add">+space such as the &quot;Meltdown&quot; approach[2].</span>
<span class="p_add">+</span>
<span class="p_add">+To mitigate this class of attacks, we create an independent set of</span>
<span class="p_add">+page tables for use only when running userspace applications.  When</span>
<span class="p_add">+the kernel is entered via syscalls, interrupts or exceptions, the</span>
<span class="p_add">+page tables are switched to the full &quot;kernel&quot; copy.  When the system</span>
<span class="p_add">+switches back to user mode, the user copy is used again.</span>
<span class="p_add">+</span>
<span class="p_add">+The userspace page tables contain only a minimal amount of kernel</span>
<span class="p_add">+data: only what is needed to enter/exit the kernel such as the</span>
<span class="p_add">+entry/exit functions themselves and the interrupt descriptor table</span>
<span class="p_add">+(IDT).  There are a few strictly unnecessary things that get mapped</span>
<span class="p_add">+such as the first C function when entering an interrupt (see</span>
<span class="p_add">+comments in pti.c).</span>
<span class="p_add">+</span>
<span class="p_add">+This approach helps to ensure that side-channel attacks leveraging</span>
<span class="p_add">+the paging structures do not function when PTI is enabled.  It can be</span>
<span class="p_add">+enabled by setting CONFIG_PAGE_TABLE_ISOLATION=y at compile time.</span>
<span class="p_add">+Once enabled at compile-time, it can be disabled at boot with the</span>
<span class="p_add">+&#39;nopti&#39; or &#39;pti=&#39; kernel parameters (see kernel-parameters.txt).</span>
<span class="p_add">+</span>
<span class="p_add">+Page Table Management</span>
<span class="p_add">+=====================</span>
<span class="p_add">+</span>
<span class="p_add">+When PTI is enabled, the kernel manages two sets of page tables.</span>
<span class="p_add">+The first set is very similar to the single set which is present in</span>
<span class="p_add">+kernels without PTI.  This includes a complete mapping of userspace</span>
<span class="p_add">+that the kernel can use for things like copy_to_user().</span>
<span class="p_add">+</span>
<span class="p_add">+Although _complete_, the user portion of the kernel page tables is</span>
<span class="p_add">+crippled by setting the NX bit in the top level.  This ensures</span>
<span class="p_add">+that any missed kernel-&gt;user CR3 switch will immediately crash</span>
<span class="p_add">+userspace upon executing its first instruction.</span>
<span class="p_add">+</span>
<span class="p_add">+The userspace page tables map only the kernel data needed to enter</span>
<span class="p_add">+and exit the kernel.  This data is entirely contained in the &#39;struct</span>
<span class="p_add">+cpu_entry_area&#39; structure which is placed in the fixmap which gives</span>
<span class="p_add">+each CPU&#39;s copy of the area a compile-time-fixed virtual address.</span>
<span class="p_add">+</span>
<span class="p_add">+For new userspace mappings, the kernel makes the entries in its</span>
<span class="p_add">+page tables like normal.  The only difference is when the kernel</span>
<span class="p_add">+makes entries in the top (PGD) level.  In addition to setting the</span>
<span class="p_add">+entry in the main kernel PGD, a copy of the entry is made in the</span>
<span class="p_add">+userspace page tables&#39; PGD.</span>
<span class="p_add">+</span>
<span class="p_add">+This sharing at the PGD level also inherently shares all the lower</span>
<span class="p_add">+layers of the page tables.  This leaves a single, shared set of</span>
<span class="p_add">+userspace page tables to manage.  One PTE to lock, one set of</span>
<span class="p_add">+accessed bits, dirty bits, etc...</span>
<span class="p_add">+</span>
<span class="p_add">+Overhead</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+Protection against side-channel attacks is important.  But,</span>
<span class="p_add">+this protection comes at a cost:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Increased Memory Use</span>
<span class="p_add">+  a. Each process now needs an order-1 PGD instead of order-0.</span>
<span class="p_add">+     (Consumes an additional 4k per process).</span>
<span class="p_add">+  b. The &#39;cpu_entry_area&#39; structure must be 2MB in size and 2MB</span>
<span class="p_add">+     aligned so that it can be mapped by setting a single PMD</span>
<span class="p_add">+     entry.  This consumes nearly 2MB of RAM once the kernel</span>
<span class="p_add">+     is decompressed, but no space in the kernel image itself.</span>
<span class="p_add">+</span>
<span class="p_add">+2. Runtime Cost</span>
<span class="p_add">+  a. CR3 manipulation to switch between the page table copies</span>
<span class="p_add">+     must be done at interrupt, syscall, and exception entry</span>
<span class="p_add">+     and exit (it can be skipped when the kernel is interrupted,</span>
<span class="p_add">+     though.)  Moves to CR3 are on the order of a hundred</span>
<span class="p_add">+     cycles, and are required at every entry and exit.</span>
<span class="p_add">+  b. A &quot;trampoline&quot; must be used for SYSCALL entry.  This</span>
<span class="p_add">+     trampoline depends on a smaller set of resources than the</span>
<span class="p_add">+     non-PTI SYSCALL entry code, so requires mapping fewer</span>
<span class="p_add">+     things into the userspace page tables.  The downside is</span>
<span class="p_add">+     that stacks must be switched at entry time.</span>
<span class="p_add">+  d. Global pages are disabled for all kernel structures not</span>
<span class="p_add">+     mapped into both kernel and userspace page tables.  This</span>
<span class="p_add">+     feature of the MMU allows different processes to share TLB</span>
<span class="p_add">+     entries mapping the kernel.  Losing the feature means more</span>
<span class="p_add">+     TLB misses after a context switch.  The actual loss of</span>
<span class="p_add">+     performance is very small, however, never exceeding 1%.</span>
<span class="p_add">+  d. Process Context IDentifiers (PCID) is a CPU feature that</span>
<span class="p_add">+     allows us to skip flushing the entire TLB when switching page</span>
<span class="p_add">+     tables by setting a special bit in CR3 when the page tables</span>
<span class="p_add">+     are changed.  This makes switching the page tables (at context</span>
<span class="p_add">+     switch, or kernel entry/exit) cheaper.  But, on systems with</span>
<span class="p_add">+     PCID support, the context switch code must flush both the user</span>
<span class="p_add">+     and kernel entries out of the TLB.  The user PCID TLB flush is</span>
<span class="p_add">+     deferred until the exit to userspace, minimizing the cost.</span>
<span class="p_add">+     See intel.com/sdm for the gory PCID/INVPCID details.</span>
<span class="p_add">+  e. The userspace page tables must be populated for each new</span>
<span class="p_add">+     process.  Even without PTI, the shared kernel mappings</span>
<span class="p_add">+     are created by copying top-level (PGD) entries into each</span>
<span class="p_add">+     new process.  But, with PTI, there are now *two* kernel</span>
<span class="p_add">+     mappings: one in the kernel page tables that maps everything</span>
<span class="p_add">+     and one for the entry/exit structures.  At fork(), we need to</span>
<span class="p_add">+     copy both.</span>
<span class="p_add">+  f. In addition to the fork()-time copying, there must also</span>
<span class="p_add">+     be an update to the userspace PGD any time a set_pgd() is done</span>
<span class="p_add">+     on a PGD used to map userspace.  This ensures that the kernel</span>
<span class="p_add">+     and userspace copies always map the same userspace</span>
<span class="p_add">+     memory.</span>
<span class="p_add">+  g. On systems without PCID support, each CR3 write flushes</span>
<span class="p_add">+     the entire TLB.  That means that each syscall, interrupt</span>
<span class="p_add">+     or exception flushes the TLB.</span>
<span class="p_add">+  h. INVPCID is a TLB-flushing instruction which allows flushing</span>
<span class="p_add">+     of TLB entries for non-current PCIDs.  Some systems support</span>
<span class="p_add">+     PCIDs, but do not support INVPCID.  On these systems, addresses</span>
<span class="p_add">+     can only be flushed from the TLB for the current PCID.  When</span>
<span class="p_add">+     flushing a kernel address, we need to flush all PCIDs, so a</span>
<span class="p_add">+     single kernel address flush will require a TLB-flushing CR3</span>
<span class="p_add">+     write upon the next use of every PCID.</span>
<span class="p_add">+</span>
<span class="p_add">+Possible Future Work</span>
<span class="p_add">+====================</span>
<span class="p_add">+1. We can be more careful about not actually writing to CR3</span>
<span class="p_add">+   unless its value is actually changed.</span>
<span class="p_add">+2. Allow PTI to be enabled/disabled at runtime in addition to the</span>
<span class="p_add">+   boot-time switching.</span>
<span class="p_add">+</span>
<span class="p_add">+Testing</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+To test stability of PTI, the following test procedure is recommended,</span>
<span class="p_add">+ideally doing all of these in parallel:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Set CONFIG_DEBUG_ENTRY=y</span>
<span class="p_add">+2. Run several copies of all of the tools/testing/selftests/x86/ tests</span>
<span class="p_add">+   (excluding MPX and protection_keys) in a loop on multiple CPUs for</span>
<span class="p_add">+   several minutes.  These tests frequently uncover corner cases in the</span>
<span class="p_add">+   kernel entry code.  In general, old kernels might cause these tests</span>
<span class="p_add">+   themselves to crash, but they should never crash the kernel.</span>
<span class="p_add">+3. Run the &#39;perf&#39; tool in a mode (top or record) that generates many</span>
<span class="p_add">+   frequent performance monitoring non-maskable interrupts (see &quot;NMI&quot;</span>
<span class="p_add">+   in /proc/interrupts).  This exercises the NMI entry/exit code which</span>
<span class="p_add">+   is known to trigger bugs in code paths that did not expect to be</span>
<span class="p_add">+   interrupted, including nested NMIs.  Using &quot;-c&quot; boosts the rate of</span>
<span class="p_add">+   NMIs, and using two -c with separate counters encourages nested NMIs</span>
<span class="p_add">+   and less deterministic behavior.</span>
<span class="p_add">+</span>
<span class="p_add">+	while true; do perf record -c 10000 -e instructions,cycles -a sleep 10; done</span>
<span class="p_add">+</span>
<span class="p_add">+4. Launch a KVM virtual machine.</span>
<span class="p_add">+5. Run 32-bit binaries on systems supporting the SYSCALL instruction.</span>
<span class="p_add">+   This has been a lightly-tested code path and needs extra scrutiny.</span>
<span class="p_add">+</span>
<span class="p_add">+Debugging</span>
<span class="p_add">+=========</span>
<span class="p_add">+</span>
<span class="p_add">+Bugs in PTI cause a few different signatures of crashes</span>
<span class="p_add">+that are worth noting here.</span>
<span class="p_add">+</span>
<span class="p_add">+ * Failures of the selftests/x86 code.  Usually a bug in one of the</span>
<span class="p_add">+   more obscure corners of entry_64.S</span>
<span class="p_add">+ * Crashes in early boot, especially around CPU bringup.  Bugs</span>
<span class="p_add">+   in the trampoline code or mappings cause these.</span>
<span class="p_add">+ * Crashes at the first interrupt.  Caused by bugs in entry_64.S,</span>
<span class="p_add">+   like screwing up a page table switch.  Also caused by</span>
<span class="p_add">+   incorrectly mapping the IRQ handler entry code.</span>
<span class="p_add">+ * Crashes at the first NMI.  The NMI code is separate from main</span>
<span class="p_add">+   interrupt handlers and can have bugs that do not affect</span>
<span class="p_add">+   normal interrupts.  Also caused by incorrectly mapping NMI</span>
<span class="p_add">+   code.  NMIs that interrupt the entry code must be very</span>
<span class="p_add">+   careful and can be the cause of crashes that show up when</span>
<span class="p_add">+   running perf.</span>
<span class="p_add">+ * Kernel crashes at the first exit to userspace.  entry_64.S</span>
<span class="p_add">+   bugs, or failing to map some of the exit code.</span>
<span class="p_add">+ * Crashes at first interrupt that interrupts userspace. The paths</span>
<span class="p_add">+   in entry_64.S that return to userspace are sometimes separate</span>
<span class="p_add">+   from the ones that return to the kernel.</span>
<span class="p_add">+ * Double faults: overflowing the kernel stack because of page</span>
<span class="p_add">+   faults upon page faults.  Caused by touching non-pti-mapped</span>
<span class="p_add">+   data in the entry code, or forgetting to switch to kernel</span>
<span class="p_add">+   CR3 before calling into C functions which are not pti-mapped.</span>
<span class="p_add">+ * Userspace segfaults early in boot, sometimes manifesting</span>
<span class="p_add">+   as mount(8) failing to mount the rootfs.  These have</span>
<span class="p_add">+   tended to be TLB invalidation issues.  Usually invalidating</span>
<span class="p_add">+   the wrong PCID, or otherwise missing an invalidation.</span>
<span class="p_add">+</span>
<span class="p_add">+1. https://gruss.cc/files/kaiser.pdf</span>
<span class="p_add">+2. https://meltdownattack.com/meltdown.pdf</span>
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index a67c5179052a..4951305eb867 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 4
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 13</span>
<span class="p_add">+SUBLEVEL = 14</span>
 EXTRAVERSION =
 NAME = Petit Gorille
 
<span class="p_header">diff --git a/arch/mips/kernel/process.c b/arch/mips/kernel/process.c</span>
<span class="p_header">index c5ff6bfe2825..2f2d176396aa 100644</span>
<span class="p_header">--- a/arch/mips/kernel/process.c</span>
<span class="p_header">+++ b/arch/mips/kernel/process.c</span>
<span class="p_chunk">@@ -705,6 +705,18 @@</span> <span class="p_context"> int mips_set_process_fp_mode(struct task_struct *task, unsigned int value)</span>
 	struct task_struct *t;
 	int max_users;
 
<span class="p_add">+	/* If nothing to change, return right away, successfully.  */</span>
<span class="p_add">+	if (value == mips_get_process_fp_mode(task))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Only accept a mode change if 64-bit FP enabled for o32.  */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_MIPS_O32_FP64_SUPPORT))</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* And only for o32 tasks.  */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_64BIT) &amp;&amp; !test_thread_flag(TIF_32BIT_REGS))</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+</span>
 	/* Check the value is valid */
 	if (value &amp; ~known_bits)
 		return -EOPNOTSUPP;
<span class="p_header">diff --git a/arch/mips/kernel/ptrace.c b/arch/mips/kernel/ptrace.c</span>
<span class="p_header">index 5a09c2901a76..c552c20237d4 100644</span>
<span class="p_header">--- a/arch/mips/kernel/ptrace.c</span>
<span class="p_header">+++ b/arch/mips/kernel/ptrace.c</span>
<span class="p_chunk">@@ -410,63 +410,160 @@</span> <span class="p_context"> static int gpr64_set(struct task_struct *target,</span>
 
 #endif /* CONFIG_64BIT */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the floating-point context to the supplied NT_PRFPREG buffer,</span>
<span class="p_add">+ * !CONFIG_CPU_HAS_MSA variant.  FP context&#39;s general register slots</span>
<span class="p_add">+ * correspond 1:1 to buffer slots.  Only general registers are copied.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int fpr_get_fpa(struct task_struct *target,</span>
<span class="p_add">+		       unsigned int *pos, unsigned int *count,</span>
<span class="p_add">+		       void **kbuf, void __user **ubuf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return user_regset_copyout(pos, count, kbuf, ubuf,</span>
<span class="p_add">+				   &amp;target-&gt;thread.fpu,</span>
<span class="p_add">+				   0, NUM_FPU_REGS * sizeof(elf_fpreg_t));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the floating-point context to the supplied NT_PRFPREG buffer,</span>
<span class="p_add">+ * CONFIG_CPU_HAS_MSA variant.  Only lower 64 bits of FP context&#39;s</span>
<span class="p_add">+ * general register slots are copied to buffer slots.  Only general</span>
<span class="p_add">+ * registers are copied.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int fpr_get_msa(struct task_struct *target,</span>
<span class="p_add">+		       unsigned int *pos, unsigned int *count,</span>
<span class="p_add">+		       void **kbuf, void __user **ubuf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+	u64 fpr_val;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(fpr_val) != sizeof(elf_fpreg_t));</span>
<span class="p_add">+	for (i = 0; i &lt; NUM_FPU_REGS; i++) {</span>
<span class="p_add">+		fpr_val = get_fpr64(&amp;target-&gt;thread.fpu.fpr[i], 0);</span>
<span class="p_add">+		err = user_regset_copyout(pos, count, kbuf, ubuf,</span>
<span class="p_add">+					  &amp;fpr_val, i * sizeof(elf_fpreg_t),</span>
<span class="p_add">+					  (i + 1) * sizeof(elf_fpreg_t));</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			return err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the floating-point context to the supplied NT_PRFPREG buffer.</span>
<span class="p_add">+ * Choose the appropriate helper for general registers, and then copy</span>
<span class="p_add">+ * the FCSR register separately.</span>
<span class="p_add">+ */</span>
 static int fpr_get(struct task_struct *target,
 		   const struct user_regset *regset,
 		   unsigned int pos, unsigned int count,
 		   void *kbuf, void __user *ubuf)
 {
<span class="p_del">-	unsigned i;</span>
<span class="p_add">+	const int fcr31_pos = NUM_FPU_REGS * sizeof(elf_fpreg_t);</span>
 	int err;
<span class="p_del">-	u64 fpr_val;</span>
 
<span class="p_del">-	/* XXX fcr31  */</span>
<span class="p_add">+	if (sizeof(target-&gt;thread.fpu.fpr[0]) == sizeof(elf_fpreg_t))</span>
<span class="p_add">+		err = fpr_get_fpa(target, &amp;pos, &amp;count, &amp;kbuf, &amp;ubuf);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		err = fpr_get_msa(target, &amp;pos, &amp;count, &amp;kbuf, &amp;ubuf);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		return err;</span>
 
<span class="p_del">-	if (sizeof(target-&gt;thread.fpu.fpr[i]) == sizeof(elf_fpreg_t))</span>
<span class="p_del">-		return user_regset_copyout(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,</span>
<span class="p_del">-					   &amp;target-&gt;thread.fpu,</span>
<span class="p_del">-					   0, sizeof(elf_fpregset_t));</span>
<span class="p_add">+	err = user_regset_copyout(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,</span>
<span class="p_add">+				  &amp;target-&gt;thread.fpu.fcr31,</span>
<span class="p_add">+				  fcr31_pos, fcr31_pos + sizeof(u32));</span>
 
<span class="p_del">-	for (i = 0; i &lt; NUM_FPU_REGS; i++) {</span>
<span class="p_del">-		fpr_val = get_fpr64(&amp;target-&gt;thread.fpu.fpr[i], 0);</span>
<span class="p_del">-		err = user_regset_copyout(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,</span>
<span class="p_del">-					  &amp;fpr_val, i * sizeof(elf_fpreg_t),</span>
<span class="p_del">-					  (i + 1) * sizeof(elf_fpreg_t));</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the supplied NT_PRFPREG buffer to the floating-point context,</span>
<span class="p_add">+ * !CONFIG_CPU_HAS_MSA variant.   Buffer slots correspond 1:1 to FP</span>
<span class="p_add">+ * context&#39;s general register slots.  Only general registers are copied.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int fpr_set_fpa(struct task_struct *target,</span>
<span class="p_add">+		       unsigned int *pos, unsigned int *count,</span>
<span class="p_add">+		       const void **kbuf, const void __user **ubuf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return user_regset_copyin(pos, count, kbuf, ubuf,</span>
<span class="p_add">+				  &amp;target-&gt;thread.fpu,</span>
<span class="p_add">+				  0, NUM_FPU_REGS * sizeof(elf_fpreg_t));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the supplied NT_PRFPREG buffer to the floating-point context,</span>
<span class="p_add">+ * CONFIG_CPU_HAS_MSA variant.  Buffer slots are copied to lower 64</span>
<span class="p_add">+ * bits only of FP context&#39;s general register slots.  Only general</span>
<span class="p_add">+ * registers are copied.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int fpr_set_msa(struct task_struct *target,</span>
<span class="p_add">+		       unsigned int *pos, unsigned int *count,</span>
<span class="p_add">+		       const void **kbuf, const void __user **ubuf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+	u64 fpr_val;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(fpr_val) != sizeof(elf_fpreg_t));</span>
<span class="p_add">+	for (i = 0; i &lt; NUM_FPU_REGS &amp;&amp; *count &gt; 0; i++) {</span>
<span class="p_add">+		err = user_regset_copyin(pos, count, kbuf, ubuf,</span>
<span class="p_add">+					 &amp;fpr_val, i * sizeof(elf_fpreg_t),</span>
<span class="p_add">+					 (i + 1) * sizeof(elf_fpreg_t));</span>
 		if (err)
 			return err;
<span class="p_add">+		set_fpr64(&amp;target-&gt;thread.fpu.fpr[i], 0, fpr_val);</span>
 	}
 
 	return 0;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Copy the supplied NT_PRFPREG buffer to the floating-point context.</span>
<span class="p_add">+ * Choose the appropriate helper for general registers, and then copy</span>
<span class="p_add">+ * the FCSR register separately.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We optimize for the case where `count % sizeof(elf_fpreg_t) == 0&#39;,</span>
<span class="p_add">+ * which is supposed to have been guaranteed by the kernel before</span>
<span class="p_add">+ * calling us, e.g. in `ptrace_regset&#39;.  We enforce that requirement,</span>
<span class="p_add">+ * so that we can safely avoid preinitializing temporaries for</span>
<span class="p_add">+ * partial register writes.</span>
<span class="p_add">+ */</span>
 static int fpr_set(struct task_struct *target,
 		   const struct user_regset *regset,
 		   unsigned int pos, unsigned int count,
 		   const void *kbuf, const void __user *ubuf)
 {
<span class="p_del">-	unsigned i;</span>
<span class="p_add">+	const int fcr31_pos = NUM_FPU_REGS * sizeof(elf_fpreg_t);</span>
<span class="p_add">+	u32 fcr31;</span>
 	int err;
<span class="p_del">-	u64 fpr_val;</span>
 
<span class="p_del">-	/* XXX fcr31  */</span>
<span class="p_add">+	BUG_ON(count % sizeof(elf_fpreg_t));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pos + count &gt; sizeof(elf_fpregset_t))</span>
<span class="p_add">+		return -EIO;</span>
 
 	init_fp_ctx(target);
 
<span class="p_del">-	if (sizeof(target-&gt;thread.fpu.fpr[i]) == sizeof(elf_fpreg_t))</span>
<span class="p_del">-		return user_regset_copyin(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,</span>
<span class="p_del">-					  &amp;target-&gt;thread.fpu,</span>
<span class="p_del">-					  0, sizeof(elf_fpregset_t));</span>
<span class="p_add">+	if (sizeof(target-&gt;thread.fpu.fpr[0]) == sizeof(elf_fpreg_t))</span>
<span class="p_add">+		err = fpr_set_fpa(target, &amp;pos, &amp;count, &amp;kbuf, &amp;ubuf);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		err = fpr_set_msa(target, &amp;pos, &amp;count, &amp;kbuf, &amp;ubuf);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		return err;</span>
 
<span class="p_del">-	BUILD_BUG_ON(sizeof(fpr_val) != sizeof(elf_fpreg_t));</span>
<span class="p_del">-	for (i = 0; i &lt; NUM_FPU_REGS &amp;&amp; count &gt;= sizeof(elf_fpreg_t); i++) {</span>
<span class="p_add">+	if (count &gt; 0) {</span>
 		err = user_regset_copyin(&amp;pos, &amp;count, &amp;kbuf, &amp;ubuf,
<span class="p_del">-					 &amp;fpr_val, i * sizeof(elf_fpreg_t),</span>
<span class="p_del">-					 (i + 1) * sizeof(elf_fpreg_t));</span>
<span class="p_add">+					 &amp;fcr31,</span>
<span class="p_add">+					 fcr31_pos, fcr31_pos + sizeof(u32));</span>
 		if (err)
 			return err;
<span class="p_del">-		set_fpr64(&amp;target-&gt;thread.fpu.fpr[i], 0, fpr_val);</span>
<span class="p_add">+</span>
<span class="p_add">+		ptrace_setfcr31(target, fcr31);</span>
 	}
 
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return err;</span>
 }
 
 enum mips_regset {
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu.c b/arch/powerpc/kvm/book3s_64_mmu.c</span>
<span class="p_header">index 29ebe2fd5867..a93d719edc90 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu.c</span>
<span class="p_chunk">@@ -235,6 +235,7 @@</span> <span class="p_context"> static int kvmppc_mmu_book3s_64_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,</span>
 		gpte-&gt;may_read = true;
 		gpte-&gt;may_write = true;
 		gpte-&gt;page_size = MMU_PAGE_4K;
<span class="p_add">+		gpte-&gt;wimg = HPTE_R_M;</span>
 
 		return 0;
 	}
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index 59247af5fd45..2645d484e945 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -65,11 +65,17 @@</span> <span class="p_context"> struct kvm_resize_hpt {</span>
 	u32 order;
 
 	/* These fields protected by kvm-&gt;lock */
<span class="p_add">+</span>
<span class="p_add">+	/* Possible values and their usage:</span>
<span class="p_add">+	 *  &lt;0     an error occurred during allocation,</span>
<span class="p_add">+	 *  -EBUSY allocation is in the progress,</span>
<span class="p_add">+	 *  0      allocation made successfuly.</span>
<span class="p_add">+	 */</span>
 	int error;
<span class="p_del">-	bool prepare_done;</span>
 
<span class="p_del">-	/* Private to the work thread, until prepare_done is true,</span>
<span class="p_del">-	 * then protected by kvm-&gt;resize_hpt_sem */</span>
<span class="p_add">+	/* Private to the work thread, until error != -EBUSY,</span>
<span class="p_add">+	 * then protected by kvm-&gt;lock.</span>
<span class="p_add">+	 */</span>
 	struct kvm_hpt_info hpt;
 };
 
<span class="p_chunk">@@ -159,8 +165,6 @@</span> <span class="p_context"> long kvmppc_alloc_reset_hpt(struct kvm *kvm, int order)</span>
 		 * Reset all the reverse-mapping chains for all memslots
 		 */
 		kvmppc_rmap_reset(kvm);
<span class="p_del">-		/* Ensure that each vcpu will flush its TLB on next entry. */</span>
<span class="p_del">-		cpumask_setall(&amp;kvm-&gt;arch.need_tlb_flush);</span>
 		err = 0;
 		goto out;
 	}
<span class="p_chunk">@@ -176,6 +180,10 @@</span> <span class="p_context"> long kvmppc_alloc_reset_hpt(struct kvm *kvm, int order)</span>
 	kvmppc_set_hpt(kvm, &amp;info);
 
 out:
<span class="p_add">+	if (err == 0)</span>
<span class="p_add">+		/* Ensure that each vcpu will flush its TLB on next entry. */</span>
<span class="p_add">+		cpumask_setall(&amp;kvm-&gt;arch.need_tlb_flush);</span>
<span class="p_add">+</span>
 	mutex_unlock(&amp;kvm-&gt;lock);
 	return err;
 }
<span class="p_chunk">@@ -1424,16 +1432,20 @@</span> <span class="p_context"> static void resize_hpt_pivot(struct kvm_resize_hpt *resize)</span>
 
 static void resize_hpt_release(struct kvm *kvm, struct kvm_resize_hpt *resize)
 {
<span class="p_del">-	BUG_ON(kvm-&gt;arch.resize_hpt != resize);</span>
<span class="p_add">+	if (WARN_ON(!mutex_is_locked(&amp;kvm-&gt;lock)))</span>
<span class="p_add">+		return;</span>
 
 	if (!resize)
 		return;
 
<span class="p_del">-	if (resize-&gt;hpt.virt)</span>
<span class="p_del">-		kvmppc_free_hpt(&amp;resize-&gt;hpt);</span>
<span class="p_add">+	if (resize-&gt;error != -EBUSY) {</span>
<span class="p_add">+		if (resize-&gt;hpt.virt)</span>
<span class="p_add">+			kvmppc_free_hpt(&amp;resize-&gt;hpt);</span>
<span class="p_add">+		kfree(resize);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	kvm-&gt;arch.resize_hpt = NULL;</span>
<span class="p_del">-	kfree(resize);</span>
<span class="p_add">+	if (kvm-&gt;arch.resize_hpt == resize)</span>
<span class="p_add">+		kvm-&gt;arch.resize_hpt = NULL;</span>
 }
 
 static void resize_hpt_prepare_work(struct work_struct *work)
<span class="p_chunk">@@ -1442,17 +1454,41 @@</span> <span class="p_context"> static void resize_hpt_prepare_work(struct work_struct *work)</span>
 						     struct kvm_resize_hpt,
 						     work);
 	struct kvm *kvm = resize-&gt;kvm;
<span class="p_del">-	int err;</span>
<span class="p_add">+	int err = 0;</span>
 
<span class="p_del">-	resize_hpt_debug(resize, &quot;resize_hpt_prepare_work(): order = %d\n&quot;,</span>
<span class="p_del">-			 resize-&gt;order);</span>
<span class="p_del">-</span>
<span class="p_del">-	err = resize_hpt_allocate(resize);</span>
<span class="p_add">+	if (WARN_ON(resize-&gt;error != -EBUSY))</span>
<span class="p_add">+		return;</span>
 
 	mutex_lock(&amp;kvm-&gt;lock);
 
<span class="p_add">+	/* Request is still current? */</span>
<span class="p_add">+	if (kvm-&gt;arch.resize_hpt == resize) {</span>
<span class="p_add">+		/* We may request large allocations here:</span>
<span class="p_add">+		 * do not sleep with kvm-&gt;lock held for a while.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mutex_unlock(&amp;kvm-&gt;lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		resize_hpt_debug(resize, &quot;resize_hpt_prepare_work(): order = %d\n&quot;,</span>
<span class="p_add">+				 resize-&gt;order);</span>
<span class="p_add">+</span>
<span class="p_add">+		err = resize_hpt_allocate(resize);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* We have strict assumption about -EBUSY</span>
<span class="p_add">+		 * when preparing for HPT resize.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (WARN_ON(err == -EBUSY))</span>
<span class="p_add">+			err = -EINPROGRESS;</span>
<span class="p_add">+</span>
<span class="p_add">+		mutex_lock(&amp;kvm-&gt;lock);</span>
<span class="p_add">+		/* It is possible that kvm-&gt;arch.resize_hpt != resize</span>
<span class="p_add">+		 * after we grab kvm-&gt;lock again.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	resize-&gt;error = err;
<span class="p_del">-	resize-&gt;prepare_done = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (kvm-&gt;arch.resize_hpt != resize)</span>
<span class="p_add">+		resize_hpt_release(kvm, resize);</span>
 
 	mutex_unlock(&amp;kvm-&gt;lock);
 }
<span class="p_chunk">@@ -1477,14 +1513,12 @@</span> <span class="p_context"> long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,</span>
 
 	if (resize) {
 		if (resize-&gt;order == shift) {
<span class="p_del">-			/* Suitable resize in progress */</span>
<span class="p_del">-			if (resize-&gt;prepare_done) {</span>
<span class="p_del">-				ret = resize-&gt;error;</span>
<span class="p_del">-				if (ret != 0)</span>
<span class="p_del">-					resize_hpt_release(kvm, resize);</span>
<span class="p_del">-			} else {</span>
<span class="p_add">+			/* Suitable resize in progress? */</span>
<span class="p_add">+			ret = resize-&gt;error;</span>
<span class="p_add">+			if (ret == -EBUSY)</span>
 				ret = 100; /* estimated time in ms */
<span class="p_del">-			}</span>
<span class="p_add">+			else if (ret)</span>
<span class="p_add">+				resize_hpt_release(kvm, resize);</span>
 
 			goto out;
 		}
<span class="p_chunk">@@ -1504,6 +1538,8 @@</span> <span class="p_context"> long kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,</span>
 		ret = -ENOMEM;
 		goto out;
 	}
<span class="p_add">+</span>
<span class="p_add">+	resize-&gt;error = -EBUSY;</span>
 	resize-&gt;order = shift;
 	resize-&gt;kvm = kvm;
 	INIT_WORK(&amp;resize-&gt;work, resize_hpt_prepare_work);
<span class="p_chunk">@@ -1558,16 +1594,12 @@</span> <span class="p_context"> long kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,</span>
 	if (!resize || (resize-&gt;order != shift))
 		goto out;
 
<span class="p_del">-	ret = -EBUSY;</span>
<span class="p_del">-	if (!resize-&gt;prepare_done)</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
 	ret = resize-&gt;error;
<span class="p_del">-	if (ret != 0)</span>
<span class="p_add">+	if (ret)</span>
 		goto out;
 
 	ret = resize_hpt_rehash(resize);
<span class="p_del">-	if (ret != 0)</span>
<span class="p_add">+	if (ret)</span>
 		goto out;
 
 	resize_hpt_pivot(resize);
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_pr.c b/arch/powerpc/kvm/book3s_pr.c</span>
<span class="p_header">index 69a09444d46e..e2ef16198456 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_pr.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_pr.c</span>
<span class="p_chunk">@@ -60,6 +60,7 @@</span> <span class="p_context"> static void kvmppc_giveup_fac(struct kvm_vcpu *vcpu, ulong fac);</span>
 #define MSR_USER32 MSR_USER
 #define MSR_USER64 MSR_USER
 #define HW_PAGE_SIZE PAGE_SIZE
<span class="p_add">+#define HPTE_R_M   _PAGE_COHERENT</span>
 #endif
 
 static bool kvmppc_is_split_real(struct kvm_vcpu *vcpu)
<span class="p_chunk">@@ -557,6 +558,7 @@</span> <span class="p_context"> int kvmppc_handle_pagefault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 		pte.eaddr = eaddr;
 		pte.vpage = eaddr &gt;&gt; 12;
 		pte.page_size = MMU_PAGE_64K;
<span class="p_add">+		pte.wimg = HPTE_R_M;</span>
 	}
 
 	switch (kvmppc_get_msr(vcpu) &amp; (MSR_DR|MSR_IR)) {
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 592c974d4558..17de6acc0eab 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -89,6 +89,7 @@</span> <span class="p_context"> config X86</span>
 	select GENERIC_CLOCKEVENTS_MIN_ADJUST
 	select GENERIC_CMOS_UPDATE
 	select GENERIC_CPU_AUTOPROBE
<span class="p_add">+	select GENERIC_CPU_VULNERABILITIES</span>
 	select GENERIC_EARLY_IOREMAP
 	select GENERIC_FIND_FIRST_BIT
 	select GENERIC_IOMAP
<span class="p_chunk">@@ -428,6 +429,19 @@</span> <span class="p_context"> config GOLDFISH</span>
        def_bool y
        depends on X86_GOLDFISH
 
<span class="p_add">+config RETPOLINE</span>
<span class="p_add">+	bool &quot;Avoid speculative indirect branches in kernel&quot;</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Compile kernel with the retpoline compiler options to guard against</span>
<span class="p_add">+	  kernel-to-user data leaks by avoiding speculative indirect</span>
<span class="p_add">+	  branches. Requires a compiler with -mindirect-branch=thunk-extern</span>
<span class="p_add">+	  support for full protection. The kernel may run slower.</span>
<span class="p_add">+</span>
<span class="p_add">+	  Without compiler support, at least indirect branches in assembler</span>
<span class="p_add">+	  code are eliminated. Since this includes the syscall entry path,</span>
<span class="p_add">+	  it is not entirely pointless.</span>
<span class="p_add">+</span>
 config INTEL_RDT
 	bool &quot;Intel Resource Director Technology support&quot;
 	default n
<span class="p_header">diff --git a/arch/x86/Makefile b/arch/x86/Makefile</span>
<span class="p_header">index a20eacd9c7e9..504b1a4535ac 100644</span>
<span class="p_header">--- a/arch/x86/Makefile</span>
<span class="p_header">+++ b/arch/x86/Makefile</span>
<span class="p_chunk">@@ -235,6 +235,14 @@</span> <span class="p_context"> KBUILD_CFLAGS += -Wno-sign-compare</span>
 #
 KBUILD_CFLAGS += -fno-asynchronous-unwind-tables
 
<span class="p_add">+# Avoid indirect branches in kernel to deal with Spectre</span>
<span class="p_add">+ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+    RETPOLINE_CFLAGS += $(call cc-option,-mindirect-branch=thunk-extern -mindirect-branch-register)</span>
<span class="p_add">+    ifneq ($(RETPOLINE_CFLAGS),)</span>
<span class="p_add">+        KBUILD_CFLAGS += $(RETPOLINE_CFLAGS) -DRETPOLINE</span>
<span class="p_add">+    endif</span>
<span class="p_add">+endif</span>
<span class="p_add">+</span>
 archscripts: scripts_basic
 	$(Q)$(MAKE) $(build)=arch/x86/tools relocs
 
<span class="p_header">diff --git a/arch/x86/crypto/aesni-intel_asm.S b/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_header">index 16627fec80b2..3d09e3aca18d 100644</span>
<span class="p_header">--- a/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_header">+++ b/arch/x86/crypto/aesni-intel_asm.S</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/inst.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 /*
  * The following macros are used to move an (un)aligned 16 byte value to/from
<span class="p_chunk">@@ -2884,7 +2885,7 @@</span> <span class="p_context"> ENTRY(aesni_xts_crypt8)</span>
 	pxor INC, STATE4
 	movdqu IV, 0x30(OUTP)
 
<span class="p_del">-	call *%r11</span>
<span class="p_add">+	CALL_NOSPEC %r11</span>
 
 	movdqu 0x00(OUTP), INC
 	pxor INC, STATE1
<span class="p_chunk">@@ -2929,7 +2930,7 @@</span> <span class="p_context"> ENTRY(aesni_xts_crypt8)</span>
 	_aesni_gf128mul_x_ble()
 	movups IV, (IVP)
 
<span class="p_del">-	call *%r11</span>
<span class="p_add">+	CALL_NOSPEC %r11</span>
 
 	movdqu 0x40(OUTP), INC
 	pxor INC, STATE1
<span class="p_header">diff --git a/arch/x86/crypto/camellia-aesni-avx-asm_64.S b/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_header">index f7c495e2863c..a14af6eb09cb 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia-aesni-avx-asm_64.S</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
<span class="p_chunk">@@ -1227,7 +1228,7 @@</span> <span class="p_context"> camellia_xts_crypt_16way:</span>
 	vpxor 14 * 16(%rax), %xmm15, %xmm14;
 	vpxor 15 * 16(%rax), %xmm15, %xmm15;
 
<span class="p_del">-	call *%r9;</span>
<span class="p_add">+	CALL_NOSPEC %r9;</span>
 
 	addq $(16 * 16), %rsp;
 
<span class="p_header">diff --git a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_header">index eee5b3982cfd..b66bbfa62f50 100644</span>
<span class="p_header">--- a/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/camellia-aesni-avx2-asm_64.S</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #define CAMELLIA_TABLE_BYTE_LEN 272
 
<span class="p_chunk">@@ -1343,7 +1344,7 @@</span> <span class="p_context"> camellia_xts_crypt_32way:</span>
 	vpxor 14 * 32(%rax), %ymm15, %ymm14;
 	vpxor 15 * 32(%rax), %ymm15, %ymm15;
 
<span class="p_del">-	call *%r9;</span>
<span class="p_add">+	CALL_NOSPEC %r9;</span>
 
 	addq $(16 * 32), %rsp;
 
<span class="p_header">diff --git a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_header">index 7a7de27c6f41..d9b734d0c8cc 100644</span>
<span class="p_header">--- a/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/crypto/crc32c-pcl-intel-asm_64.S</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/inst.h&gt;
 #include &lt;linux/linkage.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 ## ISCSI CRC 32 Implementation with crc32 and pclmulqdq Instruction
 
<span class="p_chunk">@@ -172,7 +173,7 @@</span> <span class="p_context"> continue_block:</span>
 	movzxw  (bufp, %rax, 2), len
 	lea	crc_array(%rip), bufp
 	lea     (bufp, len, 1), bufp
<span class="p_del">-	jmp     *bufp</span>
<span class="p_add">+	JMP_NOSPEC bufp</span>
 
 	################################################################
 	## 2a) PROCESS FULL BLOCKS:
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 45a63e00a6af..3f48f695d5e6 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -198,8 +198,11 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
  * PAGE_TABLE_ISOLATION PGDs are 8k.  Flip bit 12 to switch between the two
  * halves:
  */
<span class="p_del">-#define PTI_SWITCH_PGTABLES_MASK	(1&lt;&lt;PAGE_SHIFT)</span>
<span class="p_del">-#define PTI_SWITCH_MASK		(PTI_SWITCH_PGTABLES_MASK|(1&lt;&lt;X86_CR3_PTI_SWITCH_BIT))</span>
<span class="p_add">+#define PTI_USER_PGTABLE_BIT		PAGE_SHIFT</span>
<span class="p_add">+#define PTI_USER_PGTABLE_MASK		(1 &lt;&lt; PTI_USER_PGTABLE_BIT)</span>
<span class="p_add">+#define PTI_USER_PCID_BIT		X86_CR3_PTI_PCID_USER_BIT</span>
<span class="p_add">+#define PTI_USER_PCID_MASK		(1 &lt;&lt; PTI_USER_PCID_BIT)</span>
<span class="p_add">+#define PTI_USER_PGTABLE_AND_PCID_MASK  (PTI_USER_PCID_MASK | PTI_USER_PGTABLE_MASK)</span>
 
 .macro SET_NOFLUSH_BIT	reg:req
 	bts	$X86_CR3_PCID_NOFLUSH_BIT, \reg
<span class="p_chunk">@@ -208,7 +211,7 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 .macro ADJUST_KERNEL_CR3 reg:req
 	ALTERNATIVE &quot;&quot;, &quot;SET_NOFLUSH_BIT \reg&quot;, X86_FEATURE_PCID
 	/* Clear PCID and &quot;PAGE_TABLE_ISOLATION bit&quot;, point CR3 at kernel pagetables: */
<span class="p_del">-	andq    $(~PTI_SWITCH_MASK), \reg</span>
<span class="p_add">+	andq    $(~PTI_USER_PGTABLE_AND_PCID_MASK), \reg</span>
 .endm
 
 .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
<span class="p_chunk">@@ -239,15 +242,19 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	/* Flush needed, clear the bit */
 	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask
 	movq	\scratch_reg2, \scratch_reg
<span class="p_del">-	jmp	.Lwrcr3_\@</span>
<span class="p_add">+	jmp	.Lwrcr3_pcid_\@</span>
 
 .Lnoflush_\@:
 	movq	\scratch_reg2, \scratch_reg
 	SET_NOFLUSH_BIT \scratch_reg
 
<span class="p_add">+.Lwrcr3_pcid_\@:</span>
<span class="p_add">+	/* Flip the ASID to the user version */</span>
<span class="p_add">+	orq	$(PTI_USER_PCID_MASK), \scratch_reg</span>
<span class="p_add">+</span>
 .Lwrcr3_\@:
<span class="p_del">-	/* Flip the PGD and ASID to the user version */</span>
<span class="p_del">-	orq     $(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	/* Flip the PGD to the user version */</span>
<span class="p_add">+	orq     $(PTI_USER_PGTABLE_MASK), \scratch_reg</span>
 	mov	\scratch_reg, %cr3
 .Lend_\@:
 .endm
<span class="p_chunk">@@ -263,17 +270,12 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	movq	%cr3, \scratch_reg
 	movq	\scratch_reg, \save_reg
 	/*
<span class="p_del">-	 * Is the &quot;switch mask&quot; all zero?  That means that both of</span>
<span class="p_del">-	 * these are zero:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *	1. The user/kernel PCID bit, and</span>
<span class="p_del">-	 *	2. The user/kernel &quot;bit&quot; that points CR3 to the</span>
<span class="p_del">-	 *	   bottom half of the 8k PGD</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * That indicates a kernel CR3 value, not a user CR3.</span>
<span class="p_add">+	 * Test the user pagetable bit. If set, then the user page tables</span>
<span class="p_add">+	 * are active. If clear CR3 already has the kernel page table</span>
<span class="p_add">+	 * active.</span>
 	 */
<span class="p_del">-	testq	$(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_del">-	jz	.Ldone_\@</span>
<span class="p_add">+	bt	$PTI_USER_PGTABLE_BIT, \scratch_reg</span>
<span class="p_add">+	jnc	.Ldone_\@</span>
 
 	ADJUST_KERNEL_CR3 \scratch_reg
 	movq	\scratch_reg, %cr3
<span class="p_chunk">@@ -290,7 +292,7 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	 * KERNEL pages can always resume with NOFLUSH as we do
 	 * explicit flushes.
 	 */
<span class="p_del">-	bt	$X86_CR3_PTI_SWITCH_BIT, \save_reg</span>
<span class="p_add">+	bt	$PTI_USER_PGTABLE_BIT, \save_reg</span>
 	jnc	.Lnoflush_\@
 
 	/*
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index ace8f321a5a1..a1f28a54f23a 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 	.section .entry.text, &quot;ax&quot;
 
<span class="p_chunk">@@ -290,7 +291,7 @@</span> <span class="p_context"> ENTRY(ret_from_fork)</span>
 
 	/* kernel thread */
 1:	movl	%edi, %eax
<span class="p_del">-	call	*%ebx</span>
<span class="p_add">+	CALL_NOSPEC %ebx</span>
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
<span class="p_chunk">@@ -919,7 +920,7 @@</span> <span class="p_context"> common_exception:</span>
 	movl	%ecx, %es
 	TRACE_IRQS_OFF
 	movl	%esp, %eax			# pt_regs pointer
<span class="p_del">-	call	*%edi</span>
<span class="p_add">+	CALL_NOSPEC %edi</span>
 	jmp	ret_from_exception
 END(common_exception)
 
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index dd696b966e58..f5fda5f26e34 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable_types.h&gt;
 #include &lt;asm/export.h&gt;
 #include &lt;asm/frame.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 #include &lt;linux/err.h&gt;
 
 #include &quot;calling.h&quot;
<span class="p_chunk">@@ -187,7 +188,7 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64_trampoline)</span>
 	 */
 	pushq	%rdi
 	movq	$entry_SYSCALL_64_stage2, %rdi
<span class="p_del">-	jmp	*%rdi</span>
<span class="p_add">+	JMP_NOSPEC %rdi</span>
 END(entry_SYSCALL_64_trampoline)
 
 	.popsection
<span class="p_chunk">@@ -266,7 +267,12 @@</span> <span class="p_context"> entry_SYSCALL_64_fastpath:</span>
 	 * It might end up jumping to the slow path.  If it jumps, RAX
 	 * and all argument registers are clobbered.
 	 */
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	movq	sys_call_table(, %rax, 8), %rax</span>
<span class="p_add">+	call	__x86_indirect_thunk_rax</span>
<span class="p_add">+#else</span>
 	call	*sys_call_table(, %rax, 8)
<span class="p_add">+#endif</span>
 .Lentry_SYSCALL_64_after_fastpath_call:
 
 	movq	%rax, RAX(%rsp)
<span class="p_chunk">@@ -438,7 +444,7 @@</span> <span class="p_context"> ENTRY(stub_ptregs_64)</span>
 	jmp	entry_SYSCALL64_slow_path
 
 1:
<span class="p_del">-	jmp	*%rax				/* Called from C */</span>
<span class="p_add">+	JMP_NOSPEC %rax				/* Called from C */</span>
 END(stub_ptregs_64)
 
 .macro ptregs_stub func
<span class="p_chunk">@@ -517,7 +523,7 @@</span> <span class="p_context"> ENTRY(ret_from_fork)</span>
 1:
 	/* kernel thread */
 	movq	%r12, %rdi
<span class="p_del">-	call	*%rbx</span>
<span class="p_add">+	CALL_NOSPEC %rbx</span>
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
<span class="p_header">diff --git a/arch/x86/events/intel/bts.c b/arch/x86/events/intel/bts.c</span>
<span class="p_header">index 141e07b06216..24ffa1e88cf9 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/bts.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/bts.c</span>
<span class="p_chunk">@@ -582,6 +582,24 @@</span> <span class="p_context"> static __init int bts_init(void)</span>
 	if (!boot_cpu_has(X86_FEATURE_DTES64) || !x86_pmu.bts)
 		return -ENODEV;
 
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * BTS hardware writes through a virtual memory map we must</span>
<span class="p_add">+		 * either use the kernel physical map, or the user mapping of</span>
<span class="p_add">+		 * the AUX buffer.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * However, since this driver supports per-CPU and per-task inherit</span>
<span class="p_add">+		 * we cannot use the user mapping since it will not be availble</span>
<span class="p_add">+		 * if we&#39;re not running the owning process.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * With PTI we can&#39;t use the kernal map either, because its not</span>
<span class="p_add">+		 * there when we run userspace.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * For now, disable this driver when using PTI.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	bts_pmu.capabilities	= PERF_PMU_CAP_AUX_NO_SG | PERF_PMU_CAP_ITRACE |
 				  PERF_PMU_CAP_EXCLUSIVE;
 	bts_pmu.task_ctx_nr	= perf_sw_context;
<span class="p_header">diff --git a/arch/x86/include/asm/asm-prototypes.h b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">index ff700d81e91e..0927cdc4f946 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/asm-prototypes.h</span>
<span class="p_chunk">@@ -11,7 +11,32 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/preempt.h&gt;
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
 
 #ifndef CONFIG_X86_CMPXCHG64
 extern void cmpxchg8b_emu(void);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+#define INDIRECT_THUNK(reg) extern asmlinkage void __x86_indirect_thunk_e ## reg(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define INDIRECT_THUNK(reg) extern asmlinkage void __x86_indirect_thunk_r ## reg(void);</span>
<span class="p_add">+INDIRECT_THUNK(8)</span>
<span class="p_add">+INDIRECT_THUNK(9)</span>
<span class="p_add">+INDIRECT_THUNK(10)</span>
<span class="p_add">+INDIRECT_THUNK(11)</span>
<span class="p_add">+INDIRECT_THUNK(12)</span>
<span class="p_add">+INDIRECT_THUNK(13)</span>
<span class="p_add">+INDIRECT_THUNK(14)</span>
<span class="p_add">+INDIRECT_THUNK(15)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+INDIRECT_THUNK(ax)</span>
<span class="p_add">+INDIRECT_THUNK(bx)</span>
<span class="p_add">+INDIRECT_THUNK(cx)</span>
<span class="p_add">+INDIRECT_THUNK(dx)</span>
<span class="p_add">+INDIRECT_THUNK(si)</span>
<span class="p_add">+INDIRECT_THUNK(di)</span>
<span class="p_add">+INDIRECT_THUNK(bp)</span>
<span class="p_add">+INDIRECT_THUNK(sp)</span>
<span class="p_add">+#endif /* CONFIG_RETPOLINE */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 21ac898df2d8..f275447862f4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -203,6 +203,8 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
 #define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */
<span class="p_add">+#define X86_FEATURE_RETPOLINE		( 7*32+12) /* Generic Retpoline mitigation for Spectre variant 2 */</span>
<span class="p_add">+#define X86_FEATURE_RETPOLINE_AMD	( 7*32+13) /* AMD Retpoline mitigation for Spectre variant 2 */</span>
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 #define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */
 #define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */
<span class="p_chunk">@@ -342,5 +344,7 @@</span> <span class="p_context"></span>
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
 #define X86_BUG_CPU_MELTDOWN		X86_BUG(14) /* CPU is affected by meltdown attack and needs kernel page table isolation */
<span class="p_add">+#define X86_BUG_SPECTRE_V1		X86_BUG(15) /* CPU is affected by Spectre variant 1 attack with conditional branches */</span>
<span class="p_add">+#define X86_BUG_SPECTRE_V2		X86_BUG(16) /* CPU is affected by Spectre variant 2 attack with indirect branches */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/mshyperv.h b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">index 581bb54dd464..5119e4b555cc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mshyperv.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mshyperv.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nmi.h&gt;
 #include &lt;asm/io.h&gt;
 #include &lt;asm/hyperv.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 /*
  * The below CPUID leaves are present if VersionAndFeatures.HypervisorPresent
<span class="p_chunk">@@ -186,10 +187,11 @@</span> <span class="p_context"> static inline u64 hv_do_hypercall(u64 control, void *input, void *output)</span>
 		return U64_MAX;
 
 	__asm__ __volatile__(&quot;mov %4, %%r8\n&quot;
<span class="p_del">-			     &quot;call *%5&quot;</span>
<span class="p_add">+			     CALL_NOSPEC</span>
 			     : &quot;=a&quot; (hv_status), ASM_CALL_CONSTRAINT,
 			       &quot;+c&quot; (control), &quot;+d&quot; (input_address)
<span class="p_del">-			     :  &quot;r&quot; (output_address), &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+			     :  &quot;r&quot; (output_address),</span>
<span class="p_add">+				THUNK_TARGET(hv_hypercall_pg)</span>
 			     : &quot;cc&quot;, &quot;memory&quot;, &quot;r8&quot;, &quot;r9&quot;, &quot;r10&quot;, &quot;r11&quot;);
 #else
 	u32 input_address_hi = upper_32_bits(input_address);
<span class="p_chunk">@@ -200,13 +202,13 @@</span> <span class="p_context"> static inline u64 hv_do_hypercall(u64 control, void *input, void *output)</span>
 	if (!hv_hypercall_pg)
 		return U64_MAX;
 
<span class="p_del">-	__asm__ __volatile__(&quot;call *%7&quot;</span>
<span class="p_add">+	__asm__ __volatile__(CALL_NOSPEC</span>
 			     : &quot;=A&quot; (hv_status),
 			       &quot;+c&quot; (input_address_lo), ASM_CALL_CONSTRAINT
 			     : &quot;A&quot; (control),
 			       &quot;b&quot; (input_address_hi),
 			       &quot;D&quot;(output_address_hi), &quot;S&quot;(output_address_lo),
<span class="p_del">-			       &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+			       THUNK_TARGET(hv_hypercall_pg)</span>
 			     : &quot;cc&quot;, &quot;memory&quot;);
 #endif /* !x86_64 */
 	return hv_status;
<span class="p_chunk">@@ -227,10 +229,10 @@</span> <span class="p_context"> static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)</span>
 
 #ifdef CONFIG_X86_64
 	{
<span class="p_del">-		__asm__ __volatile__(&quot;call *%4&quot;</span>
<span class="p_add">+		__asm__ __volatile__(CALL_NOSPEC</span>
 				     : &quot;=a&quot; (hv_status), ASM_CALL_CONSTRAINT,
 				       &quot;+c&quot; (control), &quot;+d&quot; (input1)
<span class="p_del">-				     : &quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+				     : THUNK_TARGET(hv_hypercall_pg)</span>
 				     : &quot;cc&quot;, &quot;r8&quot;, &quot;r9&quot;, &quot;r10&quot;, &quot;r11&quot;);
 	}
 #else
<span class="p_chunk">@@ -238,13 +240,13 @@</span> <span class="p_context"> static inline u64 hv_do_fast_hypercall8(u16 code, u64 input1)</span>
 		u32 input1_hi = upper_32_bits(input1);
 		u32 input1_lo = lower_32_bits(input1);
 
<span class="p_del">-		__asm__ __volatile__ (&quot;call *%5&quot;</span>
<span class="p_add">+		__asm__ __volatile__ (CALL_NOSPEC</span>
 				      : &quot;=A&quot;(hv_status),
 					&quot;+c&quot;(input1_lo),
 					ASM_CALL_CONSTRAINT
 				      :	&quot;A&quot; (control),
 					&quot;b&quot; (input1_hi),
<span class="p_del">-					&quot;m&quot; (hv_hypercall_pg)</span>
<span class="p_add">+					THUNK_TARGET(hv_hypercall_pg)</span>
 				      : &quot;cc&quot;, &quot;edi&quot;, &quot;esi&quot;);
 	}
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">index ab022618a50a..fa11fb1fa570 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/msr-index.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/msr-index.h</span>
<span class="p_chunk">@@ -352,6 +352,9 @@</span> <span class="p_context"></span>
 #define FAM10H_MMIO_CONF_BASE_MASK	0xfffffffULL
 #define FAM10H_MMIO_CONF_BASE_SHIFT	20
 #define MSR_FAM10H_NODE_ID		0xc001100c
<span class="p_add">+#define MSR_F10H_DECFG			0xc0011029</span>
<span class="p_add">+#define MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT	1</span>
<span class="p_add">+#define MSR_F10H_DECFG_LFENCE_SERIALIZE		BIT_ULL(MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT)</span>
 
 /* K8 MSRs */
 #define MSR_K8_TOP_MEM1			0xc001001a
<span class="p_header">diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h</span>
new file mode 100644
<span class="p_header">index 000000000000..402a11c803c3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/nospec-branch.h</span>
<span class="p_chunk">@@ -0,0 +1,214 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __NOSPEC_BRANCH_H__</span>
<span class="p_add">+#define __NOSPEC_BRANCH_H__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/alternative.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Fill the CPU return stack buffer.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each entry in the RSB, if used for a speculative &#39;ret&#39;, contains an</span>
<span class="p_add">+ * infinite &#39;pause; jmp&#39; loop to capture speculative execution.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is required in various cases for retpoline and IBRS-based</span>
<span class="p_add">+ * mitigations for the Spectre variant 2 vulnerability. Sometimes to</span>
<span class="p_add">+ * eliminate potentially bogus entries from the RSB, and sometimes</span>
<span class="p_add">+ * purely to ensure that it doesn&#39;t get empty, which on some CPUs would</span>
<span class="p_add">+ * allow predictions from other (unwanted!) sources to be used.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We define a CPP macro such that it can be used from both .S files and</span>
<span class="p_add">+ * inline assembly. It&#39;s possible to do a .macro and then include that</span>
<span class="p_add">+ * from C via asm(&quot;.include &lt;asm/nospec-branch.h&gt;&quot;) but let&#39;s not go there.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define RSB_CLEAR_LOOPS		32	/* To forcibly overwrite all entries */</span>
<span class="p_add">+#define RSB_FILL_LOOPS		16	/* To avoid underflow */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Google experimented with loop-unrolling and this turned out to be</span>
<span class="p_add">+ * the optimal version — two calls, each with their own speculation</span>
<span class="p_add">+ * trap should their return address end up getting used, in a loop.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __FILL_RETURN_BUFFER(reg, nr, sp)	\</span>
<span class="p_add">+	mov	$(nr/2), reg;			\</span>
<span class="p_add">+771:						\</span>
<span class="p_add">+	call	772f;				\</span>
<span class="p_add">+773:	/* speculation trap */			\</span>
<span class="p_add">+	pause;					\</span>
<span class="p_add">+	jmp	773b;				\</span>
<span class="p_add">+772:						\</span>
<span class="p_add">+	call	774f;				\</span>
<span class="p_add">+775:	/* speculation trap */			\</span>
<span class="p_add">+	pause;					\</span>
<span class="p_add">+	jmp	775b;				\</span>
<span class="p_add">+774:						\</span>
<span class="p_add">+	dec	reg;				\</span>
<span class="p_add">+	jnz	771b;				\</span>
<span class="p_add">+	add	$(BITS_PER_LONG/8) * nr, sp;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This should be used immediately before a retpoline alternative.  It tells</span>
<span class="p_add">+ * objtool where the retpolines are so that it can make sense of the control</span>
<span class="p_add">+ * flow by just reading the original instruction(s) and ignoring the</span>
<span class="p_add">+ * alternatives.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	.Lannotate_\@:</span>
<span class="p_add">+	.pushsection .discard.nospec</span>
<span class="p_add">+	.long .Lannotate_\@ - .</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are the bare retpoline primitives for indirect jmp and call.</span>
<span class="p_add">+ * Do not use these directly; they only exist to make the ALTERNATIVE</span>
<span class="p_add">+ * invocation below less ugly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro RETPOLINE_JMP reg:req</span>
<span class="p_add">+	call	.Ldo_rop_\@</span>
<span class="p_add">+.Lspec_trap_\@:</span>
<span class="p_add">+	pause</span>
<span class="p_add">+	jmp	.Lspec_trap_\@</span>
<span class="p_add">+.Ldo_rop_\@:</span>
<span class="p_add">+	mov	\reg, (%_ASM_SP)</span>
<span class="p_add">+	ret</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is a wrapper around RETPOLINE_JMP so the called function in reg</span>
<span class="p_add">+ * returns to the instruction after the macro.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro RETPOLINE_CALL reg:req</span>
<span class="p_add">+	jmp	.Ldo_call_\@</span>
<span class="p_add">+.Ldo_retpoline_jmp_\@:</span>
<span class="p_add">+	RETPOLINE_JMP \reg</span>
<span class="p_add">+.Ldo_call_\@:</span>
<span class="p_add">+	call	.Ldo_retpoline_jmp_\@</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * JMP_NOSPEC and CALL_NOSPEC macros can be used instead of a simple</span>
<span class="p_add">+ * indirect jmp/call which may be susceptible to the Spectre variant 2</span>
<span class="p_add">+ * attack.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro JMP_NOSPEC reg:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE_2 __stringify(jmp *\reg),				\</span>
<span class="p_add">+		__stringify(RETPOLINE_JMP \reg), X86_FEATURE_RETPOLINE,	\</span>
<span class="p_add">+		__stringify(lfence; jmp *\reg), X86_FEATURE_RETPOLINE_AMD</span>
<span class="p_add">+#else</span>
<span class="p_add">+	jmp	*\reg</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro CALL_NOSPEC reg:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE_2 __stringify(call *\reg),				\</span>
<span class="p_add">+		__stringify(RETPOLINE_CALL \reg), X86_FEATURE_RETPOLINE,\</span>
<span class="p_add">+		__stringify(lfence; call *\reg), X86_FEATURE_RETPOLINE_AMD</span>
<span class="p_add">+#else</span>
<span class="p_add">+	call	*\reg</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+ /*</span>
<span class="p_add">+  * A simpler FILL_RETURN_BUFFER macro. Don&#39;t make people use the CPP</span>
<span class="p_add">+  * monstrosity above, manually.</span>
<span class="p_add">+  */</span>
<span class="p_add">+.macro FILL_RETURN_BUFFER reg:req nr:req ftr:req</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lskip_rsb_\@&quot;,				\</span>
<span class="p_add">+		__stringify(__FILL_RETURN_BUFFER(\reg,\nr,%_ASM_SP))	\</span>
<span class="p_add">+		\ftr</span>
<span class="p_add">+.Lskip_rsb_\@:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define ANNOTATE_NOSPEC_ALTERNATIVE				\</span>
<span class="p_add">+	&quot;999:\n\t&quot;						\</span>
<span class="p_add">+	&quot;.pushsection .discard.nospec\n\t&quot;			\</span>
<span class="p_add">+	&quot;.long 999b - .\n\t&quot;					\</span>
<span class="p_add">+	&quot;.popsection\n\t&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_64) &amp;&amp; defined(RETPOLINE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Since the inline asm uses the %V modifier which is only in newer GCC,</span>
<span class="p_add">+ * the 64-bit one is dependent on RETPOLINE not CONFIG_RETPOLINE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+# define CALL_NOSPEC						\</span>
<span class="p_add">+	ANNOTATE_NOSPEC_ALTERNATIVE				\</span>
<span class="p_add">+	ALTERNATIVE(						\</span>
<span class="p_add">+	&quot;call *%[thunk_target]\n&quot;,				\</span>
<span class="p_add">+	&quot;call __x86_indirect_thunk_%V[thunk_target]\n&quot;,		\</span>
<span class="p_add">+	X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;r&quot; (addr)</span>
<span class="p_add">+</span>
<span class="p_add">+#elif defined(CONFIG_X86_32) &amp;&amp; defined(CONFIG_RETPOLINE)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For i386 we use the original ret-equivalent retpoline, because</span>
<span class="p_add">+ * otherwise we&#39;ll run out of registers. We don&#39;t care about CET</span>
<span class="p_add">+ * here, anyway.</span>
<span class="p_add">+ */</span>
<span class="p_add">+# define CALL_NOSPEC ALTERNATIVE(&quot;call *%[thunk_target]\n&quot;,	\</span>
<span class="p_add">+	&quot;       jmp    904f;\n&quot;					\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;901:	call   903f;\n&quot;					\</span>
<span class="p_add">+	&quot;902:	pause;\n&quot;					\</span>
<span class="p_add">+	&quot;       jmp    902b;\n&quot;					\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;903:	addl   $4, %%esp;\n&quot;				\</span>
<span class="p_add">+	&quot;       pushl  %[thunk_target];\n&quot;			\</span>
<span class="p_add">+	&quot;       ret;\n&quot;						\</span>
<span class="p_add">+	&quot;       .align 16\n&quot;					\</span>
<span class="p_add">+	&quot;904:	call   901b;\n&quot;,				\</span>
<span class="p_add">+	X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;rm&quot; (addr)</span>
<span class="p_add">+#else /* No retpoline for C / inline asm */</span>
<span class="p_add">+# define CALL_NOSPEC &quot;call *%[thunk_target]\n&quot;</span>
<span class="p_add">+# define THUNK_TARGET(addr) [thunk_target] &quot;rm&quot; (addr)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* The Spectre V2 mitigation variants */</span>
<span class="p_add">+enum spectre_v2_mitigation {</span>
<span class="p_add">+	SPECTRE_V2_NONE,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_MINIMAL,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_MINIMAL_AMD,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_GENERIC,</span>
<span class="p_add">+	SPECTRE_V2_RETPOLINE_AMD,</span>
<span class="p_add">+	SPECTRE_V2_IBRS,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * On VMEXIT we must ensure that no RSB predictions learned in the guest</span>
<span class="p_add">+ * can be followed in the host, by overwriting the RSB completely. Both</span>
<span class="p_add">+ * retpoline and IBRS mitigations for Spectre v2 need this; only on future</span>
<span class="p_add">+ * CPUs with IBRS_ATT *might* it be avoided.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void vmexit_fill_RSB(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_RETPOLINE</span>
<span class="p_add">+	unsigned long loops = RSB_CLEAR_LOOPS / 2;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (ANNOTATE_NOSPEC_ALTERNATIVE</span>
<span class="p_add">+		      ALTERNATIVE(&quot;jmp 910f&quot;,</span>
<span class="p_add">+				  __stringify(__FILL_RETURN_BUFFER(%0, RSB_CLEAR_LOOPS, %1)),</span>
<span class="p_add">+				  X86_FEATURE_RETPOLINE)</span>
<span class="p_add">+		      &quot;910:&quot;</span>
<span class="p_add">+		      : &quot;=&amp;r&quot; (loops), ASM_CALL_CONSTRAINT</span>
<span class="p_add">+		      : &quot;r&quot; (loops) : &quot;memory&quot; );</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+#endif /* __NOSPEC_BRANCH_H__ */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/processor-flags.h b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">index 6a60fea90b9d..625a52a5594f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_chunk">@@ -40,7 +40,7 @@</span> <span class="p_context"></span>
 #define CR3_NOFLUSH	BIT_ULL(63)
 
 #ifdef CONFIG_PAGE_TABLE_ISOLATION
<span class="p_del">-# define X86_CR3_PTI_SWITCH_BIT	11</span>
<span class="p_add">+# define X86_CR3_PTI_PCID_USER_BIT	11</span>
 #endif
 
 #else
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index f9b48ce152eb..3effd3c994af 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -81,13 +81,13 @@</span> <span class="p_context"> static inline u16 kern_pcid(u16 asid)</span>
 	 * Make sure that the dynamic ASID space does not confict with the
 	 * bit we are using to switch between user and kernel ASIDs.
 	 */
<span class="p_del">-	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT));</span>
 
 	/*
 	 * The ASID being passed in here should have respected the
 	 * MAX_ASID_AVAILABLE and thus never have the switch bit set.
 	 */
<span class="p_del">-	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT));</span>
 #endif
 	/*
 	 * The dynamically-assigned ASIDs that get passed in are small
<span class="p_chunk">@@ -112,7 +112,7 @@</span> <span class="p_context"> static inline u16 user_pcid(u16 asid)</span>
 {
 	u16 ret = kern_pcid(asid);
 #ifdef CONFIG_PAGE_TABLE_ISOLATION
<span class="p_del">-	ret |= 1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT;</span>
<span class="p_add">+	ret |= 1 &lt;&lt; X86_CR3_PTI_PCID_USER_BIT;</span>
 #endif
 	return ret;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_header">index 7cb282e9e587..bfd882617613 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/xen/hypercall.h</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &lt;xen/interface/xen.h&gt;
 #include &lt;xen/interface/sched.h&gt;
<span class="p_chunk">@@ -217,9 +218,9 @@</span> <span class="p_context"> privcmd_call(unsigned call,</span>
 	__HYPERCALL_5ARG(a1, a2, a3, a4, a5);
 
 	stac();
<span class="p_del">-	asm volatile(&quot;call *%[call]&quot;</span>
<span class="p_add">+	asm volatile(CALL_NOSPEC</span>
 		     : __HYPERCALL_5PARAM
<span class="p_del">-		     : [call] &quot;a&quot; (&amp;hypercall_page[call])</span>
<span class="p_add">+		     : [thunk_target] &quot;a&quot; (&amp;hypercall_page[call])</span>
 		     : __HYPERCALL_CLOBBER5);
 	clac();
 
<span class="p_header">diff --git a/arch/x86/kernel/acpi/boot.c b/arch/x86/kernel/acpi/boot.c</span>
<span class="p_header">index 079535e53e2a..9c2a002d9297 100644</span>
<span class="p_header">--- a/arch/x86/kernel/acpi/boot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/acpi/boot.c</span>
<span class="p_chunk">@@ -342,13 +342,12 @@</span> <span class="p_context"> acpi_parse_lapic_nmi(struct acpi_subtable_header * header, const unsigned long e</span>
 #ifdef CONFIG_X86_IO_APIC
 #define MP_ISA_BUS		0
 
<span class="p_add">+static int __init mp_register_ioapic_irq(u8 bus_irq, u8 polarity,</span>
<span class="p_add">+						u8 trigger, u32 gsi);</span>
<span class="p_add">+</span>
 static void __init mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,
 					  u32 gsi)
 {
<span class="p_del">-	int ioapic;</span>
<span class="p_del">-	int pin;</span>
<span class="p_del">-	struct mpc_intsrc mp_irq;</span>
<span class="p_del">-</span>
 	/*
 	 * Check bus_irq boundary.
 	 */
<span class="p_chunk">@@ -357,14 +356,6 @@</span> <span class="p_context"> static void __init mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,</span>
 		return;
 	}
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Convert &#39;gsi&#39; to &#39;ioapic.pin&#39;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ioapic = mp_find_ioapic(gsi);</span>
<span class="p_del">-	if (ioapic &lt; 0)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	pin = mp_find_ioapic_pin(ioapic, gsi);</span>
<span class="p_del">-</span>
 	/*
 	 * TBD: This check is for faulty timer entries, where the override
 	 *      erroneously sets the trigger to level, resulting in a HUGE
<span class="p_chunk">@@ -373,16 +364,8 @@</span> <span class="p_context"> static void __init mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,</span>
 	if ((bus_irq == 0) &amp;&amp; (trigger == 3))
 		trigger = 1;
 
<span class="p_del">-	mp_irq.type = MP_INTSRC;</span>
<span class="p_del">-	mp_irq.irqtype = mp_INT;</span>
<span class="p_del">-	mp_irq.irqflag = (trigger &lt;&lt; 2) | polarity;</span>
<span class="p_del">-	mp_irq.srcbus = MP_ISA_BUS;</span>
<span class="p_del">-	mp_irq.srcbusirq = bus_irq;	/* IRQ */</span>
<span class="p_del">-	mp_irq.dstapic = mpc_ioapic_id(ioapic); /* APIC ID */</span>
<span class="p_del">-	mp_irq.dstirq = pin;	/* INTIN# */</span>
<span class="p_del">-</span>
<span class="p_del">-	mp_save_irq(&amp;mp_irq);</span>
<span class="p_del">-</span>
<span class="p_add">+	if (mp_register_ioapic_irq(bus_irq, polarity, trigger, gsi) &lt; 0)</span>
<span class="p_add">+		return;</span>
 	/*
 	 * Reset default identity mapping if gsi is also an legacy IRQ,
 	 * otherwise there will be more than one entry with the same GSI
<span class="p_chunk">@@ -429,6 +412,34 @@</span> <span class="p_context"> static int mp_config_acpi_gsi(struct device *dev, u32 gsi, int trigger,</span>
 	return 0;
 }
 
<span class="p_add">+static int __init mp_register_ioapic_irq(u8 bus_irq, u8 polarity,</span>
<span class="p_add">+						u8 trigger, u32 gsi)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mpc_intsrc mp_irq;</span>
<span class="p_add">+	int ioapic, pin;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Convert &#39;gsi&#39; to &#39;ioapic.pin&#39;(INTIN#) */</span>
<span class="p_add">+	ioapic = mp_find_ioapic(gsi);</span>
<span class="p_add">+	if (ioapic &lt; 0) {</span>
<span class="p_add">+		pr_warn(&quot;Failed to find ioapic for gsi : %u\n&quot;, gsi);</span>
<span class="p_add">+		return ioapic;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pin = mp_find_ioapic_pin(ioapic, gsi);</span>
<span class="p_add">+</span>
<span class="p_add">+	mp_irq.type = MP_INTSRC;</span>
<span class="p_add">+	mp_irq.irqtype = mp_INT;</span>
<span class="p_add">+	mp_irq.irqflag = (trigger &lt;&lt; 2) | polarity;</span>
<span class="p_add">+	mp_irq.srcbus = MP_ISA_BUS;</span>
<span class="p_add">+	mp_irq.srcbusirq = bus_irq;</span>
<span class="p_add">+	mp_irq.dstapic = mpc_ioapic_id(ioapic);</span>
<span class="p_add">+	mp_irq.dstirq = pin;</span>
<span class="p_add">+</span>
<span class="p_add">+	mp_save_irq(&amp;mp_irq);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int __init
 acpi_parse_ioapic(struct acpi_subtable_header * header, const unsigned long end)
 {
<span class="p_chunk">@@ -473,7 +484,11 @@</span> <span class="p_context"> static void __init acpi_sci_ioapic_setup(u8 bus_irq, u16 polarity, u16 trigger,</span>
 	if (acpi_sci_flags &amp; ACPI_MADT_POLARITY_MASK)
 		polarity = acpi_sci_flags &amp; ACPI_MADT_POLARITY_MASK;
 
<span class="p_del">-	mp_override_legacy_irq(bus_irq, polarity, trigger, gsi);</span>
<span class="p_add">+	if (bus_irq &lt; NR_IRQS_LEGACY)</span>
<span class="p_add">+		mp_override_legacy_irq(bus_irq, polarity, trigger, gsi);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		mp_register_ioapic_irq(bus_irq, polarity, trigger, gsi);</span>
<span class="p_add">+</span>
 	acpi_penalize_sci_irq(bus_irq, trigger, polarity);
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c</span>
<span class="p_header">index 3344d3382e91..e0b97e4d1db5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/alternative.c</span>
<span class="p_header">+++ b/arch/x86/kernel/alternative.c</span>
<span class="p_chunk">@@ -344,9 +344,12 @@</span> <span class="p_context"> recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insnbuf)</span>
 static void __init_or_module noinline optimize_nops(struct alt_instr *a, u8 *instr)
 {
 	unsigned long flags;
<span class="p_add">+	int i;</span>
 
<span class="p_del">-	if (instr[0] != 0x90)</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	for (i = 0; i &lt; a-&gt;padlen; i++) {</span>
<span class="p_add">+		if (instr[i] != 0x90)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
 
 	local_irq_save(flags);
 	add_nops(instr + (a-&gt;instrlen - a-&gt;padlen), a-&gt;padlen);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">index bcb75dc97d44..ea831c858195 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/amd.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/amd.c</span>
<span class="p_chunk">@@ -829,8 +829,32 @@</span> <span class="p_context"> static void init_amd(struct cpuinfo_x86 *c)</span>
 		set_cpu_cap(c, X86_FEATURE_K8);
 
 	if (cpu_has(c, X86_FEATURE_XMM2)) {
<span class="p_del">-		/* MFENCE stops RDTSC speculation */</span>
<span class="p_del">-		set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);</span>
<span class="p_add">+		unsigned long long val;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * A serializing LFENCE has less overhead than MFENCE, so</span>
<span class="p_add">+		 * use it for execution serialization.  On families which</span>
<span class="p_add">+		 * don&#39;t have that MSR, LFENCE is already serializing.</span>
<span class="p_add">+		 * msr_set_bit() uses the safe accessors, too, even if the MSR</span>
<span class="p_add">+		 * is not present.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		msr_set_bit(MSR_F10H_DECFG,</span>
<span class="p_add">+			    MSR_F10H_DECFG_LFENCE_SERIALIZE_BIT);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Verify that the MSR write was successful (could be running</span>
<span class="p_add">+		 * under a hypervisor) and only then assume that LFENCE is</span>
<span class="p_add">+		 * serializing.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ret = rdmsrl_safe(MSR_F10H_DECFG, &amp;val);</span>
<span class="p_add">+		if (!ret &amp;&amp; (val &amp; MSR_F10H_DECFG_LFENCE_SERIALIZE)) {</span>
<span class="p_add">+			/* A serializing LFENCE stops RDTSC speculation */</span>
<span class="p_add">+			set_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* MFENCE stops RDTSC speculation */</span>
<span class="p_add">+			set_cpu_cap(c, X86_FEATURE_MFENCE_RDTSC);</span>
<span class="p_add">+		}</span>
 	}
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index ba0b2424c9b0..e4dc26185aa7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -10,6 +10,10 @@</span> <span class="p_context"></span>
  */
 #include &lt;linux/init.h&gt;
 #include &lt;linux/utsname.h&gt;
<span class="p_add">+#include &lt;linux/cpu.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
 #include &lt;asm/bugs.h&gt;
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/processor-flags.h&gt;
<span class="p_chunk">@@ -20,6 +24,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/set_memory.h&gt;
 
<span class="p_add">+static void __init spectre_v2_select_mitigation(void);</span>
<span class="p_add">+</span>
 void __init check_bugs(void)
 {
 	identify_boot_cpu();
<span class="p_chunk">@@ -29,6 +35,9 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 		print_cpu_info(&amp;boot_cpu_data);
 	}
 
<span class="p_add">+	/* Select the proper spectre mitigation before patching alternatives */</span>
<span class="p_add">+	spectre_v2_select_mitigation();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 	/*
 	 * Check whether we are able to run this kernel safely on SMP.
<span class="p_chunk">@@ -60,3 +69,179 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 		set_memory_4k((unsigned long)__va(0), 1);
 #endif
 }
<span class="p_add">+</span>
<span class="p_add">+/* The kernel command line selection */</span>
<span class="p_add">+enum spectre_v2_mitigation_cmd {</span>
<span class="p_add">+	SPECTRE_V2_CMD_NONE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_AUTO,</span>
<span class="p_add">+	SPECTRE_V2_CMD_FORCE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE_GENERIC,</span>
<span class="p_add">+	SPECTRE_V2_CMD_RETPOLINE_AMD,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const char *spectre_v2_strings[] = {</span>
<span class="p_add">+	[SPECTRE_V2_NONE]			= &quot;Vulnerable&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_MINIMAL]		= &quot;Vulnerable: Minimal generic ASM retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_MINIMAL_AMD]	= &quot;Vulnerable: Minimal AMD ASM retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_GENERIC]		= &quot;Mitigation: Full generic retpoline&quot;,</span>
<span class="p_add">+	[SPECTRE_V2_RETPOLINE_AMD]		= &quot;Mitigation: Full AMD retpoline&quot;,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Spectre V2 mitigation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+static enum spectre_v2_mitigation spectre_v2_enabled = SPECTRE_V2_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spec2_print_if_insecure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spec2_print_if_secure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool retp_compiler(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __is_defined(RETPOLINE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool match_option(const char *arg, int arglen, const char *opt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int len = strlen(opt);</span>
<span class="p_add">+</span>
<span class="p_add">+	return len == arglen &amp;&amp; !strncmp(arg, opt, len);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char arg[20];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;spectre_v2&quot;, arg,</span>
<span class="p_add">+				  sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0)  {</span>
<span class="p_add">+		if (match_option(arg, ret, &quot;off&quot;)) {</span>
<span class="p_add">+			goto disable;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;on&quot;)) {</span>
<span class="p_add">+			spec2_print_if_secure(&quot;force enabled on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_FORCE;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline&quot;)) {</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline,amd&quot;)) {</span>
<span class="p_add">+			if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {</span>
<span class="p_add">+				pr_err(&quot;retpoline,amd selected but CPU is not AMD. Switching to AUTO select\n&quot;);</span>
<span class="p_add">+				return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;AMD retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE_AMD;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;retpoline,generic&quot;)) {</span>
<span class="p_add">+			spec2_print_if_insecure(&quot;generic retpoline selected on command line.&quot;);</span>
<span class="p_add">+			return SPECTRE_V2_CMD_RETPOLINE_GENERIC;</span>
<span class="p_add">+		} else if (match_option(arg, ret, &quot;auto&quot;)) {</span>
<span class="p_add">+			return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cmdline_find_option_bool(boot_command_line, &quot;nospectre_v2&quot;))</span>
<span class="p_add">+		return SPECTRE_V2_CMD_AUTO;</span>
<span class="p_add">+disable:</span>
<span class="p_add">+	spec2_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+	return SPECTRE_V2_CMD_NONE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init spectre_v2_select_mitigation(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();</span>
<span class="p_add">+	enum spectre_v2_mitigation mode = SPECTRE_V2_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the CPU is not affected and the command line mode is NONE or AUTO</span>
<span class="p_add">+	 * then nothing to do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &amp;&amp;</span>
<span class="p_add">+	    (cmd == SPECTRE_V2_CMD_NONE || cmd == SPECTRE_V2_CMD_AUTO))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (cmd) {</span>
<span class="p_add">+	case SPECTRE_V2_CMD_NONE:</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SPECTRE_V2_CMD_FORCE:</span>
<span class="p_add">+		/* FALLTRHU */</span>
<span class="p_add">+	case SPECTRE_V2_CMD_AUTO:</span>
<span class="p_add">+		goto retpoline_auto;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE_AMD:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_amd;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE_GENERIC:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_generic;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case SPECTRE_V2_CMD_RETPOLINE:</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_RETPOLINE))</span>
<span class="p_add">+			goto retpoline_auto;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pr_err(&quot;kernel not compiled with retpoline; no mitigation available!&quot;);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+retpoline_auto:</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {</span>
<span class="p_add">+	retpoline_amd:</span>
<span class="p_add">+		if (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {</span>
<span class="p_add">+			pr_err(&quot;LFENCE not serializing. Switching to generic retpoline\n&quot;);</span>
<span class="p_add">+			goto retpoline_generic;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :</span>
<span class="p_add">+					 SPECTRE_V2_RETPOLINE_MINIMAL_AMD;</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+	retpoline_generic:</span>
<span class="p_add">+		mode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :</span>
<span class="p_add">+					 SPECTRE_V2_RETPOLINE_MINIMAL;</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spectre_v2_enabled = mode;</span>
<span class="p_add">+	pr_info(&quot;%s\n&quot;, spectre_v2_strings[mode]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SYSFS</span>
<span class="p_add">+ssize_t cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+			  struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return sprintf(buf, &quot;Mitigation: PTI\n&quot;);</span>
<span class="p_add">+	return sprintf(buf, &quot;Vulnerable\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+	return sprintf(buf, &quot;Vulnerable\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+			    struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))</span>
<span class="p_add">+		return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return sprintf(buf, &quot;%s\n&quot;, spectre_v2_strings[spectre_v2_enabled]);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 2d3bd2215e5b..372ba3fb400f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -902,6 +902,9 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	if (c-&gt;x86_vendor != X86_VENDOR_AMD)
 		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);
 
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_SPECTRE_V1);</span>
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_SPECTRE_V2);</span>
<span class="p_add">+</span>
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/kernel/cpu/microcode/intel.c b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">index 8ccdca6d3f9e..d9e460fc7a3b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/microcode/intel.c</span>
<span class="p_chunk">@@ -910,8 +910,17 @@</span> <span class="p_context"> static bool is_blacklisted(unsigned int cpu)</span>
 {
 	struct cpuinfo_x86 *c = &amp;cpu_data(cpu);
 
<span class="p_del">-	if (c-&gt;x86 == 6 &amp;&amp; c-&gt;x86_model == INTEL_FAM6_BROADWELL_X) {</span>
<span class="p_del">-		pr_err_once(&quot;late loading on model 79 is disabled.\n&quot;);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Late loading on model 79 with microcode revision less than 0x0b000021</span>
<span class="p_add">+	 * may result in a system hang. This behavior is documented in item</span>
<span class="p_add">+	 * BDF90, #334165 (Intel Xeon Processor E7-8800/4800 v4 Product Family).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (c-&gt;x86 == 6 &amp;&amp;</span>
<span class="p_add">+	    c-&gt;x86_model == INTEL_FAM6_BROADWELL_X &amp;&amp;</span>
<span class="p_add">+	    c-&gt;x86_mask == 0x01 &amp;&amp;</span>
<span class="p_add">+	    c-&gt;microcode &lt; 0x0b000021) {</span>
<span class="p_add">+		pr_err_once(&quot;Erratum BDF90: late loading with revision &lt; 0x0b000021 (0x%x) disabled.\n&quot;, c-&gt;microcode);</span>
<span class="p_add">+		pr_err_once(&quot;Please consider either early loading through initrd/built-in or a potential BIOS update.\n&quot;);</span>
 		return true;
 	}
 
<span class="p_header">diff --git a/arch/x86/kernel/ftrace_32.S b/arch/x86/kernel/ftrace_32.S</span>
<span class="p_header">index b6c6468e10bc..4c8440de3355 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ftrace_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/ftrace_32.S</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/segment.h&gt;
 #include &lt;asm/export.h&gt;
 #include &lt;asm/ftrace.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #ifdef CC_USING_FENTRY
 # define function_hook	__fentry__
<span class="p_chunk">@@ -197,7 +198,8 @@</span> <span class="p_context"> ftrace_stub:</span>
 	movl	0x4(%ebp), %edx
 	subl	$MCOUNT_INSN_SIZE, %eax
 
<span class="p_del">-	call	*ftrace_trace_function</span>
<span class="p_add">+	movl	ftrace_trace_function, %ecx</span>
<span class="p_add">+	CALL_NOSPEC %ecx</span>
 
 	popl	%edx
 	popl	%ecx
<span class="p_chunk">@@ -241,5 +243,5 @@</span> <span class="p_context"> return_to_handler:</span>
 	movl	%eax, %ecx
 	popl	%edx
 	popl	%eax
<span class="p_del">-	jmp	*%ecx</span>
<span class="p_add">+	JMP_NOSPEC %ecx</span>
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/ftrace_64.S b/arch/x86/kernel/ftrace_64.S</span>
<span class="p_header">index c832291d948a..7cb8ba08beb9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ftrace_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/ftrace_64.S</span>
<span class="p_chunk">@@ -7,7 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/ftrace.h&gt;
 #include &lt;asm/export.h&gt;
<span class="p_del">-</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 	.code64
 	.section .entry.text, &quot;ax&quot;
<span class="p_chunk">@@ -286,8 +286,8 @@</span> <span class="p_context"> trace:</span>
 	 * ip and parent ip are used and the list function is called when
 	 * function tracing is enabled.
 	 */
<span class="p_del">-	call   *ftrace_trace_function</span>
<span class="p_del">-</span>
<span class="p_add">+	movq ftrace_trace_function, %r8</span>
<span class="p_add">+	CALL_NOSPEC %r8</span>
 	restore_mcount_regs
 
 	jmp fgraph_trace
<span class="p_chunk">@@ -329,5 +329,5 @@</span> <span class="p_context"> GLOBAL(return_to_handler)</span>
 	movq 8(%rsp), %rdx
 	movq (%rsp), %rax
 	addq $24, %rsp
<span class="p_del">-	jmp *%rdi</span>
<span class="p_add">+	JMP_NOSPEC %rdi</span>
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/irq_32.c b/arch/x86/kernel/irq_32.c</span>
<span class="p_header">index a83b3346a0e1..c1bdbd3d3232 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq_32.c</span>
<span class="p_chunk">@@ -20,6 +20,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 
 #include &lt;asm/apic.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #ifdef CONFIG_DEBUG_STACKOVERFLOW
 
<span class="p_chunk">@@ -55,11 +56,11 @@</span> <span class="p_context"> DEFINE_PER_CPU(struct irq_stack *, softirq_stack);</span>
 static void call_on_stack(void *func, void *stack)
 {
 	asm volatile(&quot;xchgl	%%ebx,%%esp	\n&quot;
<span class="p_del">-		     &quot;call	*%%edi		\n&quot;</span>
<span class="p_add">+		     CALL_NOSPEC</span>
 		     &quot;movl	%%ebx,%%esp	\n&quot;
 		     : &quot;=b&quot; (stack)
 		     : &quot;0&quot; (stack),
<span class="p_del">-		       &quot;D&quot;(func)</span>
<span class="p_add">+		       [thunk_target] &quot;D&quot;(func)</span>
 		     : &quot;memory&quot;, &quot;cc&quot;, &quot;edx&quot;, &quot;ecx&quot;, &quot;eax&quot;);
 }
 
<span class="p_chunk">@@ -95,11 +96,11 @@</span> <span class="p_context"> static inline int execute_on_irq_stack(int overflow, struct irq_desc *desc)</span>
 		call_on_stack(print_stack_overflow, isp);
 
 	asm volatile(&quot;xchgl	%%ebx,%%esp	\n&quot;
<span class="p_del">-		     &quot;call	*%%edi		\n&quot;</span>
<span class="p_add">+		     CALL_NOSPEC</span>
 		     &quot;movl	%%ebx,%%esp	\n&quot;
 		     : &quot;=a&quot; (arg1), &quot;=b&quot; (isp)
 		     :  &quot;0&quot; (desc),   &quot;1&quot; (isp),
<span class="p_del">-			&quot;D&quot; (desc-&gt;handle_irq)</span>
<span class="p_add">+			[thunk_target] &quot;D&quot; (desc-&gt;handle_irq)</span>
 		     : &quot;memory&quot;, &quot;cc&quot;, &quot;ecx&quot;);
 	return 1;
 }
<span class="p_header">diff --git a/arch/x86/kernel/tboot.c b/arch/x86/kernel/tboot.c</span>
<span class="p_header">index a4eb27918ceb..a2486f444073 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tboot.c</span>
<span class="p_chunk">@@ -138,6 +138,17 @@</span> <span class="p_context"> static int map_tboot_page(unsigned long vaddr, unsigned long pfn,</span>
 		return -1;
 	set_pte_at(&amp;tboot_mm, vaddr, pte, pfn_pte(pfn, prot));
 	pte_unmap(pte);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * PTI poisons low addresses in the kernel page tables in the</span>
<span class="p_add">+	 * name of making them unusable for userspace.  To execute</span>
<span class="p_add">+	 * code at such a low address, the poison must be cleared.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: &#39;pgd&#39; actually gets set in p4d_alloc() _or_</span>
<span class="p_add">+	 * pud_alloc() depending on 4/5-level paging.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd-&gt;pgd &amp;= ~_PAGE_NX;</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index 17fb6c6d939a..6a8284f72328 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/kvm_para.h&gt;
 #include &lt;asm/irq_remapping.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &lt;asm/virtext.h&gt;
 #include &quot;trace.h&quot;
<span class="p_chunk">@@ -4964,6 +4965,25 @@</span> <span class="p_context"> static void svm_vcpu_run(struct kvm_vcpu *vcpu)</span>
 		&quot;mov %%r13, %c[r13](%[svm]) \n\t&quot;
 		&quot;mov %%r14, %c[r14](%[svm]) \n\t&quot;
 		&quot;mov %%r15, %c[r15](%[svm]) \n\t&quot;
<span class="p_add">+#endif</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		* Clear host registers marked as clobbered to prevent</span>
<span class="p_add">+		* speculative use.</span>
<span class="p_add">+		*/</span>
<span class="p_add">+		&quot;xor %%&quot; _ASM_BX &quot;, %%&quot; _ASM_BX &quot; \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%&quot; _ASM_CX &quot;, %%&quot; _ASM_CX &quot; \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%&quot; _ASM_DX &quot;, %%&quot; _ASM_DX &quot; \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%&quot; _ASM_SI &quot;, %%&quot; _ASM_SI &quot; \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%&quot; _ASM_DI &quot;, %%&quot; _ASM_DI &quot; \n\t&quot;</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+		&quot;xor %%r8, %%r8 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r9, %%r9 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r10, %%r10 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r11, %%r11 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r12, %%r12 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r13, %%r13 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r14, %%r14 \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r15, %%r15 \n\t&quot;</span>
 #endif
 		&quot;pop %%&quot; _ASM_BP
 		:
<span class="p_chunk">@@ -4994,6 +5014,9 @@</span> <span class="p_context"> static void svm_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 		);
 
<span class="p_add">+	/* Eliminate branch target predictions from guest mode */</span>
<span class="p_add">+	vmexit_fill_RSB();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_64
 	wrmsrl(MSR_GS_BASE, svm-&gt;host.gs_base);
 #else
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 47d9432756f3..ef16cf0f7cfd 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -50,6 +50,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/irq_remapping.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
 
 #include &quot;trace.h&quot;
 #include &quot;pmu.h&quot;
<span class="p_chunk">@@ -888,8 +889,16 @@</span> <span class="p_context"> static inline short vmcs_field_to_offset(unsigned long field)</span>
 {
 	BUILD_BUG_ON(ARRAY_SIZE(vmcs_field_to_offset_table) &gt; SHRT_MAX);
 
<span class="p_del">-	if (field &gt;= ARRAY_SIZE(vmcs_field_to_offset_table) ||</span>
<span class="p_del">-	    vmcs_field_to_offset_table[field] == 0)</span>
<span class="p_add">+	if (field &gt;= ARRAY_SIZE(vmcs_field_to_offset_table))</span>
<span class="p_add">+		return -ENOENT;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME: Mitigation for CVE-2017-5753.  To be replaced with a</span>
<span class="p_add">+	 * generic mechanism.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	asm(&quot;lfence&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vmcs_field_to_offset_table[field] == 0)</span>
 		return -ENOENT;
 
 	return vmcs_field_to_offset_table[field];
<span class="p_chunk">@@ -9405,6 +9414,7 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 		/* Save guest registers, load host registers, keep flags */
 		&quot;mov %0, %c[wordsize](%%&quot; _ASM_SP &quot;) \n\t&quot;
 		&quot;pop %0 \n\t&quot;
<span class="p_add">+		&quot;setbe %c[fail](%0)\n\t&quot;</span>
 		&quot;mov %%&quot; _ASM_AX &quot;, %c[rax](%0) \n\t&quot;
 		&quot;mov %%&quot; _ASM_BX &quot;, %c[rbx](%0) \n\t&quot;
 		__ASM_SIZE(pop) &quot; %c[rcx](%0) \n\t&quot;
<span class="p_chunk">@@ -9421,12 +9431,23 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 		&quot;mov %%r13, %c[r13](%0) \n\t&quot;
 		&quot;mov %%r14, %c[r14](%0) \n\t&quot;
 		&quot;mov %%r15, %c[r15](%0) \n\t&quot;
<span class="p_add">+		&quot;xor %%r8d,  %%r8d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r9d,  %%r9d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r10d, %%r10d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r11d, %%r11d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r12d, %%r12d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r13d, %%r13d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r14d, %%r14d \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%r15d, %%r15d \n\t&quot;</span>
 #endif
 		&quot;mov %%cr2, %%&quot; _ASM_AX &quot;   \n\t&quot;
 		&quot;mov %%&quot; _ASM_AX &quot;, %c[cr2](%0) \n\t&quot;
 
<span class="p_add">+		&quot;xor %%eax, %%eax \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%ebx, %%ebx \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%esi, %%esi \n\t&quot;</span>
<span class="p_add">+		&quot;xor %%edi, %%edi \n\t&quot;</span>
 		&quot;pop  %%&quot; _ASM_BP &quot;; pop  %%&quot; _ASM_DX &quot; \n\t&quot;
<span class="p_del">-		&quot;setbe %c[fail](%0) \n\t&quot;</span>
 		&quot;.pushsection .rodata \n\t&quot;
 		&quot;.global vmx_return \n\t&quot;
 		&quot;vmx_return: &quot; _ASM_PTR &quot; 2b \n\t&quot;
<span class="p_chunk">@@ -9463,6 +9484,9 @@</span> <span class="p_context"> static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)</span>
 #endif
 	      );
 
<span class="p_add">+	/* Eliminate branch target predictions from guest mode */</span>
<span class="p_add">+	vmexit_fill_RSB();</span>
<span class="p_add">+</span>
 	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 	if (debugctlmsr)
 		update_debugctlmsr(debugctlmsr);
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 075619a92ce7..575c8953cc9a 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -4362,7 +4362,7 @@</span> <span class="p_context"> static int vcpu_mmio_read(struct kvm_vcpu *vcpu, gpa_t addr, int len, void *v)</span>
 					 addr, n, v))
 		    &amp;&amp; kvm_io_bus_read(vcpu, KVM_MMIO_BUS, addr, n, v))
 			break;
<span class="p_del">-		trace_kvm_mmio(KVM_TRACE_MMIO_READ, n, addr, *(u64 *)v);</span>
<span class="p_add">+		trace_kvm_mmio(KVM_TRACE_MMIO_READ, n, addr, v);</span>
 		handled += n;
 		addr += n;
 		len -= n;
<span class="p_chunk">@@ -4621,7 +4621,7 @@</span> <span class="p_context"> static int read_prepare(struct kvm_vcpu *vcpu, void *val, int bytes)</span>
 {
 	if (vcpu-&gt;mmio_read_completed) {
 		trace_kvm_mmio(KVM_TRACE_MMIO_READ, bytes,
<span class="p_del">-			       vcpu-&gt;mmio_fragments[0].gpa, *(u64 *)val);</span>
<span class="p_add">+			       vcpu-&gt;mmio_fragments[0].gpa, val);</span>
 		vcpu-&gt;mmio_read_completed = 0;
 		return 1;
 	}
<span class="p_chunk">@@ -4643,14 +4643,14 @@</span> <span class="p_context"> static int write_emulate(struct kvm_vcpu *vcpu, gpa_t gpa,</span>
 
 static int write_mmio(struct kvm_vcpu *vcpu, gpa_t gpa, int bytes, void *val)
 {
<span class="p_del">-	trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, bytes, gpa, *(u64 *)val);</span>
<span class="p_add">+	trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, bytes, gpa, val);</span>
 	return vcpu_mmio_write(vcpu, gpa, bytes, val);
 }
 
 static int read_exit_mmio(struct kvm_vcpu *vcpu, gpa_t gpa,
 			  void *val, int bytes)
 {
<span class="p_del">-	trace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, bytes, gpa, 0);</span>
<span class="p_add">+	trace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, bytes, gpa, NULL);</span>
 	return X86EMUL_IO_NEEDED;
 }
 
<span class="p_header">diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile</span>
<span class="p_header">index 457f681ef379..d435c89875c1 100644</span>
<span class="p_header">--- a/arch/x86/lib/Makefile</span>
<span class="p_header">+++ b/arch/x86/lib/Makefile</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"> lib-y += memcpy_$(BITS).o</span>
 lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o
 lib-$(CONFIG_INSTRUCTION_DECODER) += insn.o inat.o
 lib-$(CONFIG_RANDOMIZE_BASE) += kaslr.o
<span class="p_add">+lib-$(CONFIG_RETPOLINE) += retpoline.o</span>
 
 obj-y += msr.o msr-reg.o msr-reg-export.o hweight.o
 
<span class="p_header">diff --git a/arch/x86/lib/checksum_32.S b/arch/x86/lib/checksum_32.S</span>
<span class="p_header">index 4d34bb548b41..46e71a74e612 100644</span>
<span class="p_header">--- a/arch/x86/lib/checksum_32.S</span>
<span class="p_header">+++ b/arch/x86/lib/checksum_32.S</span>
<span class="p_chunk">@@ -29,7 +29,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/errno.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/export.h&gt;
<span class="p_del">-				</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+</span>
 /*
  * computes a partial checksum, e.g. for TCP/UDP fragments
  */
<span class="p_chunk">@@ -156,7 +157,7 @@</span> <span class="p_context"> ENTRY(csum_partial)</span>
 	negl %ebx
 	lea 45f(%ebx,%ebx,2), %ebx
 	testl %esi, %esi
<span class="p_del">-	jmp *%ebx</span>
<span class="p_add">+	JMP_NOSPEC %ebx</span>
 
 	# Handle 2-byte-aligned regions
 20:	addw (%esi), %ax
<span class="p_chunk">@@ -439,7 +440,7 @@</span> <span class="p_context"> ENTRY(csum_partial_copy_generic)</span>
 	andl $-32,%edx
 	lea 3f(%ebx,%ebx), %ebx
 	testl %esi, %esi 
<span class="p_del">-	jmp *%ebx</span>
<span class="p_add">+	JMP_NOSPEC %ebx</span>
 1:	addl $64,%esi
 	addl $64,%edi 
 	SRC(movb -32(%edx),%bl)	; SRC(movb (%edx),%bl)
<span class="p_header">diff --git a/arch/x86/lib/retpoline.S b/arch/x86/lib/retpoline.S</span>
new file mode 100644
<span class="p_header">index 000000000000..cb45c6cb465f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/lib/retpoline.S</span>
<span class="p_chunk">@@ -0,0 +1,48 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/stringify.h&gt;</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+#include &lt;asm/dwarf2.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+#include &lt;asm/export.h&gt;</span>
<span class="p_add">+#include &lt;asm/nospec-branch.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+.macro THUNK reg</span>
<span class="p_add">+	.section .text.__x86.indirect_thunk.\reg</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(__x86_indirect_thunk_\reg)</span>
<span class="p_add">+	CFI_STARTPROC</span>
<span class="p_add">+	JMP_NOSPEC %\reg</span>
<span class="p_add">+	CFI_ENDPROC</span>
<span class="p_add">+ENDPROC(__x86_indirect_thunk_\reg)</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Despite being an assembler file we can&#39;t just use .irp here</span>
<span class="p_add">+ * because __KSYM_DEPS__ only uses the C preprocessor and would</span>
<span class="p_add">+ * only see one instance of &quot;__x86_indirect_thunk_\reg&quot; rather</span>
<span class="p_add">+ * than one per register with the correct names. So we do it</span>
<span class="p_add">+ * the simple and nasty way...</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define EXPORT_THUNK(reg) EXPORT_SYMBOL(__x86_indirect_thunk_ ## reg)</span>
<span class="p_add">+#define GENERATE_THUNK(reg) THUNK reg ; EXPORT_THUNK(reg)</span>
<span class="p_add">+</span>
<span class="p_add">+GENERATE_THUNK(_ASM_AX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_BX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_CX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_DX)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_SI)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_DI)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_BP)</span>
<span class="p_add">+GENERATE_THUNK(_ASM_SP)</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+GENERATE_THUNK(r8)</span>
<span class="p_add">+GENERATE_THUNK(r9)</span>
<span class="p_add">+GENERATE_THUNK(r10)</span>
<span class="p_add">+GENERATE_THUNK(r11)</span>
<span class="p_add">+GENERATE_THUNK(r12)</span>
<span class="p_add">+GENERATE_THUNK(r13)</span>
<span class="p_add">+GENERATE_THUNK(r14)</span>
<span class="p_add">+GENERATE_THUNK(r15)</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index 43d4a4a29037..ce38f165489b 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -149,7 +149,7 @@</span> <span class="p_context"> pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
  *
  * Returns a pointer to a P4D on success, or NULL on failure.
  */
<span class="p_del">-static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
<span class="p_add">+static __init p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
 {
 	pgd_t *pgd = kernel_to_user_pgdp(pgd_offset_k(address));
 	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);
<span class="p_chunk">@@ -164,12 +164,7 @@</span> <span class="p_context"> static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
 		if (!new_p4d_page)
 			return NULL;
 
<span class="p_del">-		if (pgd_none(*pgd)) {</span>
<span class="p_del">-			set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
<span class="p_del">-			new_p4d_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_p4d_page)</span>
<span class="p_del">-			free_page(new_p4d_page);</span>
<span class="p_add">+		set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
 	}
 	BUILD_BUG_ON(pgd_large(*pgd) != 0);
 
<span class="p_chunk">@@ -182,7 +177,7 @@</span> <span class="p_context"> static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
  *
  * Returns a pointer to a PMD on success, or NULL on failure.
  */
<span class="p_del">-static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
<span class="p_add">+static __init pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 {
 	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);
 	p4d_t *p4d = pti_user_pagetable_walk_p4d(address);
<span class="p_chunk">@@ -194,12 +189,7 @@</span> <span class="p_context"> static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 		if (!new_pud_page)
 			return NULL;
 
<span class="p_del">-		if (p4d_none(*p4d)) {</span>
<span class="p_del">-			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_del">-			new_pud_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pud_page)</span>
<span class="p_del">-			free_page(new_pud_page);</span>
<span class="p_add">+		set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
 	}
 
 	pud = pud_offset(p4d, address);
<span class="p_chunk">@@ -213,12 +203,7 @@</span> <span class="p_context"> static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
 		if (!new_pmd_page)
 			return NULL;
 
<span class="p_del">-		if (pud_none(*pud)) {</span>
<span class="p_del">-			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_del">-			new_pmd_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pmd_page)</span>
<span class="p_del">-			free_page(new_pmd_page);</span>
<span class="p_add">+		set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
 	}
 
 	return pmd_offset(pud, address);
<span class="p_chunk">@@ -251,12 +236,7 @@</span> <span class="p_context"> static __init pte_t *pti_user_pagetable_walk_pte(unsigned long address)</span>
 		if (!new_pte_page)
 			return NULL;
 
<span class="p_del">-		if (pmd_none(*pmd)) {</span>
<span class="p_del">-			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
<span class="p_del">-			new_pte_page = 0;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (new_pte_page)</span>
<span class="p_del">-			free_page(new_pte_page);</span>
<span class="p_add">+		set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
 	}
 
 	pte = pte_offset_kernel(pmd, address);
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">index 39c4b35ac7a4..61975b6bcb1a 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_chunk">@@ -134,7 +134,9 @@</span> <span class="p_context"> pgd_t * __init efi_call_phys_prolog(void)</span>
 				pud[j] = *pud_offset(p4d_k, vaddr);
 			}
 		}
<span class="p_add">+		pgd_offset_k(pgd * PGDIR_SIZE)-&gt;pgd &amp;= ~_PAGE_NX;</span>
 	}
<span class="p_add">+</span>
 out:
 	__flush_tlb_all();
 
<span class="p_header">diff --git a/crypto/algapi.c b/crypto/algapi.c</span>
<span class="p_header">index aa699ff6c876..50eb828db767 100644</span>
<span class="p_header">--- a/crypto/algapi.c</span>
<span class="p_header">+++ b/crypto/algapi.c</span>
<span class="p_chunk">@@ -167,6 +167,18 @@</span> <span class="p_context"> void crypto_remove_spawns(struct crypto_alg *alg, struct list_head *list,</span>
 
 			spawn-&gt;alg = NULL;
 			spawns = &amp;inst-&gt;alg.cra_users;
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We may encounter an unregistered instance here, since</span>
<span class="p_add">+			 * an instance&#39;s spawns are set up prior to the instance</span>
<span class="p_add">+			 * being registered.  An unregistered instance will have</span>
<span class="p_add">+			 * NULL -&gt;cra_users.next, since -&gt;cra_users isn&#39;t</span>
<span class="p_add">+			 * properly initialized until registration.  But an</span>
<span class="p_add">+			 * unregistered instance cannot have any users, so treat</span>
<span class="p_add">+			 * it the same as -&gt;cra_users being empty.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (spawns-&gt;next == NULL)</span>
<span class="p_add">+				break;</span>
 		}
 	} while ((spawns = crypto_more_spawns(alg, &amp;stack, &amp;top,
 					      &amp;secondary_spawns)));
<span class="p_header">diff --git a/drivers/base/Kconfig b/drivers/base/Kconfig</span>
<span class="p_header">index bdc87907d6a1..2415ad9f6dd4 100644</span>
<span class="p_header">--- a/drivers/base/Kconfig</span>
<span class="p_header">+++ b/drivers/base/Kconfig</span>
<span class="p_chunk">@@ -236,6 +236,9 @@</span> <span class="p_context"> config GENERIC_CPU_DEVICES</span>
 config GENERIC_CPU_AUTOPROBE
 	bool
 
<span class="p_add">+config GENERIC_CPU_VULNERABILITIES</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
 config SOC_BUS
 	bool
 	select GLOB
<span class="p_header">diff --git a/drivers/base/cpu.c b/drivers/base/cpu.c</span>
<span class="p_header">index 321cd7b4d817..825964efda1d 100644</span>
<span class="p_header">--- a/drivers/base/cpu.c</span>
<span class="p_header">+++ b/drivers/base/cpu.c</span>
<span class="p_chunk">@@ -501,10 +501,58 @@</span> <span class="p_context"> static void __init cpu_dev_register_generic(void)</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_GENERIC_CPU_VULNERABILITIES</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t __weak cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return sprintf(buf, &quot;Not affected\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static DEVICE_ATTR(meltdown, 0444, cpu_show_meltdown, NULL);</span>
<span class="p_add">+static DEVICE_ATTR(spectre_v1, 0444, cpu_show_spectre_v1, NULL);</span>
<span class="p_add">+static DEVICE_ATTR(spectre_v2, 0444, cpu_show_spectre_v2, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *cpu_root_vulnerabilities_attrs[] = {</span>
<span class="p_add">+	&amp;dev_attr_meltdown.attr,</span>
<span class="p_add">+	&amp;dev_attr_spectre_v1.attr,</span>
<span class="p_add">+	&amp;dev_attr_spectre_v2.attr,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct attribute_group cpu_root_vulnerabilities_group = {</span>
<span class="p_add">+	.name  = &quot;vulnerabilities&quot;,</span>
<span class="p_add">+	.attrs = cpu_root_vulnerabilities_attrs,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init cpu_register_vulnerabilities(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (sysfs_create_group(&amp;cpu_subsys.dev_root-&gt;kobj,</span>
<span class="p_add">+			       &amp;cpu_root_vulnerabilities_group))</span>
<span class="p_add">+		pr_err(&quot;Unable to register CPU vulnerabilities\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void cpu_register_vulnerabilities(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 void __init cpu_dev_init(void)
 {
 	if (subsys_system_register(&amp;cpu_subsys, cpu_root_attr_groups))
 		panic(&quot;Failed to register CPU subsystem&quot;);
 
 	cpu_dev_register_generic();
<span class="p_add">+	cpu_register_vulnerabilities();</span>
 }
<span class="p_header">diff --git a/drivers/block/rbd.c b/drivers/block/rbd.c</span>
<span class="p_header">index adc877dfef5c..609227211295 100644</span>
<span class="p_header">--- a/drivers/block/rbd.c</span>
<span class="p_header">+++ b/drivers/block/rbd.c</span>
<span class="p_chunk">@@ -3074,13 +3074,21 @@</span> <span class="p_context"> static void format_lock_cookie(struct rbd_device *rbd_dev, char *buf)</span>
 	mutex_unlock(&amp;rbd_dev-&gt;watch_mutex);
 }
 
<span class="p_add">+static void __rbd_lock(struct rbd_device *rbd_dev, const char *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rbd_client_id cid = rbd_get_cid(rbd_dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	strcpy(rbd_dev-&gt;lock_cookie, cookie);</span>
<span class="p_add">+	rbd_set_owner_cid(rbd_dev, &amp;cid);</span>
<span class="p_add">+	queue_work(rbd_dev-&gt;task_wq, &amp;rbd_dev-&gt;acquired_lock_work);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * lock_rwsem must be held for write
  */
 static int rbd_lock(struct rbd_device *rbd_dev)
 {
 	struct ceph_osd_client *osdc = &amp;rbd_dev-&gt;rbd_client-&gt;client-&gt;osdc;
<span class="p_del">-	struct rbd_client_id cid = rbd_get_cid(rbd_dev);</span>
 	char cookie[32];
 	int ret;
 
<span class="p_chunk">@@ -3095,9 +3103,7 @@</span> <span class="p_context"> static int rbd_lock(struct rbd_device *rbd_dev)</span>
 		return ret;
 
 	rbd_dev-&gt;lock_state = RBD_LOCK_STATE_LOCKED;
<span class="p_del">-	strcpy(rbd_dev-&gt;lock_cookie, cookie);</span>
<span class="p_del">-	rbd_set_owner_cid(rbd_dev, &amp;cid);</span>
<span class="p_del">-	queue_work(rbd_dev-&gt;task_wq, &amp;rbd_dev-&gt;acquired_lock_work);</span>
<span class="p_add">+	__rbd_lock(rbd_dev, cookie);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -3883,7 +3889,7 @@</span> <span class="p_context"> static void rbd_reacquire_lock(struct rbd_device *rbd_dev)</span>
 			queue_delayed_work(rbd_dev-&gt;task_wq,
 					   &amp;rbd_dev-&gt;lock_dwork, 0);
 	} else {
<span class="p_del">-		strcpy(rbd_dev-&gt;lock_cookie, cookie);</span>
<span class="p_add">+		__rbd_lock(rbd_dev, cookie);</span>
 	}
 }
 
<span class="p_chunk">@@ -4415,7 +4421,7 @@</span> <span class="p_context"> static int rbd_init_disk(struct rbd_device *rbd_dev)</span>
 	segment_size = rbd_obj_bytes(&amp;rbd_dev-&gt;header);
 	blk_queue_max_hw_sectors(q, segment_size / SECTOR_SIZE);
 	q-&gt;limits.max_sectors = queue_max_hw_sectors(q);
<span class="p_del">-	blk_queue_max_segments(q, segment_size / SECTOR_SIZE);</span>
<span class="p_add">+	blk_queue_max_segments(q, USHRT_MAX);</span>
 	blk_queue_max_segment_size(q, segment_size);
 	blk_queue_io_min(q, segment_size);
 	blk_queue_io_opt(q, segment_size);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/gvt/gtt.c b/drivers/gpu/drm/i915/gvt/gtt.c</span>
<span class="p_header">index a385838e2919..dadacbe558ab 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/gvt/gtt.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/gvt/gtt.c</span>
<span class="p_chunk">@@ -1359,12 +1359,15 @@</span> <span class="p_context"> static int ppgtt_handle_guest_write_page_table_bytes(void *gp,</span>
 			return ret;
 	} else {
 		if (!test_bit(index, spt-&gt;post_shadow_bitmap)) {
<span class="p_add">+			int type = spt-&gt;shadow_page.type;</span>
<span class="p_add">+</span>
 			ppgtt_get_shadow_entry(spt, &amp;se, index);
 			ret = ppgtt_handle_guest_entry_removal(gpt, &amp;se, index);
 			if (ret)
 				return ret;
<span class="p_add">+			ops-&gt;set_pfn(&amp;se, vgpu-&gt;gtt.scratch_pt[type].page_mfn);</span>
<span class="p_add">+			ppgtt_set_shadow_entry(spt, &amp;se, index);</span>
 		}
<span class="p_del">-</span>
 		ppgtt_set_post_shadow(spt, index);
 	}
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c</span>
<span class="p_header">index 82498f8232eb..5c5cb2ceee49 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_drv.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_drv.c</span>
<span class="p_chunk">@@ -1693,6 +1693,7 @@</span> <span class="p_context"> static int i915_drm_resume(struct drm_device *dev)</span>
 	intel_guc_resume(dev_priv);
 
 	intel_modeset_init_hw(dev);
<span class="p_add">+	intel_init_clock_gating(dev_priv);</span>
 
 	spin_lock_irq(&amp;dev_priv-&gt;irq_lock);
 	if (dev_priv-&gt;display.hpd_irq_setup)
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_header">index ce2ed16f2a30..920c8914cec1 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_reg.h</span>
<span class="p_chunk">@@ -6987,6 +6987,8 @@</span> <span class="p_context"> enum {</span>
 #define GEN9_SLICE_COMMON_ECO_CHICKEN0		_MMIO(0x7308)
 #define  DISABLE_PIXEL_MASK_CAMMING		(1&lt;&lt;14)
 
<span class="p_add">+#define GEN9_SLICE_COMMON_ECO_CHICKEN1		_MMIO(0x731c)</span>
<span class="p_add">+</span>
 #define GEN7_L3SQCREG1				_MMIO(0xB010)
 #define  VLV_B0_WA_L3SQCREG1_VALUE		0x00D30000
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c</span>
<span class="p_header">index 1c73d5542681..095a2240af4f 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/intel_display.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/intel_display.c</span>
<span class="p_chunk">@@ -3800,6 +3800,7 @@</span> <span class="p_context"> void intel_finish_reset(struct drm_i915_private *dev_priv)</span>
 
 		intel_pps_unlock_regs_wa(dev_priv);
 		intel_modeset_init_hw(dev);
<span class="p_add">+		intel_init_clock_gating(dev_priv);</span>
 
 		spin_lock_irq(&amp;dev_priv-&gt;irq_lock);
 		if (dev_priv-&gt;display.hpd_irq_setup)
<span class="p_chunk">@@ -14406,8 +14407,6 @@</span> <span class="p_context"> void intel_modeset_init_hw(struct drm_device *dev)</span>
 
 	intel_update_cdclk(dev_priv);
 	dev_priv-&gt;cdclk.logical = dev_priv-&gt;cdclk.actual = dev_priv-&gt;cdclk.hw;
<span class="p_del">-</span>
<span class="p_del">-	intel_init_clock_gating(dev_priv);</span>
 }
 
 /*
<span class="p_chunk">@@ -15124,6 +15123,15 @@</span> <span class="p_context"> intel_modeset_setup_hw_state(struct drm_device *dev,</span>
 	struct intel_encoder *encoder;
 	int i;
 
<span class="p_add">+	if (IS_HASWELL(dev_priv)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * WaRsPkgCStateDisplayPMReq:hsw</span>
<span class="p_add">+		 * System hang if this isn&#39;t done before disabling all planes!</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		I915_WRITE(CHICKEN_PAR1_1,</span>
<span class="p_add">+			   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	intel_modeset_readout_hw_state(dev);
 
 	/* HW state is read out, now we need to sanitize this mess. */
<span class="p_chunk">@@ -15220,6 +15228,8 @@</span> <span class="p_context"> void intel_modeset_gem_init(struct drm_device *dev)</span>
 
 	intel_init_gt_powersave(dev_priv);
 
<span class="p_add">+	intel_init_clock_gating(dev_priv);</span>
<span class="p_add">+</span>
 	intel_setup_overlay(dev_priv);
 }
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/intel_engine_cs.c b/drivers/gpu/drm/i915/intel_engine_cs.c</span>
<span class="p_header">index 3c2d9cf22ed5..b6a7e492c1a3 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/intel_engine_cs.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/intel_engine_cs.c</span>
<span class="p_chunk">@@ -1125,6 +1125,11 @@</span> <span class="p_context"> static int glk_init_workarounds(struct intel_engine_cs *engine)</span>
 	if (ret)
 		return ret;
 
<span class="p_add">+	/* WA #0862: Userspace has to set &quot;Barrier Mode&quot; to avoid hangs. */</span>
<span class="p_add">+	ret = wa_ring_whitelist_reg(engine, GEN9_SLICE_COMMON_ECO_CHICKEN1);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
 	/* WaToEnableHwFixForPushConstHWBug:glk */
 	WA_SET_BIT_MASKED(COMMON_SLICE_CHICKEN2,
 			  GEN8_SBE_DISABLE_REPLAY_BUF_OPTIMIZATION);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c</span>
<span class="p_header">index cb950752c346..014e5c08571a 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/intel_pm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/intel_pm.c</span>
<span class="p_chunk">@@ -5669,12 +5669,30 @@</span> <span class="p_context"> void vlv_wm_sanitize(struct drm_i915_private *dev_priv)</span>
 	mutex_unlock(&amp;dev_priv-&gt;wm.wm_mutex);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME should probably kill this and improve</span>
<span class="p_add">+ * the real watermark readout/sanitation instead</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void ilk_init_lp_watermarks(struct drm_i915_private *dev_priv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_add">+	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_add">+	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Don&#39;t touch WM1S_LP_EN here.</span>
<span class="p_add">+	 * Doing so could cause underruns.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void ilk_wm_get_hw_state(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct ilk_wm_values *hw = &amp;dev_priv-&gt;wm.hw;
 	struct drm_crtc *crtc;
 
<span class="p_add">+	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_add">+</span>
 	for_each_crtc(dev, crtc)
 		ilk_pipe_wm_get_hw_state(crtc);
 
<span class="p_chunk">@@ -7959,18 +7977,6 @@</span> <span class="p_context"> static void g4x_disable_trickle_feed(struct drm_i915_private *dev_priv)</span>
 	}
 }
 
<span class="p_del">-static void ilk_init_lp_watermarks(struct drm_i915_private *dev_priv)</span>
<span class="p_del">-{</span>
<span class="p_del">-	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_del">-	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_del">-	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) &amp; ~WM1_LP_SR_EN);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Don&#39;t touch WM1S_LP_EN here.</span>
<span class="p_del">-	 * Doing so could cause underruns.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void ironlake_init_clock_gating(struct drm_i915_private *dev_priv)
 {
 	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
<span class="p_chunk">@@ -8004,8 +8010,6 @@</span> <span class="p_context"> static void ironlake_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 		   (I915_READ(DISP_ARB_CTL) |
 		    DISP_FBC_WM_DIS));
 
<span class="p_del">-	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_del">-</span>
 	/*
 	 * Based on the document from hardware guys the following bits
 	 * should be set unconditionally in order to enable FBC.
<span class="p_chunk">@@ -8118,8 +8122,6 @@</span> <span class="p_context"> static void gen6_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 	I915_WRITE(GEN6_GT_MODE,
 		   _MASKED_FIELD(GEN6_WIZ_HASHING_MASK, GEN6_WIZ_HASHING_16x4));
 
<span class="p_del">-	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_del">-</span>
 	I915_WRITE(CACHE_MODE_0,
 		   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
 
<span class="p_chunk">@@ -8293,8 +8295,6 @@</span> <span class="p_context"> static void broadwell_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 {
 	enum pipe pipe;
 
<span class="p_del">-	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_del">-</span>
 	/* WaSwitchSolVfFArbitrationPriority:bdw */
 	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
 
<span class="p_chunk">@@ -8349,8 +8349,6 @@</span> <span class="p_context"> static void broadwell_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 
 static void haswell_init_clock_gating(struct drm_i915_private *dev_priv)
 {
<span class="p_del">-	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_del">-</span>
 	/* L3 caching of data atomics doesn&#39;t work -- disable it. */
 	I915_WRITE(HSW_SCRATCH1, HSW_SCRATCH1_L3_DATA_ATOMICS_DISABLE);
 	I915_WRITE(HSW_ROW_CHICKEN3,
<span class="p_chunk">@@ -8394,10 +8392,6 @@</span> <span class="p_context"> static void haswell_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 	/* WaSwitchSolVfFArbitrationPriority:hsw */
 	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
 
<span class="p_del">-	/* WaRsPkgCStateDisplayPMReq:hsw */</span>
<span class="p_del">-	I915_WRITE(CHICKEN_PAR1_1,</span>
<span class="p_del">-		   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);</span>
<span class="p_del">-</span>
 	lpt_init_clock_gating(dev_priv);
 }
 
<span class="p_chunk">@@ -8405,8 +8399,6 @@</span> <span class="p_context"> static void ivybridge_init_clock_gating(struct drm_i915_private *dev_priv)</span>
 {
 	uint32_t snpcr;
 
<span class="p_del">-	ilk_init_lp_watermarks(dev_priv);</span>
<span class="p_del">-</span>
 	I915_WRITE(ILK_DSPCLK_GATE_D, ILK_VRHUNIT_CLOCK_GATE_DISABLE);
 
 	/* WaDisableEarlyCull:ivb */
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_execbuf.c b/drivers/gpu/drm/vmwgfx/vmwgfx_execbuf.c</span>
<span class="p_header">index 21c62a34e558..87e8af5776a3 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_execbuf.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_execbuf.c</span>
<span class="p_chunk">@@ -2731,6 +2731,8 @@</span> <span class="p_context"> static int vmw_cmd_dx_view_define(struct vmw_private *dev_priv,</span>
 	}
 
 	view_type = vmw_view_cmd_to_type(header-&gt;id);
<span class="p_add">+	if (view_type == vmw_view_max)</span>
<span class="p_add">+		return -EINVAL;</span>
 	cmd = container_of(header, typeof(*cmd), header);
 	ret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,
 				user_surface_converter,
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_kms.c b/drivers/gpu/drm/vmwgfx/vmwgfx_kms.c</span>
<span class="p_header">index b850562fbdd6..62c2f4be8012 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_kms.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_kms.c</span>
<span class="p_chunk">@@ -697,7 +697,6 @@</span> <span class="p_context"> vmw_du_plane_duplicate_state(struct drm_plane *plane)</span>
 	vps-&gt;pinned = 0;
 
 	/* Mapping is managed by prepare_fb/cleanup_fb */
<span class="p_del">-	memset(&amp;vps-&gt;guest_map, 0, sizeof(vps-&gt;guest_map));</span>
 	memset(&amp;vps-&gt;host_map, 0, sizeof(vps-&gt;host_map));
 	vps-&gt;cpp = 0;
 
<span class="p_chunk">@@ -760,11 +759,6 @@</span> <span class="p_context"> vmw_du_plane_destroy_state(struct drm_plane *plane,</span>
 
 
 	/* Should have been freed by cleanup_fb */
<span class="p_del">-	if (vps-&gt;guest_map.virtual) {</span>
<span class="p_del">-		DRM_ERROR(&quot;Guest mapping not freed\n&quot;);</span>
<span class="p_del">-		ttm_bo_kunmap(&amp;vps-&gt;guest_map);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	if (vps-&gt;host_map.virtual) {
 		DRM_ERROR(&quot;Host mapping not freed\n&quot;);
 		ttm_bo_kunmap(&amp;vps-&gt;host_map);
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_kms.h b/drivers/gpu/drm/vmwgfx/vmwgfx_kms.h</span>
<span class="p_header">index ff9c8389ff21..cd9da2dd79af 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_kms.h</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_kms.h</span>
<span class="p_chunk">@@ -175,7 +175,7 @@</span> <span class="p_context"> struct vmw_plane_state {</span>
 	int pinned;
 
 	/* For CPU Blit */
<span class="p_del">-	struct ttm_bo_kmap_obj host_map, guest_map;</span>
<span class="p_add">+	struct ttm_bo_kmap_obj host_map;</span>
 	unsigned int cpp;
 };
 
<span class="p_header">diff --git a/drivers/gpu/drm/vmwgfx/vmwgfx_stdu.c b/drivers/gpu/drm/vmwgfx/vmwgfx_stdu.c</span>
<span class="p_header">index ca3afae2db1f..4dee05b15552 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vmwgfx/vmwgfx_stdu.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vmwgfx/vmwgfx_stdu.c</span>
<span class="p_chunk">@@ -114,7 +114,7 @@</span> <span class="p_context"> struct vmw_screen_target_display_unit {</span>
 	bool defined;
 
 	/* For CPU Blit */
<span class="p_del">-	struct ttm_bo_kmap_obj host_map, guest_map;</span>
<span class="p_add">+	struct ttm_bo_kmap_obj host_map;</span>
 	unsigned int cpp;
 };
 
<span class="p_chunk">@@ -695,7 +695,8 @@</span> <span class="p_context"> static void vmw_stdu_dmabuf_cpu_commit(struct vmw_kms_dirty *dirty)</span>
 	s32 src_pitch, dst_pitch;
 	u8 *src, *dst;
 	bool not_used;
<span class="p_del">-</span>
<span class="p_add">+	struct ttm_bo_kmap_obj guest_map;</span>
<span class="p_add">+	int ret;</span>
 
 	if (!dirty-&gt;num_hits)
 		return;
<span class="p_chunk">@@ -706,6 +707,13 @@</span> <span class="p_context"> static void vmw_stdu_dmabuf_cpu_commit(struct vmw_kms_dirty *dirty)</span>
 	if (width == 0 || height == 0)
 		return;
 
<span class="p_add">+	ret = ttm_bo_kmap(&amp;ddirty-&gt;buf-&gt;base, 0, ddirty-&gt;buf-&gt;base.num_pages,</span>
<span class="p_add">+			  &amp;guest_map);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		DRM_ERROR(&quot;Failed mapping framebuffer for blit: %d\n&quot;,</span>
<span class="p_add">+			  ret);</span>
<span class="p_add">+		goto out_cleanup;</span>
<span class="p_add">+	}</span>
 
 	/* Assume we are blitting from Host (display_srf) to Guest (dmabuf) */
 	src_pitch = stdu-&gt;display_srf-&gt;base_size.width * stdu-&gt;cpp;
<span class="p_chunk">@@ -713,7 +721,7 @@</span> <span class="p_context"> static void vmw_stdu_dmabuf_cpu_commit(struct vmw_kms_dirty *dirty)</span>
 	src += ddirty-&gt;top * src_pitch + ddirty-&gt;left * stdu-&gt;cpp;
 
 	dst_pitch = ddirty-&gt;pitch;
<span class="p_del">-	dst = ttm_kmap_obj_virtual(&amp;stdu-&gt;guest_map, &amp;not_used);</span>
<span class="p_add">+	dst = ttm_kmap_obj_virtual(&amp;guest_map, &amp;not_used);</span>
 	dst += ddirty-&gt;fb_top * dst_pitch + ddirty-&gt;fb_left * stdu-&gt;cpp;
 
 
<span class="p_chunk">@@ -772,6 +780,7 @@</span> <span class="p_context"> static void vmw_stdu_dmabuf_cpu_commit(struct vmw_kms_dirty *dirty)</span>
 		vmw_fifo_commit(dev_priv, sizeof(*cmd));
 	}
 
<span class="p_add">+	ttm_bo_kunmap(&amp;guest_map);</span>
 out_cleanup:
 	ddirty-&gt;left = ddirty-&gt;top = ddirty-&gt;fb_left = ddirty-&gt;fb_top = S32_MAX;
 	ddirty-&gt;right = ddirty-&gt;bottom = S32_MIN;
<span class="p_chunk">@@ -1109,9 +1118,6 @@</span> <span class="p_context"> vmw_stdu_primary_plane_cleanup_fb(struct drm_plane *plane,</span>
 {
 	struct vmw_plane_state *vps = vmw_plane_state_to_vps(old_state);
 
<span class="p_del">-	if (vps-&gt;guest_map.virtual)</span>
<span class="p_del">-		ttm_bo_kunmap(&amp;vps-&gt;guest_map);</span>
<span class="p_del">-</span>
 	if (vps-&gt;host_map.virtual)
 		ttm_bo_kunmap(&amp;vps-&gt;host_map);
 
<span class="p_chunk">@@ -1277,33 +1283,11 @@</span> <span class="p_context"> vmw_stdu_primary_plane_prepare_fb(struct drm_plane *plane,</span>
 	 */
 	if (vps-&gt;content_fb_type == SEPARATE_DMA &amp;&amp;
 	    !(dev_priv-&gt;capabilities &amp; SVGA_CAP_3D)) {
<span class="p_del">-</span>
<span class="p_del">-		struct vmw_framebuffer_dmabuf *new_vfbd;</span>
<span class="p_del">-</span>
<span class="p_del">-		new_vfbd = vmw_framebuffer_to_vfbd(new_fb);</span>
<span class="p_del">-</span>
<span class="p_del">-		ret = ttm_bo_reserve(&amp;new_vfbd-&gt;buffer-&gt;base, false, false,</span>
<span class="p_del">-				     NULL);</span>
<span class="p_del">-		if (ret)</span>
<span class="p_del">-			goto out_srf_unpin;</span>
<span class="p_del">-</span>
<span class="p_del">-		ret = ttm_bo_kmap(&amp;new_vfbd-&gt;buffer-&gt;base, 0,</span>
<span class="p_del">-				  new_vfbd-&gt;buffer-&gt;base.num_pages,</span>
<span class="p_del">-				  &amp;vps-&gt;guest_map);</span>
<span class="p_del">-</span>
<span class="p_del">-		ttm_bo_unreserve(&amp;new_vfbd-&gt;buffer-&gt;base);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (ret) {</span>
<span class="p_del">-			DRM_ERROR(&quot;Failed to map content buffer to CPU\n&quot;);</span>
<span class="p_del">-			goto out_srf_unpin;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
 		ret = ttm_bo_kmap(&amp;vps-&gt;surf-&gt;res.backup-&gt;base, 0,
 				  vps-&gt;surf-&gt;res.backup-&gt;base.num_pages,
 				  &amp;vps-&gt;host_map);
 		if (ret) {
 			DRM_ERROR(&quot;Failed to map display buffer to CPU\n&quot;);
<span class="p_del">-			ttm_bo_kunmap(&amp;vps-&gt;guest_map);</span>
 			goto out_srf_unpin;
 		}
 
<span class="p_chunk">@@ -1350,7 +1334,6 @@</span> <span class="p_context"> vmw_stdu_primary_plane_atomic_update(struct drm_plane *plane,</span>
 	stdu-&gt;display_srf = vps-&gt;surf;
 	stdu-&gt;content_fb_type = vps-&gt;content_fb_type;
 	stdu-&gt;cpp = vps-&gt;cpp;
<span class="p_del">-	memcpy(&amp;stdu-&gt;guest_map, &amp;vps-&gt;guest_map, sizeof(vps-&gt;guest_map));</span>
 	memcpy(&amp;stdu-&gt;host_map, &amp;vps-&gt;host_map, sizeof(vps-&gt;host_map));
 
 	if (!stdu-&gt;defined)
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/cq.c b/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_header">index 514c1000ded1..73feeeeb4283 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_chunk">@@ -410,7 +410,7 @@</span> <span class="p_context"> void c4iw_flush_hw_cq(struct c4iw_cq *chp)</span>
 
 static int cqe_completes_wr(struct t4_cqe *cqe, struct t4_wq *wq)
 {
<span class="p_del">-	if (CQE_OPCODE(cqe) == C4IW_DRAIN_OPCODE) {</span>
<span class="p_add">+	if (DRAIN_CQE(cqe)) {</span>
 		WARN_ONCE(1, &quot;Unexpected DRAIN CQE qp id %u!\n&quot;, wq-&gt;sq.qid);
 		return 0;
 	}
<span class="p_chunk">@@ -509,7 +509,7 @@</span> <span class="p_context"> static int poll_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,</span>
 	/*
 	 * Special cqe for drain WR completions...
 	 */
<span class="p_del">-	if (CQE_OPCODE(hw_cqe) == C4IW_DRAIN_OPCODE) {</span>
<span class="p_add">+	if (DRAIN_CQE(hw_cqe)) {</span>
 		*cookie = CQE_DRAIN_COOKIE(hw_cqe);
 		*cqe = *hw_cqe;
 		goto skip_cqe;
<span class="p_chunk">@@ -766,9 +766,6 @@</span> <span class="p_context"> static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)</span>
 				c4iw_invalidate_mr(qhp-&gt;rhp,
 						   CQE_WRID_FR_STAG(&amp;cqe));
 			break;
<span class="p_del">-		case C4IW_DRAIN_OPCODE:</span>
<span class="p_del">-			wc-&gt;opcode = IB_WC_SEND;</span>
<span class="p_del">-			break;</span>
 		default:
 			pr_err(&quot;Unexpected opcode %d in the CQE received for QPID=0x%0x\n&quot;,
 			       CQE_OPCODE(&amp;cqe), CQE_QPID(&amp;cqe));
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/ev.c b/drivers/infiniband/hw/cxgb4/ev.c</span>
<span class="p_header">index 8f963df0bffc..9d25298d96fa 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/ev.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/ev.c</span>
<span class="p_chunk">@@ -109,9 +109,11 @@</span> <span class="p_context"> static void post_qp_event(struct c4iw_dev *dev, struct c4iw_cq *chp,</span>
 	if (qhp-&gt;ibqp.event_handler)
 		(*qhp-&gt;ibqp.event_handler)(&amp;event, qhp-&gt;ibqp.qp_context);
 
<span class="p_del">-	spin_lock_irqsave(&amp;chp-&gt;comp_handler_lock, flag);</span>
<span class="p_del">-	(*chp-&gt;ibcq.comp_handler)(&amp;chp-&gt;ibcq, chp-&gt;ibcq.cq_context);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;chp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	if (t4_clear_cq_armed(&amp;chp-&gt;cq)) {</span>
<span class="p_add">+		spin_lock_irqsave(&amp;chp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+		(*chp-&gt;ibcq.comp_handler)(&amp;chp-&gt;ibcq, chp-&gt;ibcq.cq_context);</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;chp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	}</span>
 }
 
 void c4iw_ev_dispatch(struct c4iw_dev *dev, struct t4_cqe *err_cqe)
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h</span>
<span class="p_header">index 819a30635d53..20c481115a99 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/iw_cxgb4.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/iw_cxgb4.h</span>
<span class="p_chunk">@@ -631,8 +631,6 @@</span> <span class="p_context"> static inline int to_ib_qp_state(int c4iw_qp_state)</span>
 	return IB_QPS_ERR;
 }
 
<span class="p_del">-#define C4IW_DRAIN_OPCODE FW_RI_SGE_EC_CR_RETURN</span>
<span class="p_del">-</span>
 static inline u32 c4iw_ib_to_tpt_access(int a)
 {
 	return (a &amp; IB_ACCESS_REMOTE_WRITE ? FW_RI_MEM_ACCESS_REM_WRITE : 0) |
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/qp.c b/drivers/infiniband/hw/cxgb4/qp.c</span>
<span class="p_header">index e69453665a17..f311ea73c806 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/qp.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/qp.c</span>
<span class="p_chunk">@@ -794,21 +794,57 @@</span> <span class="p_context"> static int ring_kernel_rq_db(struct c4iw_qp *qhp, u16 inc)</span>
 	return 0;
 }
 
<span class="p_del">-static void complete_sq_drain_wr(struct c4iw_qp *qhp, struct ib_send_wr *wr)</span>
<span class="p_add">+static int ib_to_fw_opcode(int ib_opcode)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int opcode;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (ib_opcode) {</span>
<span class="p_add">+	case IB_WR_SEND_WITH_INV:</span>
<span class="p_add">+		opcode = FW_RI_SEND_WITH_INV;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IB_WR_SEND:</span>
<span class="p_add">+		opcode = FW_RI_SEND;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IB_WR_RDMA_WRITE:</span>
<span class="p_add">+		opcode = FW_RI_RDMA_WRITE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IB_WR_RDMA_READ:</span>
<span class="p_add">+	case IB_WR_RDMA_READ_WITH_INV:</span>
<span class="p_add">+		opcode = FW_RI_READ_REQ;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IB_WR_REG_MR:</span>
<span class="p_add">+		opcode = FW_RI_FAST_REGISTER;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case IB_WR_LOCAL_INV:</span>
<span class="p_add">+		opcode = FW_RI_LOCAL_INV;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		opcode = -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return opcode;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int complete_sq_drain_wr(struct c4iw_qp *qhp, struct ib_send_wr *wr)</span>
 {
 	struct t4_cqe cqe = {};
 	struct c4iw_cq *schp;
 	unsigned long flag;
 	struct t4_cq *cq;
<span class="p_add">+	int opcode;</span>
 
 	schp = to_c4iw_cq(qhp-&gt;ibqp.send_cq);
 	cq = &amp;schp-&gt;cq;
 
<span class="p_add">+	opcode = ib_to_fw_opcode(wr-&gt;opcode);</span>
<span class="p_add">+	if (opcode &lt; 0)</span>
<span class="p_add">+		return opcode;</span>
<span class="p_add">+</span>
 	cqe.u.drain_cookie = wr-&gt;wr_id;
 	cqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |
<span class="p_del">-				 CQE_OPCODE_V(C4IW_DRAIN_OPCODE) |</span>
<span class="p_add">+				 CQE_OPCODE_V(opcode) |</span>
 				 CQE_TYPE_V(1) |
 				 CQE_SWCQE_V(1) |
<span class="p_add">+				 CQE_DRAIN_V(1) |</span>
 				 CQE_QPID_V(qhp-&gt;wq.sq.qid));
 
 	spin_lock_irqsave(&amp;schp-&gt;lock, flag);
<span class="p_chunk">@@ -817,10 +853,29 @@</span> <span class="p_context"> static void complete_sq_drain_wr(struct c4iw_qp *qhp, struct ib_send_wr *wr)</span>
 	t4_swcq_produce(cq);
 	spin_unlock_irqrestore(&amp;schp-&gt;lock, flag);
 
<span class="p_del">-	spin_lock_irqsave(&amp;schp-&gt;comp_handler_lock, flag);</span>
<span class="p_del">-	(*schp-&gt;ibcq.comp_handler)(&amp;schp-&gt;ibcq,</span>
<span class="p_del">-				   schp-&gt;ibcq.cq_context);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;schp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	if (t4_clear_cq_armed(&amp;schp-&gt;cq)) {</span>
<span class="p_add">+		spin_lock_irqsave(&amp;schp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+		(*schp-&gt;ibcq.comp_handler)(&amp;schp-&gt;ibcq,</span>
<span class="p_add">+					   schp-&gt;ibcq.cq_context);</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;schp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int complete_sq_drain_wrs(struct c4iw_qp *qhp, struct ib_send_wr *wr,</span>
<span class="p_add">+				struct ib_send_wr **bad_wr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (wr) {</span>
<span class="p_add">+		ret = complete_sq_drain_wr(qhp, wr);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			*bad_wr = wr;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		wr = wr-&gt;next;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
 }
 
 static void complete_rq_drain_wr(struct c4iw_qp *qhp, struct ib_recv_wr *wr)
<span class="p_chunk">@@ -835,9 +890,10 @@</span> <span class="p_context"> static void complete_rq_drain_wr(struct c4iw_qp *qhp, struct ib_recv_wr *wr)</span>
 
 	cqe.u.drain_cookie = wr-&gt;wr_id;
 	cqe.header = cpu_to_be32(CQE_STATUS_V(T4_ERR_SWFLUSH) |
<span class="p_del">-				 CQE_OPCODE_V(C4IW_DRAIN_OPCODE) |</span>
<span class="p_add">+				 CQE_OPCODE_V(FW_RI_SEND) |</span>
 				 CQE_TYPE_V(0) |
 				 CQE_SWCQE_V(1) |
<span class="p_add">+				 CQE_DRAIN_V(1) |</span>
 				 CQE_QPID_V(qhp-&gt;wq.sq.qid));
 
 	spin_lock_irqsave(&amp;rchp-&gt;lock, flag);
<span class="p_chunk">@@ -846,10 +902,20 @@</span> <span class="p_context"> static void complete_rq_drain_wr(struct c4iw_qp *qhp, struct ib_recv_wr *wr)</span>
 	t4_swcq_produce(cq);
 	spin_unlock_irqrestore(&amp;rchp-&gt;lock, flag);
 
<span class="p_del">-	spin_lock_irqsave(&amp;rchp-&gt;comp_handler_lock, flag);</span>
<span class="p_del">-	(*rchp-&gt;ibcq.comp_handler)(&amp;rchp-&gt;ibcq,</span>
<span class="p_del">-				   rchp-&gt;ibcq.cq_context);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;rchp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	if (t4_clear_cq_armed(&amp;rchp-&gt;cq)) {</span>
<span class="p_add">+		spin_lock_irqsave(&amp;rchp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+		(*rchp-&gt;ibcq.comp_handler)(&amp;rchp-&gt;ibcq,</span>
<span class="p_add">+					   rchp-&gt;ibcq.cq_context);</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;rchp-&gt;comp_handler_lock, flag);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void complete_rq_drain_wrs(struct c4iw_qp *qhp, struct ib_recv_wr *wr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (wr) {</span>
<span class="p_add">+		complete_rq_drain_wr(qhp, wr);</span>
<span class="p_add">+		wr = wr-&gt;next;</span>
<span class="p_add">+	}</span>
 }
 
 int c4iw_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
<span class="p_chunk">@@ -875,7 +941,7 @@</span> <span class="p_context"> int c4iw_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,</span>
 	 */
 	if (qhp-&gt;wq.flushed) {
 		spin_unlock_irqrestore(&amp;qhp-&gt;lock, flag);
<span class="p_del">-		complete_sq_drain_wr(qhp, wr);</span>
<span class="p_add">+		err = complete_sq_drain_wrs(qhp, wr, bad_wr);</span>
 		return err;
 	}
 	num_wrs = t4_sq_avail(&amp;qhp-&gt;wq);
<span class="p_chunk">@@ -1024,7 +1090,7 @@</span> <span class="p_context"> int c4iw_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,</span>
 	 */
 	if (qhp-&gt;wq.flushed) {
 		spin_unlock_irqrestore(&amp;qhp-&gt;lock, flag);
<span class="p_del">-		complete_rq_drain_wr(qhp, wr);</span>
<span class="p_add">+		complete_rq_drain_wrs(qhp, wr);</span>
 		return err;
 	}
 	num_wrs = t4_rq_avail(&amp;qhp-&gt;wq);
<span class="p_chunk">@@ -1267,48 +1333,51 @@</span> <span class="p_context"> static void __flush_qp(struct c4iw_qp *qhp, struct c4iw_cq *rchp,</span>
 
 	pr_debug(&quot;%s qhp %p rchp %p schp %p\n&quot;, __func__, qhp, rchp, schp);
 
<span class="p_del">-	/* locking hierarchy: cq lock first, then qp lock. */</span>
<span class="p_add">+	/* locking hierarchy: cqs lock first, then qp lock. */</span>
 	spin_lock_irqsave(&amp;rchp-&gt;lock, flag);
<span class="p_add">+	if (schp != rchp)</span>
<span class="p_add">+		spin_lock(&amp;schp-&gt;lock);</span>
 	spin_lock(&amp;qhp-&gt;lock);
 
 	if (qhp-&gt;wq.flushed) {
 		spin_unlock(&amp;qhp-&gt;lock);
<span class="p_add">+		if (schp != rchp)</span>
<span class="p_add">+			spin_unlock(&amp;schp-&gt;lock);</span>
 		spin_unlock_irqrestore(&amp;rchp-&gt;lock, flag);
 		return;
 	}
 	qhp-&gt;wq.flushed = 1;
<span class="p_add">+	t4_set_wq_in_error(&amp;qhp-&gt;wq);</span>
 
 	c4iw_flush_hw_cq(rchp);
 	c4iw_count_rcqes(&amp;rchp-&gt;cq, &amp;qhp-&gt;wq, &amp;count);
 	rq_flushed = c4iw_flush_rq(&amp;qhp-&gt;wq, &amp;rchp-&gt;cq, count);
<span class="p_del">-	spin_unlock(&amp;qhp-&gt;lock);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;rchp-&gt;lock, flag);</span>
 
<span class="p_del">-	/* locking hierarchy: cq lock first, then qp lock. */</span>
<span class="p_del">-	spin_lock_irqsave(&amp;schp-&gt;lock, flag);</span>
<span class="p_del">-	spin_lock(&amp;qhp-&gt;lock);</span>
 	if (schp != rchp)
 		c4iw_flush_hw_cq(schp);
 	sq_flushed = c4iw_flush_sq(qhp);
<span class="p_add">+</span>
 	spin_unlock(&amp;qhp-&gt;lock);
<span class="p_del">-	spin_unlock_irqrestore(&amp;schp-&gt;lock, flag);</span>
<span class="p_add">+	if (schp != rchp)</span>
<span class="p_add">+		spin_unlock(&amp;schp-&gt;lock);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;rchp-&gt;lock, flag);</span>
 
 	if (schp == rchp) {
<span class="p_del">-		if (t4_clear_cq_armed(&amp;rchp-&gt;cq) &amp;&amp;</span>
<span class="p_del">-		    (rq_flushed || sq_flushed)) {</span>
<span class="p_add">+		if ((rq_flushed || sq_flushed) &amp;&amp;</span>
<span class="p_add">+		    t4_clear_cq_armed(&amp;rchp-&gt;cq)) {</span>
 			spin_lock_irqsave(&amp;rchp-&gt;comp_handler_lock, flag);
 			(*rchp-&gt;ibcq.comp_handler)(&amp;rchp-&gt;ibcq,
 						   rchp-&gt;ibcq.cq_context);
 			spin_unlock_irqrestore(&amp;rchp-&gt;comp_handler_lock, flag);
 		}
 	} else {
<span class="p_del">-		if (t4_clear_cq_armed(&amp;rchp-&gt;cq) &amp;&amp; rq_flushed) {</span>
<span class="p_add">+		if (rq_flushed &amp;&amp; t4_clear_cq_armed(&amp;rchp-&gt;cq)) {</span>
 			spin_lock_irqsave(&amp;rchp-&gt;comp_handler_lock, flag);
 			(*rchp-&gt;ibcq.comp_handler)(&amp;rchp-&gt;ibcq,
 						   rchp-&gt;ibcq.cq_context);
 			spin_unlock_irqrestore(&amp;rchp-&gt;comp_handler_lock, flag);
 		}
<span class="p_del">-		if (t4_clear_cq_armed(&amp;schp-&gt;cq) &amp;&amp; sq_flushed) {</span>
<span class="p_add">+		if (sq_flushed &amp;&amp; t4_clear_cq_armed(&amp;schp-&gt;cq)) {</span>
 			spin_lock_irqsave(&amp;schp-&gt;comp_handler_lock, flag);
 			(*schp-&gt;ibcq.comp_handler)(&amp;schp-&gt;ibcq,
 						   schp-&gt;ibcq.cq_context);
<span class="p_chunk">@@ -1325,8 +1394,8 @@</span> <span class="p_context"> static void flush_qp(struct c4iw_qp *qhp)</span>
 	rchp = to_c4iw_cq(qhp-&gt;ibqp.recv_cq);
 	schp = to_c4iw_cq(qhp-&gt;ibqp.send_cq);
 
<span class="p_del">-	t4_set_wq_in_error(&amp;qhp-&gt;wq);</span>
 	if (qhp-&gt;ibqp.uobject) {
<span class="p_add">+		t4_set_wq_in_error(&amp;qhp-&gt;wq);</span>
 		t4_set_cq_in_error(&amp;rchp-&gt;cq);
 		spin_lock_irqsave(&amp;rchp-&gt;comp_handler_lock, flag);
 		(*rchp-&gt;ibcq.comp_handler)(&amp;rchp-&gt;ibcq, rchp-&gt;ibcq.cq_context);
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/t4.h b/drivers/infiniband/hw/cxgb4/t4.h</span>
<span class="p_header">index bcb80ca67d3d..80b390e861dc 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/t4.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/t4.h</span>
<span class="p_chunk">@@ -197,6 +197,11 @@</span> <span class="p_context"> struct t4_cqe {</span>
 #define CQE_SWCQE_G(x)    ((((x) &gt;&gt; CQE_SWCQE_S)) &amp; CQE_SWCQE_M)
 #define CQE_SWCQE_V(x)	  ((x)&lt;&lt;CQE_SWCQE_S)
 
<span class="p_add">+#define CQE_DRAIN_S       10</span>
<span class="p_add">+#define CQE_DRAIN_M       0x1</span>
<span class="p_add">+#define CQE_DRAIN_G(x)    ((((x) &gt;&gt; CQE_DRAIN_S)) &amp; CQE_DRAIN_M)</span>
<span class="p_add">+#define CQE_DRAIN_V(x)	  ((x)&lt;&lt;CQE_DRAIN_S)</span>
<span class="p_add">+</span>
 #define CQE_STATUS_S      5
 #define CQE_STATUS_M      0x1F
 #define CQE_STATUS_G(x)   ((((x) &gt;&gt; CQE_STATUS_S)) &amp; CQE_STATUS_M)
<span class="p_chunk">@@ -213,6 +218,7 @@</span> <span class="p_context"> struct t4_cqe {</span>
 #define CQE_OPCODE_V(x)   ((x)&lt;&lt;CQE_OPCODE_S)
 
 #define SW_CQE(x)         (CQE_SWCQE_G(be32_to_cpu((x)-&gt;header)))
<span class="p_add">+#define DRAIN_CQE(x)      (CQE_DRAIN_G(be32_to_cpu((x)-&gt;header)))</span>
 #define CQE_QPID(x)       (CQE_QPID_G(be32_to_cpu((x)-&gt;header)))
 #define CQE_TYPE(x)       (CQE_TYPE_G(be32_to_cpu((x)-&gt;header)))
 #define SQ_TYPE(x)	  (CQE_TYPE((x)))
<span class="p_header">diff --git a/drivers/infiniband/ulp/srpt/ib_srpt.c b/drivers/infiniband/ulp/srpt/ib_srpt.c</span>
<span class="p_header">index 95178b4e3565..ee578fa713c2 100644</span>
<span class="p_header">--- a/drivers/infiniband/ulp/srpt/ib_srpt.c</span>
<span class="p_header">+++ b/drivers/infiniband/ulp/srpt/ib_srpt.c</span>
<span class="p_chunk">@@ -1000,8 +1000,7 @@</span> <span class="p_context"> static int srpt_init_ch_qp(struct srpt_rdma_ch *ch, struct ib_qp *qp)</span>
 		return -ENOMEM;
 
 	attr-&gt;qp_state = IB_QPS_INIT;
<span class="p_del">-	attr-&gt;qp_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_READ |</span>
<span class="p_del">-	    IB_ACCESS_REMOTE_WRITE;</span>
<span class="p_add">+	attr-&gt;qp_access_flags = IB_ACCESS_LOCAL_WRITE;</span>
 	attr-&gt;port_num = ch-&gt;sport-&gt;port;
 	attr-&gt;pkey_index = 0;
 
<span class="p_chunk">@@ -1992,7 +1991,7 @@</span> <span class="p_context"> static int srpt_cm_req_recv(struct ib_cm_id *cm_id,</span>
 		goto destroy_ib;
 	}
 
<span class="p_del">-	guid = (__be16 *)&amp;param-&gt;primary_path-&gt;sgid.global.interface_id;</span>
<span class="p_add">+	guid = (__be16 *)&amp;param-&gt;primary_path-&gt;dgid.global.interface_id;</span>
 	snprintf(ch-&gt;ini_guid, sizeof(ch-&gt;ini_guid), &quot;%04x:%04x:%04x:%04x&quot;,
 		 be16_to_cpu(guid[0]), be16_to_cpu(guid[1]),
 		 be16_to_cpu(guid[2]), be16_to_cpu(guid[3]));
<span class="p_header">diff --git a/drivers/md/dm-bufio.c b/drivers/md/dm-bufio.c</span>
<span class="p_header">index 8e3adcb46851..6d416fdc25cb 100644</span>
<span class="p_header">--- a/drivers/md/dm-bufio.c</span>
<span class="p_header">+++ b/drivers/md/dm-bufio.c</span>
<span class="p_chunk">@@ -1611,7 +1611,8 @@</span> <span class="p_context"> static unsigned long __scan(struct dm_bufio_client *c, unsigned long nr_to_scan,</span>
 	int l;
 	struct dm_buffer *b, *tmp;
 	unsigned long freed = 0;
<span class="p_del">-	unsigned long count = nr_to_scan;</span>
<span class="p_add">+	unsigned long count = c-&gt;n_buffers[LIST_CLEAN] +</span>
<span class="p_add">+			      c-&gt;n_buffers[LIST_DIRTY];</span>
 	unsigned long retain_target = get_retain_buffers(c);
 
 	for (l = 0; l &lt; LIST_SIZE; l++) {
<span class="p_chunk">@@ -1647,8 +1648,11 @@</span> <span class="p_context"> static unsigned long</span>
 dm_bufio_shrink_count(struct shrinker *shrink, struct shrink_control *sc)
 {
 	struct dm_bufio_client *c = container_of(shrink, struct dm_bufio_client, shrinker);
<span class="p_add">+	unsigned long count = ACCESS_ONCE(c-&gt;n_buffers[LIST_CLEAN]) +</span>
<span class="p_add">+			      ACCESS_ONCE(c-&gt;n_buffers[LIST_DIRTY]);</span>
<span class="p_add">+	unsigned long retain_target = get_retain_buffers(c);</span>
 
<span class="p_del">-	return ACCESS_ONCE(c-&gt;n_buffers[LIST_CLEAN]) + ACCESS_ONCE(c-&gt;n_buffers[LIST_DIRTY]);</span>
<span class="p_add">+	return (count &lt; retain_target) ? 0 : (count - retain_target);</span>
 }
 
 /*
<span class="p_header">diff --git a/drivers/mmc/host/renesas_sdhi_core.c b/drivers/mmc/host/renesas_sdhi_core.c</span>
<span class="p_header">index fcf7235d5742..157e1d9e7725 100644</span>
<span class="p_header">--- a/drivers/mmc/host/renesas_sdhi_core.c</span>
<span class="p_header">+++ b/drivers/mmc/host/renesas_sdhi_core.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kernel.h&gt;
 #include &lt;linux/clk.h&gt;
 #include &lt;linux/slab.h&gt;
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
 #include &lt;linux/of_device.h&gt;
 #include &lt;linux/platform_device.h&gt;
 #include &lt;linux/mmc/host.h&gt;
<span class="p_chunk">@@ -667,3 +668,5 @@</span> <span class="p_context"> int renesas_sdhi_remove(struct platform_device *pdev)</span>
 	return 0;
 }
 EXPORT_SYMBOL_GPL(renesas_sdhi_remove);
<span class="p_add">+</span>
<span class="p_add">+MODULE_LICENSE(&quot;GPL v2&quot;);</span>
<span class="p_header">diff --git a/drivers/mux/core.c b/drivers/mux/core.c</span>
<span class="p_header">index 2260063b0ea8..6e5cf9d9cd99 100644</span>
<span class="p_header">--- a/drivers/mux/core.c</span>
<span class="p_header">+++ b/drivers/mux/core.c</span>
<span class="p_chunk">@@ -413,6 +413,7 @@</span> <span class="p_context"> static int of_dev_node_match(struct device *dev, const void *data)</span>
 	return dev-&gt;of_node == data;
 }
 
<span class="p_add">+/* Note this function returns a reference to the mux_chip dev. */</span>
 static struct mux_chip *of_find_mux_chip_by_node(struct device_node *np)
 {
 	struct device *dev;
<span class="p_chunk">@@ -466,6 +467,7 @@</span> <span class="p_context"> struct mux_control *mux_control_get(struct device *dev, const char *mux_name)</span>
 	    (!args.args_count &amp;&amp; (mux_chip-&gt;controllers &gt; 1))) {
 		dev_err(dev, &quot;%pOF: wrong #mux-control-cells for %pOF\n&quot;,
 			np, args.np);
<span class="p_add">+		put_device(&amp;mux_chip-&gt;dev);</span>
 		return ERR_PTR(-EINVAL);
 	}
 
<span class="p_chunk">@@ -476,10 +478,10 @@</span> <span class="p_context"> struct mux_control *mux_control_get(struct device *dev, const char *mux_name)</span>
 	if (controller &gt;= mux_chip-&gt;controllers) {
 		dev_err(dev, &quot;%pOF: bad mux controller %u specified in %pOF\n&quot;,
 			np, controller, args.np);
<span class="p_add">+		put_device(&amp;mux_chip-&gt;dev);</span>
 		return ERR_PTR(-EINVAL);
 	}
 
<span class="p_del">-	get_device(&amp;mux_chip-&gt;dev);</span>
 	return &amp;mux_chip-&gt;mux[controller];
 }
 EXPORT_SYMBOL_GPL(mux_control_get);
<span class="p_header">diff --git a/drivers/net/can/usb/gs_usb.c b/drivers/net/can/usb/gs_usb.c</span>
<span class="p_header">index 68ac3e88a8ce..8bf80ad9dc44 100644</span>
<span class="p_header">--- a/drivers/net/can/usb/gs_usb.c</span>
<span class="p_header">+++ b/drivers/net/can/usb/gs_usb.c</span>
<span class="p_chunk">@@ -449,7 +449,7 @@</span> <span class="p_context"> static int gs_usb_set_bittiming(struct net_device *netdev)</span>
 		dev_err(netdev-&gt;dev.parent, &quot;Couldn&#39;t set bittimings (err=%d)&quot;,
 			rc);
 
<span class="p_del">-	return rc;</span>
<span class="p_add">+	return (rc &gt; 0) ? 0 : rc;</span>
 }
 
 static void gs_usb_xmit_callback(struct urb *urb)
<span class="p_header">diff --git a/drivers/net/can/vxcan.c b/drivers/net/can/vxcan.c</span>
<span class="p_header">index 8404e8852a0f..b4c4a2c76437 100644</span>
<span class="p_header">--- a/drivers/net/can/vxcan.c</span>
<span class="p_header">+++ b/drivers/net/can/vxcan.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> static int vxcan_newlink(struct net *net, struct net_device *dev,</span>
 		tbp = peer_tb;
 	}
 
<span class="p_del">-	if (tbp[IFLA_IFNAME]) {</span>
<span class="p_add">+	if (ifmp &amp;&amp; tbp[IFLA_IFNAME]) {</span>
 		nla_strlcpy(ifname, tbp[IFLA_IFNAME], IFNAMSIZ);
 		name_assign_type = NET_NAME_USER;
 	} else {
<span class="p_header">diff --git a/drivers/net/ethernet/freescale/fec_main.c b/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_header">index faf7cdc97ebf..311539c6625f 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_chunk">@@ -3458,6 +3458,10 @@</span> <span class="p_context"> fec_probe(struct platform_device *pdev)</span>
 			goto failed_regulator;
 		}
 	} else {
<span class="p_add">+		if (PTR_ERR(fep-&gt;reg_phy) == -EPROBE_DEFER) {</span>
<span class="p_add">+			ret = -EPROBE_DEFER;</span>
<span class="p_add">+			goto failed_regulator;</span>
<span class="p_add">+		}</span>
 		fep-&gt;reg_phy = NULL;
 	}
 
<span class="p_chunk">@@ -3539,8 +3543,9 @@</span> <span class="p_context"> fec_probe(struct platform_device *pdev)</span>
 failed_clk:
 	if (of_phy_is_fixed_link(np))
 		of_phy_deregister_fixed_link(np);
<span class="p_del">-failed_phy:</span>
 	of_node_put(phy_node);
<span class="p_add">+failed_phy:</span>
<span class="p_add">+	dev_id--;</span>
 failed_ioremap:
 	free_netdev(ndev);
 
<span class="p_header">diff --git a/drivers/net/ethernet/intel/e1000e/ich8lan.c b/drivers/net/ethernet/intel/e1000e/ich8lan.c</span>
<span class="p_header">index d6d4ed7acf03..31277d3bb7dc 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/intel/e1000e/ich8lan.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/intel/e1000e/ich8lan.c</span>
<span class="p_chunk">@@ -1367,6 +1367,9 @@</span> <span class="p_context"> static s32 e1000_disable_ulp_lpt_lp(struct e1000_hw *hw, bool force)</span>
  *  Checks to see of the link status of the hardware has changed.  If a
  *  change in link status has been detected, then we read the PHY registers
  *  to get the current speed/duplex if link exists.
<span class="p_add">+ *</span>
<span class="p_add">+ *  Returns a negative error code (-E1000_ERR_*) or 0 (link down) or 1 (link</span>
<span class="p_add">+ *  up).</span>
  **/
 static s32 e1000_check_for_copper_link_ich8lan(struct e1000_hw *hw)
 {
<span class="p_chunk">@@ -1382,7 +1385,7 @@</span> <span class="p_context"> static s32 e1000_check_for_copper_link_ich8lan(struct e1000_hw *hw)</span>
 	 * Change or Rx Sequence Error interrupt.
 	 */
 	if (!mac-&gt;get_link_status)
<span class="p_del">-		return 0;</span>
<span class="p_add">+		return 1;</span>
 
 	/* First we want to see if the MII Status Register reports
 	 * link.  If so, then we want to get the current speed/duplex
<span class="p_chunk">@@ -1613,10 +1616,12 @@</span> <span class="p_context"> static s32 e1000_check_for_copper_link_ich8lan(struct e1000_hw *hw)</span>
 	 * different link partner.
 	 */
 	ret_val = e1000e_config_fc_after_link_up(hw);
<span class="p_del">-	if (ret_val)</span>
<span class="p_add">+	if (ret_val) {</span>
 		e_dbg(&quot;Error configuring flow control\n&quot;);
<span class="p_add">+		return ret_val;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	return ret_val;</span>
<span class="p_add">+	return 1;</span>
 }
 
 static s32 e1000_get_variants_ich8lan(struct e1000_adapter *adapter)
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlxsw/spectrum.c b/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_header">index 3ead7439821c..99bd6e88ebc7 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_chunk">@@ -4235,7 +4235,10 @@</span> <span class="p_context"> static int mlxsw_sp_netdevice_port_upper_event(struct net_device *lower_dev,</span>
 			return -EINVAL;
 		if (!info-&gt;linking)
 			break;
<span class="p_del">-		if (netdev_has_any_upper_dev(upper_dev))</span>
<span class="p_add">+		if (netdev_has_any_upper_dev(upper_dev) &amp;&amp;</span>
<span class="p_add">+		    (!netif_is_bridge_master(upper_dev) ||</span>
<span class="p_add">+		     !mlxsw_sp_bridge_device_is_offloaded(mlxsw_sp,</span>
<span class="p_add">+							  upper_dev)))</span>
 			return -EINVAL;
 		if (netif_is_lag_master(upper_dev) &amp;&amp;
 		    !mlxsw_sp_master_lag_check(mlxsw_sp, upper_dev,
<span class="p_chunk">@@ -4347,6 +4350,7 @@</span> <span class="p_context"> static int mlxsw_sp_netdevice_port_vlan_event(struct net_device *vlan_dev,</span>
 					      u16 vid)
 {
 	struct mlxsw_sp_port *mlxsw_sp_port = netdev_priv(dev);
<span class="p_add">+	struct mlxsw_sp *mlxsw_sp = mlxsw_sp_port-&gt;mlxsw_sp;</span>
 	struct netdev_notifier_changeupper_info *info = ptr;
 	struct net_device *upper_dev;
 	int err = 0;
<span class="p_chunk">@@ -4358,7 +4362,10 @@</span> <span class="p_context"> static int mlxsw_sp_netdevice_port_vlan_event(struct net_device *vlan_dev,</span>
 			return -EINVAL;
 		if (!info-&gt;linking)
 			break;
<span class="p_del">-		if (netdev_has_any_upper_dev(upper_dev))</span>
<span class="p_add">+		if (netdev_has_any_upper_dev(upper_dev) &amp;&amp;</span>
<span class="p_add">+		    (!netif_is_bridge_master(upper_dev) ||</span>
<span class="p_add">+		     !mlxsw_sp_bridge_device_is_offloaded(mlxsw_sp,</span>
<span class="p_add">+							  upper_dev)))</span>
 			return -EINVAL;
 		break;
 	case NETDEV_CHANGEUPPER:
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlxsw/spectrum.h b/drivers/net/ethernet/mellanox/mlxsw/spectrum.h</span>
<span class="p_header">index 84ce83acdc19..88892d47acae 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum.h</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum.h</span>
<span class="p_chunk">@@ -326,6 +326,8 @@</span> <span class="p_context"> int mlxsw_sp_port_bridge_join(struct mlxsw_sp_port *mlxsw_sp_port,</span>
 void mlxsw_sp_port_bridge_leave(struct mlxsw_sp_port *mlxsw_sp_port,
 				struct net_device *brport_dev,
 				struct net_device *br_dev);
<span class="p_add">+bool mlxsw_sp_bridge_device_is_offloaded(const struct mlxsw_sp *mlxsw_sp,</span>
<span class="p_add">+					 const struct net_device *br_dev);</span>
 
 /* spectrum.c */
 int mlxsw_sp_port_ets_set(struct mlxsw_sp_port *mlxsw_sp_port,
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c b/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c</span>
<span class="p_header">index 5189022a1c8c..c23cc51bb5a5 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_router.c</span>
<span class="p_chunk">@@ -2536,7 +2536,7 @@</span> <span class="p_context"> static void __mlxsw_sp_nexthop_neigh_update(struct mlxsw_sp_nexthop *nh,</span>
 {
 	if (!removing)
 		nh-&gt;should_offload = 1;
<span class="p_del">-	else if (nh-&gt;offloaded)</span>
<span class="p_add">+	else</span>
 		nh-&gt;should_offload = 0;
 	nh-&gt;update = 1;
 }
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlxsw/spectrum_switchdev.c b/drivers/net/ethernet/mellanox/mlxsw/spectrum_switchdev.c</span>
<span class="p_header">index d39ffbfcc436..f5863e5bec81 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum_switchdev.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum_switchdev.c</span>
<span class="p_chunk">@@ -134,6 +134,12 @@</span> <span class="p_context"> mlxsw_sp_bridge_device_find(const struct mlxsw_sp_bridge *bridge,</span>
 	return NULL;
 }
 
<span class="p_add">+bool mlxsw_sp_bridge_device_is_offloaded(const struct mlxsw_sp *mlxsw_sp,</span>
<span class="p_add">+					 const struct net_device *br_dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !!mlxsw_sp_bridge_device_find(mlxsw_sp-&gt;bridge, br_dev);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct mlxsw_sp_bridge_device *
 mlxsw_sp_bridge_device_create(struct mlxsw_sp_bridge *bridge,
 			      struct net_device *br_dev)
<span class="p_header">diff --git a/drivers/net/ethernet/renesas/sh_eth.c b/drivers/net/ethernet/renesas/sh_eth.c</span>
<span class="p_header">index d2e88a30f57b..db31963c5d9d 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/renesas/sh_eth.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/renesas/sh_eth.c</span>
<span class="p_chunk">@@ -3212,18 +3212,37 @@</span> <span class="p_context"> static int sh_eth_drv_probe(struct platform_device *pdev)</span>
 	/* ioremap the TSU registers */
 	if (mdp-&gt;cd-&gt;tsu) {
 		struct resource *rtsu;
<span class="p_add">+</span>
 		rtsu = platform_get_resource(pdev, IORESOURCE_MEM, 1);
<span class="p_del">-		mdp-&gt;tsu_addr = devm_ioremap_resource(&amp;pdev-&gt;dev, rtsu);</span>
<span class="p_del">-		if (IS_ERR(mdp-&gt;tsu_addr)) {</span>
<span class="p_del">-			ret = PTR_ERR(mdp-&gt;tsu_addr);</span>
<span class="p_add">+		if (!rtsu) {</span>
<span class="p_add">+			dev_err(&amp;pdev-&gt;dev, &quot;no TSU resource\n&quot;);</span>
<span class="p_add">+			ret = -ENODEV;</span>
<span class="p_add">+			goto out_release;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* We can only request the  TSU region  for the first port</span>
<span class="p_add">+		 * of the two  sharing this TSU for the probe to succeed...</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (devno % 2 == 0 &amp;&amp;</span>
<span class="p_add">+		    !devm_request_mem_region(&amp;pdev-&gt;dev, rtsu-&gt;start,</span>
<span class="p_add">+					     resource_size(rtsu),</span>
<span class="p_add">+					     dev_name(&amp;pdev-&gt;dev))) {</span>
<span class="p_add">+			dev_err(&amp;pdev-&gt;dev, &quot;can&#39;t request TSU resource.\n&quot;);</span>
<span class="p_add">+			ret = -EBUSY;</span>
<span class="p_add">+			goto out_release;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		mdp-&gt;tsu_addr = devm_ioremap(&amp;pdev-&gt;dev, rtsu-&gt;start,</span>
<span class="p_add">+					     resource_size(rtsu));</span>
<span class="p_add">+		if (!mdp-&gt;tsu_addr) {</span>
<span class="p_add">+			dev_err(&amp;pdev-&gt;dev, &quot;TSU region ioremap() failed.\n&quot;);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
 			goto out_release;
 		}
 		mdp-&gt;port = devno % 2;
 		ndev-&gt;features = NETIF_F_HW_VLAN_CTAG_FILTER;
 	}
 
<span class="p_del">-	/* initialize first or needed device */</span>
<span class="p_del">-	if (!devno || pd-&gt;needs_init) {</span>
<span class="p_add">+	/* Need to init only the first port of the two sharing a TSU */</span>
<span class="p_add">+	if (devno % 2 == 0) {</span>
 		if (mdp-&gt;cd-&gt;chip_reset)
 			mdp-&gt;cd-&gt;chip_reset(ndev);
 
<span class="p_header">diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c</span>
<span class="p_header">index 28c4d6fa096c..0ad12c81a9e4 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c</span>
<span class="p_chunk">@@ -364,9 +364,15 @@</span> <span class="p_context"> static void stmmac_eee_ctrl_timer(unsigned long arg)</span>
 bool stmmac_eee_init(struct stmmac_priv *priv)
 {
 	struct net_device *ndev = priv-&gt;dev;
<span class="p_add">+	int interface = priv-&gt;plat-&gt;interface;</span>
 	unsigned long flags;
 	bool ret = false;
 
<span class="p_add">+	if ((interface != PHY_INTERFACE_MODE_MII) &amp;&amp;</span>
<span class="p_add">+	    (interface != PHY_INTERFACE_MODE_GMII) &amp;&amp;</span>
<span class="p_add">+	    !phy_interface_mode_is_rgmii(interface))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	/* Using PCS we cannot dial with the phy registers at this stage
 	 * so we do not support extra feature like EEE.
 	 */
<span class="p_header">diff --git a/drivers/net/phy/phylink.c b/drivers/net/phy/phylink.c</span>
<span class="p_header">index 4b377b978a0b..cb85307f125b 100644</span>
<span class="p_header">--- a/drivers/net/phy/phylink.c</span>
<span class="p_header">+++ b/drivers/net/phy/phylink.c</span>
<span class="p_chunk">@@ -1428,9 +1428,8 @@</span> <span class="p_context"> static void phylink_sfp_link_down(void *upstream)</span>
 	WARN_ON(!lockdep_rtnl_is_held());
 
 	set_bit(PHYLINK_DISABLE_LINK, &amp;pl-&gt;phylink_disable_state);
<span class="p_add">+	queue_work(system_power_efficient_wq, &amp;pl-&gt;resolve);</span>
 	flush_work(&amp;pl-&gt;resolve);
<span class="p_del">-</span>
<span class="p_del">-	netif_carrier_off(pl-&gt;netdev);</span>
 }
 
 static void phylink_sfp_link_up(void *upstream)
<span class="p_header">diff --git a/drivers/net/phy/sfp-bus.c b/drivers/net/phy/sfp-bus.c</span>
<span class="p_header">index 5cb5384697ea..7ae815bee52d 100644</span>
<span class="p_header">--- a/drivers/net/phy/sfp-bus.c</span>
<span class="p_header">+++ b/drivers/net/phy/sfp-bus.c</span>
<span class="p_chunk">@@ -359,7 +359,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(sfp_register_upstream);</span>
 void sfp_unregister_upstream(struct sfp_bus *bus)
 {
 	rtnl_lock();
<span class="p_del">-	sfp_unregister_bus(bus);</span>
<span class="p_add">+	if (bus-&gt;sfp)</span>
<span class="p_add">+		sfp_unregister_bus(bus);</span>
 	bus-&gt;upstream = NULL;
 	bus-&gt;netdev = NULL;
 	rtnl_unlock();
<span class="p_chunk">@@ -464,7 +465,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(sfp_register_socket);</span>
 void sfp_unregister_socket(struct sfp_bus *bus)
 {
 	rtnl_lock();
<span class="p_del">-	sfp_unregister_bus(bus);</span>
<span class="p_add">+	if (bus-&gt;netdev)</span>
<span class="p_add">+		sfp_unregister_bus(bus);</span>
 	bus-&gt;sfp_dev = NULL;
 	bus-&gt;sfp = NULL;
 	bus-&gt;socket_ops = NULL;
<span class="p_header">diff --git a/drivers/net/wireless/intel/iwlwifi/pcie/internal.h b/drivers/net/wireless/intel/iwlwifi/pcie/internal.h</span>
<span class="p_header">index 4fb7647995c3..9875ab5ce18c 100644</span>
<span class="p_header">--- a/drivers/net/wireless/intel/iwlwifi/pcie/internal.h</span>
<span class="p_header">+++ b/drivers/net/wireless/intel/iwlwifi/pcie/internal.h</span>
<span class="p_chunk">@@ -666,11 +666,15 @@</span> <span class="p_context"> static inline u8 iwl_pcie_get_cmd_index(struct iwl_txq *q, u32 index)</span>
 	return index &amp; (q-&gt;n_window - 1);
 }
 
<span class="p_del">-static inline void *iwl_pcie_get_tfd(struct iwl_trans_pcie *trans_pcie,</span>
<span class="p_add">+static inline void *iwl_pcie_get_tfd(struct iwl_trans *trans,</span>
 				     struct iwl_txq *txq, int idx)
 {
<span class="p_del">-	return txq-&gt;tfds + trans_pcie-&gt;tfd_size * iwl_pcie_get_cmd_index(txq,</span>
<span class="p_del">-									 idx);</span>
<span class="p_add">+	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (trans-&gt;cfg-&gt;use_tfh)</span>
<span class="p_add">+		idx = iwl_pcie_get_cmd_index(txq, idx);</span>
<span class="p_add">+</span>
<span class="p_add">+	return txq-&gt;tfds + trans_pcie-&gt;tfd_size * idx;</span>
 }
 
 static inline void iwl_enable_rfkill_int(struct iwl_trans *trans)
<span class="p_header">diff --git a/drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c b/drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c</span>
<span class="p_header">index d74613fcb756..6f45c8148b27 100644</span>
<span class="p_header">--- a/drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c</span>
<span class="p_header">+++ b/drivers/net/wireless/intel/iwlwifi/pcie/tx-gen2.c</span>
<span class="p_chunk">@@ -171,8 +171,6 @@</span> <span class="p_context"> static void iwl_pcie_gen2_tfd_unmap(struct iwl_trans *trans,</span>
 
 static void iwl_pcie_gen2_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq)
 {
<span class="p_del">-	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);</span>
<span class="p_del">-</span>
 	/* rd_ptr is bounded by TFD_QUEUE_SIZE_MAX and
 	 * idx is bounded by n_window
 	 */
<span class="p_chunk">@@ -181,7 +179,7 @@</span> <span class="p_context"> static void iwl_pcie_gen2_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq)</span>
 	lockdep_assert_held(&amp;txq-&gt;lock);
 
 	iwl_pcie_gen2_tfd_unmap(trans, &amp;txq-&gt;entries[idx].meta,
<span class="p_del">-				iwl_pcie_get_tfd(trans_pcie, txq, idx));</span>
<span class="p_add">+				iwl_pcie_get_tfd(trans, txq, idx));</span>
 
 	/* free SKB */
 	if (txq-&gt;entries) {
<span class="p_chunk">@@ -367,11 +365,9 @@</span> <span class="p_context"> struct iwl_tfh_tfd *iwl_pcie_gen2_build_tfd(struct iwl_trans *trans,</span>
 					    struct sk_buff *skb,
 					    struct iwl_cmd_meta *out_meta)
 {
<span class="p_del">-	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);</span>
 	struct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb-&gt;data;
 	int idx = iwl_pcie_get_cmd_index(txq, txq-&gt;write_ptr);
<span class="p_del">-	struct iwl_tfh_tfd *tfd =</span>
<span class="p_del">-		iwl_pcie_get_tfd(trans_pcie, txq, idx);</span>
<span class="p_add">+	struct iwl_tfh_tfd *tfd = iwl_pcie_get_tfd(trans, txq, idx);</span>
 	dma_addr_t tb_phys;
 	bool amsdu;
 	int i, len, tb1_len, tb2_len, hdr_len;
<span class="p_chunk">@@ -568,8 +564,7 @@</span> <span class="p_context"> static int iwl_pcie_gen2_enqueue_hcmd(struct iwl_trans *trans,</span>
 	u8 group_id = iwl_cmd_groupid(cmd-&gt;id);
 	const u8 *cmddata[IWL_MAX_CMD_TBS_PER_TFD];
 	u16 cmdlen[IWL_MAX_CMD_TBS_PER_TFD];
<span class="p_del">-	struct iwl_tfh_tfd *tfd =</span>
<span class="p_del">-		iwl_pcie_get_tfd(trans_pcie, txq, txq-&gt;write_ptr);</span>
<span class="p_add">+	struct iwl_tfh_tfd *tfd = iwl_pcie_get_tfd(trans, txq, txq-&gt;write_ptr);</span>
 
 	memset(tfd, 0, sizeof(*tfd));
 
<span class="p_header">diff --git a/drivers/net/wireless/intel/iwlwifi/pcie/tx.c b/drivers/net/wireless/intel/iwlwifi/pcie/tx.c</span>
<span class="p_header">index c645d10d3707..4704137a26e0 100644</span>
<span class="p_header">--- a/drivers/net/wireless/intel/iwlwifi/pcie/tx.c</span>
<span class="p_header">+++ b/drivers/net/wireless/intel/iwlwifi/pcie/tx.c</span>
<span class="p_chunk">@@ -373,7 +373,7 @@</span> <span class="p_context"> static void iwl_pcie_tfd_unmap(struct iwl_trans *trans,</span>
 {
 	struct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);
 	int i, num_tbs;
<span class="p_del">-	void *tfd = iwl_pcie_get_tfd(trans_pcie, txq, index);</span>
<span class="p_add">+	void *tfd = iwl_pcie_get_tfd(trans, txq, index);</span>
 
 	/* Sanity check on number of chunks */
 	num_tbs = iwl_pcie_tfd_get_num_tbs(trans, tfd);
<span class="p_chunk">@@ -1999,7 +1999,7 @@</span> <span class="p_context"> static int iwl_fill_data_tbs(struct iwl_trans *trans, struct sk_buff *skb,</span>
 	}
 
 	trace_iwlwifi_dev_tx(trans-&gt;dev, skb,
<span class="p_del">-			     iwl_pcie_get_tfd(trans_pcie, txq, txq-&gt;write_ptr),</span>
<span class="p_add">+			     iwl_pcie_get_tfd(trans, txq, txq-&gt;write_ptr),</span>
 			     trans_pcie-&gt;tfd_size,
 			     &amp;dev_cmd-&gt;hdr, IWL_FIRST_TB_SIZE + tb1_len,
 			     hdr_len);
<span class="p_chunk">@@ -2073,7 +2073,7 @@</span> <span class="p_context"> static int iwl_fill_data_tbs_amsdu(struct iwl_trans *trans, struct sk_buff *skb,</span>
 		IEEE80211_CCMP_HDR_LEN : 0;
 
 	trace_iwlwifi_dev_tx(trans-&gt;dev, skb,
<span class="p_del">-			     iwl_pcie_get_tfd(trans_pcie, txq, txq-&gt;write_ptr),</span>
<span class="p_add">+			     iwl_pcie_get_tfd(trans, txq, txq-&gt;write_ptr),</span>
 			     trans_pcie-&gt;tfd_size,
 			     &amp;dev_cmd-&gt;hdr, IWL_FIRST_TB_SIZE + tb1_len, 0);
 
<span class="p_chunk">@@ -2406,7 +2406,7 @@</span> <span class="p_context"> int iwl_trans_pcie_tx(struct iwl_trans *trans, struct sk_buff *skb,</span>
 	memcpy(&amp;txq-&gt;first_tb_bufs[txq-&gt;write_ptr], &amp;dev_cmd-&gt;hdr,
 	       IWL_FIRST_TB_SIZE);
 
<span class="p_del">-	tfd = iwl_pcie_get_tfd(trans_pcie, txq, txq-&gt;write_ptr);</span>
<span class="p_add">+	tfd = iwl_pcie_get_tfd(trans, txq, txq-&gt;write_ptr);</span>
 	/* Set up entry for this TFD in Tx byte-count array */
 	iwl_pcie_txq_update_byte_cnt_tbl(trans, txq, le16_to_cpu(tx_cmd-&gt;len),
 					 iwl_pcie_tfd_get_num_tbs(trans, tfd));
<span class="p_header">diff --git a/drivers/platform/x86/wmi.c b/drivers/platform/x86/wmi.c</span>
<span class="p_header">index 0765b1797d4c..7f8fa42a1084 100644</span>
<span class="p_header">--- a/drivers/platform/x86/wmi.c</span>
<span class="p_header">+++ b/drivers/platform/x86/wmi.c</span>
<span class="p_chunk">@@ -1268,5 +1268,5 @@</span> <span class="p_context"> static void __exit acpi_wmi_exit(void)</span>
 	bus_unregister(&amp;wmi_bus_type);
 }
 
<span class="p_del">-subsys_initcall(acpi_wmi_init);</span>
<span class="p_add">+subsys_initcall_sync(acpi_wmi_init);</span>
 module_exit(acpi_wmi_exit);
<span class="p_header">diff --git a/drivers/staging/android/ashmem.c b/drivers/staging/android/ashmem.c</span>
<span class="p_header">index 0f695df14c9d..372ce9913e6d 100644</span>
<span class="p_header">--- a/drivers/staging/android/ashmem.c</span>
<span class="p_header">+++ b/drivers/staging/android/ashmem.c</span>
<span class="p_chunk">@@ -765,10 +765,12 @@</span> <span class="p_context"> static long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)</span>
 		break;
 	case ASHMEM_SET_SIZE:
 		ret = -EINVAL;
<span class="p_add">+		mutex_lock(&amp;ashmem_mutex);</span>
 		if (!asma-&gt;file) {
 			ret = 0;
 			asma-&gt;size = (size_t)arg;
 		}
<span class="p_add">+		mutex_unlock(&amp;ashmem_mutex);</span>
 		break;
 	case ASHMEM_GET_SIZE:
 		ret = asma-&gt;size;
<span class="p_header">diff --git a/drivers/usb/gadget/udc/core.c b/drivers/usb/gadget/udc/core.c</span>
<span class="p_header">index def1b05ffca0..284bd1a7b570 100644</span>
<span class="p_header">--- a/drivers/usb/gadget/udc/core.c</span>
<span class="p_header">+++ b/drivers/usb/gadget/udc/core.c</span>
<span class="p_chunk">@@ -1158,11 +1158,7 @@</span> <span class="p_context"> int usb_add_gadget_udc_release(struct device *parent, struct usb_gadget *gadget,</span>
 
 	udc = kzalloc(sizeof(*udc), GFP_KERNEL);
 	if (!udc)
<span class="p_del">-		goto err1;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = device_add(&amp;gadget-&gt;dev);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		goto err2;</span>
<span class="p_add">+		goto err_put_gadget;</span>
 
 	device_initialize(&amp;udc-&gt;dev);
 	udc-&gt;dev.release = usb_udc_release;
<span class="p_chunk">@@ -1171,7 +1167,11 @@</span> <span class="p_context"> int usb_add_gadget_udc_release(struct device *parent, struct usb_gadget *gadget,</span>
 	udc-&gt;dev.parent = parent;
 	ret = dev_set_name(&amp;udc-&gt;dev, &quot;%s&quot;, kobject_name(&amp;parent-&gt;kobj));
 	if (ret)
<span class="p_del">-		goto err3;</span>
<span class="p_add">+		goto err_put_udc;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = device_add(&amp;gadget-&gt;dev);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto err_put_udc;</span>
 
 	udc-&gt;gadget = gadget;
 	gadget-&gt;udc = udc;
<span class="p_chunk">@@ -1181,7 +1181,7 @@</span> <span class="p_context"> int usb_add_gadget_udc_release(struct device *parent, struct usb_gadget *gadget,</span>
 
 	ret = device_add(&amp;udc-&gt;dev);
 	if (ret)
<span class="p_del">-		goto err4;</span>
<span class="p_add">+		goto err_unlist_udc;</span>
 
 	usb_gadget_set_state(gadget, USB_STATE_NOTATTACHED);
 	udc-&gt;vbus = true;
<span class="p_chunk">@@ -1189,27 +1189,25 @@</span> <span class="p_context"> int usb_add_gadget_udc_release(struct device *parent, struct usb_gadget *gadget,</span>
 	/* pick up one of pending gadget drivers */
 	ret = check_pending_gadget_drivers(udc);
 	if (ret)
<span class="p_del">-		goto err5;</span>
<span class="p_add">+		goto err_del_udc;</span>
 
 	mutex_unlock(&amp;udc_lock);
 
 	return 0;
 
<span class="p_del">-err5:</span>
<span class="p_add">+ err_del_udc:</span>
 	device_del(&amp;udc-&gt;dev);
 
<span class="p_del">-err4:</span>
<span class="p_add">+ err_unlist_udc:</span>
 	list_del(&amp;udc-&gt;list);
 	mutex_unlock(&amp;udc_lock);
 
<span class="p_del">-err3:</span>
<span class="p_del">-	put_device(&amp;udc-&gt;dev);</span>
 	device_del(&amp;gadget-&gt;dev);
 
<span class="p_del">-err2:</span>
<span class="p_del">-	kfree(udc);</span>
<span class="p_add">+ err_put_udc:</span>
<span class="p_add">+	put_device(&amp;udc-&gt;dev);</span>
 
<span class="p_del">-err1:</span>
<span class="p_add">+ err_put_gadget:</span>
 	put_device(&amp;gadget-&gt;dev);
 	return ret;
 }
<span class="p_header">diff --git a/drivers/usb/misc/usb3503.c b/drivers/usb/misc/usb3503.c</span>
<span class="p_header">index 8e7737d7ac0a..03be5d574f23 100644</span>
<span class="p_header">--- a/drivers/usb/misc/usb3503.c</span>
<span class="p_header">+++ b/drivers/usb/misc/usb3503.c</span>
<span class="p_chunk">@@ -292,6 +292,8 @@</span> <span class="p_context"> static int usb3503_probe(struct usb3503 *hub)</span>
 	if (gpio_is_valid(hub-&gt;gpio_reset)) {
 		err = devm_gpio_request_one(dev, hub-&gt;gpio_reset,
 				GPIOF_OUT_INIT_LOW, &quot;usb3503 reset&quot;);
<span class="p_add">+		/* Datasheet defines a hardware reset to be at least 100us */</span>
<span class="p_add">+		usleep_range(100, 10000);</span>
 		if (err) {
 			dev_err(dev,
 				&quot;unable to request GPIO %d as reset pin (%d)\n&quot;,
<span class="p_header">diff --git a/drivers/usb/mon/mon_bin.c b/drivers/usb/mon/mon_bin.c</span>
<span class="p_header">index f6ae753ab99b..f932f40302df 100644</span>
<span class="p_header">--- a/drivers/usb/mon/mon_bin.c</span>
<span class="p_header">+++ b/drivers/usb/mon/mon_bin.c</span>
<span class="p_chunk">@@ -1004,7 +1004,9 @@</span> <span class="p_context"> static long mon_bin_ioctl(struct file *file, unsigned int cmd, unsigned long arg</span>
 		break;
 
 	case MON_IOCQ_RING_SIZE:
<span class="p_add">+		mutex_lock(&amp;rp-&gt;fetch_lock);</span>
 		ret = rp-&gt;b_size;
<span class="p_add">+		mutex_unlock(&amp;rp-&gt;fetch_lock);</span>
 		break;
 
 	case MON_IOCT_RING_SIZE:
<span class="p_chunk">@@ -1231,12 +1233,16 @@</span> <span class="p_context"> static int mon_bin_vma_fault(struct vm_fault *vmf)</span>
 	unsigned long offset, chunk_idx;
 	struct page *pageptr;
 
<span class="p_add">+	mutex_lock(&amp;rp-&gt;fetch_lock);</span>
 	offset = vmf-&gt;pgoff &lt;&lt; PAGE_SHIFT;
<span class="p_del">-	if (offset &gt;= rp-&gt;b_size)</span>
<span class="p_add">+	if (offset &gt;= rp-&gt;b_size) {</span>
<span class="p_add">+		mutex_unlock(&amp;rp-&gt;fetch_lock);</span>
 		return VM_FAULT_SIGBUS;
<span class="p_add">+	}</span>
 	chunk_idx = offset / CHUNK_SIZE;
 	pageptr = rp-&gt;b_vec[chunk_idx].pg;
 	get_page(pageptr);
<span class="p_add">+	mutex_unlock(&amp;rp-&gt;fetch_lock);</span>
 	vmf-&gt;page = pageptr;
 	return 0;
 }
<span class="p_header">diff --git a/drivers/usb/serial/cp210x.c b/drivers/usb/serial/cp210x.c</span>
<span class="p_header">index 412f812522ee..aed182d24d23 100644</span>
<span class="p_header">--- a/drivers/usb/serial/cp210x.c</span>
<span class="p_header">+++ b/drivers/usb/serial/cp210x.c</span>
<span class="p_chunk">@@ -127,6 +127,7 @@</span> <span class="p_context"> static const struct usb_device_id id_table[] = {</span>
 	{ USB_DEVICE(0x10C4, 0x8470) }, /* Juniper Networks BX Series System Console */
 	{ USB_DEVICE(0x10C4, 0x8477) }, /* Balluff RFID */
 	{ USB_DEVICE(0x10C4, 0x84B6) }, /* Starizona Hyperion */
<span class="p_add">+	{ USB_DEVICE(0x10C4, 0x85A7) }, /* LifeScan OneTouch Verio IQ */</span>
 	{ USB_DEVICE(0x10C4, 0x85EA) }, /* AC-Services IBUS-IF */
 	{ USB_DEVICE(0x10C4, 0x85EB) }, /* AC-Services CIS-IBUS */
 	{ USB_DEVICE(0x10C4, 0x85F8) }, /* Virtenio Preon32 */
<span class="p_chunk">@@ -177,6 +178,7 @@</span> <span class="p_context"> static const struct usb_device_id id_table[] = {</span>
 	{ USB_DEVICE(0x1843, 0x0200) }, /* Vaisala USB Instrument Cable */
 	{ USB_DEVICE(0x18EF, 0xE00F) }, /* ELV USB-I2C-Interface */
 	{ USB_DEVICE(0x18EF, 0xE025) }, /* ELV Marble Sound Board 1 */
<span class="p_add">+	{ USB_DEVICE(0x18EF, 0xE030) }, /* ELV ALC 8xxx Battery Charger */</span>
 	{ USB_DEVICE(0x18EF, 0xE032) }, /* ELV TFD500 Data Logger */
 	{ USB_DEVICE(0x1901, 0x0190) }, /* GE B850 CP2105 Recorder interface */
 	{ USB_DEVICE(0x1901, 0x0193) }, /* GE B650 CP2104 PMC interface */
<span class="p_header">diff --git a/drivers/usb/storage/unusual_uas.h b/drivers/usb/storage/unusual_uas.h</span>
<span class="p_header">index 9f356f7cf7d5..719ec68ae309 100644</span>
<span class="p_header">--- a/drivers/usb/storage/unusual_uas.h</span>
<span class="p_header">+++ b/drivers/usb/storage/unusual_uas.h</span>
<span class="p_chunk">@@ -156,6 +156,13 @@</span> <span class="p_context"> UNUSUAL_DEV(0x2109, 0x0711, 0x0000, 0x9999,</span>
 		USB_SC_DEVICE, USB_PR_DEVICE, NULL,
 		US_FL_NO_ATA_1X),
 
<span class="p_add">+/* Reported-by: Icenowy Zheng &lt;icenowy@aosc.io&gt; */</span>
<span class="p_add">+UNUSUAL_DEV(0x2537, 0x1068, 0x0000, 0x9999,</span>
<span class="p_add">+		&quot;Norelsys&quot;,</span>
<span class="p_add">+		&quot;NS1068X&quot;,</span>
<span class="p_add">+		USB_SC_DEVICE, USB_PR_DEVICE, NULL,</span>
<span class="p_add">+		US_FL_IGNORE_UAS),</span>
<span class="p_add">+</span>
 /* Reported-by: Takeo Nakayama &lt;javhera@gmx.com&gt; */
 UNUSUAL_DEV(0x357d, 0x7788, 0x0000, 0x9999,
 		&quot;JMicron&quot;,
<span class="p_header">diff --git a/drivers/usb/usbip/usbip_common.c b/drivers/usb/usbip/usbip_common.c</span>
<span class="p_header">index 17b599b923f3..7f0d22131121 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/usbip_common.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/usbip_common.c</span>
<span class="p_chunk">@@ -105,7 +105,7 @@</span> <span class="p_context"> static void usbip_dump_usb_device(struct usb_device *udev)</span>
 	dev_dbg(dev, &quot;       devnum(%d) devpath(%s) usb speed(%s)&quot;,
 		udev-&gt;devnum, udev-&gt;devpath, usb_speed_string(udev-&gt;speed));
 
<span class="p_del">-	pr_debug(&quot;tt %p, ttport %d\n&quot;, udev-&gt;tt, udev-&gt;ttport);</span>
<span class="p_add">+	pr_debug(&quot;tt hub ttport %d\n&quot;, udev-&gt;ttport);</span>
 
 	dev_dbg(dev, &quot;                    &quot;);
 	for (i = 0; i &lt; 16; i++)
<span class="p_chunk">@@ -138,12 +138,8 @@</span> <span class="p_context"> static void usbip_dump_usb_device(struct usb_device *udev)</span>
 	}
 	pr_debug(&quot;\n&quot;);
 
<span class="p_del">-	dev_dbg(dev, &quot;parent %p, bus %p\n&quot;, udev-&gt;parent, udev-&gt;bus);</span>
<span class="p_del">-</span>
<span class="p_del">-	dev_dbg(dev,</span>
<span class="p_del">-		&quot;descriptor %p, config %p, actconfig %p, rawdescriptors %p\n&quot;,</span>
<span class="p_del">-		&amp;udev-&gt;descriptor, udev-&gt;config,</span>
<span class="p_del">-		udev-&gt;actconfig, udev-&gt;rawdescriptors);</span>
<span class="p_add">+	dev_dbg(dev, &quot;parent %s, bus %s\n&quot;, dev_name(&amp;udev-&gt;parent-&gt;dev),</span>
<span class="p_add">+		udev-&gt;bus-&gt;bus_name);</span>
 
 	dev_dbg(dev, &quot;have_langid %d, string_langid %d\n&quot;,
 		udev-&gt;have_langid, udev-&gt;string_langid);
<span class="p_chunk">@@ -251,9 +247,6 @@</span> <span class="p_context"> void usbip_dump_urb(struct urb *urb)</span>
 
 	dev = &amp;urb-&gt;dev-&gt;dev;
 
<span class="p_del">-	dev_dbg(dev, &quot;   urb                   :%p\n&quot;, urb);</span>
<span class="p_del">-	dev_dbg(dev, &quot;   dev                   :%p\n&quot;, urb-&gt;dev);</span>
<span class="p_del">-</span>
 	usbip_dump_usb_device(urb-&gt;dev);
 
 	dev_dbg(dev, &quot;   pipe                  :%08x &quot;, urb-&gt;pipe);
<span class="p_chunk">@@ -262,11 +255,9 @@</span> <span class="p_context"> void usbip_dump_urb(struct urb *urb)</span>
 
 	dev_dbg(dev, &quot;   status                :%d\n&quot;, urb-&gt;status);
 	dev_dbg(dev, &quot;   transfer_flags        :%08X\n&quot;, urb-&gt;transfer_flags);
<span class="p_del">-	dev_dbg(dev, &quot;   transfer_buffer       :%p\n&quot;, urb-&gt;transfer_buffer);</span>
 	dev_dbg(dev, &quot;   transfer_buffer_length:%d\n&quot;,
 						urb-&gt;transfer_buffer_length);
 	dev_dbg(dev, &quot;   actual_length         :%d\n&quot;, urb-&gt;actual_length);
<span class="p_del">-	dev_dbg(dev, &quot;   setup_packet          :%p\n&quot;, urb-&gt;setup_packet);</span>
 
 	if (urb-&gt;setup_packet &amp;&amp; usb_pipetype(urb-&gt;pipe) == PIPE_CONTROL)
 		usbip_dump_usb_ctrlrequest(
<span class="p_chunk">@@ -276,8 +267,6 @@</span> <span class="p_context"> void usbip_dump_urb(struct urb *urb)</span>
 	dev_dbg(dev, &quot;   number_of_packets     :%d\n&quot;, urb-&gt;number_of_packets);
 	dev_dbg(dev, &quot;   interval              :%d\n&quot;, urb-&gt;interval);
 	dev_dbg(dev, &quot;   error_count           :%d\n&quot;, urb-&gt;error_count);
<span class="p_del">-	dev_dbg(dev, &quot;   context               :%p\n&quot;, urb-&gt;context);</span>
<span class="p_del">-	dev_dbg(dev, &quot;   complete              :%p\n&quot;, urb-&gt;complete);</span>
 }
 EXPORT_SYMBOL_GPL(usbip_dump_urb);
 
<span class="p_header">diff --git a/drivers/usb/usbip/vudc_rx.c b/drivers/usb/usbip/vudc_rx.c</span>
<span class="p_header">index e429b59f6f8a..d020e72b3122 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/vudc_rx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/vudc_rx.c</span>
<span class="p_chunk">@@ -132,6 +132,25 @@</span> <span class="p_context"> static int v_recv_cmd_submit(struct vudc *udc,</span>
 	urb_p-&gt;new = 1;
 	urb_p-&gt;seqnum = pdu-&gt;base.seqnum;
 
<span class="p_add">+	if (urb_p-&gt;ep-&gt;type == USB_ENDPOINT_XFER_ISOC) {</span>
<span class="p_add">+		/* validate packet size and number of packets */</span>
<span class="p_add">+		unsigned int maxp, packets, bytes;</span>
<span class="p_add">+</span>
<span class="p_add">+		maxp = usb_endpoint_maxp(urb_p-&gt;ep-&gt;desc);</span>
<span class="p_add">+		maxp *= usb_endpoint_maxp_mult(urb_p-&gt;ep-&gt;desc);</span>
<span class="p_add">+		bytes = pdu-&gt;u.cmd_submit.transfer_buffer_length;</span>
<span class="p_add">+		packets = DIV_ROUND_UP(bytes, maxp);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pdu-&gt;u.cmd_submit.number_of_packets &lt; 0 ||</span>
<span class="p_add">+		    pdu-&gt;u.cmd_submit.number_of_packets &gt; packets) {</span>
<span class="p_add">+			dev_err(&amp;udc-&gt;gadget.dev,</span>
<span class="p_add">+				&quot;CMD_SUBMIT: isoc invalid num packets %d\n&quot;,</span>
<span class="p_add">+				pdu-&gt;u.cmd_submit.number_of_packets);</span>
<span class="p_add">+			ret = -EMSGSIZE;</span>
<span class="p_add">+			goto free_urbp;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	ret = alloc_urb_from_cmd(&amp;urb_p-&gt;urb, pdu, urb_p-&gt;ep-&gt;type);
 	if (ret) {
 		usbip_event_add(&amp;udc-&gt;ud, VUDC_EVENT_ERROR_MALLOC);
<span class="p_header">diff --git a/drivers/usb/usbip/vudc_tx.c b/drivers/usb/usbip/vudc_tx.c</span>
<span class="p_header">index 234661782fa0..3ab4c86486a7 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/vudc_tx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/vudc_tx.c</span>
<span class="p_chunk">@@ -97,6 +97,13 @@</span> <span class="p_context"> static int v_send_ret_submit(struct vudc *udc, struct urbp *urb_p)</span>
 	memset(&amp;pdu_header, 0, sizeof(pdu_header));
 	memset(&amp;msg, 0, sizeof(msg));
 
<span class="p_add">+	if (urb-&gt;actual_length &gt; 0 &amp;&amp; !urb-&gt;transfer_buffer) {</span>
<span class="p_add">+		dev_err(&amp;udc-&gt;gadget.dev,</span>
<span class="p_add">+			&quot;urb: actual_length %d transfer_buffer null\n&quot;,</span>
<span class="p_add">+			urb-&gt;actual_length);</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (urb_p-&gt;type == USB_ENDPOINT_XFER_ISOC)
 		iovnum = 2 + urb-&gt;number_of_packets;
 	else
<span class="p_chunk">@@ -112,8 +119,8 @@</span> <span class="p_context"> static int v_send_ret_submit(struct vudc *udc, struct urbp *urb_p)</span>
 
 	/* 1. setup usbip_header */
 	setup_ret_submit_pdu(&amp;pdu_header, urb_p);
<span class="p_del">-	usbip_dbg_stub_tx(&quot;setup txdata seqnum: %d urb: %p\n&quot;,</span>
<span class="p_del">-			  pdu_header.base.seqnum, urb);</span>
<span class="p_add">+	usbip_dbg_stub_tx(&quot;setup txdata seqnum: %d\n&quot;,</span>
<span class="p_add">+			  pdu_header.base.seqnum);</span>
 	usbip_header_correct_endian(&amp;pdu_header, 1);
 
 	iov[iovnum].iov_base = &amp;pdu_header;
<span class="p_header">diff --git a/include/linux/bpf.h b/include/linux/bpf.h</span>
<span class="p_header">index f1af7d63d678..0bcf803f20de 100644</span>
<span class="p_header">--- a/include/linux/bpf.h</span>
<span class="p_header">+++ b/include/linux/bpf.h</span>
<span class="p_chunk">@@ -51,6 +51,7 @@</span> <span class="p_context"> struct bpf_map {</span>
 	u32 pages;
 	u32 id;
 	int numa_node;
<span class="p_add">+	bool unpriv_array;</span>
 	struct user_struct *user;
 	const struct bpf_map_ops *ops;
 	struct work_struct work;
<span class="p_chunk">@@ -195,6 +196,7 @@</span> <span class="p_context"> struct bpf_prog_aux {</span>
 struct bpf_array {
 	struct bpf_map map;
 	u32 elem_size;
<span class="p_add">+	u32 index_mask;</span>
 	/* &#39;ownership&#39; of prog_array is claimed by the first program that
 	 * is going to use this map or by the first program which FD is stored
 	 * in the map to make sure that all callers and callees have the same
<span class="p_header">diff --git a/include/linux/cpu.h b/include/linux/cpu.h</span>
<span class="p_header">index 938ea8ae0ba4..c816e6f2730c 100644</span>
<span class="p_header">--- a/include/linux/cpu.h</span>
<span class="p_header">+++ b/include/linux/cpu.h</span>
<span class="p_chunk">@@ -47,6 +47,13 @@</span> <span class="p_context"> extern void cpu_remove_dev_attr(struct device_attribute *attr);</span>
 extern int cpu_add_dev_attr_group(struct attribute_group *attrs);
 extern void cpu_remove_dev_attr_group(struct attribute_group *attrs);
 
<span class="p_add">+extern ssize_t cpu_show_meltdown(struct device *dev,</span>
<span class="p_add">+				 struct device_attribute *attr, char *buf);</span>
<span class="p_add">+extern ssize_t cpu_show_spectre_v1(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf);</span>
<span class="p_add">+extern ssize_t cpu_show_spectre_v2(struct device *dev,</span>
<span class="p_add">+				   struct device_attribute *attr, char *buf);</span>
<span class="p_add">+</span>
 extern __printf(4, 5)
 struct device *cpu_device_create(struct device *parent, void *drvdata,
 				 const struct attribute_group **groups,
<span class="p_header">diff --git a/include/linux/crash_core.h b/include/linux/crash_core.h</span>
<span class="p_header">index 06097ef30449..b511f6d24b42 100644</span>
<span class="p_header">--- a/include/linux/crash_core.h</span>
<span class="p_header">+++ b/include/linux/crash_core.h</span>
<span class="p_chunk">@@ -42,6 +42,8 @@</span> <span class="p_context"> phys_addr_t paddr_vmcoreinfo_note(void);</span>
 	vmcoreinfo_append_str(&quot;PAGESIZE=%ld\n&quot;, value)
 #define VMCOREINFO_SYMBOL(name) \
 	vmcoreinfo_append_str(&quot;SYMBOL(%s)=%lx\n&quot;, #name, (unsigned long)&amp;name)
<span class="p_add">+#define VMCOREINFO_SYMBOL_ARRAY(name) \</span>
<span class="p_add">+	vmcoreinfo_append_str(&quot;SYMBOL(%s)=%lx\n&quot;, #name, (unsigned long)name)</span>
 #define VMCOREINFO_SIZE(name) \
 	vmcoreinfo_append_str(&quot;SIZE(%s)=%lu\n&quot;, #name, \
 			      (unsigned long)sizeof(name))
<span class="p_header">diff --git a/include/linux/sh_eth.h b/include/linux/sh_eth.h</span>
<span class="p_header">index ff3642d267f7..94081e9a5010 100644</span>
<span class="p_header">--- a/include/linux/sh_eth.h</span>
<span class="p_header">+++ b/include/linux/sh_eth.h</span>
<span class="p_chunk">@@ -17,7 +17,6 @@</span> <span class="p_context"> struct sh_eth_plat_data {</span>
 	unsigned char mac_addr[ETH_ALEN];
 	unsigned no_ether_link:1;
 	unsigned ether_link_active_low:1;
<span class="p_del">-	unsigned needs_init:1;</span>
 };
 
 #endif
<span class="p_header">diff --git a/include/net/sctp/structs.h b/include/net/sctp/structs.h</span>
<span class="p_header">index 0477945de1a3..8e1e1dc490fd 100644</span>
<span class="p_header">--- a/include/net/sctp/structs.h</span>
<span class="p_header">+++ b/include/net/sctp/structs.h</span>
<span class="p_chunk">@@ -955,7 +955,7 @@</span> <span class="p_context"> void sctp_transport_burst_limited(struct sctp_transport *);</span>
 void sctp_transport_burst_reset(struct sctp_transport *);
 unsigned long sctp_transport_timeout(struct sctp_transport *);
 void sctp_transport_reset(struct sctp_transport *t);
<span class="p_del">-void sctp_transport_update_pmtu(struct sctp_transport *t, u32 pmtu);</span>
<span class="p_add">+bool sctp_transport_update_pmtu(struct sctp_transport *t, u32 pmtu);</span>
 void sctp_transport_immediate_rtx(struct sctp_transport *);
 void sctp_transport_dst_release(struct sctp_transport *t);
 void sctp_transport_dst_confirm(struct sctp_transport *t);
<span class="p_header">diff --git a/include/trace/events/kvm.h b/include/trace/events/kvm.h</span>
<span class="p_header">index e4b0b8e09932..2c735a3e6613 100644</span>
<span class="p_header">--- a/include/trace/events/kvm.h</span>
<span class="p_header">+++ b/include/trace/events/kvm.h</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> TRACE_EVENT(kvm_ack_irq,</span>
 	{ KVM_TRACE_MMIO_WRITE, &quot;write&quot; }
 
 TRACE_EVENT(kvm_mmio,
<span class="p_del">-	TP_PROTO(int type, int len, u64 gpa, u64 val),</span>
<span class="p_add">+	TP_PROTO(int type, int len, u64 gpa, void *val),</span>
 	TP_ARGS(type, len, gpa, val),
 
 	TP_STRUCT__entry(
<span class="p_chunk">@@ -225,7 +225,10 @@</span> <span class="p_context"> TRACE_EVENT(kvm_mmio,</span>
 		__entry-&gt;type		= type;
 		__entry-&gt;len		= len;
 		__entry-&gt;gpa		= gpa;
<span class="p_del">-		__entry-&gt;val		= val;</span>
<span class="p_add">+		__entry-&gt;val		= 0;</span>
<span class="p_add">+		if (val)</span>
<span class="p_add">+			memcpy(&amp;__entry-&gt;val, val,</span>
<span class="p_add">+			       min_t(u32, sizeof(__entry-&gt;val), len));</span>
 	),
 
 	TP_printk(&quot;mmio %s len %u gpa 0x%llx val 0x%llx&quot;,
<span class="p_header">diff --git a/kernel/bpf/arraymap.c b/kernel/bpf/arraymap.c</span>
<span class="p_header">index e2636737b69b..a4ae1ca44a57 100644</span>
<span class="p_header">--- a/kernel/bpf/arraymap.c</span>
<span class="p_header">+++ b/kernel/bpf/arraymap.c</span>
<span class="p_chunk">@@ -50,9 +50,10 @@</span> <span class="p_context"> static struct bpf_map *array_map_alloc(union bpf_attr *attr)</span>
 {
 	bool percpu = attr-&gt;map_type == BPF_MAP_TYPE_PERCPU_ARRAY;
 	int numa_node = bpf_map_attr_numa_node(attr);
<span class="p_add">+	u32 elem_size, index_mask, max_entries;</span>
<span class="p_add">+	bool unpriv = !capable(CAP_SYS_ADMIN);</span>
 	struct bpf_array *array;
<span class="p_del">-	u64 array_size;</span>
<span class="p_del">-	u32 elem_size;</span>
<span class="p_add">+	u64 array_size, mask64;</span>
 
 	/* check sanity of attributes */
 	if (attr-&gt;max_entries == 0 || attr-&gt;key_size != 4 ||
<span class="p_chunk">@@ -68,11 +69,32 @@</span> <span class="p_context"> static struct bpf_map *array_map_alloc(union bpf_attr *attr)</span>
 
 	elem_size = round_up(attr-&gt;value_size, 8);
 
<span class="p_add">+	max_entries = attr-&gt;max_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* On 32 bit archs roundup_pow_of_two() with max_entries that has</span>
<span class="p_add">+	 * upper most bit set in u32 space is undefined behavior due to</span>
<span class="p_add">+	 * resulting 1U &lt;&lt; 32, so do it manually here in u64 space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mask64 = fls_long(max_entries - 1);</span>
<span class="p_add">+	mask64 = 1ULL &lt;&lt; mask64;</span>
<span class="p_add">+	mask64 -= 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	index_mask = mask64;</span>
<span class="p_add">+	if (unpriv) {</span>
<span class="p_add">+		/* round up array size to nearest power of 2,</span>
<span class="p_add">+		 * since cpu will speculate within index_mask limits</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		max_entries = index_mask + 1;</span>
<span class="p_add">+		/* Check for overflows. */</span>
<span class="p_add">+		if (max_entries &lt; attr-&gt;max_entries)</span>
<span class="p_add">+			return ERR_PTR(-E2BIG);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	array_size = sizeof(*array);
 	if (percpu)
<span class="p_del">-		array_size += (u64) attr-&gt;max_entries * sizeof(void *);</span>
<span class="p_add">+		array_size += (u64) max_entries * sizeof(void *);</span>
 	else
<span class="p_del">-		array_size += (u64) attr-&gt;max_entries * elem_size;</span>
<span class="p_add">+		array_size += (u64) max_entries * elem_size;</span>
 
 	/* make sure there is no u32 overflow later in round_up() */
 	if (array_size &gt;= U32_MAX - PAGE_SIZE)
<span class="p_chunk">@@ -82,6 +104,8 @@</span> <span class="p_context"> static struct bpf_map *array_map_alloc(union bpf_attr *attr)</span>
 	array = bpf_map_area_alloc(array_size, numa_node);
 	if (!array)
 		return ERR_PTR(-ENOMEM);
<span class="p_add">+	array-&gt;index_mask = index_mask;</span>
<span class="p_add">+	array-&gt;map.unpriv_array = unpriv;</span>
 
 	/* copy mandatory map attributes */
 	array-&gt;map.map_type = attr-&gt;map_type;
<span class="p_chunk">@@ -117,12 +141,13 @@</span> <span class="p_context"> static void *array_map_lookup_elem(struct bpf_map *map, void *key)</span>
 	if (unlikely(index &gt;= array-&gt;map.max_entries))
 		return NULL;
 
<span class="p_del">-	return array-&gt;value + array-&gt;elem_size * index;</span>
<span class="p_add">+	return array-&gt;value + array-&gt;elem_size * (index &amp; array-&gt;index_mask);</span>
 }
 
 /* emit BPF instructions equivalent to C code of array_map_lookup_elem() */
 static u32 array_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)
 {
<span class="p_add">+	struct bpf_array *array = container_of(map, struct bpf_array, map);</span>
 	struct bpf_insn *insn = insn_buf;
 	u32 elem_size = round_up(map-&gt;value_size, 8);
 	const int ret = BPF_REG_0;
<span class="p_chunk">@@ -131,7 +156,12 @@</span> <span class="p_context"> static u32 array_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)</span>
 
 	*insn++ = BPF_ALU64_IMM(BPF_ADD, map_ptr, offsetof(struct bpf_array, value));
 	*insn++ = BPF_LDX_MEM(BPF_W, ret, index, 0);
<span class="p_del">-	*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 3);</span>
<span class="p_add">+	if (map-&gt;unpriv_array) {</span>
<span class="p_add">+		*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 4);</span>
<span class="p_add">+		*insn++ = BPF_ALU32_IMM(BPF_AND, ret, array-&gt;index_mask);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 3);</span>
<span class="p_add">+	}</span>
 
 	if (is_power_of_2(elem_size)) {
 		*insn++ = BPF_ALU64_IMM(BPF_LSH, ret, ilog2(elem_size));
<span class="p_chunk">@@ -153,7 +183,7 @@</span> <span class="p_context"> static void *percpu_array_map_lookup_elem(struct bpf_map *map, void *key)</span>
 	if (unlikely(index &gt;= array-&gt;map.max_entries))
 		return NULL;
 
<span class="p_del">-	return this_cpu_ptr(array-&gt;pptrs[index]);</span>
<span class="p_add">+	return this_cpu_ptr(array-&gt;pptrs[index &amp; array-&gt;index_mask]);</span>
 }
 
 int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value)
<span class="p_chunk">@@ -173,7 +203,7 @@</span> <span class="p_context"> int bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value)</span>
 	 */
 	size = round_up(map-&gt;value_size, 8);
 	rcu_read_lock();
<span class="p_del">-	pptr = array-&gt;pptrs[index];</span>
<span class="p_add">+	pptr = array-&gt;pptrs[index &amp; array-&gt;index_mask];</span>
 	for_each_possible_cpu(cpu) {
 		bpf_long_memcpy(value + off, per_cpu_ptr(pptr, cpu), size);
 		off += size;
<span class="p_chunk">@@ -221,10 +251,11 @@</span> <span class="p_context"> static int array_map_update_elem(struct bpf_map *map, void *key, void *value,</span>
 		return -EEXIST;
 
 	if (array-&gt;map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
<span class="p_del">-		memcpy(this_cpu_ptr(array-&gt;pptrs[index]),</span>
<span class="p_add">+		memcpy(this_cpu_ptr(array-&gt;pptrs[index &amp; array-&gt;index_mask]),</span>
 		       value, map-&gt;value_size);
 	else
<span class="p_del">-		memcpy(array-&gt;value + array-&gt;elem_size * index,</span>
<span class="p_add">+		memcpy(array-&gt;value +</span>
<span class="p_add">+		       array-&gt;elem_size * (index &amp; array-&gt;index_mask),</span>
 		       value, map-&gt;value_size);
 	return 0;
 }
<span class="p_chunk">@@ -258,7 +289,7 @@</span> <span class="p_context"> int bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,</span>
 	 */
 	size = round_up(map-&gt;value_size, 8);
 	rcu_read_lock();
<span class="p_del">-	pptr = array-&gt;pptrs[index];</span>
<span class="p_add">+	pptr = array-&gt;pptrs[index &amp; array-&gt;index_mask];</span>
 	for_each_possible_cpu(cpu) {
 		bpf_long_memcpy(per_cpu_ptr(pptr, cpu), value + off, size);
 		off += size;
<span class="p_chunk">@@ -609,6 +640,7 @@</span> <span class="p_context"> static void *array_of_map_lookup_elem(struct bpf_map *map, void *key)</span>
 static u32 array_of_map_gen_lookup(struct bpf_map *map,
 				   struct bpf_insn *insn_buf)
 {
<span class="p_add">+	struct bpf_array *array = container_of(map, struct bpf_array, map);</span>
 	u32 elem_size = round_up(map-&gt;value_size, 8);
 	struct bpf_insn *insn = insn_buf;
 	const int ret = BPF_REG_0;
<span class="p_chunk">@@ -617,7 +649,12 @@</span> <span class="p_context"> static u32 array_of_map_gen_lookup(struct bpf_map *map,</span>
 
 	*insn++ = BPF_ALU64_IMM(BPF_ADD, map_ptr, offsetof(struct bpf_array, value));
 	*insn++ = BPF_LDX_MEM(BPF_W, ret, index, 0);
<span class="p_del">-	*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 5);</span>
<span class="p_add">+	if (map-&gt;unpriv_array) {</span>
<span class="p_add">+		*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 6);</span>
<span class="p_add">+		*insn++ = BPF_ALU32_IMM(BPF_AND, ret, array-&gt;index_mask);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map-&gt;max_entries, 5);</span>
<span class="p_add">+	}</span>
 	if (is_power_of_2(elem_size))
 		*insn++ = BPF_ALU64_IMM(BPF_LSH, ret, ilog2(elem_size));
 	else
<span class="p_header">diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c</span>
<span class="p_header">index c5ff809e86d0..75a5c3312f46 100644</span>
<span class="p_header">--- a/kernel/bpf/verifier.c</span>
<span class="p_header">+++ b/kernel/bpf/verifier.c</span>
<span class="p_chunk">@@ -1701,6 +1701,13 @@</span> <span class="p_context"> static int check_call(struct bpf_verifier_env *env, int func_id, int insn_idx)</span>
 	err = check_func_arg(env, BPF_REG_2, fn-&gt;arg2_type, &amp;meta);
 	if (err)
 		return err;
<span class="p_add">+	if (func_id == BPF_FUNC_tail_call) {</span>
<span class="p_add">+		if (meta.map_ptr == NULL) {</span>
<span class="p_add">+			verbose(&quot;verifier bug\n&quot;);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		env-&gt;insn_aux_data[insn_idx].map_ptr = meta.map_ptr;</span>
<span class="p_add">+	}</span>
 	err = check_func_arg(env, BPF_REG_3, fn-&gt;arg3_type, &amp;meta);
 	if (err)
 		return err;
<span class="p_chunk">@@ -2486,6 +2493,11 @@</span> <span class="p_context"> static int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)</span>
 			return -EINVAL;
 		}
 
<span class="p_add">+		if (opcode == BPF_ARSH &amp;&amp; BPF_CLASS(insn-&gt;code) != BPF_ALU64) {</span>
<span class="p_add">+			verbose(&quot;BPF_ARSH not supported for 32 bit ALU\n&quot;);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		if ((opcode == BPF_LSH || opcode == BPF_RSH ||
 		     opcode == BPF_ARSH) &amp;&amp; BPF_SRC(insn-&gt;code) == BPF_K) {
 			int size = BPF_CLASS(insn-&gt;code) == BPF_ALU64 ? 64 : 32;
<span class="p_chunk">@@ -4315,6 +4327,35 @@</span> <span class="p_context"> static int fixup_bpf_calls(struct bpf_verifier_env *env)</span>
 			 */
 			insn-&gt;imm = 0;
 			insn-&gt;code = BPF_JMP | BPF_TAIL_CALL;
<span class="p_add">+</span>
<span class="p_add">+			/* instead of changing every JIT dealing with tail_call</span>
<span class="p_add">+			 * emit two extra insns:</span>
<span class="p_add">+			 * if (index &gt;= max_entries) goto out;</span>
<span class="p_add">+			 * index &amp;= array-&gt;index_mask;</span>
<span class="p_add">+			 * to avoid out-of-bounds cpu speculation</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			map_ptr = env-&gt;insn_aux_data[i + delta].map_ptr;</span>
<span class="p_add">+			if (map_ptr == BPF_MAP_PTR_POISON) {</span>
<span class="p_add">+				verbose(&quot;tail_call obusing map_ptr\n&quot;);</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			if (!map_ptr-&gt;unpriv_array)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			insn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,</span>
<span class="p_add">+						  map_ptr-&gt;max_entries, 2);</span>
<span class="p_add">+			insn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,</span>
<span class="p_add">+						    container_of(map_ptr,</span>
<span class="p_add">+								 struct bpf_array,</span>
<span class="p_add">+								 map)-&gt;index_mask);</span>
<span class="p_add">+			insn_buf[2] = *insn;</span>
<span class="p_add">+			cnt = 3;</span>
<span class="p_add">+			new_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);</span>
<span class="p_add">+			if (!new_prog)</span>
<span class="p_add">+				return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+			delta    += cnt - 1;</span>
<span class="p_add">+			env-&gt;prog = prog = new_prog;</span>
<span class="p_add">+			insn      = new_prog-&gt;insnsi + i + delta;</span>
 			continue;
 		}
 
<span class="p_header">diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c</span>
<span class="p_header">index 44857278eb8a..030e4286f14c 100644</span>
<span class="p_header">--- a/kernel/cgroup/cgroup.c</span>
<span class="p_header">+++ b/kernel/cgroup/cgroup.c</span>
<span class="p_chunk">@@ -4059,26 +4059,24 @@</span> <span class="p_context"> static void css_task_iter_advance_css_set(struct css_task_iter *it)</span>
 
 static void css_task_iter_advance(struct css_task_iter *it)
 {
<span class="p_del">-	struct list_head *l = it-&gt;task_pos;</span>
<span class="p_add">+	struct list_head *next;</span>
 
 	lockdep_assert_held(&amp;css_set_lock);
<span class="p_del">-	WARN_ON_ONCE(!l);</span>
<span class="p_del">-</span>
 repeat:
 	/*
 	 * Advance iterator to find next entry.  cset-&gt;tasks is consumed
 	 * first and then -&gt;mg_tasks.  After -&gt;mg_tasks, we move onto the
 	 * next cset.
 	 */
<span class="p_del">-	l = l-&gt;next;</span>
<span class="p_add">+	next = it-&gt;task_pos-&gt;next;</span>
 
<span class="p_del">-	if (l == it-&gt;tasks_head)</span>
<span class="p_del">-		l = it-&gt;mg_tasks_head-&gt;next;</span>
<span class="p_add">+	if (next == it-&gt;tasks_head)</span>
<span class="p_add">+		next = it-&gt;mg_tasks_head-&gt;next;</span>
 
<span class="p_del">-	if (l == it-&gt;mg_tasks_head)</span>
<span class="p_add">+	if (next == it-&gt;mg_tasks_head)</span>
 		css_task_iter_advance_css_set(it);
 	else
<span class="p_del">-		it-&gt;task_pos = l;</span>
<span class="p_add">+		it-&gt;task_pos = next;</span>
 
 	/* if PROCS, skip over tasks which aren&#39;t group leaders */
 	if ((it-&gt;flags &amp; CSS_TASK_ITER_PROCS) &amp;&amp; it-&gt;task_pos &amp;&amp;
<span class="p_header">diff --git a/kernel/crash_core.c b/kernel/crash_core.c</span>
<span class="p_header">index 6db80fc0810b..2d90996dbe77 100644</span>
<span class="p_header">--- a/kernel/crash_core.c</span>
<span class="p_header">+++ b/kernel/crash_core.c</span>
<span class="p_chunk">@@ -409,7 +409,7 @@</span> <span class="p_context"> static int __init crash_save_vmcoreinfo_init(void)</span>
 	VMCOREINFO_SYMBOL(contig_page_data);
 #endif
 #ifdef CONFIG_SPARSEMEM
<span class="p_del">-	VMCOREINFO_SYMBOL(mem_section);</span>
<span class="p_add">+	VMCOREINFO_SYMBOL_ARRAY(mem_section);</span>
 	VMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);
 	VMCOREINFO_STRUCT_SIZE(mem_section);
 	VMCOREINFO_OFFSET(mem_section, section_mem_map);
<span class="p_header">diff --git a/kernel/sched/membarrier.c b/kernel/sched/membarrier.c</span>
<span class="p_header">index dd7908743dab..9bcbacba82a8 100644</span>
<span class="p_header">--- a/kernel/sched/membarrier.c</span>
<span class="p_header">+++ b/kernel/sched/membarrier.c</span>
<span class="p_chunk">@@ -89,7 +89,9 @@</span> <span class="p_context"> static int membarrier_private_expedited(void)</span>
 		rcu_read_unlock();
 	}
 	if (!fallback) {
<span class="p_add">+		preempt_disable();</span>
 		smp_call_function_many(tmpmask, ipi_mb, NULL, 1);
<span class="p_add">+		preempt_enable();</span>
 		free_cpumask_var(tmpmask);
 	}
 	cpus_read_unlock();
<span class="p_header">diff --git a/net/8021q/vlan.c b/net/8021q/vlan.c</span>
<span class="p_header">index 4a72ee4e2ae9..cf2e70003a53 100644</span>
<span class="p_header">--- a/net/8021q/vlan.c</span>
<span class="p_header">+++ b/net/8021q/vlan.c</span>
<span class="p_chunk">@@ -111,12 +111,7 @@</span> <span class="p_context"> void unregister_vlan_dev(struct net_device *dev, struct list_head *head)</span>
 		vlan_gvrp_uninit_applicant(real_dev);
 	}
 
<span class="p_del">-	/* Take it out of our own structures, but be sure to interlock with</span>
<span class="p_del">-	 * HW accelerating devices or SW vlan input packet processing if</span>
<span class="p_del">-	 * VLAN is not 0 (leave it there for 802.1p).</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (vlan_id)</span>
<span class="p_del">-		vlan_vid_del(real_dev, vlan-&gt;vlan_proto, vlan_id);</span>
<span class="p_add">+	vlan_vid_del(real_dev, vlan-&gt;vlan_proto, vlan_id);</span>
 
 	/* Get rid of the vlan&#39;s reference to real_dev */
 	dev_put(real_dev);
<span class="p_header">diff --git a/net/bluetooth/l2cap_core.c b/net/bluetooth/l2cap_core.c</span>
<span class="p_header">index 43ba91c440bc..fc6615d59165 100644</span>
<span class="p_header">--- a/net/bluetooth/l2cap_core.c</span>
<span class="p_header">+++ b/net/bluetooth/l2cap_core.c</span>
<span class="p_chunk">@@ -3363,9 +3363,10 @@</span> <span class="p_context"> static int l2cap_parse_conf_req(struct l2cap_chan *chan, void *data, size_t data</span>
 			break;
 
 		case L2CAP_CONF_EFS:
<span class="p_del">-			remote_efs = 1;</span>
<span class="p_del">-			if (olen == sizeof(efs))</span>
<span class="p_add">+			if (olen == sizeof(efs)) {</span>
<span class="p_add">+				remote_efs = 1;</span>
 				memcpy(&amp;efs, (void *) val, olen);
<span class="p_add">+			}</span>
 			break;
 
 		case L2CAP_CONF_EWS:
<span class="p_chunk">@@ -3584,16 +3585,17 @@</span> <span class="p_context"> static int l2cap_parse_conf_rsp(struct l2cap_chan *chan, void *rsp, int len,</span>
 			break;
 
 		case L2CAP_CONF_EFS:
<span class="p_del">-			if (olen == sizeof(efs))</span>
<span class="p_add">+			if (olen == sizeof(efs)) {</span>
 				memcpy(&amp;efs, (void *)val, olen);
 
<span class="p_del">-			if (chan-&gt;local_stype != L2CAP_SERV_NOTRAFIC &amp;&amp;</span>
<span class="p_del">-			    efs.stype != L2CAP_SERV_NOTRAFIC &amp;&amp;</span>
<span class="p_del">-			    efs.stype != chan-&gt;local_stype)</span>
<span class="p_del">-				return -ECONNREFUSED;</span>
<span class="p_add">+				if (chan-&gt;local_stype != L2CAP_SERV_NOTRAFIC &amp;&amp;</span>
<span class="p_add">+				    efs.stype != L2CAP_SERV_NOTRAFIC &amp;&amp;</span>
<span class="p_add">+				    efs.stype != chan-&gt;local_stype)</span>
<span class="p_add">+					return -ECONNREFUSED;</span>
 
<span class="p_del">-			l2cap_add_conf_opt(&amp;ptr, L2CAP_CONF_EFS, sizeof(efs),</span>
<span class="p_del">-					   (unsigned long) &amp;efs, endptr - ptr);</span>
<span class="p_add">+				l2cap_add_conf_opt(&amp;ptr, L2CAP_CONF_EFS, sizeof(efs),</span>
<span class="p_add">+						   (unsigned long) &amp;efs, endptr - ptr);</span>
<span class="p_add">+			}</span>
 			break;
 
 		case L2CAP_CONF_FCS:
<span class="p_header">diff --git a/net/core/ethtool.c b/net/core/ethtool.c</span>
<span class="p_header">index 9a9a3d77e327..d374a904f1b1 100644</span>
<span class="p_header">--- a/net/core/ethtool.c</span>
<span class="p_header">+++ b/net/core/ethtool.c</span>
<span class="p_chunk">@@ -754,15 +754,6 @@</span> <span class="p_context"> static int ethtool_set_link_ksettings(struct net_device *dev,</span>
 	return dev-&gt;ethtool_ops-&gt;set_link_ksettings(dev, &amp;link_ksettings);
 }
 
<span class="p_del">-static void</span>
<span class="p_del">-warn_incomplete_ethtool_legacy_settings_conversion(const char *details)</span>
<span class="p_del">-{</span>
<span class="p_del">-	char name[sizeof(current-&gt;comm)];</span>
<span class="p_del">-</span>
<span class="p_del">-	pr_info_once(&quot;warning: `%s&#39; uses legacy ethtool link settings API, %s\n&quot;,</span>
<span class="p_del">-		     get_task_comm(name, current), details);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /* Query device for its ethtool_cmd settings.
  *
  * Backward compatibility note: for compatibility with legacy ethtool,
<span class="p_chunk">@@ -789,10 +780,8 @@</span> <span class="p_context"> static int ethtool_get_settings(struct net_device *dev, void __user *useraddr)</span>
 							   &amp;link_ksettings);
 		if (err &lt; 0)
 			return err;
<span class="p_del">-		if (!convert_link_ksettings_to_legacy_settings(&amp;cmd,</span>
<span class="p_del">-							       &amp;link_ksettings))</span>
<span class="p_del">-			warn_incomplete_ethtool_legacy_settings_conversion(</span>
<span class="p_del">-				&quot;link modes are only partially reported&quot;);</span>
<span class="p_add">+		convert_link_ksettings_to_legacy_settings(&amp;cmd,</span>
<span class="p_add">+							  &amp;link_ksettings);</span>
 
 		/* send a sensible cmd tag back to user */
 		cmd.cmd = ETHTOOL_GSET;
<span class="p_header">diff --git a/net/core/sock_diag.c b/net/core/sock_diag.c</span>
<span class="p_header">index 217f4e3b82f6..146b50e30659 100644</span>
<span class="p_header">--- a/net/core/sock_diag.c</span>
<span class="p_header">+++ b/net/core/sock_diag.c</span>
<span class="p_chunk">@@ -288,7 +288,7 @@</span> <span class="p_context"> static int sock_diag_bind(struct net *net, int group)</span>
 	case SKNLGRP_INET6_UDP_DESTROY:
 		if (!sock_diag_handlers[AF_INET6])
 			request_module(&quot;net-pf-%d-proto-%d-type-%d&quot;, PF_NETLINK,
<span class="p_del">-				       NETLINK_SOCK_DIAG, AF_INET);</span>
<span class="p_add">+				       NETLINK_SOCK_DIAG, AF_INET6);</span>
 		break;
 	}
 	return 0;
<span class="p_header">diff --git a/net/ipv6/exthdrs.c b/net/ipv6/exthdrs.c</span>
<span class="p_header">index 95516138e861..d6189c2a35e4 100644</span>
<span class="p_header">--- a/net/ipv6/exthdrs.c</span>
<span class="p_header">+++ b/net/ipv6/exthdrs.c</span>
<span class="p_chunk">@@ -884,6 +884,15 @@</span> <span class="p_context"> static void ipv6_push_rthdr4(struct sk_buff *skb, u8 *proto,</span>
 	sr_phdr-&gt;segments[0] = **addr_p;
 	*addr_p = &amp;sr_ihdr-&gt;segments[sr_ihdr-&gt;segments_left];
 
<span class="p_add">+	if (sr_ihdr-&gt;hdrlen &gt; hops * 2) {</span>
<span class="p_add">+		int tlvs_offset, tlvs_length;</span>
<span class="p_add">+</span>
<span class="p_add">+		tlvs_offset = (1 + hops * 2) &lt;&lt; 3;</span>
<span class="p_add">+		tlvs_length = (sr_ihdr-&gt;hdrlen - hops * 2) &lt;&lt; 3;</span>
<span class="p_add">+		memcpy((char *)sr_phdr + tlvs_offset,</span>
<span class="p_add">+		       (char *)sr_ihdr + tlvs_offset, tlvs_length);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_IPV6_SEG6_HMAC
 	if (sr_has_hmac(sr_phdr)) {
 		struct net *net = NULL;
<span class="p_header">diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c</span>
<span class="p_header">index f7dd51c42314..688ba5f7516b 100644</span>
<span class="p_header">--- a/net/ipv6/ip6_output.c</span>
<span class="p_header">+++ b/net/ipv6/ip6_output.c</span>
<span class="p_chunk">@@ -1735,9 +1735,10 @@</span> <span class="p_context"> struct sk_buff *ip6_make_skb(struct sock *sk,</span>
 	cork.base.opt = NULL;
 	v6_cork.opt = NULL;
 	err = ip6_setup_cork(sk, &amp;cork, &amp;v6_cork, ipc6, rt, fl6);
<span class="p_del">-	if (err)</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		ip6_cork_release(&amp;cork, &amp;v6_cork);</span>
 		return ERR_PTR(err);
<span class="p_del">-</span>
<span class="p_add">+	}</span>
 	if (ipc6-&gt;dontfrag &lt; 0)
 		ipc6-&gt;dontfrag = inet6_sk(sk)-&gt;dontfrag;
 
<span class="p_header">diff --git a/net/ipv6/ip6_tunnel.c b/net/ipv6/ip6_tunnel.c</span>
<span class="p_header">index ef958d50746b..3f46121ad139 100644</span>
<span class="p_header">--- a/net/ipv6/ip6_tunnel.c</span>
<span class="p_header">+++ b/net/ipv6/ip6_tunnel.c</span>
<span class="p_chunk">@@ -1081,10 +1081,11 @@</span> <span class="p_context"> int ip6_tnl_xmit(struct sk_buff *skb, struct net_device *dev, __u8 dsfield,</span>
 			memcpy(&amp;fl6-&gt;daddr, addr6, sizeof(fl6-&gt;daddr));
 			neigh_release(neigh);
 		}
<span class="p_del">-	} else if (!(t-&gt;parms.flags &amp;</span>
<span class="p_del">-		     (IP6_TNL_F_USE_ORIG_TCLASS | IP6_TNL_F_USE_ORIG_FWMARK))) {</span>
<span class="p_del">-		/* enable the cache only only if the routing decision does</span>
<span class="p_del">-		 * not depend on the current inner header value</span>
<span class="p_add">+	} else if (t-&gt;parms.proto != 0 &amp;&amp; !(t-&gt;parms.flags &amp;</span>
<span class="p_add">+					    (IP6_TNL_F_USE_ORIG_TCLASS |</span>
<span class="p_add">+					     IP6_TNL_F_USE_ORIG_FWMARK))) {</span>
<span class="p_add">+		/* enable the cache only if neither the outer protocol nor the</span>
<span class="p_add">+		 * routing decision depends on the current inner header value</span>
 		 */
 		use_cache = true;
 	}
<span class="p_header">diff --git a/net/rds/rdma.c b/net/rds/rdma.c</span>
<span class="p_header">index bc2f1e0977d6..634cfcb7bba6 100644</span>
<span class="p_header">--- a/net/rds/rdma.c</span>
<span class="p_header">+++ b/net/rds/rdma.c</span>
<span class="p_chunk">@@ -525,6 +525,9 @@</span> <span class="p_context"> int rds_rdma_extra_size(struct rds_rdma_args *args)</span>
 
 	local_vec = (struct rds_iovec __user *)(unsigned long) args-&gt;local_vec_addr;
 
<span class="p_add">+	if (args-&gt;nr_local == 0)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	/* figure out the number of pages in the vector */
 	for (i = 0; i &lt; args-&gt;nr_local; i++) {
 		if (copy_from_user(&amp;vec, &amp;local_vec[i],
<span class="p_chunk">@@ -874,6 +877,7 @@</span> <span class="p_context"> int rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,</span>
 err:
 	if (page)
 		put_page(page);
<span class="p_add">+	rm-&gt;atomic.op_active = 0;</span>
 	kfree(rm-&gt;atomic.op_notifier);
 
 	return ret;
<span class="p_header">diff --git a/net/sched/act_gact.c b/net/sched/act_gact.c</span>
<span class="p_header">index e29a48ef7fc3..a0ac42b3ed06 100644</span>
<span class="p_header">--- a/net/sched/act_gact.c</span>
<span class="p_header">+++ b/net/sched/act_gact.c</span>
<span class="p_chunk">@@ -159,7 +159,7 @@</span> <span class="p_context"> static void tcf_gact_stats_update(struct tc_action *a, u64 bytes, u32 packets,</span>
 	if (action == TC_ACT_SHOT)
 		this_cpu_ptr(gact-&gt;common.cpu_qstats)-&gt;drops += packets;
 
<span class="p_del">-	tm-&gt;lastuse = lastuse;</span>
<span class="p_add">+	tm-&gt;lastuse = max_t(u64, tm-&gt;lastuse, lastuse);</span>
 }
 
 static int tcf_gact_dump(struct sk_buff *skb, struct tc_action *a,
<span class="p_header">diff --git a/net/sched/act_mirred.c b/net/sched/act_mirred.c</span>
<span class="p_header">index 416627c66f08..6ce8de373f83 100644</span>
<span class="p_header">--- a/net/sched/act_mirred.c</span>
<span class="p_header">+++ b/net/sched/act_mirred.c</span>
<span class="p_chunk">@@ -238,7 +238,7 @@</span> <span class="p_context"> static void tcf_stats_update(struct tc_action *a, u64 bytes, u32 packets,</span>
 	struct tcf_t *tm = &amp;m-&gt;tcf_tm;
 
 	_bstats_cpu_update(this_cpu_ptr(a-&gt;cpu_bstats), bytes, packets);
<span class="p_del">-	tm-&gt;lastuse = lastuse;</span>
<span class="p_add">+	tm-&gt;lastuse = max_t(u64, tm-&gt;lastuse, lastuse);</span>
 }
 
 static int tcf_mirred_dump(struct sk_buff *skb, struct tc_action *a, int bind,
<span class="p_header">diff --git a/net/sctp/input.c b/net/sctp/input.c</span>
<span class="p_header">index 621b5ca3fd1c..141c9c466ec1 100644</span>
<span class="p_header">--- a/net/sctp/input.c</span>
<span class="p_header">+++ b/net/sctp/input.c</span>
<span class="p_chunk">@@ -399,20 +399,24 @@</span> <span class="p_context"> void sctp_icmp_frag_needed(struct sock *sk, struct sctp_association *asoc,</span>
 		return;
 	}
 
<span class="p_del">-	if (t-&gt;param_flags &amp; SPP_PMTUD_ENABLE) {</span>
<span class="p_del">-		/* Update transports view of the MTU */</span>
<span class="p_del">-		sctp_transport_update_pmtu(t, pmtu);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Update association pmtu. */</span>
<span class="p_del">-		sctp_assoc_sync_pmtu(asoc);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!(t-&gt;param_flags &amp; SPP_PMTUD_ENABLE))</span>
<span class="p_add">+		/* We can&#39;t allow retransmitting in such case, as the</span>
<span class="p_add">+		 * retransmission would be sized just as before, and thus we</span>
<span class="p_add">+		 * would get another icmp, and retransmit again.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return;</span>
 
<span class="p_del">-	/* Retransmit with the new pmtu setting.</span>
<span class="p_del">-	 * Normally, if PMTU discovery is disabled, an ICMP Fragmentation</span>
<span class="p_del">-	 * Needed will never be sent, but if a message was sent before</span>
<span class="p_del">-	 * PMTU discovery was disabled that was larger than the PMTU, it</span>
<span class="p_del">-	 * would not be fragmented, so it must be re-transmitted fragmented.</span>
<span class="p_add">+	/* Update transports view of the MTU. Return if no update was needed.</span>
<span class="p_add">+	 * If an update wasn&#39;t needed/possible, it also doesn&#39;t make sense to</span>
<span class="p_add">+	 * try to retransmit now.</span>
 	 */
<span class="p_add">+	if (!sctp_transport_update_pmtu(t, pmtu))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Update association pmtu. */</span>
<span class="p_add">+	sctp_assoc_sync_pmtu(asoc);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Retransmit with the new pmtu setting. */</span>
 	sctp_retransmit(&amp;asoc-&gt;outqueue, t, SCTP_RTXR_PMTUD);
 }
 
<span class="p_header">diff --git a/net/sctp/transport.c b/net/sctp/transport.c</span>
<span class="p_header">index 2d9bd3776bc8..7ef77fd7b52a 100644</span>
<span class="p_header">--- a/net/sctp/transport.c</span>
<span class="p_header">+++ b/net/sctp/transport.c</span>
<span class="p_chunk">@@ -251,28 +251,37 @@</span> <span class="p_context"> void sctp_transport_pmtu(struct sctp_transport *transport, struct sock *sk)</span>
 		transport-&gt;pathmtu = SCTP_DEFAULT_MAXSEGMENT;
 }
 
<span class="p_del">-void sctp_transport_update_pmtu(struct sctp_transport *t, u32 pmtu)</span>
<span class="p_add">+bool sctp_transport_update_pmtu(struct sctp_transport *t, u32 pmtu)</span>
 {
 	struct dst_entry *dst = sctp_transport_dst_check(t);
<span class="p_add">+	bool change = true;</span>
 
 	if (unlikely(pmtu &lt; SCTP_DEFAULT_MINSEGMENT)) {
<span class="p_del">-		pr_warn(&quot;%s: Reported pmtu %d too low, using default minimum of %d\n&quot;,</span>
<span class="p_del">-			__func__, pmtu, SCTP_DEFAULT_MINSEGMENT);</span>
<span class="p_del">-		/* Use default minimum segment size and disable</span>
<span class="p_del">-		 * pmtu discovery on this transport.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		t-&gt;pathmtu = SCTP_DEFAULT_MINSEGMENT;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		t-&gt;pathmtu = pmtu;</span>
<span class="p_add">+		pr_warn_ratelimited(&quot;%s: Reported pmtu %d too low, using default minimum of %d\n&quot;,</span>
<span class="p_add">+				    __func__, pmtu, SCTP_DEFAULT_MINSEGMENT);</span>
<span class="p_add">+		/* Use default minimum segment instead */</span>
<span class="p_add">+		pmtu = SCTP_DEFAULT_MINSEGMENT;</span>
 	}
<span class="p_add">+	pmtu = SCTP_TRUNC4(pmtu);</span>
 
 	if (dst) {
 		dst-&gt;ops-&gt;update_pmtu(dst, t-&gt;asoc-&gt;base.sk, NULL, pmtu);
 		dst = sctp_transport_dst_check(t);
 	}
 
<span class="p_del">-	if (!dst)</span>
<span class="p_add">+	if (!dst) {</span>
 		t-&gt;af_specific-&gt;get_dst(t, &amp;t-&gt;saddr, &amp;t-&gt;fl, t-&gt;asoc-&gt;base.sk);
<span class="p_add">+		dst = t-&gt;dst;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dst) {</span>
<span class="p_add">+		/* Re-fetch, as under layers may have a higher minimum size */</span>
<span class="p_add">+		pmtu = SCTP_TRUNC4(dst_mtu(dst));</span>
<span class="p_add">+		change = t-&gt;pathmtu != pmtu;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	t-&gt;pathmtu = pmtu;</span>
<span class="p_add">+</span>
<span class="p_add">+	return change;</span>
 }
 
 /* Caches the dst entry and source address for a transport&#39;s destination
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index 6614b9312b45..b5c2b5d0c6c0 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> config PAGE_TABLE_ISOLATION</span>
 	  ensuring that the majority of kernel addresses are not mapped
 	  into userspace.
 
<span class="p_del">-	  See Documentation/x86/pagetable-isolation.txt for more details.</span>
<span class="p_add">+	  See Documentation/x86/pti.txt for more details.</span>
 
 config SECURITY_INFINIBAND
 	bool &quot;Infiniband Security Hooks&quot;
<span class="p_header">diff --git a/security/apparmor/include/perms.h b/security/apparmor/include/perms.h</span>
<span class="p_header">index 2b27bb79aec4..d7b7e7115160 100644</span>
<span class="p_header">--- a/security/apparmor/include/perms.h</span>
<span class="p_header">+++ b/security/apparmor/include/perms.h</span>
<span class="p_chunk">@@ -133,6 +133,9 @@</span> <span class="p_context"> extern struct aa_perms allperms;</span>
 #define xcheck_labels_profiles(L1, L2, FN, args...)		\
 	xcheck_ns_labels((L1), (L2), xcheck_ns_profile_label, (FN), args)
 
<span class="p_add">+#define xcheck_labels(L1, L2, P, FN1, FN2)			\</span>
<span class="p_add">+	xcheck(fn_for_each((L1), (P), (FN1)), fn_for_each((L2), (P), (FN2)))</span>
<span class="p_add">+</span>
 
 void aa_perm_mask_to_str(char *str, const char *chrs, u32 mask);
 void aa_audit_perm_names(struct audit_buffer *ab, const char **names, u32 mask);
<span class="p_header">diff --git a/security/apparmor/ipc.c b/security/apparmor/ipc.c</span>
<span class="p_header">index 7ca0032e7ba9..b40678f3c1d5 100644</span>
<span class="p_header">--- a/security/apparmor/ipc.c</span>
<span class="p_header">+++ b/security/apparmor/ipc.c</span>
<span class="p_chunk">@@ -64,40 +64,48 @@</span> <span class="p_context"> static void audit_ptrace_cb(struct audit_buffer *ab, void *va)</span>
 			FLAGS_NONE, GFP_ATOMIC);
 }
 
<span class="p_add">+/* assumes check for PROFILE_MEDIATES is already done */</span>
 /* TODO: conditionals */
 static int profile_ptrace_perm(struct aa_profile *profile,
<span class="p_del">-			       struct aa_profile *peer, u32 request,</span>
<span class="p_del">-			       struct common_audit_data *sa)</span>
<span class="p_add">+			     struct aa_label *peer, u32 request,</span>
<span class="p_add">+			     struct common_audit_data *sa)</span>
 {
 	struct aa_perms perms = { };
 
<span class="p_del">-	/* need because of peer in cross check */</span>
<span class="p_del">-	if (profile_unconfined(profile) ||</span>
<span class="p_del">-	    !PROFILE_MEDIATES(profile, AA_CLASS_PTRACE))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	aad(sa)-&gt;peer = &amp;peer-&gt;label;</span>
<span class="p_del">-	aa_profile_match_label(profile, &amp;peer-&gt;label, AA_CLASS_PTRACE, request,</span>
<span class="p_add">+	aad(sa)-&gt;peer = peer;</span>
<span class="p_add">+	aa_profile_match_label(profile, peer, AA_CLASS_PTRACE, request,</span>
 			       &amp;perms);
 	aa_apply_modes_to_perms(profile, &amp;perms);
 	return aa_check_perms(profile, &amp;perms, request, sa, audit_ptrace_cb);
 }
 
<span class="p_del">-static int cross_ptrace_perm(struct aa_profile *tracer,</span>
<span class="p_del">-			     struct aa_profile *tracee, u32 request,</span>
<span class="p_del">-			     struct common_audit_data *sa)</span>
<span class="p_add">+static int profile_tracee_perm(struct aa_profile *tracee,</span>
<span class="p_add">+			       struct aa_label *tracer, u32 request,</span>
<span class="p_add">+			       struct common_audit_data *sa)</span>
 {
<span class="p_add">+	if (profile_unconfined(tracee) || unconfined(tracer) ||</span>
<span class="p_add">+	    !PROFILE_MEDIATES(tracee, AA_CLASS_PTRACE))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return profile_ptrace_perm(tracee, tracer, request, sa);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int profile_tracer_perm(struct aa_profile *tracer,</span>
<span class="p_add">+			       struct aa_label *tracee, u32 request,</span>
<span class="p_add">+			       struct common_audit_data *sa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (profile_unconfined(tracer))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	if (PROFILE_MEDIATES(tracer, AA_CLASS_PTRACE))
<span class="p_del">-		return xcheck(profile_ptrace_perm(tracer, tracee, request, sa),</span>
<span class="p_del">-			      profile_ptrace_perm(tracee, tracer,</span>
<span class="p_del">-						  request &lt;&lt; PTRACE_PERM_SHIFT,</span>
<span class="p_del">-						  sa));</span>
<span class="p_del">-	/* policy uses the old style capability check for ptrace */</span>
<span class="p_del">-	if (profile_unconfined(tracer) || tracer == tracee)</span>
<span class="p_add">+		return profile_ptrace_perm(tracer, tracee, request, sa);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* profile uses the old style capability check for ptrace */</span>
<span class="p_add">+	if (&amp;tracer-&gt;label == tracee)</span>
 		return 0;
 
 	aad(sa)-&gt;label = &amp;tracer-&gt;label;
<span class="p_del">-	aad(sa)-&gt;peer = &amp;tracee-&gt;label;</span>
<span class="p_add">+	aad(sa)-&gt;peer = tracee;</span>
 	aad(sa)-&gt;request = 0;
 	aad(sa)-&gt;error = aa_capable(&amp;tracer-&gt;label, CAP_SYS_PTRACE, 1);
 
<span class="p_chunk">@@ -115,10 +123,13 @@</span> <span class="p_context"> static int cross_ptrace_perm(struct aa_profile *tracer,</span>
 int aa_may_ptrace(struct aa_label *tracer, struct aa_label *tracee,
 		  u32 request)
 {
<span class="p_add">+	struct aa_profile *profile;</span>
<span class="p_add">+	u32 xrequest = request &lt;&lt; PTRACE_PERM_SHIFT;</span>
 	DEFINE_AUDIT_DATA(sa, LSM_AUDIT_DATA_NONE, OP_PTRACE);
 
<span class="p_del">-	return xcheck_labels_profiles(tracer, tracee, cross_ptrace_perm,</span>
<span class="p_del">-				      request, &amp;sa);</span>
<span class="p_add">+	return xcheck_labels(tracer, tracee, profile,</span>
<span class="p_add">+			profile_tracer_perm(profile, tracee, request, &amp;sa),</span>
<span class="p_add">+			profile_tracee_perm(profile, tracer, xrequest, &amp;sa));</span>
 }
 
 
<span class="p_header">diff --git a/sound/core/oss/pcm_oss.c b/sound/core/oss/pcm_oss.c</span>
<span class="p_header">index e49f448ee04f..c2db7e905f7d 100644</span>
<span class="p_header">--- a/sound/core/oss/pcm_oss.c</span>
<span class="p_header">+++ b/sound/core/oss/pcm_oss.c</span>
<span class="p_chunk">@@ -455,7 +455,6 @@</span> <span class="p_context"> static int snd_pcm_hw_param_near(struct snd_pcm_substream *pcm,</span>
 		v = snd_pcm_hw_param_last(pcm, params, var, dir);
 	else
 		v = snd_pcm_hw_param_first(pcm, params, var, dir);
<span class="p_del">-	snd_BUG_ON(v &lt; 0);</span>
 	return v;
 }
 
<span class="p_chunk">@@ -1335,8 +1334,11 @@</span> <span class="p_context"> static ssize_t snd_pcm_oss_write1(struct snd_pcm_substream *substream, const cha</span>
 
 	if ((tmp = snd_pcm_oss_make_ready(substream)) &lt; 0)
 		return tmp;
<span class="p_del">-	mutex_lock(&amp;runtime-&gt;oss.params_lock);</span>
 	while (bytes &gt; 0) {
<span class="p_add">+		if (mutex_lock_interruptible(&amp;runtime-&gt;oss.params_lock)) {</span>
<span class="p_add">+			tmp = -ERESTARTSYS;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
 		if (bytes &lt; runtime-&gt;oss.period_bytes || runtime-&gt;oss.buffer_used &gt; 0) {
 			tmp = bytes;
 			if (tmp + runtime-&gt;oss.buffer_used &gt; runtime-&gt;oss.period_bytes)
<span class="p_chunk">@@ -1380,14 +1382,18 @@</span> <span class="p_context"> static ssize_t snd_pcm_oss_write1(struct snd_pcm_substream *substream, const cha</span>
 			xfer += tmp;
 			if ((substream-&gt;f_flags &amp; O_NONBLOCK) != 0 &amp;&amp;
 			    tmp != runtime-&gt;oss.period_bytes)
<span class="p_del">-				break;</span>
<span class="p_add">+				tmp = -EAGAIN;</span>
 		}
<span class="p_del">-	}</span>
<span class="p_del">-	mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_del">-	return xfer;</span>
<span class="p_del">-</span>
  err:
<span class="p_del">-	mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_add">+		mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_add">+		if (tmp &lt; 0)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (signal_pending(current)) {</span>
<span class="p_add">+			tmp = -ERESTARTSYS;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		tmp = 0;</span>
<span class="p_add">+	}</span>
 	return xfer &gt; 0 ? (snd_pcm_sframes_t)xfer : tmp;
 }
 
<span class="p_chunk">@@ -1435,8 +1441,11 @@</span> <span class="p_context"> static ssize_t snd_pcm_oss_read1(struct snd_pcm_substream *substream, char __use</span>
 
 	if ((tmp = snd_pcm_oss_make_ready(substream)) &lt; 0)
 		return tmp;
<span class="p_del">-	mutex_lock(&amp;runtime-&gt;oss.params_lock);</span>
 	while (bytes &gt; 0) {
<span class="p_add">+		if (mutex_lock_interruptible(&amp;runtime-&gt;oss.params_lock)) {</span>
<span class="p_add">+			tmp = -ERESTARTSYS;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
 		if (bytes &lt; runtime-&gt;oss.period_bytes || runtime-&gt;oss.buffer_used &gt; 0) {
 			if (runtime-&gt;oss.buffer_used == 0) {
 				tmp = snd_pcm_oss_read2(substream, runtime-&gt;oss.buffer, runtime-&gt;oss.period_bytes, 1);
<span class="p_chunk">@@ -1467,12 +1476,16 @@</span> <span class="p_context"> static ssize_t snd_pcm_oss_read1(struct snd_pcm_substream *substream, char __use</span>
 			bytes -= tmp;
 			xfer += tmp;
 		}
<span class="p_del">-	}</span>
<span class="p_del">-	mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_del">-	return xfer;</span>
<span class="p_del">-</span>
  err:
<span class="p_del">-	mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_add">+		mutex_unlock(&amp;runtime-&gt;oss.params_lock);</span>
<span class="p_add">+		if (tmp &lt; 0)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (signal_pending(current)) {</span>
<span class="p_add">+			tmp = -ERESTARTSYS;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		tmp = 0;</span>
<span class="p_add">+	}</span>
 	return xfer &gt; 0 ? (snd_pcm_sframes_t)xfer : tmp;
 }
 
<span class="p_header">diff --git a/sound/core/oss/pcm_plugin.c b/sound/core/oss/pcm_plugin.c</span>
<span class="p_header">index cadc93792868..85a56af104bd 100644</span>
<span class="p_header">--- a/sound/core/oss/pcm_plugin.c</span>
<span class="p_header">+++ b/sound/core/oss/pcm_plugin.c</span>
<span class="p_chunk">@@ -592,18 +592,26 @@</span> <span class="p_context"> snd_pcm_sframes_t snd_pcm_plug_write_transfer(struct snd_pcm_substream *plug, st</span>
 	snd_pcm_sframes_t frames = size;
 
 	plugin = snd_pcm_plug_first(plug);
<span class="p_del">-	while (plugin &amp;&amp; frames &gt; 0) {</span>
<span class="p_add">+	while (plugin) {</span>
<span class="p_add">+		if (frames &lt;= 0)</span>
<span class="p_add">+			return frames;</span>
 		if ((next = plugin-&gt;next) != NULL) {
 			snd_pcm_sframes_t frames1 = frames;
<span class="p_del">-			if (plugin-&gt;dst_frames)</span>
<span class="p_add">+			if (plugin-&gt;dst_frames) {</span>
 				frames1 = plugin-&gt;dst_frames(plugin, frames);
<span class="p_add">+				if (frames1 &lt;= 0)</span>
<span class="p_add">+					return frames1;</span>
<span class="p_add">+			}</span>
 			if ((err = next-&gt;client_channels(next, frames1, &amp;dst_channels)) &lt; 0) {
 				return err;
 			}
 			if (err != frames1) {
 				frames = err;
<span class="p_del">-				if (plugin-&gt;src_frames)</span>
<span class="p_add">+				if (plugin-&gt;src_frames) {</span>
 					frames = plugin-&gt;src_frames(plugin, frames1);
<span class="p_add">+					if (frames &lt;= 0)</span>
<span class="p_add">+						return frames;</span>
<span class="p_add">+				}</span>
 			}
 		} else
 			dst_channels = NULL;
<span class="p_header">diff --git a/sound/core/pcm_lib.c b/sound/core/pcm_lib.c</span>
<span class="p_header">index 10e7ef7a8804..db7894bb028c 100644</span>
<span class="p_header">--- a/sound/core/pcm_lib.c</span>
<span class="p_header">+++ b/sound/core/pcm_lib.c</span>
<span class="p_chunk">@@ -1632,7 +1632,7 @@</span> <span class="p_context"> int snd_pcm_hw_param_first(struct snd_pcm_substream *pcm,</span>
 		return changed;
 	if (params-&gt;rmask) {
 		int err = snd_pcm_hw_refine(pcm, params);
<span class="p_del">-		if (snd_BUG_ON(err &lt; 0))</span>
<span class="p_add">+		if (err &lt; 0)</span>
 			return err;
 	}
 	return snd_pcm_hw_param_value(params, var, dir);
<span class="p_chunk">@@ -1678,7 +1678,7 @@</span> <span class="p_context"> int snd_pcm_hw_param_last(struct snd_pcm_substream *pcm,</span>
 		return changed;
 	if (params-&gt;rmask) {
 		int err = snd_pcm_hw_refine(pcm, params);
<span class="p_del">-		if (snd_BUG_ON(err &lt; 0))</span>
<span class="p_add">+		if (err &lt; 0)</span>
 			return err;
 	}
 	return snd_pcm_hw_param_value(params, var, dir);
<span class="p_header">diff --git a/sound/core/pcm_native.c b/sound/core/pcm_native.c</span>
<span class="p_header">index 2fec2feac387..499f75b18e09 100644</span>
<span class="p_header">--- a/sound/core/pcm_native.c</span>
<span class="p_header">+++ b/sound/core/pcm_native.c</span>
<span class="p_chunk">@@ -2582,7 +2582,7 @@</span> <span class="p_context"> static snd_pcm_sframes_t forward_appl_ptr(struct snd_pcm_substream *substream,</span>
 	return ret &lt; 0 ? ret : frames;
 }
 
<span class="p_del">-/* decrease the appl_ptr; returns the processed frames or a negative error */</span>
<span class="p_add">+/* decrease the appl_ptr; returns the processed frames or zero for error */</span>
 static snd_pcm_sframes_t rewind_appl_ptr(struct snd_pcm_substream *substream,
 					 snd_pcm_uframes_t frames,
 					 snd_pcm_sframes_t avail)
<span class="p_chunk">@@ -2599,7 +2599,12 @@</span> <span class="p_context"> static snd_pcm_sframes_t rewind_appl_ptr(struct snd_pcm_substream *substream,</span>
 	if (appl_ptr &lt; 0)
 		appl_ptr += runtime-&gt;boundary;
 	ret = pcm_lib_apply_appl_ptr(substream, appl_ptr);
<span class="p_del">-	return ret &lt; 0 ? ret : frames;</span>
<span class="p_add">+	/* NOTE: we return zero for errors because PulseAudio gets depressed</span>
<span class="p_add">+	 * upon receiving an error from rewind ioctl and stops processing</span>
<span class="p_add">+	 * any longer.  Returning zero means that no rewind is done, so</span>
<span class="p_add">+	 * it&#39;s not absolutely wrong to answer like that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return ret &lt; 0 ? 0 : frames;</span>
 }
 
 static snd_pcm_sframes_t snd_pcm_playback_rewind(struct snd_pcm_substream *substream,
<span class="p_header">diff --git a/sound/drivers/aloop.c b/sound/drivers/aloop.c</span>
<span class="p_header">index 135adb17703c..386ee829c655 100644</span>
<span class="p_header">--- a/sound/drivers/aloop.c</span>
<span class="p_header">+++ b/sound/drivers/aloop.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"></span>
 #include &lt;sound/core.h&gt;
 #include &lt;sound/control.h&gt;
 #include &lt;sound/pcm.h&gt;
<span class="p_add">+#include &lt;sound/pcm_params.h&gt;</span>
 #include &lt;sound/info.h&gt;
 #include &lt;sound/initval.h&gt;
 
<span class="p_chunk">@@ -305,19 +306,6 @@</span> <span class="p_context"> static int loopback_trigger(struct snd_pcm_substream *substream, int cmd)</span>
 	return 0;
 }
 
<span class="p_del">-static void params_change_substream(struct loopback_pcm *dpcm,</span>
<span class="p_del">-				    struct snd_pcm_runtime *runtime)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct snd_pcm_runtime *dst_runtime;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (dpcm == NULL || dpcm-&gt;substream == NULL)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	dst_runtime = dpcm-&gt;substream-&gt;runtime;</span>
<span class="p_del">-	if (dst_runtime == NULL)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	dst_runtime-&gt;hw = dpcm-&gt;cable-&gt;hw;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void params_change(struct snd_pcm_substream *substream)
 {
 	struct snd_pcm_runtime *runtime = substream-&gt;runtime;
<span class="p_chunk">@@ -329,10 +317,6 @@</span> <span class="p_context"> static void params_change(struct snd_pcm_substream *substream)</span>
 	cable-&gt;hw.rate_max = runtime-&gt;rate;
 	cable-&gt;hw.channels_min = runtime-&gt;channels;
 	cable-&gt;hw.channels_max = runtime-&gt;channels;
<span class="p_del">-	params_change_substream(cable-&gt;streams[SNDRV_PCM_STREAM_PLAYBACK],</span>
<span class="p_del">-				runtime);</span>
<span class="p_del">-	params_change_substream(cable-&gt;streams[SNDRV_PCM_STREAM_CAPTURE],</span>
<span class="p_del">-				runtime);</span>
 }
 
 static int loopback_prepare(struct snd_pcm_substream *substream)
<span class="p_chunk">@@ -620,26 +604,29 @@</span> <span class="p_context"> static unsigned int get_cable_index(struct snd_pcm_substream *substream)</span>
 static int rule_format(struct snd_pcm_hw_params *params,
 		       struct snd_pcm_hw_rule *rule)
 {
<span class="p_add">+	struct loopback_pcm *dpcm = rule-&gt;private;</span>
<span class="p_add">+	struct loopback_cable *cable = dpcm-&gt;cable;</span>
<span class="p_add">+	struct snd_mask m;</span>
 
<span class="p_del">-	struct snd_pcm_hardware *hw = rule-&gt;private;</span>
<span class="p_del">-	struct snd_mask *maskp = hw_param_mask(params, rule-&gt;var);</span>
<span class="p_del">-</span>
<span class="p_del">-	maskp-&gt;bits[0] &amp;= (u_int32_t)hw-&gt;formats;</span>
<span class="p_del">-	maskp-&gt;bits[1] &amp;= (u_int32_t)(hw-&gt;formats &gt;&gt; 32);</span>
<span class="p_del">-	memset(maskp-&gt;bits + 2, 0, (SNDRV_MASK_MAX-64) / 8); /* clear rest */</span>
<span class="p_del">-	if (! maskp-&gt;bits[0] &amp;&amp; ! maskp-&gt;bits[1])</span>
<span class="p_del">-		return -EINVAL;</span>
<span class="p_del">-	return 0;</span>
<span class="p_add">+	snd_mask_none(&amp;m);</span>
<span class="p_add">+	mutex_lock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
<span class="p_add">+	m.bits[0] = (u_int32_t)cable-&gt;hw.formats;</span>
<span class="p_add">+	m.bits[1] = (u_int32_t)(cable-&gt;hw.formats &gt;&gt; 32);</span>
<span class="p_add">+	mutex_unlock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
<span class="p_add">+	return snd_mask_refine(hw_param_mask(params, rule-&gt;var), &amp;m);</span>
 }
 
 static int rule_rate(struct snd_pcm_hw_params *params,
 		     struct snd_pcm_hw_rule *rule)
 {
<span class="p_del">-	struct snd_pcm_hardware *hw = rule-&gt;private;</span>
<span class="p_add">+	struct loopback_pcm *dpcm = rule-&gt;private;</span>
<span class="p_add">+	struct loopback_cable *cable = dpcm-&gt;cable;</span>
 	struct snd_interval t;
 
<span class="p_del">-        t.min = hw-&gt;rate_min;</span>
<span class="p_del">-        t.max = hw-&gt;rate_max;</span>
<span class="p_add">+	mutex_lock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
<span class="p_add">+	t.min = cable-&gt;hw.rate_min;</span>
<span class="p_add">+	t.max = cable-&gt;hw.rate_max;</span>
<span class="p_add">+	mutex_unlock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
         t.openmin = t.openmax = 0;
         t.integer = 0;
 	return snd_interval_refine(hw_param_interval(params, rule-&gt;var), &amp;t);
<span class="p_chunk">@@ -648,22 +635,44 @@</span> <span class="p_context"> static int rule_rate(struct snd_pcm_hw_params *params,</span>
 static int rule_channels(struct snd_pcm_hw_params *params,
 			 struct snd_pcm_hw_rule *rule)
 {
<span class="p_del">-	struct snd_pcm_hardware *hw = rule-&gt;private;</span>
<span class="p_add">+	struct loopback_pcm *dpcm = rule-&gt;private;</span>
<span class="p_add">+	struct loopback_cable *cable = dpcm-&gt;cable;</span>
 	struct snd_interval t;
 
<span class="p_del">-        t.min = hw-&gt;channels_min;</span>
<span class="p_del">-        t.max = hw-&gt;channels_max;</span>
<span class="p_add">+	mutex_lock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
<span class="p_add">+	t.min = cable-&gt;hw.channels_min;</span>
<span class="p_add">+	t.max = cable-&gt;hw.channels_max;</span>
<span class="p_add">+	mutex_unlock(&amp;dpcm-&gt;loopback-&gt;cable_lock);</span>
         t.openmin = t.openmax = 0;
         t.integer = 0;
 	return snd_interval_refine(hw_param_interval(params, rule-&gt;var), &amp;t);
 }
 
<span class="p_add">+static void free_cable(struct snd_pcm_substream *substream)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct loopback *loopback = substream-&gt;private_data;</span>
<span class="p_add">+	int dev = get_cable_index(substream);</span>
<span class="p_add">+	struct loopback_cable *cable;</span>
<span class="p_add">+</span>
<span class="p_add">+	cable = loopback-&gt;cables[substream-&gt;number][dev];</span>
<span class="p_add">+	if (!cable)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (cable-&gt;streams[!substream-&gt;stream]) {</span>
<span class="p_add">+		/* other stream is still alive */</span>
<span class="p_add">+		cable-&gt;streams[substream-&gt;stream] = NULL;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* free the cable */</span>
<span class="p_add">+		loopback-&gt;cables[substream-&gt;number][dev] = NULL;</span>
<span class="p_add">+		kfree(cable);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int loopback_open(struct snd_pcm_substream *substream)
 {
 	struct snd_pcm_runtime *runtime = substream-&gt;runtime;
 	struct loopback *loopback = substream-&gt;private_data;
 	struct loopback_pcm *dpcm;
<span class="p_del">-	struct loopback_cable *cable;</span>
<span class="p_add">+	struct loopback_cable *cable = NULL;</span>
 	int err = 0;
 	int dev = get_cable_index(substream);
 
<span class="p_chunk">@@ -682,7 +691,6 @@</span> <span class="p_context"> static int loopback_open(struct snd_pcm_substream *substream)</span>
 	if (!cable) {
 		cable = kzalloc(sizeof(*cable), GFP_KERNEL);
 		if (!cable) {
<span class="p_del">-			kfree(dpcm);</span>
 			err = -ENOMEM;
 			goto unlock;
 		}
<span class="p_chunk">@@ -700,19 +708,19 @@</span> <span class="p_context"> static int loopback_open(struct snd_pcm_substream *substream)</span>
 	/* are cached -&gt; they do not reflect the actual state */
 	err = snd_pcm_hw_rule_add(runtime, 0,
 				  SNDRV_PCM_HW_PARAM_FORMAT,
<span class="p_del">-				  rule_format, &amp;runtime-&gt;hw,</span>
<span class="p_add">+				  rule_format, dpcm,</span>
 				  SNDRV_PCM_HW_PARAM_FORMAT, -1);
 	if (err &lt; 0)
 		goto unlock;
 	err = snd_pcm_hw_rule_add(runtime, 0,
 				  SNDRV_PCM_HW_PARAM_RATE,
<span class="p_del">-				  rule_rate, &amp;runtime-&gt;hw,</span>
<span class="p_add">+				  rule_rate, dpcm,</span>
 				  SNDRV_PCM_HW_PARAM_RATE, -1);
 	if (err &lt; 0)
 		goto unlock;
 	err = snd_pcm_hw_rule_add(runtime, 0,
 				  SNDRV_PCM_HW_PARAM_CHANNELS,
<span class="p_del">-				  rule_channels, &amp;runtime-&gt;hw,</span>
<span class="p_add">+				  rule_channels, dpcm,</span>
 				  SNDRV_PCM_HW_PARAM_CHANNELS, -1);
 	if (err &lt; 0)
 		goto unlock;
<span class="p_chunk">@@ -724,6 +732,10 @@</span> <span class="p_context"> static int loopback_open(struct snd_pcm_substream *substream)</span>
 	else
 		runtime-&gt;hw = cable-&gt;hw;
  unlock:
<span class="p_add">+	if (err &lt; 0) {</span>
<span class="p_add">+		free_cable(substream);</span>
<span class="p_add">+		kfree(dpcm);</span>
<span class="p_add">+	}</span>
 	mutex_unlock(&amp;loopback-&gt;cable_lock);
 	return err;
 }
<span class="p_chunk">@@ -732,20 +744,10 @@</span> <span class="p_context"> static int loopback_close(struct snd_pcm_substream *substream)</span>
 {
 	struct loopback *loopback = substream-&gt;private_data;
 	struct loopback_pcm *dpcm = substream-&gt;runtime-&gt;private_data;
<span class="p_del">-	struct loopback_cable *cable;</span>
<span class="p_del">-	int dev = get_cable_index(substream);</span>
 
 	loopback_timer_stop(dpcm);
 	mutex_lock(&amp;loopback-&gt;cable_lock);
<span class="p_del">-	cable = loopback-&gt;cables[substream-&gt;number][dev];</span>
<span class="p_del">-	if (cable-&gt;streams[!substream-&gt;stream]) {</span>
<span class="p_del">-		/* other stream is still alive */</span>
<span class="p_del">-		cable-&gt;streams[substream-&gt;stream] = NULL;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/* free the cable */</span>
<span class="p_del">-		loopback-&gt;cables[substream-&gt;number][dev] = NULL;</span>
<span class="p_del">-		kfree(cable);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	free_cable(substream);</span>
 	mutex_unlock(&amp;loopback-&gt;cable_lock);
 	return 0;
 }
<span class="p_header">diff --git a/tools/objtool/check.c b/tools/objtool/check.c</span>
<span class="p_header">index 9b341584eb1b..f40d46e24bcc 100644</span>
<span class="p_header">--- a/tools/objtool/check.c</span>
<span class="p_header">+++ b/tools/objtool/check.c</span>
<span class="p_chunk">@@ -427,6 +427,40 @@</span> <span class="p_context"> static void add_ignores(struct objtool_file *file)</span>
 	}
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: For now, just ignore any alternatives which add retpolines.  This is</span>
<span class="p_add">+ * a temporary hack, as it doesn&#39;t allow ORC to unwind from inside a retpoline.</span>
<span class="p_add">+ * But it at least allows objtool to understand the control flow *around* the</span>
<span class="p_add">+ * retpoline.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int add_nospec_ignores(struct objtool_file *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct section *sec;</span>
<span class="p_add">+	struct rela *rela;</span>
<span class="p_add">+	struct instruction *insn;</span>
<span class="p_add">+</span>
<span class="p_add">+	sec = find_section_by_name(file-&gt;elf, &quot;.rela.discard.nospec&quot;);</span>
<span class="p_add">+	if (!sec)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(rela, &amp;sec-&gt;rela_list, list) {</span>
<span class="p_add">+		if (rela-&gt;sym-&gt;type != STT_SECTION) {</span>
<span class="p_add">+			WARN(&quot;unexpected relocation symbol type in %s&quot;, sec-&gt;name);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		insn = find_insn(file, rela-&gt;sym-&gt;sec, rela-&gt;addend);</span>
<span class="p_add">+		if (!insn) {</span>
<span class="p_add">+			WARN(&quot;bad .discard.nospec entry&quot;);</span>
<span class="p_add">+			return -1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		insn-&gt;ignore_alts = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Find the destination instructions for all jumps.
  */
<span class="p_chunk">@@ -456,6 +490,13 @@</span> <span class="p_context"> static int add_jump_destinations(struct objtool_file *file)</span>
 		} else if (rela-&gt;sym-&gt;sec-&gt;idx) {
 			dest_sec = rela-&gt;sym-&gt;sec;
 			dest_off = rela-&gt;sym-&gt;sym.st_value + rela-&gt;addend + 4;
<span class="p_add">+		} else if (strstr(rela-&gt;sym-&gt;name, &quot;_indirect_thunk_&quot;)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Retpoline jumps are really dynamic jumps in</span>
<span class="p_add">+			 * disguise, so convert them accordingly.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			insn-&gt;type = INSN_JUMP_DYNAMIC;</span>
<span class="p_add">+			continue;</span>
 		} else {
 			/* sibling call */
 			insn-&gt;jump_dest = 0;
<span class="p_chunk">@@ -502,11 +543,18 @@</span> <span class="p_context"> static int add_call_destinations(struct objtool_file *file)</span>
 			dest_off = insn-&gt;offset + insn-&gt;len + insn-&gt;immediate;
 			insn-&gt;call_dest = find_symbol_by_offset(insn-&gt;sec,
 								dest_off);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * FIXME: Thanks to retpolines, it&#39;s now considered</span>
<span class="p_add">+			 * normal for a function to call within itself.  So</span>
<span class="p_add">+			 * disable this warning for now.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+#if 0</span>
 			if (!insn-&gt;call_dest) {
 				WARN_FUNC(&quot;can&#39;t find call dest symbol at offset 0x%lx&quot;,
 					  insn-&gt;sec, insn-&gt;offset, dest_off);
 				return -1;
 			}
<span class="p_add">+#endif</span>
 		} else if (rela-&gt;sym-&gt;type == STT_SECTION) {
 			insn-&gt;call_dest = find_symbol_by_offset(rela-&gt;sym-&gt;sec,
 								rela-&gt;addend+4);
<span class="p_chunk">@@ -671,12 +719,6 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 		return ret;
 
 	list_for_each_entry_safe(special_alt, tmp, &amp;special_alts, list) {
<span class="p_del">-		alt = malloc(sizeof(*alt));</span>
<span class="p_del">-		if (!alt) {</span>
<span class="p_del">-			WARN(&quot;malloc failed&quot;);</span>
<span class="p_del">-			ret = -1;</span>
<span class="p_del">-			goto out;</span>
<span class="p_del">-		}</span>
 
 		orig_insn = find_insn(file, special_alt-&gt;orig_sec,
 				      special_alt-&gt;orig_off);
<span class="p_chunk">@@ -687,6 +729,10 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 			goto out;
 		}
 
<span class="p_add">+		/* Ignore retpoline alternatives. */</span>
<span class="p_add">+		if (orig_insn-&gt;ignore_alts)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		new_insn = NULL;
 		if (!special_alt-&gt;group || special_alt-&gt;new_len) {
 			new_insn = find_insn(file, special_alt-&gt;new_sec,
<span class="p_chunk">@@ -712,6 +758,13 @@</span> <span class="p_context"> static int add_special_section_alts(struct objtool_file *file)</span>
 				goto out;
 		}
 
<span class="p_add">+		alt = malloc(sizeof(*alt));</span>
<span class="p_add">+		if (!alt) {</span>
<span class="p_add">+			WARN(&quot;malloc failed&quot;);</span>
<span class="p_add">+			ret = -1;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		alt-&gt;insn = new_insn;
 		list_add_tail(&amp;alt-&gt;list, &amp;orig_insn-&gt;alts);
 
<span class="p_chunk">@@ -1028,6 +1081,10 @@</span> <span class="p_context"> static int decode_sections(struct objtool_file *file)</span>
 
 	add_ignores(file);
 
<span class="p_add">+	ret = add_nospec_ignores(file);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
 	ret = add_jump_destinations(file);
 	if (ret)
 		return ret;
<span class="p_header">diff --git a/tools/objtool/check.h b/tools/objtool/check.h</span>
<span class="p_header">index 47d9ea70a83d..dbadb304a410 100644</span>
<span class="p_header">--- a/tools/objtool/check.h</span>
<span class="p_header">+++ b/tools/objtool/check.h</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> struct instruction {</span>
 	unsigned int len;
 	unsigned char type;
 	unsigned long immediate;
<span class="p_del">-	bool alt_group, visited, dead_end, ignore, hint, save, restore;</span>
<span class="p_add">+	bool alt_group, visited, dead_end, ignore, hint, save, restore, ignore_alts;</span>
 	struct symbol *call_dest;
 	struct instruction *jump_dest;
 	struct list_head alts;
<span class="p_header">diff --git a/tools/testing/selftests/bpf/test_verifier.c b/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_header">index 7a2d221c4702..1241487de93f 100644</span>
<span class="p_header">--- a/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_header">+++ b/tools/testing/selftests/bpf/test_verifier.c</span>
<span class="p_chunk">@@ -272,6 +272,46 @@</span> <span class="p_context"> static struct bpf_test tests[] = {</span>
 		.errstr = &quot;invalid bpf_ld_imm64 insn&quot;,
 		.result = REJECT,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;arsh32 on imm&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 1),</span>
<span class="p_add">+			BPF_ALU32_IMM(BPF_ARSH, BPF_REG_0, 5),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.result = REJECT,</span>
<span class="p_add">+		.errstr = &quot;BPF_ARSH not supported for 32 bit ALU&quot;,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;arsh32 on reg&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 1),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_1, 5),</span>
<span class="p_add">+			BPF_ALU32_REG(BPF_ARSH, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.result = REJECT,</span>
<span class="p_add">+		.errstr = &quot;BPF_ARSH not supported for 32 bit ALU&quot;,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;arsh64 on imm&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 1),</span>
<span class="p_add">+			BPF_ALU64_IMM(BPF_ARSH, BPF_REG_0, 5),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.result = ACCEPT,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
<span class="p_add">+		&quot;arsh64 on reg&quot;,</span>
<span class="p_add">+		.insns = {</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_0, 1),</span>
<span class="p_add">+			BPF_MOV64_IMM(BPF_REG_1, 5),</span>
<span class="p_add">+			BPF_ALU64_REG(BPF_ARSH, BPF_REG_0, BPF_REG_1),</span>
<span class="p_add">+			BPF_EXIT_INSN(),</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.result = ACCEPT,</span>
<span class="p_add">+	},</span>
 	{
 		&quot;no bpf_exit&quot;,
 		.insns = {
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index 7b1adeee4b0f..91fbfa8fdc15 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -7,7 +7,7 @@</span> <span class="p_context"> include ../lib.mk</span>
 
 TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall test_mremap_vdso \
 			check_initial_reg_state sigreturn ldt_gdt iopl mpx-mini-test ioperm \
<span class="p_del">-			protection_keys test_vdso</span>
<span class="p_add">+			protection_keys test_vdso test_vsyscall</span>
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault test_syscall_vdso unwind_vdso \
 			test_FCMOV test_FCOMI test_FISTTP \
 			vdso_restorer
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_vsyscall.c b/tools/testing/selftests/x86/test_vsyscall.c</span>
new file mode 100644
<span class="p_header">index 000000000000..6e0bd52ad53d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_vsyscall.c</span>
<span class="p_chunk">@@ -0,0 +1,500 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;sys/time.h&gt;</span>
<span class="p_add">+#include &lt;time.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;sys/syscall.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;dlfcn.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;inttypes.h&gt;</span>
<span class="p_add">+#include &lt;signal.h&gt;</span>
<span class="p_add">+#include &lt;sys/ucontext.h&gt;</span>
<span class="p_add">+#include &lt;errno.h&gt;</span>
<span class="p_add">+#include &lt;err.h&gt;</span>
<span class="p_add">+#include &lt;sched.h&gt;</span>
<span class="p_add">+#include &lt;stdbool.h&gt;</span>
<span class="p_add">+#include &lt;setjmp.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+# define VSYS(x) (x)</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define VSYS(x) 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef SYS_getcpu</span>
<span class="p_add">+# ifdef __x86_64__</span>
<span class="p_add">+#  define SYS_getcpu 309</span>
<span class="p_add">+# else</span>
<span class="p_add">+#  define SYS_getcpu 318</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void sethandler(int sig, void (*handler)(int, siginfo_t *, void *),</span>
<span class="p_add">+		       int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct sigaction sa;</span>
<span class="p_add">+	memset(&amp;sa, 0, sizeof(sa));</span>
<span class="p_add">+	sa.sa_sigaction = handler;</span>
<span class="p_add">+	sa.sa_flags = SA_SIGINFO | flags;</span>
<span class="p_add">+	sigemptyset(&amp;sa.sa_mask);</span>
<span class="p_add">+	if (sigaction(sig, &amp;sa, 0))</span>
<span class="p_add">+		err(1, &quot;sigaction&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* vsyscalls and vDSO */</span>
<span class="p_add">+bool should_read_vsyscall = false;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*gtod_t)(struct timeval *tv, struct timezone *tz);</span>
<span class="p_add">+gtod_t vgtod = (gtod_t)VSYS(0xffffffffff600000);</span>
<span class="p_add">+gtod_t vdso_gtod;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef int (*vgettime_t)(clockid_t, struct timespec *);</span>
<span class="p_add">+vgettime_t vdso_gettime;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*time_func_t)(time_t *t);</span>
<span class="p_add">+time_func_t vtime = (time_func_t)VSYS(0xffffffffff600400);</span>
<span class="p_add">+time_func_t vdso_time;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef long (*getcpu_t)(unsigned *, unsigned *, void *);</span>
<span class="p_add">+getcpu_t vgetcpu = (getcpu_t)VSYS(0xffffffffff600800);</span>
<span class="p_add">+getcpu_t vdso_getcpu;</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_vdso(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *vdso = dlopen(&quot;linux-vdso.so.1&quot;, RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);</span>
<span class="p_add">+	if (!vdso)</span>
<span class="p_add">+		vdso = dlopen(&quot;linux-gate.so.1&quot;, RTLD_LAZY | RTLD_LOCAL | RTLD_NOLOAD);</span>
<span class="p_add">+	if (!vdso) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find vDSO\n&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_gtod = (gtod_t)dlsym(vdso, &quot;__vdso_gettimeofday&quot;);</span>
<span class="p_add">+	if (!vdso_gtod)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find gettimeofday in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_gettime = (vgettime_t)dlsym(vdso, &quot;__vdso_clock_gettime&quot;);</span>
<span class="p_add">+	if (!vdso_gettime)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find clock_gettime in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_time = (time_func_t)dlsym(vdso, &quot;__vdso_time&quot;);</span>
<span class="p_add">+	if (!vdso_time)</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to find time in vDSO\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_getcpu = (getcpu_t)dlsym(vdso, &quot;__vdso_getcpu&quot;);</span>
<span class="p_add">+	if (!vdso_getcpu) {</span>
<span class="p_add">+		/* getcpu() was never wired up in the 32-bit vDSO. */</span>
<span class="p_add">+		printf(&quot;[%s]\tfailed to find getcpu in vDSO\n&quot;,</span>
<span class="p_add">+		       sizeof(long) == 8 ? &quot;WARN&quot; : &quot;NOTE&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int init_vsys(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	FILE *maps;</span>
<span class="p_add">+	char line[128];</span>
<span class="p_add">+	bool found = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	maps = fopen(&quot;/proc/self/maps&quot;, &quot;r&quot;);</span>
<span class="p_add">+	if (!maps) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tCould not open /proc/self/maps -- assuming vsyscall is r-x\n&quot;);</span>
<span class="p_add">+		should_read_vsyscall = true;</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	while (fgets(line, sizeof(line), maps)) {</span>
<span class="p_add">+		char r, x;</span>
<span class="p_add">+		void *start, *end;</span>
<span class="p_add">+		char name[128];</span>
<span class="p_add">+		if (sscanf(line, &quot;%p-%p %c-%cp %*x %*x:%*x %*u %s&quot;,</span>
<span class="p_add">+			   &amp;start, &amp;end, &amp;r, &amp;x, name) != 5)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (strcmp(name, &quot;[vsyscall]&quot;))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;\tvsyscall map: %s&quot;, line);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (start != (void *)0xffffffffff600000 ||</span>
<span class="p_add">+		    end != (void *)0xffffffffff601000) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\taddress range is nonsense\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;\tvsyscall permissions are %c-%c\n&quot;, r, x);</span>
<span class="p_add">+		should_read_vsyscall = (r == &#39;r&#39;);</span>
<span class="p_add">+		if (x != &#39;x&#39;) {</span>
<span class="p_add">+			vgtod = NULL;</span>
<span class="p_add">+			vtime = NULL;</span>
<span class="p_add">+			vgetcpu = NULL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		found = true;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fclose(maps);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!found) {</span>
<span class="p_add">+		printf(&quot;\tno vsyscall map in /proc/self/maps\n&quot;);</span>
<span class="p_add">+		should_read_vsyscall = false;</span>
<span class="p_add">+		vgtod = NULL;</span>
<span class="p_add">+		vtime = NULL;</span>
<span class="p_add">+		vgetcpu = NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* syscalls */</span>
<span class="p_add">+static inline long sys_gtod(struct timeval *tv, struct timezone *tz)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_gettimeofday, tv, tz);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int sys_clock_gettime(clockid_t id, struct timespec *ts)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_clock_gettime, id, ts);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long sys_time(time_t *t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_time, t);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long sys_getcpu(unsigned * cpu, unsigned * node,</span>
<span class="p_add">+			      void* cache)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return syscall(SYS_getcpu, cpu, node, cache);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static jmp_buf jmpbuf;</span>
<span class="p_add">+</span>
<span class="p_add">+static void sigsegv(int sig, siginfo_t *info, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	siglongjmp(jmpbuf, 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static double tv_diff(const struct timeval *a, const struct timeval *b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (double)(a-&gt;tv_sec - b-&gt;tv_sec) +</span>
<span class="p_add">+		(double)((int)a-&gt;tv_usec - (int)b-&gt;tv_usec) * 1e-6;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int check_gtod(const struct timeval *tv_sys1,</span>
<span class="p_add">+		      const struct timeval *tv_sys2,</span>
<span class="p_add">+		      const struct timezone *tz_sys,</span>
<span class="p_add">+		      const char *which,</span>
<span class="p_add">+		      const struct timeval *tv_other,</span>
<span class="p_add">+		      const struct timezone *tz_other)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	double d1, d2;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tz_other &amp;&amp; (tz_sys-&gt;tz_minuteswest != tz_other-&gt;tz_minuteswest || tz_sys-&gt;tz_dsttime != tz_other-&gt;tz_dsttime)) {</span>
<span class="p_add">+		printf(&quot;[FAIL] %s tz mismatch\n&quot;, which);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	d1 = tv_diff(tv_other, tv_sys1);</span>
<span class="p_add">+	d2 = tv_diff(tv_sys2, tv_other);</span>
<span class="p_add">+	printf(&quot;\t%s time offsets: %lf %lf\n&quot;, which, d1, d2);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (d1 &lt; 0 || d2 &lt; 0) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\t%s time was inconsistent with the syscall\n&quot;, which);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\t%s gettimeofday()&#39;s timeval was okay\n&quot;, which);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_gtod(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct timeval tv_sys1, tv_sys2, tv_vdso, tv_vsys;</span>
<span class="p_add">+	struct timezone tz_sys, tz_vdso, tz_vsys;</span>
<span class="p_add">+	long ret_vdso = -1;</span>
<span class="p_add">+	long ret_vsys = -1;</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\ttest gettimeofday()\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sys_gtod(&amp;tv_sys1, &amp;tz_sys) != 0)</span>
<span class="p_add">+		err(1, &quot;syscall gettimeofday&quot;);</span>
<span class="p_add">+	if (vdso_gtod)</span>
<span class="p_add">+		ret_vdso = vdso_gtod(&amp;tv_vdso, &amp;tz_vdso);</span>
<span class="p_add">+	if (vgtod)</span>
<span class="p_add">+		ret_vsys = vgtod(&amp;tv_vsys, &amp;tz_vsys);</span>
<span class="p_add">+	if (sys_gtod(&amp;tv_sys2, &amp;tz_sys) != 0)</span>
<span class="p_add">+		err(1, &quot;syscall gettimeofday&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_gtod) {</span>
<span class="p_add">+		if (ret_vdso == 0) {</span>
<span class="p_add">+			nerrs += check_gtod(&amp;tv_sys1, &amp;tv_sys2, &amp;tz_sys, &quot;vDSO&quot;, &amp;tv_vdso, &amp;tz_vdso);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO gettimeofday() failed: %ld\n&quot;, ret_vdso);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vgtod) {</span>
<span class="p_add">+		if (ret_vsys == 0) {</span>
<span class="p_add">+			nerrs += check_gtod(&amp;tv_sys1, &amp;tv_sys2, &amp;tz_sys, &quot;vsyscall&quot;, &amp;tv_vsys, &amp;tz_vsys);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsys gettimeofday() failed: %ld\n&quot;, ret_vsys);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_time(void) {</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\ttest time()\n&quot;);</span>
<span class="p_add">+	long t_sys1, t_sys2, t_vdso = 0, t_vsys = 0;</span>
<span class="p_add">+	long t2_sys1 = -1, t2_sys2 = -1, t2_vdso = -1, t2_vsys = -1;</span>
<span class="p_add">+	t_sys1 = sys_time(&amp;t2_sys1);</span>
<span class="p_add">+	if (vdso_time)</span>
<span class="p_add">+		t_vdso = vdso_time(&amp;t2_vdso);</span>
<span class="p_add">+	if (vtime)</span>
<span class="p_add">+		t_vsys = vtime(&amp;t2_vsys);</span>
<span class="p_add">+	t_sys2 = sys_time(&amp;t2_sys2);</span>
<span class="p_add">+	if (t_sys1 &lt; 0 || t_sys1 != t2_sys1 || t_sys2 &lt; 0 || t_sys2 != t2_sys2) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tsyscall failed (ret1:%ld output1:%ld ret2:%ld output2:%ld)\n&quot;, t_sys1, t2_sys1, t_sys2, t2_sys2);</span>
<span class="p_add">+		nerrs++;</span>
<span class="p_add">+		return nerrs;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_time) {</span>
<span class="p_add">+		if (t_vdso &lt; 0 || t_vdso != t2_vdso) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO failed (ret:%ld output:%ld)\n&quot;, t_vdso, t2_vdso);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else if (t_vdso &lt; t_sys1 || t_vdso &gt; t_sys2) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO returned the wrong time (%ld %ld %ld)\n&quot;, t_sys1, t_vdso, t_sys2);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[OK]\tvDSO time() is okay\n&quot;);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vtime) {</span>
<span class="p_add">+		if (t_vsys &lt; 0 || t_vsys != t2_vsys) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall failed (ret:%ld output:%ld)\n&quot;, t_vsys, t2_vsys);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else if (t_vsys &lt; t_sys1 || t_vsys &gt; t_sys2) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall returned the wrong time (%ld %ld %ld)\n&quot;, t_sys1, t_vsys, t_sys2);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			printf(&quot;[OK]\tvsyscall time() is okay\n&quot;);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_getcpu(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+	long ret_sys, ret_vdso = -1, ret_vsys = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tgetcpu() on CPU %d\n&quot;, cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	cpu_set_t cpuset;</span>
<span class="p_add">+	CPU_ZERO(&amp;cpuset);</span>
<span class="p_add">+	CPU_SET(cpu, &amp;cpuset);</span>
<span class="p_add">+	if (sched_setaffinity(0, sizeof(cpuset), &amp;cpuset) != 0) {</span>
<span class="p_add">+		printf(&quot;[SKIP]\tfailed to force CPU %d\n&quot;, cpu);</span>
<span class="p_add">+		return nerrs;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned cpu_sys, cpu_vdso, cpu_vsys, node_sys, node_vdso, node_vsys;</span>
<span class="p_add">+	unsigned node = 0;</span>
<span class="p_add">+	bool have_node = false;</span>
<span class="p_add">+	ret_sys = sys_getcpu(&amp;cpu_sys, &amp;node_sys, 0);</span>
<span class="p_add">+	if (vdso_getcpu)</span>
<span class="p_add">+		ret_vdso = vdso_getcpu(&amp;cpu_vdso, &amp;node_vdso, 0);</span>
<span class="p_add">+	if (vgetcpu)</span>
<span class="p_add">+		ret_vsys = vgetcpu(&amp;cpu_vsys, &amp;node_vsys, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret_sys == 0) {</span>
<span class="p_add">+		if (cpu_sys != cpu) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tsyscall reported CPU %hu but should be %d\n&quot;, cpu_sys, cpu);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		have_node = true;</span>
<span class="p_add">+		node = node_sys;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vdso_getcpu) {</span>
<span class="p_add">+		if (ret_vdso) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvDSO getcpu() failed\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!have_node) {</span>
<span class="p_add">+				have_node = true;</span>
<span class="p_add">+				node = node_vdso;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (cpu_vdso != cpu) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvDSO reported CPU %hu but should be %d\n&quot;, cpu_vdso, cpu);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvDSO reported correct CPU\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (node_vdso != node) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvDSO reported node %hu but should be %hu\n&quot;, node_vdso, node);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvDSO reported correct node\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vgetcpu) {</span>
<span class="p_add">+		if (ret_vsys) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tvsyscall getcpu() failed\n&quot;);</span>
<span class="p_add">+			nerrs++;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!have_node) {</span>
<span class="p_add">+				have_node = true;</span>
<span class="p_add">+				node = node_vsys;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (cpu_vsys != cpu) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvsyscall reported CPU %hu but should be %d\n&quot;, cpu_vsys, cpu);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvsyscall reported correct CPU\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (node_vsys != node) {</span>
<span class="p_add">+				printf(&quot;[FAIL]\tvsyscall reported node %hu but should be %hu\n&quot;, node_vsys, node);</span>
<span class="p_add">+				nerrs++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				printf(&quot;[OK]\tvsyscall reported correct node\n&quot;);</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_vsys_r(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	printf(&quot;[RUN]\tChecking read access to the vsyscall page\n&quot;);</span>
<span class="p_add">+	bool can_read;</span>
<span class="p_add">+	if (sigsetjmp(jmpbuf, 1) == 0) {</span>
<span class="p_add">+		*(volatile int *)0xffffffffff600000;</span>
<span class="p_add">+		can_read = true;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		can_read = false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (can_read &amp;&amp; !should_read_vsyscall) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tWe have read access, but we shouldn&#39;t\n&quot;);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	} else if (!can_read &amp;&amp; should_read_vsyscall) {</span>
<span class="p_add">+		printf(&quot;[FAIL]\tWe don&#39;t have read access, but we should\n&quot;);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printf(&quot;[OK]\tgot expected result\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+#define X86_EFLAGS_TF (1UL &lt;&lt; 8)</span>
<span class="p_add">+static volatile sig_atomic_t num_vsyscall_traps;</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long get_eflags(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long eflags;</span>
<span class="p_add">+	asm volatile (&quot;pushfq\n\tpopq %0&quot; : &quot;=rm&quot; (eflags));</span>
<span class="p_add">+	return eflags;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_eflags(unsigned long eflags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;pushq %0\n\tpopfq&quot; : : &quot;rm&quot; (eflags) : &quot;flags&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void sigtrap(int sig, siginfo_t *info, void *ctx_void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ucontext_t *ctx = (ucontext_t *)ctx_void;</span>
<span class="p_add">+	unsigned long ip = ctx-&gt;uc_mcontext.gregs[REG_RIP];</span>
<span class="p_add">+</span>
<span class="p_add">+	if (((ip ^ 0xffffffffff600000UL) &amp; ~0xfffUL) == 0)</span>
<span class="p_add">+		num_vsyscall_traps++;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int test_native_vsyscall(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	time_t tmp;</span>
<span class="p_add">+	bool is_native;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vtime)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[RUN]\tchecking for native vsyscall\n&quot;);</span>
<span class="p_add">+	sethandler(SIGTRAP, sigtrap, 0);</span>
<span class="p_add">+	set_eflags(get_eflags() | X86_EFLAGS_TF);</span>
<span class="p_add">+	vtime(&amp;tmp);</span>
<span class="p_add">+	set_eflags(get_eflags() &amp; ~X86_EFLAGS_TF);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If vsyscalls are emulated, we expect a single trap in the</span>
<span class="p_add">+	 * vsyscall page -- the call instruction will trap with RIP</span>
<span class="p_add">+	 * pointing to the entry point before emulation takes over.</span>
<span class="p_add">+	 * In native mode, we expect two traps, since whatever code</span>
<span class="p_add">+	 * the vsyscall page contains will be more than just a ret</span>
<span class="p_add">+	 * instruction.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	is_native = (num_vsyscall_traps &gt; 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;\tvsyscalls are %s (%d instructions in vsyscall page)\n&quot;,</span>
<span class="p_add">+	       (is_native ? &quot;native&quot; : &quot;emulated&quot;),</span>
<span class="p_add">+	       (int)num_vsyscall_traps);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+int main(int argc, char **argv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nerrs = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	init_vdso();</span>
<span class="p_add">+	nerrs += init_vsys();</span>
<span class="p_add">+</span>
<span class="p_add">+	nerrs += test_gtod();</span>
<span class="p_add">+	nerrs += test_time();</span>
<span class="p_add">+	nerrs += test_getcpu(0);</span>
<span class="p_add">+	nerrs += test_getcpu(1);</span>
<span class="p_add">+</span>
<span class="p_add">+	sethandler(SIGSEGV, sigsegv, 0);</span>
<span class="p_add">+	nerrs += test_vsys_r();</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __x86_64__</span>
<span class="p_add">+	nerrs += test_native_vsyscall();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	return nerrs ? 1 : 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/virt/kvm/arm/mmio.c b/virt/kvm/arm/mmio.c</span>
<span class="p_header">index b6e715fd3c90..dac7ceb1a677 100644</span>
<span class="p_header">--- a/virt/kvm/arm/mmio.c</span>
<span class="p_header">+++ b/virt/kvm/arm/mmio.c</span>
<span class="p_chunk">@@ -112,7 +112,7 @@</span> <span class="p_context"> int kvm_handle_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		}
 
 		trace_kvm_mmio(KVM_TRACE_MMIO_READ, len, run-&gt;mmio.phys_addr,
<span class="p_del">-			       data);</span>
<span class="p_add">+			       &amp;data);</span>
 		data = vcpu_data_host_to_guest(vcpu, data, len);
 		vcpu_set_reg(vcpu, vcpu-&gt;arch.mmio_decode.rt, data);
 	}
<span class="p_chunk">@@ -182,14 +182,14 @@</span> <span class="p_context"> int io_mem_abort(struct kvm_vcpu *vcpu, struct kvm_run *run,</span>
 		data = vcpu_data_guest_to_host(vcpu, vcpu_get_reg(vcpu, rt),
 					       len);
 
<span class="p_del">-		trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, len, fault_ipa, data);</span>
<span class="p_add">+		trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, len, fault_ipa, &amp;data);</span>
 		kvm_mmio_write_buf(data_buf, len, data);
 
 		ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
 				       data_buf);
 	} else {
 		trace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, len,
<span class="p_del">-			       fault_ipa, 0);</span>
<span class="p_add">+			       fault_ipa, NULL);</span>
 
 		ret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, fault_ipa, len,
 				      data_buf);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



