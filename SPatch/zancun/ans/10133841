
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4.14,30/74] x86/cpu_entry_area: Move it out of the fixmap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4.14,30/74] x86/cpu_entry_area: Move it out of the fixmap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 27, 2017, 4:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171227164615.298926928@linuxfoundation.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10133841/mbox/"
   >mbox</a>
|
   <a href="/patch/10133841/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10133841/">/patch/10133841/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A63C960388 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 16:48:59 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 99A332D48B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 16:48:59 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8E4962D4BD; Wed, 27 Dec 2017 16:48:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3282F2D48B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Dec 2017 16:48:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752941AbdL0Qsy (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 27 Dec 2017 11:48:54 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:36148 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752885AbdL0Qsq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 27 Dec 2017 11:48:46 -0500
Received: from localhost (LFbn-1-12258-90.w90-92.abo.wanadoo.fr
	[90.92.71.90])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 089EFBD0;
	Wed, 27 Dec 2017 16:48:45 +0000 (UTC)
From: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	stable@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;,
	Juergen Gross &lt;jgross@suse.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;
Subject: [PATCH 4.14 30/74] x86/cpu_entry_area: Move it out of the fixmap
Date: Wed, 27 Dec 2017 17:46:03 +0100
Message-Id: &lt;20171227164615.298926928@linuxfoundation.org&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20171227164614.109898944@linuxfoundation.org&gt;
References: &lt;20171227164614.109898944@linuxfoundation.org&gt;
User-Agent: quilt/0.65
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Dec. 27, 2017, 4:46 p.m.</div>
<pre class="content">
4.14-stable review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>

commit 92a0f81d89571e3e8759366e050ee05cc545ef99 upstream.

Put the cpu_entry_area into a separate P4D entry. The fixmap gets too big
and 0-day already hit a case where the fixmap PTEs were cleared by
cleanup_highmap().

Aside of that the fixmap API is a pain as it&#39;s all backwards.
<span class="signed-off-by">
Signed-off-by: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
Cc: Andy Lutomirski &lt;luto@kernel.org&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
Cc: H. Peter Anvin &lt;hpa@zytor.com&gt;
Cc: Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;
Cc: Juergen Gross &lt;jgross@suse.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: linux-kernel@vger.kernel.org
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span>

---
 Documentation/x86/x86_64/mm.txt         |    2 
 arch/x86/include/asm/cpu_entry_area.h   |   18 ++++++++
 arch/x86/include/asm/desc.h             |    1 
 arch/x86/include/asm/fixmap.h           |   32 ---------------
 arch/x86/include/asm/pgtable_32_types.h |   15 +++++--
 arch/x86/include/asm/pgtable_64_types.h |   47 +++++++++++++---------
 arch/x86/kernel/dumpstack.c             |    1 
 arch/x86/kernel/traps.c                 |    5 +-
 arch/x86/mm/cpu_entry_area.c            |   66 ++++++++++++++++++++++++--------
 arch/x86/mm/dump_pagetables.c           |    6 ++
 arch/x86/mm/init_32.c                   |    6 ++
 arch/x86/mm/kasan_init_64.c             |   29 +++++++-------
 arch/x86/mm/pgtable_32.c                |    1 
 arch/x86/xen/mmu_pv.c                   |    2 
 14 files changed, 143 insertions(+), 88 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_chunk">@@ -35,6 +36,7 @@</span> <span class="p_context"> ffd4000000000000 - ffd5ffffffffffff (=49</span>
 ... unused hole ...
 ffdf000000000000 - fffffc0000000000 (=53 bits) kasan shadow memory (8PB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_header">--- a/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_chunk">@@ -43,10 +43,26 @@</span> <span class="p_context"> struct cpu_entry_area {</span>
 };
 
 #define CPU_ENTRY_AREA_SIZE	(sizeof(struct cpu_entry_area))
<span class="p_del">-#define CPU_ENTRY_AREA_PAGES	(CPU_ENTRY_AREA_SIZE / PAGE_SIZE)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_TOT_SIZE	(CPU_ENTRY_AREA_SIZE * NR_CPUS)</span>
 
 DECLARE_PER_CPU(struct cpu_entry_area *, cpu_entry_area);
 
 extern void setup_cpu_entry_areas(void);
<span class="p_add">+extern void cea_set_pte(void *cea_vaddr, phys_addr_t pa, pgprot_t flags);</span>
<span class="p_add">+</span>
<span class="p_add">+#define	CPU_ENTRY_AREA_RO_IDT		CPU_ENTRY_AREA_BASE</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PER_CPU		(CPU_ENTRY_AREA_RO_IDT + PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_RO_IDT_VADDR	((void *)CPU_ENTRY_AREA_RO_IDT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_MAP_SIZE			\</span>
<span class="p_add">+	(CPU_ENTRY_AREA_PER_CPU + CPU_ENTRY_AREA_TOT_SIZE - CPU_ENTRY_AREA_BASE)</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct cpu_entry_area *get_cpu_entry_area(int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct entry_stack *cpu_entry_stack(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;get_cpu_entry_area(cpu)-&gt;entry_stack_page.stack;</span>
<span class="p_add">+}</span>
 
 #endif
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/mmu.h&gt;
 #include &lt;asm/fixmap.h&gt;
 #include &lt;asm/irq_vectors.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 
 #include &lt;linux/smp.h&gt;
 #include &lt;linux/percpu.h&gt;
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -25,7 +25,6 @@</span> <span class="p_context"></span>
 #else
 #include &lt;uapi/asm/vsyscall.h&gt;
 #endif
<span class="p_del">-#include &lt;asm/cpu_entry_area.h&gt;</span>
 
 /*
  * We can&#39;t declare FIXADDR_TOP as variable for x86_64 because vsyscall
<span class="p_chunk">@@ -84,7 +83,6 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 	FIX_IO_APIC_BASE_0,
 	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + MAX_IO_APICS - 1,
 #endif
<span class="p_del">-	FIX_RO_IDT,	/* Virtual mapping for read-only IDT */</span>
 #ifdef CONFIG_X86_32
 	FIX_KMAP_BEGIN,	/* reserved pte&#39;s for temporary kernel mappings */
 	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
<span class="p_chunk">@@ -100,9 +98,6 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 #ifdef	CONFIG_X86_INTEL_MID
 	FIX_LNW_VRTC,
 #endif
<span class="p_del">-	/* Fixmap entries to remap the GDTs, one per processor. */</span>
<span class="p_del">-	FIX_CPU_ENTRY_AREA_TOP,</span>
<span class="p_del">-	FIX_CPU_ENTRY_AREA_BOTTOM = FIX_CPU_ENTRY_AREA_TOP + (CPU_ENTRY_AREA_PAGES * NR_CPUS) - 1,</span>
 
 #ifdef CONFIG_ACPI_APEI_GHES
 	/* Used for GHES mapping from assorted contexts */
<span class="p_chunk">@@ -143,7 +138,7 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 extern void reserve_top_address(unsigned long reserve);
 
 #define FIXADDR_SIZE	(__end_of_permanent_fixed_addresses &lt;&lt; PAGE_SHIFT)
<span class="p_del">-#define FIXADDR_START		(FIXADDR_TOP - FIXADDR_SIZE)</span>
<span class="p_add">+#define FIXADDR_START	(FIXADDR_TOP - FIXADDR_SIZE)</span>
 
 extern int fixmaps_set;
 
<span class="p_chunk">@@ -191,30 +186,5 @@</span> <span class="p_context"> void __init *early_memremap_decrypted_wp</span>
 void __early_set_fixmap(enum fixed_addresses idx,
 			phys_addr_t phys, pgprot_t flags);
 
<span class="p_del">-static inline unsigned int __get_cpu_entry_area_page_index(int cpu, int page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	BUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);</span>
<span class="p_del">-</span>
<span class="p_del">-	return FIX_CPU_ENTRY_AREA_BOTTOM - cpu*CPU_ENTRY_AREA_PAGES - page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define __get_cpu_entry_area_offset_index(cpu, offset) ({		\</span>
<span class="p_del">-	BUILD_BUG_ON(offset % PAGE_SIZE != 0);				\</span>
<span class="p_del">-	__get_cpu_entry_area_page_index(cpu, offset / PAGE_SIZE);	\</span>
<span class="p_del">-	})</span>
<span class="p_del">-</span>
<span class="p_del">-#define get_cpu_entry_area_index(cpu, field)				\</span>
<span class="p_del">-	__get_cpu_entry_area_offset_index((cpu), offsetof(struct cpu_entry_area, field))</span>
<span class="p_del">-</span>
<span class="p_del">-static inline struct cpu_entry_area *get_cpu_entry_area(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (struct cpu_entry_area *)__fix_to_virt(__get_cpu_entry_area_page_index(cpu, 0));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline struct entry_stack *cpu_entry_stack(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return &amp;get_cpu_entry_area(cpu)-&gt;entry_stack_page.stack;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* !__ASSEMBLY__ */
 #endif /* _ASM_X86_FIXMAP_H */
<span class="p_header">--- a/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_32_types.h</span>
<span class="p_chunk">@@ -38,13 +38,22 @@</span> <span class="p_context"> extern bool __vmalloc_start_set; /* set</span>
 #define LAST_PKMAP 1024
 #endif
 
<span class="p_del">-#define PKMAP_BASE ((FIXADDR_START - PAGE_SIZE * (LAST_PKMAP + 1))	\</span>
<span class="p_del">-		    &amp; PMD_MASK)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Define this here and validate with BUILD_BUG_ON() in pgtable_32.c</span>
<span class="p_add">+ * to avoid include recursion hell</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PAGES	(NR_CPUS * 40)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_BASE				\</span>
<span class="p_add">+	((FIXADDR_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) &amp; PMD_MASK)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PKMAP_BASE		\</span>
<span class="p_add">+	((CPU_ENTRY_AREA_BASE - PAGE_SIZE) &amp; PMD_MASK)</span>
 
 #ifdef CONFIG_HIGHMEM
 # define VMALLOC_END	(PKMAP_BASE - 2 * PAGE_SIZE)
 #else
<span class="p_del">-# define VMALLOC_END	(FIXADDR_START - 2 * PAGE_SIZE)</span>
<span class="p_add">+# define VMALLOC_END	(CPU_ENTRY_AREA_BASE - 2 * PAGE_SIZE)</span>
 #endif
 
 #define MODULES_VADDR	VMALLOC_START
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -76,32 +76,41 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 
 /* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
<span class="p_del">-#define MAXMEM		_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)</span>
<span class="p_add">+#define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_5LEVEL
<span class="p_del">-#define VMALLOC_SIZE_TB _AC(16384, UL)</span>
<span class="p_del">-#define __VMALLOC_BASE	_AC(0xff92000000000000, UL)</span>
<span class="p_del">-#define __VMEMMAP_BASE	_AC(0xffd4000000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(16384, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xff92000000000000, UL)</span>
<span class="p_add">+# define __VMEMMAP_BASE		_AC(0xffd4000000000000, UL)</span>
 #else
<span class="p_del">-#define VMALLOC_SIZE_TB	_AC(32, UL)</span>
<span class="p_del">-#define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)</span>
<span class="p_del">-#define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(32, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)</span>
<span class="p_add">+# define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_RANDOMIZE_MEMORY
<span class="p_del">-#define VMALLOC_START	vmalloc_base</span>
<span class="p_del">-#define VMEMMAP_START	vmemmap_base</span>
<span class="p_add">+# define VMALLOC_START		vmalloc_base</span>
<span class="p_add">+# define VMEMMAP_START		vmemmap_base</span>
 #else
<span class="p_del">-#define VMALLOC_START	__VMALLOC_BASE</span>
<span class="p_del">-#define VMEMMAP_START	__VMEMMAP_BASE</span>
<span class="p_add">+# define VMALLOC_START		__VMALLOC_BASE</span>
<span class="p_add">+# define VMEMMAP_START		__VMEMMAP_BASE</span>
 #endif /* CONFIG_RANDOMIZE_MEMORY */
<span class="p_del">-#define VMALLOC_END	(VMALLOC_START + _AC((VMALLOC_SIZE_TB &lt;&lt; 40) - 1, UL))</span>
<span class="p_del">-#define MODULES_VADDR    (__START_KERNEL_map + KERNEL_IMAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define VMALLOC_END		(VMALLOC_START + _AC((VMALLOC_SIZE_TB &lt;&lt; 40) - 1, UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define MODULES_VADDR		(__START_KERNEL_map + KERNEL_IMAGE_SIZE)</span>
 /* The module sections ends with the start of the fixmap */
<span class="p_del">-#define MODULES_END   __fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_del">-#define MODULES_LEN   (MODULES_END - MODULES_VADDR)</span>
<span class="p_del">-#define ESPFIX_PGD_ENTRY _AC(-2, UL)</span>
<span class="p_del">-#define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)</span>
<span class="p_del">-#define EFI_VA_START	 ( -4 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_del">-#define EFI_VA_END	 (-68 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_add">+#define MODULES_END		__fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_add">+#define MODULES_LEN		(MODULES_END - MODULES_VADDR)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ESPFIX_PGD_ENTRY	_AC(-2, UL)</span>
<span class="p_add">+#define ESPFIX_BASE_ADDR	(ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PGD	_AC(-3, UL)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_BASE	(CPU_ENTRY_AREA_PGD &lt;&lt; P4D_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define EFI_VA_START		( -4 * (_AC(1, UL) &lt;&lt; 30))</span>
<span class="p_add">+#define EFI_VA_END		(-68 * (_AC(1, UL) &lt;&lt; 30))</span>
 
 #define EARLY_DYNAMIC_PAGE_TABLES	64
 
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nmi.h&gt;
 #include &lt;linux/sysfs.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/stacktrace.h&gt;
 #include &lt;asm/unwind.h&gt;
 
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -951,8 +951,9 @@</span> <span class="p_context"> void __init trap_init(void)</span>
 	 * &quot;sidt&quot; instruction will not leak the location of the kernel, and
 	 * to defend the IDT against arbitrary memory write vulnerabilities.
 	 * It will be reloaded in cpu_init() */
<span class="p_del">-	__set_fixmap(FIX_RO_IDT, __pa_symbol(idt_table), PAGE_KERNEL_RO);</span>
<span class="p_del">-	idt_descr.address = fix_to_virt(FIX_RO_IDT);</span>
<span class="p_add">+	cea_set_pte(CPU_ENTRY_AREA_RO_IDT_VADDR, __pa_symbol(idt_table),</span>
<span class="p_add">+		    PAGE_KERNEL_RO);</span>
<span class="p_add">+	idt_descr.address = CPU_ENTRY_AREA_RO_IDT;</span>
 
 	/*
 	 * Should be a barrier for any external CPU state:
<span class="p_header">--- a/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_header">+++ b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_chunk">@@ -15,11 +15,27 @@</span> <span class="p_context"> static DEFINE_PER_CPU_PAGE_ALIGNED(char,</span>
 	[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ]);
 #endif
 
<span class="p_add">+struct cpu_entry_area *get_cpu_entry_area(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long va = CPU_ENTRY_AREA_PER_CPU + cpu * CPU_ENTRY_AREA_SIZE;</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return (struct cpu_entry_area *) va;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(get_cpu_entry_area);</span>
<span class="p_add">+</span>
<span class="p_add">+void cea_set_pte(void *cea_vaddr, phys_addr_t pa, pgprot_t flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long va = (unsigned long) cea_vaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pte_vaddr(va, pfn_pte(pa &gt;&gt; PAGE_SHIFT, flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init
<span class="p_del">-set_percpu_fixmap_pages(int idx, void *ptr, int pages, pgprot_t prot)</span>
<span class="p_add">+cea_map_percpu_pages(void *cea_vaddr, void *ptr, int pages, pgprot_t prot)</span>
 {
<span class="p_del">-	for ( ; pages; pages--, idx--, ptr += PAGE_SIZE)</span>
<span class="p_del">-		__set_fixmap(idx, per_cpu_ptr_to_phys(ptr), prot);</span>
<span class="p_add">+	for ( ; pages; pages--, cea_vaddr+= PAGE_SIZE, ptr += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea_vaddr, per_cpu_ptr_to_phys(ptr), prot);</span>
 }
 
 /* Setup the fixmap mappings only once per-processor */
<span class="p_chunk">@@ -47,10 +63,12 @@</span> <span class="p_context"> static void __init setup_cpu_entry_area(</span>
 	pgprot_t tss_prot = PAGE_KERNEL;
 #endif
 
<span class="p_del">-	__set_fixmap(get_cpu_entry_area_index(cpu, gdt), get_cpu_gdt_paddr(cpu), gdt_prot);</span>
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, entry_stack_page),</span>
<span class="p_del">-				per_cpu_ptr(&amp;entry_stack_storage, cpu), 1,</span>
<span class="p_del">-				PAGE_KERNEL);</span>
<span class="p_add">+	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;gdt, get_cpu_gdt_paddr(cpu),</span>
<span class="p_add">+		    gdt_prot);</span>
<span class="p_add">+</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;entry_stack_page,</span>
<span class="p_add">+			     per_cpu_ptr(&amp;entry_stack_storage, cpu), 1,</span>
<span class="p_add">+			     PAGE_KERNEL);</span>
 
 	/*
 	 * The Intel SDM says (Volume 3, 7.2.1):
<span class="p_chunk">@@ -72,10 +90,9 @@</span> <span class="p_context"> static void __init setup_cpu_entry_area(</span>
 	BUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^
 		      offsetofend(struct tss_struct, x86_tss)) &amp; PAGE_MASK);
 	BUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, tss),</span>
<span class="p_del">-				&amp;per_cpu(cpu_tss_rw, cpu),</span>
<span class="p_del">-				sizeof(struct tss_struct) / PAGE_SIZE,</span>
<span class="p_del">-				tss_prot);</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;tss,</span>
<span class="p_add">+			     &amp;per_cpu(cpu_tss_rw, cpu),</span>
<span class="p_add">+			     sizeof(struct tss_struct) / PAGE_SIZE, tss_prot);</span>
 
 #ifdef CONFIG_X86_32
 	per_cpu(cpu_entry_area, cpu) = get_cpu_entry_area(cpu);
<span class="p_chunk">@@ -85,20 +102,37 @@</span> <span class="p_context"> static void __init setup_cpu_entry_area(</span>
 	BUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);
 	BUILD_BUG_ON(sizeof(exception_stacks) !=
 		     sizeof(((struct cpu_entry_area *)0)-&gt;exception_stacks));
<span class="p_del">-	set_percpu_fixmap_pages(get_cpu_entry_area_index(cpu, exception_stacks),</span>
<span class="p_del">-				&amp;per_cpu(exception_stacks, cpu),</span>
<span class="p_del">-				sizeof(exception_stacks) / PAGE_SIZE,</span>
<span class="p_del">-				PAGE_KERNEL);</span>
<span class="p_add">+	cea_map_percpu_pages(&amp;get_cpu_entry_area(cpu)-&gt;exception_stacks,</span>
<span class="p_add">+			     &amp;per_cpu(exception_stacks, cpu),</span>
<span class="p_add">+			     sizeof(exception_stacks) / PAGE_SIZE, PAGE_KERNEL);</span>
 
<span class="p_del">-	__set_fixmap(get_cpu_entry_area_index(cpu, entry_trampoline),</span>
<span class="p_add">+	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;entry_trampoline,</span>
 		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
 #endif
 }
 
<span class="p_add">+static __init void setup_cpu_entry_area_ptes(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	unsigned long start, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(CPU_ENTRY_AREA_PAGES * PAGE_SIZE &lt; CPU_ENTRY_AREA_MAP_SIZE);</span>
<span class="p_add">+	BUG_ON(CPU_ENTRY_AREA_BASE &amp; ~PMD_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	start = CPU_ENTRY_AREA_BASE;</span>
<span class="p_add">+	end = start + CPU_ENTRY_AREA_MAP_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; start &lt; end; start += PMD_SIZE)</span>
<span class="p_add">+		populate_extra_pte(start);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init setup_cpu_entry_areas(void)
 {
 	unsigned int cpu;
 
<span class="p_add">+	setup_cpu_entry_area_ptes();</span>
<span class="p_add">+</span>
 	for_each_possible_cpu(cpu)
 		setup_cpu_entry_area(cpu);
 }
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -58,6 +58,7 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
 #ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
 #endif
<span class="p_chunk">@@ -81,6 +82,7 @@</span> <span class="p_context"> static struct addr_marker address_marker</span>
 	[KASAN_SHADOW_START_NR]	= { KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },
 	[KASAN_SHADOW_END_NR]	= { KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },
 #endif
<span class="p_add">+	[CPU_ENTRY_AREA_NR]	= { CPU_ENTRY_AREA_BASE,&quot;CPU entry Area&quot; },</span>
 #ifdef CONFIG_X86_ESPFIX64
 	[ESPFIX_START_NR]	= { ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },
 #endif
<span class="p_chunk">@@ -104,6 +106,7 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 #ifdef CONFIG_HIGHMEM
 	PKMAP_BASE_NR,
 #endif
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
 	FIXADDR_START_NR,
 	END_OF_SPACE_NR,
 };
<span class="p_chunk">@@ -116,6 +119,7 @@</span> <span class="p_context"> static struct addr_marker address_marker</span>
 #ifdef CONFIG_HIGHMEM
 	[PKMAP_BASE_NR]		= { 0UL,		&quot;Persistent kmap() Area&quot; },
 #endif
<span class="p_add">+	[CPU_ENTRY_AREA_NR]	= { 0UL,		&quot;CPU entry area&quot; },</span>
 	[FIXADDR_START_NR]	= { 0UL,		&quot;Fixmap area&quot; },
 	[END_OF_SPACE_NR]	= { -1,			NULL }
 };
<span class="p_chunk">@@ -541,8 +545,8 @@</span> <span class="p_context"> static int __init pt_dump_init(void)</span>
 	address_markers[PKMAP_BASE_NR].start_address = PKMAP_BASE;
 # endif
 	address_markers[FIXADDR_START_NR].start_address = FIXADDR_START;
<span class="p_add">+	address_markers[CPU_ENTRY_AREA_NR].start_address = CPU_ENTRY_AREA_BASE;</span>
 #endif
<span class="p_del">-</span>
 	return 0;
 }
 __initcall(pt_dump_init);
<span class="p_header">--- a/arch/x86/mm/init_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_32.c</span>
<span class="p_chunk">@@ -50,6 +50,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/set_memory.h&gt;
 #include &lt;asm/page_types.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/init.h&gt;
 
 #include &quot;mm_internal.h&quot;
<span class="p_chunk">@@ -766,6 +767,7 @@</span> <span class="p_context"> void __init mem_init(void)</span>
 	mem_init_print_info(NULL);
 	printk(KERN_INFO &quot;virtual kernel memory layout:\n&quot;
 		&quot;    fixmap  : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;
<span class="p_add">+		&quot;  cpu_entry : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;</span>
 #ifdef CONFIG_HIGHMEM
 		&quot;    pkmap   : 0x%08lx - 0x%08lx   (%4ld kB)\n&quot;
 #endif
<span class="p_chunk">@@ -777,6 +779,10 @@</span> <span class="p_context"> void __init mem_init(void)</span>
 		FIXADDR_START, FIXADDR_TOP,
 		(FIXADDR_TOP - FIXADDR_START) &gt;&gt; 10,
 
<span class="p_add">+		CPU_ENTRY_AREA_BASE,</span>
<span class="p_add">+		CPU_ENTRY_AREA_BASE + CPU_ENTRY_AREA_MAP_SIZE,</span>
<span class="p_add">+		CPU_ENTRY_AREA_MAP_SIZE &gt;&gt; 10,</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HIGHMEM
 		PKMAP_BASE, PKMAP_BASE+LAST_PKMAP*PAGE_SIZE,
 		(LAST_PKMAP*PAGE_SIZE) &gt;&gt; 10,
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/sections.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 
 extern struct range pfn_mapped[E820_MAX_ENTRIES];
 
<span class="p_chunk">@@ -322,31 +323,33 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 		map_range(&amp;pfn_mapped[i]);
 	}
 
<span class="p_del">-	kasan_populate_zero_shadow(</span>
<span class="p_del">-		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),</span>
<span class="p_del">-		kasan_mem_to_shadow((void *)__START_KERNEL_map));</span>
<span class="p_del">-</span>
<span class="p_del">-	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_del">-			      (unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_del">-			      early_pfn_to_nid(__pa(_stext)));</span>
<span class="p_del">-</span>
<span class="p_del">-	shadow_cpu_entry_begin = (void *)__fix_to_virt(FIX_CPU_ENTRY_AREA_BOTTOM);</span>
<span class="p_add">+	shadow_cpu_entry_begin = (void *)CPU_ENTRY_AREA_BASE;</span>
 	shadow_cpu_entry_begin = kasan_mem_to_shadow(shadow_cpu_entry_begin);
 	shadow_cpu_entry_begin = (void *)round_down((unsigned long)shadow_cpu_entry_begin,
 						PAGE_SIZE);
 
<span class="p_del">-	shadow_cpu_entry_end = (void *)(__fix_to_virt(FIX_CPU_ENTRY_AREA_TOP) + PAGE_SIZE);</span>
<span class="p_add">+	shadow_cpu_entry_end = (void *)(CPU_ENTRY_AREA_BASE +</span>
<span class="p_add">+					CPU_ENTRY_AREA_MAP_SIZE);</span>
 	shadow_cpu_entry_end = kasan_mem_to_shadow(shadow_cpu_entry_end);
 	shadow_cpu_entry_end = (void *)round_up((unsigned long)shadow_cpu_entry_end,
 					PAGE_SIZE);
 
<span class="p_del">-	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),</span>
<span class="p_del">-				   shadow_cpu_entry_begin);</span>
<span class="p_add">+	kasan_populate_zero_shadow(</span>
<span class="p_add">+		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),</span>
<span class="p_add">+		shadow_cpu_entry_begin);</span>
 
 	kasan_populate_shadow((unsigned long)shadow_cpu_entry_begin,
 			      (unsigned long)shadow_cpu_entry_end, 0);
 
<span class="p_del">-	kasan_populate_zero_shadow(shadow_cpu_entry_end, (void *)KASAN_SHADOW_END);</span>
<span class="p_add">+	kasan_populate_zero_shadow(shadow_cpu_entry_end,</span>
<span class="p_add">+				kasan_mem_to_shadow((void *)__START_KERNEL_map));</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),</span>
<span class="p_add">+			      (unsigned long)kasan_mem_to_shadow(_end),</span>
<span class="p_add">+			      early_pfn_to_nid(__pa(_stext)));</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),</span>
<span class="p_add">+				(void *)KASAN_SHADOW_END);</span>
 
 	load_cr3(init_top_pgt);
 	__flush_tlb_all();
<span class="p_header">--- a/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -10,6 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/pagemap.h&gt;
 #include &lt;linux/spinlock.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/fixmap.h&gt;
<span class="p_header">--- a/arch/x86/xen/mmu_pv.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu_pv.c</span>
<span class="p_chunk">@@ -2261,7 +2261,6 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx,</span>
 
 	switch (idx) {
 	case FIX_BTMAP_END ... FIX_BTMAP_BEGIN:
<span class="p_del">-	case FIX_RO_IDT:</span>
 #ifdef CONFIG_X86_32
 	case FIX_WP_TEST:
 # ifdef CONFIG_HIGHMEM
<span class="p_chunk">@@ -2272,7 +2271,6 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx,</span>
 #endif
 	case FIX_TEXT_POKE0:
 	case FIX_TEXT_POKE1:
<span class="p_del">-	case FIX_CPU_ENTRY_AREA_TOP ... FIX_CPU_ENTRY_AREA_BOTTOM:</span>
 		/* All local page mappings */
 		pte = pfn_pte(phys, prot);
 		break;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



