
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv5,REBASED,9/9] x86/mm: Allow to have userspace mappings above 47-bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv5,REBASED,9/9] x86/mm: Allow to have userspace mappings above 47-bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 15, 2017, 12:12 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170515121218.27610-10-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9726829/mbox/"
   >mbox</a>
|
   <a href="/patch/9726829/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9726829/">/patch/9726829/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7D1D96028A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 May 2017 12:12:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6E1522865E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 May 2017 12:12:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 60D6C28987; Mon, 15 May 2017 12:12:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 104102865E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 15 May 2017 12:12:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758652AbdEOMMm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 15 May 2017 08:12:42 -0400
Received: from mga04.intel.com ([192.55.52.120]:24600 &quot;EHLO mga04.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1758484AbdEOMMe (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 15 May 2017 08:12:34 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by fmsmga104.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	15 May 2017 05:12:33 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.38,344,1491289200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1147919127&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga001.fm.intel.com with ESMTP; 15 May 2017 05:12:30 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id C714141F; Mon, 15 May 2017 15:12:25 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: x86@kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	linux-api@vger.kernel.org
Subject: [PATCHv5,
	REBASED 9/9] x86/mm: Allow to have userspace mappings above 47-bits
Date: Mon, 15 May 2017 15:12:18 +0300
Message-Id: &lt;20170515121218.27610-10-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170515121218.27610-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170515121218.27610-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - May 15, 2017, 12:12 p.m.</div>
<pre class="content">
On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

To mitigate this, we are not going to allocate virtual address space
above 47-bit by default.

But userspace can ask for allocation from full address space by
specifying hint address (with or without MAP_FIXED) above 47-bits.

If hint address set above 47-bit, but MAP_FIXED is not specified, we try
to look for unmapped area by specified address. If it&#39;s already
occupied, we look for unmapped area in *full* address space, rather than
from 47-bit window.

A high hint address would only affect the allocation in question, but not
any future mmap()s.

Specifying high hint address on older kernel or on machine without 5-level
paging support is safe. The hint will be ignored and kernel will fall back
to allocation from 47-bit address space.

This approach helps to easily make application&#39;s memory allocator aware
about large address space without manually tracking allocated virtual
address space.

One important case we need to handle here is interaction with MPX.
MPX (without MAWA( extension cannot handle addresses above 47-bit, so we
need to make sure that MPX cannot be enabled we already have VMA above
the boundary and forbid creating such VMAs once MPX is enabled.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;</span>
Cc: linux-api@vger.kernel.org
---
 arch/x86/include/asm/elf.h       |  4 ++--
 arch/x86/include/asm/mpx.h       |  9 +++++++++
 arch/x86/include/asm/processor.h | 11 ++++++++---
 arch/x86/kernel/sys_x86_64.c     | 30 ++++++++++++++++++++++++++----
 arch/x86/mm/hugetlbpage.c        | 27 +++++++++++++++++++++++----
 arch/x86/mm/mmap.c               |  6 +++---
 arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-
 7 files changed, 103 insertions(+), 17 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - May 15, 2017, 2:49 p.m.</div>
<pre class="content">
Hi Kirill,

[auto build test ERROR on linus/master]
[also build test ERROR on v4.12-rc1 next-20170515]
[cannot apply to tip/x86/core xen-tip/linux-next]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Kirill-A-Shutemov/x86-5-level-paging-enabling-for-v4-12-Part-4/20170515-202736
config: i386-defconfig (attached as .config)
compiler: gcc-6 (Debian 6.2.0-3) 6.2.0 20160901
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All error/warnings (new ones prefixed by &gt;&gt;):

   In file included from include/linux/cache.h:4:0,
                    from include/linux/printk.h:8,
                    from include/linux/kernel.h:13,
                    from mm/mmap.c:11:
   mm/mmap.c: In function &#39;arch_get_unmapped_area_topdown&#39;:
   arch/x86/include/asm/processor.h:878:50: error: &#39;TASK_SIZE_LOW&#39; undeclared (first use in this function)
    #define TASK_UNMAPPED_BASE  __TASK_UNMAPPED_BASE(TASK_SIZE_LOW)
                                                     ^
   include/uapi/linux/kernel.h:10:41: note: in definition of macro &#39;__ALIGN_KERNEL_MASK&#39;
    #define __ALIGN_KERNEL_MASK(x, mask) (((x) + (mask)) &amp; ~(mask))
                                            ^
   include/linux/kernel.h:49:22: note: in expansion of macro &#39;__ALIGN_KERNEL&#39;
    #define ALIGN(x, a)  __ALIGN_KERNEL((x), (a))
                         ^~~~~~~~~~~~~~
   include/linux/mm.h:132:26: note: in expansion of macro &#39;ALIGN&#39;
    #define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)
                             ^~~~~
   arch/x86/include/asm/processor.h:877:42: note: in expansion of macro &#39;PAGE_ALIGN&#39;
    #define __TASK_UNMAPPED_BASE(task_size) (PAGE_ALIGN(task_size / 3))
                                             ^~~~~~~~~~
   arch/x86/include/asm/processor.h:878:29: note: in expansion of macro &#39;__TASK_UNMAPPED_BASE&#39;
    #define TASK_UNMAPPED_BASE  __TASK_UNMAPPED_BASE(TASK_SIZE_LOW)
                                ^~~~~~~~~~~~~~~~~~~~
<span class="quote">&gt;&gt; mm/mmap.c:2043:20: note: in expansion of macro &#39;TASK_UNMAPPED_BASE&#39;</span>
      info.low_limit = TASK_UNMAPPED_BASE;
                       ^~~~~~~~~~~~~~~~~~
   arch/x86/include/asm/processor.h:878:50: note: each undeclared identifier is reported only once for each function it appears in
    #define TASK_UNMAPPED_BASE  __TASK_UNMAPPED_BASE(TASK_SIZE_LOW)
                                                     ^
   include/uapi/linux/kernel.h:10:41: note: in definition of macro &#39;__ALIGN_KERNEL_MASK&#39;
    #define __ALIGN_KERNEL_MASK(x, mask) (((x) + (mask)) &amp; ~(mask))
                                            ^
   include/linux/kernel.h:49:22: note: in expansion of macro &#39;__ALIGN_KERNEL&#39;
    #define ALIGN(x, a)  __ALIGN_KERNEL((x), (a))
                         ^~~~~~~~~~~~~~
   include/linux/mm.h:132:26: note: in expansion of macro &#39;ALIGN&#39;
    #define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)
                             ^~~~~
   arch/x86/include/asm/processor.h:877:42: note: in expansion of macro &#39;PAGE_ALIGN&#39;
    #define __TASK_UNMAPPED_BASE(task_size) (PAGE_ALIGN(task_size / 3))
                                             ^~~~~~~~~~
   arch/x86/include/asm/processor.h:878:29: note: in expansion of macro &#39;__TASK_UNMAPPED_BASE&#39;
    #define TASK_UNMAPPED_BASE  __TASK_UNMAPPED_BASE(TASK_SIZE_LOW)
                                ^~~~~~~~~~~~~~~~~~~~
<span class="quote">&gt;&gt; mm/mmap.c:2043:20: note: in expansion of macro &#39;TASK_UNMAPPED_BASE&#39;</span>
      info.low_limit = TASK_UNMAPPED_BASE;
                       ^~~~~~~~~~~~~~~~~~
--
   In file included from include/linux/elf.h:4:0,
                    from include/linux/module.h:15,
                    from fs/binfmt_elf.c:12:
   fs/binfmt_elf.c: In function &#39;load_elf_binary&#39;:
<span class="quote">&gt;&gt; arch/x86/include/asm/elf.h:253:27: error: &#39;TASK_SIZE_LOW&#39; undeclared (first use in this function)</span>
    #define ELF_ET_DYN_BASE  (TASK_SIZE_LOW / 3 * 2)
                              ^
<span class="quote">&gt;&gt; fs/binfmt_elf.c:937:16: note: in expansion of macro &#39;ELF_ET_DYN_BASE&#39;</span>
       load_bias = ELF_ET_DYN_BASE - vaddr;
                   ^~~~~~~~~~~~~~~
   arch/x86/include/asm/elf.h:253:27: note: each undeclared identifier is reported only once for each function it appears in
    #define ELF_ET_DYN_BASE  (TASK_SIZE_LOW / 3 * 2)
                              ^
<span class="quote">&gt;&gt; fs/binfmt_elf.c:937:16: note: in expansion of macro &#39;ELF_ET_DYN_BASE&#39;</span>
       load_bias = ELF_ET_DYN_BASE - vaddr;
                   ^~~~~~~~~~~~~~~

vim +/TASK_SIZE_LOW +253 arch/x86/include/asm/elf.h

   247	
   248	/* This is the location that an ET_DYN program is loaded if exec&#39;ed.  Typical
   249	   use of this is to invoke &quot;./ld.so someprog&quot; to test out a new version of
   250	   the loader.  We need to make sure that it is out of the way of the program
   251	   that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
   252	
<span class="quote"> &gt; 253	#define ELF_ET_DYN_BASE		(TASK_SIZE_LOW / 3 * 2)</span>
   254	
   255	/* This yields a mask that user programs can use to figure out what
   256	   instruction set this CPU supports.  This could be done in user space,

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - May 18, 2017, 11:43 a.m.</div>
<pre class="content">
On Mon 15-05-17 15:12:18, Kirill A. Shutemov wrote:
[...]
<span class="quote">&gt; @@ -195,6 +207,16 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>

I have two questions/concerns here. The above assumes that any address above
1&lt;&lt;47 will use the _whole_ address space. Is this what we want? What
if somebody does mmap(1&lt;&lt;52, ...) because he wants to (ab)use 53+ bits
for some other purpose? Shouldn&#39;t we cap the high_limit by the given
address?

Another thing would be that 
	/* requesting a specific address */
	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma(mm, addr);
		if (TASK_SIZE - len &gt;= addr &amp;&amp;
				(!vma || addr + len &lt;= vma-&gt;vm_start))
			return addr;
	}

would fail for mmap(-1UL, ...) which is good because we do want to
fallback to vm_unmapped_area and have randomized address which is
ensured by your info.high_limit += ... but that wouldn&#39;t work for
mmap(1&lt;&lt;N, ...) where N&gt;47. So the first such mapping won&#39;t be
randomized while others will be. This is quite unexpected I would say.
So it should be documented at least or maybe we want to skip the above
shortcut for addr &gt; DEFAULT_MAP_WINDOW altogether.

The patch looks sensible other than that.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - May 18, 2017, 3:19 p.m.</div>
<pre class="content">
On Thu, May 18, 2017 at 01:43:59PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Mon 15-05-17 15:12:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; @@ -195,6 +207,16 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; &gt; +		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	info.align_mask = 0;</span>
<span class="quote">&gt; &gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt;  	if (filp) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have two questions/concerns here. The above assumes that any address above</span>
<span class="quote">&gt; 1&lt;&lt;47 will use the _whole_ address space. Is this what we want?</span>

Yes, I believe so.
<span class="quote">
&gt; What if somebody does mmap(1&lt;&lt;52, ...) because he wants to (ab)use 53+</span>
<span class="quote">&gt; bits for some other purpose? Shouldn&#39;t we cap the high_limit by the</span>
<span class="quote">&gt; given address?</span>

This would screw existing semantics of hint address -- &quot;map here if
free, please&quot;.
<span class="quote">
&gt; Another thing would be that </span>
<span class="quote">&gt; 	/* requesting a specific address */</span>
<span class="quote">&gt; 	if (addr) {</span>
<span class="quote">&gt; 		addr = PAGE_ALIGN(addr);</span>
<span class="quote">&gt; 		vma = find_vma(mm, addr);</span>
<span class="quote">&gt; 		if (TASK_SIZE - len &gt;= addr &amp;&amp;</span>
<span class="quote">&gt; 				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="quote">&gt; 			return addr;</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; would fail for mmap(-1UL, ...) which is good because we do want to</span>
<span class="quote">&gt; fallback to vm_unmapped_area and have randomized address which is</span>
<span class="quote">&gt; ensured by your info.high_limit += ... but that wouldn&#39;t work for</span>
<span class="quote">&gt; mmap(1&lt;&lt;N, ...) where N&gt;47. So the first such mapping won&#39;t be</span>
<span class="quote">&gt; randomized while others will be. This is quite unexpected I would say.</span>
<span class="quote">&gt; So it should be documented at least or maybe we want to skip the above</span>
<span class="quote">&gt; shortcut for addr &gt; DEFAULT_MAP_WINDOW altogether.</span>

Again, you&#39;re missing existing semantics of hint address. You may have a
reason to set hint address above 47-bit, besides getting access to full
address space.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - May 18, 2017, 3:27 p.m.</div>
<pre class="content">
On Thu 18-05-17 18:19:52, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, May 18, 2017 at 01:43:59PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 15-05-17 15:12:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; @@ -195,6 +207,16 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt; &gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; +	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; &gt; &gt; +		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt;  	info.align_mask = 0;</span>
<span class="quote">&gt; &gt; &gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; &gt;  	if (filp) {</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I have two questions/concerns here. The above assumes that any address above</span>
<span class="quote">&gt; &gt; 1&lt;&lt;47 will use the _whole_ address space. Is this what we want?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, I believe so.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; What if somebody does mmap(1&lt;&lt;52, ...) because he wants to (ab)use 53+</span>
<span class="quote">&gt; &gt; bits for some other purpose? Shouldn&#39;t we cap the high_limit by the</span>
<span class="quote">&gt; &gt; given address?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This would screw existing semantics of hint address -- &quot;map here if</span>
<span class="quote">&gt; free, please&quot;.</span>

Well, the given address is just _hint_. We are still allowed to map to a
different place. And it is not specified whether the resulting mapping
is above or below that address. So I do not think it would screw the
existing semantic. Or do I miss something?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - May 18, 2017, 3:41 p.m.</div>
<pre class="content">
On Thu, May 18, 2017 at 05:27:36PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Thu 18-05-17 18:19:52, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Thu, May 18, 2017 at 01:43:59PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Mon 15-05-17 15:12:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -195,6 +207,16 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; &gt; &gt; &gt; +		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.align_mask = 0;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	if (filp) {</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I have two questions/concerns here. The above assumes that any address above</span>
<span class="quote">&gt; &gt; &gt; 1&lt;&lt;47 will use the _whole_ address space. Is this what we want?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes, I believe so.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; What if somebody does mmap(1&lt;&lt;52, ...) because he wants to (ab)use 53+</span>
<span class="quote">&gt; &gt; &gt; bits for some other purpose? Shouldn&#39;t we cap the high_limit by the</span>
<span class="quote">&gt; &gt; &gt; given address?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This would screw existing semantics of hint address -- &quot;map here if</span>
<span class="quote">&gt; &gt; free, please&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, the given address is just _hint_. We are still allowed to map to a</span>
<span class="quote">&gt; different place. And it is not specified whether the resulting mapping</span>
<span class="quote">&gt; is above or below that address. So I do not think it would screw the</span>
<span class="quote">&gt; existing semantic. Or do I miss something?</span>

You are right, that this behaviour is not fixed by any standard or written
down in documentation, but it&#39;s de-facto policy of Linux mmap(2) the
beginning.

And we need to be very careful when messing with this.

I believe that qemu linux-user to some extend relies on this behaviour to
do 32-bit allocations on 64-bit machine.

https://github.com/qemu/qemu/blob/master/linux-user/mmap.c#L256
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - May 18, 2017, 3:50 p.m.</div>
<pre class="content">
On Thu 18-05-17 18:41:35, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, May 18, 2017 at 05:27:36PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Thu 18-05-17 18:19:52, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, May 18, 2017 at 01:43:59PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Mon 15-05-17 15:12:18, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -195,6 +207,16 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	info.align_mask = 0;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	if (filp) {</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I have two questions/concerns here. The above assumes that any address above</span>
<span class="quote">&gt; &gt; &gt; &gt; 1&lt;&lt;47 will use the _whole_ address space. Is this what we want?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Yes, I believe so.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; What if somebody does mmap(1&lt;&lt;52, ...) because he wants to (ab)use 53+</span>
<span class="quote">&gt; &gt; &gt; &gt; bits for some other purpose? Shouldn&#39;t we cap the high_limit by the</span>
<span class="quote">&gt; &gt; &gt; &gt; given address?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This would screw existing semantics of hint address -- &quot;map here if</span>
<span class="quote">&gt; &gt; &gt; free, please&quot;.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Well, the given address is just _hint_. We are still allowed to map to a</span>
<span class="quote">&gt; &gt; different place. And it is not specified whether the resulting mapping</span>
<span class="quote">&gt; &gt; is above or below that address. So I do not think it would screw the</span>
<span class="quote">&gt; &gt; existing semantic. Or do I miss something?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You are right, that this behaviour is not fixed by any standard or written</span>
<span class="quote">&gt; down in documentation, but it&#39;s de-facto policy of Linux mmap(2) the</span>
<span class="quote">&gt; beginning.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And we need to be very careful when messing with this.</span>

I am sorry but I still do not understand. You already touch this
semantic. mmap(-1UL,...) will already returns basically arbitrary
address. All I am asking for is that mmap doesn&#39;t return higher address
than the given one whent address &gt; 1&lt;&lt;47. We do not have any such users
currently so it won&#39;t be a change in behavior while it would allow
different sized address spaces naturally.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index e8ab9a46bc68..7a30513a4046 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(TASK_SIZE_LOW / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_chunk">@@ -304,7 +304,7 @@</span> <span class="p_context"> static inline int mmap_is_ia32(void)</span>
 }
 
 extern unsigned long tasksize_32bit(void);
<span class="p_del">-extern unsigned long tasksize_64bit(void);</span>
<span class="p_add">+extern unsigned long tasksize_64bit(int full_addr_space);</span>
 extern unsigned long get_mmap_base(int is_legacy);
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="p_header">index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mpx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mpx.h</span>
<span class="p_chunk">@@ -73,6 +73,9 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 }
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
 		      unsigned long start, unsigned long end);
<span class="p_add">+</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags);</span>
 #else
 static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)
 {
<span class="p_chunk">@@ -94,6 +97,12 @@</span> <span class="p_context"> static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
 				    unsigned long start, unsigned long end)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="p_add">+		unsigned long len, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_X86_INTEL_MPX */
 
 #endif /* _ASM_X86_MPX_H */
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 3cada998a402..aaed58b03ddb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -795,6 +795,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
<span class="p_add">+#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
<span class="p_chunk">@@ -834,7 +835,9 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
<span class="p_del">-#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="p_add">+#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFAULT_MAP_WINDOW	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
<span class="p_chunk">@@ -842,12 +845,14 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define IA32_PAGE_OFFSET	((current-&gt;personality &amp; ADDR_LIMIT_3GB) ? \
 					0xc0000000 : 0xFFFFe000)
 
<span class="p_add">+#define TASK_SIZE_LOW		(test_thread_flag(TIF_ADDR32) ? \</span>
<span class="p_add">+					IA32_PAGE_OFFSET : DEFAULT_MAP_WINDOW)</span>
 #define TASK_SIZE		(test_thread_flag(TIF_ADDR32) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		TASK_SIZE_LOW</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -870,7 +875,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * space during mmap&#39;s.
  */
 #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
<span class="p_del">-#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE_LOW)</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 207b8f2582c7..74d1587b181d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/compat.h&gt;
 #include &lt;asm/ia32.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 /*
  * Align a virtual address to avoid aliasing in the I$ on AMD F15h.
<span class="p_chunk">@@ -100,8 +101,8 @@</span> <span class="p_context"> SYSCALL_DEFINE6(mmap, unsigned long, addr, unsigned long, len,</span>
 	return error;
 }
 
<span class="p_del">-static void find_start_end(unsigned long flags, unsigned long *begin,</span>
<span class="p_del">-			   unsigned long *end)</span>
<span class="p_add">+static void find_start_end(unsigned long addr, unsigned long flags,</span>
<span class="p_add">+		unsigned long *begin, unsigned long *end)</span>
 {
 	if (!in_compat_syscall() &amp;&amp; (flags &amp; MAP_32BIT)) {
 		/* This is usually used needed to map code in small
<span class="p_chunk">@@ -120,7 +121,10 @@</span> <span class="p_context"> static void find_start_end(unsigned long flags, unsigned long *begin,</span>
 	}
 
 	*begin	= get_mmap_base(1);
<span class="p_del">-	*end	= in_compat_syscall() ? tasksize_32bit() : tasksize_64bit();</span>
<span class="p_add">+	if (in_compat_syscall())</span>
<span class="p_add">+		*end = tasksize_32bit();</span>
<span class="p_add">+	else</span>
<span class="p_add">+		*end = tasksize_64bit(addr &gt; DEFAULT_MAP_WINDOW);</span>
 }
 
 unsigned long
<span class="p_chunk">@@ -132,10 +136,14 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	struct vm_unmapped_area_info info;
 	unsigned long begin, end;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (flags &amp; MAP_FIXED)
 		return addr;
 
<span class="p_del">-	find_start_end(flags, &amp;begin, &amp;end);</span>
<span class="p_add">+	find_start_end(addr, flags, &amp;begin, &amp;end);</span>
 
 	if (len &gt; end)
 		return -ENOMEM;
<span class="p_chunk">@@ -171,6 +179,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
<span class="p_chunk">@@ -195,6 +207,16 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * !in_compat_syscall() check to avoid high addresses for x32.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 302f43fd9c28..730f00250acb 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/elf.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 #if 0	/* This is just for testing */
 struct page *
<span class="p_chunk">@@ -85,25 +86,38 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = get_mmap_base(1);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
 	info.high_limit = in_compat_syscall() ?
<span class="p_del">-		tasksize_32bit() : tasksize_64bit();</span>
<span class="p_add">+		tasksize_32bit() : tasksize_64bit(addr &gt; DEFAULT_MAP_WINDOW);</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
 }
 
 static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,
<span class="p_del">-		unsigned long addr0, unsigned long len,</span>
<span class="p_add">+		unsigned long addr, unsigned long len,</span>
 		unsigned long pgoff, unsigned long flags)
 {
 	struct hstate *h = hstate_file(file);
 	struct vm_unmapped_area_info info;
<span class="p_del">-	unsigned long addr;</span>
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -118,7 +132,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = TASK_SIZE_LOW;</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -135,6 +149,11 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_add">+</span>
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index 19ad095b41df..199050249d60 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -42,9 +42,9 @@</span> <span class="p_context"> unsigned long tasksize_32bit(void)</span>
 	return IA32_PAGE_OFFSET;
 }
 
<span class="p_del">-unsigned long tasksize_64bit(void)</span>
<span class="p_add">+unsigned long tasksize_64bit(int full_addr_space)</span>
 {
<span class="p_del">-	return TASK_SIZE_MAX;</span>
<span class="p_add">+	return full_addr_space ? TASK_SIZE_MAX : DEFAULT_MAP_WINDOW;</span>
 }
 
 static unsigned long stack_maxrandom_size(unsigned long task_size)
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> void arch_pick_mmap_layout(struct mm_struct *mm)</span>
 		mm-&gt;get_unmapped_area = arch_get_unmapped_area_topdown;
 
 	arch_pick_mmap_base(&amp;mm-&gt;mmap_base, &amp;mm-&gt;mmap_legacy_base,
<span class="p_del">-			arch_rnd(mmap64_rnd_bits), tasksize_64bit());</span>
<span class="p_add">+			arch_rnd(mmap64_rnd_bits), tasksize_64bit(0));</span>
 
 #ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES
 	/*
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index 1c34b767c84c..8c8da27e8549 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -355,10 +355,19 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 */
 	bd_base = mpx_get_bounds_dir();
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+</span>
<span class="p_add">+	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="p_add">+	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="p_add">+		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="p_add">+				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="p_add">+				current-&gt;comm, current-&gt;pid);</span>
<span class="p_add">+		ret = -ENXIO;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
<span class="p_del">-</span>
<span class="p_add">+out:</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -1030,3 +1039,25 @@</span> <span class="p_context"> void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (ret)
 		force_sig(SIGSEGV, current);
 }
<span class="p_add">+</span>
<span class="p_add">+/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (flags &amp; MAP_FIXED)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="p_add">+	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



