
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>x86: static_cpu_has_safe: discard dynamic check after init - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    x86: static_cpu_has_safe: discard dynamic check after init</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 19, 2016, 1:57 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160119135714.GD15071@pd.tnic&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8062781/mbox/"
   >mbox</a>
|
   <a href="/patch/8062781/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8062781/">/patch/8062781/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id B00D49F1CC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 19 Jan 2016 13:57:57 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 428ED2039C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 19 Jan 2016 13:57:56 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A2E40201F4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 19 Jan 2016 13:57:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755189AbcASN5v (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 19 Jan 2016 08:57:51 -0500
Received: from mx2.suse.de ([195.135.220.15]:55736 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751808AbcASN5m (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 19 Jan 2016 08:57:42 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 4FE43ABE5;
	Tue, 19 Jan 2016 13:57:37 +0000 (UTC)
Received: by pd.tnic (Postfix, from userid 1000)
	id 8E9C51606A3; Tue, 19 Jan 2016 14:57:14 +0100 (CET)
Date: Tue, 19 Jan 2016 14:57:14 +0100
From: Borislav Petkov &lt;bp@suse.de&gt;
To: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Brian Gerst &lt;brgerst@gmail.com&gt;
Cc: the arch/x86 maintainers &lt;x86@kernel.org&gt;,
	Linux Kernel Mailing List &lt;linux-kernel@vger.kernel.org&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, Denys Vlasenko &lt;dvlasenk@redhat.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Subject: Re: [PATCH] x86: static_cpu_has_safe: discard dynamic check after
	init
Message-ID: &lt;20160119135714.GD15071@pd.tnic&gt;
References: &lt;CAMzpN2hHVhjJD57wYnRLPZ_58FQGvSEn511ywGhgXzuSAZXD_A@mail.gmail.com&gt;
	&lt;20160117103337.GC8549@pd.tnic&gt;
	&lt;CAMzpN2i+LELbLaBbxDS0MF1Y0sn_Jm3ztCWATzMyrp4rnhWRQA@mail.gmail.com&gt;
	&lt;20160118181457.GG12651@pd.tnic&gt;
	&lt;CALCETrVABHeHGyhxaATpSGcJX60f2rvVtVFc-5YQKQip-04qoQ@mail.gmail.com&gt;
	&lt;20160118183921.GH12651@pd.tnic&gt; &lt;569D40CE.5090506@zytor.com&gt;
	&lt;20160118230554.GJ12651@pd.tnic&gt;
	&lt;3D4E057B-AB03-4C12-B59D-774E8954C742@zytor.com&gt;
	&lt;20160118232547.GK12651@pd.tnic&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: &lt;20160118232547.GK12651@pd.tnic&gt;
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a> - Jan. 19, 2016, 1:57 p.m.</div>
<pre class="content">
On Tue, Jan 19, 2016 at 12:25:47AM +0100, Borislav Petkov wrote:
<span class="quote">&gt; Luckily, I have this disassembler tool which dumps the alternatives</span>
<span class="quote">&gt; sections in a more readable format. I can dump all the static_cpu_has()</span>
<span class="quote">&gt; call sites tomorrow and we can see what gcc generates.</span>

Ok, below is the list of all 12(!) places where static_cpu_has() has
generated a 2-byte JMP. Which has saved us a whopping 36 bytes! On an
x86_64 allyesconfig!

Patch removing it below. I better look at a 32-bit allyesconfig too
first, though.

old insn VA: 0xffffffff8146b5ac, CPU feat: X86_FEATURE_PCOMMIT, size: 2, padlen: 0
wmb_pmem:
 ffffffff8146b5ac:      eb 02                   jmp ffffffff8146b5b0
repl insn: 0xffffffff8ca7c651, size: 0

old insn VA: 0xffffffff8200dcd6, CPU feat: X86_FEATURE_PCOMMIT, size: 2, padlen: 0
arch_has_wmb_pmem:
 ffffffff8200dcd6:      eb 02                   jmp ffffffff8200dcda
repl insn: 0xffffffff8ca7ebd8, size: 0

old insn VA: 0xffffffff828afe51, CPU feat: X86_FEATURE_PCOMMIT, size: 2, padlen: 0
arch_has_wmb_pmem:
 ffffffff828afe51:      eb 02                   jmp ffffffff828afe55
repl insn: 0xffffffff8ca80f98, size: 0

old insn VA: 0xffffffff81072f77, CPU feat: X86_FEATURE_NRIPS, size: 2, padlen: 0
rdpmc_interception:
 ffffffff81072f77:      eb 4d                   jmp ffffffff81072fc6
repl insn: 0xffffffff8ca79f6b, size: 0

old insn VA: 0xffffffff8107437d, CPU feat: X86_FEATURE_NRIPS, size: 2, padlen: 0
svm_queue_exception:
 ffffffff8107437d:      eb 6f                   jmp ffffffff810743ee
repl insn: 0xffffffff8ca79f9f, size: 0

old insn VA: 0xffffffff8107741b, CPU feat: X86_FEATURE_NRIPS, size: 2, padlen: 0
svm_check_intercept:
 ffffffff8107741b:      eb 67                   jmp ffffffff81077484
repl insn: 0xffffffff8ca79fed, size: 0

old insn VA: 0xffffffff8107741b, CPU feat: X86_FEATURE_NRIPS, size: 2, padlen: 0
svm_check_intercept:
 ffffffff8107741b:      eb 67                   jmp ffffffff81077484
repl insn: 0xffffffff8ca79fed, size: 0

old insn VA: 0xffffffff81075c4f, CPU feat: X86_FEATURE_TSCRATEMSR, size: 2, padlen: 0
svm_hardware_enable:
 ffffffff81075c4f:      eb 57                   jmp ffffffff81075ca8
repl insn: 0xffffffff8ca79fb9, size: 0

old insn VA: 0xffffffff81072c00, CPU feat: X86_FEATURE_DECODEASSISTS, size: 2, padlen: 0
invlpg_interception:
 ffffffff81072c00:      eb 55                   jmp ffffffff81072c57
repl insn: 0xffffffff8ca79f51, size: 0

old insn VA: 0xffffffff8107097c, CPU feat: X86_FEATURE_FLUSHBYASID, size: 2, padlen: 0
svm_flush_tlb:
 ffffffff8107097c:      eb 35                   jmp ffffffff810709b3
repl insn: 0xffffffff8ca79ee9, size: 0

old insn VA: 0xffffffff8108c0d0, CPU feat: X86_BUG_SYSRET_SS_ATTRS, size: 2, padlen: 0
__switch_to:
 ffffffff8108c0d0:      eb 70                   jmp ffffffff8108c142
repl insn: 0xffffffff8ca7a1a7, size: 0

old insn VA: 0xffffffff81075d90, CPU feat: X86_BUG_AMD_TLB_MMATCH, size: 2, padlen: 0
svm_hardware_enable:
 ffffffff81075d90:      eb 7c                   jmp ffffffff81075e0e
repl insn: 0xffffffff8ca79fd3, size: 0

---
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a> - Jan. 19, 2016, 4:23 p.m.</div>
<pre class="content">
On Tue, Jan 19, 2016 at 02:57:14PM +0100, Borislav Petkov wrote:
<span class="quote">&gt; Patch removing it below. I better look at a 32-bit allyesconfig too</span>
<span class="quote">&gt; first, though.</span>

Yap, no 2-byte jumps in the static_cpu_has()-generated code on an 32-bit
allyesconfig build.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - Jan. 20, 2016, 4:03 a.m.</div>
<pre class="content">
On 01/19/16 05:57, Borislav Petkov wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; old insn VA: 0xffffffff8108c0d0, CPU feat: X86_BUG_SYSRET_SS_ATTRS, size: 2, padlen: 0</span>
<span class="quote">&gt; __switch_to:</span>
<span class="quote">&gt;  ffffffff8108c0d0:      eb 70                   jmp ffffffff8108c142</span>
<span class="quote">&gt; repl insn: 0xffffffff8ca7a1a7, size: 0</span>
<span class="quote">&gt; </span>

This is the only one I could possibly imagine mattering.  Would it be
possible to get the disassembly here?

	-hpa
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a> - Jan. 20, 2016, 10:33 a.m.</div>
<pre class="content">
On Tue, Jan 19, 2016 at 08:03:55PM -0800, H. Peter Anvin wrote:
<span class="quote">&gt; On 01/19/16 05:57, Borislav Petkov wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; old insn VA: 0xffffffff8108c0d0, CPU feat: X86_BUG_SYSRET_SS_ATTRS, size: 2, padlen: 0</span>
<span class="quote">&gt; &gt; __switch_to:</span>
<span class="quote">&gt; &gt;  ffffffff8108c0d0:      eb 70                   jmp ffffffff8108c142</span>
<span class="quote">&gt; &gt; repl insn: 0xffffffff8ca7a1a7, size: 0</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is the only one I could possibly imagine mattering.  Would it be</span>
<span class="quote">&gt; possible to get the disassembly here?</span>

Sure, I&#39;ve kept the vmlinux, see below:

ffffffff8108c0d0:       eb 70                   jmp    ffffffff8108c142 &lt;__switch_to+0x838&gt;

jumps to:

ffffffff8108c142:       48 ff 05 b7 08 aa 0b    incq   0xbaa08b7(%rip)

which is something-gcov. In any case, it jumps over the SS fixup code:

        if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {

		...

                unsigned short ss_sel;
                savesegment(ss, ss_sel);
                if (ss_sel != __KERNEL_DS)
                        loadsegment(ss, __KERNEL_DS);
        }

I guess we could fix that with an alternative_call() where @oldinstr is
empty so that on !X86_BUG_SYSRET_SS_ATTRS machines we simply return and
on the others, we do a CALL fixup_ss or so.

We have to pay attention to clobbers, though, similar to the POPCNT fun
in __arch_hweight32().

ffffffff8108b90a &lt;__switch_to&gt;:
ffffffff8108b90a:       55                      push   %rbp
ffffffff8108b90b:       48 8d 87 c0 2d 00 00    lea    0x2dc0(%rdi),%rax
ffffffff8108b912:       48 c7 c1 80 f6 1d 00    mov    $0x1df680,%rcx
ffffffff8108b919:       48 89 e5                mov    %rsp,%rbp
ffffffff8108b91c:       41 57                   push   %r15
ffffffff8108b91e:       45 31 ff                xor    %r15d,%r15d
ffffffff8108b921:       41 56                   push   %r14

...

ffffffff8108c0c2:       48 ff 05 07 09 aa 0b    incq   0xbaa0907(%rip)        # ffffffff8cb2c9d0 &lt;__gcov0.__switch_to+0xc0&gt;
ffffffff8108c0c9:       48 ff 05 08 09 aa 0b    incq   0xbaa0908(%rip)        # ffffffff8cb2c9d8 &lt;__gcov0.__switch_to+0xc8&gt;
ffffffff8108c0d0:       eb 70                   jmp    ffffffff8108c142 &lt;__switch_to+0x838&gt;
ffffffff8108c0d2:       48 ff 05 07 09 aa 0b    incq   0xbaa0907(%rip)        # ffffffff8cb2c9e0 &lt;__gcov0.__switch_to+0xd0&gt;
ffffffff8108c0d9:       48 ff 05 08 09 aa 0b    incq   0xbaa0908(%rip)        # ffffffff8cb2c9e8 &lt;__gcov0.__switch_to+0xd8&gt;
ffffffff8108c0e0:       48 ff 05 11 09 aa 0b    incq   0xbaa0911(%rip)        # ffffffff8cb2c9f8 &lt;__gcov0.__switch_to+0xe8&gt;
ffffffff8108c0e7:       66 8c d0                mov    %ss,%ax
ffffffff8108c0ea:       66 83 f8 18             cmp    $0x18,%ax
ffffffff8108c0ee:       75 17                   jne    ffffffff8108c107 &lt;__switch_to+0x7fd&gt;
ffffffff8108c0f0:       eb 57                   jmp    ffffffff8108c149 &lt;__switch_to+0x83f&gt;
ffffffff8108c0f2:       48 ff 05 cf 08 aa 0b    incq   0xbaa08cf(%rip)        # ffffffff8cb2c9c8 &lt;__gcov0.__switch_to+0xb8&gt;
ffffffff8108c0f9:       e8 db b0 01 00          callq  ffffffff810a71d9 &lt;warn_pre_alternatives&gt;
ffffffff8108c0fe:       48 ff 05 eb 08 aa 0b    incq   0xbaa08eb(%rip)        # ffffffff8cb2c9f0 &lt;__gcov0.__switch_to+0xe0&gt;
ffffffff8108c105:       eb 3b                   jmp    ffffffff8108c142 &lt;__switch_to+0x838&gt;
ffffffff8108c107:       48 ff 05 fa 08 aa 0b    incq   0xbaa08fa(%rip)        # ffffffff8cb2ca08 &lt;__gcov0.__switch_to+0xf8&gt;
ffffffff8108c10e:       b8 18 00 00 00          mov    $0x18,%eax
ffffffff8108c113:       8e d0                   mov    %eax,%ss
ffffffff8108c115:       48 ff 05 f4 08 aa 0b    incq   0xbaa08f4(%rip)        # ffffffff8cb2ca10 &lt;__gcov0.__switch_to+0x100&gt;
ffffffff8108c11c:       eb 2b                   jmp    ffffffff8108c149 &lt;__switch_to+0x83f&gt;
ffffffff8108c11e:       48 ff 05 9b 02 aa 0b    incq   0xbaa029b(%rip)        # ffffffff8cb2c3c0 &lt;__gcov0.copy_xregs_to_kernel+0x30&gt;
ffffffff8108c125:       e9 91 f9 ff ff          jmpq   ffffffff8108babb &lt;__switch_to+0x1b1&gt;
ffffffff8108c12a:       48 ff 05 bf 02 aa 0b    incq   0xbaa02bf(%rip)        # ffffffff8cb2c3f0 &lt;__gcov0.copy_kernel_to_xregs+0x20&gt;
ffffffff8108c131:       e9 84 fe ff ff          jmpq   ffffffff8108bfba &lt;__switch_to+0x6b0&gt;
ffffffff8108c136:       48 ff 05 43 02 aa 0b    incq   0xbaa0243(%rip)        # ffffffff8cb2c380 &lt;__gcov0.copy_kernel_to_fxregs+0x20&gt;
ffffffff8108c13d:       e9 db fe ff ff          jmpq   ffffffff8108c01d &lt;__switch_to+0x713&gt;
ffffffff8108c142:       48 ff 05 b7 08 aa 0b    incq   0xbaa08b7(%rip)        # ffffffff8cb2ca00 &lt;__gcov0.__switch_to+0xf0&gt;
ffffffff8108c149:       48 83 c4 28             add    $0x28,%rsp
ffffffff8108c14d:       4c 89 e0                mov    %r12,%rax
ffffffff8108c150:       5b                      pop    %rbx
ffffffff8108c151:       41 5c                   pop    %r12
ffffffff8108c153:       41 5d                   pop    %r13
ffffffff8108c155:       41 5e                   pop    %r14
ffffffff8108c157:       41 5f                   pop    %r15
ffffffff8108c159:       5d                      pop    %rbp
ffffffff8108c15a:       c3                      retq
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - Jan. 20, 2016, 10:41 a.m.</div>
<pre class="content">
On January 20, 2016 2:33:45 AM PST, Borislav Petkov &lt;bp@suse.de&gt; wrote:
<span class="quote">&gt;On Tue, Jan 19, 2016 at 08:03:55PM -0800, H. Peter Anvin wrote:</span>
<span class="quote">&gt;&gt; On 01/19/16 05:57, Borislav Petkov wrote:</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; old insn VA: 0xffffffff8108c0d0, CPU feat: X86_BUG_SYSRET_SS_ATTRS,</span>
<span class="quote">&gt;size: 2, padlen: 0</span>
<span class="quote">&gt;&gt; &gt; __switch_to:</span>
<span class="quote">&gt;&gt; &gt;  ffffffff8108c0d0:      eb 70                   jmp</span>
<span class="quote">&gt;ffffffff8108c142</span>
<span class="quote">&gt;&gt; &gt; repl insn: 0xffffffff8ca7a1a7, size: 0</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; This is the only one I could possibly imagine mattering.  Would it be</span>
<span class="quote">&gt;&gt; possible to get the disassembly here?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Sure, I&#39;ve kept the vmlinux, see below:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;ffffffff8108c0d0:       eb 70                   jmp    ffffffff8108c142</span>
<span class="quote">&gt;&lt;__switch_to+0x838&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;jumps to:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;ffffffff8108c142:       48 ff 05 b7 08 aa 0b    incq   0xbaa08b7(%rip)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;which is something-gcov. In any case, it jumps over the SS fixup code:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;        if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;		...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                unsigned short ss_sel;</span>
<span class="quote">&gt;                savesegment(ss, ss_sel);</span>
<span class="quote">&gt;                if (ss_sel != __KERNEL_DS)</span>
<span class="quote">&gt;                        loadsegment(ss, __KERNEL_DS);</span>
<span class="quote">&gt;        }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;I guess we could fix that with an alternative_call() where @oldinstr is</span>
<span class="quote">&gt;empty so that on !X86_BUG_SYSRET_SS_ATTRS machines we simply return and</span>
<span class="quote">&gt;on the others, we do a CALL fixup_ss or so.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;We have to pay attention to clobbers, though, similar to the POPCNT fun</span>
<span class="quote">&gt;in __arch_hweight32().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;ffffffff8108b90a &lt;__switch_to&gt;:</span>
<span class="quote">&gt;ffffffff8108b90a:       55                      push   %rbp</span>
<span class="quote">&gt;ffffffff8108b90b:       48 8d 87 c0 2d 00 00    lea   </span>
<span class="quote">&gt;0x2dc0(%rdi),%rax</span>
<span class="quote">&gt;ffffffff8108b912:       48 c7 c1 80 f6 1d 00    mov    $0x1df680,%rcx</span>
<span class="quote">&gt;ffffffff8108b919:       48 89 e5                mov    %rsp,%rbp</span>
<span class="quote">&gt;ffffffff8108b91c:       41 57                   push   %r15</span>
<span class="quote">&gt;ffffffff8108b91e:       45 31 ff                xor    %r15d,%r15d</span>
<span class="quote">&gt;ffffffff8108b921:       41 56                   push   %r14</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;ffffffff8108c0c2:       48 ff 05 07 09 aa 0b    incq   0xbaa0907(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9d0 &lt;__gcov0.__switch_to+0xc0&gt;</span>
<span class="quote">&gt;ffffffff8108c0c9:       48 ff 05 08 09 aa 0b    incq   0xbaa0908(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9d8 &lt;__gcov0.__switch_to+0xc8&gt;</span>
<span class="quote">&gt;ffffffff8108c0d0:       eb 70                   jmp    ffffffff8108c142</span>
<span class="quote">&gt;&lt;__switch_to+0x838&gt;</span>
<span class="quote">&gt;ffffffff8108c0d2:       48 ff 05 07 09 aa 0b    incq   0xbaa0907(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9e0 &lt;__gcov0.__switch_to+0xd0&gt;</span>
<span class="quote">&gt;ffffffff8108c0d9:       48 ff 05 08 09 aa 0b    incq   0xbaa0908(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9e8 &lt;__gcov0.__switch_to+0xd8&gt;</span>
<span class="quote">&gt;ffffffff8108c0e0:       48 ff 05 11 09 aa 0b    incq   0xbaa0911(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9f8 &lt;__gcov0.__switch_to+0xe8&gt;</span>
<span class="quote">&gt;ffffffff8108c0e7:       66 8c d0                mov    %ss,%ax</span>
<span class="quote">&gt;ffffffff8108c0ea:       66 83 f8 18             cmp    $0x18,%ax</span>
<span class="quote">&gt;ffffffff8108c0ee:       75 17                   jne    ffffffff8108c107</span>
<span class="quote">&gt;&lt;__switch_to+0x7fd&gt;</span>
<span class="quote">&gt;ffffffff8108c0f0:       eb 57                   jmp    ffffffff8108c149</span>
<span class="quote">&gt;&lt;__switch_to+0x83f&gt;</span>
<span class="quote">&gt;ffffffff8108c0f2:       48 ff 05 cf 08 aa 0b    incq   0xbaa08cf(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9c8 &lt;__gcov0.__switch_to+0xb8&gt;</span>
<span class="quote">&gt;ffffffff8108c0f9:       e8 db b0 01 00          callq  ffffffff810a71d9</span>
<span class="quote">&gt;&lt;warn_pre_alternatives&gt;</span>
<span class="quote">&gt;ffffffff8108c0fe:       48 ff 05 eb 08 aa 0b    incq   0xbaa08eb(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c9f0 &lt;__gcov0.__switch_to+0xe0&gt;</span>
<span class="quote">&gt;ffffffff8108c105:       eb 3b                   jmp    ffffffff8108c142</span>
<span class="quote">&gt;&lt;__switch_to+0x838&gt;</span>
<span class="quote">&gt;ffffffff8108c107:       48 ff 05 fa 08 aa 0b    incq   0xbaa08fa(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2ca08 &lt;__gcov0.__switch_to+0xf8&gt;</span>
<span class="quote">&gt;ffffffff8108c10e:       b8 18 00 00 00          mov    $0x18,%eax</span>
<span class="quote">&gt;ffffffff8108c113:       8e d0                   mov    %eax,%ss</span>
<span class="quote">&gt;ffffffff8108c115:       48 ff 05 f4 08 aa 0b    incq   0xbaa08f4(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2ca10 &lt;__gcov0.__switch_to+0x100&gt;</span>
<span class="quote">&gt;ffffffff8108c11c:       eb 2b                   jmp    ffffffff8108c149</span>
<span class="quote">&gt;&lt;__switch_to+0x83f&gt;</span>
<span class="quote">&gt;ffffffff8108c11e:       48 ff 05 9b 02 aa 0b    incq   0xbaa029b(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c3c0 &lt;__gcov0.copy_xregs_to_kernel+0x30&gt;</span>
<span class="quote">&gt;ffffffff8108c125:       e9 91 f9 ff ff          jmpq   ffffffff8108babb</span>
<span class="quote">&gt;&lt;__switch_to+0x1b1&gt;</span>
<span class="quote">&gt;ffffffff8108c12a:       48 ff 05 bf 02 aa 0b    incq   0xbaa02bf(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c3f0 &lt;__gcov0.copy_kernel_to_xregs+0x20&gt;</span>
<span class="quote">&gt;ffffffff8108c131:       e9 84 fe ff ff          jmpq   ffffffff8108bfba</span>
<span class="quote">&gt;&lt;__switch_to+0x6b0&gt;</span>
<span class="quote">&gt;ffffffff8108c136:       48 ff 05 43 02 aa 0b    incq   0xbaa0243(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2c380 &lt;__gcov0.copy_kernel_to_fxregs+0x20&gt;</span>
<span class="quote">&gt;ffffffff8108c13d:       e9 db fe ff ff          jmpq   ffffffff8108c01d</span>
<span class="quote">&gt;&lt;__switch_to+0x713&gt;</span>
<span class="quote">&gt;ffffffff8108c142:       48 ff 05 b7 08 aa 0b    incq   0xbaa08b7(%rip) </span>
<span class="quote">&gt;      # ffffffff8cb2ca00 &lt;__gcov0.__switch_to+0xf0&gt;</span>
<span class="quote">&gt;ffffffff8108c149:       48 83 c4 28             add    $0x28,%rsp</span>
<span class="quote">&gt;ffffffff8108c14d:       4c 89 e0                mov    %r12,%rax</span>
<span class="quote">&gt;ffffffff8108c150:       5b                      pop    %rbx</span>
<span class="quote">&gt;ffffffff8108c151:       41 5c                   pop    %r12</span>
<span class="quote">&gt;ffffffff8108c153:       41 5d                   pop    %r13</span>
<span class="quote">&gt;ffffffff8108c155:       41 5e                   pop    %r14</span>
<span class="quote">&gt;ffffffff8108c157:       41 5f                   pop    %r15</span>
<span class="quote">&gt;ffffffff8108c159:       5d                      pop    %rbp</span>
<span class="quote">&gt;ffffffff8108c15a:       c3                      retq</span>

Ah.  What would be even more of a win would be to rebias static_cpu_has_bug() so that the fallthrough case is the functional one.  Easily done by reversing the labels.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="p_header">index 9b18ed9..68a2d1f 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig.debug</span>
<span class="p_header">+++ b/arch/x86/Kconfig.debug</span>
<span class="p_chunk">@@ -350,16 +350,6 @@</span> <span class="p_context"> config DEBUG_IMR_SELFTEST</span>
 
 	  If unsure say N here.
 
<span class="p_del">-config X86_DEBUG_STATIC_CPU_HAS</span>
<span class="p_del">-	bool &quot;Debug alternatives&quot;</span>
<span class="p_del">-	depends on DEBUG_KERNEL</span>
<span class="p_del">-	---help---</span>
<span class="p_del">-	  This option causes additional code to be generated which</span>
<span class="p_del">-	  fails if static_cpu_has() is used before alternatives have</span>
<span class="p_del">-	  run.</span>
<span class="p_del">-</span>
<span class="p_del">-	  If unsure, say N.</span>
<span class="p_del">-</span>
 config X86_DEBUG_FPU
 	bool &quot;Debug the x86 FPU code&quot;
 	depends on DEBUG_KERNEL
<span class="p_header">diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c</span>
<span class="p_header">index 0366374..c2d7a97 100644</span>
<span class="p_header">--- a/arch/x86/entry/common.c</span>
<span class="p_header">+++ b/arch/x86/entry/common.c</span>
<span class="p_chunk">@@ -477,7 +477,7 @@</span> <span class="p_context"> __visible long do_fast_syscall_32(struct pt_regs *regs)</span>
 	 * We don&#39;t allow syscalls at all from VM86 mode, but we still
 	 * need to check VM, because we might be returning from sys_vm86.
 	 */
<span class="p_del">-	return static_cpu_has(X86_FEATURE_SEP) &amp;&amp;</span>
<span class="p_add">+	return static_cpu_has_safe(X86_FEATURE_SEP) &amp;&amp;</span>
 		regs-&gt;cs == __USER_CS &amp;&amp; regs-&gt;ss == __USER_DS &amp;&amp;
 		regs-&gt;ip == landing_pad &amp;&amp;
 		(regs-&gt;flags &amp; (X86_EFLAGS_RF | X86_EFLAGS_TF | X86_EFLAGS_VM)) == 0;
<span class="p_header">diff --git a/arch/x86/include/asm/archrandom.h b/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">index 69f1366..2fb511b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/archrandom.h</span>
<span class="p_chunk">@@ -114,8 +114,8 @@</span> <span class="p_context"> GET_SEED(arch_get_random_seed_int, unsigned int, RDSEED_INT, ASM_NOP4);</span>
 
 #endif /* CONFIG_X86_64 */
 
<span class="p_del">-#define arch_has_random()	static_cpu_has(X86_FEATURE_RDRAND)</span>
<span class="p_del">-#define arch_has_random_seed()	static_cpu_has(X86_FEATURE_RDSEED)</span>
<span class="p_add">+#define arch_has_random()	static_cpu_has_safe(X86_FEATURE_RDRAND)</span>
<span class="p_add">+#define arch_has_random_seed()	static_cpu_has_safe(X86_FEATURE_RDSEED)</span>
 
 #else
 
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index 7ad8c94..5fe399a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -419,89 +419,6 @@</span> <span class="p_context"> extern bool __static_cpu_has_safe(u16 bit);</span>
  * These are only valid after alternatives have run, but will statically
  * patch the target code for additional performance.
  */
<span class="p_del">-static __always_inline __pure bool __static_cpu_has(u16 bit)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifdef CC_HAVE_ASM_GOTO</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_DEBUG_STATIC_CPU_HAS</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Catch too early usage of this before alternatives</span>
<span class="p_del">-		 * have run.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		asm_volatile_goto(&quot;1: jmp %l[t_warn]\n&quot;</span>
<span class="p_del">-			 &quot;2:\n&quot;</span>
<span class="p_del">-			 &quot;.section .altinstructions,\&quot;a\&quot;\n&quot;</span>
<span class="p_del">-			 &quot; .long 1b - .\n&quot;</span>
<span class="p_del">-			 &quot; .long 0\n&quot;		/* no replacement */</span>
<span class="p_del">-			 &quot; .word %P0\n&quot;		/* 1: do replace */</span>
<span class="p_del">-			 &quot; .byte 2b - 1b\n&quot;	/* source len */</span>
<span class="p_del">-			 &quot; .byte 0\n&quot;		/* replacement len */</span>
<span class="p_del">-			 &quot; .byte 0\n&quot;		/* pad len */</span>
<span class="p_del">-			 &quot;.previous\n&quot;</span>
<span class="p_del">-			 /* skipping size check since replacement size = 0 */</span>
<span class="p_del">-			 : : &quot;i&quot; (X86_FEATURE_ALWAYS) : : t_warn);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-		asm_volatile_goto(&quot;1: jmp %l[t_no]\n&quot;</span>
<span class="p_del">-			 &quot;2:\n&quot;</span>
<span class="p_del">-			 &quot;.section .altinstructions,\&quot;a\&quot;\n&quot;</span>
<span class="p_del">-			 &quot; .long 1b - .\n&quot;</span>
<span class="p_del">-			 &quot; .long 0\n&quot;		/* no replacement */</span>
<span class="p_del">-			 &quot; .word %P0\n&quot;		/* feature bit */</span>
<span class="p_del">-			 &quot; .byte 2b - 1b\n&quot;	/* source len */</span>
<span class="p_del">-			 &quot; .byte 0\n&quot;		/* replacement len */</span>
<span class="p_del">-			 &quot; .byte 0\n&quot;		/* pad len */</span>
<span class="p_del">-			 &quot;.previous\n&quot;</span>
<span class="p_del">-			 /* skipping size check since replacement size = 0 */</span>
<span class="p_del">-			 : : &quot;i&quot; (bit) : : t_no);</span>
<span class="p_del">-		return true;</span>
<span class="p_del">-	t_no:</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_DEBUG_STATIC_CPU_HAS</span>
<span class="p_del">-	t_warn:</span>
<span class="p_del">-		warn_pre_alternatives();</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#else /* CC_HAVE_ASM_GOTO */</span>
<span class="p_del">-</span>
<span class="p_del">-		u8 flag;</span>
<span class="p_del">-		/* Open-coded due to __stringify() in ALTERNATIVE() */</span>
<span class="p_del">-		asm volatile(&quot;1: movb $0,%0\n&quot;</span>
<span class="p_del">-			     &quot;2:\n&quot;</span>
<span class="p_del">-			     &quot;.section .altinstructions,\&quot;a\&quot;\n&quot;</span>
<span class="p_del">-			     &quot; .long 1b - .\n&quot;</span>
<span class="p_del">-			     &quot; .long 3f - .\n&quot;</span>
<span class="p_del">-			     &quot; .word %P1\n&quot;		/* feature bit */</span>
<span class="p_del">-			     &quot; .byte 2b - 1b\n&quot;		/* source len */</span>
<span class="p_del">-			     &quot; .byte 4f - 3f\n&quot;		/* replacement len */</span>
<span class="p_del">-			     &quot; .byte 0\n&quot;		/* pad len */</span>
<span class="p_del">-			     &quot;.previous\n&quot;</span>
<span class="p_del">-			     &quot;.section .discard,\&quot;aw\&quot;,@progbits\n&quot;</span>
<span class="p_del">-			     &quot; .byte 0xff + (4f-3f) - (2b-1b)\n&quot; /* size check */</span>
<span class="p_del">-			     &quot;.previous\n&quot;</span>
<span class="p_del">-			     &quot;.section .altinstr_replacement,\&quot;ax\&quot;\n&quot;</span>
<span class="p_del">-			     &quot;3: movb $1,%0\n&quot;</span>
<span class="p_del">-			     &quot;4:\n&quot;</span>
<span class="p_del">-			     &quot;.previous\n&quot;</span>
<span class="p_del">-			     : &quot;=qm&quot; (flag) : &quot;i&quot; (bit));</span>
<span class="p_del">-		return flag;</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CC_HAVE_ASM_GOTO */</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define static_cpu_has(bit)					\</span>
<span class="p_del">-(								\</span>
<span class="p_del">-	__builtin_constant_p(boot_cpu_has(bit)) ?		\</span>
<span class="p_del">-		boot_cpu_has(bit) :				\</span>
<span class="p_del">-	__builtin_constant_p(bit) ?				\</span>
<span class="p_del">-		__static_cpu_has(bit) :				\</span>
<span class="p_del">-		boot_cpu_has(bit)				\</span>
<span class="p_del">-)</span>
<span class="p_del">-</span>
 static __always_inline __pure bool _static_cpu_has_safe(u16 bit)
 {
 #ifdef CC_HAVE_ASM_GOTO
<span class="p_chunk">@@ -588,7 +505,6 @@</span> <span class="p_context"> static __always_inline __pure bool _static_cpu_has_safe(u16 bit)</span>
 /*
  * gcc 3.x is too stupid to do the static test; fall back to dynamic.
  */
<span class="p_del">-#define static_cpu_has(bit)		boot_cpu_has(bit)</span>
 #define static_cpu_has_safe(bit)	boot_cpu_has(bit)
 #endif
 
<span class="p_chunk">@@ -596,7 +512,6 @@</span> <span class="p_context"> static __always_inline __pure bool _static_cpu_has_safe(u16 bit)</span>
 #define set_cpu_bug(c, bit)		set_cpu_cap(c, (bit))
 #define clear_cpu_bug(c, bit)		clear_cpu_cap(c, (bit))
 
<span class="p_del">-#define static_cpu_has_bug(bit)		static_cpu_has((bit))</span>
 #define static_cpu_has_bug_safe(bit)	static_cpu_has_safe((bit))
 #define boot_cpu_has_bug(bit)		cpu_has_bug(&amp;boot_cpu_data, (bit))
 
<span class="p_header">diff --git a/arch/x86/include/asm/mwait.h b/arch/x86/include/asm/mwait.h</span>
<span class="p_header">index c70689b..2bd6e47 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mwait.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mwait.h</span>
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> static inline void __sti_mwait(unsigned long eax, unsigned long ecx)</span>
 static inline void mwait_idle_with_hints(unsigned long eax, unsigned long ecx)
 {
 	if (!current_set_polling_and_test()) {
<span class="p_del">-		if (static_cpu_has_bug(X86_BUG_CLFLUSH_MONITOR)) {</span>
<span class="p_add">+		if (static_cpu_has_bug_safe(X86_BUG_CLFLUSH_MONITOR)) {</span>
 			mb();
 			clflush((void *)&amp;current_thread_info()-&gt;flags);
 			mb();
<span class="p_header">diff --git a/arch/x86/include/asm/pmem.h b/arch/x86/include/asm/pmem.h</span>
<span class="p_header">index 1544fab..5d7abb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pmem.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pmem.h</span>
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> static inline bool __arch_has_wmb_pmem(void)</span>
 	 * We require that wmb() be an &#39;sfence&#39;, that is only guaranteed on
 	 * 64-bit builds
 	 */
<span class="p_del">-	return static_cpu_has(X86_FEATURE_PCOMMIT);</span>
<span class="p_add">+	return static_cpu_has_safe(X86_FEATURE_PCOMMIT);</span>
 }
 #endif /* CONFIG_ARCH_HAS_PMEM_API */
 #endif /* __ASM_X86_PMEM_H__ */
<span class="p_header">diff --git a/arch/x86/include/asm/qspinlock.h b/arch/x86/include/asm/qspinlock.h</span>
<span class="p_header">index eaba080..f456616 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/qspinlock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/qspinlock.h</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> static inline void queued_spin_unlock(struct qspinlock *lock)</span>
 #define virt_spin_lock virt_spin_lock
 static inline bool virt_spin_lock(struct qspinlock *lock)
 {
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_HYPERVISOR))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_HYPERVISOR))</span>
 		return false;
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">index 18ca99f..f8a6cfb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/proc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/proc.c</span>
<span class="p_chunk">@@ -32,11 +32,11 @@</span> <span class="p_context"> static void show_cpuinfo_misc(struct seq_file *m, struct cpuinfo_x86 *c)</span>
 		   &quot;fpu_exception\t: %s\n&quot;
 		   &quot;cpuid level\t: %d\n&quot;
 		   &quot;wp\t\t: %s\n&quot;,
<span class="p_del">-		   static_cpu_has_bug(X86_BUG_FDIV) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_del">-		   static_cpu_has_bug(X86_BUG_F00F) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_del">-		   static_cpu_has_bug(X86_BUG_COMA) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_del">-		   static_cpu_has(X86_FEATURE_FPU) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_del">-		   static_cpu_has(X86_FEATURE_FPU) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_add">+		   static_cpu_has_bug_safe(X86_BUG_FDIV) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_add">+		   static_cpu_has_bug_safe(X86_BUG_F00F) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_add">+		   static_cpu_has_bug_safe(X86_BUG_COMA) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_add">+		   static_cpu_has_safe(X86_FEATURE_FPU) ? &quot;yes&quot; : &quot;no&quot;,</span>
<span class="p_add">+		   static_cpu_has_safe(X86_FEATURE_FPU) ? &quot;yes&quot; : &quot;no&quot;,</span>
 		   c-&gt;cpuid_level,
 		   c-&gt;wp_works_ok ? &quot;yes&quot; : &quot;no&quot;);
 }
<span class="p_header">diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c</span>
<span class="p_header">index 0bc3490..9b0163c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/regset.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/regset.c</span>
<span class="p_chunk">@@ -275,7 +275,7 @@</span> <span class="p_context"> int fpregs_get(struct task_struct *target, const struct user_regset *regset,</span>
 
 	fpu__activate_fpstate_read(fpu);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_FPU))</span>
 		return fpregs_soft_get(target, regset, pos, count, kbuf, ubuf);
 
 	if (!cpu_has_fxsr)
<span class="p_chunk">@@ -306,7 +306,7 @@</span> <span class="p_context"> int fpregs_set(struct task_struct *target, const struct user_regset *regset,</span>
 	fpu__activate_fpstate_write(fpu);
 	fpstate_sanitize_xstate(fpu);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_FPU))</span>
 		return fpregs_soft_set(target, regset, pos, count, kbuf, ubuf);
 
 	if (!cpu_has_fxsr)
<span class="p_header">diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c</span>
<span class="p_header">index 31c6a60..05b6ede 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/signal.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/signal.c</span>
<span class="p_chunk">@@ -162,7 +162,7 @@</span> <span class="p_context"> int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)</span>
 	if (!access_ok(VERIFY_WRITE, buf, size))
 		return -EACCES;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_FPU))</span>
 		return fpregs_soft_get(current, NULL, 0,
 			sizeof(struct user_i387_ia32_struct), NULL,
 			(struct _fpstate_32 __user *) buf) ? -1 : 1;
<span class="p_chunk">@@ -267,7 +267,7 @@</span> <span class="p_context"> static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)</span>
 
 	fpu__activate_curr(fpu);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_FPU))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_FPU))</span>
 		return fpregs_soft_set(current, NULL,
 				       0, sizeof(struct user_i387_ia32_struct),
 				       NULL, buf) != 0;
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index b9d99e0..c49a284 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -411,7 +411,7 @@</span> <span class="p_context"> __switch_to(struct task_struct *prev_p, struct task_struct *next_p)</span>
 		     task_thread_info(prev_p)-&gt;flags &amp; _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p, tss);
 
<span class="p_del">-	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {</span>
<span class="p_add">+	if (static_cpu_has_bug_safe(X86_BUG_SYSRET_SS_ATTRS)) {</span>
 		/*
 		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
 		 * does not update the cached descriptor.  As a result, if we
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h</span>
<span class="p_header">index c8eda14..89d5ad7 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.h</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.h</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"> static inline bool guest_cpuid_has_xsave(struct kvm_vcpu *vcpu)</span>
 {
 	struct kvm_cpuid_entry2 *best;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_XSAVE))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_XSAVE))</span>
 		return false;
 
 	best = kvm_find_cpuid_entry(vcpu, 1, 0);
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index c13a64b..1892bdd 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -516,7 +516,7 @@</span> <span class="p_context"> static void skip_emulated_instruction(struct kvm_vcpu *vcpu)</span>
 	struct vcpu_svm *svm = to_svm(vcpu);
 
 	if (svm-&gt;vmcb-&gt;control.next_rip != 0) {
<span class="p_del">-		WARN_ON_ONCE(!static_cpu_has(X86_FEATURE_NRIPS));</span>
<span class="p_add">+		WARN_ON_ONCE(!static_cpu_has_safe(X86_FEATURE_NRIPS));</span>
 		svm-&gt;next_rip = svm-&gt;vmcb-&gt;control.next_rip;
 	}
 
<span class="p_chunk">@@ -548,7 +548,7 @@</span> <span class="p_context"> static void svm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr,</span>
 	    nested_svm_check_exception(svm, nr, has_error_code, error_code))
 		return;
 
<span class="p_del">-	if (nr == BP_VECTOR &amp;&amp; !static_cpu_has(X86_FEATURE_NRIPS)) {</span>
<span class="p_add">+	if (nr == BP_VECTOR &amp;&amp; !static_cpu_has_safe(X86_FEATURE_NRIPS)) {</span>
 		unsigned long rip, old_rip = kvm_rip_read(&amp;svm-&gt;vcpu);
 
 		/*
<span class="p_chunk">@@ -577,7 +577,7 @@</span> <span class="p_context"> static void svm_init_erratum_383(void)</span>
 	int err;
 	u64 val;
 
<span class="p_del">-	if (!static_cpu_has_bug(X86_BUG_AMD_TLB_MMATCH))</span>
<span class="p_add">+	if (!static_cpu_has_bug_safe(X86_BUG_AMD_TLB_MMATCH))</span>
 		return;
 
 	/* Use _safe variants to not break nested virtualization */
<span class="p_chunk">@@ -631,7 +631,7 @@</span> <span class="p_context"> static int has_svm(void)</span>
 static void svm_hardware_disable(void)
 {
 	/* Make sure we clean up behind us */
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_TSCRATEMSR))</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_TSCRATEMSR))</span>
 		wrmsrl(MSR_AMD64_TSC_RATIO, TSC_RATIO_DEFAULT);
 
 	cpu_svm_disable();
<span class="p_chunk">@@ -674,7 +674,7 @@</span> <span class="p_context"> static int svm_hardware_enable(void)</span>
 
 	wrmsrl(MSR_VM_HSAVE_PA, page_to_pfn(sd-&gt;save_area) &lt;&lt; PAGE_SHIFT);
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_TSCRATEMSR)) {</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_TSCRATEMSR)) {</span>
 		wrmsrl(MSR_AMD64_TSC_RATIO, TSC_RATIO_DEFAULT);
 		__this_cpu_write(current_tsc_ratio, TSC_RATIO_DEFAULT);
 	}
<span class="p_chunk">@@ -1233,7 +1233,7 @@</span> <span class="p_context"> static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 	for (i = 0; i &lt; NR_HOST_SAVE_USER_MSRS; i++)
 		rdmsrl(host_save_user_msrs[i], svm-&gt;host_user_msrs[i]);
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_TSCRATEMSR)) {</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_TSCRATEMSR)) {</span>
 		u64 tsc_ratio = vcpu-&gt;arch.tsc_scaling_ratio;
 		if (tsc_ratio != __this_cpu_read(current_tsc_ratio)) {
 			__this_cpu_write(current_tsc_ratio, tsc_ratio);
<span class="p_chunk">@@ -1241,7 +1241,7 @@</span> <span class="p_context"> static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)</span>
 		}
 	}
 	/* This assumes that the kernel never uses MSR_TSC_AUX */
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_RDTSCP))</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_RDTSCP))</span>
 		wrmsrl(MSR_TSC_AUX, svm-&gt;tsc_aux);
 }
 
<span class="p_chunk">@@ -2806,7 +2806,7 @@</span> <span class="p_context"> static int iret_interception(struct vcpu_svm *svm)</span>
 
 static int invlpg_interception(struct vcpu_svm *svm)
 {
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_DECODEASSISTS))</span>
 		return emulate_instruction(&amp;svm-&gt;vcpu, 0) == EMULATE_DONE;
 
 	kvm_mmu_invlpg(&amp;svm-&gt;vcpu, svm-&gt;vmcb-&gt;control.exit_info_1);
<span class="p_chunk">@@ -2823,7 +2823,7 @@</span> <span class="p_context"> static int rdpmc_interception(struct vcpu_svm *svm)</span>
 {
 	int err;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_NRIPS))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_NRIPS))</span>
 		return emulate_on_interception(svm);
 
 	err = kvm_rdpmc(&amp;svm-&gt;vcpu);
<span class="p_chunk">@@ -2864,7 +2864,7 @@</span> <span class="p_context"> static int cr_interception(struct vcpu_svm *svm)</span>
 	unsigned long val;
 	int err;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_DECODEASSISTS))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_DECODEASSISTS))</span>
 		return emulate_on_interception(svm);
 
 	if (unlikely((svm-&gt;vmcb-&gt;control.exit_info_1 &amp; CR_VALID) == 0))
<span class="p_chunk">@@ -3710,7 +3710,7 @@</span> <span class="p_context"> static void svm_flush_tlb(struct kvm_vcpu *vcpu)</span>
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_FLUSHBYASID))</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_FLUSHBYASID))</span>
 		svm-&gt;vmcb-&gt;control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;
 	else
 		svm-&gt;asid_generation--;
<span class="p_chunk">@@ -4282,7 +4282,7 @@</span> <span class="p_context"> static int svm_check_intercept(struct kvm_vcpu *vcpu,</span>
 	}
 
 	/* TODO: Advertise NRIPS to guest hypervisor unconditionally */
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_NRIPS))</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_NRIPS))</span>
 		vmcb-&gt;control.next_rip  = info-&gt;next_rip;
 	vmcb-&gt;control.exit_code = icpt_info.exit_code;
 	vmexit = nested_svm_exit_handled(svm);
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index e2951b6..4bef603 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -8993,7 +8993,7 @@</span> <span class="p_context"> static void vmx_cpuid_update(struct kvm_vcpu *vcpu)</span>
 	if (cpu_has_secondary_exec_ctrls())
 		vmcs_set_secondary_exec_control(secondary_exec_ctl);
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_PCOMMIT) &amp;&amp; nested) {</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_PCOMMIT) &amp;&amp; nested) {</span>
 		if (guest_cpuid_has_pcommit(vcpu))
 			vmx-&gt;nested.nested_vmx_secondary_ctls_high |=
 				SECONDARY_EXEC_PCOMMIT;
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index eef44d9..711e9bc 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -1033,7 +1033,7 @@</span> <span class="p_context"> static inline bool smap_violation(int error_code, struct pt_regs *regs)</span>
 	if (!IS_ENABLED(CONFIG_X86_SMAP))
 		return false;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_SMAP))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_SMAP))</span>
 		return false;
 
 	if (error_code &amp; PF_USER)
<span class="p_header">diff --git a/arch/x86/ras/mce_amd_inj.c b/arch/x86/ras/mce_amd_inj.c</span>
<span class="p_header">index 55d38cf..191650f 100644</span>
<span class="p_header">--- a/arch/x86/ras/mce_amd_inj.c</span>
<span class="p_header">+++ b/arch/x86/ras/mce_amd_inj.c</span>
<span class="p_chunk">@@ -275,7 +275,7 @@</span> <span class="p_context"> static void do_inject(void)</span>
 	 * only on the node base core. Refer to D18F3x44[NbMcaToMstCpuEn] for
 	 * Fam10h and later BKDGs.
 	 */
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_AMD_DCM) &amp;&amp; b == 4) {</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_AMD_DCM) &amp;&amp; b == 4) {</span>
 		toggle_nb_mca_mst_cpu(amd_get_nb_id(cpu));
 		cpu = get_nbc_for_node(amd_get_nb_id(cpu));
 	}
<span class="p_header">diff --git a/drivers/cpufreq/amd_freq_sensitivity.c b/drivers/cpufreq/amd_freq_sensitivity.c</span>
<span class="p_header">index f6b79ab..87b0cf4 100644</span>
<span class="p_header">--- a/drivers/cpufreq/amd_freq_sensitivity.c</span>
<span class="p_header">+++ b/drivers/cpufreq/amd_freq_sensitivity.c</span>
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> static int __init amd_freq_sensitivity_init(void)</span>
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)
 		return -ENODEV;
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_PROC_FEEDBACK))</span>
<span class="p_add">+	if (!static_cpu_has_safe(X86_FEATURE_PROC_FEEDBACK))</span>
 		return -ENODEV;
 
 	if (rdmsrl_safe(MSR_AMD64_FREQ_SENSITIVITY_ACTUAL, &amp;val))
<span class="p_header">diff --git a/drivers/cpufreq/powernow-k8.c b/drivers/cpufreq/powernow-k8.c</span>
<span class="p_header">index 0b5bf13..aa71612 100644</span>
<span class="p_header">--- a/drivers/cpufreq/powernow-k8.c</span>
<span class="p_header">+++ b/drivers/cpufreq/powernow-k8.c</span>
<span class="p_chunk">@@ -1193,7 +1193,7 @@</span> <span class="p_context"> static int powernowk8_init(void)</span>
 	unsigned int i, supported_cpus = 0;
 	int ret;
 
<span class="p_del">-	if (static_cpu_has(X86_FEATURE_HW_PSTATE)) {</span>
<span class="p_add">+	if (static_cpu_has_safe(X86_FEATURE_HW_PSTATE)) {</span>
 		__request_acpi_cpufreq();
 		return -ENODEV;
 	}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



