
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Revert &quot;x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation&quot; - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Revert &quot;x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation&quot;</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 23, 2017, 9:52 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170423095244.7brzd7wcpag7es2c@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9694699/mbox/"
   >mbox</a>
|
   <a href="/patch/9694699/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9694699/">/patch/9694699/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A2B1960245 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Apr 2017 09:53:05 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7D77F26E98
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Apr 2017 09:53:05 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6E748223B3; Sun, 23 Apr 2017 09:53:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9B1D4223B3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Apr 2017 09:53:02 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1044597AbdDWJw6 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 23 Apr 2017 05:52:58 -0400
Received: from mail-wr0-f196.google.com ([209.85.128.196]:36616 &quot;EHLO
	mail-wr0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756391AbdDWJwt (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 23 Apr 2017 05:52:49 -0400
Received: by mail-wr0-f196.google.com with SMTP id v42so8707533wrc.3;
	Sun, 23 Apr 2017 02:52:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:date:from:to:cc:subject:message-id:references:mime-version
	:content-disposition:in-reply-to:user-agent;
	bh=0M2y2kLmLIME9UQVrJBpgPZAXRD2RL6EbDgW9rqtZwc=;
	b=uB6/NszO54aScuK7uG1f25XzLBURspXJyxL1KbsUwFhSt0f0TylNTFPtwe6Xtsy4eA
	szxe9FSMP11o/i8gZUbKKP/f/JjmLVEh41zXUK6D4cUXfRfkE8ACAYBiVuV6aW37jRwZ
	MGslN3SM1egkNuCNMozvR8GZx9tQQuWtFYsC9c+JC/RkQBDvi7PAdu/mKHiXGhCQZoca
	Nwwm48GW0RGfShlKAROgeQG11mp6EkcrLbipK2ADIMZTn1rTbBbDsG410P8sAmbswwsa
	Xfwu1l2OaQkb1V6pJGq2YoHHo6N/pIEL5hukkTIiJhqbKBZEfi5cBxJxMelaGfqsl73/
	ATAg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:references:mime-version:content-disposition:in-reply-to:user-agent; 
	bh=0M2y2kLmLIME9UQVrJBpgPZAXRD2RL6EbDgW9rqtZwc=;
	b=TFu7I+hAvfHJ+XlXWsg+2bUIrHASXJr7e1sZPmREUWb8hGaf18RLCMAaNGcpgj5VbD
	LRs0d831QVGtdga53/ysL5OlbEPsaKrPaPKkLzMkenWHxiIzwMPVu1duCbG7PhuAhTTa
	UW9aPn9w27Bq0B3jTDnR1ONXpPHrBb9KFG37vahaJRyVMsVKEaiBfw5rOMDXXjEiKnsr
	pHM7K5/sLBRUrRD7MeFQv+krGXZIw/CchSlrirMRUyy4/rEJi0OnMGOagCY9F6GnKpnV
	2VgFq5hsWfLCxa9+B7xwyTDrR9HvoLYbF9a8o/4n6wjNCJ0jsl5RgAew8JL3W47brUiE
	kWrg==
X-Gm-Message-State: AN3rC/6cyiUHDxyQQkkCABqK31QvKjs8whmn7iQEtJ24j3PfK7YAk+hs
	DIm5QAHd2Mmt8Q==
X-Received: by 10.223.132.163 with SMTP id 32mr934305wrg.154.1492941167371; 
	Sun, 23 Apr 2017 02:52:47 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	i21sm18127297wrc.50.2017.04.23.02.52.46
	(version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
	Sun, 23 Apr 2017 02:52:46 -0700 (PDT)
Date: Sun, 23 Apr 2017 11:52:44 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	steve.capper@linaro.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Linux Kernel Mailing List &lt;linux-kernel@vger.kernel.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Borislav Petkov &lt;bp@alien8.de&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	dann.frazier@canonical.com,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Michal Hocko &lt;mhocko@suse.cz&gt;, linux-tip-commits@vger.kernel.org
Subject: [PATCH] Revert &quot;x86/mm/gup: Switch GUP to the generic
	get_user_page_fast() implementation&quot;
Message-ID: &lt;20170423095244.7brzd7wcpag7es2c@gmail.com&gt;
References: &lt;CAA9_cmf7=aGXKoQFkzS_UJtznfRtWofitDpV2AyGwpaRGKyQkg@mail.gmail.com&gt;
	&lt;20170421141628.ruxxnq54jvuhiqnz@node.shutemov.name&gt;
	&lt;CAPcyv4g5WkSCRBUPhXx3iF1KEQQ0RfmgnmaAzrT3dwuQFxYUEQ@mail.gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;CAPcyv4g5WkSCRBUPhXx3iF1KEQQ0RfmgnmaAzrT3dwuQFxYUEQ@mail.gmail.com&gt;
User-Agent: NeoMutt/20170113 (1.7.2)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - April 23, 2017, 9:52 a.m.</div>
<pre class="content">
* Dan Williams &lt;dan.j.williams@intel.com&gt; wrote:
<span class="quote">
&gt; &gt; I can&#39;t find the issue either.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Is it something reproducible without hardware? In KVM?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You can do it in KVM, just boot with the memmap=ss!nn parameter to</span>
<span class="quote">&gt; simulate pmem. In this case I&#39;m booting with memmap=4G!8G, you should</span>
<span class="quote">&gt; also specify &quot;nokaslr&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; If yes, could you share the test-case?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, run:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     ./autogen.sh</span>
<span class="quote">&gt;     ./configure CFLAGS=&#39;-g -O0&#39; --prefix=/usr --sysconfdir=/etc</span>
<span class="quote">&gt; --libdir=/usr/lib64</span>
<span class="quote">&gt;     make TESTS=device-dax check</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...from a checkout of the ndctl project:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     https://github.com/pmem/ndctl</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let me know if you run into any problems getting the test to build or run.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; [   35.423841] WARNING: CPU: 8 PID: 245 at lib/percpu-refcount.c:155</span>
<span class="quote">&gt; &gt;&gt; percpu_ref_switch_to_atomic_rcu+0x1f5/0x200</span>
<span class="quote">&gt; &gt;&gt; [   35.425328] percpu ref (dax_pmem_percpu_release [dax_pmem]) &lt;= 0</span>
<span class="quote">&gt; &gt;&gt; (0) after switching to atomic</span>

Since the bug appears to be pretty severe (GUP race causing possible memory 
corruption that could affect a lot of code), and the merge window is awfully 
close, plus the reproducer appears to be pretty quick, I&#39;ve queued up the
revert below for the time being, to not block the rest of the pending
tip:x86/mm changes.

I&#39;d have loved to see this conversion in v4.12, but not at any cost.

Thanks,

	Ingo
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
==================&gt;
From 6dd29b3df975582ef429b5b93c899e6575785940 Mon Sep 17 00:00:00 2001
From: Ingo Molnar &lt;mingo@kernel.org&gt;
Date: Sun, 23 Apr 2017 11:37:17 +0200
Subject: [PATCH] Revert &quot;x86/mm/gup: Switch GUP to the generic get_user_page_fast() implementation&quot;

This reverts commit 2947ba054a4dabbd82848728d765346886050029.

Dan Williams reported dax-pmem kernel warnings with the following signature:

   WARNING: CPU: 8 PID: 245 at lib/percpu-refcount.c:155 percpu_ref_switch_to_atomic_rcu+0x1f5/0x200
   percpu ref (dax_pmem_percpu_release [dax_pmem]) &lt;= 0 (0) after switching to atomic

... and bisected it to this commit, which suggests possible memory corruption
caused by the x86 fast-GUP conversion.

He also pointed out:

 &quot;
  This is similar to the backtrace when we were not properly handling
  pud faults and was fixed with this commit: 220ced1676c4 &quot;mm: fix
  get_user_pages() vs device-dax pud mappings&quot;

  I&#39;ve found some missing _devmap checks in the generic
  get_user_pages_fast() path, but this does not fix the regression
  [...]
 &quot;

So given that there are known bugs, and a pretty robust looking bisection
points to this commit suggesting that are unknown bugs in the conversion
as well, revert it for the time being - we&#39;ll re-try in v4.13.

Reported-by: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
Cc: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Michal Hocko &lt;mhocko@suse.cz&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: aneesh.kumar@linux.vnet.ibm.com
Cc: dann.frazier@canonical.com
Cc: dave.hansen@intel.com
Cc: steve.capper@linaro.org
Cc: linux-kernel@vger.kernel.org
Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
<span class="p_del">---</span>
 arch/arm/Kconfig                      |   2 +-
 arch/arm64/Kconfig                    |   2 +-
 arch/powerpc/Kconfig                  |   2 +-
 arch/x86/Kconfig                      |   3 -
 arch/x86/include/asm/mmu_context.h    |  12 +
 arch/x86/include/asm/pgtable-3level.h |  47 ----
 arch/x86/include/asm/pgtable.h        |  53 ----
 arch/x86/include/asm/pgtable_64.h     |  16 +-
 arch/x86/mm/Makefile                  |   2 +-
 arch/x86/mm/gup.c                     | 496 ++++++++++++++++++++++++++++++++++
 mm/Kconfig                            |   2 +-
 mm/gup.c                              |  10 +-
 12 files changed, 519 insertions(+), 128 deletions(-)

<span class="p_header">diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig</span>
<span class="p_header">index 454fadd077ad..0d4e71b42c77 100644</span>
<span class="p_header">--- a/arch/arm/Kconfig</span>
<span class="p_header">+++ b/arch/arm/Kconfig</span>
<span class="p_chunk">@@ -1666,7 +1666,7 @@</span> <span class="p_context"> config ARCH_SELECT_MEMORY_MODEL</span>
 config HAVE_ARCH_PFN_VALID
 	def_bool ARCH_HAS_HOLES_MEMORYMODEL || !SPARSEMEM
 
<span class="p_del">-config HAVE_GENERIC_GUP</span>
<span class="p_add">+config HAVE_GENERIC_RCU_GUP</span>
 	def_bool y
 	depends on ARM_LPAE
 
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index af62bf79721a..3741859765cf 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -205,7 +205,7 @@</span> <span class="p_context"> config GENERIC_CALIBRATE_DELAY</span>
 config ZONE_DMA
 	def_bool y
 
<span class="p_del">-config HAVE_GENERIC_GUP</span>
<span class="p_add">+config HAVE_GENERIC_RCU_GUP</span>
 	def_bool y
 
 config ARCH_DMA_ADDR_T_64BIT
<span class="p_header">diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig</span>
<span class="p_header">index 3a716b2dcde9..97a8bc8a095c 100644</span>
<span class="p_header">--- a/arch/powerpc/Kconfig</span>
<span class="p_header">+++ b/arch/powerpc/Kconfig</span>
<span class="p_chunk">@@ -135,7 +135,7 @@</span> <span class="p_context"> config PPC</span>
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_TRACER
 	select HAVE_GCC_PLUGINS
<span class="p_del">-	select HAVE_GENERIC_GUP</span>
<span class="p_add">+	select HAVE_GENERIC_RCU_GUP</span>
 	select HAVE_HW_BREAKPOINT		if PERF_EVENTS &amp;&amp; (PPC_BOOK3S || PPC_8xx)
 	select HAVE_IDE
 	select HAVE_IOREMAP_PROT
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index a641b900fc1f..2bde14451e54 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -2789,9 +2789,6 @@</span> <span class="p_context"> config X86_DMA_REMAP</span>
 	bool
 	depends on STA2X11
 
<span class="p_del">-config HAVE_GENERIC_GUP</span>
<span class="p_del">-	def_bool y</span>
<span class="p_del">-</span>
 source &quot;net/Kconfig&quot;
 
 source &quot;drivers/Kconfig&quot;
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 6e933d2d88d9..68b329d77b3a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -220,6 +220,18 @@</span> <span class="p_context"> static inline int vma_pkey(struct vm_area_struct *vma)</span>
 }
 #endif
 
<span class="p_add">+static inline bool __pkru_allows_pkey(u16 pkey, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 pkru = read_pkru();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!__pkru_allows_read(pkru, pkey))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	if (write &amp;&amp; !__pkru_allows_write(pkru, pkey))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * We only want to enforce protection keys on the current process
  * because we effectively have no access to PKRU for other
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable-3level.h b/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_header">index c8821bab938f..50d35e3185f5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable-3level.h</span>
<span class="p_chunk">@@ -212,51 +212,4 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_clear(pud_t *pudp)</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t){ (pte).pte_high })
 #define __swp_entry_to_pte(x)		((pte_t){ { .pte_high = (x).val } })
 
<span class="p_del">-#define gup_get_pte gup_get_pte</span>
<span class="p_del">-/*</span>
<span class="p_del">- * WARNING: only to be used in the get_user_pages_fast() implementation.</span>
<span class="p_del">- *</span>
<span class="p_del">- * With get_user_pages_fast(), we walk down the pagetables without taking</span>
<span class="p_del">- * any locks.  For this we would like to load the pointers atomically,</span>
<span class="p_del">- * but that is not possible (without expensive cmpxchg8b) on PAE.  What</span>
<span class="p_del">- * we do have is the guarantee that a PTE will only either go from not</span>
<span class="p_del">- * present to present, or present to not present or both -- it will not</span>
<span class="p_del">- * switch to a completely different present page without a TLB flush in</span>
<span class="p_del">- * between; something that we are blocking by holding interrupts off.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Setting ptes from not present to present goes:</span>
<span class="p_del">- *</span>
<span class="p_del">- *   ptep-&gt;pte_high = h;</span>
<span class="p_del">- *   smp_wmb();</span>
<span class="p_del">- *   ptep-&gt;pte_low = l;</span>
<span class="p_del">- *</span>
<span class="p_del">- * And present to not present goes:</span>
<span class="p_del">- *</span>
<span class="p_del">- *   ptep-&gt;pte_low = 0;</span>
<span class="p_del">- *   smp_wmb();</span>
<span class="p_del">- *   ptep-&gt;pte_high = 0;</span>
<span class="p_del">- *</span>
<span class="p_del">- * We must ensure here that the load of pte_low sees &#39;l&#39; iff pte_high</span>
<span class="p_del">- * sees &#39;h&#39;. We load pte_high *after* loading pte_low, which ensures we</span>
<span class="p_del">- * don&#39;t see an older value of pte_high.  *Then* we recheck pte_low,</span>
<span class="p_del">- * which ensures that we haven&#39;t picked up a changed pte high. We might</span>
<span class="p_del">- * have gotten rubbish values from pte_low and pte_high, but we are</span>
<span class="p_del">- * guaranteed that pte_low will not have the present bit set *unless*</span>
<span class="p_del">- * it is &#39;l&#39;. Because get_user_pages_fast() only operates on present ptes</span>
<span class="p_del">- * we&#39;re safe.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline pte_t gup_get_pte(pte_t *ptep)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pte_t pte;</span>
<span class="p_del">-</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		pte.pte_low = ptep-&gt;pte_low;</span>
<span class="p_del">-		smp_rmb();</span>
<span class="p_del">-		pte.pte_high = ptep-&gt;pte_high;</span>
<span class="p_del">-		smp_rmb();</span>
<span class="p_del">-	} while (unlikely(pte.pte_low != ptep-&gt;pte_low));</span>
<span class="p_del">-</span>
<span class="p_del">-	return pte;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* _ASM_X86_PGTABLE_3LEVEL_H */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 942482ac36a8..f5af95a0c6b8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -244,11 +244,6 @@</span> <span class="p_context"> static inline int pud_devmap(pud_t pud)</span>
 	return 0;
 }
 #endif
<span class="p_del">-</span>
<span class="p_del">-static inline int pgd_devmap(pgd_t pgd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
 #endif
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
<span class="p_chunk">@@ -1190,54 +1185,6 @@</span> <span class="p_context"> static inline u16 pte_flags_pkey(unsigned long pte_flags)</span>
 #endif
 }
 
<span class="p_del">-static inline bool __pkru_allows_pkey(u16 pkey, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 pkru = read_pkru();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!__pkru_allows_read(pkru, pkey))</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-	if (write &amp;&amp; !__pkru_allows_write(pkru, pkey))</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * &#39;pteval&#39; can come from a PTE, PMD or PUD.  We only check</span>
<span class="p_del">- * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the</span>
<span class="p_del">- * same value on all 3 types.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline bool __pte_access_permitted(unsigned long pteval, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (write)</span>
<span class="p_del">-		need_pte_bits |= _PAGE_RW;</span>
<span class="p_del">-</span>
<span class="p_del">-	if ((pteval &amp; need_pte_bits) != need_pte_bits)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define pte_access_permitted pte_access_permitted</span>
<span class="p_del">-static inline bool pte_access_permitted(pte_t pte, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __pte_access_permitted(pte_val(pte), write);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define pmd_access_permitted pmd_access_permitted</span>
<span class="p_del">-static inline bool pmd_access_permitted(pmd_t pmd, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __pte_access_permitted(pmd_val(pmd), write);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define pud_access_permitted pud_access_permitted</span>
<span class="p_del">-static inline bool pud_access_permitted(pud_t pud, bool write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __pte_access_permitted(pud_val(pud), write);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #include &lt;asm-generic/pgtable.h&gt;
 #endif	/* __ASSEMBLY__ */
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 12ea31274eb6..9991224f6238 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -227,20 +227,6 @@</span> <span class="p_context"> extern void cleanup_highmap(void);</span>
 extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
 extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
 
<span class="p_del">-#define gup_fast_permitted gup_fast_permitted</span>
<span class="p_del">-static inline bool gup_fast_permitted(unsigned long start, int nr_pages,</span>
<span class="p_del">-		int write)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long len, end;</span>
<span class="p_del">-</span>
<span class="p_del">-	len = (unsigned long)nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-	end = start + len;</span>
<span class="p_del">-	if (end &lt; start)</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-	if (end &gt;&gt; __VIRTUAL_MASK_SHIFT)</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #endif /* !__ASSEMBLY__ */
<span class="p_add">+</span>
 #endif /* _ASM_X86_PGTABLE_64_H */
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 0fbdcb64f9f8..96d2b847e09e 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 KCOV_INSTRUMENT_tlb.o	:= n
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_del">-	    pat.o pgtable.o physaddr.o setup_nx.o tlb.o</span>
<span class="p_add">+	    pat.o pgtable.o physaddr.o gup.o setup_nx.o tlb.o</span>
 
 # Make sure __phys_addr has no stackprotector
 nostackp := $(call cc-option, -fno-stack-protector)
<span class="p_header">diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
new file mode 100644
<span class="p_header">index 000000000000..456dfdfd2249</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -0,0 +1,496 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Lockless get_user_pages_fast for x86</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2008 Nick Piggin</span>
<span class="p_add">+ * Copyright (C) 2008 Novell Inc.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/vmstat.h&gt;</span>
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t gup_get_pte(pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_X86_PAE</span>
<span class="p_add">+	return READ_ONCE(*ptep);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * With get_user_pages_fast, we walk down the pagetables without taking</span>
<span class="p_add">+	 * any locks.  For this we would like to load the pointers atomically,</span>
<span class="p_add">+	 * but that is not possible (without expensive cmpxchg8b) on PAE.  What</span>
<span class="p_add">+	 * we do have is the guarantee that a pte will only either go from not</span>
<span class="p_add">+	 * present to present, or present to not present or both -- it will not</span>
<span class="p_add">+	 * switch to a completely different present page without a TLB flush in</span>
<span class="p_add">+	 * between; something that we are blocking by holding interrupts off.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Setting ptes from not present to present goes:</span>
<span class="p_add">+	 * ptep-&gt;pte_high = h;</span>
<span class="p_add">+	 * smp_wmb();</span>
<span class="p_add">+	 * ptep-&gt;pte_low = l;</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * And present to not present goes:</span>
<span class="p_add">+	 * ptep-&gt;pte_low = 0;</span>
<span class="p_add">+	 * smp_wmb();</span>
<span class="p_add">+	 * ptep-&gt;pte_high = 0;</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We must ensure here that the load of pte_low sees l iff pte_high</span>
<span class="p_add">+	 * sees h. We load pte_high *after* loading pte_low, which ensures we</span>
<span class="p_add">+	 * don&#39;t see an older value of pte_high.  *Then* we recheck pte_low,</span>
<span class="p_add">+	 * which ensures that we haven&#39;t picked up a changed pte high. We might</span>
<span class="p_add">+	 * have got rubbish values from pte_low and pte_high, but we are</span>
<span class="p_add">+	 * guaranteed that pte_low will not have the present bit set *unless*</span>
<span class="p_add">+	 * it is &#39;l&#39;. And get_user_pages_fast only operates on present ptes, so</span>
<span class="p_add">+	 * we&#39;re safe.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * gup_get_pte should not be used or copied outside gup.c without being</span>
<span class="p_add">+	 * very careful -- it does not atomically load the pte or anything that</span>
<span class="p_add">+	 * is likely to be useful for you.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	pte.pte_low = ptep-&gt;pte_low;</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	pte.pte_high = ptep-&gt;pte_high;</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	if (unlikely(pte.pte_low != ptep-&gt;pte_low))</span>
<span class="p_add">+		goto retry;</span>
<span class="p_add">+</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void undo_dev_pagemap(int *nr, int nr_start, struct page **pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while ((*nr) - nr_start) {</span>
<span class="p_add">+		struct page *page = pages[--(*nr)];</span>
<span class="p_add">+</span>
<span class="p_add">+		ClearPageReferenced(page);</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * &#39;pteval&#39; can come from a pte, pmd, pud or p4d.  We only check</span>
<span class="p_add">+ * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the</span>
<span class="p_add">+ * same value on all 4 types.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int pte_allows_gup(unsigned long pteval, int write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (write)</span>
<span class="p_add">+		need_pte_bits |= _PAGE_RW;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((pteval &amp; need_pte_bits) != need_pte_bits)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check memory protection keys permissions. */</span>
<span class="p_add">+	if (!__pkru_allows_pkey(pte_flags_pkey(pteval), write))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The performance critical leaf functions are made noinline otherwise gcc</span>
<span class="p_add">+ * inlines everything into a single function which results in too much</span>
<span class="p_add">+ * register pressure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static noinline int gup_pte_range(pmd_t pmd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct dev_pagemap *pgmap = NULL;</span>
<span class="p_add">+	int nr_start = *nr, ret = 0;</span>
<span class="p_add">+	pte_t *ptep, *ptem;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Keep the original mapped PTE value (ptem) around since we</span>
<span class="p_add">+	 * might increment ptep off the end of the page when finishing</span>
<span class="p_add">+	 * our loop iteration.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ptem = ptep = pte_offset_map(&amp;pmd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte_t pte = gup_get_pte(ptep);</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Similar to the PMD case, NUMA hinting must take slow path */</span>
<span class="p_add">+		if (pte_protnone(pte))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_allows_gup(pte_val(pte), write))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pte_devmap(pte)) {</span>
<span class="p_add">+			pgmap = get_dev_pagemap(pte_pfn(pte), pgmap);</span>
<span class="p_add">+			if (unlikely(!pgmap)) {</span>
<span class="p_add">+				undo_dev_pagemap(nr, nr_start, pages);</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else if (pte_special(pte))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));</span>
<span class="p_add">+		page = pte_page(pte);</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+		put_dev_pagemap(pgmap);</span>
<span class="p_add">+		SetPageReferenced(page);</span>
<span class="p_add">+		pages[*nr] = page;</span>
<span class="p_add">+		(*nr)++;</span>
<span class="p_add">+</span>
<span class="p_add">+	} while (ptep++, addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+	if (addr == end)</span>
<span class="p_add">+		ret = 1;</span>
<span class="p_add">+	pte_unmap(ptem);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void get_head_page_multiple(struct page *page, int nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_PAGE(page != compound_head(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(page_count(page) == 0, page);</span>
<span class="p_add">+	page_ref_add(page, nr);</span>
<span class="p_add">+	SetPageReferenced(page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __gup_device_huge(unsigned long pfn, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int nr_start = *nr;</span>
<span class="p_add">+	struct dev_pagemap *pgmap = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		struct page *page = pfn_to_page(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+		pgmap = get_dev_pagemap(pfn, pgmap);</span>
<span class="p_add">+		if (unlikely(!pgmap)) {</span>
<span class="p_add">+			undo_dev_pagemap(nr, nr_start, pages);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		SetPageReferenced(page);</span>
<span class="p_add">+		pages[*nr] = page;</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+		put_dev_pagemap(pgmap);</span>
<span class="p_add">+		(*nr)++;</span>
<span class="p_add">+		pfn++;</span>
<span class="p_add">+	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __gup_device_huge_pmd(pmd_t pmd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long fault_pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+	fault_pfn = pmd_pfn(pmd) + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	return __gup_device_huge(fault_pfn, addr, end, pages, nr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __gup_device_huge_pud(pud_t pud, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long fault_pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+	fault_pfn = pud_pfn(pud) + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	return __gup_device_huge(fault_pfn, addr, end, pages, nr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static noinline int gup_huge_pmd(pmd_t pmd, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *head, *page;</span>
<span class="p_add">+	int refs;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pte_allows_gup(pmd_val(pmd), write))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(!pfn_valid(pmd_pfn(pmd)));</span>
<span class="p_add">+	if (pmd_devmap(pmd))</span>
<span class="p_add">+		return __gup_device_huge_pmd(pmd, addr, end, pages, nr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* hugepages are never &quot;special&quot; */</span>
<span class="p_add">+	VM_BUG_ON(pmd_flags(pmd) &amp; _PAGE_SPECIAL);</span>
<span class="p_add">+</span>
<span class="p_add">+	refs = 0;</span>
<span class="p_add">+	head = pmd_page(pmd);</span>
<span class="p_add">+	page = head + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		pages[*nr] = page;</span>
<span class="p_add">+		(*nr)++;</span>
<span class="p_add">+		page++;</span>
<span class="p_add">+		refs++;</span>
<span class="p_add">+	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+	get_head_page_multiple(head, refs);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="p_add">+		int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pmd_t *pmdp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmdp = pmd_offset(&amp;pud, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pmd_t pmd = *pmdp;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		if (pmd_none(pmd))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="p_add">+			 * slowpath for accounting purposes and so that they</span>
<span class="p_add">+			 * can be serialised against THP migration.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (pmd_protnone(pmd))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+			if (!gup_huge_pmd(pmd, addr, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!gup_pte_range(pmd, addr, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (pmdp++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static noinline int gup_huge_pud(pud_t pud, unsigned long addr,</span>
<span class="p_add">+		unsigned long end, int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *head, *page;</span>
<span class="p_add">+	int refs;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pte_allows_gup(pud_val(pud), write))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(!pfn_valid(pud_pfn(pud)));</span>
<span class="p_add">+	if (pud_devmap(pud))</span>
<span class="p_add">+		return __gup_device_huge_pud(pud, addr, end, pages, nr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* hugepages are never &quot;special&quot; */</span>
<span class="p_add">+	VM_BUG_ON(pud_flags(pud) &amp; _PAGE_SPECIAL);</span>
<span class="p_add">+</span>
<span class="p_add">+	refs = 0;</span>
<span class="p_add">+	head = pud_page(pud);</span>
<span class="p_add">+	page = head + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		pages[*nr] = page;</span>
<span class="p_add">+		(*nr)++;</span>
<span class="p_add">+		page++;</span>
<span class="p_add">+		refs++;</span>
<span class="p_add">+	} while (addr += PAGE_SIZE, addr != end);</span>
<span class="p_add">+	get_head_page_multiple(head, refs);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pud_t *pudp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pudp = pud_offset(&amp;p4d, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pud_t pud = *pudp;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (pud_none(pud))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		if (unlikely(pud_large(pud))) {</span>
<span class="p_add">+			if (!gup_huge_pud(pud, addr, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (!gup_pmd_range(pud, addr, next, write, pages, nr))</span>
<span class="p_add">+				return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (pudp++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,</span>
<span class="p_add">+			int write, struct page **pages, int *nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	p4d_t *p4dp;</span>
<span class="p_add">+</span>
<span class="p_add">+	p4dp = p4d_offset(&amp;pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		p4d_t p4d = *p4dp;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = p4d_addr_end(addr, end);</span>
<span class="p_add">+		if (p4d_none(p4d))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		BUILD_BUG_ON(p4d_large(p4d));</span>
<span class="p_add">+		if (!gup_pud_range(p4d, addr, next, write, pages, nr))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+	} while (p4dp++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Like get_user_pages_fast() except its IRQ-safe in that it won&#39;t fall</span>
<span class="p_add">+ * back to the regular GUP.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="p_add">+			  struct page **pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	unsigned long addr, len, end;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	pgd_t *pgdp;</span>
<span class="p_add">+	int nr = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	addr = start;</span>
<span class="p_add">+	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+	end = start + len;</span>
<span class="p_add">+	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,</span>
<span class="p_add">+					(void __user *)start, len)))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * XXX: batch / limit &#39;nr&#39;, to avoid large irq off latency</span>
<span class="p_add">+	 * needs some instrumenting to determine the common sizes used by</span>
<span class="p_add">+	 * important workloads (eg. DB2), and whether limiting the batch size</span>
<span class="p_add">+	 * will decrease performance.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * It seems like we&#39;re in the clear for the moment. Direct-IO is</span>
<span class="p_add">+	 * the main guy that batches up lots of get_user_pages, and even</span>
<span class="p_add">+	 * they are limited to 64-at-a-time which is not so many.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This doesn&#39;t prevent pagetable teardown, but does prevent</span>
<span class="p_add">+	 * the pagetables and pages from being freed on x86.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * So long as we atomically load page table pointers versus teardown</span>
<span class="p_add">+	 * (which we do on x86, with the above PAE exception), we can follow the</span>
<span class="p_add">+	 * address down to the the page and take a ref on it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd_t pgd = *pgdp;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		if (pgd_none(pgd))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	} while (pgdp++, addr = next, addr != end);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	return nr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * get_user_pages_fast() - pin user pages in memory</span>
<span class="p_add">+ * @start:	starting user address</span>
<span class="p_add">+ * @nr_pages:	number of pages from start to pin</span>
<span class="p_add">+ * @write:	whether pages will be written to</span>
<span class="p_add">+ * @pages:	array that receives pointers to the pages pinned.</span>
<span class="p_add">+ * 		Should be at least nr_pages long.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Attempt to pin user pages in memory without taking mm-&gt;mmap_sem.</span>
<span class="p_add">+ * If not successful, it will fall back to taking the lock and</span>
<span class="p_add">+ * calling get_user_pages().</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns number of pages pinned. This may be fewer than the number</span>
<span class="p_add">+ * requested. If nr_pages is 0 or negative, returns 0. If no pages</span>
<span class="p_add">+ * were pinned, returns -errno.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="p_add">+			struct page **pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	unsigned long addr, len, end;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pgd_t *pgdp;</span>
<span class="p_add">+	int nr = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	addr = start;</span>
<span class="p_add">+	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	end = start + len;</span>
<span class="p_add">+	if (end &lt; start)</span>
<span class="p_add">+		goto slow_irqon;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	if (end &gt;&gt; __VIRTUAL_MASK_SHIFT)</span>
<span class="p_add">+		goto slow_irqon;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * XXX: batch / limit &#39;nr&#39;, to avoid large irq off latency</span>
<span class="p_add">+	 * needs some instrumenting to determine the common sizes used by</span>
<span class="p_add">+	 * important workloads (eg. DB2), and whether limiting the batch size</span>
<span class="p_add">+	 * will decrease performance.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * It seems like we&#39;re in the clear for the moment. Direct-IO is</span>
<span class="p_add">+	 * the main guy that batches up lots of get_user_pages, and even</span>
<span class="p_add">+	 * they are limited to 64-at-a-time which is not so many.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This doesn&#39;t prevent pagetable teardown, but does prevent</span>
<span class="p_add">+	 * the pagetables and pages from being freed on x86.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * So long as we atomically load page table pointers versus teardown</span>
<span class="p_add">+	 * (which we do on x86, with the above PAE exception), we can follow the</span>
<span class="p_add">+	 * address down to the the page and take a ref on it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd_t pgd = *pgdp;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		if (pgd_none(pgd))</span>
<span class="p_add">+			goto slow;</span>
<span class="p_add">+		if (!gup_p4d_range(pgd, addr, next, write, pages, &amp;nr))</span>
<span class="p_add">+			goto slow;</span>
<span class="p_add">+	} while (pgdp++, addr = next, addr != end);</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(nr != (end - start) &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	return nr;</span>
<span class="p_add">+</span>
<span class="p_add">+	{</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+slow:</span>
<span class="p_add">+		local_irq_enable();</span>
<span class="p_add">+slow_irqon:</span>
<span class="p_add">+		/* Try to get the remaining pages with get_user_pages */</span>
<span class="p_add">+		start += nr &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		pages += nr;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = get_user_pages_unlocked(start,</span>
<span class="p_add">+					      (end - start) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+					      pages, write ? FOLL_WRITE : 0);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Have to be a bit careful with return values */</span>
<span class="p_add">+		if (nr &gt; 0) {</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				ret = nr;</span>
<span class="p_add">+			else</span>
<span class="p_add">+				ret += nr;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index c89f472b658c..9b8fccb969dc 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> config HAVE_MEMBLOCK_NODE_MAP</span>
 config HAVE_MEMBLOCK_PHYS_MAP
 	bool
 
<span class="p_del">-config HAVE_GENERIC_GUP</span>
<span class="p_add">+config HAVE_GENERIC_RCU_GUP</span>
 	bool
 
 config ARCH_DISCARD_MEMBLOCK
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 2559a3987de7..527ec2c6cca3 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -1155,7 +1155,7 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
 #endif /* CONFIG_ELF_CORE */
 
 /*
<span class="p_del">- * Generic Fast GUP</span>
<span class="p_add">+ * Generic RCU Fast GUP</span>
  *
  * get_user_pages_fast attempts to pin user pages by walking the page
  * tables directly and avoids taking locks. Thus the walker needs to be
<span class="p_chunk">@@ -1176,8 +1176,8 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
  * Before activating this code, please be aware that the following assumptions
  * are currently made:
  *
<span class="p_del">- *  *) Either HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to</span>
<span class="p_del">- *  free pages containing page tables or TLB flushing requires IPI broadcast.</span>
<span class="p_add">+ *  *) HAVE_RCU_TABLE_FREE is enabled, and tlb_remove_table is used to free</span>
<span class="p_add">+ *      pages containing page tables.</span>
  *
  *  *) ptes can be read atomically by the architecture.
  *
<span class="p_chunk">@@ -1187,7 +1187,7 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
  *
  * This code is based heavily on the PowerPC implementation by Nick Piggin.
  */
<span class="p_del">-#ifdef CONFIG_HAVE_GENERIC_GUP</span>
<span class="p_add">+#ifdef CONFIG_HAVE_GENERIC_RCU_GUP</span>
 
 #ifndef gup_get_pte
 /*
<span class="p_chunk">@@ -1677,4 +1677,4 @@</span> <span class="p_context"> int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 	return ret;
 }
 
<span class="p_del">-#endif /* CONFIG_HAVE_GENERIC_GUP */</span>
<span class="p_add">+#endif /* CONFIG_HAVE_GENERIC_RCU_GUP */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



