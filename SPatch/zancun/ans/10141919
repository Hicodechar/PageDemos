
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/6] mm, hugetlb: integrate giga hugetlb more naturally to the allocation path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/6] mm, hugetlb: integrate giga hugetlb more naturally to the allocation path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 3, 2018, 9:32 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180103093213.26329-3-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10141919/mbox/"
   >mbox</a>
|
   <a href="/patch/10141919/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10141919/">/patch/10141919/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	97ED160594 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:32:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7EB7228CE4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:32:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7D6F92901B; Wed,  3 Jan 2018 09:32:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E80E02902D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 09:32:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752116AbeACJcf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 3 Jan 2018 04:32:35 -0500
Received: from mail-pl0-f66.google.com ([209.85.160.66]:43654 &quot;EHLO
	mail-pl0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751933AbeACJcb (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 3 Jan 2018 04:32:31 -0500
Received: by mail-pl0-f66.google.com with SMTP id z5so916810plo.10
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 03 Jan 2018 01:32:31 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=ZVP3exBKy8vyVeL4DpnxBrr1TjKD3GaBj1Z1uwBWW4U=;
	b=n1jcLBK49LpdXDoNxEZs12CeByRuhGwABt8SAmn3H9SLHVA5Npx+wzgAEXOGr49YyH
	s3L/Y4G0x0wL9lrtXt6OWiRALgLQJIhu5h1v8jFpm9eKOON8bXJun8dQQwxTfsBawhf6
	fJbYUitPuYj2cjeh+wgz/ZLjaEH5wlTrZZr5TI4bbDctpKxt+3aLxpXD25GFfoRzuKPf
	fgI6R0U/Md2A5R3yUkTNK/BX9y/3FvB8HGXKrn2NOJvcYklEXnWvUpj5y+yu2NO2+KPw
	fb4KOI6D7q0PGQBh2d3D7+AumFn46vB48hqbULhn3oodsK7U2De5Gw7UuvCQuOQorXSG
	OO1w==
X-Gm-Message-State: AKGB3mJeCy/QRyT9NlXrj4qltvZLaYL/kU2CAi/Bv4RUPsDnFhZVfqvX
	84smPNUKBx8mrXi0O4iQ51o=
X-Google-Smtp-Source: ACJfBouLZfZ//UIOxVIgSLfsBfH+DFnZBzYKTg6lWFyXpDkrifuUQ0Uowu4Fo5BJjaSDFRupnUhrVQ==
X-Received: by 10.84.240.130 with SMTP id z2mr840212plk.429.1514971950663;
	Wed, 03 Jan 2018 01:32:30 -0800 (PST)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	c2sm1790628pgq.48.2018.01.03.01.32.28
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 03 Jan 2018 01:32:29 -0800 (PST)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: &lt;linux-mm@kvack.org&gt;, Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 2/6] mm,
	hugetlb: integrate giga hugetlb more naturally to the allocation
	path
Date: Wed,  3 Jan 2018 10:32:09 +0100
Message-Id: &lt;20180103093213.26329-3-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20180103093213.26329-1-mhocko@kernel.org&gt;
References: &lt;20180103093213.26329-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Jan. 3, 2018, 9:32 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Gigantic hugetlb pages were ingrown to the hugetlb code as an alien
specie with a lot of special casing. The allocation path is not an
exception. Unnecessarily so to be honest. It is true that the underlying
allocator is different but that is an implementation detail.

This patch unifies the hugetlb allocation path that a prepares fresh
pool pages. alloc_fresh_gigantic_page basically copies alloc_fresh_huge_page
logic so we can move everything there. This will simplify set_max_huge_pages
which doesn&#39;t have to care about what kind of huge page we allocate.

Changes since RFC
- compile fix for !CONFIG_ARCH_HAS_GIGANTIC_PAGE
<span class="reviewed-by">
Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 mm/hugetlb.c | 55 ++++++++++++++-----------------------------------------
 1 file changed, 14 insertions(+), 41 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8959667f539..360765156c7c 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1106,7 +1106,8 @@</span> <span class="p_context"> static bool zone_spans_last_pfn(const struct zone *zone,</span>
 	return zone_spans_pfn(zone, last_pfn);
 }
 
<span class="p_del">-static struct page *alloc_gigantic_page(int nid, struct hstate *h)</span>
<span class="p_add">+static struct page *alloc_gigantic_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+		int nid, nodemask_t *nodemask)</span>
 {
 	unsigned int order = huge_page_order(h);
 	unsigned long nr_pages = 1 &lt;&lt; order;
<span class="p_chunk">@@ -1114,11 +1115,9 @@</span> <span class="p_context"> static struct page *alloc_gigantic_page(int nid, struct hstate *h)</span>
 	struct zonelist *zonelist;
 	struct zone *zone;
 	struct zoneref *z;
<span class="p_del">-	gfp_t gfp_mask;</span>
 
<span class="p_del">-	gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;</span>
 	zonelist = node_zonelist(nid, gfp_mask);
<span class="p_del">-	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), NULL) {</span>
<span class="p_add">+	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nodemask) {</span>
 		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
 
 		pfn = ALIGN(zone-&gt;zone_start_pfn, nr_pages);
<span class="p_chunk">@@ -1149,42 +1148,13 @@</span> <span class="p_context"> static struct page *alloc_gigantic_page(int nid, struct hstate *h)</span>
 static void prep_new_huge_page(struct hstate *h, struct page *page, int nid);
 static void prep_compound_gigantic_page(struct page *page, unsigned int order);
 
<span class="p_del">-static struct page *alloc_fresh_gigantic_page_node(struct hstate *h, int nid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-</span>
<span class="p_del">-	page = alloc_gigantic_page(nid, h);</span>
<span class="p_del">-	if (page) {</span>
<span class="p_del">-		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="p_del">-		prep_new_huge_page(h, page, nid);</span>
<span class="p_del">-		put_page(page); /* free it into the hugepage allocator */</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int alloc_fresh_gigantic_page(struct hstate *h,</span>
<span class="p_del">-				nodemask_t *nodes_allowed)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_del">-	int nr_nodes, node;</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {</span>
<span class="p_del">-		page = alloc_fresh_gigantic_page_node(h, node);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return 1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #else /* !CONFIG_ARCH_HAS_GIGANTIC_PAGE */
 static inline bool gigantic_page_supported(void) { return false; }
<span class="p_add">+static struct page *alloc_gigantic_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+		int nid, nodemask_t *nodemask) { return NULL; }</span>
 static inline void free_gigantic_page(struct page *page, unsigned int order) { }
 static inline void destroy_compound_gigantic_page(struct page *page,
 						unsigned int order) { }
<span class="p_del">-static inline int alloc_fresh_gigantic_page(struct hstate *h,</span>
<span class="p_del">-					nodemask_t *nodes_allowed) { return 0; }</span>
 #endif
 
 static void update_and_free_page(struct hstate *h, struct page *page)
<span class="p_chunk">@@ -1410,8 +1380,12 @@</span> <span class="p_context"> static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
 	gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
 
 	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_del">-		page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_del">-				node, nodes_allowed);</span>
<span class="p_add">+		if (hstate_is_gigantic(h))</span>
<span class="p_add">+			page = alloc_gigantic_page(h, gfp_mask,</span>
<span class="p_add">+					node, nodes_allowed);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_add">+					node, nodes_allowed);</span>
 		if (page)
 			break;
 
<span class="p_chunk">@@ -1420,6 +1394,8 @@</span> <span class="p_context"> static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
 	if (!page)
 		return 0;
 
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		prep_compound_gigantic_page(page, huge_page_order(h));</span>
 	prep_new_huge_page(h, page, page_to_nid(page));
 	put_page(page); /* free it into the hugepage allocator */
 
<span class="p_chunk">@@ -2307,10 +2283,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 		/* yield cpu to avoid soft lockup */
 		cond_resched();
 
<span class="p_del">-		if (hstate_is_gigantic(h))</span>
<span class="p_del">-			ret = alloc_fresh_gigantic_page(h, nodes_allowed);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			ret = alloc_fresh_huge_page(h, nodes_allowed);</span>
<span class="p_add">+		ret = alloc_fresh_huge_page(h, nodes_allowed);</span>
 		spin_lock(&amp;hugetlb_lock);
 		if (!ret)
 			goto out;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



