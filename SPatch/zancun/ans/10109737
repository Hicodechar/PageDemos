
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv4,12/12] mm/thp: Remove pmd_huge_split_prepare - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv4,12/12] mm/thp: Remove pmd_huge_split_prepare</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 13, 2017, 10:57 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171213105756.69879-13-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10109737/mbox/"
   >mbox</a>
|
   <a href="/patch/10109737/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10109737/">/patch/10109737/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	04A5460352 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Dec 2017 10:59:27 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EB2C5286A3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Dec 2017 10:59:26 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DFDAE28F88; Wed, 13 Dec 2017 10:59:26 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 20B4A286A3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Dec 2017 10:59:25 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752935AbdLMK7W (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 13 Dec 2017 05:59:22 -0500
Received: from mga09.intel.com ([134.134.136.24]:3274 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752543AbdLMK7Q (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 13 Dec 2017 05:59:16 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	13 Dec 2017 02:59:15 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.45,397,1508828400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;13085790&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga001.fm.intel.com with ESMTP; 13 Dec 2017 02:59:12 -0800
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 7EE5F7C2; Wed, 13 Dec 2017 12:58:05 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;, Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, linux-arch@vger.kernel.org,
	linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv4 12/12] mm/thp: Remove pmd_huge_split_prepare
Date: Wed, 13 Dec 2017 13:57:56 +0300
Message-Id: &lt;20171213105756.69879-13-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.15.0
In-Reply-To: &lt;20171213105756.69879-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20171213105756.69879-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Dec. 13, 2017, 10:57 a.m.</div>
<pre class="content">
<span class="from">From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>

Instead of marking the pmd ready for split, invalidate the pmd. This should
take care of powerpc requirement. Only side effect is that we mark the pmd
invalid early. This can result in us blocking access to the page a bit longer
if we race against a thp split.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
[kirill.shutemov@linux.intel.com: rebased, dirty THP once]
<span class="signed-off-by">Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/powerpc/include/asm/book3s/64/hash-4k.h  |  2 -
 arch/powerpc/include/asm/book3s/64/hash-64k.h |  2 -
 arch/powerpc/include/asm/book3s/64/pgtable.h  |  9 ----
 arch/powerpc/include/asm/book3s/64/radix.h    |  6 ---
 arch/powerpc/mm/pgtable-hash64.c              | 22 --------
 include/asm-generic/pgtable.h                 |  8 ---
 mm/huge_memory.c                              | 74 +++++++++++++--------------
 7 files changed, 36 insertions(+), 87 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/hash-4k.h b/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_header">index 197ced1eaaa0..2d9df40446f6 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_chunk">@@ -101,8 +101,6 @@</span> <span class="p_context"> extern pmd_t hash__pmdp_collapse_flush(struct vm_area_struct *vma,</span>
 extern void hash__pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 					 pgtable_t pgtable);
 extern pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);
<span class="p_del">-extern void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-				      unsigned long address, pmd_t *pmdp);</span>
 extern pmd_t hash__pmdp_huge_get_and_clear(struct mm_struct *mm,
 				       unsigned long addr, pmd_t *pmdp);
 extern int hash__has_transparent_hugepage(void);
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/hash-64k.h b/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_header">index 8d40cf03cb67..cb46d1034f33 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_chunk">@@ -203,8 +203,6 @@</span> <span class="p_context"> extern pmd_t hash__pmdp_collapse_flush(struct vm_area_struct *vma,</span>
 extern void hash__pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 					 pgtable_t pgtable);
 extern pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);
<span class="p_del">-extern void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-				      unsigned long address, pmd_t *pmdp);</span>
 extern pmd_t hash__pmdp_huge_get_and_clear(struct mm_struct *mm,
 				       unsigned long addr, pmd_t *pmdp);
 extern int hash__has_transparent_hugepage(void);
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/pgtable.h b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">index ee19d5bbee06..6ca1208cedcb 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_chunk">@@ -1140,15 +1140,6 @@</span> <span class="p_context"> static inline pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm,</span>
 extern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 			     pmd_t *pmdp);
 
<span class="p_del">-#define __HAVE_ARCH_PMDP_HUGE_SPLIT_PREPARE</span>
<span class="p_del">-static inline void pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (radix_enabled())</span>
<span class="p_del">-		return radix__pmdp_huge_split_prepare(vma, address, pmdp);</span>
<span class="p_del">-	return hash__pmdp_huge_split_prepare(vma, address, pmdp);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define pmd_move_must_withdraw pmd_move_must_withdraw
 struct spinlock;
 static inline int pmd_move_must_withdraw(struct spinlock *new_pmd_ptl,
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/radix.h b/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_header">index 19c44e1495ae..365010f66570 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_chunk">@@ -269,12 +269,6 @@</span> <span class="p_context"> static inline pmd_t radix__pmd_mkhuge(pmd_t pmd)</span>
 		return __pmd(pmd_val(pmd) | _PAGE_PTE | R_PAGE_LARGE);
 	return __pmd(pmd_val(pmd) | _PAGE_PTE);
 }
<span class="p_del">-static inline void radix__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					    unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Nothing to do for radix. */</span>
<span class="p_del">-	return;</span>
<span class="p_del">-}</span>
 
 extern unsigned long radix__pmd_hugepage_update(struct mm_struct *mm, unsigned long addr,
 					  pmd_t *pmdp, unsigned long clr,
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable-hash64.c b/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_header">index ec277913e01b..469808e77e58 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_chunk">@@ -296,28 +296,6 @@</span> <span class="p_context"> pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)</span>
 	return pgtable;
 }
 
<span class="p_del">-void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-			       unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_del">-	VM_BUG_ON(REGION_ID(address) != USER_REGION_ID);</span>
<span class="p_del">-	VM_BUG_ON(pmd_devmap(*pmdp));</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We can&#39;t mark the pmd none here, because that will cause a race</span>
<span class="p_del">-	 * against exit_mmap. We need to continue mark pmd TRANS HUGE, while</span>
<span class="p_del">-	 * we spilt, but at the same time we wan&#39;t rest of the ppc64 code</span>
<span class="p_del">-	 * not to insert hash pte on this, because we will be modifying</span>
<span class="p_del">-	 * the deposited pgtable in the caller of this function. Hence</span>
<span class="p_del">-	 * clear the _PAGE_USER so that we move the fault handling to</span>
<span class="p_del">-	 * higher level function and that will serialize against ptl.</span>
<span class="p_del">-	 * We need to flush existing hash pte entries here even though,</span>
<span class="p_del">-	 * the translation is still valid, because we will withdraw</span>
<span class="p_del">-	 * pgtable_t after this.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pmd_hugepage_update(vma-&gt;vm_mm, address, pmdp, 0, _PAGE_PRIVILEGED);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * A linux hugepage PMD was changed and the corresponding hash table entries
  * neesd to be flushed.
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index f449c71cbdc0..687d5719d8ee 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -329,14 +329,6 @@</span> <span class="p_context"> extern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,</span>
 			    pmd_t *pmdp);
 #endif
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_HUGE_SPLIT_PREPARE</span>
<span class="p_del">-static inline void pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #ifndef __HAVE_ARCH_PTE_SAME
 static inline int pte_same(pte_t pte_a, pte_t pte_b)
 {
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 10278d03d60f..10ea2e63ef33 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2063,7 +2063,7 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct page *page;
 	pgtable_t pgtable;
<span class="p_del">-	pmd_t old, _pmd;</span>
<span class="p_add">+	pmd_t old_pmd, _pmd;</span>
 	bool young, write, soft_dirty, pmd_migration = false;
 	unsigned long addr;
 	int i;
<span class="p_chunk">@@ -2106,23 +2106,51 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		return __split_huge_zero_page_pmd(vma, haddr, pmd);
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Up to this point the pmd is present and huge and userland has the</span>
<span class="p_add">+	 * whole access to the hugepage during the split (which happens in</span>
<span class="p_add">+	 * place). If we overwrite the pmd with the not-huge version pointing</span>
<span class="p_add">+	 * to the pte here (which of course we could if all CPUs were bug</span>
<span class="p_add">+	 * free), userland could trigger a small page size TLB miss on the</span>
<span class="p_add">+	 * small sized TLB while the hugepage TLB entry is still established in</span>
<span class="p_add">+	 * the huge TLB. Some CPU doesn&#39;t like that.</span>
<span class="p_add">+	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum</span>
<span class="p_add">+	 * 383 on page 93. Intel should be safe but is also warns that it&#39;s</span>
<span class="p_add">+	 * only safe if the permission and cache attributes of the two entries</span>
<span class="p_add">+	 * loaded in the two TLB is identical (which should be the case here).</span>
<span class="p_add">+	 * But it is generally safer to never allow small and huge TLB entries</span>
<span class="p_add">+	 * for the same virtual address to be loaded simultaneously. So instead</span>
<span class="p_add">+	 * of doing &quot;pmd_populate(); flush_pmd_tlb_range();&quot; we first mark the</span>
<span class="p_add">+	 * current pmd notpresent (atomically because here the pmd_trans_huge</span>
<span class="p_add">+	 * and pmd_trans_splitting must remain set at all times on the pmd</span>
<span class="p_add">+	 * until the split is complete for this pmd), then we flush the SMP TLB</span>
<span class="p_add">+	 * and finally we write the non-huge version of the pmd entry with</span>
<span class="p_add">+	 * pmd_populate.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	old_pmd = pmdp_invalidate(vma, haddr, pmd);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
<span class="p_del">-	pmd_migration = is_pmd_migration_entry(*pmd);</span>
<span class="p_add">+	pmd_migration = is_pmd_migration_entry(old_pmd);</span>
 	if (pmd_migration) {
 		swp_entry_t entry;
 
<span class="p_del">-		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		entry = pmd_to_swp_entry(old_pmd);</span>
 		page = pfn_to_page(swp_offset(entry));
 	} else
 #endif
<span class="p_del">-		page = pmd_page(*pmd);</span>
<span class="p_add">+		page = pmd_page(old_pmd);</span>
 	VM_BUG_ON_PAGE(!page_count(page), page);
 	page_ref_add(page, HPAGE_PMD_NR - 1);
<span class="p_del">-	write = pmd_write(*pmd);</span>
<span class="p_del">-	young = pmd_young(*pmd);</span>
<span class="p_del">-	soft_dirty = pmd_soft_dirty(*pmd);</span>
<span class="p_add">+	if (pmd_dirty(old_pmd))</span>
<span class="p_add">+		SetPageDirty(page);</span>
<span class="p_add">+	write = pmd_write(old_pmd);</span>
<span class="p_add">+	young = pmd_young(old_pmd);</span>
<span class="p_add">+	soft_dirty = pmd_soft_dirty(old_pmd);</span>
 
<span class="p_del">-	pmdp_huge_split_prepare(vma, haddr, pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Withdraw the table only after we mark the pmd entry invalid.</span>
<span class="p_add">+	 * This&#39;s critical for some architectures (Power).</span>
<span class="p_add">+	 */</span>
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 	pmd_populate(mm, &amp;_pmd, pgtable);
 
<span class="p_chunk">@@ -2176,36 +2204,6 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	}
 
 	smp_wmb(); /* make pte visible before pmd */
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Up to this point the pmd is present and huge and userland has the</span>
<span class="p_del">-	 * whole access to the hugepage during the split (which happens in</span>
<span class="p_del">-	 * place). If we overwrite the pmd with the not-huge version pointing</span>
<span class="p_del">-	 * to the pte here (which of course we could if all CPUs were bug</span>
<span class="p_del">-	 * free), userland could trigger a small page size TLB miss on the</span>
<span class="p_del">-	 * small sized TLB while the hugepage TLB entry is still established in</span>
<span class="p_del">-	 * the huge TLB. Some CPU doesn&#39;t like that.</span>
<span class="p_del">-	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum</span>
<span class="p_del">-	 * 383 on page 93. Intel should be safe but is also warns that it&#39;s</span>
<span class="p_del">-	 * only safe if the permission and cache attributes of the two entries</span>
<span class="p_del">-	 * loaded in the two TLB is identical (which should be the case here).</span>
<span class="p_del">-	 * But it is generally safer to never allow small and huge TLB entries</span>
<span class="p_del">-	 * for the same virtual address to be loaded simultaneously. So instead</span>
<span class="p_del">-	 * of doing &quot;pmd_populate(); flush_pmd_tlb_range();&quot; we first mark the</span>
<span class="p_del">-	 * current pmd notpresent (atomically because here the pmd_trans_huge</span>
<span class="p_del">-	 * and pmd_trans_splitting must remain set at all times on the pmd</span>
<span class="p_del">-	 * until the split is complete for this pmd), then we flush the SMP TLB</span>
<span class="p_del">-	 * and finally we write the non-huge version of the pmd entry with</span>
<span class="p_del">-	 * pmd_populate.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	old = pmdp_invalidate(vma, haddr, pmd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Transfer dirty bit using value returned by pmd_invalidate() to be</span>
<span class="p_del">-	 * sure we don&#39;t race with CPU that can set the bit under us.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (pmd_dirty(old))</span>
<span class="p_del">-		SetPageDirty(page);</span>
<span class="p_del">-</span>
 	pmd_populate(mm, pmd, pgtable);
 
 	if (freeze) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



