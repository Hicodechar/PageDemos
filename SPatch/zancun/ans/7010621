
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[07/15] HMM: mm add helper to update page table when migrating memory v2. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [07/15] HMM: mm add helper to update page table when migrating memory v2.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 13, 2015, 7:37 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1439494651-1255-8-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7010621/mbox/"
   >mbox</a>
|
   <a href="/patch/7010621/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7010621/">/patch/7010621/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 7D51BC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 13 Aug 2015 19:40:55 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id E9F0D202FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 13 Aug 2015 19:40:53 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 2228B20546
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 13 Aug 2015 19:40:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754371AbbHMTkn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 13 Aug 2015 15:40:43 -0400
Received: from mx1.redhat.com ([209.132.183.28]:40423 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754153AbbHMTiC (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 13 Aug 2015 15:38:02 -0400
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by mx1.redhat.com (Postfix) with ESMTPS id 143848E383;
	Thu, 13 Aug 2015 19:38:02 +0000 (UTC)
Received: from dhcp-10-19-62-215.boston.devel.redhat.com
	(dhcp40-164.desklab.eng.bos.redhat.com [10.19.40.164] (may be
	forged))
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id t7DJbYNi032173; Thu, 13 Aug 2015 15:37:58 -0400
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;, &lt;joro@8bytes.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Johannes Weiner &lt;jweiner@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Airlie &lt;airlied@redhat.com&gt;, Brendan Conoboy &lt;blc@redhat.com&gt;,
	Joe Donohue &lt;jdonohue@redhat.com&gt;, Christophe Harle &lt;charle@nvidia.com&gt;,
	Duncan Poole &lt;dpoole@nvidia.com&gt;, Sherry Cheung &lt;SCheung@nvidia.com&gt;,
	Subhash Gutti &lt;sgutti@nvidia.com&gt;, John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Lucien Dunning &lt;ldunning@nvidia.com&gt;,
	Cameron Buschardt &lt;cabuschardt@nvidia.com&gt;,
	Arvind Gopalakrishnan &lt;arvindg@nvidia.com&gt;,
	Haggai Eran &lt;haggaie@mellanox.com&gt;,
	Shachar Raindel &lt;raindel@mellanox.com&gt;, Liran Liss &lt;liranl@mellanox.com&gt;,
	Roland Dreier &lt;roland@purestorage.com&gt;, Ben Sander &lt;ben.sander@amd.com&gt;,
	Greg Stoner &lt;Greg.Stoner@amd.com&gt;, John Bridgman &lt;John.Bridgman@amd.com&gt;,
	Michael Mantor &lt;Michael.Mantor@amd.com&gt;,
	Paul Blinzer &lt;Paul.Blinzer@amd.com&gt;,
	Leonid Shamis &lt;Leonid.Shamis@amd.com&gt;,
	Laurent Morichetti &lt;Laurent.Morichetti@amd.com&gt;,
	Alexander Deucher &lt;Alexander.Deucher@amd.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
Subject: [PATCH 07/15] HMM: mm add helper to update page table when
	migrating memory v2.
Date: Thu, 13 Aug 2015 15:37:23 -0400
Message-Id: &lt;1439494651-1255-8-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1439494651-1255-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1439494651-1255-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Aug. 13, 2015, 7:37 p.m.</div>
<pre class="content">
For doing memory migration to remote memory we need to unmap range
of anonymous memory from CPU page table and replace page table entry
with special HMM entry.

This is a multi-stage process, first we save and replace page table
entry with special HMM entry, also flushing tlb in the process. If
we run into non allocated entry we either use the zero page or we
allocate new page. For swaped entry we try to swap them in.

Once we have set the page table entry to the special entry we check
the page backing each of the address to make sure that only page
table mappings are holding reference on the page, which means we
can safely migrate the page to device memory. Because the CPU page
table entry are special entry, no get_user_pages() can reference
the page anylonger. So we are safe from race on that front. Note
that the page can still be referenced by get_user_pages() from
other process but in that case the page is write protected and
as we do not drop the mapcount nor the page count we know that
all user of get_user_pages() are only doing read only access (on
write access they would allocate a new page).

Once we have identified all the page that are safe to migrate the
first function return and let HMM schedule the migration with the
device driver.

Finaly there is a cleanup function that will drop the mapcount and
reference count on all page that have been successfully migrated,
or restore the page table entry otherwise.

Changed since v1:
  - Fix pmd/pte allocation when migrating.
  - Fix reverse logic on mm_forbids_zeropage()
  - Add comment on why we add to lru list new page.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
---
 include/linux/mm.h |  14 ++
 mm/memory.c        | 471 +++++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 485 insertions(+)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index eb1e9b2..0a6a292 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2250,6 +2250,20 @@</span> <span class="p_context"> static inline void hmm_mm_init(struct mm_struct *mm)</span>
 	mm-&gt;hmm = NULL;
 }
 
<span class="p_add">+int mm_hmm_migrate(struct mm_struct *mm,</span>
<span class="p_add">+		   struct vm_area_struct *vma,</span>
<span class="p_add">+		   pte_t *save_pte,</span>
<span class="p_add">+		   bool *backoff,</span>
<span class="p_add">+		   const void *mmu_notifier_exclude,</span>
<span class="p_add">+		   unsigned long start,</span>
<span class="p_add">+		   unsigned long end);</span>
<span class="p_add">+void mm_hmm_migrate_cleanup(struct mm_struct *mm,</span>
<span class="p_add">+			    struct vm_area_struct *vma,</span>
<span class="p_add">+			    pte_t *save_pte,</span>
<span class="p_add">+			    dma_addr_t *hmm_pte,</span>
<span class="p_add">+			    unsigned long start,</span>
<span class="p_add">+			    unsigned long end);</span>
<span class="p_add">+</span>
 int mm_hmm_migrate_back(struct mm_struct *mm,
 			struct vm_area_struct *vma,
 			pte_t *new_pte,
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index b2b3677..d34e610 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -54,6 +54,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/memcontrol.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
 #include &lt;linux/hmm.h&gt;
<span class="p_add">+#include &lt;linux/hmm_pt.h&gt;</span>
 #include &lt;linux/kallsyms.h&gt;
 #include &lt;linux/swapops.h&gt;
 #include &lt;linux/elf.h&gt;
<span class="p_chunk">@@ -3723,6 +3724,476 @@</span> <span class="p_context"> void mm_hmm_migrate_back_cleanup(struct mm_struct *mm,</span>
 	}
 }
 EXPORT_SYMBOL(mm_hmm_migrate_back_cleanup);
<span class="p_add">+</span>
<span class="p_add">+/* mm_hmm_migrate() - unmap range and set special HMM pte for it.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: The mm struct.</span>
<span class="p_add">+ * @vma: The vm area struct the range is in.</span>
<span class="p_add">+ * @save_pte: array where to save current CPU page table entry value.</span>
<span class="p_add">+ * @backoff: Pointer toward a boolean indicating that we need to stop.</span>
<span class="p_add">+ * @exclude: The mmu_notifier listener to exclude from mmu_notifier callback.</span>
<span class="p_add">+ * @start: Start address of the range (inclusive).</span>
<span class="p_add">+ * @end: End address of the range (exclusive).</span>
<span class="p_add">+ * Returns: 0 on success, -EINVAL if some argument where invalid, -ENOMEM if</span>
<span class="p_add">+ * it failed allocating memory for performing the operation, -EFAULT if some</span>
<span class="p_add">+ * memory backing the range is in bad state, -EAGAIN if backoff flag turned</span>
<span class="p_add">+ * to true.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The process of memory migration is bit involve, first we must set all CPU</span>
<span class="p_add">+ * page table entry to the special HMM locked entry ensuring us exclusive</span>
<span class="p_add">+ * control over the page table entry (ie no other process can change the page</span>
<span class="p_add">+ * table but us).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * While doing that we must handle empty and swaped entry. For empty entry we</span>
<span class="p_add">+ * either use the zero page or allocate a new page. For swap entry we call</span>
<span class="p_add">+ * __handle_mm_fault() to try to faultin the page (swap entry can be a number</span>
<span class="p_add">+ * of thing).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Once we have unmapped we need to check that we can effectively migrate the</span>
<span class="p_add">+ * page, by testing that no one is holding a reference on the page beside the</span>
<span class="p_add">+ * reference taken by each page mapping.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On success every valid entry inside save_pte array is an entry that can be</span>
<span class="p_add">+ * migrated.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that this function does not free any of the page, nor does it updates</span>
<span class="p_add">+ * the various memcg counter (exception being for accounting new allocation).</span>
<span class="p_add">+ * This happen inside the mm_hmm_migrate_cleanup() function.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+int mm_hmm_migrate(struct mm_struct *mm,</span>
<span class="p_add">+		   struct vm_area_struct *vma,</span>
<span class="p_add">+		   pte_t *save_pte,</span>
<span class="p_add">+		   bool *backoff,</span>
<span class="p_add">+		   const void *mmu_notifier_exclude,</span>
<span class="p_add">+		   unsigned long start,</span>
<span class="p_add">+		   unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t hmm_entry = swp_entry_to_pte(make_hmm_entry_locked());</span>
<span class="p_add">+	struct mmu_notifier_range range = {</span>
<span class="p_add">+		.start = start,</span>
<span class="p_add">+		.end = end,</span>
<span class="p_add">+		.event = MMU_MIGRATE,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	unsigned long addr = start, i;</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Only allow anonymous mapping and sanity check arguments. */</span>
<span class="p_add">+	if (vma-&gt;vm_ops || unlikely(vma-&gt;vm_flags &amp; (VM_PFNMAP|VM_MIXEDMAP)))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	end = PAGE_ALIGN(end);</span>
<span class="p_add">+	if (start &gt;= end || end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Only need to test on the last address of the range. */</span>
<span class="p_add">+	if (check_stack_guard_page(vma, end) &lt; 0)</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Try to fail early on. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma)))</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, range.start, range.end);</span>
<span class="p_add">+	update_hiwater_rss(mm);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start_excluding(mm, &amp;range,</span>
<span class="p_add">+						      mmu_notifier_exclude);</span>
<span class="p_add">+	tlb_start_vma(&amp;tlb, vma);</span>
<span class="p_add">+	for (addr = range.start, i = 0; addr &lt; end &amp;&amp; !ret;) {</span>
<span class="p_add">+		unsigned long cstart, next, npages = 0;</span>
<span class="p_add">+		spinlock_t *ptl;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Pretty much the exact same logic as __handle_mm_fault(),</span>
<span class="p_add">+		 * exception being the handling of huge pmd.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		pudp = pud_alloc(mm, pgdp, addr);</span>
<span class="p_add">+		if (!pudp) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pmdp = pmd_alloc(mm, pudp, addr);</span>
<span class="p_add">+		if (!pmdp) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (unlikely(pmd_trans_splitting(*pmdp))) {</span>
<span class="p_add">+			wait_split_huge_page(vma-&gt;anon_vma, pmdp);</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (unlikely(pmd_none(*pmdp)) &amp;&amp;</span>
<span class="p_add">+		    unlikely(__pte_alloc(mm, vma, pmdp, addr))) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If an huge pmd materialized from under us split it and break</span>
<span class="p_add">+		 * out of the loop to retry.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (unlikely(pmd_trans_huge(*pmdp))) {</span>
<span class="p_add">+			split_huge_page_pmd(vma, addr, pmdp);</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * A regular pmd is established and it can&#39;t morph into a huge</span>
<span class="p_add">+		 * pmd from under us anymore at this point because we hold the</span>
<span class="p_add">+		 * mmap_sem read mode and khugepaged takes it in write mode. So</span>
<span class="p_add">+		 * now it&#39;s safe to run pte_offset_map().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+		for (i = (addr - start) &gt;&gt; PAGE_SHIFT, cstart = addr,</span>
<span class="p_add">+		     next = min((addr + PMD_SIZE) &amp; PMD_MASK, end);</span>
<span class="p_add">+		     addr &lt; next; addr += PAGE_SIZE, ptep++, i++) {</span>
<span class="p_add">+			save_pte[i] = ptep_get_and_clear(mm, addr, ptep);</span>
<span class="p_add">+			tlb_remove_tlb_entry(&amp;tlb, ptep, addr);</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, hmm_entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pte_present(save_pte[i]))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!pte_none(save_pte[i])) {</span>
<span class="p_add">+				set_pte_at(mm, addr, ptep, save_pte[i]);</span>
<span class="p_add">+				ret = -ENOENT;</span>
<span class="p_add">+				ptep++;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * TODO: This mm_forbids_zeropage() really does not</span>
<span class="p_add">+			 * apply to us. First it seems only S390 have it set,</span>
<span class="p_add">+			 * second we are not even using the zero page entry</span>
<span class="p_add">+			 * to populate the CPU page table, thought on error</span>
<span class="p_add">+			 * we might use the save_pte entry to set the CPU</span>
<span class="p_add">+			 * page table entry.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * Live with that oddity for now.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (mm_forbids_zeropage(mm)) {</span>
<span class="p_add">+				pte_clear(mm, addr, &amp;save_pte[i]);</span>
<span class="p_add">+				npages++;</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			save_pte[i] = pte_mkspecial(pfn_pte(my_zero_pfn(addr),</span>
<span class="p_add">+						    vma-&gt;vm_page_prot));</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * So we must allocate pages before checking for error, which</span>
<span class="p_add">+		 * here indicate that one entry is a swap entry. We need to</span>
<span class="p_add">+		 * allocate first because otherwise there is no easy way to</span>
<span class="p_add">+		 * know on retry or in error code path wether the CPU page</span>
<span class="p_add">+		 * table locked HMM entry is ours or from some other thread.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!npages)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (next = addr, addr = cstart,</span>
<span class="p_add">+		     i = (addr - start) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		     addr &lt; next; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+			struct mem_cgroup *memcg;</span>
<span class="p_add">+			struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pte_present(save_pte[i]) || !pte_none(save_pte[i]))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			page = alloc_zeroed_user_highpage_movable(vma, addr);</span>
<span class="p_add">+			if (!page) {</span>
<span class="p_add">+				ret = -ENOMEM;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			__SetPageUptodate(page);</span>
<span class="p_add">+			if (mem_cgroup_try_charge(page, mm, GFP_KERNEL,</span>
<span class="p_add">+						  &amp;memcg)) {</span>
<span class="p_add">+				page_cache_release(page);</span>
<span class="p_add">+				ret = -ENOMEM;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			save_pte[i] = mk_pte(page, vma-&gt;vm_page_prot);</span>
<span class="p_add">+			if (vma-&gt;vm_flags &amp; VM_WRITE)</span>
<span class="p_add">+				save_pte[i] = pte_mkwrite(save_pte[i]);</span>
<span class="p_add">+			inc_mm_counter_fast(mm, MM_ANONPAGES);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Because we set the page table entry to the special</span>
<span class="p_add">+			 * HMM locked entry we know no other process might do</span>
<span class="p_add">+			 * anything with it and thus we can safely account the</span>
<span class="p_add">+			 * page without holding any lock at this point.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			page_add_new_anon_rmap(page, vma, addr);</span>
<span class="p_add">+			mem_cgroup_commit_charge(page, memcg, false);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Add to active list so we know vmscan will not waste</span>
<span class="p_add">+			 * its time with that page while we are still using it.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			lru_cache_add_active_or_unevictable(page, vma);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	tlb_end_vma(&amp;tlb, vma);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end_excluding(mm, &amp;range,</span>
<span class="p_add">+						    mmu_notifier_exclude);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, range.start, range.end);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (backoff &amp;&amp; *backoff) {</span>
<span class="p_add">+		/* Stick to the range we updated. */</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		end = addr;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if something is missing or something went wrong. */</span>
<span class="p_add">+	if (ret == -ENOENT) {</span>
<span class="p_add">+		int flags = FAULT_FLAG_ALLOW_RETRY;</span>
<span class="p_add">+</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Using __handle_mm_fault() as current-&gt;mm != mm ie we</span>
<span class="p_add">+			 * might have been call from a kernel thread on behalf</span>
<span class="p_add">+			 * of a driver and all accounting handle_mm_fault() is</span>
<span class="p_add">+			 * pointless in our case.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = __handle_mm_fault(mm, vma, addr, flags);</span>
<span class="p_add">+			flags |= FAULT_FLAG_TRIED;</span>
<span class="p_add">+		} while ((ret &amp; VM_FAULT_RETRY));</span>
<span class="p_add">+		if ((ret &amp; VM_FAULT_ERROR)) {</span>
<span class="p_add">+			/* Stick to the range we updated. */</span>
<span class="p_add">+			end = addr;</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		range.start = addr;</span>
<span class="p_add">+		goto retry;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (ret == -EAGAIN) {</span>
<span class="p_add">+		range.start = addr;</span>
<span class="p_add">+		goto retry;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		/* Stick to the range we updated. */</span>
<span class="p_add">+		end = addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point no one else can take a reference on the page from this</span>
<span class="p_add">+	 * process CPU page table. So we can safely check wether we can migrate</span>
<span class="p_add">+	 * or not the page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	for (addr = start, i = 0; addr &lt; end;) {</span>
<span class="p_add">+		unsigned long next;</span>
<span class="p_add">+		spinlock_t *ptl;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We know for certain that we did set special swap entry for</span>
<span class="p_add">+		 * the range and HMM entry are mark as locked so it means that</span>
<span class="p_add">+		 * no one beside us can modify them which apply that all level</span>
<span class="p_add">+		 * of the CPU page table are valid.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		pudp = pud_offset(pgdp, addr);</span>
<span class="p_add">+		VM_BUG_ON(!pudp);</span>
<span class="p_add">+		pmdp = pmd_offset(pudp, addr);</span>
<span class="p_add">+		VM_BUG_ON(!pmdp || pmd_bad(*pmdp) || pmd_none(*pmdp) ||</span>
<span class="p_add">+			  pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+		for (next = min((addr + PMD_SIZE) &amp; PMD_MASK, end),</span>
<span class="p_add">+		     i = (addr - start) &gt;&gt; PAGE_SHIFT; addr &lt; next;</span>
<span class="p_add">+		     addr += PAGE_SIZE, ptep++, i++) {</span>
<span class="p_add">+			struct page *page;</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+			int swapped;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pte_to_swp_entry(save_pte[i]);</span>
<span class="p_add">+			if (is_hmm_entry(entry)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Logic here is pretty involve. If save_pte is</span>
<span class="p_add">+				 * an HMM special swap entry then it means that</span>
<span class="p_add">+				 * we failed to swap in that page so error must</span>
<span class="p_add">+				 * be set.</span>
<span class="p_add">+				 *</span>
<span class="p_add">+				 * If that&#39;s not the case than it means we are</span>
<span class="p_add">+				 * seriously screw.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				VM_BUG_ON(!ret);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This can not happen, no one else can replace our</span>
<span class="p_add">+			 * special entry and as range end is re-ajusted on</span>
<span class="p_add">+			 * error.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			entry = pte_to_swp_entry(*ptep);</span>
<span class="p_add">+			VM_BUG_ON(!is_hmm_entry_locked(entry));</span>
<span class="p_add">+</span>
<span class="p_add">+			/* On error or backoff restore all the saved pte. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto restore;</span>
<span class="p_add">+</span>
<span class="p_add">+			page = vm_normal_page(vma, addr, save_pte[i]);</span>
<span class="p_add">+			/* The zero page is fine to migrate. */</span>
<span class="p_add">+			if (!page)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Check that only CPU mapping hold a reference on the</span>
<span class="p_add">+			 * page. To make thing simpler we just refuse bail out</span>
<span class="p_add">+			 * if page_mapcount() != page_count() (also accounting</span>
<span class="p_add">+			 * for swap cache).</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * There is a small window here where wp_page_copy()</span>
<span class="p_add">+			 * might have decremented mapcount but have not yet</span>
<span class="p_add">+			 * decremented the page count. This is not an issue as</span>
<span class="p_add">+			 * we backoff in that case.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			swapped = PageSwapCache(page);</span>
<span class="p_add">+			if (page_mapcount(page) + swapped == page_count(page))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+restore:</span>
<span class="p_add">+			/* Ok we have to restore that page. */</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, save_pte[i]);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * No need to invalidate - it was non-present</span>
<span class="p_add">+			 * before.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			update_mmu_cache(vma, addr, ptep);</span>
<span class="p_add">+			pte_clear(mm, addr, &amp;save_pte[i]);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mm_hmm_migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+/* mm_hmm_migrate_cleanup() - unmap range cleanup.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: The mm struct.</span>
<span class="p_add">+ * @vma: The vm area struct the range is in.</span>
<span class="p_add">+ * @save_pte: Array where to save current CPU page table entry value.</span>
<span class="p_add">+ * @hmm_pte: Array of HMM table entry indicating if migration was successful.</span>
<span class="p_add">+ * @start: Start address of the range (inclusive).</span>
<span class="p_add">+ * @end: End address of the range (exclusive).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is call after mm_hmm_migrate() and after effective migration. It will</span>
<span class="p_add">+ * restore CPU page table entry for page that not been migrated or in case of</span>
<span class="p_add">+ * failure.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It will free pages that have been migrated and updates appropriate counters,</span>
<span class="p_add">+ * it will also &quot;unlock&quot; special HMM pte entry.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void mm_hmm_migrate_cleanup(struct mm_struct *mm,</span>
<span class="p_add">+			    struct vm_area_struct *vma,</span>
<span class="p_add">+			    pte_t *save_pte,</span>
<span class="p_add">+			    dma_addr_t *hmm_pte,</span>
<span class="p_add">+			    unsigned long start,</span>
<span class="p_add">+			    unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t hmm_entry = swp_entry_to_pte(make_hmm_entry());</span>
<span class="p_add">+	struct page *pages[MMU_GATHER_BUNDLE];</span>
<span class="p_add">+	unsigned long addr, c, i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start, i = 0; addr &lt; end;) {</span>
<span class="p_add">+		unsigned long next;</span>
<span class="p_add">+		spinlock_t *ptl;</span>
<span class="p_add">+		pgd_t *pgdp;</span>
<span class="p_add">+		pud_t *pudp;</span>
<span class="p_add">+		pmd_t *pmdp;</span>
<span class="p_add">+		pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We know for certain that we did set special swap entry for</span>
<span class="p_add">+		 * the range and HMM entry are mark as locked so it means that</span>
<span class="p_add">+		 * no one beside us can modify them which apply that all level</span>
<span class="p_add">+		 * of the CPU page table are valid.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pgdp = pgd_offset(mm, addr);</span>
<span class="p_add">+		pudp = pud_offset(pgdp, addr);</span>
<span class="p_add">+		VM_BUG_ON(!pudp);</span>
<span class="p_add">+		pmdp = pmd_offset(pudp, addr);</span>
<span class="p_add">+		VM_BUG_ON(!pmdp || pmd_bad(*pmdp) || pmd_none(*pmdp) ||</span>
<span class="p_add">+			  pmd_trans_huge(*pmdp));</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+		for (next = min((addr + PMD_SIZE) &amp; PMD_MASK, end),</span>
<span class="p_add">+		     i = (addr - start) &gt;&gt; PAGE_SHIFT; addr &lt; next;</span>
<span class="p_add">+		     addr += PAGE_SIZE, ptep++, i++) {</span>
<span class="p_add">+			struct page *page;</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * This can&#39;t happen no one else can replace our</span>
<span class="p_add">+			 * precious special entry.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			entry = pte_to_swp_entry(*ptep);</span>
<span class="p_add">+			VM_BUG_ON(!is_hmm_entry_locked(entry));</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!hmm_pte_test_valid_dev(&amp;hmm_pte[i])) {</span>
<span class="p_add">+				/* Ok we have to restore that page. */</span>
<span class="p_add">+				set_pte_at(mm, addr, ptep, save_pte[i]);</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * No need to invalidate - it was non-present</span>
<span class="p_add">+				 * before.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				update_mmu_cache(vma, addr, ptep);</span>
<span class="p_add">+				pte_clear(mm, addr, &amp;save_pte[i]);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Set unlocked entry. */</span>
<span class="p_add">+			set_pte_at(mm, addr, ptep, hmm_entry);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * No need to invalidate - it was non-present</span>
<span class="p_add">+			 * before.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			update_mmu_cache(vma, addr, ptep);</span>
<span class="p_add">+</span>
<span class="p_add">+			page = vm_normal_page(vma, addr, save_pte[i]);</span>
<span class="p_add">+			/* The zero page is fine to migrate. */</span>
<span class="p_add">+			if (!page)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			page_remove_rmap(page);</span>
<span class="p_add">+			dec_mm_counter_fast(mm, MM_ANONPAGES);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Free pages. */</span>
<span class="p_add">+	for (addr = start, i = 0, c = 0; addr &lt; end; i++, addr += PAGE_SIZE) {</span>
<span class="p_add">+		if (pte_none(save_pte[i]))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (c &gt;= MMU_GATHER_BUNDLE) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * TODO: What we really want to do is keep the memory</span>
<span class="p_add">+			 * accounted inside the memory group and inside rss</span>
<span class="p_add">+			 * while still freeing the page. So that migration</span>
<span class="p_add">+			 * back from device memory will not fail because we</span>
<span class="p_add">+			 * go over memory group limit.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			free_pages_and_swap_cache(pages, c);</span>
<span class="p_add">+			c = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pages[c] = vm_normal_page(vma, addr, save_pte[i]);</span>
<span class="p_add">+		c = pages[c] ? c + 1 : c;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(mm_hmm_migrate_cleanup);</span>
 #endif
 
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



