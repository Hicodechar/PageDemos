
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V3,3/5] mm: mlock: Introduce VM_LOCKONFAULT and add mlock flags to enable it - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V3,3/5] mm: mlock: Introduce VM_LOCKONFAULT and add mlock flags to enable it</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 7, 2015, 5:03 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436288623-13007-4-git-send-email-emunson@akamai.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6735681/mbox/"
   >mbox</a>
|
   <a href="/patch/6735681/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6735681/">/patch/6735681/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 0512BC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Jul 2015 17:10:29 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 8658320606
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Jul 2015 17:10:27 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id E8D96206E5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Jul 2015 17:10:25 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932421AbbGGRKO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 7 Jul 2015 13:10:14 -0400
Received: from a23-79-238-175.deploy.static.akamaitechnologies.com
	([23.79.238.175]:64668
	&quot;EHLO prod-mail-xrelay07.akamai.com&quot; rhost-flags-OK-FAIL-OK-OK)
	by vger.kernel.org with ESMTP id S1757592AbbGGRJw (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 7 Jul 2015 13:09:52 -0400
X-Greylist: delayed 359 seconds by postgrey-1.27 at vger.kernel.org;
	Tue, 07 Jul 2015 13:09:51 EDT
Received: from prod-mail-xrelay07.akamai.com (localhost.localdomain
	[127.0.0.1]) by postfix.imss70 (Postfix) with ESMTP id C420448E0E;
	Tue,  7 Jul 2015 17:04:01 +0000 (GMT)
Received: from prod-mail-relay07.akamai.com (prod-mail-relay07.akamai.com
	[172.17.121.112])
	by prod-mail-xrelay07.akamai.com (Postfix) with ESMTP id A706048E12; 
	Tue,  7 Jul 2015 17:04:01 +0000 (GMT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=akamai.com; s=a1;
	t=1436288641; bh=tnPn97hMOe9M1kNxhqRwoX6y/3vdowUiJkqYHe7PLTI=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=h9xto/gqNWWshigCuQ2iyfnofjhlQQdTp05dd7BwQGDcK67sIFbN4kyKMP57q8qdE
	dfhc8HPiRA235eU7BWVI7HPCxQbZh4rwAGpOZo2vFXhZIzmZPGaKaFC6V0LSbqv0Yc
	Y6GbaISo5ErtBzI+ca4XBW61nbmuDCmGqFk5s7G4=
Received: from bos-lp6ds.kendall.corp.akamai.com (unknown [172.28.13.149])
	by prod-mail-relay07.akamai.com (Postfix) with ESMTP id 4631D8008A;
	Tue,  7 Jul 2015 17:03:51 +0000 (GMT)
From: Eric B Munson &lt;emunson@akamai.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Eric B Munson &lt;emunson@akamai.com&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, linux-alpha@vger.kernel.org,
	linux-kernel@vger.kernel.org, linux-mips@linux-mips.org,
	linux-parisc@vger.kernel.org, linuxppc-dev@lists.ozlabs.org,
	sparclinux@vger.kernel.org, linux-xtensa@linux-xtensa.org,
	linux-mm@kvack.org, linux-arch@vger.kernel.org, linux-api@vger.kernel.org
Subject: [PATCH V3 3/5] mm: mlock: Introduce VM_LOCKONFAULT and add mlock
	flags to enable it
Date: Tue,  7 Jul 2015 13:03:41 -0400
Message-Id: &lt;1436288623-13007-4-git-send-email-emunson@akamai.com&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1436288623-13007-1-git-send-email-emunson@akamai.com&gt;
References: &lt;1436288623-13007-1-git-send-email-emunson@akamai.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 7, 2015, 5:03 p.m.</div>
<pre class="content">
The cost of faulting in all memory to be locked can be very high when
working with large mappings.  If only portions of the mapping will be
used this can incur a high penalty for locking.

For the example of a large file, this is the usage pattern for a large
statical language model (probably applies to other statical or graphical
models as well).  For the security example, any application transacting
in data that cannot be swapped out (credit card data, medical records,
etc).

This patch introduces the ability to request that pages are not
pre-faulted, but are placed on the unevictable LRU when they are finally
faulted in.  This can be done area at a time via the
mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These
calls can be undone via munlock2(MLOCK_ONFAULT) or
munlockall2(MCL_ONFAULT).

To keep accounting checks out of the page fault path, users are billed
for the entire mapping lock as if MLOCK_LOCKED was used.
<span class="signed-off-by">
Signed-off-by: Eric B Munson &lt;emunson@akamai.com&gt;</span>
Cc: Michal Hocko &lt;mhocko@suse.cz&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: linux-alpha@vger.kernel.org
Cc: linux-kernel@vger.kernel.org
Cc: linux-mips@linux-mips.org
Cc: linux-parisc@vger.kernel.org
Cc: linuxppc-dev@lists.ozlabs.org
Cc: sparclinux@vger.kernel.org
Cc: linux-xtensa@linux-xtensa.org
Cc: linux-mm@kvack.org
Cc: linux-arch@vger.kernel.org
Cc: linux-api@vger.kernel.org
---
 arch/alpha/include/uapi/asm/mman.h   |  2 +
 arch/mips/include/uapi/asm/mman.h    |  2 +
 arch/parisc/include/uapi/asm/mman.h  |  2 +
 arch/powerpc/include/uapi/asm/mman.h |  2 +
 arch/sparc/include/uapi/asm/mman.h   |  2 +
 arch/tile/include/uapi/asm/mman.h    |  3 ++
 arch/xtensa/include/uapi/asm/mman.h  |  2 +
 fs/proc/task_mmu.c                   |  1 +
 include/linux/mm.h                   |  1 +
 include/uapi/asm-generic/mman.h      |  2 +
 mm/mlock.c                           | 72 ++++++++++++++++++++++++++----------
 mm/mmap.c                            |  4 +-
 mm/swap.c                            |  3 +-
 13 files changed, 75 insertions(+), 23 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=94">Jonathan Corbet</a> - July 8, 2015, 7:23 p.m.</div>
<pre class="content">
On Tue,  7 Jul 2015 13:03:41 -0400
Eric B Munson &lt;emunson@akamai.com&gt; wrote:
<span class="quote">
&gt; This patch introduces the ability to request that pages are not</span>
<span class="quote">&gt; pre-faulted, but are placed on the unevictable LRU when they are finally</span>
<span class="quote">&gt; faulted in.  This can be done area at a time via the</span>
<span class="quote">&gt; mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These</span>
<span class="quote">&gt; calls can be undone via munlock2(MLOCK_ONFAULT) or</span>
<span class="quote">&gt; munlockall2(MCL_ONFAULT).</span>

Quick, possibly dumb question: I&#39;ve been beating my head against these for
a little bit, and I can&#39;t figure out what&#39;s supposed to happen in this
case:

	mlock2(addr, len, MLOCK_ONFAULT);
	munlock2(addr, len, MLOCK_LOCKED);

It looks to me like it will clear VM_LOCKED without actually unlocking any
pages.  Is that the intended result?

Thanks,

jon
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 8, 2015, 8:34 p.m.</div>
<pre class="content">
On Wed, 08 Jul 2015, Jonathan Corbet wrote:
<span class="quote">
&gt; On Tue,  7 Jul 2015 13:03:41 -0400</span>
<span class="quote">&gt; Eric B Munson &lt;emunson@akamai.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; This patch introduces the ability to request that pages are not</span>
<span class="quote">&gt; &gt; pre-faulted, but are placed on the unevictable LRU when they are finally</span>
<span class="quote">&gt; &gt; faulted in.  This can be done area at a time via the</span>
<span class="quote">&gt; &gt; mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These</span>
<span class="quote">&gt; &gt; calls can be undone via munlock2(MLOCK_ONFAULT) or</span>
<span class="quote">&gt; &gt; munlockall2(MCL_ONFAULT).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quick, possibly dumb question: I&#39;ve been beating my head against these for</span>
<span class="quote">&gt; a little bit, and I can&#39;t figure out what&#39;s supposed to happen in this</span>
<span class="quote">&gt; case:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	mlock2(addr, len, MLOCK_ONFAULT);</span>
<span class="quote">&gt; 	munlock2(addr, len, MLOCK_LOCKED);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It looks to me like it will clear VM_LOCKED without actually unlocking any</span>
<span class="quote">&gt; pages.  Is that the intended result?</span>

This is not quite right, what happens when you call munlock2(addr, len,
MLOCK_LOCKED); is we call apply_vma_flags(addr, len, VM_LOCKED, false).
The false argument means that we intend to clear the specified flags.
Here is the relevant snippet:
...
                newflags = vma-&gt;vm_flags;
                if (add_flags) {
                        newflags &amp;= ~(VM_LOCKED | VM_LOCKONFAULT);
                        newflags |= flags;
                } else {
                        newflags &amp;= ~flags;
                }
...

Note that when we are adding flags, we first clear both VM_LOCKED and
VM_LOCKONFAULT.  This was done to match the behavior found in
mlockall().  When we are remove flags, we simply clear the specified
flag(s).

So in your example the state of the VMAs covered by addr and len would
remain unchanged.

It sounds like apply_vma_flags() needs a comment covering this topic, I
will include that in the set I am working on now.

Eric
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=94">Jonathan Corbet</a> - July 8, 2015, 9:17 p.m.</div>
<pre class="content">
On Wed, 8 Jul 2015 16:34:56 -0400
Eric B Munson &lt;emunson@akamai.com&gt; wrote:
<span class="quote">
&gt; &gt; Quick, possibly dumb question: I&#39;ve been beating my head against these for</span>
<span class="quote">&gt; &gt; a little bit, and I can&#39;t figure out what&#39;s supposed to happen in this</span>
<span class="quote">&gt; &gt; case:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	mlock2(addr, len, MLOCK_ONFAULT);</span>
<span class="quote">&gt; &gt; 	munlock2(addr, len, MLOCK_LOCKED);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It looks to me like it will clear VM_LOCKED without actually unlocking any</span>
<span class="quote">&gt; &gt; pages.  Is that the intended result?  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is not quite right, what happens when you call munlock2(addr, len,</span>
<span class="quote">&gt; MLOCK_LOCKED); is we call apply_vma_flags(addr, len, VM_LOCKED, false).</span>

From your explanation, it looks like what I said *was* right...what I was
missing was the fact that VM_LOCKED isn&#39;t set in the first place.  So that
call would be a no-op, clearing a flag that&#39;s already cleared.

One other question...if I call mlock2(MLOCK_ONFAULT) on a range that
already has resident pages, I believe that those pages will not be locked
until they are reclaimed and faulted back in again, right?  I suspect that
could be surprising to users.

Thanks,

jon
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 9, 2015, 6:46 p.m.</div>
<pre class="content">
On Wed, 08 Jul 2015, Jonathan Corbet wrote:
<span class="quote">
&gt; On Wed, 8 Jul 2015 16:34:56 -0400</span>
<span class="quote">&gt; Eric B Munson &lt;emunson@akamai.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; Quick, possibly dumb question: I&#39;ve been beating my head against these for</span>
<span class="quote">&gt; &gt; &gt; a little bit, and I can&#39;t figure out what&#39;s supposed to happen in this</span>
<span class="quote">&gt; &gt; &gt; case:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; 	mlock2(addr, len, MLOCK_ONFAULT);</span>
<span class="quote">&gt; &gt; &gt; 	munlock2(addr, len, MLOCK_LOCKED);</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It looks to me like it will clear VM_LOCKED without actually unlocking any</span>
<span class="quote">&gt; &gt; &gt; pages.  Is that the intended result?  </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is not quite right, what happens when you call munlock2(addr, len,</span>
<span class="quote">&gt; &gt; MLOCK_LOCKED); is we call apply_vma_flags(addr, len, VM_LOCKED, false).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From your explanation, it looks like what I said *was* right...what I was</span>
<span class="quote">&gt; missing was the fact that VM_LOCKED isn&#39;t set in the first place.  So that</span>
<span class="quote">&gt; call would be a no-op, clearing a flag that&#39;s already cleared.</span>

Sorry, I misread the original.  You are correct with the addition that
the call to munlock2(MLOCK_LOCKED) is a noop in this case.
<span class="quote">
&gt; </span>
<span class="quote">&gt; One other question...if I call mlock2(MLOCK_ONFAULT) on a range that</span>
<span class="quote">&gt; already has resident pages, I believe that those pages will not be locked</span>
<span class="quote">&gt; until they are reclaimed and faulted back in again, right?  I suspect that</span>
<span class="quote">&gt; could be surprising to users.</span>

That is the case.  I am looking into what it would take to find only the
present pages in a range and lock them, if that is the behavior that is
preferred I can include it in the updated series.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; jon</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=94">Jonathan Corbet</a> - July 10, 2015, 4:11 p.m.</div>
<pre class="content">
On Thu, 9 Jul 2015 14:46:35 -0400
Eric B Munson &lt;emunson@akamai.com&gt; wrote:
<span class="quote">
&gt; &gt; One other question...if I call mlock2(MLOCK_ONFAULT) on a range that</span>
<span class="quote">&gt; &gt; already has resident pages, I believe that those pages will not be locked</span>
<span class="quote">&gt; &gt; until they are reclaimed and faulted back in again, right?  I suspect that</span>
<span class="quote">&gt; &gt; could be surprising to users.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That is the case.  I am looking into what it would take to find only the</span>
<span class="quote">&gt; present pages in a range and lock them, if that is the behavior that is</span>
<span class="quote">&gt; preferred I can include it in the updated series.</span>

For whatever my $0.02 is worth, I think that should be done.  Otherwise
the mlock2() interface is essentially nondeterministic; you&#39;ll never
really know if a specific page is locked or not.

Thanks,

jon
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 10, 2015, 4:19 p.m.</div>
<pre class="content">
On Fri, 10 Jul 2015, Jonathan Corbet wrote:
<span class="quote">
&gt; On Thu, 9 Jul 2015 14:46:35 -0400</span>
<span class="quote">&gt; Eric B Munson &lt;emunson@akamai.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; One other question...if I call mlock2(MLOCK_ONFAULT) on a range that</span>
<span class="quote">&gt; &gt; &gt; already has resident pages, I believe that those pages will not be locked</span>
<span class="quote">&gt; &gt; &gt; until they are reclaimed and faulted back in again, right?  I suspect that</span>
<span class="quote">&gt; &gt; &gt; could be surprising to users.  </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That is the case.  I am looking into what it would take to find only the</span>
<span class="quote">&gt; &gt; present pages in a range and lock them, if that is the behavior that is</span>
<span class="quote">&gt; &gt; preferred I can include it in the updated series.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For whatever my $0.02 is worth, I think that should be done.  Otherwise</span>
<span class="quote">&gt; the mlock2() interface is essentially nondeterministic; you&#39;ll never</span>
<span class="quote">&gt; really know if a specific page is locked or not.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; jon</span>

Okay, I likely won&#39;t have the new set out today then.  This change is
more invasive.  IIUC, I need an equivalent to __get_user_page() skips
pages which are not present instead of faulting in and the call chain to
get to it.  Unless there is an easier way that I am missing.

Eric
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 21, 2015, 3:35 p.m.</div>
<pre class="content">
On 07/10/2015 06:19 PM, Eric B Munson wrote:
<span class="quote">&gt; On Fri, 10 Jul 2015, Jonathan Corbet wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Thu, 9 Jul 2015 14:46:35 -0400</span>
<span class="quote">&gt;&gt; Eric B Munson &lt;emunson@akamai.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; One other question...if I call mlock2(MLOCK_ONFAULT) on a range that</span>
<span class="quote">&gt;&gt;&gt;&gt; already has resident pages, I believe that those pages will not be locked</span>
<span class="quote">&gt;&gt;&gt;&gt; until they are reclaimed and faulted back in again, right?  I suspect that</span>
<span class="quote">&gt;&gt;&gt;&gt; could be surprising to users.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That is the case.  I am looking into what it would take to find only the</span>
<span class="quote">&gt;&gt;&gt; present pages in a range and lock them, if that is the behavior that is</span>
<span class="quote">&gt;&gt;&gt; preferred I can include it in the updated series.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For whatever my $0.02 is worth, I think that should be done.  Otherwise</span>
<span class="quote">&gt;&gt; the mlock2() interface is essentially nondeterministic; you&#39;ll never</span>
<span class="quote">&gt;&gt; really know if a specific page is locked or not.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Thanks,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; jon</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Okay, I likely won&#39;t have the new set out today then.  This change is</span>
<span class="quote">&gt; more invasive.  IIUC, I need an equivalent to __get_user_page() skips</span>
<span class="quote">&gt; pages which are not present instead of faulting in and the call chain to</span>
<span class="quote">&gt; get to it.  Unless there is an easier way that I am missing.</span>

IIRC having page PageMlocked and put on unevictable list isn&#39;t necessary 
to prevent it from being reclaimed. It&#39;s just to prevent it from being 
scanned for reclaim in the first place. When attempting to unmap the 
page, vma flags are still checked, see the code in try_to_unmap_one(). 
You should probably extend the checks to your new VM_ flag as it is done 
for VM_LOCKED and then you shouldn&#39;t need to walk the pages to mlock 
them (although it would probably still be better for the accounting 
accuracy).
<span class="quote">
&gt; Eric</span>
<span class="quote">&gt;</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_header">index ec72436..77ae8db 100644</span>
<span class="p_header">--- a/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -37,8 +37,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	 8192		/* lock all currently mapped pages */
 #define MCL_FUTURE	16384		/* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	32768		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/arch/mips/include/uapi/asm/mman.h b/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_header">index 67c1cdf..71ed81d 100644</span>
<span class="p_header">--- a/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -61,11 +61,13 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/arch/parisc/include/uapi/asm/mman.h b/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_header">index daab994..c0871ce 100644</span>
<span class="p_header">--- a/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -31,8 +31,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL     0               /* no further special treatment */
 #define MADV_RANDOM     1               /* expect random page references */
<span class="p_header">diff --git a/arch/powerpc/include/uapi/asm/mman.h b/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_header">index 189e85f..f93f7eb 100644</span>
<span class="p_header">--- a/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -22,8 +22,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
 #define MCL_FUTURE      0x4000          /* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	0x8000		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
 #define MAP_NONBLOCK	0x10000		/* do not block on IO */
<span class="p_header">diff --git a/arch/sparc/include/uapi/asm/mman.h b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">index 13d51be..8cd2ebc 100644</span>
<span class="p_header">--- a/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -17,8 +17,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
 #define MCL_FUTURE      0x4000          /* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	0x8000		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
 #define MAP_NONBLOCK	0x10000		/* do not block on IO */
<span class="p_header">diff --git a/arch/tile/include/uapi/asm/mman.h b/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_header">index f69ce48..acdd013 100644</span>
<span class="p_header">--- a/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -36,11 +36,14 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
<span class="p_add">+</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 
 #endif /* _ASM_TILE_MMAN_H */
<span class="p_header">diff --git a/arch/xtensa/include/uapi/asm/mman.h b/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_header">index 11f354f..5725a15 100644</span>
<span class="p_header">--- a/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -74,11 +74,13 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index ca1e091..38d69fc 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -579,6 +579,7 @@</span> <span class="p_context"> static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)</span>
 #ifdef CONFIG_X86_INTEL_MPX
 		[ilog2(VM_MPX)]		= &quot;mp&quot;,
 #endif
<span class="p_add">+		[ilog2(VM_LOCKONFAULT)]	= &quot;lf&quot;,</span>
 		[ilog2(VM_LOCKED)]	= &quot;lo&quot;,
 		[ilog2(VM_IO)]		= &quot;io&quot;,
 		[ilog2(VM_SEQ_READ)]	= &quot;sr&quot;,
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2e872f9..ae40c7d 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -127,6 +127,7 @@</span> <span class="p_context"> extern unsigned int kobjsize(const void *objp);</span>
 #define VM_PFNMAP	0x00000400	/* Page-ranges managed without &quot;struct page&quot;, just pure PFN */
 #define VM_DENYWRITE	0x00000800	/* ETXTBSY on write attempts.. */
 
<span class="p_add">+#define VM_LOCKONFAULT	0x00001000	/* Lock the pages covered when they are faulted in */</span>
 #define VM_LOCKED	0x00002000
 #define VM_IO           0x00004000	/* Memory mapped I/O or similar */
 
<span class="p_header">diff --git a/include/uapi/asm-generic/mman.h b/include/uapi/asm-generic/mman.h</span>
<span class="p_header">index 242436b..555aab0 100644</span>
<span class="p_header">--- a/include/uapi/asm-generic/mman.h</span>
<span class="p_header">+++ b/include/uapi/asm-generic/mman.h</span>
<span class="p_chunk">@@ -17,7 +17,9 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #endif /* __ASM_GENERIC_MMAN_H */
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index d6e61d6..d9414d6 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -502,11 +502,12 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 	pgoff_t pgoff;
 	int nr_pages;
 	int ret = 0;
<span class="p_del">-	int lock = !!(newflags &amp; VM_LOCKED);</span>
<span class="p_add">+	int lock = !!(newflags &amp; (VM_LOCKED | VM_LOCKONFAULT));</span>
 
 	if (newflags == vma-&gt;vm_flags || (vma-&gt;vm_flags &amp; VM_SPECIAL) ||
 	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current-&gt;mm))
<span class="p_del">-		goto out;	/* don&#39;t set VM_LOCKED,  don&#39;t count */</span>
<span class="p_add">+		/* don&#39;t set VM_LOCKED or VM_LOCKONFAULT and don&#39;t count */</span>
<span class="p_add">+		goto out;</span>
 
 	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, newflags, vma-&gt;anon_vma,
<span class="p_chunk">@@ -581,10 +582,12 @@</span> <span class="p_context"> static int apply_vma_flags(unsigned long start, size_t len,</span>
 		/* Here we know that  vma-&gt;vm_start &lt;= nstart &lt; vma-&gt;vm_end. */
 
 		newflags = vma-&gt;vm_flags;
<span class="p_del">-		if (add_flags)</span>
<span class="p_add">+		if (add_flags) {</span>
<span class="p_add">+			newflags &amp;= ~(VM_LOCKED | VM_LOCKONFAULT);</span>
 			newflags |= flags;
<span class="p_del">-		else</span>
<span class="p_add">+		} else {</span>
 			newflags &amp;= ~flags;
<span class="p_add">+		}</span>
 
 		tmp = vma-&gt;vm_end;
 		if (tmp &gt; end)
<span class="p_chunk">@@ -637,9 +640,12 @@</span> <span class="p_context"> static int do_mlock(unsigned long start, size_t len, vm_flags_t flags)</span>
 	if (error)
 		return error;
 
<span class="p_del">-	error = __mm_populate(start, len, 0);</span>
<span class="p_del">-	if (error)</span>
<span class="p_del">-		return __mlock_posix_error_return(error);</span>
<span class="p_add">+	if (flags &amp; VM_LOCKED) {</span>
<span class="p_add">+		error = __mm_populate(start, len, 0);</span>
<span class="p_add">+		if (error)</span>
<span class="p_add">+			return __mlock_posix_error_return(error);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -650,10 +656,14 @@</span> <span class="p_context"> SYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)</span>
 
 SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)
 {
<span class="p_del">-	if (!flags || flags &amp; ~MLOCK_LOCKED)</span>
<span class="p_add">+	if (!flags || (flags &amp; ~(MLOCK_LOCKED | MLOCK_ONFAULT)) ||</span>
<span class="p_add">+	    flags == (MLOCK_LOCKED | MLOCK_ONFAULT))</span>
 		return -EINVAL;
 
<span class="p_del">-	return do_mlock(start, len, VM_LOCKED);</span>
<span class="p_add">+	if (flags &amp; MLOCK_LOCKED)</span>
<span class="p_add">+		return do_mlock(start, len, VM_LOCKED);</span>
<span class="p_add">+</span>
<span class="p_add">+	return do_mlock(start, len, VM_LOCKONFAULT);</span>
 }
 
 static int do_munlock(unsigned long start, size_t len, vm_flags_t flags)
<span class="p_chunk">@@ -677,26 +687,41 @@</span> <span class="p_context"> SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)</span>
 
 SYSCALL_DEFINE3(munlock2, unsigned long, start, size_t, len, int, flags)
 {
<span class="p_del">-	if (!flags || flags &amp; ~MLOCK_LOCKED)</span>
<span class="p_add">+	vm_flags_t to_clear = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!flags || flags &amp; ~(MLOCK_LOCKED | MLOCK_ONFAULT))</span>
 		return -EINVAL;
<span class="p_del">-	return do_munlock(start, len, VM_LOCKED);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; MLOCK_LOCKED)</span>
<span class="p_add">+		to_clear |= VM_LOCKED;</span>
<span class="p_add">+	if (flags &amp; MLOCK_ONFAULT)</span>
<span class="p_add">+		to_clear |= VM_LOCKONFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return do_munlock(start, len, to_clear);</span>
 }
 
 static int do_mlockall(int flags)
 {
 	struct vm_area_struct * vma, * prev = NULL;
<span class="p_add">+	vm_flags_t to_add;</span>
 
 	if (flags &amp; MCL_FUTURE)
 		current-&gt;mm-&gt;def_flags |= VM_LOCKED;
 	if (flags == MCL_FUTURE)
 		goto out;
 
<span class="p_add">+	if (flags &amp; MCL_ONFAULT) {</span>
<span class="p_add">+		current-&gt;mm-&gt;def_flags |= VM_LOCKONFAULT;</span>
<span class="p_add">+		to_add = VM_LOCKONFAULT;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		to_add = VM_LOCKED;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	for (vma = current-&gt;mm-&gt;mmap; vma ; vma = prev-&gt;vm_next) {
 		vm_flags_t newflags;
 
<span class="p_del">-		newflags = vma-&gt;vm_flags &amp; ~VM_LOCKED;</span>
<span class="p_del">-		if (flags &amp; MCL_CURRENT)</span>
<span class="p_del">-			newflags |= VM_LOCKED;</span>
<span class="p_add">+		newflags = vma-&gt;vm_flags &amp; ~(VM_LOCKED | VM_LOCKONFAULT);</span>
<span class="p_add">+		newflags |= to_add;</span>
 
 		/* Ignore errors */
 		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);
<span class="p_chunk">@@ -711,7 +736,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 	unsigned long lock_limit;
 	int ret = -EINVAL;
 
<span class="p_del">-	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE)))</span>
<span class="p_add">+	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)) ||</span>
<span class="p_add">+	    (flags &amp; (MCL_FUTURE | MCL_ONFAULT)) == (MCL_FUTURE | MCL_ONFAULT))</span>
 		goto out;
 
 	ret = -EPERM;
<span class="p_chunk">@@ -740,18 +766,24 @@</span> <span class="p_context"> out:</span>
 static int do_munlockall(int flags)
 {
 	struct vm_area_struct * vma, * prev = NULL;
<span class="p_add">+	vm_flags_t to_clear = 0;</span>
 
 	if (flags &amp; MCL_FUTURE)
 		current-&gt;mm-&gt;def_flags &amp;= ~VM_LOCKED;
<span class="p_add">+	if (flags &amp; MCL_ONFAULT)</span>
<span class="p_add">+		current-&gt;mm-&gt;def_flags &amp;= ~VM_LOCKONFAULT;</span>
 	if (flags == MCL_FUTURE)
 		goto out;
 
<span class="p_add">+	if (flags &amp; MCL_CURRENT)</span>
<span class="p_add">+		to_clear |= VM_LOCKED;</span>
<span class="p_add">+	if (flags &amp; MCL_ONFAULT)</span>
<span class="p_add">+		to_clear |= VM_LOCKONFAULT;</span>
<span class="p_add">+</span>
 	for (vma = current-&gt;mm-&gt;mmap; vma ; vma = prev-&gt;vm_next) {
 		vm_flags_t newflags;
 
<span class="p_del">-		newflags = vma-&gt;vm_flags;</span>
<span class="p_del">-		if (flags &amp; MCL_CURRENT)</span>
<span class="p_del">-			newflags &amp;= ~VM_LOCKED;</span>
<span class="p_add">+		newflags = vma-&gt;vm_flags &amp; ~to_clear;</span>
 
 		/* Ignore errors */
 		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);
<span class="p_chunk">@@ -766,7 +798,7 @@</span> <span class="p_context"> SYSCALL_DEFINE0(munlockall)</span>
 	int ret;
 
 	down_write(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_del">-	ret = do_munlockall(MCL_CURRENT | MCL_FUTURE);</span>
<span class="p_add">+	ret = do_munlockall(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT);</span>
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -775,7 +807,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(munlockall2, int, flags)</span>
 {
 	int ret = -EINVAL;
 
<span class="p_del">-	if (!flags || flags &amp; ~(MCL_CURRENT | MCL_FUTURE))</span>
<span class="p_add">+	if (!flags || flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT))</span>
 		return ret;
 
 	down_write(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index aa632ad..eb970ba 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -1232,8 +1232,8 @@</span> <span class="p_context"> static inline int mlock_future_check(struct mm_struct *mm,</span>
 {
 	unsigned long locked, lock_limit;
 
<span class="p_del">-	/*  mlock MCL_FUTURE? */</span>
<span class="p_del">-	if (flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	/*  mlock MCL_FUTURE or MCL_ONFAULT? */</span>
<span class="p_add">+	if (flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		locked = len &gt;&gt; PAGE_SHIFT;
 		locked += mm-&gt;locked_vm;
 		lock_limit = rlimit(RLIMIT_MEMLOCK);
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index a3a0a2f..3580a21 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -710,7 +710,8 @@</span> <span class="p_context"> void lru_cache_add_active_or_unevictable(struct page *page,</span>
 {
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 
<span class="p_del">-	if (likely((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {</span>
<span class="p_add">+	if (likely((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) == 0) ||</span>
<span class="p_add">+		   (vma-&gt;vm_flags &amp; VM_SPECIAL)) {</span>
 		SetPageActive(page);
 		lru_cache_add(page);
 		return;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



