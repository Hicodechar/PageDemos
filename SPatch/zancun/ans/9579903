
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv3,33/33] mm, x86: introduce PR_SET_MAX_VADDR and PR_GET_MAX_VADDR - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv3,33/33] mm, x86: introduce PR_SET_MAX_VADDR and PR_GET_MAX_VADDR</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 17, 2017, 2:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170217141328.164563-34-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9579903/mbox/"
   >mbox</a>
|
   <a href="/patch/9579903/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9579903/">/patch/9579903/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B7F71600C5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:18:30 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A66DF286E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:18:30 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9B173286E4; Fri, 17 Feb 2017 14:18:30 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3328E286E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Feb 2017 14:18:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934503AbdBQOS1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Feb 2017 09:18:27 -0500
Received: from mga05.intel.com ([192.55.52.43]:54702 &quot;EHLO mga05.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934308AbdBQOOJ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Feb 2017 09:14:09 -0500
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
	by fmsmga105.fm.intel.com with ESMTP; 17 Feb 2017 06:14:08 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.35,172,1484035200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;824354946&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by FMSMGA003.fm.intel.com with ESMTP; 17 Feb 2017 06:14:04 -0800
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id E857BD29; Fri, 17 Feb 2017 16:13:45 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;, linux-api@vger.kernel.org
Subject: [PATCHv3 33/33] mm,
	x86: introduce PR_SET_MAX_VADDR and PR_GET_MAX_VADDR
Date: Fri, 17 Feb 2017 17:13:28 +0300
Message-Id: &lt;20170217141328.164563-34-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170217141328.164563-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170217141328.164563-1-kirill.shutemov@linux.intel.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Feb. 17, 2017, 2:13 p.m.</div>
<pre class="content">
This patch introduces two new prctl(2) handles to manage maximum virtual
address available to userspace to map.

On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

The patch aims to address this compatibility issue.

MM would use the address as upper limit of virtual address available to
map by userspace, instead of TASK_SIZE.

The limit will be equal to TASK_SIZE everywhere, but the machine
with 5-level paging enabled. In this case, the default limit would be
(1UL &lt;&lt; 47) - PAGE_SIZE. Itâ€™s current x86-64 TASK_SIZE_MAX with 4-level
paging which known to be safe.

Changing the limit would affect only future virtual address space
allocations. Currently existing VMAs are intact.

MPX can&#39;t at the moment handle addresses above 47-bits, so we refuse to
increase the limit above 47-bits. We also refuse to enable MPX if the
limit is already above 47-bits or if there is a VMA above the 47-bit
boundary.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: linux-api@vger.kernel.org
---
 arch/x86/include/asm/elf.h         |  2 +-
 arch/x86/include/asm/mmu.h         |  2 ++
 arch/x86/include/asm/mmu_context.h |  1 +
 arch/x86/include/asm/processor.h   | 25 ++++++++++++++++++++-----
 arch/x86/kernel/process.c          | 18 ++++++++++++++++++
 arch/x86/kernel/sys_x86_64.c       |  6 +++---
 arch/x86/mm/hugetlbpage.c          |  8 ++++----
 arch/x86/mm/mmap.c                 |  4 ++--
 arch/x86/mm/mpx.c                  | 17 ++++++++++++++++-
 fs/binfmt_aout.c                   |  2 --
 fs/binfmt_elf.c                    | 10 +++++-----
 fs/hugetlbfs/inode.c               |  6 +++---
 include/linux/sched.h              |  8 ++++++++
 include/uapi/linux/prctl.h         |  3 +++
 kernel/events/uprobes.c            |  5 +++--
 kernel/sys.c                       | 23 ++++++++++++++++++++---
 mm/mmap.c                          | 20 +++++++++++---------
 mm/mremap.c                        |  3 ++-
 mm/nommu.c                         |  2 +-
 mm/shmem.c                         |  8 ++++----
 20 files changed, 127 insertions(+), 46 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Feb. 17, 2017, 4:50 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov
&lt;kirill.shutemov@linux.intel.com&gt; wrote:
<span class="quote">&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt; address available to userspace to map.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; leads to crashes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The patch aims to address this compatibility issue.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; MM would use the address as upper limit of virtual address available to</span>
<span class="quote">&gt; map by userspace, instead of TASK_SIZE.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The limit will be equal to TASK_SIZE everywhere, but the machine</span>
<span class="quote">&gt; with 5-level paging enabled. In this case, the default limit would be</span>
<span class="quote">&gt; (1UL &lt;&lt; 47) - PAGE_SIZE. Itâ€™s current x86-64 TASK_SIZE_MAX with 4-level</span>
<span class="quote">&gt; paging which known to be safe.</span>


I think this patch need to be split up.  In particular, the addition
and use of mmap_max_addr() should be its own patch that doesn&#39;t change
any semantics.
<span class="quote">
&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; index 306c7e12af55..50bdfd6ab866 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -117,6 +117,7 @@ static inline int init_new_context(struct task_struct *tsk,</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;         #endif</span>
<span class="quote">&gt;         init_new_context_ldt(tsk, mm);</span>
<span class="quote">&gt; +       mm-&gt;context.max_vaddr = MAX_VADDR_DEFAULT;</span>

Is this actually correct for 32-bit binaries?  Although, given the
stuff Dmitry is working on, it might pay to separately track the
32-bit and 64-bit limits per mm.  If you haven&#39;t been following it,
Dmitry is trying to fix a bug in which an explicit 32-bit syscall
(int80 or similar) in an otherwise 64-bit process can allocate a VMA
above 4GB that gets truncated.

Also, why the macro?  Why not just put the number in here?
<span class="quote">
&gt; -#define TASK_SIZE_MAX  ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt; +#define TASK_SIZE_MAX  ((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>

This should be in the
<span class="quote">
&gt; -#define STACK_TOP              TASK_SIZE</span>
<span class="quote">&gt; +#define STACK_TOP              mmap_max_addr()</span>

Off the top of my head, this looks wrong.  The 32-bit check got lost, I think.
<span class="quote">
&gt; +unsigned long set_max_vaddr(unsigned long addr)</span>
<span class="quote">&gt; +{</span>

Perhaps this function could set a different field depending on
is_compat_syscall().


Anyway, can you and Dmitry try to reconcile your patches?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Feb. 17, 2017, 5:19 p.m.</div>
<pre class="content">
On 02/17/2017 06:13 AM, Kirill A. Shutemov wrote:
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Default maximum virtual address. This is required for</span>
<span class="quote">&gt; + * compatibility with applications that assumes 47-bit VA.</span>
<span class="quote">&gt; + * The limit can be changed with prctl(PR_SET_MAX_VADDR).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define MAX_VADDR_DEFAULT	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>

This is a bit goofy.  It&#39;s not the largest virtual adddress that can be
accessed, but the beginning of the last page.

Isn&#39;t this easier to deal with in userspace if we make it a &quot;limit&quot;, so
we can do:

	if (addr &gt;= limit)
		// error

Now, we have to do:
	
	prctl(PR_GET_MAX_VADDR, &amp;max_vaddr, 0, 0, 0);
	if (addr &gt; (max_vaddr + PAGE_SIZE))
		// error

I don&#39;t care what you track in the kernel, but I think we need to
provide a more usable number out to userspace.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Feb. 17, 2017, 5:21 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 9:19 AM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt; On 02/17/2017 06:13 AM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Default maximum virtual address. This is required for</span>
<span class="quote">&gt;&gt; + * compatibility with applications that assumes 47-bit VA.</span>
<span class="quote">&gt;&gt; + * The limit can be changed with prctl(PR_SET_MAX_VADDR).</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define MAX_VADDR_DEFAULT    ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is a bit goofy.  It&#39;s not the largest virtual adddress that can be</span>
<span class="quote">&gt; accessed, but the beginning of the last page.</span>

No, it really is the limit.  We don&#39;t allow user code to map the last
page because ti would be a root hole due to SYSRET.  Thanks, Intel.
See the comment near TASK_SIZE_MAX IIRC.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 17, 2017, 8:02 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov
&lt;kirill.shutemov@linux.intel.com&gt; wrote:
<span class="quote">&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt; address available to userspace to map.</span>

So this is my least favorite patch of the whole series, for a couple of reasons:

 (a) adding new code, and mixing it with the mindless TASK_SIZE -&gt;
get_max_addr() conversion.

 (b) what&#39;s the point of that whole TASK_SIZE vs get_max_addr() thing?
When use one, when the other?

so I think this patch needs a lot more thought and/or explanation.

Honestly, (a) is a no-brainer, and can be fixed by just splitting the
patch up. But I think (b) is more fundamental.

In particular, I think that get_max_addr() thing is badly defined.
When should you use TASK_SIZE, when should you use TASK_SIZE_MAX, and
when should you use get_max_addr()? I don&#39;t find that clear at all,
and I think that needs to be a whole lot more explicit and documented.

I also get he feeling that the whole thing is unnecessary. I&#39;m
wondering if we should just instead say that the whole 47 vs 56-bit
virtual address is _purely_ about &quot;get_unmapped_area()&quot;, and nothing
else.

IOW, I&#39;m wondering if we can&#39;t just say that

 - if the processor and kernel support 56-bit user address space, then
you can *always* use the whole space

 - but by default, get_unmapped_area() will only return mappings that
fit in the 47 bit address space.

So if you use MAP_FIXED and give an address in the high range, it will
just always work, and the MM will always consider the task size to be
the full address space.

But for the common case where a process does no use MAP_FIXED, the
kernel will never give a high address by default, and you have to do
the process control thing to say &quot;I want those high addresses&quot;.

Hmm?

In other words, I&#39;d like to at least start out trying to keep the
differences between the 47-bit and 56-bit models as simple and minimal
as possible. Not make such a big deal out of it.

We already have &quot;arch_get_unmapped_area()&quot; that controls the whole
&quot;what will non-MAP_FIXED mmap allocations return&quot;, so I&#39;d hope that
the above kind of semantics could be done without *any* actual
TASK_SIZE changes _anywhere_ in the VM code.

Comments?

      Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Feb. 17, 2017, 8:12 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 12:02 PM, Linus Torvalds
&lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt;&gt; address available to userspace to map.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So this is my least favorite patch of the whole series, for a couple of reasons:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  (a) adding new code, and mixing it with the mindless TASK_SIZE -&gt;</span>
<span class="quote">&gt; get_max_addr() conversion.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  (b) what&#39;s the point of that whole TASK_SIZE vs get_max_addr() thing?</span>
<span class="quote">&gt; When use one, when the other?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; so I think this patch needs a lot more thought and/or explanation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Honestly, (a) is a no-brainer, and can be fixed by just splitting the</span>
<span class="quote">&gt; patch up. But I think (b) is more fundamental.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In particular, I think that get_max_addr() thing is badly defined.</span>
<span class="quote">&gt; When should you use TASK_SIZE, when should you use TASK_SIZE_MAX, and</span>
<span class="quote">&gt; when should you use get_max_addr()? I don&#39;t find that clear at all,</span>
<span class="quote">&gt; and I think that needs to be a whole lot more explicit and documented.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I also get he feeling that the whole thing is unnecessary. I&#39;m</span>
<span class="quote">&gt; wondering if we should just instead say that the whole 47 vs 56-bit</span>
<span class="quote">&gt; virtual address is _purely_ about &quot;get_unmapped_area()&quot;, and nothing</span>
<span class="quote">&gt; else.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; IOW, I&#39;m wondering if we can&#39;t just say that</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - if the processor and kernel support 56-bit user address space, then</span>
<span class="quote">&gt; you can *always* use the whole space</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - but by default, get_unmapped_area() will only return mappings that</span>
<span class="quote">&gt; fit in the 47 bit address space.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So if you use MAP_FIXED and give an address in the high range, it will</span>
<span class="quote">&gt; just always work, and the MM will always consider the task size to be</span>
<span class="quote">&gt; the full address space.</span>

At the very least, I&#39;d want to see
MAP_FIXED_BUT_DONT_BLOODY_UNMAP_ANYTHING.  I *hate* the current
interface.
<span class="quote">
&gt;</span>
<span class="quote">&gt; But for the common case where a process does no use MAP_FIXED, the</span>
<span class="quote">&gt; kernel will never give a high address by default, and you have to do</span>
<span class="quote">&gt; the process control thing to say &quot;I want those high addresses&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hmm?</span>

How about MAP_LIMIT where the address passed in is interpreted as an
upper bound instead of a fixed address?

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 17, 2017, 9:01 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 12:12 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; At the very least, I&#39;d want to see</span>
<span class="quote">&gt; MAP_FIXED_BUT_DONT_BLOODY_UNMAP_ANYTHING.  I *hate* the current</span>
<span class="quote">&gt; interface.</span>

That&#39;s unrelated, but I guess w could add a MAP_NOUNMAP flag, and then
you can use MAP_FIXED | MAP_NOUNMAP or something.

But that has nothing to do with the 47-vs-56 bit issue.
<span class="quote">
&gt; How about MAP_LIMIT where the address passed in is interpreted as an</span>
<span class="quote">&gt; upper bound instead of a fixed address?</span>

Again, that&#39;s a unrelated semantic issue. Right now - if you don&#39;t
pass in MAP_FIXED at all, the &quot;addr&quot; argument is used as a starting
value for deciding where to find an unmapped area. But there is no way
to specify the end. That would basically be what the process control
thing would be (not per-system-call, but per-thread ).

                 Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Feb. 17, 2017, 9:04 p.m.</div>
<pre class="content">
On 02/17/2017 12:02 PM, Linus Torvalds wrote:
<span class="quote">&gt; So if you use MAP_FIXED and give an address in the high range, it will</span>
<span class="quote">&gt; just always work, and the MM will always consider the task size to be</span>
<span class="quote">&gt; the full address space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But for the common case where a process does no use MAP_FIXED, the</span>
<span class="quote">&gt; kernel will never give a high address by default, and you have to do</span>
<span class="quote">&gt; the process control thing to say &quot;I want those high addresses&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm?</span>

Assuming that folks tend to hard-code MAP_FIXED addresses, they&#39;ll be
&lt;48 bits and everything will work splendidly.  But, if folks do
something like take the CPU-enumerated virtual address size and use that
as a starting point, I can see things breaking.

MPX would definitely break if the hardware saw one of those high
addresses and was not ready for it.  It ends up just chopping off the
high bits of the address, so:

	0x10000000000000
and
	0x20000000000000

index into the same spot in the bounds tables.  It does this unless you
put the hardware in the new mode that uses the larger tables, and
consumes more bits of the virtual address.

Is this likely to break anything in practice?  Nah.  But it would nice
to avoid it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 17, 2017, 9:10 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 1:04 PM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Is this likely to break anything in practice?  Nah.  But it would nice</span>
<span class="quote">&gt; to avoid it.</span>

So I go the other way: what *I* would like to avoid is odd code that
is hard to follow. I&#39;d much rather make the code be simple and the
rules be straightforward, and not introduce that complicated
&quot;different address limits&quot; thing at all.

Then, _if_ we ever find a case where it makes a difference, we could
go the more complex route. But not first implementation, and not
without a real example of why we shouldn&#39;t just keep things simple.

              Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - Feb. 17, 2017, 9:50 p.m.</div>
<pre class="content">
On February 17, 2017 1:10:27 PM PST, Linus Torvalds &lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">&gt;On Fri, Feb 17, 2017 at 1:04 PM, Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt;wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is this likely to break anything in practice?  Nah.  But it would</span>
<span class="quote">&gt;nice</span>
<span class="quote">&gt;&gt; to avoid it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;So I go the other way: what *I* would like to avoid is odd code that</span>
<span class="quote">&gt;is hard to follow. I&#39;d much rather make the code be simple and the</span>
<span class="quote">&gt;rules be straightforward, and not introduce that complicated</span>
<span class="quote">&gt;&quot;different address limits&quot; thing at all.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Then, _if_ we ever find a case where it makes a difference, we could</span>
<span class="quote">&gt;go the more complex route. But not first implementation, and not</span>
<span class="quote">&gt;without a real example of why we shouldn&#39;t just keep things simple.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;              Linus</span>

However, we already have different address limits for different threads and/or syscall interfaces - 3 GiB (32-bit with legacy flag), 4 GiB (32-bit or x32), or 128 TiB... and for a while we had a 512 GiB option, too.  In that sense an address cap makes sense and generalizes what we already have.

It would be pretty hideous for the user, long term, to be artificially restricted to a legacy address cap unless they manage the address space themselves.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Feb. 17, 2017, 11:02 p.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 1:01 PM, Linus Torvalds
&lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 12:12 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; At the very least, I&#39;d want to see</span>
<span class="quote">&gt;&gt; MAP_FIXED_BUT_DONT_BLOODY_UNMAP_ANYTHING.  I *hate* the current</span>
<span class="quote">&gt;&gt; interface.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s unrelated, but I guess w could add a MAP_NOUNMAP flag, and then</span>
<span class="quote">&gt; you can use MAP_FIXED | MAP_NOUNMAP or something.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But that has nothing to do with the 47-vs-56 bit issue.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; How about MAP_LIMIT where the address passed in is interpreted as an</span>
<span class="quote">&gt;&gt; upper bound instead of a fixed address?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Again, that&#39;s a unrelated semantic issue. Right now - if you don&#39;t</span>
<span class="quote">&gt; pass in MAP_FIXED at all, the &quot;addr&quot; argument is used as a starting</span>
<span class="quote">&gt; value for deciding where to find an unmapped area. But there is no way</span>
<span class="quote">&gt; to specify the end. That would basically be what the process control</span>
<span class="quote">&gt; thing would be (not per-system-call, but per-thread ).</span>
<span class="quote">&gt;</span>

What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit
limit unless a specific mmap call requests otherwise, can we at least
have an interface that doesn&#39;t suck?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - Feb. 17, 2017, 11:11 p.m.</div>
<pre class="content">
On February 17, 2017 3:02:33 PM PST, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">&gt;On Fri, Feb 17, 2017 at 1:01 PM, Linus Torvalds</span>
<span class="quote">&gt;&lt;torvalds@linux-foundation.org&gt; wrote:</span>
<span class="quote">&gt;&gt; On Fri, Feb 17, 2017 at 12:12 PM, Andy Lutomirski</span>
<span class="quote">&gt;&lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; At the very least, I&#39;d want to see</span>
<span class="quote">&gt;&gt;&gt; MAP_FIXED_BUT_DONT_BLOODY_UNMAP_ANYTHING.  I *hate* the current</span>
<span class="quote">&gt;&gt;&gt; interface.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s unrelated, but I guess w could add a MAP_NOUNMAP flag, and</span>
<span class="quote">&gt;then</span>
<span class="quote">&gt;&gt; you can use MAP_FIXED | MAP_NOUNMAP or something.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But that has nothing to do with the 47-vs-56 bit issue.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; How about MAP_LIMIT where the address passed in is interpreted as an</span>
<span class="quote">&gt;&gt;&gt; upper bound instead of a fixed address?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Again, that&#39;s a unrelated semantic issue. Right now - if you don&#39;t</span>
<span class="quote">&gt;&gt; pass in MAP_FIXED at all, the &quot;addr&quot; argument is used as a starting</span>
<span class="quote">&gt;&gt; value for deciding where to find an unmapped area. But there is no</span>
<span class="quote">&gt;way</span>
<span class="quote">&gt;&gt; to specify the end. That would basically be what the process control</span>
<span class="quote">&gt;&gt; thing would be (not per-system-call, but per-thread ).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit</span>
<span class="quote">&gt;limit unless a specific mmap call requests otherwise, can we at least</span>
<span class="quote">&gt;have an interface that doesn&#39;t suck?</span>

Let&#39;s not, please.

But we really want this interface anyway.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172837">Michael Pratt</a> - Feb. 21, 2017, 6:10 a.m.</div>
<pre class="content">
Sigh... apologies for the HTML. Trying again...

On Mon, Feb 20, 2017 at 9:21 PM, Michael Pratt &lt;linux@pratt.im&gt; wrote:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 3:02 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; On Fri, Feb 17, 2017 at 1:01 PM, Linus Torvalds</span>
<span class="quote">&gt;&gt; &lt;torvalds@linux-foundation.org&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On Fri, Feb 17, 2017 at 12:12 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; At the very least, I&#39;d want to see</span>
<span class="quote">&gt;&gt;&gt;&gt; MAP_FIXED_BUT_DONT_BLOODY_UNMAP_ANYTHING.  I *hate* the current</span>
<span class="quote">&gt;&gt;&gt;&gt; interface.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That&#39;s unrelated, but I guess w could add a MAP_NOUNMAP flag, and then</span>
<span class="quote">&gt;&gt;&gt; you can use MAP_FIXED | MAP_NOUNMAP or something.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; But that has nothing to do with the 47-vs-56 bit issue.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; How about MAP_LIMIT where the address passed in is interpreted as an</span>
<span class="quote">&gt;&gt;&gt;&gt; upper bound instead of a fixed address?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Again, that&#39;s a unrelated semantic issue. Right now - if you don&#39;t</span>
<span class="quote">&gt;&gt;&gt; pass in MAP_FIXED at all, the &quot;addr&quot; argument is used as a starting</span>
<span class="quote">&gt;&gt;&gt; value for deciding where to find an unmapped area. But there is no way</span>
<span class="quote">&gt;&gt;&gt; to specify the end. That would basically be what the process control</span>
<span class="quote">&gt;&gt;&gt; thing would be (not per-system-call, but per-thread ).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit</span>
<span class="quote">&gt;&gt; limit unless a specific mmap call requests otherwise, can we at least</span>
<span class="quote">&gt;&gt; have an interface that doesn&#39;t suck?</span>

I&#39;ve got a set of patches that I&#39;ve meant to send out as an RFC for a
while that tries to address userspace control of address space layout
and covers many of these ideas.

There is a new syscall and set of prctls for controlling the &quot;mmap
layout&quot; (i.e., get_unmapped_area search range) that look something
like this:

struct mmap_layout {
unsigned long start;
unsigned long end;
/*
* These are equivalent to mmap_legacy_base and mmap_base,
* but are not really needed in this proposal.
*/
unsigned long low_base;
unsigned long high_base;
unsigned long flags;
};

/* For flags */
#define MMAP_TOPDOWN 1

struct layout_mmap_args {
unsigned long addr;
unsigned long len;
unsigned long prot;
unsigned long flags;
unsigned long fd;
unsigned long off;
struct mmap_layout layout;
};

void *layout_mmap(struct layout_mmap_args *args);

int prctl(PR_GET_MMAP_LAYOUT, struct mmap_layout *layout);
int prctl(PR_SET_MMAP_LAYOUT, struct mmap_layout *layout);

The prctls control the default range that mmap and friends will
allocate. For 56-bit user address space, it could default to
[mmap_min_addr, 1&lt;&lt;47), as Linus suggests. Applications that want the
full address space can increase it to cover the entire range.

The layout_mmap syscall allows one-off mappings that fall outside the
default layout, and nicely solves the &quot;MAP_FIXED but don&#39;t unmap
anything problem&quot; by passing an explicit range to check without
actually setting MAP_FIXED.

This idea is quite similar to the MAX_VADDR + default
get_unmapped_area behavior ides, just more generalized to give
userspace more control over the ultimate behavior of
get_unmapped_area.


PS. Apologies if my email client screwed up this message. I didn&#39;t
have this thread in my client and have tried to import it from another
account.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Feb. 21, 2017, 10:34 a.m.</div>
<pre class="content">
On Fri, Feb 17, 2017 at 03:21:27PM -0800, Linus Torvalds wrote:
<span class="quote">&gt; On Feb 17, 2017 3:02 PM, &quot;Andy Lutomirski&quot; &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt;   What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit</span>
<span class="quote">&gt; &gt;   limit unless a specific mmap call requests otherwise, can we at least</span>
<span class="quote">&gt; &gt;   have an interface that doesn&#39;t suck?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, I&#39;m not suggesting specific mmap calls at all. I&#39;m suggesting the complete</span>
<span class="quote">&gt; opposite: not having some magical &quot;max address&quot; at all in the VM layer. Keep</span>
<span class="quote">&gt; all the existing TASK_SIZE defines as-is, and just make those be the new 56-bit</span>
<span class="quote">&gt; limit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But to then not make most processes use it, just make the default x86</span>
<span class="quote">&gt; arch_get_free_area() return an address limited to the old 47-bit limit. So</span>
<span class="quote">&gt; effectively all legacy programs work exactly the same way they always did.</span>

arch_get_unmapped_area() changes would not cover STACK_TOP which is
currently defined as TASK_SIZE (on both x86 and arm64). I don&#39;t think it
matters much (normally such upper bits tricks are done on heap objects)
but you may find some weird user program that passes pointers to the
stack around and expects bits 48-63 to be masked out. If that&#39;s a real
issue, we could also limit STACK_TOP to 47-bit (48-bit on arm64).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Feb. 21, 2017, 10:47 a.m.</div>
<pre class="content">
On Tue, Feb 21, 2017 at 10:34:02AM +0000, Catalin Marinas wrote:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 03:21:27PM -0800, Linus Torvalds wrote:</span>
<span class="quote">&gt; &gt; On Feb 17, 2017 3:02 PM, &quot;Andy Lutomirski&quot; &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;   What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit</span>
<span class="quote">&gt; &gt; &gt;   limit unless a specific mmap call requests otherwise, can we at least</span>
<span class="quote">&gt; &gt; &gt;   have an interface that doesn&#39;t suck?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No, I&#39;m not suggesting specific mmap calls at all. I&#39;m suggesting the complete</span>
<span class="quote">&gt; &gt; opposite: not having some magical &quot;max address&quot; at all in the VM layer. Keep</span>
<span class="quote">&gt; &gt; all the existing TASK_SIZE defines as-is, and just make those be the new 56-bit</span>
<span class="quote">&gt; &gt; limit.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But to then not make most processes use it, just make the default x86</span>
<span class="quote">&gt; &gt; arch_get_free_area() return an address limited to the old 47-bit limit. So</span>
<span class="quote">&gt; &gt; effectively all legacy programs work exactly the same way they always did.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; arch_get_unmapped_area() changes would not cover STACK_TOP which is</span>
<span class="quote">&gt; currently defined as TASK_SIZE (on both x86 and arm64). I don&#39;t think it</span>
<span class="quote">&gt; matters much (normally such upper bits tricks are done on heap objects)</span>
<span class="quote">&gt; but you may find some weird user program that passes pointers to the</span>
<span class="quote">&gt; stack around and expects bits 48-63 to be masked out. If that&#39;s a real</span>
<span class="quote">&gt; issue, we could also limit STACK_TOP to 47-bit (48-bit on arm64).</span>

I&#39;ve limited STACK_TOP to 47-bit in my implementation of Linus&#39; proposal:

http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Feb. 21, 2017, 10:54 a.m.</div>
<pre class="content">
On Tue, Feb 21, 2017 at 01:47:36PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Tue, Feb 21, 2017 at 10:34:02AM +0000, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Fri, Feb 17, 2017 at 03:21:27PM -0800, Linus Torvalds wrote:</span>
<span class="quote">&gt; &gt; &gt; On Feb 17, 2017 3:02 PM, &quot;Andy Lutomirski&quot; &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;   What I&#39;m trying to say is: if we&#39;re going to do the route of 48-bit</span>
<span class="quote">&gt; &gt; &gt; &gt;   limit unless a specific mmap call requests otherwise, can we at least</span>
<span class="quote">&gt; &gt; &gt; &gt;   have an interface that doesn&#39;t suck?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; No, I&#39;m not suggesting specific mmap calls at all. I&#39;m suggesting the complete</span>
<span class="quote">&gt; &gt; &gt; opposite: not having some magical &quot;max address&quot; at all in the VM layer. Keep</span>
<span class="quote">&gt; &gt; &gt; all the existing TASK_SIZE defines as-is, and just make those be the new 56-bit</span>
<span class="quote">&gt; &gt; &gt; limit.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; But to then not make most processes use it, just make the default x86</span>
<span class="quote">&gt; &gt; &gt; arch_get_free_area() return an address limited to the old 47-bit limit. So</span>
<span class="quote">&gt; &gt; &gt; effectively all legacy programs work exactly the same way they always did.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; arch_get_unmapped_area() changes would not cover STACK_TOP which is</span>
<span class="quote">&gt; &gt; currently defined as TASK_SIZE (on both x86 and arm64). I don&#39;t think it</span>
<span class="quote">&gt; &gt; matters much (normally such upper bits tricks are done on heap objects)</span>
<span class="quote">&gt; &gt; but you may find some weird user program that passes pointers to the</span>
<span class="quote">&gt; &gt; stack around and expects bits 48-63 to be masked out. If that&#39;s a real</span>
<span class="quote">&gt; &gt; issue, we could also limit STACK_TOP to 47-bit (48-bit on arm64).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve limited STACK_TOP to 47-bit in my implementation of Linus&#39; proposal:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name</span>

Ah, sorry for the noise then (still catching up with this thread; at
some point we&#39;ll need to add 52-bit VA support to arm64, though with 4
levels only).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137481">Dmitry Safonov</a> - Feb. 21, 2017, 11:54 a.m.</div>
<pre class="content">
2017-02-17 19:50 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:
<span class="quote">&gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt;&gt; address available to userspace to map.</span>
...
<span class="quote">&gt; Anyway, can you and Dmitry try to reconcile your patches?</span>

So, how can I help that?
Is there the patch&#39;s version, on which I could rebase?
Here are BTW the last patches, which I will resend with trivial ifdef-fixup
after the merge window:
http://marc.info/?i=20170214183621.2537-1-dsafonov%20()%20virtuozzo%20!%20com
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Feb. 21, 2017, 12:42 p.m.</div>
<pre class="content">
On Tue, Feb 21, 2017 at 02:54:20PM +0300, Dmitry Safonov wrote:
<span class="quote">&gt; 2017-02-17 19:50 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:</span>
<span class="quote">&gt; &gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt; &gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt; &gt;&gt; address available to userspace to map.</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; &gt; Anyway, can you and Dmitry try to reconcile your patches?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, how can I help that?</span>
<span class="quote">&gt; Is there the patch&#39;s version, on which I could rebase?</span>
<span class="quote">&gt; Here are BTW the last patches, which I will resend with trivial ifdef-fixup</span>
<span class="quote">&gt; after the merge window:</span>
<span class="quote">&gt; http://marc.info/?i=20170214183621.2537-1-dsafonov%20()%20virtuozzo%20!%20com</span>

Could you check if this patch collides with anything you do:

http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137481">Dmitry Safonov</a> - March 6, 2017, 2 p.m.</div>
<pre class="content">
2017-02-21 15:42 GMT+03:00 Kirill A. Shutemov &lt;kirill@shutemov.name&gt;:
<span class="quote">&gt; On Tue, Feb 21, 2017 at 02:54:20PM +0300, Dmitry Safonov wrote:</span>
<span class="quote">&gt;&gt; 2017-02-17 19:50 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:</span>
<span class="quote">&gt;&gt; &gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt;&gt; &gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt;&gt; &gt;&gt; address available to userspace to map.</span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt; &gt; Anyway, can you and Dmitry try to reconcile your patches?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So, how can I help that?</span>
<span class="quote">&gt;&gt; Is there the patch&#39;s version, on which I could rebase?</span>
<span class="quote">&gt;&gt; Here are BTW the last patches, which I will resend with trivial ifdef-fixup</span>
<span class="quote">&gt;&gt; after the merge window:</span>
<span class="quote">&gt;&gt; http://marc.info/?i=20170214183621.2537-1-dsafonov%20()%20virtuozzo%20!%20com</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Could you check if this patch collides with anything you do:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name</span>

Ok, sorry for the late reply - it was the merge window anyway and I&#39;ve got
urgent work to do.

Let&#39;s see:

I&#39;ll need minor merge fixup here:
<span class="quote">&gt;-#define TASK_UNMAPPED_BASE (PAGE_ALIGN(TASK_SIZE / 3))</span>
<span class="quote">&gt;+#define TASK_UNMAPPED_BASE (PAGE_ALIGN(DEFAULT_MAP_WINDOW / 3))</span>
while in my patches:
<span class="quote">&gt;+#define __TASK_UNMAPPED_BASE(task_size)        (PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt;+#define TASK_UNMAPPED_BASE             __TASK_UNMAPPED_BASE(TASK_SIZE)</span>

This should be just fine with my changes:
<span class="quote">&gt;- info.high_limit = end;</span>
<span class="quote">&gt;+ info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>

This will need another minor fixup:
<span class="quote">&gt;-#define MAX_GAP (TASK_SIZE/6*5)</span>
<span class="quote">&gt;+#define MAX_GAP (DEFAULT_MAP_WINDOW/6*5)</span>
I&#39;ve moved it from macro to mmap_base() as local var,
which depends on task_size parameter.

That&#39;s all, as far as I can see at this moment.
Does not seems hard to fix. So I suggest sending patches sets
in parallel, the second accepted will rebase the set.
Is it convenient for you?
If you have/will have some questions about my patches, I&#39;ll be
open to answer.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - March 6, 2017, 2:15 p.m.</div>
<pre class="content">
On 03/06/2017 05:17 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Mar 06, 2017 at 05:00:28PM +0300, Dmitry Safonov wrote:</span>
<span class="quote">&gt;&gt; 2017-02-21 15:42 GMT+03:00 Kirill A. Shutemov &lt;kirill@shutemov.name&gt;:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Feb 21, 2017 at 02:54:20PM +0300, Dmitry Safonov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; 2017-02-17 19:50 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; address available to userspace to map.</span>
<span class="quote">&gt;&gt;&gt;&gt; ...</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Anyway, can you and Dmitry try to reconcile your patches?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; So, how can I help that?</span>
<span class="quote">&gt;&gt;&gt;&gt; Is there the patch&#39;s version, on which I could rebase?</span>
<span class="quote">&gt;&gt;&gt;&gt; Here are BTW the last patches, which I will resend with trivial ifdef-fixup</span>
<span class="quote">&gt;&gt;&gt;&gt; after the merge window:</span>
<span class="quote">&gt;&gt;&gt;&gt; http://marc.info/?i=20170214183621.2537-1-dsafonov%20()%20virtuozzo%20!%20com</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Could you check if this patch collides with anything you do:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ok, sorry for the late reply - it was the merge window anyway and I&#39;ve got</span>
<span class="quote">&gt;&gt; urgent work to do.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Let&#39;s see:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;ll need minor merge fixup here:</span>
<span class="quote">&gt;&gt;&gt; -#define TASK_UNMAPPED_BASE (PAGE_ALIGN(TASK_SIZE / 3))</span>
<span class="quote">&gt;&gt;&gt; +#define TASK_UNMAPPED_BASE (PAGE_ALIGN(DEFAULT_MAP_WINDOW / 3))</span>
<span class="quote">&gt;&gt; while in my patches:</span>
<span class="quote">&gt;&gt;&gt; +#define __TASK_UNMAPPED_BASE(task_size)        (PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt;&gt;&gt; +#define TASK_UNMAPPED_BASE             __TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This should be just fine with my changes:</span>
<span class="quote">&gt;&gt;&gt; - info.high_limit = end;</span>
<span class="quote">&gt;&gt;&gt; + info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This will need another minor fixup:</span>
<span class="quote">&gt;&gt;&gt; -#define MAX_GAP (TASK_SIZE/6*5)</span>
<span class="quote">&gt;&gt;&gt; +#define MAX_GAP (DEFAULT_MAP_WINDOW/6*5)</span>
<span class="quote">&gt;&gt; I&#39;ve moved it from macro to mmap_base() as local var,</span>
<span class="quote">&gt;&gt; which depends on task_size parameter.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s all, as far as I can see at this moment.</span>
<span class="quote">&gt;&gt; Does not seems hard to fix. So I suggest sending patches sets</span>
<span class="quote">&gt;&gt; in parallel, the second accepted will rebase the set.</span>
<span class="quote">&gt;&gt; Is it convenient for you?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Works for me.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In fact, I&#39;ve just sent v4 of the patchset.</span>
<span class="quote">&gt;</span>

Ok, thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 6, 2017, 2:17 p.m.</div>
<pre class="content">
On Mon, Mar 06, 2017 at 05:00:28PM +0300, Dmitry Safonov wrote:
<span class="quote">&gt; 2017-02-21 15:42 GMT+03:00 Kirill A. Shutemov &lt;kirill@shutemov.name&gt;:</span>
<span class="quote">&gt; &gt; On Tue, Feb 21, 2017 at 02:54:20PM +0300, Dmitry Safonov wrote:</span>
<span class="quote">&gt; &gt;&gt; 2017-02-17 19:50 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:</span>
<span class="quote">&gt; &gt;&gt; &gt; On Fri, Feb 17, 2017 at 6:13 AM, Kirill A. Shutemov</span>
<span class="quote">&gt; &gt;&gt; &gt; &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; This patch introduces two new prctl(2) handles to manage maximum virtual</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; address available to userspace to map.</span>
<span class="quote">&gt; &gt;&gt; ...</span>
<span class="quote">&gt; &gt;&gt; &gt; Anyway, can you and Dmitry try to reconcile your patches?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; So, how can I help that?</span>
<span class="quote">&gt; &gt;&gt; Is there the patch&#39;s version, on which I could rebase?</span>
<span class="quote">&gt; &gt;&gt; Here are BTW the last patches, which I will resend with trivial ifdef-fixup</span>
<span class="quote">&gt; &gt;&gt; after the merge window:</span>
<span class="quote">&gt; &gt;&gt; http://marc.info/?i=20170214183621.2537-1-dsafonov%20()%20virtuozzo%20!%20com</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Could you check if this patch collides with anything you do:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; http://lkml.kernel.org/r/20170220131515.GA9502@node.shutemov.name</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, sorry for the late reply - it was the merge window anyway and I&#39;ve got</span>
<span class="quote">&gt; urgent work to do.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let&#39;s see:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll need minor merge fixup here:</span>
<span class="quote">&gt; &gt;-#define TASK_UNMAPPED_BASE (PAGE_ALIGN(TASK_SIZE / 3))</span>
<span class="quote">&gt; &gt;+#define TASK_UNMAPPED_BASE (PAGE_ALIGN(DEFAULT_MAP_WINDOW / 3))</span>
<span class="quote">&gt; while in my patches:</span>
<span class="quote">&gt; &gt;+#define __TASK_UNMAPPED_BASE(task_size)        (PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt; &gt;+#define TASK_UNMAPPED_BASE             __TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should be just fine with my changes:</span>
<span class="quote">&gt; &gt;- info.high_limit = end;</span>
<span class="quote">&gt; &gt;+ info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This will need another minor fixup:</span>
<span class="quote">&gt; &gt;-#define MAX_GAP (TASK_SIZE/6*5)</span>
<span class="quote">&gt; &gt;+#define MAX_GAP (DEFAULT_MAP_WINDOW/6*5)</span>
<span class="quote">&gt; I&#39;ve moved it from macro to mmap_base() as local var,</span>
<span class="quote">&gt; which depends on task_size parameter.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s all, as far as I can see at this moment.</span>
<span class="quote">&gt; Does not seems hard to fix. So I suggest sending patches sets</span>
<span class="quote">&gt; in parallel, the second accepted will rebase the set.</span>
<span class="quote">&gt; Is it convenient for you?</span>

Works for me.

In fact, I&#39;ve just sent v4 of the patchset.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index e7f155c3045e..5ce6f2b2b105 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(mmap_max_addr() / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_header">diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="p_header">index f9813b6d8b80..174dc3b60165 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -35,6 +35,8 @@</span> <span class="p_context"> typedef struct {</span>
 	/* address of the bounds directory */
 	void __user *bd_addr;
 #endif
<span class="p_add">+	/* maximum virtual address the process can create VMA at */</span>
<span class="p_add">+	unsigned long max_vaddr;</span>
 } mm_context_t;
 
 #ifdef CONFIG_SMP
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 306c7e12af55..50bdfd6ab866 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -117,6 +117,7 @@</span> <span class="p_context"> static inline int init_new_context(struct task_struct *tsk,</span>
 	}
 	#endif
 	init_new_context_ldt(tsk, mm);
<span class="p_add">+	mm-&gt;context.max_vaddr = MAX_VADDR_DEFAULT;</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index e6cfe7ba2d65..173f9a6b3b6b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -789,8 +789,9 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  */
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_del">-#define STACK_TOP_MAX		STACK_TOP</span>
<span class="p_add">+#define MAX_VADDR_DEFAULT	TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		mmap_max_addr()</span>
<span class="p_add">+#define STACK_TOP_MAX		TASK_SIZE</span>
 
 #define INIT_THREAD  {							  \
 	.sp0			= TOP_OF_INIT_STACK,			  \
<span class="p_chunk">@@ -828,7 +829,14 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
<span class="p_del">-#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="p_add">+#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Default maximum virtual address. This is required for</span>
<span class="p_add">+ * compatibility with applications that assumes 47-bit VA.</span>
<span class="p_add">+ * The limit can be changed with prctl(PR_SET_MAX_VADDR).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define MAX_VADDR_DEFAULT	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
<span class="p_chunk">@@ -841,7 +849,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		mmap_max_addr()</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -863,7 +871,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
  */
<span class="p_del">-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 3))</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(mmap_max_addr() / 3))</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_chunk">@@ -892,6 +900,13 @@</span> <span class="p_context"> static inline int mpx_disable_management(void)</span>
 }
 #endif /* CONFIG_X86_INTEL_MPX */
 
<span class="p_add">+extern unsigned long set_max_vaddr(unsigned long addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#define SET_MAX_VADDR(addr)	set_max_vaddr(addr)</span>
<span class="p_add">+#define GET_MAX_VADDR()		READ_ONCE(current-&gt;mm-&gt;context.max_vaddr)</span>
<span class="p_add">+</span>
<span class="p_add">+#define mmap_max_addr() min(TASK_SIZE, GET_MAX_VADDR())</span>
<span class="p_add">+</span>
 extern u16 amd_get_nb_id(int cpu);
 extern u32 amd_get_nodes_per_socket(void);
 
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index b615a1113f58..ddc5af35f146 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/vm86.h&gt;
 #include &lt;asm/switch_to.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 /*
  * per-CPU TSS segments. Threads are completely &#39;soft&#39; on Linux,
<span class="p_chunk">@@ -536,3 +537,20 @@</span> <span class="p_context"> unsigned long get_wchan(struct task_struct *p)</span>
 	put_task_stack(p);
 	return ret;
 }
<span class="p_add">+</span>
<span class="p_add">+unsigned long set_max_vaddr(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (addr &gt; TASK_SIZE_MAX)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * MPX cannot handle addresses above 47-bits. Refuse to increase</span>
<span class="p_add">+	 * max_vaddr above the limit if MPX is enabled.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; MAX_VADDR_DEFAULT &amp;&amp; kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	current-&gt;mm-&gt;context.max_vaddr = addr;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index a55ed63b9f91..e31f5b0c5468 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> static void find_start_end(unsigned long flags, unsigned long *begin,</span>
 		}
 	} else {
 		*begin = current-&gt;mm-&gt;mmap_legacy_base;
<span class="p_del">-		*end = TASK_SIZE;</span>
<span class="p_add">+		*end = mmap_max_addr();</span>
 	}
 }
 
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_unmapped_area_info info;
 
 	/* requested length too big for entire address space */
<span class="p_del">-	if (len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (TASK_SIZE - len &gt;= addr &amp;&amp;</span>
<span class="p_add">+		if (mmap_max_addr() - len &gt;= addr &amp;&amp;</span>
 				(!vma || addr + len &lt;= vma-&gt;vm_start))
 			return addr;
 	}
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 2ae8584b44c7..b55b04b82097 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -82,7 +82,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = current-&gt;mm-&gt;mmap_legacy_base;
<span class="p_del">-	info.high_limit = TASK_SIZE;</span>
<span class="p_add">+	info.high_limit = mmap_max_addr();</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -114,7 +114,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = mmap_max_addr();</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -131,7 +131,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_del">-	if (len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -143,7 +143,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 	if (addr) {
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (TASK_SIZE - len &gt;= addr &amp;&amp;</span>
<span class="p_add">+		if (mmap_max_addr() - len &gt;= addr &amp;&amp;</span>
 		    (!vma || addr + len &lt;= vma-&gt;vm_start))
 			return addr;
 	}
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index d2dc0438d654..c22f0b802576 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -52,7 +52,7 @@</span> <span class="p_context"> static unsigned long stack_maxrandom_size(void)</span>
  * Leave an at least ~128 MB hole with possible stack randomization.
  */
 #define MIN_GAP (128*1024*1024UL + stack_maxrandom_size())
<span class="p_del">-#define MAX_GAP (TASK_SIZE/6*5)</span>
<span class="p_add">+#define MAX_GAP (mmap_max_addr()/6*5)</span>
 
 static int mmap_is_legacy(void)
 {
<span class="p_chunk">@@ -90,7 +90,7 @@</span> <span class="p_context"> static unsigned long mmap_base(unsigned long rnd)</span>
 	else if (gap &gt; MAX_GAP)
 		gap = MAX_GAP;
 
<span class="p_del">-	return PAGE_ALIGN(TASK_SIZE - gap - rnd);</span>
<span class="p_add">+	return PAGE_ALIGN(mmap_max_addr() - gap - rnd);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index af59f808742f..c19707d3e104 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -354,10 +354,25 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 */
 	bd_base = mpx_get_bounds_dir();
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * MPX doesn&#39;t support addresses above 47-bits yes.</span>
<span class="p_add">+	 * Make sure it&#39;s not allowed to map above the limit and nothing is</span>
<span class="p_add">+	 * mapped there before enabling.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mmap_max_addr() &gt; MAX_VADDR_DEFAULT ||</span>
<span class="p_add">+			find_vma(mm, MAX_VADDR_DEFAULT)) {</span>
<span class="p_add">+		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="p_add">+				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="p_add">+				current-&gt;comm, current-&gt;pid);</span>
<span class="p_add">+		ret = -ENXIO;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
<span class="p_del">-</span>
<span class="p_add">+out:</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_header">diff --git a/fs/binfmt_aout.c b/fs/binfmt_aout.c</span>
<span class="p_header">index 2a59139f520b..7a7f6dba6b00 100644</span>
<span class="p_header">--- a/fs/binfmt_aout.c</span>
<span class="p_header">+++ b/fs/binfmt_aout.c</span>
<span class="p_chunk">@@ -121,8 +121,6 @@</span> <span class="p_context"> static struct linux_binfmt aout_format = {</span>
 	.min_coredump	= PAGE_SIZE
 };
 
<span class="p_del">-#define BAD_ADDR(x)	((unsigned long)(x) &gt;= TASK_SIZE)</span>
<span class="p_del">-</span>
 static int set_brk(unsigned long start, unsigned long end)
 {
 	start = PAGE_ALIGN(start);
<span class="p_header">diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c</span>
<span class="p_header">index 422370293cfd..b5dbea735c6d 100644</span>
<span class="p_header">--- a/fs/binfmt_elf.c</span>
<span class="p_header">+++ b/fs/binfmt_elf.c</span>
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> static struct linux_binfmt elf_format = {</span>
 	.min_coredump	= ELF_EXEC_PAGESIZE,
 };
 
<span class="p_del">-#define BAD_ADDR(x) ((unsigned long)(x) &gt;= TASK_SIZE)</span>
<span class="p_add">+#define BAD_ADDR(x) ((unsigned long)(x) &gt;= mmap_max_addr())</span>
 
 static int set_brk(unsigned long start, unsigned long end)
 {
<span class="p_chunk">@@ -587,8 +587,8 @@</span> <span class="p_context"> static unsigned long load_elf_interp(struct elfhdr *interp_elf_ex,</span>
 			k = load_addr + eppnt-&gt;p_vaddr;
 			if (BAD_ADDR(k) ||
 			    eppnt-&gt;p_filesz &gt; eppnt-&gt;p_memsz ||
<span class="p_del">-			    eppnt-&gt;p_memsz &gt; TASK_SIZE ||</span>
<span class="p_del">-			    TASK_SIZE - eppnt-&gt;p_memsz &lt; k) {</span>
<span class="p_add">+			    eppnt-&gt;p_memsz &gt; mmap_max_addr() ||</span>
<span class="p_add">+			    mmap_max_addr() - eppnt-&gt;p_memsz &lt; k) {</span>
 				error = -ENOMEM;
 				goto out;
 			}
<span class="p_chunk">@@ -960,8 +960,8 @@</span> <span class="p_context"> static int load_elf_binary(struct linux_binprm *bprm)</span>
 		 * &lt;= p_memsz so it is only necessary to check p_memsz.
 		 */
 		if (BAD_ADDR(k) || elf_ppnt-&gt;p_filesz &gt; elf_ppnt-&gt;p_memsz ||
<span class="p_del">-		    elf_ppnt-&gt;p_memsz &gt; TASK_SIZE ||</span>
<span class="p_del">-		    TASK_SIZE - elf_ppnt-&gt;p_memsz &lt; k) {</span>
<span class="p_add">+		    elf_ppnt-&gt;p_memsz &gt; mmap_max_addr() ||</span>
<span class="p_add">+		    mmap_max_addr() - elf_ppnt-&gt;p_memsz &lt; k) {</span>
 			/* set_brk can never work. Avoid overflows. */
 			retval = -EINVAL;
 			goto out_free_dentry;
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 54de77e78775..e132e93b85fb 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -178,7 +178,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_del">-	if (len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	if (flags &amp; MAP_FIXED) {
<span class="p_chunk">@@ -190,7 +190,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 	if (addr) {
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (TASK_SIZE - len &gt;= addr &amp;&amp;</span>
<span class="p_add">+		if (mmap_max_addr() - len &gt;= addr &amp;&amp;</span>
 		    (!vma || addr + len &lt;= vma-&gt;vm_start))
 			return addr;
 	}
<span class="p_chunk">@@ -198,7 +198,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-	info.high_limit = TASK_SIZE;</span>
<span class="p_add">+	info.high_limit = mmap_max_addr();</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index ad3ec9ec61f7..bf47a62fde5d 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -3671,4 +3671,12 @@</span> <span class="p_context"> void cpufreq_add_update_util_hook(int cpu, struct update_util_data *data,</span>
 void cpufreq_remove_update_util_hook(int cpu);
 #endif /* CONFIG_CPU_FREQ */
 
<span class="p_add">+#ifndef mmap_max_addr</span>
<span class="p_add">+#define mmap_max_addr mmap_max_addr</span>
<span class="p_add">+static inline unsigned long mmap_max_addr(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return TASK_SIZE;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/include/uapi/linux/prctl.h b/include/uapi/linux/prctl.h</span>
<span class="p_header">index a8d0759a9e40..e9478ccd4386 100644</span>
<span class="p_header">--- a/include/uapi/linux/prctl.h</span>
<span class="p_header">+++ b/include/uapi/linux/prctl.h</span>
<span class="p_chunk">@@ -197,4 +197,7 @@</span> <span class="p_context"> struct prctl_mm_map {</span>
 # define PR_CAP_AMBIENT_LOWER		3
 # define PR_CAP_AMBIENT_CLEAR_ALL	4
 
<span class="p_add">+#define PR_SET_MAX_VADDR	48</span>
<span class="p_add">+#define PR_GET_MAX_VADDR	49</span>
<span class="p_add">+</span>
 #endif /* _LINUX_PRCTL_H */
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index d416f3baf392..651f571a1a79 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -1142,8 +1142,9 @@</span> <span class="p_context"> static int xol_add_vma(struct mm_struct *mm, struct xol_area *area)</span>
 
 	if (!area-&gt;vaddr) {
 		/* Try to map as high as possible, this is only a hint. */
<span class="p_del">-		area-&gt;vaddr = get_unmapped_area(NULL, TASK_SIZE - PAGE_SIZE,</span>
<span class="p_del">-						PAGE_SIZE, 0, 0);</span>
<span class="p_add">+		area-&gt;vaddr = get_unmapped_area(NULL,</span>
<span class="p_add">+				mmap_max_addr() - PAGE_SIZE,</span>
<span class="p_add">+				PAGE_SIZE, 0, 0);</span>
 		if (area-&gt;vaddr &amp; ~PAGE_MASK) {
 			ret = area-&gt;vaddr;
 			goto fail;
<span class="p_header">diff --git a/kernel/sys.c b/kernel/sys.c</span>
<span class="p_header">index 842914ef7de4..366ba7be92a7 100644</span>
<span class="p_header">--- a/kernel/sys.c</span>
<span class="p_header">+++ b/kernel/sys.c</span>
<span class="p_chunk">@@ -103,6 +103,12 @@</span> <span class="p_context"></span>
 #ifndef SET_FP_MODE
 # define SET_FP_MODE(a,b)	(-EINVAL)
 #endif
<span class="p_add">+#ifndef SET_MAX_VADDR</span>
<span class="p_add">+# define SET_MAX_VADDR(a)	(-EINVAL)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifndef GET_MAX_VADDR</span>
<span class="p_add">+# define GET_MAX_VADDR()	(-EINVAL)</span>
<span class="p_add">+#endif</span>
 
 /*
  * this is where the system-wide overflow UID and GID are defined, for
<span class="p_chunk">@@ -1718,7 +1724,7 @@</span> <span class="p_context"> static int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)</span>
  */
 static int validate_prctl_map(struct prctl_mm_map *prctl_map)
 {
<span class="p_del">-	unsigned long mmap_max_addr = TASK_SIZE;</span>
<span class="p_add">+	unsigned long max_addr = mmap_max_addr();</span>
 	struct mm_struct *mm = current-&gt;mm;
 	int error = -EINVAL, i;
 
<span class="p_chunk">@@ -1743,7 +1749,7 @@</span> <span class="p_context"> static int validate_prctl_map(struct prctl_mm_map *prctl_map)</span>
 	for (i = 0; i &lt; ARRAY_SIZE(offsets); i++) {
 		u64 val = *(u64 *)((char *)prctl_map + offsets[i]);
 
<span class="p_del">-		if ((unsigned long)val &gt;= mmap_max_addr ||</span>
<span class="p_add">+		if ((unsigned long)val &gt;= max_addr ||</span>
 		    (unsigned long)val &lt; mmap_min_addr)
 			goto out;
 	}
<span class="p_chunk">@@ -1949,7 +1955,7 @@</span> <span class="p_context"> static int prctl_set_mm(int opt, unsigned long addr,</span>
 	if (opt == PR_SET_MM_AUXV)
 		return prctl_set_auxv(mm, addr, arg4);
 
<span class="p_del">-	if (addr &gt;= TASK_SIZE || addr &lt; mmap_min_addr)</span>
<span class="p_add">+	if (addr &gt;= mmap_max_addr() || addr &lt; mmap_min_addr)</span>
 		return -EINVAL;
 
 	error = -EINVAL;
<span class="p_chunk">@@ -2261,6 +2267,17 @@</span> <span class="p_context"> SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,</span>
 	case PR_GET_FP_MODE:
 		error = GET_FP_MODE(me);
 		break;
<span class="p_add">+	case PR_SET_MAX_VADDR:</span>
<span class="p_add">+		if (arg3 || arg4 || arg5)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		error = SET_MAX_VADDR(arg2);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PR_GET_MAX_VADDR:</span>
<span class="p_add">+		if (arg3 || arg4 || arg5)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		error = put_user(GET_MAX_VADDR(),</span>
<span class="p_add">+				(unsigned long __user *) arg2);</span>
<span class="p_add">+		break;</span>
 	default:
 		error = -EINVAL;
 		break;
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index dc4291dcc99b..a3384f23359e 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -1966,7 +1966,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	struct vm_area_struct *vma;
 	struct vm_unmapped_area_info info;
 
<span class="p_del">-	if (len &gt; TASK_SIZE - mmap_min_addr)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr() - mmap_min_addr)</span>
 		return -ENOMEM;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -1975,15 +1975,16 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;</span>
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		if (mmap_max_addr() - len &gt;= addr &amp;&amp;</span>
<span class="p_add">+				addr &gt;= mmap_min_addr &amp;&amp;</span>
<span class="p_add">+				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
 			return addr;
 	}
 
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = mm-&gt;mmap_base;
<span class="p_del">-	info.high_limit = TASK_SIZE;</span>
<span class="p_add">+	info.high_limit = mmap_max_addr();</span>
 	info.align_mask = 0;
 	return vm_unmapped_area(&amp;info);
 }
<span class="p_chunk">@@ -2005,7 +2006,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	struct vm_unmapped_area_info info;
 
 	/* requested length too big for entire address space */
<span class="p_del">-	if (len &gt; TASK_SIZE - mmap_min_addr)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr() - mmap_min_addr)</span>
 		return -ENOMEM;
 
 	if (flags &amp; MAP_FIXED)
<span class="p_chunk">@@ -2015,7 +2016,8 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
<span class="p_del">-		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;</span>
<span class="p_add">+		if (mmap_max_addr() - len &gt;= addr &amp;&amp;</span>
<span class="p_add">+				addr &gt;= mmap_min_addr &amp;&amp;</span>
 				(!vma || addr + len &lt;= vma-&gt;vm_start))
 			return addr;
 	}
<span class="p_chunk">@@ -2037,7 +2039,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = mmap_max_addr();</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -2057,7 +2059,7 @@</span> <span class="p_context"> get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,</span>
 		return error;
 
 	/* Careful about overflows.. */
<span class="p_del">-	if (len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	get_area = current-&gt;mm-&gt;get_unmapped_area;
<span class="p_chunk">@@ -2078,7 +2080,7 @@</span> <span class="p_context"> get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,</span>
 	if (IS_ERR_VALUE(addr))
 		return addr;
 
<span class="p_del">-	if (addr &gt; TASK_SIZE - len)</span>
<span class="p_add">+	if (addr &gt; mmap_max_addr() - len)</span>
 		return -ENOMEM;
 	if (offset_in_page(addr))
 		return -EINVAL;
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 2b3bfcd51c75..a8b4fba3dce6 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -433,7 +433,8 @@</span> <span class="p_context"> static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
 	if (offset_in_page(new_addr))
 		goto out;
 
<span class="p_del">-	if (new_len &gt; TASK_SIZE || new_addr &gt; TASK_SIZE - new_len)</span>
<span class="p_add">+	if (new_len &gt; mmap_max_addr() ||</span>
<span class="p_add">+			new_addr &gt; mmap_max_addr() - new_len)</span>
 		goto out;
 
 	/* Ensure the old/new locations do not overlap */
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index 24f9f5f39145..6043b8b82083 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -905,7 +905,7 @@</span> <span class="p_context"> static int validate_mmap_request(struct file *file,</span>
 
 	/* Careful about overflows.. */
 	rlen = PAGE_ALIGN(len);
<span class="p_del">-	if (!rlen || rlen &gt; TASK_SIZE)</span>
<span class="p_add">+	if (!rlen || rlen &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	/* offset overflow? */
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index 3a7587a0314d..54d1ebfb577d 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -1983,7 +1983,7 @@</span> <span class="p_context"> unsigned long shmem_get_unmapped_area(struct file *file,</span>
 	unsigned long inflated_addr;
 	unsigned long inflated_offset;
 
<span class="p_del">-	if (len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (len &gt; mmap_max_addr())</span>
 		return -ENOMEM;
 
 	get_area = current-&gt;mm-&gt;get_unmapped_area;
<span class="p_chunk">@@ -1995,7 +1995,7 @@</span> <span class="p_context"> unsigned long shmem_get_unmapped_area(struct file *file,</span>
 		return addr;
 	if (addr &amp; ~PAGE_MASK)
 		return addr;
<span class="p_del">-	if (addr &gt; TASK_SIZE - len)</span>
<span class="p_add">+	if (addr &gt; mmap_max_addr() - len)</span>
 		return addr;
 
 	if (shmem_huge == SHMEM_HUGE_DENY)
<span class="p_chunk">@@ -2038,7 +2038,7 @@</span> <span class="p_context"> unsigned long shmem_get_unmapped_area(struct file *file,</span>
 		return addr;
 
 	inflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;
<span class="p_del">-	if (inflated_len &gt; TASK_SIZE)</span>
<span class="p_add">+	if (inflated_len &gt; mmap_max_addr())</span>
 		return addr;
 	if (inflated_len &lt; len)
 		return addr;
<span class="p_chunk">@@ -2054,7 +2054,7 @@</span> <span class="p_context"> unsigned long shmem_get_unmapped_area(struct file *file,</span>
 	if (inflated_offset &gt; offset)
 		inflated_addr += HPAGE_PMD_SIZE;
 
<span class="p_del">-	if (inflated_addr &gt; TASK_SIZE - len)</span>
<span class="p_add">+	if (inflated_addr &gt; mmap_max_addr() - len)</span>
 		return addr;
 	return inflated_addr;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



