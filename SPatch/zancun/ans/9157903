
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,1/2] mm, tree wide: replace __GFP_REPEAT by __GFP_RETRY_HARD with more useful semantic - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,1/2] mm, tree wide: replace __GFP_REPEAT by __GFP_RETRY_HARD with more useful semantic</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 6, 2016, 11:32 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465212736-14637-2-git-send-email-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9157903/mbox/"
   >mbox</a>
|
   <a href="/patch/9157903/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9157903/">/patch/9157903/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5A8E060759 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jun 2016 11:33:06 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 49E852656B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jun 2016 11:33:06 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3E9AA26E5D; Mon,  6 Jun 2016 11:33:06 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D08C9267EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Jun 2016 11:33:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751958AbcFFLcl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Jun 2016 07:32:41 -0400
Received: from mail-wm0-f66.google.com ([74.125.82.66]:32972 &quot;EHLO
	mail-wm0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751360AbcFFLcb (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Jun 2016 07:32:31 -0400
Received: by mail-wm0-f66.google.com with SMTP id c74so7497640wme.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 06 Jun 2016 04:32:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=AJa0xdPTYGvhLu4fLlL1XSnm/AlBtqMQBvOGX0AEMOQ=;
	b=aXNz1WKhN4SWx8mZ8FYXhP7aXc91Zea1WO+7EdixilpY+zsCIQ5wkQvp0GdmL3bVYC
	+JMWrJeMv8LPq2EVNO+KpaE9LYg7maZWCxXjN0VpxzVUUxXUrxQbUi2kM0YwmLHyLshM
	e7oyutCtCjUGM96jv0ETM7CmliRTME6mcrhpNWVY29p9DTV6dae0Cosn3xyhGFJjus9X
	eZyf9VzFiyaRB9NRp5fZZh9pxbyItfUsRKtJtRQvDCQynj4WPjS+ZoeQKh0L/uRD2Tt9
	F75XypCSiEw27IMS8TNym+PHY3DwsTmPtk2eQXK/dgXoFmGHP0bmEVeITq+KCV4fEPQr
	casQ==
X-Gm-Message-State: ALyK8tL8Vld8BEQs95PnP1Kcm+yybN0z+SgXyyNS5IzYfXQ4c3cEcJF+v3RhWkoMGvPYnQ==
X-Received: by 10.194.150.130 with SMTP id ui2mr15289968wjb.11.1465212749201;
	Mon, 06 Jun 2016 04:32:29 -0700 (PDT)
Received: from tiehlicka.suse.cz (nat1.scz.suse.com. [213.151.88.250])
	by smtp.gmail.com with ESMTPSA id
	p9sm19821940wjv.21.2016.06.06.04.32.28
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 06 Jun 2016 04:32:28 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Chinner &lt;david@fromorbit.com&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 1/2] mm,
	tree wide: replace __GFP_REPEAT by __GFP_RETRY_HARD with more
	useful semantic
Date: Mon,  6 Jun 2016 13:32:15 +0200
Message-Id: &lt;1465212736-14637-2-git-send-email-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.8.1
In-Reply-To: &lt;1465212736-14637-1-git-send-email-mhocko@kernel.org&gt;
References: &lt;1465212736-14637-1-git-send-email-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 6, 2016, 11:32 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

__GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to
the page allocator. This has been true but only for allocations requests
larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for
smaller sizes. This is a bit unfortunate because there is no way to
express the same semantic for those requests and they are considered too
important to fail so they might end up looping in the page allocator for
ever, similarly to GFP_NOFAIL requests.

Now that the whole tree has been cleaned up and accidental or misled
usage of __GFP_REPEAT flag has been removed for !costly requests we can
give the original flag a better name and more importantly a more useful
semantic. Let&#39;s rename it to __GFP_RETRY_HARD which tells the user that
the allocator would try really hard but there is no promise of a
success. This will work independent of the order and overrides the
default allocator behavior. Page allocator users have several levels of
guarantee vs. cost options (take GFP_KERNEL as an example)
- GFP_KERNEL &amp; ~__GFP_RECLAIM - optimistic allocation without _any_
  attempt to free memory at all. The most light weight mode which even
  doesn&#39;t kick the background reclaim. Should be used carefully because
  it might deplete the memory and the next user might hit the more
  aggressive reclaim
- GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM (or GFP_NOWAIT)- optimistic
  allocation without any attempt to free memory from the current context
  but can wake kswapd to reclaim memory if the zone is below the low
  watermark. Can be used from either atomic contexts or when the request
  is a performance optimization and there is another fallback for a slow
  path.
- (GFP_KERNEL|__GFP_HIGH) &amp; ~__GFP_DIRECT_RECLAIM (aka GFP_ATOMIC) - non
  sleeping allocation with an expensive fallback so it can access some
  portion of memory reserves. Usually used from interrupt/bh context with
  an expensive slow path fallback.
- GFP_KERNEL - both background and direct reclaim are allowed and the
  _default_ page allocator behavior is used. That means that !costly
  allocation requests are basically nofail (unless the requesting task
  is killed by the OOM killer) and costly will fail early rather than
  cause disruptive reclaim.
- GFP_KERNEL | __GFP_NORETRY - overrides the default allocator behavior and
  all allocation requests fail early rather than cause disruptive
  reclaim (one round of reclaim in this implementation). No OOM killer
  is invoked.
- GFP_KERNEL | __GFP_RETRY_HARD - overrides the default allocator behavior
  and all allocation requests try really hard, !costly are allowed to
  invoke OOM killer. The request will fail if no progress is expected.
- GFP_KERNEL | __GFP_NOFAIL - overrides the default allocator behavior
  and all allocation requests will loop endlessly until they
  succeed. This might be really dangerous especially for larger orders.

Existing users of __GFP_REPEAT are changed to __GFP_RETRY_HARD because
they already had their semantic. No new users are added.
__alloc_pages_slowpath is changed to bail out for __GFP_RETRY_HARD if
there is no progress and we have already passed the OOM point. This
means that all the reclaim opportunities have been exhausted and
retrying doesn&#39;t make much sense most probably.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 Documentation/DMA-ISA-LPC.txt                |  2 +-
 arch/powerpc/include/asm/book3s/64/pgalloc.h |  2 +-
 arch/powerpc/kvm/book3s_64_mmu_hv.c          |  2 +-
 drivers/block/xen-blkfront.c                 |  2 +-
 drivers/mmc/host/wbsd.c                      |  2 +-
 drivers/s390/char/vmcp.c                     |  2 +-
 drivers/target/target_core_transport.c       |  2 +-
 drivers/vhost/net.c                          |  2 +-
 drivers/vhost/scsi.c                         |  2 +-
 drivers/vhost/vhost.c                        |  2 +-
 fs/btrfs/check-integrity.c                   |  2 +-
 fs/btrfs/raid56.c                            |  2 +-
 include/linux/gfp.h                          | 32 +++++++++++++++++++---------
 include/linux/slab.h                         |  3 ++-
 include/trace/events/mmflags.h               |  2 +-
 mm/huge_memory.c                             |  2 +-
 mm/hugetlb.c                                 |  4 ++--
 mm/internal.h                                |  2 +-
 mm/page_alloc.c                              | 19 ++++++++++++++---
 mm/sparse-vmemmap.c                          |  4 ++--
 mm/vmscan.c                                  |  8 +++----
 net/core/dev.c                               |  6 +++---
 net/core/skbuff.c                            |  2 +-
 net/sched/sch_fq.c                           |  2 +-
 tools/perf/builtin-kmem.c                    |  2 +-
 25 files changed, 69 insertions(+), 43 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - June 7, 2016, 12:11 p.m.</div>
<pre class="content">
On 2016/06/06 20:32, Michal Hocko wrote:
<span class="quote">&gt; diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; index 669fef1e2bb6..a4b0f18a69ab 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/vhost.c</span>
<span class="quote">&gt; +++ b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; @@ -707,7 +707,7 @@ static int vhost_memory_reg_sort_cmp(const void *p1, const void *p2)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void *vhost_kvzalloc(unsigned long size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt; +	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>

Remaining __GFP_REPEAT users are not always doing costly allocations.
Sometimes they pass __GFP_REPEAT because the size is given from userspace.
Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.

What I think more important is hearing from __GFP_REPEAT users how hard they
want to retry. It is possible that they want to retry unless SIGKILL is
delivered, but passing __GFP_NOFAIL is too hard, and therefore __GFP_REPEAT
is used instead. It is possible that they use __GFP_NOFAIL || __GFP_KILLABLE
if __GFP_KILLABLE were available. In my module (though I&#39;m not using
__GFP_REPEAT), I want to retry unless SIGKILL is delivered.
<span class="quote">
&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 180f5afc5a1f..faa3d4a27850 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -3262,7 +3262,7 @@ should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,</span>
<span class="quote">&gt;  		return compaction_zonelist_suitable(ac, order, alloc_flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * !costly requests are much more important than __GFP_REPEAT</span>
<span class="quote">&gt; +	 * !costly requests are much more important than __GFP_RETRY_HARD</span>
<span class="quote">&gt;  	 * costly ones because they are de facto nofail and invoke OOM</span>
<span class="quote">&gt;  	 * killer to move on while costly can fail and users are ready</span>
<span class="quote">&gt;  	 * to cope with that. 1/4 retries is rather arbitrary but we</span>
<span class="quote">&gt; @@ -3550,6 +3550,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	enum compact_result compact_result;</span>
<span class="quote">&gt;  	int compaction_retries = 0;</span>
<span class="quote">&gt;  	int no_progress_loops = 0;</span>
<span class="quote">&gt; +	bool passed_oom = false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * In the slowpath, we sanity check order to avoid ever trying to</span>
<span class="quote">&gt; @@ -3680,9 +3681,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt; -	 * __GFP_REPEAT</span>
<span class="quote">&gt; +	 * __GFP_RETRY_HARD</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt; +	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_HARD))</span>
<span class="quote">&gt;  		goto noretry;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -3711,6 +3712,17 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  				compaction_retries))</span>
<span class="quote">&gt;  		goto retry;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We have already exhausted all our reclaim opportunities including</span>
<span class="quote">&gt; +	 * the OOM killer without any success so it is time to admit defeat.</span>
<span class="quote">&gt; +	 * We do not care about the order because we want all orders to behave</span>
<span class="quote">&gt; +	 * consistently including !costly ones. costly are handled in</span>
<span class="quote">&gt; +	 * __alloc_pages_may_oom and will bail out even before the first OOM</span>
<span class="quote">&gt; +	 * killer invocation</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (passed_oom &amp;&amp; (gfp_mask &amp; __GFP_RETRY_HARD))</span>
<span class="quote">&gt; +		goto nopage;</span>
<span class="quote">&gt; +</span>

If __GFP_REPEAT was passed because the size is not known at compile time, this
will break &quot;!costly allocations will retry unless TIF_MEMDIE is set&quot; behavior.
<span class="quote">
&gt;  	/* Reclaim has failed us, start killing things */</span>
<span class="quote">&gt;  	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="quote">&gt;  	if (page)</span>
<span class="quote">&gt; @@ -3719,6 +3731,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	/* Retry as long as the OOM killer is making progress */</span>
<span class="quote">&gt;  	if (did_some_progress) {</span>
<span class="quote">&gt;  		no_progress_loops = 0;</span>
<span class="quote">&gt; +		passed_oom = true;</span>

This is too premature. did_some_progress != 0 after returning from
__alloc_pages_may_oom() does not mean the OOM killer was invoked. It only means
that mutex_trylock(&amp;oom_lock) was attempted. It is possible that somebody else
is on the way to call out_of_memory(). It is possible that the OOM reaper is
about to start reaping memory. Giving up after 1 jiffie of sleep is too fast.
<span class="quote">
&gt;  		goto retry;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 7, 2016, 12:31 p.m.</div>
<pre class="content">
On Tue 07-06-16 21:11:03, Tetsuo Handa wrote:
<span class="quote">&gt; On 2016/06/06 20:32, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; &gt; index 669fef1e2bb6..a4b0f18a69ab 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/vhost/vhost.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; &gt; @@ -707,7 +707,7 @@ static int vhost_memory_reg_sort_cmp(const void *p1, const void *p2)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static void *vhost_kvzalloc(unsigned long size)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; -	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt; &gt; +	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Remaining __GFP_REPEAT users are not always doing costly allocations.</span>

Yes but...
<span class="quote">
&gt; Sometimes they pass __GFP_REPEAT because the size is given from userspace.</span>
<span class="quote">&gt; Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.</span>

Would that be a regression though? Strictly speaking the __GFP_REPEAT
documentation was explicit to not loop for ever. So nobody should have
expected nofail semantic pretty much by definition. The fact that our
previous implementation was not fully conforming to the documentation is
just an implementation detail.  All the remaining users of __GFP_REPEAT
_have_ to be prepared for the allocation failure. So what exactly is the
problem with them?
<span class="quote">
&gt; What I think more important is hearing from __GFP_REPEAT users how hard they</span>
<span class="quote">&gt; want to retry. It is possible that they want to retry unless SIGKILL is</span>
<span class="quote">&gt; delivered, but passing __GFP_NOFAIL is too hard, and therefore __GFP_REPEAT</span>
<span class="quote">&gt; is used instead. It is possible that they use __GFP_NOFAIL || __GFP_KILLABLE</span>
<span class="quote">&gt; if __GFP_KILLABLE were available. In my module (though I&#39;m not using</span>
<span class="quote">&gt; __GFP_REPEAT), I want to retry unless SIGKILL is delivered.</span>

To be honest killability for a particular allocation request sounds
like a hack to me. Just consider the expected semantic. How do you
handle when one path uses explicit __GFP_KILLABLE while other path (from
the same syscall) is not... If anything this would have to be process
context wise.
 
[...]
<span class="quote">&gt; &gt;  	/* Reclaim has failed us, start killing things */</span>
<span class="quote">&gt; &gt;  	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="quote">&gt; &gt;  	if (page)</span>
<span class="quote">&gt; &gt; @@ -3719,6 +3731,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt;  	/* Retry as long as the OOM killer is making progress */</span>
<span class="quote">&gt; &gt;  	if (did_some_progress) {</span>
<span class="quote">&gt; &gt;  		no_progress_loops = 0;</span>
<span class="quote">&gt; &gt; +		passed_oom = true;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is too premature. did_some_progress != 0 after returning from</span>
<span class="quote">&gt; __alloc_pages_may_oom() does not mean the OOM killer was invoked. It only means</span>
<span class="quote">&gt; that mutex_trylock(&amp;oom_lock) was attempted.</span>

which means that we have reached the OOM condition and _somebody_ is
actaully handling the OOM on our behalf.
<span class="quote">
&gt; It is possible that somebody else</span>
<span class="quote">&gt; is on the way to call out_of_memory(). It is possible that the OOM reaper is</span>
<span class="quote">&gt; about to start reaping memory. Giving up after 1 jiffie of sleep is too fast.</span>

Sure this will always be racy. But the primary point is that we have
passed the OOM line and then passed through all the retries to get to
the same state again. This sounds like a pretty natural boundary to tell
we have tried hard enough to rather fail and let the caller handle the
fallback.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - June 11, 2016, 2:35 p.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Tue 07-06-16 21:11:03, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; Remaining __GFP_REPEAT users are not always doing costly allocations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes but...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Sometimes they pass __GFP_REPEAT because the size is given from userspace.</span>
<span class="quote">&gt; &gt; Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Would that be a regression though? Strictly speaking the __GFP_REPEAT</span>
<span class="quote">&gt; documentation was explicit to not loop for ever. So nobody should have</span>
<span class="quote">&gt; expected nofail semantic pretty much by definition. The fact that our</span>
<span class="quote">&gt; previous implementation was not fully conforming to the documentation is</span>
<span class="quote">&gt; just an implementation detail.  All the remaining users of __GFP_REPEAT</span>
<span class="quote">&gt; _have_ to be prepared for the allocation failure. So what exactly is the</span>
<span class="quote">&gt; problem with them?</span>

A !costly allocation becomes weaker than now if __GFP_RETRY_HARD is passed.
<span class="quote">
&gt; &gt; &gt;  	/* Reclaim has failed us, start killing things */</span>
<span class="quote">&gt; &gt; &gt;  	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="quote">&gt; &gt; &gt;  	if (page)</span>
<span class="quote">&gt; &gt; &gt; @@ -3719,6 +3731,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt; &gt;  	/* Retry as long as the OOM killer is making progress */</span>
<span class="quote">&gt; &gt; &gt;  	if (did_some_progress) {</span>
<span class="quote">&gt; &gt; &gt;  		no_progress_loops = 0;</span>
<span class="quote">&gt; &gt; &gt; +		passed_oom = true;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is too premature. did_some_progress != 0 after returning from</span>
<span class="quote">&gt; &gt; __alloc_pages_may_oom() does not mean the OOM killer was invoked. It only means</span>
<span class="quote">&gt; &gt; that mutex_trylock(&amp;oom_lock) was attempted.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; which means that we have reached the OOM condition and _somebody_ is</span>
<span class="quote">&gt; actaully handling the OOM on our behalf.</span>

That _somebody_ might release oom_lock without invoking the OOM killer (e.g.
doing !__GFP_FS allocation), which means that we have reached the OOM condition
and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes
as weak as __GFP_NORETRY. I think this is a regression.
<span class="quote">


&gt; &gt; What I think more important is hearing from __GFP_REPEAT users how hard they</span>
<span class="quote">&gt; &gt; want to retry. It is possible that they want to retry unless SIGKILL is</span>
<span class="quote">&gt; &gt; delivered, but passing __GFP_NOFAIL is too hard, and therefore __GFP_REPEAT</span>
<span class="quote">&gt; &gt; is used instead. It is possible that they use __GFP_NOFAIL || __GFP_KILLABLE</span>
<span class="quote">&gt; &gt; if __GFP_KILLABLE were available. In my module (though I&#39;m not using</span>
<span class="quote">&gt; &gt; __GFP_REPEAT), I want to retry unless SIGKILL is delivered.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To be honest killability for a particular allocation request sounds</span>
<span class="quote">&gt; like a hack to me. Just consider the expected semantic. How do you</span>
<span class="quote">&gt; handle when one path uses explicit __GFP_KILLABLE while other path (from</span>
<span class="quote">&gt; the same syscall) is not... If anything this would have to be process</span>
<span class="quote">&gt; context wise.</span>

I didn&#39;t catch your question. But making code killable should be considered
good unless it complicates error handling paths.

Since we are not setting TIF_MEMDIE to all OOM-killed threads, OOM-killed
threads will have to loop until mutex_trylock(&amp;oom_lock) succeeds in order
to get TIF_MEMDIE by calling out_of_memory(), which is a needless delay.

Many allocations from syscall context can give up upon SIGKILL. We don&#39;t
need to allow OOM-killed threads to use memory reserves if that allocation
is killable.

Converting down_write(&amp;mm-&gt;mmap_sem) to down_write_killable(&amp;mm-&gt;mmap_sem)
is considered good. But converting kmalloc(GFP_KERNEL) to
kmalloc(GFP_KERNEL | __GFP_KILLABLE) is considered hack. Why?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 13, 2016, 11:37 a.m.</div>
<pre class="content">
On Sat 11-06-16 23:35:49, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Tue 07-06-16 21:11:03, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; Remaining __GFP_REPEAT users are not always doing costly allocations.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes but...</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Sometimes they pass __GFP_REPEAT because the size is given from userspace.</span>
<span class="quote">&gt; &gt; &gt; Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Would that be a regression though? Strictly speaking the __GFP_REPEAT</span>
<span class="quote">&gt; &gt; documentation was explicit to not loop for ever. So nobody should have</span>
<span class="quote">&gt; &gt; expected nofail semantic pretty much by definition. The fact that our</span>
<span class="quote">&gt; &gt; previous implementation was not fully conforming to the documentation is</span>
<span class="quote">&gt; &gt; just an implementation detail.  All the remaining users of __GFP_REPEAT</span>
<span class="quote">&gt; &gt; _have_ to be prepared for the allocation failure. So what exactly is the</span>
<span class="quote">&gt; &gt; problem with them?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A !costly allocation becomes weaker than now if __GFP_RETRY_HARD is passed.</span>

That is true. But it is not weaker than the __GFP_REPEAT actually ever
promissed. __GFP_REPEAT explicitly said to not retry _for_ever_. The
fact that we have ignored it is sad but that is what I am trying to
address here.
<span class="quote">
&gt; &gt; &gt; &gt;  	/* Reclaim has failed us, start killing things */</span>
<span class="quote">&gt; &gt; &gt; &gt;  	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="quote">&gt; &gt; &gt; &gt;  	if (page)</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -3719,6 +3731,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt; &gt; &gt;  	/* Retry as long as the OOM killer is making progress */</span>
<span class="quote">&gt; &gt; &gt; &gt;  	if (did_some_progress) {</span>
<span class="quote">&gt; &gt; &gt; &gt;  		no_progress_loops = 0;</span>
<span class="quote">&gt; &gt; &gt; &gt; +		passed_oom = true;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This is too premature. did_some_progress != 0 after returning from</span>
<span class="quote">&gt; &gt; &gt; __alloc_pages_may_oom() does not mean the OOM killer was invoked. It only means</span>
<span class="quote">&gt; &gt; &gt; that mutex_trylock(&amp;oom_lock) was attempted.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; which means that we have reached the OOM condition and _somebody_ is</span>
<span class="quote">&gt; &gt; actaully handling the OOM on our behalf.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That _somebody_ might release oom_lock without invoking the OOM killer (e.g.</span>
<span class="quote">&gt; doing !__GFP_FS allocation), which means that we have reached the OOM condition</span>
<span class="quote">&gt; and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes</span>
<span class="quote">&gt; as weak as __GFP_NORETRY. I think this is a regression.</span>

I really fail to see your point. We are talking about a gfp flag which
tells the allocator to retry as much as it is feasible. Getting through
all the reclaim attempts two times without any progress sounds like a
fair criterion. Well, we could try $NUM times but that wouldn&#39;t make too
much difference to what you are writing above. The fact whether somebody
has been killed or not is not really that important IMHO.
<span class="quote">
&gt; &gt; &gt; What I think more important is hearing from __GFP_REPEAT users how hard they</span>
<span class="quote">&gt; &gt; &gt; want to retry. It is possible that they want to retry unless SIGKILL is</span>
<span class="quote">&gt; &gt; &gt; delivered, but passing __GFP_NOFAIL is too hard, and therefore __GFP_REPEAT</span>
<span class="quote">&gt; &gt; &gt; is used instead. It is possible that they use __GFP_NOFAIL || __GFP_KILLABLE</span>
<span class="quote">&gt; &gt; &gt; if __GFP_KILLABLE were available. In my module (though I&#39;m not using</span>
<span class="quote">&gt; &gt; &gt; __GFP_REPEAT), I want to retry unless SIGKILL is delivered.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To be honest killability for a particular allocation request sounds</span>
<span class="quote">&gt; &gt; like a hack to me. Just consider the expected semantic. How do you</span>
<span class="quote">&gt; &gt; handle when one path uses explicit __GFP_KILLABLE while other path (from</span>
<span class="quote">&gt; &gt; the same syscall) is not... If anything this would have to be process</span>
<span class="quote">&gt; &gt; context wise.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I didn&#39;t catch your question. But making code killable should be considered</span>
<span class="quote">&gt; good unless it complicates error handling paths.</span>

What I meant was this
	kmalloc(GFP_KILLABLE)
	func1
	  kmalloc(GFP_KERNEL)

is still not killable context because whatever you call (func1) might
have a different view about killability. So the per allocation context
will not work reliably.
<span class="quote">
&gt; Since we are not setting TIF_MEMDIE to all OOM-killed threads, OOM-killed</span>
<span class="quote">&gt; threads will have to loop until mutex_trylock(&amp;oom_lock) succeeds in order</span>
<span class="quote">&gt; to get TIF_MEMDIE by calling out_of_memory(), which is a needless delay.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Many allocations from syscall context can give up upon SIGKILL. We don&#39;t</span>
<span class="quote">&gt; need to allow OOM-killed threads to use memory reserves if that allocation</span>
<span class="quote">&gt; is killable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Converting down_write(&amp;mm-&gt;mmap_sem) to down_write_killable(&amp;mm-&gt;mmap_sem)</span>
<span class="quote">&gt; is considered good. But converting kmalloc(GFP_KERNEL) to</span>
<span class="quote">&gt; kmalloc(GFP_KERNEL | __GFP_KILLABLE) is considered hack. Why?</span>

Because unblocking the killable context is meant to help others who want
to take the lock to make a forward progress. While the killable
allocation context is only about the particular allocation to fail when
the task is killed. There is no direct resource to release. So unless
all the allocation in the same scope are killable this will not help
anything. See my point?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - June 13, 2016, 2:54 p.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Sat 11-06-16 23:35:49, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tue 07-06-16 21:11:03, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; Remaining __GFP_REPEAT users are not always doing costly allocations.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Yes but...</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Sometimes they pass __GFP_REPEAT because the size is given from userspace.</span>
<span class="quote">&gt; &gt; &gt; &gt; Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Would that be a regression though? Strictly speaking the __GFP_REPEAT</span>
<span class="quote">&gt; &gt; &gt; documentation was explicit to not loop for ever. So nobody should have</span>
<span class="quote">&gt; &gt; &gt; expected nofail semantic pretty much by definition. The fact that our</span>
<span class="quote">&gt; &gt; &gt; previous implementation was not fully conforming to the documentation is</span>
<span class="quote">&gt; &gt; &gt; just an implementation detail.  All the remaining users of __GFP_REPEAT</span>
<span class="quote">&gt; &gt; &gt; _have_ to be prepared for the allocation failure. So what exactly is the</span>
<span class="quote">&gt; &gt; &gt; problem with them?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A !costly allocation becomes weaker than now if __GFP_RETRY_HARD is passed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That is true. But it is not weaker than the __GFP_REPEAT actually ever</span>
<span class="quote">&gt; promissed. __GFP_REPEAT explicitly said to not retry _for_ever_. The</span>
<span class="quote">&gt; fact that we have ignored it is sad but that is what I am trying to</span>
<span class="quote">&gt; address here.</span>

Whatever you rename __GFP_REPEAT to, it sounds strange to me that !costly
__GFP_REPEAT allocations are weaker than !costly !__GFP_REPEAT allocations.
Are you planning to make !costly !__GFP_REPEAT allocations to behave like
__GFP_NORETRY?
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	/* Reclaim has failed us, start killing things */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	if (page)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -3719,6 +3731,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	/* Retry as long as the OOM killer is making progress */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  	if (did_some_progress) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  		no_progress_loops = 0;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +		passed_oom = true;</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; This is too premature. did_some_progress != 0 after returning from</span>
<span class="quote">&gt; &gt; &gt; &gt; __alloc_pages_may_oom() does not mean the OOM killer was invoked. It only means</span>
<span class="quote">&gt; &gt; &gt; &gt; that mutex_trylock(&amp;oom_lock) was attempted.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; which means that we have reached the OOM condition and _somebody_ is</span>
<span class="quote">&gt; &gt; &gt; actaully handling the OOM on our behalf.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That _somebody_ might release oom_lock without invoking the OOM killer (e.g.</span>
<span class="quote">&gt; &gt; doing !__GFP_FS allocation), which means that we have reached the OOM condition</span>
<span class="quote">&gt; &gt; and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes</span>
<span class="quote">&gt; &gt; as weak as __GFP_NORETRY. I think this is a regression.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I really fail to see your point. We are talking about a gfp flag which</span>
<span class="quote">&gt; tells the allocator to retry as much as it is feasible. Getting through</span>
<span class="quote">&gt; all the reclaim attempts two times without any progress sounds like a</span>
<span class="quote">&gt; fair criterion. Well, we could try $NUM times but that wouldn&#39;t make too</span>
<span class="quote">&gt; much difference to what you are writing above. The fact whether somebody</span>
<span class="quote">&gt; has been killed or not is not really that important IMHO.</span>

If all the reclaim attempt first time made no progress, all the reclaim
attempt second time unlikely make progress unless the OOM killer kills
something. Thus, doing all the reclaim attempts two times without any progress
without killing somebody sounds almost equivalent to doing all the reclaim
attempt only once.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 13, 2016, 3:17 p.m.</div>
<pre class="content">
On Mon 13-06-16 23:54:13, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Sat 11-06-16 23:35:49, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Tue 07-06-16 21:11:03, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Remaining __GFP_REPEAT users are not always doing costly allocations.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Yes but...</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Sometimes they pass __GFP_REPEAT because the size is given from userspace.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Thus, unconditional s/__GFP_REPEAT/__GFP_RETRY_HARD/g is not good.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Would that be a regression though? Strictly speaking the __GFP_REPEAT</span>
<span class="quote">&gt; &gt; &gt; &gt; documentation was explicit to not loop for ever. So nobody should have</span>
<span class="quote">&gt; &gt; &gt; &gt; expected nofail semantic pretty much by definition. The fact that our</span>
<span class="quote">&gt; &gt; &gt; &gt; previous implementation was not fully conforming to the documentation is</span>
<span class="quote">&gt; &gt; &gt; &gt; just an implementation detail.  All the remaining users of __GFP_REPEAT</span>
<span class="quote">&gt; &gt; &gt; &gt; _have_ to be prepared for the allocation failure. So what exactly is the</span>
<span class="quote">&gt; &gt; &gt; &gt; problem with them?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; A !costly allocation becomes weaker than now if __GFP_RETRY_HARD is passed.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That is true. But it is not weaker than the __GFP_REPEAT actually ever</span>
<span class="quote">&gt; &gt; promissed. __GFP_REPEAT explicitly said to not retry _for_ever_. The</span>
<span class="quote">&gt; &gt; fact that we have ignored it is sad but that is what I am trying to</span>
<span class="quote">&gt; &gt; address here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Whatever you rename __GFP_REPEAT to, it sounds strange to me that !costly</span>
<span class="quote">&gt; __GFP_REPEAT allocations are weaker than !costly !__GFP_REPEAT allocations.</span>
<span class="quote">&gt; Are you planning to make !costly !__GFP_REPEAT allocations to behave like</span>
<span class="quote">&gt; __GFP_NORETRY?</span>

The patch description tries to explain the difference:
__GFP_NORETRY doesn&#39;t retry at all
__GFP_RETRY_HARD retries as hard as feasible
__GFP_NOFAIL tells the retry for ever

all of them regardless of the order. This is the way how to tell the
allocator to change its default behavior which might be, and actually
is, different depending on the order.

[...]
<span class="quote">&gt; &gt; &gt; That _somebody_ might release oom_lock without invoking the OOM killer (e.g.</span>
<span class="quote">&gt; &gt; &gt; doing !__GFP_FS allocation), which means that we have reached the OOM condition</span>
<span class="quote">&gt; &gt; &gt; and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes</span>
<span class="quote">&gt; &gt; &gt; as weak as __GFP_NORETRY. I think this is a regression.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I really fail to see your point. We are talking about a gfp flag which</span>
<span class="quote">&gt; &gt; tells the allocator to retry as much as it is feasible. Getting through</span>
<span class="quote">&gt; &gt; all the reclaim attempts two times without any progress sounds like a</span>
<span class="quote">&gt; &gt; fair criterion. Well, we could try $NUM times but that wouldn&#39;t make too</span>
<span class="quote">&gt; &gt; much difference to what you are writing above. The fact whether somebody</span>
<span class="quote">&gt; &gt; has been killed or not is not really that important IMHO.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If all the reclaim attempt first time made no progress, all the reclaim</span>
<span class="quote">&gt; attempt second time unlikely make progress unless the OOM killer kills</span>
<span class="quote">&gt; something. Thus, doing all the reclaim attempts two times without any progress</span>
<span class="quote">&gt; without killing somebody sounds almost equivalent to doing all the reclaim</span>
<span class="quote">&gt; attempt only once.</span>

Yes, that is possible. You might have a GFP_NOFS only load where nothing
really invokes the OOM killer. Does that actually matter, though? The
semantic of the flag is to retry hard while the page allocator believes
it can make a forward progress. But not for ever. We never know whether
a progress is possible at all. We have certain heuristics when to give
up, try to invoke OOM killer and try again hoping things have changed.
This is not much different except we declare that no hope to getting to
the OOM point again without being able to succeed. Are you suggesting
a more precise heuristic? Or do you claim that we do not need a flag
which would put a middle ground between __GFP_NORETRY and __GFP_NOFAIL
which are on the extreme sides?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - June 14, 2016, 11:12 a.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; &gt; &gt; &gt; That _somebody_ might release oom_lock without invoking the OOM killer (e.g.</span>
<span class="quote">&gt; &gt; &gt; &gt; doing !__GFP_FS allocation), which means that we have reached the OOM condition</span>
<span class="quote">&gt; &gt; &gt; &gt; and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes</span>
<span class="quote">&gt; &gt; &gt; &gt; as weak as __GFP_NORETRY. I think this is a regression.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I really fail to see your point. We are talking about a gfp flag which</span>
<span class="quote">&gt; &gt; &gt; tells the allocator to retry as much as it is feasible. Getting through</span>
<span class="quote">&gt; &gt; &gt; all the reclaim attempts two times without any progress sounds like a</span>
<span class="quote">&gt; &gt; &gt; fair criterion. Well, we could try $NUM times but that wouldn&#39;t make too</span>
<span class="quote">&gt; &gt; &gt; much difference to what you are writing above. The fact whether somebody</span>
<span class="quote">&gt; &gt; &gt; has been killed or not is not really that important IMHO.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If all the reclaim attempt first time made no progress, all the reclaim</span>
<span class="quote">&gt; &gt; attempt second time unlikely make progress unless the OOM killer kills</span>
<span class="quote">&gt; &gt; something. Thus, doing all the reclaim attempts two times without any progress</span>
<span class="quote">&gt; &gt; without killing somebody sounds almost equivalent to doing all the reclaim</span>
<span class="quote">&gt; &gt; attempt only once.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, that is possible. You might have a GFP_NOFS only load where nothing</span>
<span class="quote">&gt; really invokes the OOM killer. Does that actually matter, though? The</span>
<span class="quote">&gt; semantic of the flag is to retry hard while the page allocator believes</span>
<span class="quote">&gt; it can make a forward progress. But not for ever. We never know whether</span>
<span class="quote">&gt; a progress is possible at all. We have certain heuristics when to give</span>
<span class="quote">&gt; up, try to invoke OOM killer and try again hoping things have changed.</span>
<span class="quote">&gt; This is not much different except we declare that no hope to getting to</span>
<span class="quote">&gt; the OOM point again without being able to succeed. Are you suggesting</span>
<span class="quote">&gt; a more precise heuristic? Or do you claim that we do not need a flag</span>
<span class="quote">&gt; which would put a middle ground between __GFP_NORETRY and __GFP_NOFAIL</span>
<span class="quote">&gt; which are on the extreme sides?</span>

Well, maybe we can get rid of __GFP_RETRY (or make __GFP_RETRY used for only
huge pages). Many __GFP_RETRY users are ready to fall back to vmalloc().

We are not sure whether such __GFP_RETRY users want to retry with OOM-killing
somebody (we don&#39;t have __GFP_MAY_OOM_KILL which explicitly asks for &quot;retry
with OOM-killing somebody&quot;).

If __GFP_RETRY means nothing but try once more,

	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);
	if (!n)
		n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);

will emulate it.



----- arch/powerpc/include/asm/book3s/64/pgalloc.h -----

static inline pgd_t *radix__pgd_alloc(struct mm_struct *mm)
{
#ifdef CONFIG_PPC_64K_PAGES
        return (pgd_t *)__get_free_page(PGALLOC_GFP);
#else
        struct page *page;
        page = alloc_pages(GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO | __GFP_REPEAT, 4);
        if (!page)
                return NULL;
        return (pgd_t *) page_address(page);
#endif
}

----- arch/powerpc/kvm/book3s_64_mmu_hv.c -----

        kvm-&gt;arch.hpt_cma_alloc = 0;
        page = kvm_alloc_hpt(1ul &lt;&lt; (order - PAGE_SHIFT));
        if (page) {
                hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
                memset((void *)hpt, 0, (1ul &lt;&lt; order));
                kvm-&gt;arch.hpt_cma_alloc = 1;
        }

        /* Lastly try successively smaller sizes from the page allocator */
        /* Only do this if userspace didn&#39;t specify a size via ioctl */
        while (!hpt &amp;&amp; order &gt; 18 &amp;&amp; !htab_orderp) {
                hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|
                                       __GFP_NOWARN, order - PAGE_SHIFT);
                if (!hpt)
                        --order;
        }

        if (!hpt)
                return -ENOMEM;

----- drivers/vhost/vhost.c -----

static void *vhost_kvzalloc(unsigned long size)
{
        void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);

        if (!n)
                n = vzalloc(size);
        return n;
}

----- drivers/vhost/scsi.c -----

        vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!vs) {
                vs = vzalloc(sizeof(*vs));
                if (!vs)
                        goto err_vs;
        }

----- drivers/vhost/net.c -----

        n = kmalloc(sizeof *n, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!n) {
                n = vmalloc(sizeof *n);
                if (!n)
                        return -ENOMEM;
        }

----- drivers/block/xen-blkfront.c -----

                /* Stage 1: Make a safe copy of the shadow state. */
                copy = kmemdup(rinfo-&gt;shadow, sizeof(rinfo-&gt;shadow),
                               GFP_NOIO | __GFP_REPEAT | __GFP_HIGH);
                if (!copy)
                        return -ENOMEM;

----- drivers/mmc/host/wbsd.c -----

        /*
         * We need to allocate a special buffer in
         * order for ISA to be able to DMA to it.
         */
        host-&gt;dma_buffer = kmalloc(65536,
                GFP_NOIO | GFP_DMA | __GFP_REPEAT | __GFP_NOWARN);
        if (!host-&gt;dma_buffer)
                goto free;

----- drivers/target/target_core_transport.c -----

        se_sess-&gt;sess_cmd_map = kzalloc(tag_num * tag_size,
                                        GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!se_sess-&gt;sess_cmd_map) {
                se_sess-&gt;sess_cmd_map = vzalloc(tag_num * tag_size);
                if (!se_sess-&gt;sess_cmd_map) {
                        pr_err(&quot;Unable to allocate se_sess-&gt;sess_cmd_map\n&quot;);
                        return -ENOMEM;
                }
        }

----- drivers/s390/char/vmcp.c -----

        if (mutex_lock_interruptible(&amp;session-&gt;mutex)) {
                kfree(cmd);
                return -ERESTARTSYS;
        }
        if (!session-&gt;response)
                session-&gt;response = (char *)__get_free_pages(GFP_KERNEL
                                                | __GFP_REPEAT | GFP_DMA,
                                                get_order(session-&gt;bufsize));
        if (!session-&gt;response) {
                mutex_unlock(&amp;session-&gt;mutex);
                kfree(cmd);
                return -ENOMEM;
        }

----- fs/btrfs/raid56.c -----

        table_size = sizeof(*table) + sizeof(*h) * num_entries;
        table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!table) {
                table = vzalloc(table_size);
                if (!table)
                        return -ENOMEM;
        }

----- fs/btrfs/check-integrity.c -----

        state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!state) {
                state = vzalloc(sizeof(*state));
                if (!state) {
                        printk(KERN_INFO &quot;btrfs check-integrity: vzalloc() failed!\n&quot;);
                        return -1;
                }
        }

----- mm/sparse-vmemmap.c -----

void * __meminit vmemmap_alloc_block(unsigned long size, int node)
{
        /* If the main allocator is up use that, fallback to bootmem. */
        if (slab_is_available()) {
                struct page *page;

                if (node_state(node, N_HIGH_MEMORY))
                        page = alloc_pages_node(
                                node, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,
                                get_order(size));
                else
                        page = alloc_pages(
                                GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,
                                get_order(size));
                if (page)
                        return page_address(page);
                return NULL;
        } else
                return __earlyonly_bootmem_alloc(node, size, size,
                                __pa(MAX_DMA_ADDRESS));
}

----- mm/hugetlb.c -----

static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)
{
        struct page *page;

        page = __alloc_pages_node(nid,
                htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
                                                __GFP_REPEAT|__GFP_NOWARN,
                huge_page_order(h));
        if (page) {
                prep_new_huge_page(h, page, nid);
        }

        return page;
}

static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,
                struct vm_area_struct *vma, unsigned long addr, int nid)
{
        int order = huge_page_order(h);
        gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;
        unsigned int cpuset_mems_cookie;

----- net/core/skbuff.c -----

        gfp_head = gfp_mask;
        if (gfp_head &amp; __GFP_DIRECT_RECLAIM)
                gfp_head |= __GFP_REPEAT;

        *errcode = -ENOBUFS;
        skb = alloc_skb(header_len, gfp_head);
        if (!skb)
                return NULL;

----- net/core/dev.c -----

        rx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!rx) {
                rx = vzalloc(sz);
                if (!rx)
                        return -ENOMEM;
        }
        dev-&gt;_rx = rx;

        tx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!tx) {
                tx = vzalloc(sz);
                if (!tx)
                        return -ENOMEM;
        }
        dev-&gt;_tx = tx;

        p = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);
        if (!p)
                p = vzalloc(alloc_size);
        if (!p)
                return NULL;

----- net/sched/sch_fq.c -----

static void *fq_alloc_node(size_t sz, int node)
{
        void *ptr;

        ptr = kmalloc_node(sz, GFP_KERNEL | __GFP_REPEAT | __GFP_NOWARN, node);
        if (!ptr)
                ptr = vmalloc_node(sz, node);
        return ptr;
}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 14, 2016, 6:54 p.m.</div>
<pre class="content">
On Tue 14-06-16 20:12:08, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; That _somebody_ might release oom_lock without invoking the OOM killer (e.g.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; doing !__GFP_FS allocation), which means that we have reached the OOM condition</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; and nobody is actually handling the OOM on our behalf. __GFP_RETRY_HARD becomes</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; as weak as __GFP_NORETRY. I think this is a regression.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I really fail to see your point. We are talking about a gfp flag which</span>
<span class="quote">&gt; &gt; &gt; &gt; tells the allocator to retry as much as it is feasible. Getting through</span>
<span class="quote">&gt; &gt; &gt; &gt; all the reclaim attempts two times without any progress sounds like a</span>
<span class="quote">&gt; &gt; &gt; &gt; fair criterion. Well, we could try $NUM times but that wouldn&#39;t make too</span>
<span class="quote">&gt; &gt; &gt; &gt; much difference to what you are writing above. The fact whether somebody</span>
<span class="quote">&gt; &gt; &gt; &gt; has been killed or not is not really that important IMHO.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If all the reclaim attempt first time made no progress, all the reclaim</span>
<span class="quote">&gt; &gt; &gt; attempt second time unlikely make progress unless the OOM killer kills</span>
<span class="quote">&gt; &gt; &gt; something. Thus, doing all the reclaim attempts two times without any progress</span>
<span class="quote">&gt; &gt; &gt; without killing somebody sounds almost equivalent to doing all the reclaim</span>
<span class="quote">&gt; &gt; &gt; attempt only once.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes, that is possible. You might have a GFP_NOFS only load where nothing</span>
<span class="quote">&gt; &gt; really invokes the OOM killer. Does that actually matter, though? The</span>
<span class="quote">&gt; &gt; semantic of the flag is to retry hard while the page allocator believes</span>
<span class="quote">&gt; &gt; it can make a forward progress. But not for ever. We never know whether</span>
<span class="quote">&gt; &gt; a progress is possible at all. We have certain heuristics when to give</span>
<span class="quote">&gt; &gt; up, try to invoke OOM killer and try again hoping things have changed.</span>
<span class="quote">&gt; &gt; This is not much different except we declare that no hope to getting to</span>
<span class="quote">&gt; &gt; the OOM point again without being able to succeed. Are you suggesting</span>
<span class="quote">&gt; &gt; a more precise heuristic? Or do you claim that we do not need a flag</span>
<span class="quote">&gt; &gt; which would put a middle ground between __GFP_NORETRY and __GFP_NOFAIL</span>
<span class="quote">&gt; &gt; which are on the extreme sides?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, maybe we can get rid of __GFP_RETRY (or make __GFP_RETRY used for only</span>
<span class="quote">&gt; huge pages). Many __GFP_RETRY users are ready to fall back to vmalloc().</span>

But some of them should try hard before they fall back to vmalloc. And
let me repeat, there valid usecases when you want to to tell the
allocator to not retry !costly requests for ever.
<span class="quote">
&gt; We are not sure whether such __GFP_RETRY users want to retry with OOM-killing</span>
<span class="quote">&gt; somebody (we don&#39;t have __GFP_MAY_OOM_KILL which explicitly asks for &quot;retry</span>
<span class="quote">&gt; with OOM-killing somebody&quot;).</span>

And we do not want something like __GFP_MAY_OOM_KILL. We have
__GFP_NORETRY to tell to bail out early. We do not want callers to
control the OOM behavior. That is an MM internal thing IMHO.
<span class="quote">
&gt; If __GFP_RETRY means nothing but try once more,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);</span>
<span class="quote">&gt; 	if (!n)</span>
<span class="quote">&gt; 		n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; will emulate it.</span>

It won&#39;t. __GFP_NORETRY is way too weak because it only invokes
optimistic compaction so the success rate would be really small even if
you retry with the same flag multiple times in a row. We definitely need
a stronger mode to tell that the allocator should really try hard before
it fails.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/DMA-ISA-LPC.txt b/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_header">index b1a19835e907..5b594dfb1783 100644</span>
<span class="p_header">--- a/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_header">+++ b/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_chunk">@@ -42,7 +42,7 @@</span> <span class="p_context"> requirements you pass the flag GFP_DMA to kmalloc.</span>
 
 Unfortunately the memory available for ISA DMA is scarce so unless you
 allocate the memory during boot-up it&#39;s a good idea to also pass
<span class="p_del">-__GFP_REPEAT and __GFP_NOWARN to make the allocater try a bit harder.</span>
<span class="p_add">+__GFP_RETRY_HARD and __GFP_NOWARN to make the allocater try a bit harder.</span>
 
 (This scarcity also means that you should allocate the buffer as
 early as possible and not release it until the driver is unloaded.)
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/pgalloc.h b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_header">index d14fcf82c00c..be3b996915a9 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> static inline pgd_t *radix__pgd_alloc(struct mm_struct *mm)</span>
 	return (pgd_t *)__get_free_page(PGALLOC_GFP);
 #else
 	struct page *page;
<span class="p_del">-	page = alloc_pages(PGALLOC_GFP | __GFP_REPEAT, 4);</span>
<span class="p_add">+	page = alloc_pages(PGALLOC_GFP | __GFP_RETRY_HARD, 4);</span>
 	if (!page)
 		return NULL;
 	return (pgd_t *) page_address(page);
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index 05f09ae82587..204484fbda51 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -72,7 +72,7 @@</span> <span class="p_context"> long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 	/* Lastly try successively smaller sizes from the page allocator */
 	/* Only do this if userspace didn&#39;t specify a size via ioctl */
 	while (!hpt &amp;&amp; order &gt; PPC_MIN_HPT_ORDER &amp;&amp; !htab_orderp) {
<span class="p_del">-		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|</span>
<span class="p_add">+		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_RETRY_HARD|</span>
 				       __GFP_NOWARN, order - PAGE_SHIFT);
 		if (!hpt)
 			--order;
<span class="p_header">diff --git a/drivers/block/xen-blkfront.c b/drivers/block/xen-blkfront.c</span>
<span class="p_header">index ca13df854639..2633e1c32a45 100644</span>
<span class="p_header">--- a/drivers/block/xen-blkfront.c</span>
<span class="p_header">+++ b/drivers/block/xen-blkfront.c</span>
<span class="p_chunk">@@ -2029,7 +2029,7 @@</span> <span class="p_context"> static int blkif_recover(struct blkfront_info *info)</span>
 		rinfo = &amp;info-&gt;rinfo[r_index];
 		/* Stage 1: Make a safe copy of the shadow state. */
 		copy = kmemdup(rinfo-&gt;shadow, sizeof(rinfo-&gt;shadow),
<span class="p_del">-			       GFP_NOIO | __GFP_REPEAT | __GFP_HIGH);</span>
<span class="p_add">+			       GFP_NOIO | __GFP_RETRY_HARD | __GFP_HIGH);</span>
 		if (!copy)
 			return -ENOMEM;
 
<span class="p_header">diff --git a/drivers/mmc/host/wbsd.c b/drivers/mmc/host/wbsd.c</span>
<span class="p_header">index c3fd16d997ca..cb71a383c4ec 100644</span>
<span class="p_header">--- a/drivers/mmc/host/wbsd.c</span>
<span class="p_header">+++ b/drivers/mmc/host/wbsd.c</span>
<span class="p_chunk">@@ -1386,7 +1386,7 @@</span> <span class="p_context"> static void wbsd_request_dma(struct wbsd_host *host, int dma)</span>
 	 * order for ISA to be able to DMA to it.
 	 */
 	host-&gt;dma_buffer = kmalloc(WBSD_DMA_SIZE,
<span class="p_del">-		GFP_NOIO | GFP_DMA | __GFP_REPEAT | __GFP_NOWARN);</span>
<span class="p_add">+		GFP_NOIO | GFP_DMA | __GFP_RETRY_HARD | __GFP_NOWARN);</span>
 	if (!host-&gt;dma_buffer)
 		goto free;
 
<span class="p_header">diff --git a/drivers/s390/char/vmcp.c b/drivers/s390/char/vmcp.c</span>
<span class="p_header">index 2a67b496a9e2..d5ecf3007ac6 100644</span>
<span class="p_header">--- a/drivers/s390/char/vmcp.c</span>
<span class="p_header">+++ b/drivers/s390/char/vmcp.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> vmcp_write(struct file *file, const char __user *buff, size_t count,</span>
 	}
 	if (!session-&gt;response)
 		session-&gt;response = (char *)__get_free_pages(GFP_KERNEL
<span class="p_del">-						| __GFP_REPEAT | GFP_DMA,</span>
<span class="p_add">+						| __GFP_RETRY_HARD | GFP_DMA,</span>
 						get_order(session-&gt;bufsize));
 	if (!session-&gt;response) {
 		mutex_unlock(&amp;session-&gt;mutex);
<span class="p_header">diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c</span>
<span class="p_header">index 5ab3967dda43..6102177b2c7b 100644</span>
<span class="p_header">--- a/drivers/target/target_core_transport.c</span>
<span class="p_header">+++ b/drivers/target/target_core_transport.c</span>
<span class="p_chunk">@@ -251,7 +251,7 @@</span> <span class="p_context"> int transport_alloc_session_tags(struct se_session *se_sess,</span>
 	int rc;
 
 	se_sess-&gt;sess_cmd_map = kzalloc(tag_num * tag_size,
<span class="p_del">-					GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+					GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!se_sess-&gt;sess_cmd_map) {
 		se_sess-&gt;sess_cmd_map = vzalloc(tag_num * tag_size);
 		if (!se_sess-&gt;sess_cmd_map) {
<span class="p_header">diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="p_header">index f744eeb3e2b4..820a4715eb38 100644</span>
<span class="p_header">--- a/drivers/vhost/net.c</span>
<span class="p_header">+++ b/drivers/vhost/net.c</span>
<span class="p_chunk">@@ -747,7 +747,7 @@</span> <span class="p_context"> static int vhost_net_open(struct inode *inode, struct file *f)</span>
 	struct vhost_virtqueue **vqs;
 	int i;
 
<span class="p_del">-	n = kmalloc(sizeof *n, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	n = kmalloc(sizeof *n, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!n) {
 		n = vmalloc(sizeof *n);
 		if (!n)
<span class="p_header">diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c</span>
<span class="p_header">index 9d6320e8ff3e..7150b45e092f 100644</span>
<span class="p_header">--- a/drivers/vhost/scsi.c</span>
<span class="p_header">+++ b/drivers/vhost/scsi.c</span>
<span class="p_chunk">@@ -1405,7 +1405,7 @@</span> <span class="p_context"> static int vhost_scsi_open(struct inode *inode, struct file *f)</span>
 	struct vhost_virtqueue **vqs;
 	int r = -ENOMEM, i;
 
<span class="p_del">-	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!vs) {
 		vs = vzalloc(sizeof(*vs));
 		if (!vs)
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index 669fef1e2bb6..a4b0f18a69ab 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -707,7 +707,7 @@</span> <span class="p_context"> static int vhost_memory_reg_sort_cmp(const void *p1, const void *p2)</span>
 
 static void *vhost_kvzalloc(unsigned long size)
 {
<span class="p_del">-	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	void *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 
 	if (!n)
 		n = vzalloc(size);
<span class="p_header">diff --git a/fs/btrfs/check-integrity.c b/fs/btrfs/check-integrity.c</span>
<span class="p_header">index b677a6ea6001..f8658fbbfa60 100644</span>
<span class="p_header">--- a/fs/btrfs/check-integrity.c</span>
<span class="p_header">+++ b/fs/btrfs/check-integrity.c</span>
<span class="p_chunk">@@ -3049,7 +3049,7 @@</span> <span class="p_context"> int btrfsic_mount(struct btrfs_root *root,</span>
 		       root-&gt;sectorsize, PAGE_SIZE);
 		return -1;
 	}
<span class="p_del">-	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!state) {
 		state = vzalloc(sizeof(*state));
 		if (!state) {
<span class="p_header">diff --git a/fs/btrfs/raid56.c b/fs/btrfs/raid56.c</span>
<span class="p_header">index f8b6d411a034..db2bba5cb90a 100644</span>
<span class="p_header">--- a/fs/btrfs/raid56.c</span>
<span class="p_header">+++ b/fs/btrfs/raid56.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> int btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)</span>
 	 * of a failing mount.
 	 */
 	table_size = sizeof(*table) + sizeof(*h) * num_entries;
<span class="p_del">-	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!table) {
 		table = vzalloc(table_size);
 		if (!table)
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index c29e9d347bc6..9961086eac2e 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define ___GFP_FS		0x80u
 #define ___GFP_COLD		0x100u
 #define ___GFP_NOWARN		0x200u
<span class="p_del">-#define ___GFP_REPEAT		0x400u</span>
<span class="p_add">+#define ___GFP_RETRY_HARD		0x400u</span>
 #define ___GFP_NOFAIL		0x800u
 #define ___GFP_NORETRY		0x1000u
 #define ___GFP_MEMALLOC		0x2000u
<span class="p_chunk">@@ -132,26 +132,38 @@</span> <span class="p_context"> struct vm_area_struct;</span>
  *
  * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.
  *
<span class="p_del">- * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="p_del">- *   _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="p_add">+ * The default allocator behavior depends on the request size. We have a concept</span>
<span class="p_add">+ * of so called costly allocations (with order &gt; PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="p_add">+ * !costly allocations are too essential to fail so they are implicitly</span>
<span class="p_add">+ * non-failing (with some exceptions like OOM victims might fail) by default while</span>
<span class="p_add">+ * costly requests try to be not disruptive and back off even without invoking</span>
<span class="p_add">+ * the OOM killer. The following three modifiers might be used to override some of</span>
<span class="p_add">+ * these implicit rules</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="p_add">+ *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="p_add">+ *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="p_add">+ *   implementation. This is a default mode for costly allocations.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __GFP_RETRY_HARD: Try hard to allocate the memory, but the allocation attempt</span>
<span class="p_add">+ *   _might_ fail. All viable forms of memory reclaim are tried before the fail</span>
<span class="p_add">+ *   including the OOM killer for !costly allocations. This can be used to override</span>
<span class="p_add">+ *   non-failing default behavior for !costly requests as well as fortify costly</span>
<span class="p_add">+ *   requests.</span>
  *
  * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
  *   cannot handle allocation failures. New users should be evaluated carefully
  *   (and the flag should be used only when there is no reasonable failure
  *   policy) but it is definitely preferable to use the flag rather than
<span class="p_del">- *   opencode endless loop around allocator.</span>
<span class="p_del">- *</span>
<span class="p_del">- * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="p_del">- *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="p_del">- *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="p_del">- *   implementation.</span>
<span class="p_add">+ *   opencode endless loop around allocator. Using this flag for costly allocations</span>
<span class="p_add">+ *   is _highly_ discouraged.</span>
  */
 #define __GFP_IO	((__force gfp_t)___GFP_IO)
 #define __GFP_FS	((__force gfp_t)___GFP_FS)
 #define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */
 #define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */
 #define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))
<span class="p_del">-#define __GFP_REPEAT	((__force gfp_t)___GFP_REPEAT)</span>
<span class="p_add">+#define __GFP_RETRY_HARD	((__force gfp_t)___GFP_RETRY_HARD)</span>
 #define __GFP_NOFAIL	((__force gfp_t)___GFP_NOFAIL)
 #define __GFP_NORETRY	((__force gfp_t)___GFP_NORETRY)
 
<span class="p_header">diff --git a/include/linux/slab.h b/include/linux/slab.h</span>
<span class="p_header">index aeb3e6d00a66..cacd437fdbf4 100644</span>
<span class="p_header">--- a/include/linux/slab.h</span>
<span class="p_header">+++ b/include/linux/slab.h</span>
<span class="p_chunk">@@ -457,7 +457,8 @@</span> <span class="p_context"> static __always_inline void *kmalloc_large(size_t size, gfp_t flags)</span>
  *
  * %__GFP_NOWARN - If allocation fails, don&#39;t issue any warnings.
  *
<span class="p_del">- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.</span>
<span class="p_add">+ * %__GFP_RETRY_HARD - Try really hard to succeed the allocation but fail</span>
<span class="p_add">+ *   eventually.</span>
  *
  * There are other flags available as well, but these are not intended
  * for general use, and so are not documented here. For a full list of
<span class="p_header">diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h</span>
<span class="p_header">index 43cedbf0c759..c5f488767860 100644</span>
<span class="p_header">--- a/include/trace/events/mmflags.h</span>
<span class="p_header">+++ b/include/trace/events/mmflags.h</span>
<span class="p_chunk">@@ -30,7 +30,7 @@</span> <span class="p_context"></span>
 	{(unsigned long)__GFP_FS,		&quot;__GFP_FS&quot;},		\
 	{(unsigned long)__GFP_COLD,		&quot;__GFP_COLD&quot;},		\
 	{(unsigned long)__GFP_NOWARN,		&quot;__GFP_NOWARN&quot;},	\
<span class="p_del">-	{(unsigned long)__GFP_REPEAT,		&quot;__GFP_REPEAT&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_RETRY_HARD,	&quot;__GFP_RETRY_HARD&quot;},	\</span>
 	{(unsigned long)__GFP_NOFAIL,		&quot;__GFP_NOFAIL&quot;},	\
 	{(unsigned long)__GFP_NORETRY,		&quot;__GFP_NORETRY&quot;},	\
 	{(unsigned long)__GFP_COMP,		&quot;__GFP_COMP&quot;},		\
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index acd374e200cf..69872951f653 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -378,7 +378,7 @@</span> <span class="p_context"> static ssize_t single_flag_store(struct kobject *kobj,</span>
 
 /*
  * Currently defrag only disables __GFP_NOWAIT for allocation. A blind
<span class="p_del">- * __GFP_REPEAT is too aggressive, it&#39;s never worth swapping tons of</span>
<span class="p_add">+ * __GFP_RETRY_HARD is too aggressive, it&#39;s never worth swapping tons of</span>
  * memory just to allocate one more hugepage.
  */
 static ssize_t defrag_show(struct kobject *kobj,
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index e197cd7080e6..62306b5a302a 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1363,7 +1363,7 @@</span> <span class="p_context"> static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
 
 	page = __alloc_pages_node(nid,
 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
<span class="p_del">-						__GFP_REPEAT|__GFP_NOWARN,</span>
<span class="p_add">+						__GFP_RETRY_HARD|__GFP_NOWARN,</span>
 		huge_page_order(h));
 	if (page) {
 		prep_new_huge_page(h, page, nid);
<span class="p_chunk">@@ -1480,7 +1480,7 @@</span> <span class="p_context"> static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
 		struct vm_area_struct *vma, unsigned long addr, int nid)
 {
 	int order = huge_page_order(h);
<span class="p_del">-	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="p_add">+	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_RETRY_HARD|__GFP_NOWARN;</span>
 	unsigned int cpuset_mems_cookie;
 
 	/*
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 420bbe300bcd..083c87c539b6 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -23,7 +23,7 @@</span> <span class="p_context"></span>
  * hints such as HIGHMEM usage.
  */
 #define GFP_RECLAIM_MASK (__GFP_RECLAIM|__GFP_HIGH|__GFP_IO|__GFP_FS|\
<span class="p_del">-			__GFP_NOWARN|__GFP_REPEAT|__GFP_NOFAIL|\</span>
<span class="p_add">+			__GFP_NOWARN|__GFP_RETRY_HARD|__GFP_NOFAIL|\</span>
 			__GFP_NORETRY|__GFP_MEMALLOC|__GFP_NOMEMALLOC)
 
 /* The GFP flags allowed during early boot */
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 180f5afc5a1f..faa3d4a27850 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3262,7 +3262,7 @@</span> <span class="p_context"> should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,</span>
 		return compaction_zonelist_suitable(ac, order, alloc_flags);
 
 	/*
<span class="p_del">-	 * !costly requests are much more important than __GFP_REPEAT</span>
<span class="p_add">+	 * !costly requests are much more important than __GFP_RETRY_HARD</span>
 	 * costly ones because they are de facto nofail and invoke OOM
 	 * killer to move on while costly can fail and users are ready
 	 * to cope with that. 1/4 retries is rather arbitrary but we
<span class="p_chunk">@@ -3550,6 +3550,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	enum compact_result compact_result;
 	int compaction_retries = 0;
 	int no_progress_loops = 0;
<span class="p_add">+	bool passed_oom = false;</span>
 
 	/*
 	 * In the slowpath, we sanity check order to avoid ever trying to
<span class="p_chunk">@@ -3680,9 +3681,9 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 
 	/*
 	 * Do not retry costly high order allocations unless they are
<span class="p_del">-	 * __GFP_REPEAT</span>
<span class="p_add">+	 * __GFP_RETRY_HARD</span>
 	 */
<span class="p_del">-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="p_add">+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_HARD))</span>
 		goto noretry;
 
 	/*
<span class="p_chunk">@@ -3711,6 +3712,17 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 				compaction_retries))
 		goto retry;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We have already exhausted all our reclaim opportunities including</span>
<span class="p_add">+	 * the OOM killer without any success so it is time to admit defeat.</span>
<span class="p_add">+	 * We do not care about the order because we want all orders to behave</span>
<span class="p_add">+	 * consistently including !costly ones. costly are handled in</span>
<span class="p_add">+	 * __alloc_pages_may_oom and will bail out even before the first OOM</span>
<span class="p_add">+	 * killer invocation</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (passed_oom &amp;&amp; (gfp_mask &amp; __GFP_RETRY_HARD))</span>
<span class="p_add">+		goto nopage;</span>
<span class="p_add">+</span>
 	/* Reclaim has failed us, start killing things */
 	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);
 	if (page)
<span class="p_chunk">@@ -3719,6 +3731,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	/* Retry as long as the OOM killer is making progress */
 	if (did_some_progress) {
 		no_progress_loops = 0;
<span class="p_add">+		passed_oom = true;</span>
 		goto retry;
 	}
 
<span class="p_header">diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c</span>
<span class="p_header">index 68885dcbaf40..261facd8e1c8 100644</span>
<span class="p_header">--- a/mm/sparse-vmemmap.c</span>
<span class="p_header">+++ b/mm/sparse-vmemmap.c</span>
<span class="p_chunk">@@ -56,11 +56,11 @@</span> <span class="p_context"> void * __meminit vmemmap_alloc_block(unsigned long size, int node)</span>
 
 		if (node_state(node, N_HIGH_MEMORY))
 			page = alloc_pages_node(
<span class="p_del">-				node, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="p_add">+				node, GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_HARD,</span>
 				get_order(size));
 		else
 			page = alloc_pages(
<span class="p_del">-				GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="p_add">+				GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_HARD,</span>
 				get_order(size));
 		if (page)
 			return page_address(page);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 93ba33789ac6..ff21efe06430 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -2319,18 +2319,18 @@</span> <span class="p_context"> static inline bool should_continue_reclaim(struct zone *zone,</span>
 		return false;
 
 	/* Consider stopping depending on scan and reclaim activity */
<span class="p_del">-	if (sc-&gt;gfp_mask &amp; __GFP_REPEAT) {</span>
<span class="p_add">+	if (sc-&gt;gfp_mask &amp; __GFP_RETRY_HARD) {</span>
 		/*
<span class="p_del">-		 * For __GFP_REPEAT allocations, stop reclaiming if the</span>
<span class="p_add">+		 * For __GFP_RETRY_HARD allocations, stop reclaiming if the</span>
 		 * full LRU list has been scanned and we are still failing
 		 * to reclaim pages. This full LRU scan is potentially
<span class="p_del">-		 * expensive but a __GFP_REPEAT caller really wants to succeed</span>
<span class="p_add">+		 * expensive but a __GFP_RETRY_HARD caller really wants to succeed</span>
 		 */
 		if (!nr_reclaimed &amp;&amp; !nr_scanned)
 			return false;
 	} else {
 		/*
<span class="p_del">-		 * For non-__GFP_REPEAT allocations which can presumably</span>
<span class="p_add">+		 * For non-__GFP_RETRY_HARD allocations which can presumably</span>
 		 * fail without consequence, stop if we failed to reclaim
 		 * any pages from the last SWAP_CLUSTER_MAX number of
 		 * pages that were scanned. This will return to the
<span class="p_header">diff --git a/net/core/dev.c b/net/core/dev.c</span>
<span class="p_header">index 904ff431d570..8a916dd1d833 100644</span>
<span class="p_header">--- a/net/core/dev.c</span>
<span class="p_header">+++ b/net/core/dev.c</span>
<span class="p_chunk">@@ -6897,7 +6897,7 @@</span> <span class="p_context"> static int netif_alloc_rx_queues(struct net_device *dev)</span>
 
 	BUG_ON(count &lt; 1);
 
<span class="p_del">-	rx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	rx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!rx) {
 		rx = vzalloc(sz);
 		if (!rx)
<span class="p_chunk">@@ -6939,7 +6939,7 @@</span> <span class="p_context"> static int netif_alloc_netdev_queues(struct net_device *dev)</span>
 	if (count &lt; 1 || count &gt; 0xffff)
 		return -EINVAL;
 
<span class="p_del">-	tx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	tx = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!tx) {
 		tx = vzalloc(sz);
 		if (!tx)
<span class="p_chunk">@@ -7477,7 +7477,7 @@</span> <span class="p_context"> struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,</span>
 	/* ensure 32-byte alignment of whole construct */
 	alloc_size += NETDEV_ALIGN - 1;
 
<span class="p_del">-	p = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	p = kzalloc(alloc_size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_HARD);</span>
 	if (!p)
 		p = vzalloc(alloc_size);
 	if (!p)
<span class="p_header">diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="p_header">index e7ec6d3ad5f0..c4c667a7b39b 100644</span>
<span class="p_header">--- a/net/core/skbuff.c</span>
<span class="p_header">+++ b/net/core/skbuff.c</span>
<span class="p_chunk">@@ -4621,7 +4621,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 
 	gfp_head = gfp_mask;
 	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)
<span class="p_del">-		gfp_head |= __GFP_REPEAT;</span>
<span class="p_add">+		gfp_head |= __GFP_RETRY_HARD;</span>
 
 	*errcode = -ENOBUFS;
 	skb = alloc_skb(header_len, gfp_head);
<span class="p_header">diff --git a/net/sched/sch_fq.c b/net/sched/sch_fq.c</span>
<span class="p_header">index 3c6a47d66a04..7ed2224df628 100644</span>
<span class="p_header">--- a/net/sched/sch_fq.c</span>
<span class="p_header">+++ b/net/sched/sch_fq.c</span>
<span class="p_chunk">@@ -599,7 +599,7 @@</span> <span class="p_context"> static void *fq_alloc_node(size_t sz, int node)</span>
 {
 	void *ptr;
 
<span class="p_del">-	ptr = kmalloc_node(sz, GFP_KERNEL | __GFP_REPEAT | __GFP_NOWARN, node);</span>
<span class="p_add">+	ptr = kmalloc_node(sz, GFP_KERNEL | __GFP_RETRY_HARD | __GFP_NOWARN, node);</span>
 	if (!ptr)
 		ptr = vmalloc_node(sz, node);
 	return ptr;
<span class="p_header">diff --git a/tools/perf/builtin-kmem.c b/tools/perf/builtin-kmem.c</span>
<span class="p_header">index 58adfee230de..a63f8f4e6a6a 100644</span>
<span class="p_header">--- a/tools/perf/builtin-kmem.c</span>
<span class="p_header">+++ b/tools/perf/builtin-kmem.c</span>
<span class="p_chunk">@@ -627,7 +627,7 @@</span> <span class="p_context"> static const struct {</span>
 	{ &quot;__GFP_FS&quot;,			&quot;F&quot; },
 	{ &quot;__GFP_COLD&quot;,			&quot;CO&quot; },
 	{ &quot;__GFP_NOWARN&quot;,		&quot;NWR&quot; },
<span class="p_del">-	{ &quot;__GFP_REPEAT&quot;,		&quot;R&quot; },</span>
<span class="p_add">+	{ &quot;__GFP_RETRY_HARD&quot;,		&quot;R&quot; },</span>
 	{ &quot;__GFP_NOFAIL&quot;,		&quot;NF&quot; },
 	{ &quot;__GFP_NORETRY&quot;,		&quot;NR&quot; },
 	{ &quot;__GFP_COMP&quot;,			&quot;C&quot; },

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



