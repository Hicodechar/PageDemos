
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/2] perf: add arm64 smmuv3 pmu driver - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/2] perf: add arm64 smmuv3 pmu driver</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=8626">Neil Leeder</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 4, 2017, 7:59 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1501876754-1064-3-git-send-email-nleeder@codeaurora.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9881969/mbox/"
   >mbox</a>
|
   <a href="/patch/9881969/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9881969/">/patch/9881969/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A9A7060375 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Aug 2017 20:00:05 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 987B7289E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Aug 2017 20:00:05 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8D2B9289EF; Fri,  4 Aug 2017 20:00:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ED15B289E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  4 Aug 2017 20:00:03 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752173AbdHDUAB (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 4 Aug 2017 16:00:01 -0400
Received: from smtp.codeaurora.org ([198.145.29.96]:38062 &quot;EHLO
	smtp.codeaurora.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751870AbdHDT76 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 4 Aug 2017 15:59:58 -0400
Received: by smtp.codeaurora.org (Postfix, from userid 1000)
	id 3FAE9606DC; Fri,  4 Aug 2017 19:59:58 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1501876798;
	bh=RyCZq7OpzG4MundjQLsSnQaXuRkLDbKHhe+tWfCcNDA=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=gqTaAjNzNZamKroSgKnBsnvTFiXGd/Mv8xyJflrLEUlL2px73rT7f7+0fqHfpUzK9
	w82LIsE6EIksl2kNwBskDAO4/ZuuEE292rg7xtm5ZoBFGZ8BHSBUwKnslaX5LoWL2Q
	pBqRgNZufcg/jK/bIQH210qd3UVeqNDOI6xiuiNA=
Received: from codeaurora.org (global_nat1_iad_fw.qualcomm.com
	[129.46.232.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-SHA256 (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: nleeder@smtp.codeaurora.org)
	by smtp.codeaurora.org (Postfix) with ESMTPSA id E70AE60251;
	Fri,  4 Aug 2017 19:59:55 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1501876796;
	bh=RyCZq7OpzG4MundjQLsSnQaXuRkLDbKHhe+tWfCcNDA=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=U7oR0gU1DChPq/jIXjOaoMpuIv71zqabTciieU7KtOUJSh2Fm4GU3bd2JQs28Ojor
	Gty0gBDe/eFntOz7wzv1PfRq8rp6yj/K4UOWscuH9j0dfYR7HWH4SclG9m7eKo0di2
	5GvlKlpaoWnjfRrIBSW8DWQBzXO2F5K5HWGl0el4=
DMARC-Filter: OpenDMARC Filter v1.3.2 smtp.codeaurora.org E70AE60251
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	dmarc=none (p=none dis=none)
	header.from=codeaurora.org
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	spf=none smtp.mailfrom=nleeder@codeaurora.org
From: Neil Leeder &lt;nleeder@codeaurora.org&gt;
To: Will Deacon &lt;will.deacon@arm.com&gt;, Mark Rutland &lt;mark.rutland@arm.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-arm-kernel@lists.infradead.org,
	Mark Langsdorf &lt;mlangsdo@redhat.com&gt;,
	Mark Salter &lt;msalter@redhat.com&gt;, Jon Masters &lt;jcm@redhat.com&gt;,
	Timur Tabi &lt;timur@codeaurora.org&gt;,
	Mark Brown &lt;broonie@kernel.org&gt;, nleeder@codeaurora.org
Subject: [PATCH 2/2] perf: add arm64 smmuv3 pmu driver
Date: Fri,  4 Aug 2017 15:59:14 -0400
Message-Id: &lt;1501876754-1064-3-git-send-email-nleeder@codeaurora.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1501876754-1064-1-git-send-email-nleeder@codeaurora.org&gt;
References: &lt;1501876754-1064-1-git-send-email-nleeder@codeaurora.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=8626">Neil Leeder</a> - Aug. 4, 2017, 7:59 p.m.</div>
<pre class="content">
Adds a new driver to support the SMMU v3 PMU and add it into the
perf events framework.

Each SMMU node may have multiple PMUs associated with it, each of
which may support different events.

PMUs are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;
is the physical page address of the SMMU PMCG.
For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840

Filtering by stream id is done by specifying filtering parameters
with the event. Options are:
  filter_enable    - 0 = no filtering, 1 = filtering enabled
  filter_span      - 0 = exact match, 1 = pattern match
  filter_sec       - applies to non-secure (0) or secure (1) namespace
  filter_stream_id - pattern to filter against
Further filtering information is available in the SMMU documentation.

Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,
                      filter_span=1,filter_stream_id=0x42/ -a pwd
Applies filter pattern 0x42 to transaction events.

SMMU events are not attributable to a CPU, so task mode and sampling
are not supported.
<span class="signed-off-by">
Signed-off-by: Neil Leeder &lt;nleeder@codeaurora.org&gt;</span>
---
 drivers/perf/Kconfig          |   9 +
 drivers/perf/Makefile         |   1 +
 drivers/perf/arm_smmuv3_pmu.c | 813 ++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 823 insertions(+)
 create mode 100644 drivers/perf/arm_smmuv3_pmu.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - Aug. 7, 2017, 2:31 p.m.</div>
<pre class="content">
On 04/08/17 20:59, Neil Leeder wrote:
<span class="quote">&gt; Adds a new driver to support the SMMU v3 PMU and add it into the</span>
<span class="quote">&gt; perf events framework.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Each SMMU node may have multiple PMUs associated with it, each of</span>
<span class="quote">&gt; which may support different events.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; PMUs are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="quote">&gt; is the physical page address of the SMMU PMCG.</span>
<span class="quote">&gt; For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>

This seems a bit rough - is it at feasible to at least chase the node
reference and namespace them by the associated component, e.g. something
like &quot;arm-smmu-v3.x:pmcg.y&quot;? The user can always dig the associated
physical address out of /proc/iomem if necessary.
<span class="quote">
&gt; Filtering by stream id is done by specifying filtering parameters</span>
<span class="quote">&gt; with the event. Options are:</span>
<span class="quote">&gt;   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="quote">&gt;   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="quote">&gt;   filter_sec       - applies to non-secure (0) or secure (1) namespace</span>

I&#39;m a little dubious as to how useful it is to expose this, since we
can&#39;t see the true value of SCR.SO so have no way of knowing what we&#39;ll
actually end up counting.
<span class="quote">
&gt;   filter_stream_id - pattern to filter against</span>
<span class="quote">&gt; Further filtering information is available in the SMMU documentation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,</span>
<span class="quote">&gt;                       filter_span=1,filter_stream_id=0x42/ -a pwd</span>
<span class="quote">&gt; Applies filter pattern 0x42 to transaction events.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; SMMU events are not attributable to a CPU, so task mode and sampling</span>
<span class="quote">&gt; are not supported.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Neil Leeder &lt;nleeder@codeaurora.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/perf/Kconfig          |   9 +</span>
<span class="quote">&gt;  drivers/perf/Makefile         |   1 +</span>
<span class="quote">&gt;  drivers/perf/arm_smmuv3_pmu.c | 813 ++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  3 files changed, 823 insertions(+)</span>
<span class="quote">&gt;  create mode 100644 drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/perf/Kconfig b/drivers/perf/Kconfig</span>
<span class="quote">&gt; index e5197ff..e7721d1 100644</span>
<span class="quote">&gt; --- a/drivers/perf/Kconfig</span>
<span class="quote">&gt; +++ b/drivers/perf/Kconfig</span>
<span class="quote">&gt; @@ -17,6 +17,15 @@ config ARM_PMU_ACPI</span>
<span class="quote">&gt;  	depends on ARM_PMU &amp;&amp; ACPI</span>
<span class="quote">&gt;  	def_bool y</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config ARM_SMMUV3_PMU</span>
<span class="quote">&gt; +	 bool &quot;ARM SMMUv3 PMU&quot;</span>
<span class="quote">&gt; +	 depends on PERF_EVENTS &amp;&amp; ARM64 &amp;&amp; ACPI</span>

PERF_EVENTS is already a top-level dependency now.
<span class="quote">
&gt; +	   help</span>
<span class="quote">&gt; +	   Provides support for the SMMU version 3 performance monitor unit (PMU)</span>
<span class="quote">&gt; +	   on ARM-based systems.</span>
<span class="quote">&gt; +	   Adds the SMMU PMU into the perf events subsystem for</span>
<span class="quote">&gt; +	   monitoring SMMU performance events.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config QCOM_L2_PMU</span>
<span class="quote">&gt;  	bool &quot;Qualcomm Technologies L2-cache PMU&quot;</span>
<span class="quote">&gt;  	depends on ARCH_QCOM &amp;&amp; ARM64 &amp;&amp; ACPI</span>
<span class="quote">&gt; diff --git a/drivers/perf/Makefile b/drivers/perf/Makefile</span>
<span class="quote">&gt; index 6420bd4..3012f5e 100644</span>
<span class="quote">&gt; --- a/drivers/perf/Makefile</span>
<span class="quote">&gt; +++ b/drivers/perf/Makefile</span>
<span class="quote">&gt; @@ -1,5 +1,6 @@</span>
<span class="quote">&gt;  obj-$(CONFIG_ARM_PMU) += arm_pmu.o arm_pmu_platform.o</span>
<span class="quote">&gt;  obj-$(CONFIG_ARM_PMU_ACPI) += arm_pmu_acpi.o</span>
<span class="quote">&gt; +obj-$(CONFIG_ARM_SMMUV3_PMU) += arm_smmuv3_pmu.o</span>
<span class="quote">&gt;  obj-$(CONFIG_QCOM_L2_PMU)	+= qcom_l2_pmu.o</span>
<span class="quote">&gt;  obj-$(CONFIG_QCOM_L3_PMU) += qcom_l3_pmu.o</span>
<span class="quote">&gt;  obj-$(CONFIG_XGENE_PMU) += xgene_pmu.o</span>
<span class="quote">&gt; diff --git a/drivers/perf/arm_smmuv3_pmu.c b/drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..1e70791</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; @@ -0,0 +1,813 @@</span>
<span class="quote">&gt; +/* Copyright (c) 2017 The Linux Foundation. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License version 2 and</span>
<span class="quote">&gt; + * only version 2 as published by the Free Software Foundation.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * This driver adds support for perf events to use the Performance</span>
<span class="quote">&gt; + * Monitor Counter Groups (PMCG) associated with an SMMUv3 node</span>
<span class="quote">&gt; + * to monitor that node.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Devices are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="quote">&gt; + * is the physical page address of the SMMU PMCG.</span>
<span class="quote">&gt; + * For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Filtering by stream id is done by specifying filtering parameters</span>
<span class="quote">&gt; + * with the event. options are:</span>
<span class="quote">&gt; + *   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="quote">&gt; + *   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="quote">&gt; + *   filter_sec       - filter applies to non-secure (0) or secure (1) namespace</span>
<span class="quote">&gt; + *   filter_stream_id - pattern to filter against</span>
<span class="quote">&gt; + * Further filtering information is available in the SMMU documentation.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,</span>
<span class="quote">&gt; + *                       filter_span=1,filter_stream_id=0x42/ -a pwd</span>
<span class="quote">&gt; + * Applies filter pattern 0x42 to transaction events.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * SMMU events are not attributable to a CPU, so task mode and sampling</span>
<span class="quote">&gt; + * are not supported.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/acpi.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/acpi_iort.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/bitops.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/cpuhotplug.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/device.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/errno.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/interrupt.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/irq.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/list.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/msi.h&gt;</span>

Is MSI support planned?
<span class="quote">
&gt; +#include &lt;linux/perf_event.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/platform_device.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/smp.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/sysfs.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/local64.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVCNTR0               0x0</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVCNTR(n, stride)     (SMMU_PMCG_EVCNTR0 + (n) * (stride))</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVTYPER0              0x400</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVTYPER(n)            (SMMU_PMCG_EVTYPER0 + (n) * 4)</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVTYPER_SEC_SID_SHIFT       30</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT      29</span>
<span class="quote">&gt; +#define SMMU_PMCG_EVTYPER_EVENT_MASK          GENMASK(15, 0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_SVR0                  0x600</span>
<span class="quote">&gt; +#define SMMU_PMCG_SVR(n, stride)        (SMMU_PMCG_SVR0 + (n) * (stride))</span>
<span class="quote">&gt; +#define SMMU_PMCG_SMR0                  0xA00</span>
<span class="quote">&gt; +#define SMMU_PMCG_SMR(n)                (SMMU_PMCG_SMR0 + (n) * 4)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CNTENSET0             0xC00</span>
<span class="quote">&gt; +#define SMMU_PMCG_CNTENCLR0             0xC20</span>
<span class="quote">&gt; +#define SMMU_PMCG_INTENSET0             0xC40</span>
<span class="quote">&gt; +#define SMMU_PMCG_INTENCLR0             0xC60</span>
<span class="quote">&gt; +#define SMMU_PMCG_OVSCLR0               0xC80</span>
<span class="quote">&gt; +#define SMMU_PMCG_OVSSET0               0xCC0</span>
<span class="quote">&gt; +#define SMMU_PMCG_CAPR                  0xD88</span>
<span class="quote">&gt; +#define SMMU_PMCG_SCR                   0xDF8</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR                  0xE00</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_SID_FILTER_TYPE        BIT(23)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_CAPTURE                BIT(22)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_MSI                    BIT(21)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_RELOC_CTRS             BIT(20)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_SIZE_MASK              GENMASK(13, 8)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_SIZE_SHIFT             8</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_COUNTER_SIZE_32        31</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_NCTR_MASK              GENMASK(5, 0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CFGR_NCTR_SHIFT             0</span>
<span class="quote">&gt; +#define SMMU_PMCG_CR                    0xE04</span>
<span class="quote">&gt; +#define SMMU_PMCG_CR_ENABLE                   BIT(0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_CEID0                 0xE20</span>
<span class="quote">&gt; +#define SMMU_PMCG_CEID1                 0xE28</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CTRL              0xE50</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CTRL_IRQEN              BIT(0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CTRLACK           0xE54</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CTRLACK_IRQEN           BIT(0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CFG0              0xE58</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CFG0_ADDR_MASK          GENMASK(51, 2)</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CFG1              0xE60</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_CFG2              0xE64</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_STATUS            0xE68</span>
<span class="quote">&gt; +#define SMMU_PMCG_IRQ_STATUS_IRQ_ABT          BIT(0)</span>
<span class="quote">&gt; +#define SMMU_PMCG_AIDR                  0xE70</span>

Several of these are unused (although at least IRQ0_CFG1 probably should
be, to zero it to a known state if we aren&#39;t supporting MSIs).
<span class="quote">
&gt; +#define SMMU_COUNTER_RELOAD             BIT(31)</span>
<span class="quote">&gt; +#define SMMU_DEFAULT_FILTER_SEC         0</span>
<span class="quote">&gt; +#define SMMU_DEFAULT_FILTER_SPAN        1</span>
<span class="quote">&gt; +#define SMMU_DEFAULT_FILTER_STREAM_ID   GENMASK(31, 0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SMMU_MAX_COUNTERS               64</span>
<span class="quote">&gt; +#define SMMU_MAX_EVENT_ID               128</span>
<span class="quote">&gt; +#define SMMU_NUM_EVENTS_U32             (SMMU_MAX_EVENT_ID / sizeof(u32))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SMMU_PA_SHIFT                   12</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Events */</span>
<span class="quote">&gt; +#define SMMU_PMU_CYCLES                 0</span>
<span class="quote">&gt; +#define SMMU_PMU_TRANSACTION            1</span>
<span class="quote">&gt; +#define SMMU_PMU_TLB_MISS               2</span>
<span class="quote">&gt; +#define SMMU_PMU_CONFIG_CACHE_MISS      3</span>
<span class="quote">&gt; +#define SMMU_PMU_TRANS_TABLE_WALK       4</span>
<span class="quote">&gt; +#define SMMU_PMU_CONFIG_STRUCT_ACCESS   5</span>
<span class="quote">&gt; +#define SMMU_PMU_PCIE_ATS_TRANS_RQ      6</span>
<span class="quote">&gt; +#define SMMU_PMU_PCIE_ATS_TRANS_PASSED  7</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpuhp_state_num;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct smmu_pmu {</span>
<span class="quote">&gt; +	struct hlist_node node;</span>
<span class="quote">&gt; +	struct perf_event *events[SMMU_MAX_COUNTERS];</span>
<span class="quote">&gt; +	DECLARE_BITMAP(used_counters, SMMU_MAX_COUNTERS);</span>
<span class="quote">&gt; +	DECLARE_BITMAP(supported_events, SMMU_MAX_EVENT_ID);</span>
<span class="quote">&gt; +	unsigned int irq;</span>
<span class="quote">&gt; +	unsigned int on_cpu;</span>
<span class="quote">&gt; +	struct pmu pmu;</span>
<span class="quote">&gt; +	unsigned int num_counters;</span>
<span class="quote">&gt; +	struct platform_device *pdev;</span>
<span class="quote">&gt; +	void __iomem *reg_base;</span>
<span class="quote">&gt; +	void __iomem *reloc_base;</span>
<span class="quote">&gt; +	u64 counter_present_mask;</span>
<span class="quote">&gt; +	u64 counter_mask;</span>
<span class="quote">&gt; +	bool reg_size_32;</span>

This guy is redundant...
<span class="quote">
&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define to_smmu_pmu(p) (container_of(p, struct smmu_pmu, pmu))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SMMU_PMU_EVENT_ATTR_EXTRACTOR(_name, _config, _size, _shift)    \</span>
<span class="quote">&gt; +	static inline u32 get_##_name(struct perf_event *event)         \</span>
<span class="quote">&gt; +	{                                                               \</span>
<span class="quote">&gt; +		return (event-&gt;attr._config &gt;&gt; (_shift)) &amp;              \</span>
<span class="quote">&gt; +			GENMASK_ULL((_size) - 1, 0);                    \</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(event, config, 16, 0);</span>
<span class="quote">&gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_stream_id, config1, 32, 0);</span>
<span class="quote">&gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_span, config1, 1, 32);</span>
<span class="quote">&gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_sec, config1, 1, 33);</span>
<span class="quote">&gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_enable, config1, 1, 34);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_enable(struct pmu *pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	writel(SMMU_PMCG_CR_ENABLE, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="quote">&gt; +	writel(SMMU_PMCG_IRQ_CTRL_IRQEN,</span>
<span class="quote">&gt; +	       smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_disable(struct pmu *pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="quote">&gt; +	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_counter_set_value(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; +					      u32 idx, u64 value)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (smmu_pmu-&gt;reg_size_32)</span>

...since it would be just as efficient to directly test
smmu_pmu-&gt;counter_mask &amp; BIT(32) here and below.
<span class="quote">
&gt; +		writel(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		writeq(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline u64 smmu_pmu_counter_get_value(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u64 value;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="quote">&gt; +		value = readl(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		value = readq(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return value;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_counter_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENSET0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_counter_disable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_interrupt_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENSET0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_interrupt_disable(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; +					      u32 idx)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_reset(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; smmu_pmu-&gt;num_counters; i++) {</span>
<span class="quote">&gt; +		smmu_pmu_counter_disable(smmu_pmu, i);</span>
<span class="quote">&gt; +		smmu_pmu_interrupt_disable(smmu_pmu, i);</span>
<span class="quote">&gt; +	}</span>

Surely it would be far quicker and simpler to do this?

	writeq(smmu_pmu-&gt;counter_present_mask,
		smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);
	writeq(smmu_pmu-&gt;counter_present_mask,
		smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);
<span class="quote">
&gt; +	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_set_evtyper(struct smmu_pmu *smmu_pmu, u32 idx,</span>
<span class="quote">&gt; +					u32 val)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_EVTYPER(idx));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void smmu_pmu_set_smr(struct smmu_pmu *smmu_pmu, u32 idx, u32 val)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_SMR(idx));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline u64 smmu_pmu_getreset_ovsr(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u64 result = readq_relaxed(smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSSET0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	writeq(result, smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSCLR0);</span>
<span class="quote">&gt; +	return result;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool smmu_pmu_has_overflowed(struct smmu_pmu *smmu_pmu, u64 ovsr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !!(ovsr &amp; smmu_pmu-&gt;counter_present_mask);</span>
<span class="quote">&gt; +}</span>

Personally, I find these helpers abstracting simple reads/writes
actually make the code harder to follow, especially when they&#39;re each
used a grand total of once. That may well just be me, though.
<span class="quote">
&gt; +static void smmu_pmu_event_update(struct perf_event *event)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +	u64 delta, prev, now;</span>
<span class="quote">&gt; +	u32 idx = hwc-&gt;idx;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	do {</span>
<span class="quote">&gt; +		prev = local64_read(&amp;hwc-&gt;prev_count);</span>
<span class="quote">&gt; +		now = smmu_pmu_counter_get_value(smmu_pmu, idx);</span>
<span class="quote">&gt; +	} while (local64_cmpxchg(&amp;hwc-&gt;prev_count, prev, now) != prev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* handle overflow. */</span>
<span class="quote">&gt; +	delta = now - prev;</span>
<span class="quote">&gt; +	delta &amp;= smmu_pmu-&gt;counter_mask;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local64_add(delta, &amp;event-&gt;count);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_set_period(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; +				struct hw_perf_event *hwc)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u32 idx = hwc-&gt;idx;</span>
<span class="quote">&gt; +	u64 new;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We limit the max period to half the max counter value of the smallest</span>
<span class="quote">&gt; +	 * counter size, so that even in the case of extreme interrupt latency</span>
<span class="quote">&gt; +	 * the counter will (hopefully) not wrap past its initial value.</span>
<span class="quote">&gt; +	 */</span>

Having once fought to properly understand the underlying logic I despise
this unhelpfully-vague comment, but that&#39;s not your fault ;)
<span class="quote">
&gt; +	new = SMMU_COUNTER_RELOAD;</span>

Given that we *are* following the &quot;use the top counter bit as an
implicit overflow bit&quot; pattern of arm_pmu, it feels a bit weird to not
use the actual half-maximum value here (especially since it&#39;s easily
computable from counter_mask). I&#39;m about 85% sure it probably still
works, but as ever inconsistency breeds uncertainty.
<span class="quote">&gt; +	local64_set(&amp;hwc-&gt;prev_count, new);</span>
<span class="quote">&gt; +	smmu_pmu_counter_set_value(smmu_pmu, idx, new);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static irqreturn_t smmu_pmu_handle_irq(int irq_num, void *data)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = data;</span>
<span class="quote">&gt; +	u64 ovsr;</span>
<span class="quote">&gt; +	unsigned int idx;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ovsr = smmu_pmu_getreset_ovsr(smmu_pmu);</span>
<span class="quote">&gt; +	if (!smmu_pmu_has_overflowed(smmu_pmu, ovsr))</span>

You have an architectural guarantee that unimplemented bits of OVSSET0
are RES0, so checking !ovsr is sufficient.
<span class="quote">
&gt; +		return IRQ_NONE;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for_each_set_bit(idx, (unsigned long *)&amp;ovsr, smmu_pmu-&gt;num_counters) {</span>
<span class="quote">&gt; +		struct perf_event *event = smmu_pmu-&gt;events[idx];</span>
<span class="quote">&gt; +		struct hw_perf_event *hwc;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (WARN_ON_ONCE(!event))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		smmu_pmu_event_update(event);</span>
<span class="quote">&gt; +		hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return IRQ_HANDLED;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static unsigned int smmu_pmu_get_event_idx(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned int idx;</span>
<span class="quote">&gt; +	unsigned int num_ctrs = smmu_pmu-&gt;num_counters;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	idx = find_first_zero_bit(smmu_pmu-&gt;used_counters, num_ctrs);</span>
<span class="quote">&gt; +	if (idx == num_ctrs)</span>
<span class="quote">&gt; +		/* The counters are all in use. */</span>
<span class="quote">&gt; +		return -EAGAIN;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	set_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return idx;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Implementation of abstract pmu functionality required by</span>
<span class="quote">&gt; + * the core perf events code.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int smmu_pmu_event_init(struct perf_event *event)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	struct perf_event *sibling;</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; +	u32 event_id;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (event-&gt;attr.type != event-&gt;pmu-&gt;type)</span>
<span class="quote">&gt; +		return -ENOENT;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (hwc-&gt;sample_period) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +				    &quot;Sampling not supported\n&quot;);</span>
<span class="quote">&gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (event-&gt;cpu &lt; 0) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +				    &quot;Per-task mode not supported\n&quot;);</span>
<span class="quote">&gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* We cannot filter accurately so we just don&#39;t allow it. */</span>
<span class="quote">&gt; +	if (event-&gt;attr.exclude_user || event-&gt;attr.exclude_kernel ||</span>
<span class="quote">&gt; +	    event-&gt;attr.exclude_hv || event-&gt;attr.exclude_idle) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +				    &quot;Can&#39;t exclude execution levels\n&quot;);</span>
<span class="quote">&gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Verify specified event is supported on this PMU */</span>
<span class="quote">&gt; +	event_id = get_event(event);</span>
<span class="quote">&gt; +	if ((event_id &gt;= SMMU_MAX_EVENT_ID) ||</span>

What about raw imp-def events?
<span class="quote">
&gt; +	    (!test_bit(event_id, smmu_pmu-&gt;supported_events))) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +				    &quot;Invalid event %d for this PMU\n&quot;,</span>
<span class="quote">&gt; +				    event_id);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Don&#39;t allow groups with mixed PMUs, except for s/w events */</span>
<span class="quote">&gt; +	if (event-&gt;group_leader-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="quote">&gt; +	    !is_software_event(event-&gt;group_leader)) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +			 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	list_for_each_entry(sibling, &amp;event-&gt;group_leader-&gt;sibling_list,</span>
<span class="quote">&gt; +			    group_entry)</span>
<span class="quote">&gt; +		if (sibling-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="quote">&gt; +		    !is_software_event(sibling)) {</span>
<span class="quote">&gt; +			dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +				 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="quote">&gt; +			return -EINVAL;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Ensure all events in a group are on the same cpu */</span>
<span class="quote">&gt; +	if ((event-&gt;group_leader != event) &amp;&amp;</span>
<span class="quote">&gt; +	    (event-&gt;cpu != event-&gt;group_leader-&gt;cpu)) {</span>
<span class="quote">&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; +			 &quot;Can&#39;t create group on CPUs %d and %d&quot;,</span>
<span class="quote">&gt; +			 event-&gt;cpu, event-&gt;group_leader-&gt;cpu);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	hwc-&gt;idx = -1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Ensure all events are on the same cpu so all events are in the</span>
<span class="quote">&gt; +	 * same cpu context, to avoid races on pmu_enable etc.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	event-&gt;cpu = smmu_pmu-&gt;on_cpu;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_event_start(struct perf_event *event, int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; +	u32 evtyper;</span>
<span class="quote">&gt; +	u32 filter_sec;</span>
<span class="quote">&gt; +	u32 filter_span;</span>
<span class="quote">&gt; +	u32 filter_stream_id;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	hwc-&gt;state = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (get_filter_enable(event)) {</span>
<span class="quote">&gt; +		filter_sec = get_filter_sec(event);</span>
<span class="quote">&gt; +		filter_span = get_filter_span(event);</span>
<span class="quote">&gt; +		filter_stream_id = get_filter_stream_id(event);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		filter_sec = SMMU_DEFAULT_FILTER_SEC;</span>
<span class="quote">&gt; +		filter_span = SMMU_DEFAULT_FILTER_SPAN;</span>
<span class="quote">&gt; +		filter_stream_id = SMMU_DEFAULT_FILTER_STREAM_ID;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	evtyper = get_event(event) |</span>
<span class="quote">&gt; +		  filter_span &lt;&lt; SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT |</span>
<span class="quote">&gt; +		  filter_sec &lt;&lt; SMMU_PMCG_EVTYPER_SEC_SID_SHIFT;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_set_evtyper(smmu_pmu, idx, evtyper);</span>
<span class="quote">&gt; +	smmu_pmu_set_smr(smmu_pmu, idx, filter_stream_id);</span>
<span class="quote">&gt; +	smmu_pmu_interrupt_enable(smmu_pmu, idx);</span>
<span class="quote">&gt; +	smmu_pmu_counter_enable(smmu_pmu, idx);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_event_stop(struct perf_event *event, int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (hwc-&gt;state &amp; PERF_HES_STOPPED)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_interrupt_disable(smmu_pmu, idx);</span>
<span class="quote">&gt; +	smmu_pmu_counter_disable(smmu_pmu, idx);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (flags &amp; PERF_EF_UPDATE)</span>
<span class="quote">&gt; +		smmu_pmu_event_update(event);</span>
<span class="quote">&gt; +	hwc-&gt;state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int smmu_pmu_event_add(struct perf_event *event, int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	int idx;</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	idx = smmu_pmu_get_event_idx(smmu_pmu);</span>
<span class="quote">&gt; +	if (idx &lt; 0)</span>
<span class="quote">&gt; +		return idx;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	hwc-&gt;idx = idx;</span>
<span class="quote">&gt; +	hwc-&gt;state = PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="quote">&gt; +	smmu_pmu-&gt;events[idx] = event;</span>
<span class="quote">&gt; +	local64_set(&amp;hwc-&gt;prev_count, 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (flags &amp; PERF_EF_START)</span>
<span class="quote">&gt; +		smmu_pmu_event_start(event, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Propagate changes to the userspace mapping. */</span>
<span class="quote">&gt; +	perf_event_update_userpage(event);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_event_del(struct perf_event *event, int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_event_stop(event, flags | PERF_EF_UPDATE);</span>
<span class="quote">&gt; +	smmu_pmu-&gt;events[idx] = NULL;</span>
<span class="quote">&gt; +	clear_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	perf_event_update_userpage(event);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_event_read(struct perf_event *event)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	smmu_pmu_event_update(event);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* cpumask */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static ssize_t smmu_pmu_cpumask_show(struct device *dev,</span>
<span class="quote">&gt; +				     struct device_attribute *attr,</span>
<span class="quote">&gt; +				     char *buf)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return cpumap_print_to_pagebuf(true, buf, cpumask_of(smmu_pmu-&gt;on_cpu));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct device_attribute smmu_pmu_cpumask_attr =</span>
<span class="quote">&gt; +		__ATTR(cpumask, 0444, smmu_pmu_cpumask_show, NULL);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct attribute *smmu_pmu_cpumask_attrs[] = {</span>
<span class="quote">&gt; +	&amp;smmu_pmu_cpumask_attr.attr,</span>
<span class="quote">&gt; +	NULL,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct attribute_group smmu_pmu_cpumask_group = {</span>
<span class="quote">&gt; +	.attrs = smmu_pmu_cpumask_attrs,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Events */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +ssize_t smmu_pmu_event_show(struct device *dev,</span>
<span class="quote">&gt; +			    struct device_attribute *attr, char *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return sprintf(page, &quot;event=0x%02llx\n&quot;, pmu_attr-&gt;id);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define SMMU_EVENT_ATTR(_name, _id)					  \</span>
<span class="quote">&gt; +	(&amp;((struct perf_pmu_events_attr[]) {				  \</span>
<span class="quote">&gt; +		{ .attr = __ATTR(_name, 0444, smmu_pmu_event_show, NULL), \</span>
<span class="quote">&gt; +		  .id = _id, }						  \</span>
<span class="quote">&gt; +	})[0].attr.attr)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct attribute *smmu_pmu_events[] = {</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(cycles, SMMU_PMU_CYCLES),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(transaction, SMMU_PMU_TRANSACTION),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(tlb_miss, SMMU_PMU_TLB_MISS),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(config_cache_miss, SMMU_PMU_CONFIG_CACHE_MISS),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(trans_table_walk, SMMU_PMU_TRANS_TABLE_WALK),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(config_struct_access, SMMU_PMU_CONFIG_STRUCT_ACCESS),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(pcie_ats_trans_rq, SMMU_PMU_PCIE_ATS_TRANS_RQ),</span>
<span class="quote">&gt; +	SMMU_EVENT_ATTR(pcie_ats_trans_passed, SMMU_PMU_PCIE_ATS_TRANS_PASSED),</span>
<span class="quote">&gt; +	NULL</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static umode_t smmu_pmu_event_is_visible(struct kobject *kobj,</span>
<span class="quote">&gt; +					 struct attribute *attr, int unused)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct device *dev = kobj_to_dev(kobj);</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="quote">&gt; +	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr.attr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (test_bit(pmu_attr-&gt;id, smmu_pmu-&gt;supported_events))</span>
<span class="quote">&gt; +		return attr-&gt;mode;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +static struct attribute_group smmu_pmu_events_group = {</span>
<span class="quote">&gt; +	.name = &quot;events&quot;,</span>
<span class="quote">&gt; +	.attrs = smmu_pmu_events,</span>
<span class="quote">&gt; +	.is_visible = smmu_pmu_event_is_visible,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Formats */</span>
<span class="quote">&gt; +PMU_FORMAT_ATTR(event,		   &quot;config:0-15&quot;);</span>
<span class="quote">&gt; +PMU_FORMAT_ATTR(filter_stream_id,  &quot;config1:0-31&quot;);</span>
<span class="quote">&gt; +PMU_FORMAT_ATTR(filter_span,	   &quot;config1:32&quot;);</span>
<span class="quote">&gt; +PMU_FORMAT_ATTR(filter_sec,	   &quot;config1:33&quot;);</span>
<span class="quote">&gt; +PMU_FORMAT_ATTR(filter_enable,	   &quot;config1:34&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct attribute *smmu_pmu_formats[] = {</span>
<span class="quote">&gt; +	&amp;format_attr_event.attr,</span>
<span class="quote">&gt; +	&amp;format_attr_filter_stream_id.attr,</span>
<span class="quote">&gt; +	&amp;format_attr_filter_span.attr,</span>
<span class="quote">&gt; +	&amp;format_attr_filter_sec.attr,</span>
<span class="quote">&gt; +	&amp;format_attr_filter_enable.attr,</span>
<span class="quote">&gt; +	NULL</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct attribute_group smmu_pmu_format_group = {</span>
<span class="quote">&gt; +	.name = &quot;format&quot;,</span>
<span class="quote">&gt; +	.attrs = smmu_pmu_formats,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static const struct attribute_group *smmu_pmu_attr_grps[] = {</span>
<span class="quote">&gt; +	&amp;smmu_pmu_cpumask_group,</span>
<span class="quote">&gt; +	&amp;smmu_pmu_events_group,</span>
<span class="quote">&gt; +	&amp;smmu_pmu_format_group,</span>
<span class="quote">&gt; +	NULL,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Generic device handlers</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static unsigned int get_num_counters(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u32 cfgr = readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return ((cfgr &amp; SMMU_PMCG_CFGR_NCTR_MASK) &gt;&gt; SMMU_PMCG_CFGR_NCTR_SHIFT)</span>
<span class="quote">&gt; +		+ 1;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int smmu_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; +	unsigned int target;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu = hlist_entry_safe(node, struct smmu_pmu, node);</span>

Is it ever valid for node to be NULL? If we can&#39;t trust it to be one of
the PMUs we registered I think all bets are off anyway.
<span class="quote">
&gt; +	if (cpu != smmu_pmu-&gt;on_cpu)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	target = cpumask_any_but(cpu_online_mask, cpu);</span>
<span class="quote">&gt; +	if (target &gt;= nr_cpu_ids)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	perf_pmu_migrate_context(&amp;smmu_pmu-&gt;pmu, cpu, target);</span>
<span class="quote">&gt; +	smmu_pmu-&gt;on_cpu = target;</span>
<span class="quote">&gt; +	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(target)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int smmu_pmu_probe(struct platform_device *pdev)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; +	struct resource *mem_resource_0, *mem_resource_1;</span>
<span class="quote">&gt; +	void __iomem *mem_map_0, *mem_map_1;</span>
<span class="quote">&gt; +	unsigned int reg_size;</span>
<span class="quote">&gt; +	int err;</span>
<span class="quote">&gt; +	int irq;</span>
<span class="quote">&gt; +	u32 ceid[SMMU_NUM_EVENTS_U32];</span>
<span class="quote">&gt; +	u64 ceid_64;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*smmu_pmu), GFP_KERNEL);</span>
<span class="quote">&gt; +	if (!smmu_pmu)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	platform_set_drvdata(pdev, smmu_pmu);</span>
<span class="quote">&gt; +	smmu_pmu-&gt;pmu = (struct pmu) {</span>
<span class="quote">&gt; +		.task_ctx_nr    = perf_invalid_context,</span>
<span class="quote">&gt; +		.pmu_enable	= smmu_pmu_enable,</span>
<span class="quote">&gt; +		.pmu_disable	= smmu_pmu_disable,</span>
<span class="quote">&gt; +		.event_init	= smmu_pmu_event_init,</span>
<span class="quote">&gt; +		.add		= smmu_pmu_event_add,</span>
<span class="quote">&gt; +		.del		= smmu_pmu_event_del,</span>
<span class="quote">&gt; +		.start		= smmu_pmu_event_start,</span>
<span class="quote">&gt; +		.stop		= smmu_pmu_event_stop,</span>
<span class="quote">&gt; +		.read		= smmu_pmu_event_read,</span>
<span class="quote">&gt; +		.attr_groups	= smmu_pmu_attr_grps,</span>
<span class="quote">&gt; +	};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mem_resource_0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);</span>
<span class="quote">&gt; +	mem_map_0 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (IS_ERR(mem_map_0)) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="quote">&gt; +			&amp;mem_resource_0-&gt;start);</span>
<span class="quote">&gt; +		return PTR_ERR(mem_map_0);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu-&gt;reg_base = mem_map_0;</span>
<span class="quote">&gt; +	smmu_pmu-&gt;pmu.name =</span>
<span class="quote">&gt; +		devm_kasprintf(&amp;pdev-&gt;dev, GFP_KERNEL, &quot;smmu_0_%llx&quot;,</span>
<span class="quote">&gt; +			       (mem_resource_0-&gt;start) &gt;&gt; SMMU_PA_SHIFT);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!smmu_pmu-&gt;pmu.name) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Failed to create PMU name&quot;);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID0);</span>
<span class="quote">&gt; +	ceid[0] = ceid_64 &amp; GENMASK(31, 0);</span>

It took a second look to determine that that masking does nothing...
<span class="quote">
&gt; +	ceid[1] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID1);</span>
<span class="quote">&gt; +	ceid[2] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="quote">&gt; +	ceid[3] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt; +	bitmap_from_u32array(smmu_pmu-&gt;supported_events, SMMU_MAX_EVENT_ID,</span>
<span class="quote">&gt; +			     ceid, SMMU_NUM_EVENTS_U32);</span>

...but then the whole lot might be cleaner and simpler with a u64[2]
cast to u32* (or unioned to u32[4]) as necessary.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/* Determine if page 1 is present */</span>
<span class="quote">&gt; +	if (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="quote">&gt; +	    SMMU_PMCG_CFGR_RELOC_CTRS) {</span>
<span class="quote">&gt; +		mem_resource_1 = platform_get_resource(pdev, IORESOURCE_MEM, 1);</span>
<span class="quote">&gt; +		mem_map_1 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (IS_ERR(mem_map_1)) {</span>
<span class="quote">&gt; +			dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="quote">&gt; +				&amp;mem_resource_1-&gt;start);</span>
<span class="quote">&gt; +			return PTR_ERR(mem_map_1);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		smmu_pmu-&gt;reloc_base = mem_map_1;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		smmu_pmu-&gt;reloc_base = smmu_pmu-&gt;reg_base;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	irq = platform_get_irq(pdev, 0);</span>
<span class="quote">&gt; +	if (irq &lt; 0) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev,</span>
<span class="quote">&gt; +			&quot;Failed to get valid irq for smmu @%pa\n&quot;,</span>
<span class="quote">&gt; +			&amp;mem_resource_0-&gt;start);</span>
<span class="quote">&gt; +		return irq;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	err = devm_request_irq(&amp;pdev-&gt;dev, irq, smmu_pmu_handle_irq,</span>
<span class="quote">&gt; +			       IRQF_NOBALANCING | IRQF_SHARED | IRQF_NO_THREAD,</span>
<span class="quote">&gt; +			       &quot;smmu-pmu&quot;, smmu_pmu);</span>
<span class="quote">&gt; +	if (err) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev,</span>
<span class="quote">&gt; +			&quot;Unable to request IRQ%d for SMMU PMU counters\n&quot;, irq);</span>
<span class="quote">&gt; +		return err;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu-&gt;irq = irq;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Pick one CPU to be the preferred one to use */</span>
<span class="quote">&gt; +	smmu_pmu-&gt;on_cpu = smp_processor_id();</span>
<span class="quote">&gt; +	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(smmu_pmu-&gt;on_cpu)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu-&gt;num_counters = get_num_counters(smmu_pmu);</span>
<span class="quote">&gt; +	smmu_pmu-&gt;pdev = pdev;</span>
<span class="quote">&gt; +	smmu_pmu-&gt;counter_present_mask = GENMASK(smmu_pmu-&gt;num_counters - 1, 0);</span>
<span class="quote">&gt; +	reg_size = (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="quote">&gt; +		    SMMU_PMCG_CFGR_SIZE_MASK) &gt;&gt; SMMU_PMCG_CFGR_SIZE_SHIFT;</span>
<span class="quote">&gt; +	smmu_pmu-&gt;reg_size_32 = (reg_size == SMMU_PMCG_CFGR_COUNTER_SIZE_32);</span>
<span class="quote">&gt; +	smmu_pmu-&gt;counter_mask = GENMASK_ULL(reg_size, 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_reset(smmu_pmu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	err = cpuhp_state_add_instance_nocalls(cpuhp_state_num,</span>
<span class="quote">&gt; +					       &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; +	if (err) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering hotplug&quot;, err);</span>
<span class="quote">&gt; +		return err;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	err = perf_pmu_register(&amp;smmu_pmu-&gt;pmu, smmu_pmu-&gt;pmu.name, -1);</span>
<span class="quote">&gt; +	if (err) {</span>
<span class="quote">&gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering SMMU PMU\n&quot;, err);</span>
<span class="quote">&gt; +		goto out_unregister;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dev_info(&amp;pdev-&gt;dev, &quot;Registered SMMU PMU @ %pa using %d counters\n&quot;,</span>
<span class="quote">&gt; +		 &amp;mem_resource_0-&gt;start, smmu_pmu-&gt;num_counters);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return err;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_unregister:</span>
<span class="quote">&gt; +	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; +	return err;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int smmu_pmu_remove(struct platform_device *pdev)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	perf_pmu_unregister(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; +	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void smmu_pmu_shutdown(struct platform_device *pdev)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct platform_driver smmu_pmu_driver = {</span>
<span class="quote">&gt; +	.driver = {</span>
<span class="quote">&gt; +		.name = &quot;arm-smmu-pmu&quot;,</span>

Nit: &quot;arm-smmu-v3-pmu&quot; please, for consistency with the IOMMU driver
naming. There is a SMMUv2 PMU driver in the works, too ;)

Robin.
<span class="quote">
&gt; +	},</span>
<span class="quote">&gt; +	.probe = smmu_pmu_probe,</span>
<span class="quote">&gt; +	.remove = smmu_pmu_remove,</span>
<span class="quote">&gt; +	.shutdown = smmu_pmu_shutdown,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __init arm_smmu_pmu_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	cpuhp_state_num = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,</span>
<span class="quote">&gt; +				      &quot;perf/arm/smmupmu:online&quot;,</span>
<span class="quote">&gt; +				      NULL,</span>
<span class="quote">&gt; +				      smmu_pmu_offline_cpu);</span>
<span class="quote">&gt; +	if (cpuhp_state_num &lt; 0)</span>
<span class="quote">&gt; +		return cpuhp_state_num;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return platform_driver_register(&amp;smmu_pmu_driver);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +module_init(arm_smmu_pmu_init);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __exit arm_smmu_pmu_exit(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	platform_driver_unregister(&amp;smmu_pmu_driver);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +module_exit(arm_smmu_pmu_exit);</span>
<span class="quote">&gt; +MODULE_LICENSE(&quot;GPL v2&quot;);</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=8626">Neil Leeder</a> - Aug. 7, 2017, 9:18 p.m.</div>
<pre class="content">
Hi Robin,
Thank you for your comments.

On 8/7/2017 10:31 AM, Robin Murphy wrote:
<span class="quote">&gt; On 04/08/17 20:59, Neil Leeder wrote:</span>
<span class="quote">&gt;&gt; PMUs are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="quote">&gt;&gt; is the physical page address of the SMMU PMCG.</span>
<span class="quote">&gt;&gt; For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This seems a bit rough - is it at feasible to at least chase the node</span>
<span class="quote">&gt; reference and namespace them by the associated component, e.g. something</span>
<span class="quote">&gt; like &quot;arm-smmu-v3.x:pmcg.y&quot;? The user can always dig the associated</span>
<span class="quote">&gt; physical address out of /proc/iomem if necessary.</span>
<span class="quote">&gt; </span>
That looks like it may be better - I&#39;ll look into it. 
<span class="quote">
&gt;&gt; Filtering by stream id is done by specifying filtering parameters</span>
<span class="quote">&gt;&gt; with the event. Options are:</span>
<span class="quote">&gt;&gt;   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="quote">&gt;&gt;   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="quote">&gt;&gt;   filter_sec       - applies to non-secure (0) or secure (1) namespace</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m a little dubious as to how useful it is to expose this, since we</span>
<span class="quote">&gt; can&#39;t see the true value of SCR.SO so have no way of knowing what we&#39;ll</span>
<span class="quote">&gt; actually end up counting.</span>

I can remove the sec filter.
<span class="quote">
&gt;&gt; +config ARM_SMMUV3_PMU</span>
<span class="quote">&gt;&gt; +	 bool &quot;ARM SMMUv3 PMU&quot;</span>
<span class="quote">&gt;&gt; +	 depends on PERF_EVENTS &amp;&amp; ARM64 &amp;&amp; ACPI</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; PERF_EVENTS is already a top-level dependency now.</span>
<span class="quote">&gt; </span>
OK
<span class="quote">
&gt;&gt; +#include &lt;linux/msi.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is MSI support planned?</span>
<span class="quote">&gt; </span>
Not in this patchset. I&#39;ll remove the include.
<span class="quote">
&gt;&gt; +#define SMMU_PMCG_EVCNTR0               0x0</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVCNTR(n, stride)     (SMMU_PMCG_EVCNTR0 + (n) * (stride))</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVTYPER0              0x400</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVTYPER(n)            (SMMU_PMCG_EVTYPER0 + (n) * 4)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVTYPER_SEC_SID_SHIFT       30</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT      29</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_EVTYPER_EVENT_MASK          GENMASK(15, 0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_SVR0                  0x600</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_SVR(n, stride)        (SMMU_PMCG_SVR0 + (n) * (stride))</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_SMR0                  0xA00</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_SMR(n)                (SMMU_PMCG_SMR0 + (n) * 4)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CNTENSET0             0xC00</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CNTENCLR0             0xC20</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_INTENSET0             0xC40</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_INTENCLR0             0xC60</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_OVSCLR0               0xC80</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_OVSSET0               0xCC0</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CAPR                  0xD88</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_SCR                   0xDF8</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR                  0xE00</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_SID_FILTER_TYPE        BIT(23)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_CAPTURE                BIT(22)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_MSI                    BIT(21)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_RELOC_CTRS             BIT(20)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_SIZE_MASK              GENMASK(13, 8)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_SIZE_SHIFT             8</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_COUNTER_SIZE_32        31</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_NCTR_MASK              GENMASK(5, 0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CFGR_NCTR_SHIFT             0</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CR                    0xE04</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CR_ENABLE                   BIT(0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CEID0                 0xE20</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_CEID1                 0xE28</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CTRL              0xE50</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CTRL_IRQEN              BIT(0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CTRLACK           0xE54</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CTRLACK_IRQEN           BIT(0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CFG0              0xE58</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CFG0_ADDR_MASK          GENMASK(51, 2)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CFG1              0xE60</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_CFG2              0xE64</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_STATUS            0xE68</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_IRQ_STATUS_IRQ_ABT          BIT(0)</span>
<span class="quote">&gt;&gt; +#define SMMU_PMCG_AIDR                  0xE70</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Several of these are unused (although at least IRQ0_CFG1 probably should</span>
<span class="quote">&gt; be, to zero it to a known state if we aren&#39;t supporting MSIs).</span>
<span class="quote">&gt; </span>
I can remove the unused defines and clear IRQ_CFG1.
<span class="quote">
&gt;&gt; +	bool reg_size_32;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This guy is redundant...</span>
<span class="quote">&gt; </span>
[...]
<span class="quote">&gt;&gt; +	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...since it would be just as efficient to directly test</span>
<span class="quote">&gt; smmu_pmu-&gt;counter_mask &amp; BIT(32) here and below.</span>
<span class="quote">&gt; </span>
OK
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; smmu_pmu-&gt;num_counters; i++) {</span>
<span class="quote">&gt;&gt; +		smmu_pmu_counter_disable(smmu_pmu, i);</span>
<span class="quote">&gt;&gt; +		smmu_pmu_interrupt_disable(smmu_pmu, i);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Surely it would be far quicker and simpler to do this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	writeq(smmu_pmu-&gt;counter_present_mask,</span>
<span class="quote">&gt; 		smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);</span>
<span class="quote">&gt; 	writeq(smmu_pmu-&gt;counter_present_mask,</span>
<span class="quote">&gt; 		smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);</span>
<span class="quote">&gt; </span>
OK
<span class="quote">
&gt;&gt; +static inline bool smmu_pmu_has_overflowed(struct smmu_pmu *smmu_pmu, u64 ovsr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return !!(ovsr &amp; smmu_pmu-&gt;counter_present_mask);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Personally, I find these helpers abstracting simple reads/writes</span>
<span class="quote">&gt; actually make the code harder to follow, especially when they&#39;re each</span>
<span class="quote">&gt; used a grand total of once. That may well just be me, though.</span>
<span class="quote">&gt; </span>
At least this one will go away with the below change to the interrupt handler.
<span class="quote">
&gt;&gt; +	new = SMMU_COUNTER_RELOAD;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that we *are* following the &quot;use the top counter bit as an</span>
<span class="quote">&gt; implicit overflow bit&quot; pattern of arm_pmu, it feels a bit weird to not</span>
<span class="quote">&gt; use the actual half-maximum value here (especially since it&#39;s easily</span>
<span class="quote">&gt; computable from counter_mask). I&#39;m about 85% sure it probably still</span>
<span class="quote">&gt; works, but as ever inconsistency breeds uncertainty.</span>

I thought that if we&#39;re happy with BIT(31) working fine with 32-bit registers,
it should also work for larger registers, so there was no need to waste more of
their bits. But I can change it to be half-max for all of them.
<span class="quote">
&gt;&gt; +static irqreturn_t smmu_pmu_handle_irq(int irq_num, void *data)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct smmu_pmu *smmu_pmu = data;</span>
<span class="quote">&gt;&gt; +	u64 ovsr;</span>
<span class="quote">&gt;&gt; +	unsigned int idx;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ovsr = smmu_pmu_getreset_ovsr(smmu_pmu);</span>
<span class="quote">&gt;&gt; +	if (!smmu_pmu_has_overflowed(smmu_pmu, ovsr))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You have an architectural guarantee that unimplemented bits of OVSSET0</span>
<span class="quote">&gt; are RES0, so checking !ovsr is sufficient.</span>
<span class="quote">&gt; </span>
OK
<span class="quote">
&gt;&gt; +	/* Verify specified event is supported on this PMU */</span>
<span class="quote">&gt;&gt; +	event_id = get_event(event);</span>
<span class="quote">&gt;&gt; +	if ((event_id &gt;= SMMU_MAX_EVENT_ID) ||</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about raw imp-def events?</span>
<span class="quote">&gt; </span>
I can keep the check for common events, but also allow any raw event
in the imp-def range.
<span class="quote">
&gt;&gt; +	    (!test_bit(event_id, smmu_pmu-&gt;supported_events))) {</span>
<span class="quote">&gt;&gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt;&gt; +				    &quot;Invalid event %d for this PMU\n&quot;,</span>
<span class="quote">&gt;&gt; +				    event_id);</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	}</span>
[...]
<span class="quote">
&gt;&gt; +static int smmu_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt;&gt; +	unsigned int target;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	smmu_pmu = hlist_entry_safe(node, struct smmu_pmu, node);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it ever valid for node to be NULL? If we can&#39;t trust it to be one of</span>
<span class="quote">&gt; the PMUs we registered I think all bets are off anyway.</span>
<span class="quote">&gt; </span>
I was following the logic in arm-ccn.c and arm-cci.c. If it works for them
I would hope it works here too.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID0);</span>
<span class="quote">&gt;&gt; +	ceid[0] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It took a second look to determine that that masking does nothing...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +	ceid[1] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt;&gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID1);</span>
<span class="quote">&gt;&gt; +	ceid[2] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="quote">&gt;&gt; +	ceid[3] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt;&gt; +	bitmap_from_u32array(smmu_pmu-&gt;supported_events, SMMU_MAX_EVENT_ID,</span>
<span class="quote">&gt;&gt; +			     ceid, SMMU_NUM_EVENTS_U32);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...but then the whole lot might be cleaner and simpler with a u64[2]</span>
<span class="quote">&gt; cast to u32* (or unioned to u32[4]) as necessary.</span>
<span class="quote">&gt; </span>
I&#39;ve rewritten this about 4 different ways and didn&#39;t love any of them,
including this one. I can re-do this as you suggest.
<span class="quote">
&gt;&gt; +static struct platform_driver smmu_pmu_driver = {</span>
<span class="quote">&gt;&gt; +	.driver = {</span>
<span class="quote">&gt;&gt; +		.name = &quot;arm-smmu-pmu&quot;,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nit: &quot;arm-smmu-v3-pmu&quot; please, for consistency with the IOMMU driver</span>
<span class="quote">&gt; naming. There is a SMMUv2 PMU driver in the works, too ;)</span>
<span class="quote">&gt; </span>
ok

Thanks,

Neil
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174583">Linu Cherian</a> - Dec. 5, 2017, 5:01 a.m.</div>
<pre class="content">
Hi Robin,

On Mon Aug 07, 2017 at 03:31:24PM +0100, Robin Murphy wrote:
<span class="quote">&gt; On 04/08/17 20:59, Neil Leeder wrote:</span>
<span class="quote">&gt; &gt; Adds a new driver to support the SMMU v3 PMU and add it into the</span>
<span class="quote">&gt; &gt; perf events framework.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Each SMMU node may have multiple PMUs associated with it, each of</span>
<span class="quote">&gt; &gt; which may support different events.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; PMUs are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="quote">&gt; &gt; is the physical page address of the SMMU PMCG.</span>
<span class="quote">&gt; &gt; For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This seems a bit rough - is it at feasible to at least chase the node</span>
<span class="quote">&gt; reference and namespace them by the associated component, e.g. something</span>
<span class="quote">&gt; like &quot;arm-smmu-v3.x:pmcg.y&quot;? The user can always dig the associated</span>
<span class="quote">&gt; physical address out of /proc/iomem if necessary.</span>
<span class="quote">&gt;</span>

Assuming x indicates the smmu v3 node id and y indicates pmcg group id.
If are making such a assumption, 

# Would like to clarified if the architecture gurantees that a smmu pmcg group is always 
  associated with a specific SMMU node. 

From, Section 10.2 in SMMU Architecture Specification 
&quot;A counter group is conceptually free-standing and has no interdependency on the behavior of other counter
groups or even on the SMMU configuration itself&quot;

Does the above and also Fig 19 from the same section,
imply that a PMCG group need not be restricted to a SMMU node ?
<span class="quote"> 
&gt; &gt; Filtering by stream id is done by specifying filtering parameters</span>
<span class="quote">&gt; &gt; with the event. Options are:</span>
<span class="quote">&gt; &gt;   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="quote">&gt; &gt;   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="quote">&gt; &gt;   filter_sec       - applies to non-secure (0) or secure (1) namespace</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m a little dubious as to how useful it is to expose this, since we</span>
<span class="quote">&gt; can&#39;t see the true value of SCR.SO so have no way of knowing what we&#39;ll</span>
<span class="quote">&gt; actually end up counting.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;   filter_stream_id - pattern to filter against</span>
<span class="quote">&gt; &gt; Further filtering information is available in the SMMU documentation.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,</span>
<span class="quote">&gt; &gt;                       filter_span=1,filter_stream_id=0x42/ -a pwd</span>
<span class="quote">&gt; &gt; Applies filter pattern 0x42 to transaction events.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; SMMU events are not attributable to a CPU, so task mode and sampling</span>
<span class="quote">&gt; &gt; are not supported.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Neil Leeder &lt;nleeder@codeaurora.org&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  drivers/perf/Kconfig          |   9 +</span>
<span class="quote">&gt; &gt;  drivers/perf/Makefile         |   1 +</span>
<span class="quote">&gt; &gt;  drivers/perf/arm_smmuv3_pmu.c | 813 ++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  3 files changed, 823 insertions(+)</span>
<span class="quote">&gt; &gt;  create mode 100644 drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/drivers/perf/Kconfig b/drivers/perf/Kconfig</span>
<span class="quote">&gt; &gt; index e5197ff..e7721d1 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/perf/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/drivers/perf/Kconfig</span>
<span class="quote">&gt; &gt; @@ -17,6 +17,15 @@ config ARM_PMU_ACPI</span>
<span class="quote">&gt; &gt;  	depends on ARM_PMU &amp;&amp; ACPI</span>
<span class="quote">&gt; &gt;  	def_bool y</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +config ARM_SMMUV3_PMU</span>
<span class="quote">&gt; &gt; +	 bool &quot;ARM SMMUv3 PMU&quot;</span>
<span class="quote">&gt; &gt; +	 depends on PERF_EVENTS &amp;&amp; ARM64 &amp;&amp; ACPI</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; PERF_EVENTS is already a top-level dependency now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	   help</span>
<span class="quote">&gt; &gt; +	   Provides support for the SMMU version 3 performance monitor unit (PMU)</span>
<span class="quote">&gt; &gt; +	   on ARM-based systems.</span>
<span class="quote">&gt; &gt; +	   Adds the SMMU PMU into the perf events subsystem for</span>
<span class="quote">&gt; &gt; +	   monitoring SMMU performance events.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  config QCOM_L2_PMU</span>
<span class="quote">&gt; &gt;  	bool &quot;Qualcomm Technologies L2-cache PMU&quot;</span>
<span class="quote">&gt; &gt;  	depends on ARCH_QCOM &amp;&amp; ARM64 &amp;&amp; ACPI</span>
<span class="quote">&gt; &gt; diff --git a/drivers/perf/Makefile b/drivers/perf/Makefile</span>
<span class="quote">&gt; &gt; index 6420bd4..3012f5e 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/perf/Makefile</span>
<span class="quote">&gt; &gt; +++ b/drivers/perf/Makefile</span>
<span class="quote">&gt; &gt; @@ -1,5 +1,6 @@</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_ARM_PMU) += arm_pmu.o arm_pmu_platform.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_ARM_PMU_ACPI) += arm_pmu_acpi.o</span>
<span class="quote">&gt; &gt; +obj-$(CONFIG_ARM_SMMUV3_PMU) += arm_smmuv3_pmu.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_QCOM_L2_PMU)	+= qcom_l2_pmu.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_QCOM_L3_PMU) += qcom_l3_pmu.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_XGENE_PMU) += xgene_pmu.o</span>
<span class="quote">&gt; &gt; diff --git a/drivers/perf/arm_smmuv3_pmu.c b/drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; &gt; new file mode 100644</span>
<span class="quote">&gt; &gt; index 0000000..1e70791</span>
<span class="quote">&gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; +++ b/drivers/perf/arm_smmuv3_pmu.c</span>
<span class="quote">&gt; &gt; @@ -0,0 +1,813 @@</span>
<span class="quote">&gt; &gt; +/* Copyright (c) 2017 The Linux Foundation. All rights reserved.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; &gt; + * it under the terms of the GNU General Public License version 2 and</span>
<span class="quote">&gt; &gt; + * only version 2 as published by the Free Software Foundation.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; &gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; &gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; &gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * This driver adds support for perf events to use the Performance</span>
<span class="quote">&gt; &gt; + * Monitor Counter Groups (PMCG) associated with an SMMUv3 node</span>
<span class="quote">&gt; &gt; + * to monitor that node.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Devices are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="quote">&gt; &gt; + * is the physical page address of the SMMU PMCG.</span>
<span class="quote">&gt; &gt; + * For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Filtering by stream id is done by specifying filtering parameters</span>
<span class="quote">&gt; &gt; + * with the event. options are:</span>
<span class="quote">&gt; &gt; + *   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="quote">&gt; &gt; + *   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="quote">&gt; &gt; + *   filter_sec       - filter applies to non-secure (0) or secure (1) namespace</span>
<span class="quote">&gt; &gt; + *   filter_stream_id - pattern to filter against</span>
<span class="quote">&gt; &gt; + * Further filtering information is available in the SMMU documentation.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,</span>
<span class="quote">&gt; &gt; + *                       filter_span=1,filter_stream_id=0x42/ -a pwd</span>
<span class="quote">&gt; &gt; + * Applies filter pattern 0x42 to transaction events.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * SMMU events are not attributable to a CPU, so task mode and sampling</span>
<span class="quote">&gt; &gt; + * are not supported.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;linux/acpi.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/acpi_iort.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/bitops.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/cpuhotplug.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/cpumask.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/device.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/errno.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/interrupt.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/irq.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/list.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/msi.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is MSI support planned?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +#include &lt;linux/perf_event.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/platform_device.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/smp.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/sysfs.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/types.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;asm/local64.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVCNTR0               0x0</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVCNTR(n, stride)     (SMMU_PMCG_EVCNTR0 + (n) * (stride))</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVTYPER0              0x400</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVTYPER(n)            (SMMU_PMCG_EVTYPER0 + (n) * 4)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVTYPER_SEC_SID_SHIFT       30</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT      29</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_EVTYPER_EVENT_MASK          GENMASK(15, 0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_SVR0                  0x600</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_SVR(n, stride)        (SMMU_PMCG_SVR0 + (n) * (stride))</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_SMR0                  0xA00</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_SMR(n)                (SMMU_PMCG_SMR0 + (n) * 4)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CNTENSET0             0xC00</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CNTENCLR0             0xC20</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_INTENSET0             0xC40</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_INTENCLR0             0xC60</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_OVSCLR0               0xC80</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_OVSSET0               0xCC0</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CAPR                  0xD88</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_SCR                   0xDF8</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR                  0xE00</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_SID_FILTER_TYPE        BIT(23)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_CAPTURE                BIT(22)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_MSI                    BIT(21)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_RELOC_CTRS             BIT(20)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_SIZE_MASK              GENMASK(13, 8)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_SIZE_SHIFT             8</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_COUNTER_SIZE_32        31</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_NCTR_MASK              GENMASK(5, 0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CFGR_NCTR_SHIFT             0</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CR                    0xE04</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CR_ENABLE                   BIT(0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CEID0                 0xE20</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_CEID1                 0xE28</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CTRL              0xE50</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CTRL_IRQEN              BIT(0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CTRLACK           0xE54</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CTRLACK_IRQEN           BIT(0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CFG0              0xE58</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CFG0_ADDR_MASK          GENMASK(51, 2)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CFG1              0xE60</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_CFG2              0xE64</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_STATUS            0xE68</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_IRQ_STATUS_IRQ_ABT          BIT(0)</span>
<span class="quote">&gt; &gt; +#define SMMU_PMCG_AIDR                  0xE70</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Several of these are unused (although at least IRQ0_CFG1 probably should</span>
<span class="quote">&gt; be, to zero it to a known state if we aren&#39;t supporting MSIs).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +#define SMMU_COUNTER_RELOAD             BIT(31)</span>
<span class="quote">&gt; &gt; +#define SMMU_DEFAULT_FILTER_SEC         0</span>
<span class="quote">&gt; &gt; +#define SMMU_DEFAULT_FILTER_SPAN        1</span>
<span class="quote">&gt; &gt; +#define SMMU_DEFAULT_FILTER_STREAM_ID   GENMASK(31, 0)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define SMMU_MAX_COUNTERS               64</span>
<span class="quote">&gt; &gt; +#define SMMU_MAX_EVENT_ID               128</span>
<span class="quote">&gt; &gt; +#define SMMU_NUM_EVENTS_U32             (SMMU_MAX_EVENT_ID / sizeof(u32))</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define SMMU_PA_SHIFT                   12</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Events */</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_CYCLES                 0</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_TRANSACTION            1</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_TLB_MISS               2</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_CONFIG_CACHE_MISS      3</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_TRANS_TABLE_WALK       4</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_CONFIG_STRUCT_ACCESS   5</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_PCIE_ATS_TRANS_RQ      6</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_PCIE_ATS_TRANS_PASSED  7</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int cpuhp_state_num;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +struct smmu_pmu {</span>
<span class="quote">&gt; &gt; +	struct hlist_node node;</span>
<span class="quote">&gt; &gt; +	struct perf_event *events[SMMU_MAX_COUNTERS];</span>
<span class="quote">&gt; &gt; +	DECLARE_BITMAP(used_counters, SMMU_MAX_COUNTERS);</span>
<span class="quote">&gt; &gt; +	DECLARE_BITMAP(supported_events, SMMU_MAX_EVENT_ID);</span>
<span class="quote">&gt; &gt; +	unsigned int irq;</span>
<span class="quote">&gt; &gt; +	unsigned int on_cpu;</span>
<span class="quote">&gt; &gt; +	struct pmu pmu;</span>
<span class="quote">&gt; &gt; +	unsigned int num_counters;</span>
<span class="quote">&gt; &gt; +	struct platform_device *pdev;</span>
<span class="quote">&gt; &gt; +	void __iomem *reg_base;</span>
<span class="quote">&gt; &gt; +	void __iomem *reloc_base;</span>
<span class="quote">&gt; &gt; +	u64 counter_present_mask;</span>
<span class="quote">&gt; &gt; +	u64 counter_mask;</span>
<span class="quote">&gt; &gt; +	bool reg_size_32;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This guy is redundant...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define to_smmu_pmu(p) (container_of(p, struct smmu_pmu, pmu))</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define SMMU_PMU_EVENT_ATTR_EXTRACTOR(_name, _config, _size, _shift)    \</span>
<span class="quote">&gt; &gt; +	static inline u32 get_##_name(struct perf_event *event)         \</span>
<span class="quote">&gt; &gt; +	{                                                               \</span>
<span class="quote">&gt; &gt; +		return (event-&gt;attr._config &gt;&gt; (_shift)) &amp;              \</span>
<span class="quote">&gt; &gt; +			GENMASK_ULL((_size) - 1, 0);                    \</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(event, config, 16, 0);</span>
<span class="quote">&gt; &gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_stream_id, config1, 32, 0);</span>
<span class="quote">&gt; &gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_span, config1, 1, 32);</span>
<span class="quote">&gt; &gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_sec, config1, 1, 33);</span>
<span class="quote">&gt; &gt; +SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_enable, config1, 1, 34);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_enable(struct pmu *pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	writel(SMMU_PMCG_CR_ENABLE, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="quote">&gt; &gt; +	writel(SMMU_PMCG_IRQ_CTRL_IRQEN,</span>
<span class="quote">&gt; &gt; +	       smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_disable(struct pmu *pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="quote">&gt; &gt; +	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_counter_set_value(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; &gt; +					      u32 idx, u64 value)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...since it would be just as efficient to directly test</span>
<span class="quote">&gt; smmu_pmu-&gt;counter_mask &amp; BIT(32) here and below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +		writel(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="quote">&gt; &gt; +	else</span>
<span class="quote">&gt; &gt; +		writeq(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline u64 smmu_pmu_counter_get_value(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	u64 value;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="quote">&gt; &gt; +		value = readl(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="quote">&gt; &gt; +	else</span>
<span class="quote">&gt; &gt; +		value = readq(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return value;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_counter_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENSET0);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_counter_disable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_interrupt_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENSET0);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_interrupt_disable(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; &gt; +					      u32 idx)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_reset(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned int i;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; smmu_pmu-&gt;num_counters; i++) {</span>
<span class="quote">&gt; &gt; +		smmu_pmu_counter_disable(smmu_pmu, i);</span>
<span class="quote">&gt; &gt; +		smmu_pmu_interrupt_disable(smmu_pmu, i);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Surely it would be far quicker and simpler to do this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	writeq(smmu_pmu-&gt;counter_present_mask,</span>
<span class="quote">&gt; 		smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);</span>
<span class="quote">&gt; 	writeq(smmu_pmu-&gt;counter_present_mask,</span>
<span class="quote">&gt; 		smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_set_evtyper(struct smmu_pmu *smmu_pmu, u32 idx,</span>
<span class="quote">&gt; &gt; +					u32 val)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_EVTYPER(idx));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline void smmu_pmu_set_smr(struct smmu_pmu *smmu_pmu, u32 idx, u32 val)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_SMR(idx));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline u64 smmu_pmu_getreset_ovsr(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	u64 result = readq_relaxed(smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSSET0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	writeq(result, smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSCLR0);</span>
<span class="quote">&gt; &gt; +	return result;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline bool smmu_pmu_has_overflowed(struct smmu_pmu *smmu_pmu, u64 ovsr)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return !!(ovsr &amp; smmu_pmu-&gt;counter_present_mask);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Personally, I find these helpers abstracting simple reads/writes</span>
<span class="quote">&gt; actually make the code harder to follow, especially when they&#39;re each</span>
<span class="quote">&gt; used a grand total of once. That may well just be me, though.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +static void smmu_pmu_event_update(struct perf_event *event)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +	u64 delta, prev, now;</span>
<span class="quote">&gt; &gt; +	u32 idx = hwc-&gt;idx;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	do {</span>
<span class="quote">&gt; &gt; +		prev = local64_read(&amp;hwc-&gt;prev_count);</span>
<span class="quote">&gt; &gt; +		now = smmu_pmu_counter_get_value(smmu_pmu, idx);</span>
<span class="quote">&gt; &gt; +	} while (local64_cmpxchg(&amp;hwc-&gt;prev_count, prev, now) != prev);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* handle overflow. */</span>
<span class="quote">&gt; &gt; +	delta = now - prev;</span>
<span class="quote">&gt; &gt; +	delta &amp;= smmu_pmu-&gt;counter_mask;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	local64_add(delta, &amp;event-&gt;count);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_set_period(struct smmu_pmu *smmu_pmu,</span>
<span class="quote">&gt; &gt; +				struct hw_perf_event *hwc)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	u32 idx = hwc-&gt;idx;</span>
<span class="quote">&gt; &gt; +	u64 new;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * We limit the max period to half the max counter value of the smallest</span>
<span class="quote">&gt; &gt; +	 * counter size, so that even in the case of extreme interrupt latency</span>
<span class="quote">&gt; &gt; +	 * the counter will (hopefully) not wrap past its initial value.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having once fought to properly understand the underlying logic I despise</span>
<span class="quote">&gt; this unhelpfully-vague comment, but that&#39;s not your fault ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	new = SMMU_COUNTER_RELOAD;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that we *are* following the &quot;use the top counter bit as an</span>
<span class="quote">&gt; implicit overflow bit&quot; pattern of arm_pmu, it feels a bit weird to not</span>
<span class="quote">&gt; use the actual half-maximum value here (especially since it&#39;s easily</span>
<span class="quote">&gt; computable from counter_mask). I&#39;m about 85% sure it probably still</span>
<span class="quote">&gt; works, but as ever inconsistency breeds uncertainty.</span>
<span class="quote">&gt; &gt; +	local64_set(&amp;hwc-&gt;prev_count, new);</span>
<span class="quote">&gt; &gt; +	smmu_pmu_counter_set_value(smmu_pmu, idx, new);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static irqreturn_t smmu_pmu_handle_irq(int irq_num, void *data)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = data;</span>
<span class="quote">&gt; &gt; +	u64 ovsr;</span>
<span class="quote">&gt; &gt; +	unsigned int idx;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	ovsr = smmu_pmu_getreset_ovsr(smmu_pmu);</span>
<span class="quote">&gt; &gt; +	if (!smmu_pmu_has_overflowed(smmu_pmu, ovsr))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You have an architectural guarantee that unimplemented bits of OVSSET0</span>
<span class="quote">&gt; are RES0, so checking !ovsr is sufficient.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +		return IRQ_NONE;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	for_each_set_bit(idx, (unsigned long *)&amp;ovsr, smmu_pmu-&gt;num_counters) {</span>
<span class="quote">&gt; &gt; +		struct perf_event *event = smmu_pmu-&gt;events[idx];</span>
<span class="quote">&gt; &gt; +		struct hw_perf_event *hwc;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (WARN_ON_ONCE(!event))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		smmu_pmu_event_update(event);</span>
<span class="quote">&gt; &gt; +		hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return IRQ_HANDLED;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static unsigned int smmu_pmu_get_event_idx(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned int idx;</span>
<span class="quote">&gt; &gt; +	unsigned int num_ctrs = smmu_pmu-&gt;num_counters;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	idx = find_first_zero_bit(smmu_pmu-&gt;used_counters, num_ctrs);</span>
<span class="quote">&gt; &gt; +	if (idx == num_ctrs)</span>
<span class="quote">&gt; &gt; +		/* The counters are all in use. */</span>
<span class="quote">&gt; &gt; +		return -EAGAIN;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	set_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return idx;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Implementation of abstract pmu functionality required by</span>
<span class="quote">&gt; &gt; + * the core perf events code.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int smmu_pmu_event_init(struct perf_event *event)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	struct perf_event *sibling;</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; &gt; +	u32 event_id;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (event-&gt;attr.type != event-&gt;pmu-&gt;type)</span>
<span class="quote">&gt; &gt; +		return -ENOENT;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (hwc-&gt;sample_period) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +				    &quot;Sampling not supported\n&quot;);</span>
<span class="quote">&gt; &gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (event-&gt;cpu &lt; 0) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +				    &quot;Per-task mode not supported\n&quot;);</span>
<span class="quote">&gt; &gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* We cannot filter accurately so we just don&#39;t allow it. */</span>
<span class="quote">&gt; &gt; +	if (event-&gt;attr.exclude_user || event-&gt;attr.exclude_kernel ||</span>
<span class="quote">&gt; &gt; +	    event-&gt;attr.exclude_hv || event-&gt;attr.exclude_idle) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +				    &quot;Can&#39;t exclude execution levels\n&quot;);</span>
<span class="quote">&gt; &gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Verify specified event is supported on this PMU */</span>
<span class="quote">&gt; &gt; +	event_id = get_event(event);</span>
<span class="quote">&gt; &gt; +	if ((event_id &gt;= SMMU_MAX_EVENT_ID) ||</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about raw imp-def events?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	    (!test_bit(event_id, smmu_pmu-&gt;supported_events))) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +				    &quot;Invalid event %d for this PMU\n&quot;,</span>
<span class="quote">&gt; &gt; +				    event_id);</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Don&#39;t allow groups with mixed PMUs, except for s/w events */</span>
<span class="quote">&gt; &gt; +	if (event-&gt;group_leader-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="quote">&gt; &gt; +	    !is_software_event(event-&gt;group_leader)) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +			 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	list_for_each_entry(sibling, &amp;event-&gt;group_leader-&gt;sibling_list,</span>
<span class="quote">&gt; &gt; +			    group_entry)</span>
<span class="quote">&gt; &gt; +		if (sibling-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="quote">&gt; &gt; +		    !is_software_event(sibling)) {</span>
<span class="quote">&gt; &gt; +			dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +				 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="quote">&gt; &gt; +			return -EINVAL;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Ensure all events in a group are on the same cpu */</span>
<span class="quote">&gt; &gt; +	if ((event-&gt;group_leader != event) &amp;&amp;</span>
<span class="quote">&gt; &gt; +	    (event-&gt;cpu != event-&gt;group_leader-&gt;cpu)) {</span>
<span class="quote">&gt; &gt; +		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +			 &quot;Can&#39;t create group on CPUs %d and %d&quot;,</span>
<span class="quote">&gt; &gt; +			 event-&gt;cpu, event-&gt;group_leader-&gt;cpu);</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	hwc-&gt;idx = -1;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Ensure all events are on the same cpu so all events are in the</span>
<span class="quote">&gt; &gt; +	 * same cpu context, to avoid races on pmu_enable etc.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	event-&gt;cpu = smmu_pmu-&gt;on_cpu;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_event_start(struct perf_event *event, int flags)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; &gt; +	u32 evtyper;</span>
<span class="quote">&gt; &gt; +	u32 filter_sec;</span>
<span class="quote">&gt; &gt; +	u32 filter_span;</span>
<span class="quote">&gt; &gt; +	u32 filter_stream_id;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	hwc-&gt;state = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (get_filter_enable(event)) {</span>
<span class="quote">&gt; &gt; +		filter_sec = get_filter_sec(event);</span>
<span class="quote">&gt; &gt; +		filter_span = get_filter_span(event);</span>
<span class="quote">&gt; &gt; +		filter_stream_id = get_filter_stream_id(event);</span>
<span class="quote">&gt; &gt; +	} else {</span>
<span class="quote">&gt; &gt; +		filter_sec = SMMU_DEFAULT_FILTER_SEC;</span>
<span class="quote">&gt; &gt; +		filter_span = SMMU_DEFAULT_FILTER_SPAN;</span>
<span class="quote">&gt; &gt; +		filter_stream_id = SMMU_DEFAULT_FILTER_STREAM_ID;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	evtyper = get_event(event) |</span>
<span class="quote">&gt; &gt; +		  filter_span &lt;&lt; SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT |</span>
<span class="quote">&gt; &gt; +		  filter_sec &lt;&lt; SMMU_PMCG_EVTYPER_SEC_SID_SHIFT;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_set_evtyper(smmu_pmu, idx, evtyper);</span>
<span class="quote">&gt; &gt; +	smmu_pmu_set_smr(smmu_pmu, idx, filter_stream_id);</span>
<span class="quote">&gt; &gt; +	smmu_pmu_interrupt_enable(smmu_pmu, idx);</span>
<span class="quote">&gt; &gt; +	smmu_pmu_counter_enable(smmu_pmu, idx);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_event_stop(struct perf_event *event, int flags)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (hwc-&gt;state &amp; PERF_HES_STOPPED)</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_interrupt_disable(smmu_pmu, idx);</span>
<span class="quote">&gt; &gt; +	smmu_pmu_counter_disable(smmu_pmu, idx);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (flags &amp; PERF_EF_UPDATE)</span>
<span class="quote">&gt; &gt; +		smmu_pmu_event_update(event);</span>
<span class="quote">&gt; &gt; +	hwc-&gt;state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int smmu_pmu_event_add(struct perf_event *event, int flags)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	int idx;</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	idx = smmu_pmu_get_event_idx(smmu_pmu);</span>
<span class="quote">&gt; &gt; +	if (idx &lt; 0)</span>
<span class="quote">&gt; &gt; +		return idx;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	hwc-&gt;idx = idx;</span>
<span class="quote">&gt; &gt; +	hwc-&gt;state = PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;events[idx] = event;</span>
<span class="quote">&gt; &gt; +	local64_set(&amp;hwc-&gt;prev_count, 0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (flags &amp; PERF_EF_START)</span>
<span class="quote">&gt; &gt; +		smmu_pmu_event_start(event, flags);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Propagate changes to the userspace mapping. */</span>
<span class="quote">&gt; &gt; +	perf_event_update_userpage(event);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_event_del(struct perf_event *event, int flags)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="quote">&gt; &gt; +	int idx = hwc-&gt;idx;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_event_stop(event, flags | PERF_EF_UPDATE);</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;events[idx] = NULL;</span>
<span class="quote">&gt; &gt; +	clear_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	perf_event_update_userpage(event);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_event_read(struct perf_event *event)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	smmu_pmu_event_update(event);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* cpumask */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static ssize_t smmu_pmu_cpumask_show(struct device *dev,</span>
<span class="quote">&gt; &gt; +				     struct device_attribute *attr,</span>
<span class="quote">&gt; &gt; +				     char *buf)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return cpumap_print_to_pagebuf(true, buf, cpumask_of(smmu_pmu-&gt;on_cpu));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct device_attribute smmu_pmu_cpumask_attr =</span>
<span class="quote">&gt; &gt; +		__ATTR(cpumask, 0444, smmu_pmu_cpumask_show, NULL);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct attribute *smmu_pmu_cpumask_attrs[] = {</span>
<span class="quote">&gt; &gt; +	&amp;smmu_pmu_cpumask_attr.attr,</span>
<span class="quote">&gt; &gt; +	NULL,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct attribute_group smmu_pmu_cpumask_group = {</span>
<span class="quote">&gt; &gt; +	.attrs = smmu_pmu_cpumask_attrs,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Events */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +ssize_t smmu_pmu_event_show(struct device *dev,</span>
<span class="quote">&gt; &gt; +			    struct device_attribute *attr, char *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return sprintf(page, &quot;event=0x%02llx\n&quot;, pmu_attr-&gt;id);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define SMMU_EVENT_ATTR(_name, _id)					  \</span>
<span class="quote">&gt; &gt; +	(&amp;((struct perf_pmu_events_attr[]) {				  \</span>
<span class="quote">&gt; &gt; +		{ .attr = __ATTR(_name, 0444, smmu_pmu_event_show, NULL), \</span>
<span class="quote">&gt; &gt; +		  .id = _id, }						  \</span>
<span class="quote">&gt; &gt; +	})[0].attr.attr)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct attribute *smmu_pmu_events[] = {</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(cycles, SMMU_PMU_CYCLES),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(transaction, SMMU_PMU_TRANSACTION),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(tlb_miss, SMMU_PMU_TLB_MISS),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(config_cache_miss, SMMU_PMU_CONFIG_CACHE_MISS),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(trans_table_walk, SMMU_PMU_TRANS_TABLE_WALK),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(config_struct_access, SMMU_PMU_CONFIG_STRUCT_ACCESS),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(pcie_ats_trans_rq, SMMU_PMU_PCIE_ATS_TRANS_RQ),</span>
<span class="quote">&gt; &gt; +	SMMU_EVENT_ATTR(pcie_ats_trans_passed, SMMU_PMU_PCIE_ATS_TRANS_PASSED),</span>
<span class="quote">&gt; &gt; +	NULL</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static umode_t smmu_pmu_event_is_visible(struct kobject *kobj,</span>
<span class="quote">&gt; &gt; +					 struct attribute *attr, int unused)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct device *dev = kobj_to_dev(kobj);</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="quote">&gt; &gt; +	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr.attr);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (test_bit(pmu_attr-&gt;id, smmu_pmu-&gt;supported_events))</span>
<span class="quote">&gt; &gt; +		return attr-&gt;mode;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +static struct attribute_group smmu_pmu_events_group = {</span>
<span class="quote">&gt; &gt; +	.name = &quot;events&quot;,</span>
<span class="quote">&gt; &gt; +	.attrs = smmu_pmu_events,</span>
<span class="quote">&gt; &gt; +	.is_visible = smmu_pmu_event_is_visible,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Formats */</span>
<span class="quote">&gt; &gt; +PMU_FORMAT_ATTR(event,		   &quot;config:0-15&quot;);</span>
<span class="quote">&gt; &gt; +PMU_FORMAT_ATTR(filter_stream_id,  &quot;config1:0-31&quot;);</span>
<span class="quote">&gt; &gt; +PMU_FORMAT_ATTR(filter_span,	   &quot;config1:32&quot;);</span>
<span class="quote">&gt; &gt; +PMU_FORMAT_ATTR(filter_sec,	   &quot;config1:33&quot;);</span>
<span class="quote">&gt; &gt; +PMU_FORMAT_ATTR(filter_enable,	   &quot;config1:34&quot;);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct attribute *smmu_pmu_formats[] = {</span>
<span class="quote">&gt; &gt; +	&amp;format_attr_event.attr,</span>
<span class="quote">&gt; &gt; +	&amp;format_attr_filter_stream_id.attr,</span>
<span class="quote">&gt; &gt; +	&amp;format_attr_filter_span.attr,</span>
<span class="quote">&gt; &gt; +	&amp;format_attr_filter_sec.attr,</span>
<span class="quote">&gt; &gt; +	&amp;format_attr_filter_enable.attr,</span>
<span class="quote">&gt; &gt; +	NULL</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct attribute_group smmu_pmu_format_group = {</span>
<span class="quote">&gt; &gt; +	.name = &quot;format&quot;,</span>
<span class="quote">&gt; &gt; +	.attrs = smmu_pmu_formats,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static const struct attribute_group *smmu_pmu_attr_grps[] = {</span>
<span class="quote">&gt; &gt; +	&amp;smmu_pmu_cpumask_group,</span>
<span class="quote">&gt; &gt; +	&amp;smmu_pmu_events_group,</span>
<span class="quote">&gt; &gt; +	&amp;smmu_pmu_format_group,</span>
<span class="quote">&gt; &gt; +	NULL,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Generic device handlers</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static unsigned int get_num_counters(struct smmu_pmu *smmu_pmu)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	u32 cfgr = readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return ((cfgr &amp; SMMU_PMCG_CFGR_NCTR_MASK) &gt;&gt; SMMU_PMCG_CFGR_NCTR_SHIFT)</span>
<span class="quote">&gt; &gt; +		+ 1;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int smmu_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; &gt; +	unsigned int target;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu = hlist_entry_safe(node, struct smmu_pmu, node);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it ever valid for node to be NULL? If we can&#39;t trust it to be one of</span>
<span class="quote">&gt; the PMUs we registered I think all bets are off anyway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	if (cpu != smmu_pmu-&gt;on_cpu)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	target = cpumask_any_but(cpu_online_mask, cpu);</span>
<span class="quote">&gt; &gt; +	if (target &gt;= nr_cpu_ids)</span>
<span class="quote">&gt; &gt; +		return 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	perf_pmu_migrate_context(&amp;smmu_pmu-&gt;pmu, cpu, target);</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;on_cpu = target;</span>
<span class="quote">&gt; &gt; +	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(target)));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int smmu_pmu_probe(struct platform_device *pdev)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu;</span>
<span class="quote">&gt; &gt; +	struct resource *mem_resource_0, *mem_resource_1;</span>
<span class="quote">&gt; &gt; +	void __iomem *mem_map_0, *mem_map_1;</span>
<span class="quote">&gt; &gt; +	unsigned int reg_size;</span>
<span class="quote">&gt; &gt; +	int err;</span>
<span class="quote">&gt; &gt; +	int irq;</span>
<span class="quote">&gt; &gt; +	u32 ceid[SMMU_NUM_EVENTS_U32];</span>
<span class="quote">&gt; &gt; +	u64 ceid_64;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*smmu_pmu), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +	if (!smmu_pmu)</span>
<span class="quote">&gt; &gt; +		return -ENOMEM;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	platform_set_drvdata(pdev, smmu_pmu);</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;pmu = (struct pmu) {</span>
<span class="quote">&gt; &gt; +		.task_ctx_nr    = perf_invalid_context,</span>
<span class="quote">&gt; &gt; +		.pmu_enable	= smmu_pmu_enable,</span>
<span class="quote">&gt; &gt; +		.pmu_disable	= smmu_pmu_disable,</span>
<span class="quote">&gt; &gt; +		.event_init	= smmu_pmu_event_init,</span>
<span class="quote">&gt; &gt; +		.add		= smmu_pmu_event_add,</span>
<span class="quote">&gt; &gt; +		.del		= smmu_pmu_event_del,</span>
<span class="quote">&gt; &gt; +		.start		= smmu_pmu_event_start,</span>
<span class="quote">&gt; &gt; +		.stop		= smmu_pmu_event_stop,</span>
<span class="quote">&gt; &gt; +		.read		= smmu_pmu_event_read,</span>
<span class="quote">&gt; &gt; +		.attr_groups	= smmu_pmu_attr_grps,</span>
<span class="quote">&gt; &gt; +	};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mem_resource_0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);</span>
<span class="quote">&gt; &gt; +	mem_map_0 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (IS_ERR(mem_map_0)) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="quote">&gt; &gt; +			&amp;mem_resource_0-&gt;start);</span>
<span class="quote">&gt; &gt; +		return PTR_ERR(mem_map_0);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;reg_base = mem_map_0;</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;pmu.name =</span>
<span class="quote">&gt; &gt; +		devm_kasprintf(&amp;pdev-&gt;dev, GFP_KERNEL, &quot;smmu_0_%llx&quot;,</span>
<span class="quote">&gt; &gt; +			       (mem_resource_0-&gt;start) &gt;&gt; SMMU_PA_SHIFT);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!smmu_pmu-&gt;pmu.name) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Failed to create PMU name&quot;);</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID0);</span>
<span class="quote">&gt; &gt; +	ceid[0] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It took a second look to determine that that masking does nothing...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	ceid[1] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt; &gt; +	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID1);</span>
<span class="quote">&gt; &gt; +	ceid[2] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="quote">&gt; &gt; +	ceid[3] = ceid_64 &gt;&gt; 32;</span>
<span class="quote">&gt; &gt; +	bitmap_from_u32array(smmu_pmu-&gt;supported_events, SMMU_MAX_EVENT_ID,</span>
<span class="quote">&gt; &gt; +			     ceid, SMMU_NUM_EVENTS_U32);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...but then the whole lot might be cleaner and simpler with a u64[2]</span>
<span class="quote">&gt; cast to u32* (or unioned to u32[4]) as necessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Determine if page 1 is present */</span>
<span class="quote">&gt; &gt; +	if (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="quote">&gt; &gt; +	    SMMU_PMCG_CFGR_RELOC_CTRS) {</span>
<span class="quote">&gt; &gt; +		mem_resource_1 = platform_get_resource(pdev, IORESOURCE_MEM, 1);</span>
<span class="quote">&gt; &gt; +		mem_map_1 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_1);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (IS_ERR(mem_map_1)) {</span>
<span class="quote">&gt; &gt; +			dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="quote">&gt; &gt; +				&amp;mem_resource_1-&gt;start);</span>
<span class="quote">&gt; &gt; +			return PTR_ERR(mem_map_1);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +		smmu_pmu-&gt;reloc_base = mem_map_1;</span>
<span class="quote">&gt; &gt; +	} else {</span>
<span class="quote">&gt; &gt; +		smmu_pmu-&gt;reloc_base = smmu_pmu-&gt;reg_base;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	irq = platform_get_irq(pdev, 0);</span>
<span class="quote">&gt; &gt; +	if (irq &lt; 0) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +			&quot;Failed to get valid irq for smmu @%pa\n&quot;,</span>
<span class="quote">&gt; &gt; +			&amp;mem_resource_0-&gt;start);</span>
<span class="quote">&gt; &gt; +		return irq;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	err = devm_request_irq(&amp;pdev-&gt;dev, irq, smmu_pmu_handle_irq,</span>
<span class="quote">&gt; &gt; +			       IRQF_NOBALANCING | IRQF_SHARED | IRQF_NO_THREAD,</span>
<span class="quote">&gt; &gt; +			       &quot;smmu-pmu&quot;, smmu_pmu);</span>
<span class="quote">&gt; &gt; +	if (err) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev,</span>
<span class="quote">&gt; &gt; +			&quot;Unable to request IRQ%d for SMMU PMU counters\n&quot;, irq);</span>
<span class="quote">&gt; &gt; +		return err;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;irq = irq;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* Pick one CPU to be the preferred one to use */</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;on_cpu = smp_processor_id();</span>
<span class="quote">&gt; &gt; +	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(smmu_pmu-&gt;on_cpu)));</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;num_counters = get_num_counters(smmu_pmu);</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;pdev = pdev;</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;counter_present_mask = GENMASK(smmu_pmu-&gt;num_counters - 1, 0);</span>
<span class="quote">&gt; &gt; +	reg_size = (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="quote">&gt; &gt; +		    SMMU_PMCG_CFGR_SIZE_MASK) &gt;&gt; SMMU_PMCG_CFGR_SIZE_SHIFT;</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;reg_size_32 = (reg_size == SMMU_PMCG_CFGR_COUNTER_SIZE_32);</span>
<span class="quote">&gt; &gt; +	smmu_pmu-&gt;counter_mask = GENMASK_ULL(reg_size, 0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_reset(smmu_pmu);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	err = cpuhp_state_add_instance_nocalls(cpuhp_state_num,</span>
<span class="quote">&gt; &gt; +					       &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; &gt; +	if (err) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering hotplug&quot;, err);</span>
<span class="quote">&gt; &gt; +		return err;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	err = perf_pmu_register(&amp;smmu_pmu-&gt;pmu, smmu_pmu-&gt;pmu.name, -1);</span>
<span class="quote">&gt; &gt; +	if (err) {</span>
<span class="quote">&gt; &gt; +		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering SMMU PMU\n&quot;, err);</span>
<span class="quote">&gt; &gt; +		goto out_unregister;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dev_info(&amp;pdev-&gt;dev, &quot;Registered SMMU PMU @ %pa using %d counters\n&quot;,</span>
<span class="quote">&gt; &gt; +		 &amp;mem_resource_0-&gt;start, smmu_pmu-&gt;num_counters);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return err;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +out_unregister:</span>
<span class="quote">&gt; &gt; +	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; &gt; +	return err;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int smmu_pmu_remove(struct platform_device *pdev)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	perf_pmu_unregister(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; &gt; +	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void smmu_pmu_shutdown(struct platform_device *pdev)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct platform_driver smmu_pmu_driver = {</span>
<span class="quote">&gt; &gt; +	.driver = {</span>
<span class="quote">&gt; &gt; +		.name = &quot;arm-smmu-pmu&quot;,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nit: &quot;arm-smmu-v3-pmu&quot; please, for consistency with the IOMMU driver</span>
<span class="quote">&gt; naming. There is a SMMUv2 PMU driver in the works, too ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Robin.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	},</span>
<span class="quote">&gt; &gt; +	.probe = smmu_pmu_probe,</span>
<span class="quote">&gt; &gt; +	.remove = smmu_pmu_remove,</span>
<span class="quote">&gt; &gt; +	.shutdown = smmu_pmu_shutdown,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int __init arm_smmu_pmu_init(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	cpuhp_state_num = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,</span>
<span class="quote">&gt; &gt; +				      &quot;perf/arm/smmupmu:online&quot;,</span>
<span class="quote">&gt; &gt; +				      NULL,</span>
<span class="quote">&gt; &gt; +				      smmu_pmu_offline_cpu);</span>
<span class="quote">&gt; &gt; +	if (cpuhp_state_num &lt; 0)</span>
<span class="quote">&gt; &gt; +		return cpuhp_state_num;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return platform_driver_register(&amp;smmu_pmu_driver);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +module_init(arm_smmu_pmu_init);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void __exit arm_smmu_pmu_exit(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	platform_driver_unregister(&amp;smmu_pmu_driver);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +module_exit(arm_smmu_pmu_exit);</span>
<span class="quote">&gt; &gt; +MODULE_LICENSE(&quot;GPL v2&quot;);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; linux-arm-kernel mailing list</span>
<span class="quote">&gt; linux-arm-kernel@lists.infradead.org</span>
<span class="quote">&gt; http://lists.infradead.org/mailman/listinfo/linux-arm-kernel</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/perf/Kconfig b/drivers/perf/Kconfig</span>
<span class="p_header">index e5197ff..e7721d1 100644</span>
<span class="p_header">--- a/drivers/perf/Kconfig</span>
<span class="p_header">+++ b/drivers/perf/Kconfig</span>
<span class="p_chunk">@@ -17,6 +17,15 @@</span> <span class="p_context"> config ARM_PMU_ACPI</span>
 	depends on ARM_PMU &amp;&amp; ACPI
 	def_bool y
 
<span class="p_add">+config ARM_SMMUV3_PMU</span>
<span class="p_add">+	 bool &quot;ARM SMMUv3 PMU&quot;</span>
<span class="p_add">+	 depends on PERF_EVENTS &amp;&amp; ARM64 &amp;&amp; ACPI</span>
<span class="p_add">+	   help</span>
<span class="p_add">+	   Provides support for the SMMU version 3 performance monitor unit (PMU)</span>
<span class="p_add">+	   on ARM-based systems.</span>
<span class="p_add">+	   Adds the SMMU PMU into the perf events subsystem for</span>
<span class="p_add">+	   monitoring SMMU performance events.</span>
<span class="p_add">+</span>
 config QCOM_L2_PMU
 	bool &quot;Qualcomm Technologies L2-cache PMU&quot;
 	depends on ARCH_QCOM &amp;&amp; ARM64 &amp;&amp; ACPI
<span class="p_header">diff --git a/drivers/perf/Makefile b/drivers/perf/Makefile</span>
<span class="p_header">index 6420bd4..3012f5e 100644</span>
<span class="p_header">--- a/drivers/perf/Makefile</span>
<span class="p_header">+++ b/drivers/perf/Makefile</span>
<span class="p_chunk">@@ -1,5 +1,6 @@</span> <span class="p_context"></span>
 obj-$(CONFIG_ARM_PMU) += arm_pmu.o arm_pmu_platform.o
 obj-$(CONFIG_ARM_PMU_ACPI) += arm_pmu_acpi.o
<span class="p_add">+obj-$(CONFIG_ARM_SMMUV3_PMU) += arm_smmuv3_pmu.o</span>
 obj-$(CONFIG_QCOM_L2_PMU)	+= qcom_l2_pmu.o
 obj-$(CONFIG_QCOM_L3_PMU) += qcom_l3_pmu.o
 obj-$(CONFIG_XGENE_PMU) += xgene_pmu.o
<span class="p_header">diff --git a/drivers/perf/arm_smmuv3_pmu.c b/drivers/perf/arm_smmuv3_pmu.c</span>
new file mode 100644
<span class="p_header">index 0000000..1e70791</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/drivers/perf/arm_smmuv3_pmu.c</span>
<span class="p_chunk">@@ -0,0 +1,813 @@</span> <span class="p_context"></span>
<span class="p_add">+/* Copyright (c) 2017 The Linux Foundation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 and</span>
<span class="p_add">+ * only version 2 as published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This driver adds support for perf events to use the Performance</span>
<span class="p_add">+ * Monitor Counter Groups (PMCG) associated with an SMMUv3 node</span>
<span class="p_add">+ * to monitor that node.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Devices are named smmu_0_&lt;phys_addr_page&gt; where &lt;phys_addr_page&gt;</span>
<span class="p_add">+ * is the physical page address of the SMMU PMCG.</span>
<span class="p_add">+ * For example, the SMMU PMCG at 0xff88840000 is named smmu_0_ff88840</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Filtering by stream id is done by specifying filtering parameters</span>
<span class="p_add">+ * with the event. options are:</span>
<span class="p_add">+ *   filter_enable    - 0 = no filtering, 1 = filtering enabled</span>
<span class="p_add">+ *   filter_span      - 0 = exact match, 1 = pattern match</span>
<span class="p_add">+ *   filter_sec       - filter applies to non-secure (0) or secure (1) namespace</span>
<span class="p_add">+ *   filter_stream_id - pattern to filter against</span>
<span class="p_add">+ * Further filtering information is available in the SMMU documentation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Example: perf stat -e smmu_0_ff88840/transaction,filter_enable=1,</span>
<span class="p_add">+ *                       filter_span=1,filter_stream_id=0x42/ -a pwd</span>
<span class="p_add">+ * Applies filter pattern 0x42 to transaction events.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * SMMU events are not attributable to a CPU, so task mode and sampling</span>
<span class="p_add">+ * are not supported.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/acpi.h&gt;</span>
<span class="p_add">+#include &lt;linux/acpi_iort.h&gt;</span>
<span class="p_add">+#include &lt;linux/bitops.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpuhotplug.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpumask.h&gt;</span>
<span class="p_add">+#include &lt;linux/device.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/interrupt.h&gt;</span>
<span class="p_add">+#include &lt;linux/irq.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/msi.h&gt;</span>
<span class="p_add">+#include &lt;linux/perf_event.h&gt;</span>
<span class="p_add">+#include &lt;linux/platform_device.h&gt;</span>
<span class="p_add">+#include &lt;linux/smp.h&gt;</span>
<span class="p_add">+#include &lt;linux/sysfs.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/local64.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_PMCG_EVCNTR0               0x0</span>
<span class="p_add">+#define SMMU_PMCG_EVCNTR(n, stride)     (SMMU_PMCG_EVCNTR0 + (n) * (stride))</span>
<span class="p_add">+#define SMMU_PMCG_EVTYPER0              0x400</span>
<span class="p_add">+#define SMMU_PMCG_EVTYPER(n)            (SMMU_PMCG_EVTYPER0 + (n) * 4)</span>
<span class="p_add">+#define SMMU_PMCG_EVTYPER_SEC_SID_SHIFT       30</span>
<span class="p_add">+#define SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT      29</span>
<span class="p_add">+#define SMMU_PMCG_EVTYPER_EVENT_MASK          GENMASK(15, 0)</span>
<span class="p_add">+#define SMMU_PMCG_SVR0                  0x600</span>
<span class="p_add">+#define SMMU_PMCG_SVR(n, stride)        (SMMU_PMCG_SVR0 + (n) * (stride))</span>
<span class="p_add">+#define SMMU_PMCG_SMR0                  0xA00</span>
<span class="p_add">+#define SMMU_PMCG_SMR(n)                (SMMU_PMCG_SMR0 + (n) * 4)</span>
<span class="p_add">+#define SMMU_PMCG_CNTENSET0             0xC00</span>
<span class="p_add">+#define SMMU_PMCG_CNTENCLR0             0xC20</span>
<span class="p_add">+#define SMMU_PMCG_INTENSET0             0xC40</span>
<span class="p_add">+#define SMMU_PMCG_INTENCLR0             0xC60</span>
<span class="p_add">+#define SMMU_PMCG_OVSCLR0               0xC80</span>
<span class="p_add">+#define SMMU_PMCG_OVSSET0               0xCC0</span>
<span class="p_add">+#define SMMU_PMCG_CAPR                  0xD88</span>
<span class="p_add">+#define SMMU_PMCG_SCR                   0xDF8</span>
<span class="p_add">+#define SMMU_PMCG_CFGR                  0xE00</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_SID_FILTER_TYPE        BIT(23)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_CAPTURE                BIT(22)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_MSI                    BIT(21)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_RELOC_CTRS             BIT(20)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_SIZE_MASK              GENMASK(13, 8)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_SIZE_SHIFT             8</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_COUNTER_SIZE_32        31</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_NCTR_MASK              GENMASK(5, 0)</span>
<span class="p_add">+#define SMMU_PMCG_CFGR_NCTR_SHIFT             0</span>
<span class="p_add">+#define SMMU_PMCG_CR                    0xE04</span>
<span class="p_add">+#define SMMU_PMCG_CR_ENABLE                   BIT(0)</span>
<span class="p_add">+#define SMMU_PMCG_CEID0                 0xE20</span>
<span class="p_add">+#define SMMU_PMCG_CEID1                 0xE28</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CTRL              0xE50</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CTRL_IRQEN              BIT(0)</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CTRLACK           0xE54</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CTRLACK_IRQEN           BIT(0)</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CFG0              0xE58</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CFG0_ADDR_MASK          GENMASK(51, 2)</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CFG1              0xE60</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_CFG2              0xE64</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_STATUS            0xE68</span>
<span class="p_add">+#define SMMU_PMCG_IRQ_STATUS_IRQ_ABT          BIT(0)</span>
<span class="p_add">+#define SMMU_PMCG_AIDR                  0xE70</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_COUNTER_RELOAD             BIT(31)</span>
<span class="p_add">+#define SMMU_DEFAULT_FILTER_SEC         0</span>
<span class="p_add">+#define SMMU_DEFAULT_FILTER_SPAN        1</span>
<span class="p_add">+#define SMMU_DEFAULT_FILTER_STREAM_ID   GENMASK(31, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_MAX_COUNTERS               64</span>
<span class="p_add">+#define SMMU_MAX_EVENT_ID               128</span>
<span class="p_add">+#define SMMU_NUM_EVENTS_U32             (SMMU_MAX_EVENT_ID / sizeof(u32))</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_PA_SHIFT                   12</span>
<span class="p_add">+</span>
<span class="p_add">+/* Events */</span>
<span class="p_add">+#define SMMU_PMU_CYCLES                 0</span>
<span class="p_add">+#define SMMU_PMU_TRANSACTION            1</span>
<span class="p_add">+#define SMMU_PMU_TLB_MISS               2</span>
<span class="p_add">+#define SMMU_PMU_CONFIG_CACHE_MISS      3</span>
<span class="p_add">+#define SMMU_PMU_TRANS_TABLE_WALK       4</span>
<span class="p_add">+#define SMMU_PMU_CONFIG_STRUCT_ACCESS   5</span>
<span class="p_add">+#define SMMU_PMU_PCIE_ATS_TRANS_RQ      6</span>
<span class="p_add">+#define SMMU_PMU_PCIE_ATS_TRANS_PASSED  7</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpuhp_state_num;</span>
<span class="p_add">+</span>
<span class="p_add">+struct smmu_pmu {</span>
<span class="p_add">+	struct hlist_node node;</span>
<span class="p_add">+	struct perf_event *events[SMMU_MAX_COUNTERS];</span>
<span class="p_add">+	DECLARE_BITMAP(used_counters, SMMU_MAX_COUNTERS);</span>
<span class="p_add">+	DECLARE_BITMAP(supported_events, SMMU_MAX_EVENT_ID);</span>
<span class="p_add">+	unsigned int irq;</span>
<span class="p_add">+	unsigned int on_cpu;</span>
<span class="p_add">+	struct pmu pmu;</span>
<span class="p_add">+	unsigned int num_counters;</span>
<span class="p_add">+	struct platform_device *pdev;</span>
<span class="p_add">+	void __iomem *reg_base;</span>
<span class="p_add">+	void __iomem *reloc_base;</span>
<span class="p_add">+	u64 counter_present_mask;</span>
<span class="p_add">+	u64 counter_mask;</span>
<span class="p_add">+	bool reg_size_32;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define to_smmu_pmu(p) (container_of(p, struct smmu_pmu, pmu))</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_PMU_EVENT_ATTR_EXTRACTOR(_name, _config, _size, _shift)    \</span>
<span class="p_add">+	static inline u32 get_##_name(struct perf_event *event)         \</span>
<span class="p_add">+	{                                                               \</span>
<span class="p_add">+		return (event-&gt;attr._config &gt;&gt; (_shift)) &amp;              \</span>
<span class="p_add">+			GENMASK_ULL((_size) - 1, 0);                    \</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+SMMU_PMU_EVENT_ATTR_EXTRACTOR(event, config, 16, 0);</span>
<span class="p_add">+SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_stream_id, config1, 32, 0);</span>
<span class="p_add">+SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_span, config1, 1, 32);</span>
<span class="p_add">+SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_sec, config1, 1, 33);</span>
<span class="p_add">+SMMU_PMU_EVENT_ATTR_EXTRACTOR(filter_enable, config1, 1, 34);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_enable(struct pmu *pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	writel(SMMU_PMCG_CR_ENABLE, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="p_add">+	writel(SMMU_PMCG_IRQ_CTRL_IRQEN,</span>
<span class="p_add">+	       smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_disable(struct pmu *pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(pmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_CR);</span>
<span class="p_add">+	writel(0, smmu_pmu-&gt;reg_base + SMMU_PMCG_IRQ_CTRL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_counter_set_value(struct smmu_pmu *smmu_pmu,</span>
<span class="p_add">+					      u32 idx, u64 value)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="p_add">+		writel(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="p_add">+	else</span>
<span class="p_add">+		writeq(value, smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u64 smmu_pmu_counter_get_value(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 value;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (smmu_pmu-&gt;reg_size_32)</span>
<span class="p_add">+		value = readl(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 4));</span>
<span class="p_add">+	else</span>
<span class="p_add">+		value = readq(smmu_pmu-&gt;reloc_base + SMMU_PMCG_EVCNTR(idx, 8));</span>
<span class="p_add">+</span>
<span class="p_add">+	return value;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_counter_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENSET0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_counter_disable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_CNTENCLR0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_interrupt_enable(struct smmu_pmu *smmu_pmu, u32 idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENSET0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_interrupt_disable(struct smmu_pmu *smmu_pmu,</span>
<span class="p_add">+					      u32 idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writeq(BIT(idx), smmu_pmu-&gt;reg_base + SMMU_PMCG_INTENCLR0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_reset(struct smmu_pmu *smmu_pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; smmu_pmu-&gt;num_counters; i++) {</span>
<span class="p_add">+		smmu_pmu_counter_disable(smmu_pmu, i);</span>
<span class="p_add">+		smmu_pmu_interrupt_disable(smmu_pmu, i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_set_evtyper(struct smmu_pmu *smmu_pmu, u32 idx,</span>
<span class="p_add">+					u32 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_EVTYPER(idx));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void smmu_pmu_set_smr(struct smmu_pmu *smmu_pmu, u32 idx, u32 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	writel(val, smmu_pmu-&gt;reg_base + SMMU_PMCG_SMR(idx));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u64 smmu_pmu_getreset_ovsr(struct smmu_pmu *smmu_pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 result = readq_relaxed(smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSSET0);</span>
<span class="p_add">+</span>
<span class="p_add">+	writeq(result, smmu_pmu-&gt;reloc_base + SMMU_PMCG_OVSCLR0);</span>
<span class="p_add">+	return result;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool smmu_pmu_has_overflowed(struct smmu_pmu *smmu_pmu, u64 ovsr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !!(ovsr &amp; smmu_pmu-&gt;counter_present_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_event_update(struct perf_event *event)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+	u64 delta, prev, now;</span>
<span class="p_add">+	u32 idx = hwc-&gt;idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		prev = local64_read(&amp;hwc-&gt;prev_count);</span>
<span class="p_add">+		now = smmu_pmu_counter_get_value(smmu_pmu, idx);</span>
<span class="p_add">+	} while (local64_cmpxchg(&amp;hwc-&gt;prev_count, prev, now) != prev);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* handle overflow. */</span>
<span class="p_add">+	delta = now - prev;</span>
<span class="p_add">+	delta &amp;= smmu_pmu-&gt;counter_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	local64_add(delta, &amp;event-&gt;count);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_set_period(struct smmu_pmu *smmu_pmu,</span>
<span class="p_add">+				struct hw_perf_event *hwc)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 idx = hwc-&gt;idx;</span>
<span class="p_add">+	u64 new;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We limit the max period to half the max counter value of the smallest</span>
<span class="p_add">+	 * counter size, so that even in the case of extreme interrupt latency</span>
<span class="p_add">+	 * the counter will (hopefully) not wrap past its initial value.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	new = SMMU_COUNTER_RELOAD;</span>
<span class="p_add">+</span>
<span class="p_add">+	local64_set(&amp;hwc-&gt;prev_count, new);</span>
<span class="p_add">+	smmu_pmu_counter_set_value(smmu_pmu, idx, new);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static irqreturn_t smmu_pmu_handle_irq(int irq_num, void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = data;</span>
<span class="p_add">+	u64 ovsr;</span>
<span class="p_add">+	unsigned int idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	ovsr = smmu_pmu_getreset_ovsr(smmu_pmu);</span>
<span class="p_add">+	if (!smmu_pmu_has_overflowed(smmu_pmu, ovsr))</span>
<span class="p_add">+		return IRQ_NONE;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_set_bit(idx, (unsigned long *)&amp;ovsr, smmu_pmu-&gt;num_counters) {</span>
<span class="p_add">+		struct perf_event *event = smmu_pmu-&gt;events[idx];</span>
<span class="p_add">+		struct hw_perf_event *hwc;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (WARN_ON_ONCE(!event))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		smmu_pmu_event_update(event);</span>
<span class="p_add">+		hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+</span>
<span class="p_add">+		smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return IRQ_HANDLED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int smmu_pmu_get_event_idx(struct smmu_pmu *smmu_pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int idx;</span>
<span class="p_add">+	unsigned int num_ctrs = smmu_pmu-&gt;num_counters;</span>
<span class="p_add">+</span>
<span class="p_add">+	idx = find_first_zero_bit(smmu_pmu-&gt;used_counters, num_ctrs);</span>
<span class="p_add">+	if (idx == num_ctrs)</span>
<span class="p_add">+		/* The counters are all in use. */</span>
<span class="p_add">+		return -EAGAIN;</span>
<span class="p_add">+</span>
<span class="p_add">+	set_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="p_add">+</span>
<span class="p_add">+	return idx;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Implementation of abstract pmu functionality required by</span>
<span class="p_add">+ * the core perf events code.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static int smmu_pmu_event_init(struct perf_event *event)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	struct perf_event *sibling;</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu;</span>
<span class="p_add">+	u32 event_id;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (event-&gt;attr.type != event-&gt;pmu-&gt;type)</span>
<span class="p_add">+		return -ENOENT;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hwc-&gt;sample_period) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+				    &quot;Sampling not supported\n&quot;);</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (event-&gt;cpu &lt; 0) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+				    &quot;Per-task mode not supported\n&quot;);</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We cannot filter accurately so we just don&#39;t allow it. */</span>
<span class="p_add">+	if (event-&gt;attr.exclude_user || event-&gt;attr.exclude_kernel ||</span>
<span class="p_add">+	    event-&gt;attr.exclude_hv || event-&gt;attr.exclude_idle) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+				    &quot;Can&#39;t exclude execution levels\n&quot;);</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Verify specified event is supported on this PMU */</span>
<span class="p_add">+	event_id = get_event(event);</span>
<span class="p_add">+	if ((event_id &gt;= SMMU_MAX_EVENT_ID) ||</span>
<span class="p_add">+	    (!test_bit(event_id, smmu_pmu-&gt;supported_events))) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+				    &quot;Invalid event %d for this PMU\n&quot;,</span>
<span class="p_add">+				    event_id);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Don&#39;t allow groups with mixed PMUs, except for s/w events */</span>
<span class="p_add">+	if (event-&gt;group_leader-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="p_add">+	    !is_software_event(event-&gt;group_leader)) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+			 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(sibling, &amp;event-&gt;group_leader-&gt;sibling_list,</span>
<span class="p_add">+			    group_entry)</span>
<span class="p_add">+		if (sibling-&gt;pmu != event-&gt;pmu &amp;&amp;</span>
<span class="p_add">+		    !is_software_event(sibling)) {</span>
<span class="p_add">+			dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+				 &quot;Can&#39;t create mixed PMU group\n&quot;);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Ensure all events in a group are on the same cpu */</span>
<span class="p_add">+	if ((event-&gt;group_leader != event) &amp;&amp;</span>
<span class="p_add">+	    (event-&gt;cpu != event-&gt;group_leader-&gt;cpu)) {</span>
<span class="p_add">+		dev_dbg_ratelimited(&amp;smmu_pmu-&gt;pdev-&gt;dev,</span>
<span class="p_add">+			 &quot;Can&#39;t create group on CPUs %d and %d&quot;,</span>
<span class="p_add">+			 event-&gt;cpu, event-&gt;group_leader-&gt;cpu);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	hwc-&gt;idx = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Ensure all events are on the same cpu so all events are in the</span>
<span class="p_add">+	 * same cpu context, to avoid races on pmu_enable etc.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	event-&gt;cpu = smmu_pmu-&gt;on_cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_event_start(struct perf_event *event, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	int idx = hwc-&gt;idx;</span>
<span class="p_add">+	u32 evtyper;</span>
<span class="p_add">+	u32 filter_sec;</span>
<span class="p_add">+	u32 filter_span;</span>
<span class="p_add">+	u32 filter_stream_id;</span>
<span class="p_add">+</span>
<span class="p_add">+	hwc-&gt;state = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_set_period(smmu_pmu, hwc);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (get_filter_enable(event)) {</span>
<span class="p_add">+		filter_sec = get_filter_sec(event);</span>
<span class="p_add">+		filter_span = get_filter_span(event);</span>
<span class="p_add">+		filter_stream_id = get_filter_stream_id(event);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		filter_sec = SMMU_DEFAULT_FILTER_SEC;</span>
<span class="p_add">+		filter_span = SMMU_DEFAULT_FILTER_SPAN;</span>
<span class="p_add">+		filter_stream_id = SMMU_DEFAULT_FILTER_STREAM_ID;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	evtyper = get_event(event) |</span>
<span class="p_add">+		  filter_span &lt;&lt; SMMU_PMCG_EVTYPER_SID_SPAN_SHIFT |</span>
<span class="p_add">+		  filter_sec &lt;&lt; SMMU_PMCG_EVTYPER_SEC_SID_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_set_evtyper(smmu_pmu, idx, evtyper);</span>
<span class="p_add">+	smmu_pmu_set_smr(smmu_pmu, idx, filter_stream_id);</span>
<span class="p_add">+	smmu_pmu_interrupt_enable(smmu_pmu, idx);</span>
<span class="p_add">+	smmu_pmu_counter_enable(smmu_pmu, idx);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_event_stop(struct perf_event *event, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	int idx = hwc-&gt;idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hwc-&gt;state &amp; PERF_HES_STOPPED)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_interrupt_disable(smmu_pmu, idx);</span>
<span class="p_add">+	smmu_pmu_counter_disable(smmu_pmu, idx);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; PERF_EF_UPDATE)</span>
<span class="p_add">+		smmu_pmu_event_update(event);</span>
<span class="p_add">+	hwc-&gt;state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int smmu_pmu_event_add(struct perf_event *event, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	int idx;</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	idx = smmu_pmu_get_event_idx(smmu_pmu);</span>
<span class="p_add">+	if (idx &lt; 0)</span>
<span class="p_add">+		return idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	hwc-&gt;idx = idx;</span>
<span class="p_add">+	hwc-&gt;state = PERF_HES_STOPPED | PERF_HES_UPTODATE;</span>
<span class="p_add">+	smmu_pmu-&gt;events[idx] = event;</span>
<span class="p_add">+	local64_set(&amp;hwc-&gt;prev_count, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; PERF_EF_START)</span>
<span class="p_add">+		smmu_pmu_event_start(event, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Propagate changes to the userspace mapping. */</span>
<span class="p_add">+	perf_event_update_userpage(event);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_event_del(struct perf_event *event, int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hw_perf_event *hwc = &amp;event-&gt;hw;</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(event-&gt;pmu);</span>
<span class="p_add">+	int idx = hwc-&gt;idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_event_stop(event, flags | PERF_EF_UPDATE);</span>
<span class="p_add">+	smmu_pmu-&gt;events[idx] = NULL;</span>
<span class="p_add">+	clear_bit(idx, smmu_pmu-&gt;used_counters);</span>
<span class="p_add">+</span>
<span class="p_add">+	perf_event_update_userpage(event);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_event_read(struct perf_event *event)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smmu_pmu_event_update(event);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* cpumask */</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t smmu_pmu_cpumask_show(struct device *dev,</span>
<span class="p_add">+				     struct device_attribute *attr,</span>
<span class="p_add">+				     char *buf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="p_add">+</span>
<span class="p_add">+	return cpumap_print_to_pagebuf(true, buf, cpumask_of(smmu_pmu-&gt;on_cpu));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct device_attribute smmu_pmu_cpumask_attr =</span>
<span class="p_add">+		__ATTR(cpumask, 0444, smmu_pmu_cpumask_show, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *smmu_pmu_cpumask_attrs[] = {</span>
<span class="p_add">+	&amp;smmu_pmu_cpumask_attr.attr,</span>
<span class="p_add">+	NULL,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute_group smmu_pmu_cpumask_group = {</span>
<span class="p_add">+	.attrs = smmu_pmu_cpumask_attrs,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Events */</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t smmu_pmu_event_show(struct device *dev,</span>
<span class="p_add">+			    struct device_attribute *attr, char *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);</span>
<span class="p_add">+</span>
<span class="p_add">+	return sprintf(page, &quot;event=0x%02llx\n&quot;, pmu_attr-&gt;id);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define SMMU_EVENT_ATTR(_name, _id)					  \</span>
<span class="p_add">+	(&amp;((struct perf_pmu_events_attr[]) {				  \</span>
<span class="p_add">+		{ .attr = __ATTR(_name, 0444, smmu_pmu_event_show, NULL), \</span>
<span class="p_add">+		  .id = _id, }						  \</span>
<span class="p_add">+	})[0].attr.attr)</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *smmu_pmu_events[] = {</span>
<span class="p_add">+	SMMU_EVENT_ATTR(cycles, SMMU_PMU_CYCLES),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(transaction, SMMU_PMU_TRANSACTION),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(tlb_miss, SMMU_PMU_TLB_MISS),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(config_cache_miss, SMMU_PMU_CONFIG_CACHE_MISS),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(trans_table_walk, SMMU_PMU_TRANS_TABLE_WALK),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(config_struct_access, SMMU_PMU_CONFIG_STRUCT_ACCESS),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(pcie_ats_trans_rq, SMMU_PMU_PCIE_ATS_TRANS_RQ),</span>
<span class="p_add">+	SMMU_EVENT_ATTR(pcie_ats_trans_passed, SMMU_PMU_PCIE_ATS_TRANS_PASSED),</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static umode_t smmu_pmu_event_is_visible(struct kobject *kobj,</span>
<span class="p_add">+					 struct attribute *attr, int unused)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct device *dev = kobj_to_dev(kobj);</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = to_smmu_pmu(dev_get_drvdata(dev));</span>
<span class="p_add">+	struct perf_pmu_events_attr *pmu_attr;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmu_attr = container_of(attr, struct perf_pmu_events_attr, attr.attr);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (test_bit(pmu_attr-&gt;id, smmu_pmu-&gt;supported_events))</span>
<span class="p_add">+		return attr-&gt;mode;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static struct attribute_group smmu_pmu_events_group = {</span>
<span class="p_add">+	.name = &quot;events&quot;,</span>
<span class="p_add">+	.attrs = smmu_pmu_events,</span>
<span class="p_add">+	.is_visible = smmu_pmu_event_is_visible,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Formats */</span>
<span class="p_add">+PMU_FORMAT_ATTR(event,		   &quot;config:0-15&quot;);</span>
<span class="p_add">+PMU_FORMAT_ATTR(filter_stream_id,  &quot;config1:0-31&quot;);</span>
<span class="p_add">+PMU_FORMAT_ATTR(filter_span,	   &quot;config1:32&quot;);</span>
<span class="p_add">+PMU_FORMAT_ATTR(filter_sec,	   &quot;config1:33&quot;);</span>
<span class="p_add">+PMU_FORMAT_ATTR(filter_enable,	   &quot;config1:34&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute *smmu_pmu_formats[] = {</span>
<span class="p_add">+	&amp;format_attr_event.attr,</span>
<span class="p_add">+	&amp;format_attr_filter_stream_id.attr,</span>
<span class="p_add">+	&amp;format_attr_filter_span.attr,</span>
<span class="p_add">+	&amp;format_attr_filter_sec.attr,</span>
<span class="p_add">+	&amp;format_attr_filter_enable.attr,</span>
<span class="p_add">+	NULL</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct attribute_group smmu_pmu_format_group = {</span>
<span class="p_add">+	.name = &quot;format&quot;,</span>
<span class="p_add">+	.attrs = smmu_pmu_formats,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct attribute_group *smmu_pmu_attr_grps[] = {</span>
<span class="p_add">+	&amp;smmu_pmu_cpumask_group,</span>
<span class="p_add">+	&amp;smmu_pmu_events_group,</span>
<span class="p_add">+	&amp;smmu_pmu_format_group,</span>
<span class="p_add">+	NULL,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generic device handlers</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int get_num_counters(struct smmu_pmu *smmu_pmu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 cfgr = readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ((cfgr &amp; SMMU_PMCG_CFGR_NCTR_MASK) &gt;&gt; SMMU_PMCG_CFGR_NCTR_SHIFT)</span>
<span class="p_add">+		+ 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int smmu_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu;</span>
<span class="p_add">+	unsigned int target;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu = hlist_entry_safe(node, struct smmu_pmu, node);</span>
<span class="p_add">+	if (cpu != smmu_pmu-&gt;on_cpu)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	target = cpumask_any_but(cpu_online_mask, cpu);</span>
<span class="p_add">+	if (target &gt;= nr_cpu_ids)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	perf_pmu_migrate_context(&amp;smmu_pmu-&gt;pmu, cpu, target);</span>
<span class="p_add">+	smmu_pmu-&gt;on_cpu = target;</span>
<span class="p_add">+	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(target)));</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int smmu_pmu_probe(struct platform_device *pdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu;</span>
<span class="p_add">+	struct resource *mem_resource_0, *mem_resource_1;</span>
<span class="p_add">+	void __iomem *mem_map_0, *mem_map_1;</span>
<span class="p_add">+	unsigned int reg_size;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+	int irq;</span>
<span class="p_add">+	u32 ceid[SMMU_NUM_EVENTS_U32];</span>
<span class="p_add">+	u64 ceid_64;</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(*smmu_pmu), GFP_KERNEL);</span>
<span class="p_add">+	if (!smmu_pmu)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	platform_set_drvdata(pdev, smmu_pmu);</span>
<span class="p_add">+	smmu_pmu-&gt;pmu = (struct pmu) {</span>
<span class="p_add">+		.task_ctx_nr    = perf_invalid_context,</span>
<span class="p_add">+		.pmu_enable	= smmu_pmu_enable,</span>
<span class="p_add">+		.pmu_disable	= smmu_pmu_disable,</span>
<span class="p_add">+		.event_init	= smmu_pmu_event_init,</span>
<span class="p_add">+		.add		= smmu_pmu_event_add,</span>
<span class="p_add">+		.del		= smmu_pmu_event_del,</span>
<span class="p_add">+		.start		= smmu_pmu_event_start,</span>
<span class="p_add">+		.stop		= smmu_pmu_event_stop,</span>
<span class="p_add">+		.read		= smmu_pmu_event_read,</span>
<span class="p_add">+		.attr_groups	= smmu_pmu_attr_grps,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	mem_resource_0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);</span>
<span class="p_add">+	mem_map_0 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ERR(mem_map_0)) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="p_add">+			&amp;mem_resource_0-&gt;start);</span>
<span class="p_add">+		return PTR_ERR(mem_map_0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu-&gt;reg_base = mem_map_0;</span>
<span class="p_add">+	smmu_pmu-&gt;pmu.name =</span>
<span class="p_add">+		devm_kasprintf(&amp;pdev-&gt;dev, GFP_KERNEL, &quot;smmu_0_%llx&quot;,</span>
<span class="p_add">+			       (mem_resource_0-&gt;start) &gt;&gt; SMMU_PA_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!smmu_pmu-&gt;pmu.name) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;Failed to create PMU name&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID0);</span>
<span class="p_add">+	ceid[0] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="p_add">+	ceid[1] = ceid_64 &gt;&gt; 32;</span>
<span class="p_add">+	ceid_64 = readq(smmu_pmu-&gt;reg_base + SMMU_PMCG_CEID1);</span>
<span class="p_add">+	ceid[2] = ceid_64 &amp; GENMASK(31, 0);</span>
<span class="p_add">+	ceid[3] = ceid_64 &gt;&gt; 32;</span>
<span class="p_add">+	bitmap_from_u32array(smmu_pmu-&gt;supported_events, SMMU_MAX_EVENT_ID,</span>
<span class="p_add">+			     ceid, SMMU_NUM_EVENTS_U32);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Determine if page 1 is present */</span>
<span class="p_add">+	if (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="p_add">+	    SMMU_PMCG_CFGR_RELOC_CTRS) {</span>
<span class="p_add">+		mem_resource_1 = platform_get_resource(pdev, IORESOURCE_MEM, 1);</span>
<span class="p_add">+		mem_map_1 = devm_ioremap_resource(&amp;pdev-&gt;dev, mem_resource_1);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (IS_ERR(mem_map_1)) {</span>
<span class="p_add">+			dev_err(&amp;pdev-&gt;dev, &quot;Can&#39;t map SMMU PMU @%pa\n&quot;,</span>
<span class="p_add">+				&amp;mem_resource_1-&gt;start);</span>
<span class="p_add">+			return PTR_ERR(mem_map_1);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		smmu_pmu-&gt;reloc_base = mem_map_1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		smmu_pmu-&gt;reloc_base = smmu_pmu-&gt;reg_base;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	irq = platform_get_irq(pdev, 0);</span>
<span class="p_add">+	if (irq &lt; 0) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev,</span>
<span class="p_add">+			&quot;Failed to get valid irq for smmu @%pa\n&quot;,</span>
<span class="p_add">+			&amp;mem_resource_0-&gt;start);</span>
<span class="p_add">+		return irq;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	err = devm_request_irq(&amp;pdev-&gt;dev, irq, smmu_pmu_handle_irq,</span>
<span class="p_add">+			       IRQF_NOBALANCING | IRQF_SHARED | IRQF_NO_THREAD,</span>
<span class="p_add">+			       &quot;smmu-pmu&quot;, smmu_pmu);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev,</span>
<span class="p_add">+			&quot;Unable to request IRQ%d for SMMU PMU counters\n&quot;, irq);</span>
<span class="p_add">+		return err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu-&gt;irq = irq;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Pick one CPU to be the preferred one to use */</span>
<span class="p_add">+	smmu_pmu-&gt;on_cpu = smp_processor_id();</span>
<span class="p_add">+	WARN_ON(irq_set_affinity(smmu_pmu-&gt;irq, cpumask_of(smmu_pmu-&gt;on_cpu)));</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu-&gt;num_counters = get_num_counters(smmu_pmu);</span>
<span class="p_add">+	smmu_pmu-&gt;pdev = pdev;</span>
<span class="p_add">+	smmu_pmu-&gt;counter_present_mask = GENMASK(smmu_pmu-&gt;num_counters - 1, 0);</span>
<span class="p_add">+	reg_size = (readl(smmu_pmu-&gt;reg_base + SMMU_PMCG_CFGR) &amp;</span>
<span class="p_add">+		    SMMU_PMCG_CFGR_SIZE_MASK) &gt;&gt; SMMU_PMCG_CFGR_SIZE_SHIFT;</span>
<span class="p_add">+	smmu_pmu-&gt;reg_size_32 = (reg_size == SMMU_PMCG_CFGR_COUNTER_SIZE_32);</span>
<span class="p_add">+	smmu_pmu-&gt;counter_mask = GENMASK_ULL(reg_size, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_reset(smmu_pmu);</span>
<span class="p_add">+</span>
<span class="p_add">+	err = cpuhp_state_add_instance_nocalls(cpuhp_state_num,</span>
<span class="p_add">+					       &amp;smmu_pmu-&gt;node);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering hotplug&quot;, err);</span>
<span class="p_add">+		return err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	err = perf_pmu_register(&amp;smmu_pmu-&gt;pmu, smmu_pmu-&gt;pmu.name, -1);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;Error %d registering SMMU PMU\n&quot;, err);</span>
<span class="p_add">+		goto out_unregister;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	dev_info(&amp;pdev-&gt;dev, &quot;Registered SMMU PMU @ %pa using %d counters\n&quot;,</span>
<span class="p_add">+		 &amp;mem_resource_0-&gt;start, smmu_pmu-&gt;num_counters);</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unregister:</span>
<span class="p_add">+	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int smmu_pmu_remove(struct platform_device *pdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="p_add">+</span>
<span class="p_add">+	perf_pmu_unregister(&amp;smmu_pmu-&gt;pmu);</span>
<span class="p_add">+	cpuhp_state_remove_instance_nocalls(cpuhp_state_num, &amp;smmu_pmu-&gt;node);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void smmu_pmu_shutdown(struct platform_device *pdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct smmu_pmu *smmu_pmu = platform_get_drvdata(pdev);</span>
<span class="p_add">+</span>
<span class="p_add">+	smmu_pmu_disable(&amp;smmu_pmu-&gt;pmu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct platform_driver smmu_pmu_driver = {</span>
<span class="p_add">+	.driver = {</span>
<span class="p_add">+		.name = &quot;arm-smmu-pmu&quot;,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	.probe = smmu_pmu_probe,</span>
<span class="p_add">+	.remove = smmu_pmu_remove,</span>
<span class="p_add">+	.shutdown = smmu_pmu_shutdown,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init arm_smmu_pmu_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpuhp_state_num = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,</span>
<span class="p_add">+				      &quot;perf/arm/smmupmu:online&quot;,</span>
<span class="p_add">+				      NULL,</span>
<span class="p_add">+				      smmu_pmu_offline_cpu);</span>
<span class="p_add">+	if (cpuhp_state_num &lt; 0)</span>
<span class="p_add">+		return cpuhp_state_num;</span>
<span class="p_add">+</span>
<span class="p_add">+	return platform_driver_register(&amp;smmu_pmu_driver);</span>
<span class="p_add">+}</span>
<span class="p_add">+module_init(arm_smmu_pmu_init);</span>
<span class="p_add">+</span>
<span class="p_add">+static void __exit arm_smmu_pmu_exit(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	platform_driver_unregister(&amp;smmu_pmu_driver);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+module_exit(arm_smmu_pmu_exit);</span>
<span class="p_add">+MODULE_LICENSE(&quot;GPL v2&quot;);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



