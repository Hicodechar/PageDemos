
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3.16,1/7] mm: larger stack guard gap, between vmas - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3.16,1/7] mm: larger stack guard gap, between vmas</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 27, 2017, 11:10 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;lsq.1498561834.794227558@decadent.org.uk&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9811427/mbox/"
   >mbox</a>
|
   <a href="/patch/9811427/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9811427/">/patch/9811427/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A0495603D7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:14:27 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 92138283AF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:14:27 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 86C67285BE; Tue, 27 Jun 2017 11:14:27 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BBE6A283AF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 27 Jun 2017 11:14:25 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753140AbdF0LOV (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 27 Jun 2017 07:14:21 -0400
Received: from shadbolt.e.decadent.org.uk ([88.96.1.126]:34835 &quot;EHLO
	shadbolt.e.decadent.org.uk&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752484AbdF0LMi (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 27 Jun 2017 07:12:38 -0400
Received: from [2a02:8011:400e:2:6f00:88c8:c921:d332] (helo=deadeye)
	by shadbolt.decadent.org.uk with esmtps
	(TLS1.2:ECDHE_RSA_AES_256_GCM_SHA384:256) (Exim 4.84_2)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dPoQ8-0006D3-Nu; Tue, 27 Jun 2017 12:12:36 +0100
Received: from ben by deadeye with local (Exim 4.89)
	(envelope-from &lt;ben@decadent.org.uk&gt;)
	id 1dPoQ3-0001JS-Jn; Tue, 27 Jun 2017 12:12:31 +0100
Content-Type: text/plain; charset=&quot;UTF-8&quot;
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
MIME-Version: 1.0
From: Ben Hutchings &lt;ben@decadent.org.uk&gt;
To: linux-kernel@vger.kernel.org, stable@vger.kernel.org
CC: akpm@linux-foundation.org,
	&quot;Linus Torvalds&quot; &lt;torvalds@linux-foundation.org&gt;,
	&quot;Hugh Dickins&quot; &lt;hughd@google.com&gt;,
	&quot;Michal Hocko&quot; &lt;mhocko@suse.com&gt;, &quot;Helge Deller&quot; &lt;deller@gmx.de&gt;
Date: Tue, 27 Jun 2017 12:10:34 +0100
Message-ID: &lt;lsq.1498561834.794227558@decadent.org.uk&gt;
X-Mailer: LinuxStableQueue (scripts by bwh)
Subject: [PATCH 3.16 1/7] mm: larger stack guard gap, between vmas
In-Reply-To: &lt;lsq.1498561833.742501160@decadent.org.uk&gt;
X-SA-Exim-Connect-IP: 2a02:8011:400e:2:6f00:88c8:c921:d332
X-SA-Exim-Mail-From: ben@decadent.org.uk
X-SA-Exim-Scanned: No (on shadbolt.decadent.org.uk);
	SAEximRunCond expanded to false
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131">Ben Hutchings</a> - June 27, 2017, 11:10 a.m.</div>
<pre class="content">
3.16.45-rc1 review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: Hugh Dickins &lt;hughd@google.com&gt;</span>

commit 1be7107fbe18eed3e319a6c3e83c78254b693acb upstream.

Stack guard page is a useful feature to reduce a risk of stack smashing
into a different mapping. We have been using a single page gap which
is sufficient to prevent having stack adjacent to a different mapping.
But this seems to be insufficient in the light of the stack usage in
userspace. E.g. glibc uses as large as 64kB alloca() in many commonly
used functions. Others use constructs liks gid_t buffer[NGROUPS_MAX]
which is 256kB or stack strings with MAX_ARG_STRLEN.

This will become especially dangerous for suid binaries and the default
no limit for the stack size limit because those applications can be
tricked to consume a large portion of the stack and a single glibc call
could jump over the guard page. These attacks are not theoretical,
unfortunatelly.

Make those attacks less probable by increasing the stack guard gap
to 1MB (on systems with 4k pages; but make it depend on the page size
because systems with larger base pages might cap stack allocations in
the PAGE_SIZE units) which should cover larger alloca() and VLA stack
allocations. It is obviously not a full fix because the problem is
somehow inherent, but it should reduce attack space a lot.

One could argue that the gap size should be configurable from userspace,
but that can be done later when somebody finds that the new 1MB is wrong
for some special case applications.  For now, add a kernel command line
option (stack_guard_gap) to specify the stack gap size (in page units).

Implementation wise, first delete all the old code for stack guard page:
because although we could get away with accounting one extra page in a
stack vma, accounting a larger gap can break userspace - case in point,
a program run with &quot;ulimit -S -v 20000&quot; failed when the 1MB gap was
counted for RLIMIT_AS; similar problems could come with RLIMIT_MLOCK
and strict non-overcommit mode.

Instead of keeping gap inside the stack vma, maintain the stack guard
gap as a gap between vmas: using vm_start_gap() in place of vm_start
(or vm_end_gap() in place of vm_end if VM_GROWSUP) in just those few
places which need to respect the gap - mainly arch_get_unmapped_area(),
and and the vma tree&#39;s subtree_gap support for that.

Original-patch-by: Oleg Nesterov &lt;oleg@redhat.com&gt;
Original-patch-by: Michal Hocko &lt;mhocko@suse.com&gt;
<span class="signed-off-by">Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="tested-by">Tested-by: Helge Deller &lt;deller@gmx.de&gt; # parisc</span>
<span class="signed-off-by">Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
[Hugh Dickins: Backported to 3.16]
<span class="signed-off-by">Signed-off-by: Ben Hutchings &lt;ben@decadent.org.uk&gt;</span>
---
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -3154,6 +3154,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">--- a/arch/arc/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arc/mm/mmap.c</span>
<span class="p_chunk">@@ -64,7 +64,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_a</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/sys_parisc.c</span>
<span class="p_chunk">@@ -88,7 +88,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	unsigned long task_size = TASK_SIZE;
 	int do_color_align, last_mmap;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -115,9 +115,10 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		else
 			addr = PAGE_ALIGN(addr);
 
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			goto found_addr;
 	}
 
<span class="p_chunk">@@ -141,7 +142,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	int do_color_align, last_mmap;
<span class="p_chunk">@@ -175,9 +176,11 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			addr = COLOR_ALIGN(addr, last_mmap, pgoff);
 		else
 			addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			goto found_addr;
 	}
 
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -103,7 +103,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -118,7 +118,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -181,7 +181,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -265,7 +265,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -127,7 +127,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -156,7 +156,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">--- a/arch/xtensa/kernel/syscall.c</span>
<span class="p_header">+++ b/arch/xtensa/kernel/syscall.c</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(str</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -171,7 +171,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *f</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -273,11 +273,7 @@</span> <span class="p_context"> show_map_vma(struct seq_file *m, struct</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_setwidth(m, 25 + sizeof(void *) * 6 - 1);
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu &quot;,
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1241,34 +1241,6 @@</span> <span class="p_context"> int set_page_dirty_lock(struct page *pag</span>
 int clear_page_dirty_for_io(struct page *page);
 int get_cmdline(struct task_struct *task, char *buffer, int buflen);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern pid_t
 vm_is_stack(struct task_struct *task, struct vm_area_struct *vma, int in_group);
 
<span class="p_chunk">@@ -1914,6 +1886,7 @@</span> <span class="p_context"> void page_cache_async_readahead(struct a</span>
 
 unsigned long max_sane_readahead(unsigned long nr);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1942,6 +1915,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * fi</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -266,11 +266,6 @@</span> <span class="p_context"> static int faultin_page(struct task_stru</span>
 	unsigned int fault_flags = 0;
 	int ret;
 
<span class="p_del">-	/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-	if ((*flags &amp; FOLL_MLOCK) &amp;&amp;</span>
<span class="p_del">-			(stack_guard_page_start(vma, address) ||</span>
<span class="p_del">-			 stack_guard_page_end(vma, address + PAGE_SIZE)))</span>
<span class="p_del">-		return -ENOENT;</span>
 	if (*flags &amp; FOLL_WRITE)
 		fault_flags |= FAULT_FLAG_WRITE;
 	if (nonblocking)
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2589,40 +2589,6 @@</span> <span class="p_context"> out_release:</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -2641,10 +2607,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_s</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -266,6 +266,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 	bool populate;
 
<span class="p_chunk">@@ -311,7 +312,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -334,10 +336,22 @@</span> <span class="p_context"> out:</span>
 
 static long vma_compute_subtree_gap(struct vm_area_struct *vma)
 {
<span class="p_del">-	unsigned long max, subtree_gap;</span>
<span class="p_del">-	max = vma-&gt;vm_start;</span>
<span class="p_del">-	if (vma-&gt;vm_prev)</span>
<span class="p_del">-		max -= vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_add">+	unsigned long max, prev_end, subtree_gap;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we</span>
<span class="p_add">+	 * allow two stack_guard_gaps between them here, and when choosing</span>
<span class="p_add">+	 * an unmapped area; whereas when expanding we only require one.</span>
<span class="p_add">+	 * That&#39;s a little inconsistent, but keeps the code here simpler.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max = vm_start_gap(vma);</span>
<span class="p_add">+	if (vma-&gt;vm_prev) {</span>
<span class="p_add">+		prev_end = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+		if (max &gt; prev_end)</span>
<span class="p_add">+			max -= prev_end;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			max = 0;</span>
<span class="p_add">+	}</span>
 	if (vma-&gt;vm_rb.rb_left) {
 		subtree_gap = rb_entry(vma-&gt;vm_rb.rb_left,
 				struct vm_area_struct, vm_rb)-&gt;rb_subtree_gap;
<span class="p_chunk">@@ -426,7 +440,7 @@</span> <span class="p_context"> static void validate_mm(struct mm_struct</span>
 			anon_vma_unlock_read(anon_vma);
 		}
 
<span class="p_del">-		highest_address = vma-&gt;vm_end;</span>
<span class="p_add">+		highest_address = vm_end_gap(vma);</span>
 		vma = vma-&gt;vm_next;
 		i++;
 	}
<span class="p_chunk">@@ -594,7 +608,7 @@</span> <span class="p_context"> void __vma_link_rb(struct mm_struct *mm,</span>
 	if (vma-&gt;vm_next)
 		vma_gap_update(vma-&gt;vm_next);
 	else
<span class="p_del">-		mm-&gt;highest_vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 
 	/*
 	 * vma-&gt;vm_prev wasn&#39;t known when we followed the rbtree to find the
<span class="p_chunk">@@ -846,7 +860,7 @@</span> <span class="p_context"> again:			remove_next = 1 + (end &gt; next-&gt;</span>
 			vma_gap_update(vma);
 		if (end_changed) {
 			if (!next)
<span class="p_del">-				mm-&gt;highest_vm_end = end;</span>
<span class="p_add">+				mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 			else if (!adjust_next)
 				vma_gap_update(next);
 		}
<span class="p_chunk">@@ -889,7 +903,7 @@</span> <span class="p_context"> again:			remove_next = 1 + (end &gt; next-&gt;</span>
 		else if (next)
 			vma_gap_update(next);
 		else
<span class="p_del">-			mm-&gt;highest_vm_end = end;</span>
<span class="p_add">+			VM_WARN_ON(mm-&gt;highest_vm_end != vm_end_gap(vma));</span>
 	}
 	if (insert &amp;&amp; file)
 		uprobe_mmap(insert);
<span class="p_chunk">@@ -1702,7 +1716,7 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_un</span>
 
 	while (true) {
 		/* Visit left subtree if it looks promising */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &gt;= low_limit &amp;&amp; vma-&gt;vm_rb.rb_left) {
 			struct vm_area_struct *left =
 				rb_entry(vma-&gt;vm_rb.rb_left,
<span class="p_chunk">@@ -1713,7 +1727,7 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_un</span>
 			}
 		}
 
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start &gt; high_limit)
<span class="p_chunk">@@ -1740,8 +1754,8 @@</span> <span class="p_context"> check_current:</span>
 			vma = rb_entry(rb_parent(prev),
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_left) {
<span class="p_del">-				gap_start = vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_del">-				gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+				gap_start = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+				gap_end = vm_start_gap(vma);</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1805,7 +1819,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(stru</span>
 
 	while (true) {
 		/* Visit right subtree if it looks promising */
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 		if (gap_start &lt;= high_limit &amp;&amp; vma-&gt;vm_rb.rb_right) {
 			struct vm_area_struct *right =
 				rb_entry(vma-&gt;vm_rb.rb_right,
<span class="p_chunk">@@ -1818,7 +1832,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(stru</span>
 
 check_current:
 		/* Check if current node has a suitable gap */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &lt; low_limit)
 			return -ENOMEM;
 		if (gap_start &lt;= high_limit &amp;&amp; gap_end - gap_start &gt;= length)
<span class="p_chunk">@@ -1844,7 +1858,7 @@</span> <span class="p_context"> check_current:</span>
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_right) {
 				gap_start = vma-&gt;vm_prev ?
<span class="p_del">-					vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+					vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1882,7 +1896,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct vm_unmapped_area_info info;
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
<span class="p_chunk">@@ -1893,9 +1907,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1918,7 +1933,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -1933,9 +1948,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct fi</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -2061,21 +2077,19 @@</span> <span class="p_context"> find_vma_prev(struct mm_struct *mm, unsi</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -2116,17 +2130,30 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_a</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error = 0;
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
 		return -EFAULT;
 
 	/* Guard against wrapping around to address 0. */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+	if (!address)</span>
 		return -ENOMEM;
 
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &lt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_chunk">@@ -2167,7 +2194,7 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct</span>
 				if (vma-&gt;vm_next)
 					vma_gap_update(vma-&gt;vm_next);
 				else
<span class="p_del">-					vma-&gt;vm_mm-&gt;highest_vm_end = address;</span>
<span class="p_add">+					vma-&gt;vm_mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 				spin_unlock(&amp;vma-&gt;vm_mm-&gt;page_table_lock);
 
 				perf_event_mmap(vma);
<span class="p_chunk">@@ -2187,6 +2214,8 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
 	address &amp;= PAGE_MASK;
<span class="p_chunk">@@ -2194,6 +2223,17 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_stru</span>
 	if (error)
 		return error;
 
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
<span class="p_chunk">@@ -2245,28 +2285,25 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_stru</span>
 	return error;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Note how expand_stack() refuses to expand the stack all the way to</span>
<span class="p_del">- * abut the next virtual mapping, *unless* that mapping itself is also</span>
<span class="p_del">- * a stack mapping. We want to leave room for a guard page, after all</span>
<span class="p_del">- * (the guard page itself is not added here, that is done by the</span>
<span class="p_del">- * actual page faulting logic)</span>
<span class="p_del">- *</span>
<span class="p_del">- * This matches the behavior of the guard page logic (see mm/memory.c:</span>
<span class="p_del">- * check_stack_guard_page()), which only allows the guard page to be</span>
<span class="p_del">- * removed under these circumstances.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *next;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	next = vma-&gt;vm_next;</span>
<span class="p_del">-	if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE) {</span>
<span class="p_del">-		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_upwards(vma, address);
 }
 
<span class="p_chunk">@@ -2288,14 +2325,6 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, un</span>
 #else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *prev;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	prev = vma-&gt;vm_prev;</span>
<span class="p_del">-	if (prev &amp;&amp; prev-&gt;vm_end == address) {</span>
<span class="p_del">-		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_downwards(vma, address);
 }
 
<span class="p_chunk">@@ -2391,7 +2420,7 @@</span> <span class="p_context"> detach_vmas_to_be_unmapped(struct mm_str</span>
 		vma-&gt;vm_prev = prev;
 		vma_gap_update(vma);
 	} else
<span class="p_del">-		mm-&gt;highest_vm_end = prev ? prev-&gt;vm_end : 0;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = prev ? vm_end_gap(prev) : 0;</span>
 	tail_vma-&gt;vm_next = NULL;
 
 	/* Kill the cache */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



