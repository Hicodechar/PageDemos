
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC] vhost, mm: make sure that oom_reaper doesn&#39;t reap memory read by vhost - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC] vhost, mm: make sure that oom_reaper doesn&#39;t reap memory read by vhost</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 17, 2016, 9 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1466154017-2222-1-git-send-email-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9183087/mbox/"
   >mbox</a>
|
   <a href="/patch/9183087/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9183087/">/patch/9183087/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	307DA6075D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jun 2016 09:00:36 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 16A932690E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jun 2016 09:00:36 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 08AE327DD0; Fri, 17 Jun 2016 09:00:36 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 58FC12690E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jun 2016 09:00:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933095AbcFQJA3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Jun 2016 05:00:29 -0400
Received: from mail-wm0-f68.google.com ([74.125.82.68]:35792 &quot;EHLO
	mail-wm0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753942AbcFQJAY (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Jun 2016 05:00:24 -0400
Received: by mail-wm0-f68.google.com with SMTP id k184so17024030wme.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 17 Jun 2016 02:00:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id;
	bh=6P6Ozh8pjaxbbfYENNtzvuoM3z4nsdmrNsBXOw12iaM=;
	b=As/lhgTBo3i9pFAJykqbZ0KLRGxAEUKgdav7zvEiYsHB5iklgoGvs9U6+z/DhEn0+R
	XNcl5mjDj6Mdq9xYW7hwlKb0D5Ka4bjajejbVbjZToqMpUNrw5x0QpuPViCUKzoC2txz
	rXdmgPqiK8BKhALavTPBvEwR27FuHVt3rzQSTc7pJBEE9Z+wbQ0dhmeSiFkhK+3coNrG
	GzKKQ8IN1fB9q57eXZCGayX8KSUYuxVjB7WXTk7gLXnBYCEfFSsBouCR/XCQp/txHlMG
	3fYIcNqggt4hv7GKAGsIRl+kr8ryxW1oOt5eABYjg/YU+nMR1ITppx2aB5lRwGRT8SoJ
	5YPw==
X-Gm-Message-State: ALyK8tILYLKaYn6IUqPNT0E9Faobvd2yLbCcdziN3nxxMTHdmDqPTJgOyd5iULUjUCcq+A==
X-Received: by 10.194.109.232 with SMTP id hv8mr1155929wjb.115.1466154022107;
	Fri, 17 Jun 2016 02:00:22 -0700 (PDT)
Received: from tiehlicka.suse.cz ([2a02:a03f:240:c200:2677:3ff:fe92:9ac4])
	by smtp.gmail.com with ESMTPSA id
	g3sm47748353wjb.47.2016.06.17.02.00.21
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Fri, 17 Jun 2016 02:00:21 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &quot;Michael S. Tsirkin&quot; &lt;mst@redhat.com&gt;
Cc: Vladimir Davydov &lt;vdavydov@parallels.com&gt;,
	Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	&lt;linux-mm@kvack.org&gt;, virtualization@lists.linux-foundation.org,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH] vhost,
	mm: make sure that oom_reaper doesn&#39;t reap memory read by vhost
Date: Fri, 17 Jun 2016 11:00:17 +0200
Message-Id: &lt;1466154017-2222-1-git-send-email-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.8.1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 17, 2016, 9 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

vhost driver relies on copy_from_user/get_user from a kernel thread.
This makes it impossible to reap the memory of an oom victim which
shares mm with the vhost kernel thread because it could see a zero
page unexpectedly and theoretically make an incorrect decision visible
outside of the killed task context.

Make sure that each place which can read from userspace is annotated
properly and it uses copy_from_user_mm, __get_user_mm resp.
copy_from_iter_mm. Each will get the target mm as an argument and it
performs a pessimistic check to rule out that the oom_reaper could
possibly unmap the particular page. __oom_reap_task then just needs to
mark the mm as unstable before it unmaps any page.

This is a preparatory patch without any functional changes because
the oom reaper doesn&#39;t touch mm shared with kthreads yet.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
Hi Michael,
we have discussed [1] that vhost_worker pins the mm of a potential
oom victim for too long which result into an OOM storm when other
processes have to be killed. One way to address this issue would
be to pin mm_count rather than mm_users and revalidate it before
actually doing the copy (mmget_not_zero). You had concerns about
more atomic operations in the data path. Another way would be to
postpone exit_mm_victim to after exit_task_work but as it turned
out other task might have the device open and pin the mm indirectly
[2].

Now I would like to attack the issue from another side which would
be more generic. I would like to make mm&#39;s which are shared with
kthreads oom reapable in general. This is currently not allowed
because we do not want to risk that a kthread would see an already
unmapped page - aka see a newly allocated zero page. At the same
time this is really desirable because it helps to guarantee a forward
progress on the OOM.

It seems that vhost usage would suffer from this problem because
it reads from the userspace to get (status) flags and makes some
decisions based on the read value.  I do not understand the code so I
couldn&#39;t evaluate whether that would lead to some real problems so I
conservatively assumed it wouldn&#39;t handle that gracefully. If this is
incorrect and all the paths can just cope with seeing zeros unexpectedly
then great and I will drop the patch and move over to the oom specific
further steps.

Therefore I am proposing a kthread safe API which allows to read from
userspace and also makes sure to do a proper exclusion with the oom
reaper. A race would be reported by EFAULT which is already handled.
Performance wise it would add two tests to the copy from user
paths. Does the following change makes sense to you and would be
acceptable? If yes I will follow up with another patch which will allow
oom reaper for mm shared with kthread.

Thanks!

[1] http://lkml.kernel.org/r/1456765329-14890-1-git-send-email-vdavydov@virtuozzo.com
[2] http://lkml.kernel.org/r/20160301181136-mutt-send-email-mst@redhat.com

 drivers/vhost/scsi.c    |  2 +-
 drivers/vhost/vhost.c   | 18 +++++++++---------
 include/linux/sched.h   |  1 +
 include/linux/uaccess.h | 22 ++++++++++++++++++++++
 include/linux/uio.h     | 10 ++++++++++
 mm/oom_kill.c           |  6 ++++++
 6 files changed, 49 insertions(+), 10 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - June 18, 2016, 12:09 a.m.</div>
<pre class="content">
On Fri, Jun 17, 2016 at 11:00:17AM +0200, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; vhost driver relies on copy_from_user/get_user from a kernel thread.</span>
<span class="quote">&gt; This makes it impossible to reap the memory of an oom victim which</span>
<span class="quote">&gt; shares mm with the vhost kernel thread because it could see a zero</span>
<span class="quote">&gt; page unexpectedly and theoretically make an incorrect decision visible</span>
<span class="quote">&gt; outside of the killed task context.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Make sure that each place which can read from userspace is annotated</span>
<span class="quote">&gt; properly and it uses copy_from_user_mm, __get_user_mm resp.</span>
<span class="quote">&gt; copy_from_iter_mm. Each will get the target mm as an argument and it</span>
<span class="quote">&gt; performs a pessimistic check to rule out that the oom_reaper could</span>
<span class="quote">&gt; possibly unmap the particular page. __oom_reap_task then just needs to</span>
<span class="quote">&gt; mark the mm as unstable before it unmaps any page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is a preparatory patch without any functional changes because</span>
<span class="quote">&gt; the oom reaper doesn&#39;t touch mm shared with kthreads yet.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Will review. Answer to question below:
<span class="quote">
&gt; ---</span>
<span class="quote">&gt; Hi Michael,</span>
<span class="quote">&gt; we have discussed [1] that vhost_worker pins the mm of a potential</span>
<span class="quote">&gt; oom victim for too long which result into an OOM storm when other</span>
<span class="quote">&gt; processes have to be killed. One way to address this issue would</span>
<span class="quote">&gt; be to pin mm_count rather than mm_users and revalidate it before</span>
<span class="quote">&gt; actually doing the copy (mmget_not_zero). You had concerns about</span>
<span class="quote">&gt; more atomic operations in the data path. Another way would be to</span>
<span class="quote">&gt; postpone exit_mm_victim to after exit_task_work but as it turned</span>
<span class="quote">&gt; out other task might have the device open and pin the mm indirectly</span>
<span class="quote">&gt; [2].</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now I would like to attack the issue from another side which would</span>
<span class="quote">&gt; be more generic. I would like to make mm&#39;s which are shared with</span>
<span class="quote">&gt; kthreads oom reapable in general. This is currently not allowed</span>
<span class="quote">&gt; because we do not want to risk that a kthread would see an already</span>
<span class="quote">&gt; unmapped page - aka see a newly allocated zero page. At the same</span>
<span class="quote">&gt; time this is really desirable because it helps to guarantee a forward</span>
<span class="quote">&gt; progress on the OOM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It seems that vhost usage would suffer from this problem because</span>
<span class="quote">&gt; it reads from the userspace to get (status) flags and makes some</span>
<span class="quote">&gt; decisions based on the read value.  I do not understand the code so I</span>
<span class="quote">&gt; couldn&#39;t evaluate whether that would lead to some real problems so I</span>
<span class="quote">&gt; conservatively assumed it wouldn&#39;t handle that gracefully.</span>

Getting an error from __get_user and friends is handled gracefully.
Getting zero instead of a real value will cause userspace
memory corruption.
<span class="quote">
&gt; If this is</span>
<span class="quote">&gt; incorrect and all the paths can just cope with seeing zeros unexpectedly</span>
<span class="quote">&gt; then great and I will drop the patch and move over to the oom specific</span>
<span class="quote">&gt; further steps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Therefore I am proposing a kthread safe API which allows to read from</span>
<span class="quote">&gt; userspace and also makes sure to do a proper exclusion with the oom</span>
<span class="quote">&gt; reaper. A race would be reported by EFAULT which is already handled.</span>
<span class="quote">&gt; Performance wise it would add two tests to the copy from user</span>
<span class="quote">&gt; paths. Does the following change makes sense to you and would be</span>
<span class="quote">&gt; acceptable? If yes I will follow up with another patch which will allow</span>
<span class="quote">&gt; oom reaper for mm shared with kthread.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] http://lkml.kernel.org/r/1456765329-14890-1-git-send-email-vdavydov@virtuozzo.com</span>
<span class="quote">&gt; [2] http://lkml.kernel.org/r/20160301181136-mutt-send-email-mst@redhat.com</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  drivers/vhost/scsi.c    |  2 +-</span>
<span class="quote">&gt;  drivers/vhost/vhost.c   | 18 +++++++++---------</span>
<span class="quote">&gt;  include/linux/sched.h   |  1 +</span>
<span class="quote">&gt;  include/linux/uaccess.h | 22 ++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/uio.h     | 10 ++++++++++</span>
<span class="quote">&gt;  mm/oom_kill.c           |  6 ++++++</span>
<span class="quote">&gt;  6 files changed, 49 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c</span>
<span class="quote">&gt; index 0e6fd556c982..2c8dc0b9a21f 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/scsi.c</span>
<span class="quote">&gt; +++ b/drivers/vhost/scsi.c</span>
<span class="quote">&gt; @@ -932,7 +932,7 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		iov_iter_init(&amp;out_iter, WRITE, vq-&gt;iov, out, out_size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		ret = copy_from_iter(req, req_size, &amp;out_iter);</span>
<span class="quote">&gt; +		ret = copy_from_iter_mm(vq-&gt;dev-&gt;mm, req, req_size, &amp;out_iter);</span>
<span class="quote">&gt;  		if (unlikely(ret != req_size)) {</span>
<span class="quote">&gt;  			vq_err(vq, &quot;Faulted on copy_from_iter\n&quot;);</span>
<span class="quote">&gt;  			vhost_scsi_send_bad_target(vs, vq, head, out);</span>
<span class="quote">&gt; diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; index 669fef1e2bb6..14959ba43cb4 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/vhost.c</span>
<span class="quote">&gt; +++ b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; @@ -1212,7 +1212,7 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  		r = -EFAULT;</span>
<span class="quote">&gt;  		goto err;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	r = __get_user(last_used_idx, &amp;vq-&gt;used-&gt;idx);</span>
<span class="quote">&gt; +	r = __get_user_mm(vq-&gt;dev-&gt;mm, last_used_idx, &amp;vq-&gt;used-&gt;idx);</span>
<span class="quote">&gt;  	if (r)</span>
<span class="quote">&gt;  		goto err;</span>
<span class="quote">&gt;  	vq-&gt;last_used_idx = vhost16_to_cpu(vq, last_used_idx);</span>
<span class="quote">&gt; @@ -1328,7 +1328,7 @@ static int get_indirect(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  			       i, count);</span>
<span class="quote">&gt;  			return -EINVAL;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		if (unlikely(copy_from_iter(&amp;desc, sizeof(desc), &amp;from) !=</span>
<span class="quote">&gt; +		if (unlikely(copy_from_iter_mm(vq-&gt;dev-&gt;mm, &amp;desc, sizeof(desc), &amp;from) !=</span>
<span class="quote">&gt;  			     sizeof(desc))) {</span>
<span class="quote">&gt;  			vq_err(vq, &quot;Failed indirect descriptor: idx %d, %zx\n&quot;,</span>
<span class="quote">&gt;  			       i, (size_t)vhost64_to_cpu(vq, indirect-&gt;addr) + i * sizeof desc);</span>
<span class="quote">&gt; @@ -1392,7 +1392,7 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Check it isn&#39;t doing very strange things with descriptor numbers. */</span>
<span class="quote">&gt;  	last_avail_idx = vq-&gt;last_avail_idx;</span>
<span class="quote">&gt; -	if (unlikely(__get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx))) {</span>
<span class="quote">&gt; +	if (unlikely(__get_user_mm(vq-&gt;dev-&gt;mm, avail_idx, &amp;vq-&gt;avail-&gt;idx))) {</span>
<span class="quote">&gt;  		vq_err(vq, &quot;Failed to access avail idx at %p\n&quot;,</span>
<span class="quote">&gt;  		       &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; @@ -1414,7 +1414,7 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Grab the next descriptor number they&#39;re advertising, and increment</span>
<span class="quote">&gt;  	 * the index we&#39;ve seen. */</span>
<span class="quote">&gt; -	if (unlikely(__get_user(ring_head,</span>
<span class="quote">&gt; +	if (unlikely(__get_user_mm(vq-&gt;dev-&gt;mm, ring_head,</span>
<span class="quote">&gt;  				&amp;vq-&gt;avail-&gt;ring[last_avail_idx &amp; (vq-&gt;num - 1)]))) {</span>
<span class="quote">&gt;  		vq_err(vq, &quot;Failed to read head: idx %d address %p\n&quot;,</span>
<span class="quote">&gt;  		       last_avail_idx,</span>
<span class="quote">&gt; @@ -1450,7 +1450,7 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  			       i, vq-&gt;num, head);</span>
<span class="quote">&gt;  			return -EINVAL;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		ret = __copy_from_user(&amp;desc, vq-&gt;desc + i, sizeof desc);</span>
<span class="quote">&gt; +		ret = __copy_from_user_mm(vq-&gt;dev-&gt;mm, &amp;desc, vq-&gt;desc + i, sizeof desc);</span>
<span class="quote">&gt;  		if (unlikely(ret)) {</span>
<span class="quote">&gt;  			vq_err(vq, &quot;Failed to get descriptor: idx %d addr %p\n&quot;,</span>
<span class="quote">&gt;  			       i, vq-&gt;desc + i);</span>
<span class="quote">&gt; @@ -1622,7 +1622,7 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {</span>
<span class="quote">&gt;  		__virtio16 flags;</span>
<span class="quote">&gt; -		if (__get_user(flags, &amp;vq-&gt;avail-&gt;flags)) {</span>
<span class="quote">&gt; +		if (__get_user_mm(dev-&gt;mm, flags, &amp;vq-&gt;avail-&gt;flags)) {</span>
<span class="quote">&gt;  			vq_err(vq, &quot;Failed to get flags&quot;);</span>
<span class="quote">&gt;  			return true;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -1636,7 +1636,7 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  	if (unlikely(!v))</span>
<span class="quote">&gt;  		return true;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (__get_user(event, vhost_used_event(vq))) {</span>
<span class="quote">&gt; +	if (__get_user_mm(dev-&gt;mm, event, vhost_used_event(vq))) {</span>
<span class="quote">&gt;  		vq_err(vq, &quot;Failed to get used event idx&quot;);</span>
<span class="quote">&gt;  		return true;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1678,7 +1678,7 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  	__virtio16 avail_idx;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	r = __get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt; +	r = __get_user_mm(dev-&gt;mm, avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt;  	if (r)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1713,7 +1713,7 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  	/* They could have slipped one in as we were doing that: make</span>
<span class="quote">&gt;  	 * sure it&#39;s written, then check again. */</span>
<span class="quote">&gt;  	smp_mb();</span>
<span class="quote">&gt; -	r = __get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt; +	r = __get_user_mm(dev-&gt;mm,avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>

space after , pls
<span class="quote">
&gt;  	if (r) {</span>
<span class="quote">&gt;  		vq_err(vq, &quot;Failed to check avail idx at %p: %d\n&quot;,</span>
<span class="quote">&gt;  		       &amp;vq-&gt;avail-&gt;idx, r);</span>
<span class="quote">&gt; diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="quote">&gt; index 6d81a1eb974a..2b00ac7faa18 100644</span>
<span class="quote">&gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; @@ -513,6 +513,7 @@ static inline int get_dumpable(struct mm_struct *mm)</span>
<span class="quote">&gt;  #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */</span>
<span class="quote">&gt;  #define MMF_OOM_REAPED		21	/* mm has been already reaped */</span>
<span class="quote">&gt;  #define MMF_OOM_NOT_REAPABLE	22	/* mm couldn&#39;t be reaped */</span>
<span class="quote">&gt; +#define MMF_UNSTABLE		23	/* mm is unstable for copy_from_user */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h</span>
<span class="quote">&gt; index 349557825428..b1f314fca3c8 100644</span>
<span class="quote">&gt; --- a/include/linux/uaccess.h</span>
<span class="quote">&gt; +++ b/include/linux/uaccess.h</span>
<span class="quote">&gt; @@ -76,6 +76,28 @@ static inline unsigned long __copy_from_user_nocache(void *to,</span>
<span class="quote">&gt;  #endif		/* ARCH_HAS_NOCACHE_UACCESS */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * A safe variant of __get_user for for use_mm() users to have a</span>
<span class="quote">&gt; + * gurantee that the address space wasn&#39;t reaped in the background</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __get_user_mm(mm, x, ptr)				\</span>
<span class="quote">&gt; +({								\</span>
<span class="quote">&gt; +	int ___gu_err = __get_user(x, ptr);			\</span>
<span class="quote">&gt; +	if (!___gu_err &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))	\</span>

test_bit is somewhat expensive. See my old mail
	x86/bitops: implement __test_bit

I dropped it as virtio just switched to simple &amp;/| for features,
but we might need something like this now.
<span class="quote">


&gt; +		___gu_err = -EFAULT;				\</span>
<span class="quote">&gt; +	___gu_err;						\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* similar to __get_user_mm */</span>
<span class="quote">&gt; +static inline __must_check long __copy_from_user_mm(struct mm_struct *mm,</span>
<span class="quote">&gt; +		void *to, const void __user * from, unsigned long n)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long ret = __copy_from_user(to, from, n);</span>
<span class="quote">&gt; +	if (!ret &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * probe_kernel_read(): safely attempt to read from a location</span>
<span class="quote">&gt;   * @dst: pointer to the buffer that shall take the data</span>
<span class="quote">&gt;   * @src: address to read from</span>
<span class="quote">&gt; diff --git a/include/linux/uio.h b/include/linux/uio.h</span>
<span class="quote">&gt; index 1b5d1cd796e2..4be6b24003d8 100644</span>
<span class="quote">&gt; --- a/include/linux/uio.h</span>
<span class="quote">&gt; +++ b/include/linux/uio.h</span>
<span class="quote">&gt; @@ -9,6 +9,7 @@</span>
<span class="quote">&gt;  #ifndef __LINUX_UIO_H</span>
<span class="quote">&gt;  #define __LINUX_UIO_H</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt;  #include &lt;uapi/linux/uio.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -84,6 +85,15 @@ size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,</span>
<span class="quote">&gt;  			 struct iov_iter *i);</span>
<span class="quote">&gt;  size_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i);</span>
<span class="quote">&gt;  size_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline size_t copy_from_iter_mm(struct mm_struct *mm, void *addr,</span>
<span class="quote">&gt; +		size_t bytes, struct iov_iter *i)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	size_t ret = copy_from_iter(addr, bytes, i);</span>
<span class="quote">&gt; +	if (!IS_ERR_VALUE(ret) &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  size_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i);</span>
<span class="quote">&gt;  size_t iov_iter_zero(size_t bytes, struct iov_iter *);</span>
<span class="quote">&gt;  unsigned long iov_iter_alignment(const struct iov_iter *i);</span>
<span class="quote">&gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; index 6303bc7caeda..3fa43e96a59b 100644</span>
<span class="quote">&gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; @@ -506,6 +506,12 @@ static bool __oom_reap_task(struct task_struct *tsk)</span>
<span class="quote">&gt;  		goto mm_drop;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Tell all users of get_user_mm/copy_from_user_mm that the content</span>
<span class="quote">&gt; +	 * is no longer stable.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);</span>
<span class="quote">&gt; +</span>

do we need some kind of barrier after this?

and if yes - does flag read need a barrier before it too?
<span class="quote">
&gt;  	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt;  	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt;  		if (is_vm_hugetlb_page(vma))</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.8.1</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 19, 2016, 9:35 p.m.</div>
<pre class="content">
On Sat 18-06-16 03:09:02, Michael S. Tsirkin wrote:
<span class="quote">&gt; On Fri, Jun 17, 2016 at 11:00:17AM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; It seems that vhost usage would suffer from this problem because</span>
<span class="quote">&gt; &gt; it reads from the userspace to get (status) flags and makes some</span>
<span class="quote">&gt; &gt; decisions based on the read value.  I do not understand the code so I</span>
<span class="quote">&gt; &gt; couldn&#39;t evaluate whether that would lead to some real problems so I</span>
<span class="quote">&gt; &gt; conservatively assumed it wouldn&#39;t handle that gracefully.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Getting an error from __get_user and friends is handled gracefully.</span>
<span class="quote">&gt; Getting zero instead of a real value will cause userspace</span>
<span class="quote">&gt; memory corruption.</span>

OK, thanks for the confirmation! I will add this to the changelog. I
assume that the memory corruption could &quot;leak out&quot; of the mm we just
read from, right? I am asking because the mm and all its users will die
by SIGKILL so they will not &quot;see&quot; the corruption. I am not familiar with the
vhost transfer model but I guess it wouldn&#39;t be uncommon if the target
memory could be a shared object (e.g. tmpfs or a regular file) so it
would outlive the mm.

[...]
<span class="quote">
&gt; &gt; @@ -1713,7 +1713,7 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
<span class="quote">&gt; &gt;  	/* They could have slipped one in as we were doing that: make</span>
<span class="quote">&gt; &gt;  	 * sure it&#39;s written, then check again. */</span>
<span class="quote">&gt; &gt;  	smp_mb();</span>
<span class="quote">&gt; &gt; -	r = __get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt; &gt; +	r = __get_user_mm(dev-&gt;mm,avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; space after , pls</span>

sure
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  	if (r) {</span>
<span class="quote">&gt; &gt;  		vq_err(vq, &quot;Failed to check avail idx at %p: %d\n&quot;,</span>
<span class="quote">&gt; &gt;  		       &amp;vq-&gt;avail-&gt;idx, r);</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; index 6d81a1eb974a..2b00ac7faa18 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; @@ -513,6 +513,7 @@ static inline int get_dumpable(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt;  #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */</span>
<span class="quote">&gt; &gt;  #define MMF_OOM_REAPED		21	/* mm has been already reaped */</span>
<span class="quote">&gt; &gt;  #define MMF_OOM_NOT_REAPABLE	22	/* mm couldn&#39;t be reaped */</span>
<span class="quote">&gt; &gt; +#define MMF_UNSTABLE		23	/* mm is unstable for copy_from_user */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; index 349557825428..b1f314fca3c8 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; @@ -76,6 +76,28 @@ static inline unsigned long __copy_from_user_nocache(void *to,</span>
<span class="quote">&gt; &gt;  #endif		/* ARCH_HAS_NOCACHE_UACCESS */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt; + * A safe variant of __get_user for for use_mm() users to have a</span>
<span class="quote">&gt; &gt; + * gurantee that the address space wasn&#39;t reaped in the background</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#define __get_user_mm(mm, x, ptr)				\</span>
<span class="quote">&gt; &gt; +({								\</span>
<span class="quote">&gt; &gt; +	int ___gu_err = __get_user(x, ptr);			\</span>
<span class="quote">&gt; &gt; +	if (!___gu_err &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))	\</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; test_bit is somewhat expensive. See my old mail</span>
<span class="quote">&gt; 	x86/bitops: implement __test_bit</span>

Do you have a msg_id?
<span class="quote">
&gt; I dropped it as virtio just switched to simple &amp;/| for features,</span>
<span class="quote">&gt; but we might need something like this now.</span>

Is this such a hot path that something like this would make a visible
difference? 
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +		___gu_err = -EFAULT;				\</span>
<span class="quote">&gt; &gt; +	___gu_err;						\</span>
<span class="quote">&gt; &gt; +})</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* similar to __get_user_mm */</span>
<span class="quote">&gt; &gt; +static inline __must_check long __copy_from_user_mm(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt; +		void *to, const void __user * from, unsigned long n)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	long ret = __copy_from_user(to, from, n);</span>
<span class="quote">&gt; &gt; +	if (!ret &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))</span>
<span class="quote">&gt; &gt; +		return -EFAULT;</span>
<span class="quote">&gt; &gt; +	return ret;</span>

And I&#39;ve just noticed that this is not correct. We need 
	if ((ret &gt;= 0) &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))

[...]
<span class="quote">
&gt; &gt; diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; index 6303bc7caeda..3fa43e96a59b 100644</span>
<span class="quote">&gt; &gt; --- a/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; +++ b/mm/oom_kill.c</span>
<span class="quote">&gt; &gt; @@ -506,6 +506,12 @@ static bool __oom_reap_task(struct task_struct *tsk)</span>
<span class="quote">&gt; &gt;  		goto mm_drop;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Tell all users of get_user_mm/copy_from_user_mm that the content</span>
<span class="quote">&gt; &gt; +	 * is no longer stable.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; do we need some kind of barrier after this?</span>

Well I believe we don&#39;t because unmapping the memory will likely
imply memory barriers on the way.
<span class="quote">
&gt; </span>
<span class="quote">&gt; and if yes - does flag read need a barrier before it too?</span>

A good question. I was basically assuming the same as above. If we didn&#39;t fault
then the oom reaper wouldn&#39;t touch that memory and so we are safe even when
we see the outdated mm flags, if the memory was reaped then we have to page
fault and that should imply memory barrier AFAIU.

Does that make sense?
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  	tlb_gather_mmu(&amp;tlb, mm, 0, -1);</span>
<span class="quote">&gt; &gt;  	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt; &gt;  		if (is_vm_hugetlb_page(vma))</span>
<span class="quote">&gt; &gt; -- </span>
<span class="quote">&gt; &gt; 2.8.1</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 19, 2016, 9:52 p.m.</div>
<pre class="content">
On Sun 19-06-16 23:35:43, Michal Hocko wrote:
<span class="quote">&gt; On Sat 18-06-16 03:09:02, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt; &gt; On Fri, Jun 17, 2016 at 11:00:17AM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; &gt;  /*</span>
<span class="quote">&gt; &gt; &gt; + * A safe variant of __get_user for for use_mm() users to have a</span>
<span class="quote">&gt; &gt; &gt; + * gurantee that the address space wasn&#39;t reaped in the background</span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; +#define __get_user_mm(mm, x, ptr)				\</span>
<span class="quote">&gt; &gt; &gt; +({								\</span>
<span class="quote">&gt; &gt; &gt; +	int ___gu_err = __get_user(x, ptr);			\</span>
<span class="quote">&gt; &gt; &gt; +	if (!___gu_err &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))	\</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; test_bit is somewhat expensive. See my old mail</span>
<span class="quote">&gt; &gt; 	x86/bitops: implement __test_bit</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you have a msg_id?</span>

Found it
http://lkml.kernel.org/r/1440776707-22016-1-git-send-email-mst@redhat.com
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 20, 2016, 9:35 a.m.</div>
<pre class="content">
On Sun 19-06-16 23:35:43, Michal Hocko wrote:
<span class="quote">&gt; On Sat 18-06-16 03:09:02, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt; &gt; On Fri, Jun 17, 2016 at 11:00:17AM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; &gt; diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; &gt; index 349557825428..b1f314fca3c8 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/uaccess.h</span>
<span class="quote">&gt; &gt; &gt; @@ -76,6 +76,28 @@ static inline unsigned long __copy_from_user_nocache(void *to,</span>
<span class="quote">&gt; &gt; &gt;  #endif		/* ARCH_HAS_NOCACHE_UACCESS */</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt;  /*</span>
<span class="quote">&gt; &gt; &gt; + * A safe variant of __get_user for for use_mm() users to have a</span>
<span class="quote">&gt; &gt; &gt; + * gurantee that the address space wasn&#39;t reaped in the background</span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; +#define __get_user_mm(mm, x, ptr)				\</span>
<span class="quote">&gt; &gt; &gt; +({								\</span>
<span class="quote">&gt; &gt; &gt; +	int ___gu_err = __get_user(x, ptr);			\</span>
<span class="quote">&gt; &gt; &gt; +	if (!___gu_err &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))	\</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; test_bit is somewhat expensive. See my old mail</span>
<span class="quote">&gt; &gt; 	x86/bitops: implement __test_bit</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you have a msg_id?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I dropped it as virtio just switched to simple &amp;/| for features,</span>
<span class="quote">&gt; &gt; but we might need something like this now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this such a hot path that something like this would make a visible</span>
<span class="quote">&gt; difference? </span>

OK, so I&#39;ve tried to apply your patch [1] and updated both __get_user_mm
and __copy_from_user_mm and the result is a code size reduction:
   text    data     bss     dec     hex filename
  12835       2      32   12869    3245 drivers/vhost/vhost.o
  12882       2      32   12916    3274 drivers/vhost/vhost.o.before

This is really tiny and I cannot tell anything about the performance. Should
I resurrect your patch and push it together with this change or this can happen
later?

[1] http://lkml.kernel.org/r/1440776707-22016-1-git-send-email-mst@redhat.com
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c</span>
<span class="p_header">index 0e6fd556c982..2c8dc0b9a21f 100644</span>
<span class="p_header">--- a/drivers/vhost/scsi.c</span>
<span class="p_header">+++ b/drivers/vhost/scsi.c</span>
<span class="p_chunk">@@ -932,7 +932,7 @@</span> <span class="p_context"> vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)</span>
 		 */
 		iov_iter_init(&amp;out_iter, WRITE, vq-&gt;iov, out, out_size);
 
<span class="p_del">-		ret = copy_from_iter(req, req_size, &amp;out_iter);</span>
<span class="p_add">+		ret = copy_from_iter_mm(vq-&gt;dev-&gt;mm, req, req_size, &amp;out_iter);</span>
 		if (unlikely(ret != req_size)) {
 			vq_err(vq, &quot;Faulted on copy_from_iter\n&quot;);
 			vhost_scsi_send_bad_target(vs, vq, head, out);
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index 669fef1e2bb6..14959ba43cb4 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -1212,7 +1212,7 @@</span> <span class="p_context"> int vhost_vq_init_access(struct vhost_virtqueue *vq)</span>
 		r = -EFAULT;
 		goto err;
 	}
<span class="p_del">-	r = __get_user(last_used_idx, &amp;vq-&gt;used-&gt;idx);</span>
<span class="p_add">+	r = __get_user_mm(vq-&gt;dev-&gt;mm, last_used_idx, &amp;vq-&gt;used-&gt;idx);</span>
 	if (r)
 		goto err;
 	vq-&gt;last_used_idx = vhost16_to_cpu(vq, last_used_idx);
<span class="p_chunk">@@ -1328,7 +1328,7 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 			       i, count);
 			return -EINVAL;
 		}
<span class="p_del">-		if (unlikely(copy_from_iter(&amp;desc, sizeof(desc), &amp;from) !=</span>
<span class="p_add">+		if (unlikely(copy_from_iter_mm(vq-&gt;dev-&gt;mm, &amp;desc, sizeof(desc), &amp;from) !=</span>
 			     sizeof(desc))) {
 			vq_err(vq, &quot;Failed indirect descriptor: idx %d, %zx\n&quot;,
 			       i, (size_t)vhost64_to_cpu(vq, indirect-&gt;addr) + i * sizeof desc);
<span class="p_chunk">@@ -1392,7 +1392,7 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 
 	/* Check it isn&#39;t doing very strange things with descriptor numbers. */
 	last_avail_idx = vq-&gt;last_avail_idx;
<span class="p_del">-	if (unlikely(__get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx))) {</span>
<span class="p_add">+	if (unlikely(__get_user_mm(vq-&gt;dev-&gt;mm, avail_idx, &amp;vq-&gt;avail-&gt;idx))) {</span>
 		vq_err(vq, &quot;Failed to access avail idx at %p\n&quot;,
 		       &amp;vq-&gt;avail-&gt;idx);
 		return -EFAULT;
<span class="p_chunk">@@ -1414,7 +1414,7 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 
 	/* Grab the next descriptor number they&#39;re advertising, and increment
 	 * the index we&#39;ve seen. */
<span class="p_del">-	if (unlikely(__get_user(ring_head,</span>
<span class="p_add">+	if (unlikely(__get_user_mm(vq-&gt;dev-&gt;mm, ring_head,</span>
 				&amp;vq-&gt;avail-&gt;ring[last_avail_idx &amp; (vq-&gt;num - 1)]))) {
 		vq_err(vq, &quot;Failed to read head: idx %d address %p\n&quot;,
 		       last_avail_idx,
<span class="p_chunk">@@ -1450,7 +1450,7 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 			       i, vq-&gt;num, head);
 			return -EINVAL;
 		}
<span class="p_del">-		ret = __copy_from_user(&amp;desc, vq-&gt;desc + i, sizeof desc);</span>
<span class="p_add">+		ret = __copy_from_user_mm(vq-&gt;dev-&gt;mm, &amp;desc, vq-&gt;desc + i, sizeof desc);</span>
 		if (unlikely(ret)) {
 			vq_err(vq, &quot;Failed to get descriptor: idx %d addr %p\n&quot;,
 			       i, vq-&gt;desc + i);
<span class="p_chunk">@@ -1622,7 +1622,7 @@</span> <span class="p_context"> static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
 
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
 		__virtio16 flags;
<span class="p_del">-		if (__get_user(flags, &amp;vq-&gt;avail-&gt;flags)) {</span>
<span class="p_add">+		if (__get_user_mm(dev-&gt;mm, flags, &amp;vq-&gt;avail-&gt;flags)) {</span>
 			vq_err(vq, &quot;Failed to get flags&quot;);
 			return true;
 		}
<span class="p_chunk">@@ -1636,7 +1636,7 @@</span> <span class="p_context"> static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
 	if (unlikely(!v))
 		return true;
 
<span class="p_del">-	if (__get_user(event, vhost_used_event(vq))) {</span>
<span class="p_add">+	if (__get_user_mm(dev-&gt;mm, event, vhost_used_event(vq))) {</span>
 		vq_err(vq, &quot;Failed to get used event idx&quot;);
 		return true;
 	}
<span class="p_chunk">@@ -1678,7 +1678,7 @@</span> <span class="p_context"> bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
 	__virtio16 avail_idx;
 	int r;
 
<span class="p_del">-	r = __get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="p_add">+	r = __get_user_mm(dev-&gt;mm, avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
 	if (r)
 		return false;
 
<span class="p_chunk">@@ -1713,7 +1713,7 @@</span> <span class="p_context"> bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
 	/* They could have slipped one in as we were doing that: make
 	 * sure it&#39;s written, then check again. */
 	smp_mb();
<span class="p_del">-	r = __get_user(avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
<span class="p_add">+	r = __get_user_mm(dev-&gt;mm,avail_idx, &amp;vq-&gt;avail-&gt;idx);</span>
 	if (r) {
 		vq_err(vq, &quot;Failed to check avail idx at %p: %d\n&quot;,
 		       &amp;vq-&gt;avail-&gt;idx, r);
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 6d81a1eb974a..2b00ac7faa18 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -513,6 +513,7 @@</span> <span class="p_context"> static inline int get_dumpable(struct mm_struct *mm)</span>
 #define MMF_RECALC_UPROBES	20	/* MMF_HAS_UPROBES can be wrong */
 #define MMF_OOM_REAPED		21	/* mm has been already reaped */
 #define MMF_OOM_NOT_REAPABLE	22	/* mm couldn&#39;t be reaped */
<span class="p_add">+#define MMF_UNSTABLE		23	/* mm is unstable for copy_from_user */</span>
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)
 
<span class="p_header">diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h</span>
<span class="p_header">index 349557825428..b1f314fca3c8 100644</span>
<span class="p_header">--- a/include/linux/uaccess.h</span>
<span class="p_header">+++ b/include/linux/uaccess.h</span>
<span class="p_chunk">@@ -76,6 +76,28 @@</span> <span class="p_context"> static inline unsigned long __copy_from_user_nocache(void *to,</span>
 #endif		/* ARCH_HAS_NOCACHE_UACCESS */
 
 /*
<span class="p_add">+ * A safe variant of __get_user for for use_mm() users to have a</span>
<span class="p_add">+ * gurantee that the address space wasn&#39;t reaped in the background</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __get_user_mm(mm, x, ptr)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	int ___gu_err = __get_user(x, ptr);			\</span>
<span class="p_add">+	if (!___gu_err &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))	\</span>
<span class="p_add">+		___gu_err = -EFAULT;				\</span>
<span class="p_add">+	___gu_err;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/* similar to __get_user_mm */</span>
<span class="p_add">+static inline __must_check long __copy_from_user_mm(struct mm_struct *mm,</span>
<span class="p_add">+		void *to, const void __user * from, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long ret = __copy_from_user(to, from, n);</span>
<span class="p_add">+	if (!ret &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * probe_kernel_read(): safely attempt to read from a location
  * @dst: pointer to the buffer that shall take the data
  * @src: address to read from
<span class="p_header">diff --git a/include/linux/uio.h b/include/linux/uio.h</span>
<span class="p_header">index 1b5d1cd796e2..4be6b24003d8 100644</span>
<span class="p_header">--- a/include/linux/uio.h</span>
<span class="p_header">+++ b/include/linux/uio.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
 #ifndef __LINUX_UIO_H
 #define __LINUX_UIO_H
 
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;linux/kernel.h&gt;
 #include &lt;uapi/linux/uio.h&gt;
 
<span class="p_chunk">@@ -84,6 +85,15 @@</span> <span class="p_context"> size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,</span>
 			 struct iov_iter *i);
 size_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i);
 size_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i);
<span class="p_add">+</span>
<span class="p_add">+static inline size_t copy_from_iter_mm(struct mm_struct *mm, void *addr,</span>
<span class="p_add">+		size_t bytes, struct iov_iter *i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t ret = copy_from_iter(addr, bytes, i);</span>
<span class="p_add">+	if (!IS_ERR_VALUE(ret) &amp;&amp; test_bit(MMF_UNSTABLE, &amp;mm-&gt;flags))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
 size_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i);
 size_t iov_iter_zero(size_t bytes, struct iov_iter *);
 unsigned long iov_iter_alignment(const struct iov_iter *i);
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 6303bc7caeda..3fa43e96a59b 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -506,6 +506,12 @@</span> <span class="p_context"> static bool __oom_reap_task(struct task_struct *tsk)</span>
 		goto mm_drop;
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Tell all users of get_user_mm/copy_from_user_mm that the content</span>
<span class="p_add">+	 * is no longer stable.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);</span>
<span class="p_add">+</span>
 	tlb_gather_mmu(&amp;tlb, mm, 0, -1);
 	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {
 		if (is_vm_hugetlb_page(vma))

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



