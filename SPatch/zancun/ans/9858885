
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm, oom: allow oom reaper to race with exit_mmap - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm, oom: allow oom reaper to race with exit_mmap</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 24, 2017, 7:23 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170724072332.31903-1-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9858885/mbox/"
   >mbox</a>
|
   <a href="/patch/9858885/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9858885/">/patch/9858885/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1835660349 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Jul 2017 07:23:52 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0757A2832B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Jul 2017 07:23:52 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F039828562; Mon, 24 Jul 2017 07:23:51 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 49E462832B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Jul 2017 07:23:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753004AbdGXHXs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 24 Jul 2017 03:23:48 -0400
Received: from mail-wr0-f195.google.com ([209.85.128.195]:33320 &quot;EHLO
	mail-wr0-f195.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751873AbdGXHXm (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 24 Jul 2017 03:23:42 -0400
Received: by mail-wr0-f195.google.com with SMTP id y43so16718323wrd.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 24 Jul 2017 00:23:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id;
	bh=kBakNKP77ZhjHQa/Gct0/24tKjbJdh5v9TsH2XAAcu4=;
	b=laQuUmM96pTRkWMB9i/bqcFdoadB+BsHEQL6OI2aq/NM4rhxmYxyEe72B4eRNESTvS
	rrMrH/6N5t9w/AuG69wZfbUBHyHG/XsJwsm5XrZuyQM2lx7iOOrTbxM9iSwbbXBc3xvD
	BqO5ycDcmYn86/va5z2XiJLY+A7yZOySYP/5sCLu0kYNVCdyzPFqPhEeqBhc3idz1UKA
	Jb4SpnaOMPIaQ5OWsqiZF04X5IX49x36kRC7QVswEEGdMduJ8O0piIjb79umW8J8fik6
	MS+bfa0x0JrJckag1DXUYDgTkeUtbzj/He6eItvJDXD4OnYQNU0p9rRqggezEZM4biI4
	6+9A==
X-Gm-Message-State: AIVw113N7rVhqdyWXTGxDmc2nsySMMzSOzF/g2W+Wz1iArBBNOZrGz2m
	M3VbOl4hvFuxeQ==
X-Received: by 10.223.136.78 with SMTP id e14mr14546841wre.84.1500881020660; 
	Mon, 24 Jul 2017 00:23:40 -0700 (PDT)
Received: from tiehlicka.suse.cz (bband-dyn36.95-103-97.t-com.sk.
	[95.103.97.36]) by smtp.gmail.com with ESMTPSA id
	v8sm13613424wrd.28.2017.07.24.00.23.38
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 24 Jul 2017 00:23:39 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;,
	Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH] mm, oom: allow oom reaper to race with exit_mmap
Date: Mon, 24 Jul 2017 09:23:32 +0200
Message-Id: &lt;20170724072332.31903-1-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.13.2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 24, 2017, 7:23 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

David has noticed that the oom killer might kill additional tasks while
the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks
the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down
to 0. The race is as follows

oom_reap_task				do_exit
					  exit_mm
  __oom_reap_task_mm
					    mmput
					      __mmput
    mmget_not_zero # fails
    						exit_mmap # frees memory
  set_bit(MMF_OOM_SKIP)

The victim is still visible to the OOM killer until it is unhashed.

Currently we try to reduce a risk of this race by taking oom_lock
and wait for out_of_memory sleep while holding the lock to give the
victim some time to exit. This is quite suboptimal approach because
there is no guarantee the victim (especially a large one) will manage
to unmap its address space and free enough memory to the particular oom
domain which needs a memory (e.g. a specific NUMA node).

Fix this problem by allowing __oom_reap_task_mm and __mmput path to
race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed
to run in parallel with other unmappers (hence the mmap_sem for read).

The only tricky part is to exclude page tables tear down and all
operations which modify the address space in the __mmput path. exit_mmap
doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing
really forbids us to use mmap_sem for write, though. In fact we are
already relying on this lock earlier in the __mmput path to synchronize
with ksm and khugepaged.

Take the exclusive mmap_sem when calling free_pgtables and destroying
vmas to sync with __oom_reap_task_mm which take the lock for read. All
other operations can safely race with the parallel unmap.

Changes
- bail on null mm-&gt;mmap early as per David Rientjes

Reported-by: David Rientjes &lt;rientjes@google.com&gt;
Fixes: 26db62f179d1 (&quot;oom: keep mm of the killed task available&quot;)
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
Hi,
I&#39;ve sent this as an RFC [1] previously and it seems that the original
issue has been resolved [2], although an explicit tested-by would be
appreciated of course. Hugh has pointed out [3] that using mmap_sem
in exit_mmap will allow to drop the tricky
	down_write(mmap_sem);
	up_write(mmap_sem);
in both paths. I hope I will get to that in a forseeable future.

I am not yet sure this is important enough to merge to stable trees,
I would rather wait for a report to show up.

[1] http://lkml.kernel.org/r/20170626130346.26314-1-mhocko@kernel.org
[2] http://lkml.kernel.org/r/alpine.DEB.2.10.1707111336250.60183@chino.kir.corp.google.com
[3] http://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils

 mm/mmap.c     |  7 +++++++
 mm/oom_kill.c | 45 +++++++--------------------------------------
 2 files changed, 14 insertions(+), 38 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 24, 2017, 2 p.m.</div>
<pre class="content">
On Mon, Jul 24, 2017 at 09:23:32AM +0200, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; David has noticed that the oom killer might kill additional tasks while</span>
<span class="quote">&gt; the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks</span>
<span class="quote">&gt; the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down</span>
<span class="quote">&gt; to 0. The race is as follows</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; oom_reap_task				do_exit</span>
<span class="quote">&gt; 					  exit_mm</span>
<span class="quote">&gt;   __oom_reap_task_mm</span>
<span class="quote">&gt; 					    mmput</span>
<span class="quote">&gt; 					      __mmput</span>
<span class="quote">&gt;     mmget_not_zero # fails</span>
<span class="quote">&gt;     						exit_mmap # frees memory</span>
<span class="quote">&gt;   set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The victim is still visible to the OOM killer until it is unhashed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently we try to reduce a risk of this race by taking oom_lock</span>
<span class="quote">&gt; and wait for out_of_memory sleep while holding the lock to give the</span>
<span class="quote">&gt; victim some time to exit. This is quite suboptimal approach because</span>
<span class="quote">&gt; there is no guarantee the victim (especially a large one) will manage</span>
<span class="quote">&gt; to unmap its address space and free enough memory to the particular oom</span>
<span class="quote">&gt; domain which needs a memory (e.g. a specific NUMA node).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fix this problem by allowing __oom_reap_task_mm and __mmput path to</span>
<span class="quote">&gt; race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed</span>
<span class="quote">&gt; to run in parallel with other unmappers (hence the mmap_sem for read).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The only tricky part is to exclude page tables tear down and all</span>
<span class="quote">&gt; operations which modify the address space in the __mmput path. exit_mmap</span>
<span class="quote">&gt; doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing</span>
<span class="quote">&gt; really forbids us to use mmap_sem for write, though. In fact we are</span>
<span class="quote">&gt; already relying on this lock earlier in the __mmput path to synchronize</span>
<span class="quote">&gt; with ksm and khugepaged.</span>

That&#39;s true, but we take mmap_sem there for small portion of cases.

It&#39;s quite different from taking the lock unconditionally. I&#39;m worry about
scalability implication of such move. On bigger machines it can be big
hit.

Should we do performance/scalability evaluation of the patch before
getting it applied?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 24, 2017, 2:15 p.m.</div>
<pre class="content">
On Mon 24-07-17 17:00:08, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Jul 24, 2017 at 09:23:32AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; David has noticed that the oom killer might kill additional tasks while</span>
<span class="quote">&gt; &gt; the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks</span>
<span class="quote">&gt; &gt; the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down</span>
<span class="quote">&gt; &gt; to 0. The race is as follows</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; oom_reap_task				do_exit</span>
<span class="quote">&gt; &gt; 					  exit_mm</span>
<span class="quote">&gt; &gt;   __oom_reap_task_mm</span>
<span class="quote">&gt; &gt; 					    mmput</span>
<span class="quote">&gt; &gt; 					      __mmput</span>
<span class="quote">&gt; &gt;     mmget_not_zero # fails</span>
<span class="quote">&gt; &gt;     						exit_mmap # frees memory</span>
<span class="quote">&gt; &gt;   set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The victim is still visible to the OOM killer until it is unhashed.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Currently we try to reduce a risk of this race by taking oom_lock</span>
<span class="quote">&gt; &gt; and wait for out_of_memory sleep while holding the lock to give the</span>
<span class="quote">&gt; &gt; victim some time to exit. This is quite suboptimal approach because</span>
<span class="quote">&gt; &gt; there is no guarantee the victim (especially a large one) will manage</span>
<span class="quote">&gt; &gt; to unmap its address space and free enough memory to the particular oom</span>
<span class="quote">&gt; &gt; domain which needs a memory (e.g. a specific NUMA node).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Fix this problem by allowing __oom_reap_task_mm and __mmput path to</span>
<span class="quote">&gt; &gt; race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed</span>
<span class="quote">&gt; &gt; to run in parallel with other unmappers (hence the mmap_sem for read).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The only tricky part is to exclude page tables tear down and all</span>
<span class="quote">&gt; &gt; operations which modify the address space in the __mmput path. exit_mmap</span>
<span class="quote">&gt; &gt; doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing</span>
<span class="quote">&gt; &gt; really forbids us to use mmap_sem for write, though. In fact we are</span>
<span class="quote">&gt; &gt; already relying on this lock earlier in the __mmput path to synchronize</span>
<span class="quote">&gt; &gt; with ksm and khugepaged.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s true, but we take mmap_sem there for small portion of cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s quite different from taking the lock unconditionally. I&#39;m worry about</span>
<span class="quote">&gt; scalability implication of such move. On bigger machines it can be big</span>
<span class="quote">&gt; hit.</span>

What kind of scalability implication you have in mind? There is
basically a zero contention on the mmap_sem that late in the exit path
so this should be pretty much a fast path of the down_write. I agree it
is not 0 cost but the cost of the address space freeing should basically
make it a noise.
<span class="quote">
&gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; getting it applied?</span>

What kind of test(s) would you be interested in?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 24, 2017, 2:51 p.m.</div>
<pre class="content">
On Mon, Jul 24, 2017 at 04:15:26PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Mon 24-07-17 17:00:08, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jul 24, 2017 at 09:23:32AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; David has noticed that the oom killer might kill additional tasks while</span>
<span class="quote">&gt; &gt; &gt; the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks</span>
<span class="quote">&gt; &gt; &gt; the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down</span>
<span class="quote">&gt; &gt; &gt; to 0. The race is as follows</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; oom_reap_task				do_exit</span>
<span class="quote">&gt; &gt; &gt; 					  exit_mm</span>
<span class="quote">&gt; &gt; &gt;   __oom_reap_task_mm</span>
<span class="quote">&gt; &gt; &gt; 					    mmput</span>
<span class="quote">&gt; &gt; &gt; 					      __mmput</span>
<span class="quote">&gt; &gt; &gt;     mmget_not_zero # fails</span>
<span class="quote">&gt; &gt; &gt;     						exit_mmap # frees memory</span>
<span class="quote">&gt; &gt; &gt;   set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The victim is still visible to the OOM killer until it is unhashed.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Currently we try to reduce a risk of this race by taking oom_lock</span>
<span class="quote">&gt; &gt; &gt; and wait for out_of_memory sleep while holding the lock to give the</span>
<span class="quote">&gt; &gt; &gt; victim some time to exit. This is quite suboptimal approach because</span>
<span class="quote">&gt; &gt; &gt; there is no guarantee the victim (especially a large one) will manage</span>
<span class="quote">&gt; &gt; &gt; to unmap its address space and free enough memory to the particular oom</span>
<span class="quote">&gt; &gt; &gt; domain which needs a memory (e.g. a specific NUMA node).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Fix this problem by allowing __oom_reap_task_mm and __mmput path to</span>
<span class="quote">&gt; &gt; &gt; race. __oom_reap_task_mm is basically MADV_DONTNEED and that is allowed</span>
<span class="quote">&gt; &gt; &gt; to run in parallel with other unmappers (hence the mmap_sem for read).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The only tricky part is to exclude page tables tear down and all</span>
<span class="quote">&gt; &gt; &gt; operations which modify the address space in the __mmput path. exit_mmap</span>
<span class="quote">&gt; &gt; &gt; doesn&#39;t expect any other users so it doesn&#39;t use any locking. Nothing</span>
<span class="quote">&gt; &gt; &gt; really forbids us to use mmap_sem for write, though. In fact we are</span>
<span class="quote">&gt; &gt; &gt; already relying on this lock earlier in the __mmput path to synchronize</span>
<span class="quote">&gt; &gt; &gt; with ksm and khugepaged.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s true, but we take mmap_sem there for small portion of cases.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s quite different from taking the lock unconditionally. I&#39;m worry about</span>
<span class="quote">&gt; &gt; scalability implication of such move. On bigger machines it can be big</span>
<span class="quote">&gt; &gt; hit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What kind of scalability implication you have in mind? There is</span>
<span class="quote">&gt; basically a zero contention on the mmap_sem that late in the exit path</span>
<span class="quote">&gt; so this should be pretty much a fast path of the down_write. I agree it</span>
<span class="quote">&gt; is not 0 cost but the cost of the address space freeing should basically</span>
<span class="quote">&gt; make it a noise.</span>

Even in fast path case, it adds two atomic operation per-process. If the
cache line is not exclusive to the core by the time of exit(2) it can be
noticible.

... but I guess it&#39;s not very hot scenario.

I guess I&#39;m just too cautious here. :)
<span class="quote">
&gt; &gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; &gt; getting it applied?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What kind of test(s) would you be interested in?</span>

Can we at lest check that number of /bin/true we can spawn per second
wouldn&#39;t be harmed by the patch? ;)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 24, 2017, 4:11 p.m.</div>
<pre class="content">
On Mon 24-07-17 17:51:42, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Jul 24, 2017 at 04:15:26PM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; What kind of scalability implication you have in mind? There is</span>
<span class="quote">&gt; &gt; basically a zero contention on the mmap_sem that late in the exit path</span>
<span class="quote">&gt; &gt; so this should be pretty much a fast path of the down_write. I agree it</span>
<span class="quote">&gt; &gt; is not 0 cost but the cost of the address space freeing should basically</span>
<span class="quote">&gt; &gt; make it a noise.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Even in fast path case, it adds two atomic operation per-process. If the</span>
<span class="quote">&gt; cache line is not exclusive to the core by the time of exit(2) it can be</span>
<span class="quote">&gt; noticible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... but I guess it&#39;s not very hot scenario.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess I&#39;m just too cautious here. :)</span>

I definitely did not want to handwave your concern. I just think we can
rule out the slow path and didn&#39;t think about the fast path overhead.
<span class="quote">
&gt; &gt; &gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; &gt; &gt; getting it applied?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; What kind of test(s) would you be interested in?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we at lest check that number of /bin/true we can spawn per second</span>
<span class="quote">&gt; wouldn&#39;t be harmed by the patch? ;)</span>

OK, so measuring a single /bin/true doesn&#39;t tell anything so I&#39;ve done
root@test1:~# cat a.sh 
#!/bin/sh

NR=$1
for i in $(seq $NR)
do
        /bin/true
done

in my virtual machine (on a otherwise idle host) with 4 cpus and 2GB of
RAM

Unpatched kernel
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 53.57
        System time (seconds): 26.12
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.46
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 53.90
        System time (seconds): 26.23
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.77
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 54.02
        System time (seconds): 26.18
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.92

patched kernel
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 53.81
        System time (seconds): 26.55
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.99
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 53.78
        System time (seconds): 26.15
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.67
root@test1:~# /usr/bin/time -v ./a.sh 100000 
        Command being timed: &quot;./a.sh 100000&quot;
        User time (seconds): 54.08
        System time (seconds): 26.87
        Percent of CPU this job got: 100%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 1:20.52

the results very quite a lot (have a look at the user time which
shouldn&#39;t have no reason to vary at all - maybe the virtual machine
aspect?). I would say that we are still reasonably close to a noise
here. Considering that /bin/true would close to the worst case I think
this looks reasonably. What do you think?

If you absolutely insist, I can make the lock conditional only for oom
victims. That would still mean current-&gt;signal-&gt;oom_mm pointers fetches
and a 2 branches.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - July 24, 2017, 4:42 p.m.</div>
<pre class="content">
Hi Michal,

[auto build test ERROR on mmotm/master]
[also build test ERROR on v4.13-rc2 next-20170724]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Michal-Hocko/mm-oom-allow-oom-reaper-to-race-with-exit_mmap/20170724-233159
base:   git://git.cmpxchg.org/linux-mmotm.git master
config: x86_64-randconfig-x016-201730 (attached as .config)
compiler: gcc-6 (Debian 6.2.0-3) 6.2.0 20160901
reproduce:
        # save the attached .config to linux build tree
        make ARCH=x86_64 

All error/warnings (new ones prefixed by &gt;&gt;):

   mm/oom_kill.c: In function &#39;__oom_reap_task_mm&#39;:
<span class="quote">&gt;&gt; mm/oom_kill.c:523:9: error: &#39;ret&#39; undeclared (first use in this function)</span>
     return ret;
            ^~~
   mm/oom_kill.c:523:9: note: each undeclared identifier is reported only once for each function it appears in
<span class="quote">&gt;&gt; mm/oom_kill.c:524:1: warning: control reaches end of non-void function [-Wreturn-type]</span>
    }
    ^

vim +/ret +523 mm/oom_kill.c

03049269d Michal Hocko       2016-03-25  468  
7ebffa455 Tetsuo Handa       2016-10-07  469  static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)
aac453635 Michal Hocko       2016-03-25  470  {
aac453635 Michal Hocko       2016-03-25  471  	struct mmu_gather tlb;
aac453635 Michal Hocko       2016-03-25  472  	struct vm_area_struct *vma;
e2fe14564 Michal Hocko       2016-05-27  473  
aac453635 Michal Hocko       2016-03-25  474  	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {
fa60da35c Andrew Morton      2017-07-14  475  		trace_skip_task_reaping(tsk-&gt;pid);
ed7a155c6 Michal Hocko       2017-07-24  476  		return false;
e5e3f4c4f Michal Hocko       2016-07-26  477  	}
e5e3f4c4f Michal Hocko       2016-07-26  478  
ed7a155c6 Michal Hocko       2017-07-24  479  	/* There is nothing to reap so bail out without signs in the log */
ed7a155c6 Michal Hocko       2017-07-24  480  	if (!mm-&gt;mmap)
ed7a155c6 Michal Hocko       2017-07-24  481  		goto unlock;
aac453635 Michal Hocko       2016-03-25  482  
fa60da35c Andrew Morton      2017-07-14  483  	trace_start_task_reaping(tsk-&gt;pid);
fa60da35c Andrew Morton      2017-07-14  484  
3f70dc38c Michal Hocko       2016-10-07  485  	/*
3f70dc38c Michal Hocko       2016-10-07  486  	 * Tell all users of get_user/copy_from_user etc... that the content
3f70dc38c Michal Hocko       2016-10-07  487  	 * is no longer stable. No barriers really needed because unmapping
3f70dc38c Michal Hocko       2016-10-07  488  	 * should imply barriers already and the reader would hit a page fault
3f70dc38c Michal Hocko       2016-10-07  489  	 * if it stumbled over a reaped memory.
3f70dc38c Michal Hocko       2016-10-07  490  	 */
3f70dc38c Michal Hocko       2016-10-07  491  	set_bit(MMF_UNSTABLE, &amp;mm-&gt;flags);
3f70dc38c Michal Hocko       2016-10-07  492  
aac453635 Michal Hocko       2016-03-25  493  	tlb_gather_mmu(&amp;tlb, mm, 0, -1);
aac453635 Michal Hocko       2016-03-25  494  	for (vma = mm-&gt;mmap ; vma; vma = vma-&gt;vm_next) {
235190738 Kirill A. Shutemov 2017-02-22  495  		if (!can_madv_dontneed_vma(vma))
aac453635 Michal Hocko       2016-03-25  496  			continue;
aac453635 Michal Hocko       2016-03-25  497  
aac453635 Michal Hocko       2016-03-25  498  		/*
aac453635 Michal Hocko       2016-03-25  499  		 * Only anonymous pages have a good chance to be dropped
aac453635 Michal Hocko       2016-03-25  500  		 * without additional steps which we cannot afford as we
aac453635 Michal Hocko       2016-03-25  501  		 * are OOM already.
aac453635 Michal Hocko       2016-03-25  502  		 *
aac453635 Michal Hocko       2016-03-25  503  		 * We do not even care about fs backed pages because all
aac453635 Michal Hocko       2016-03-25  504  		 * which are reclaimable have already been reclaimed and
aac453635 Michal Hocko       2016-03-25  505  		 * we do not want to block exit_mmap by keeping mm ref
aac453635 Michal Hocko       2016-03-25  506  		 * count elevated without a good reason.
aac453635 Michal Hocko       2016-03-25  507  		 */
aac453635 Michal Hocko       2016-03-25  508  		if (vma_is_anonymous(vma) || !(vma-&gt;vm_flags &amp; VM_SHARED))
aac453635 Michal Hocko       2016-03-25  509  			unmap_page_range(&amp;tlb, vma, vma-&gt;vm_start, vma-&gt;vm_end,
3e8715fdc Kirill A. Shutemov 2017-02-22  510  					 NULL);
aac453635 Michal Hocko       2016-03-25  511  	}
aac453635 Michal Hocko       2016-03-25  512  	tlb_finish_mmu(&amp;tlb, 0, -1);
bc448e897 Michal Hocko       2016-03-25  513  	pr_info(&quot;oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n&quot;,
bc448e897 Michal Hocko       2016-03-25  514  			task_pid_nr(tsk), tsk-&gt;comm,
bc448e897 Michal Hocko       2016-03-25  515  			K(get_mm_counter(mm, MM_ANONPAGES)),
bc448e897 Michal Hocko       2016-03-25  516  			K(get_mm_counter(mm, MM_FILEPAGES)),
bc448e897 Michal Hocko       2016-03-25  517  			K(get_mm_counter(mm, MM_SHMEMPAGES)));
36324a990 Michal Hocko       2016-03-25  518  
fa60da35c Andrew Morton      2017-07-14  519  	trace_finish_task_reaping(tsk-&gt;pid);
ed7a155c6 Michal Hocko       2017-07-24  520  unlock:
ed7a155c6 Michal Hocko       2017-07-24  521  	up_read(&amp;mm-&gt;mmap_sem);
ed7a155c6 Michal Hocko       2017-07-24  522  
aac453635 Michal Hocko       2016-03-25 @523  	return ret;
aac453635 Michal Hocko       2016-03-25 @524  }
aac453635 Michal Hocko       2016-03-25  525  

:::::: The code at line 523 was first introduced by commit
:::::: aac453635549699c13a84ea1456d5b0e574ef855 mm, oom: introduce oom reaper

:::::: TO: Michal Hocko &lt;mhocko@suse.com&gt;
:::::: CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 24, 2017, 6:12 p.m.</div>
<pre class="content">
On Tue 25-07-17 00:42:05, kbuild test robot wrote:
<span class="quote">&gt; Hi Michal,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [auto build test ERROR on mmotm/master]</span>
<span class="quote">&gt; [also build test ERROR on v4.13-rc2 next-20170724]</span>
<span class="quote">&gt; [if your patch is applied to the wrong git tree, please drop us a note to help improve the system]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; url:    https://github.com/0day-ci/linux/commits/Michal-Hocko/mm-oom-allow-oom-reaper-to-race-with-exit_mmap/20170724-233159</span>
<span class="quote">&gt; base:   git://git.cmpxchg.org/linux-mmotm.git master</span>
<span class="quote">&gt; config: x86_64-randconfig-x016-201730 (attached as .config)</span>
<span class="quote">&gt; compiler: gcc-6 (Debian 6.2.0-3) 6.2.0 20160901</span>
<span class="quote">&gt; reproduce:</span>
<span class="quote">&gt;         # save the attached .config to linux build tree</span>
<span class="quote">&gt;         make ARCH=x86_64 </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; All error/warnings (new ones prefixed by &gt;&gt;):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    mm/oom_kill.c: In function &#39;__oom_reap_task_mm&#39;:</span>
<span class="quote">&gt; &gt;&gt; mm/oom_kill.c:523:9: error: &#39;ret&#39; undeclared (first use in this function)</span>
<span class="quote">&gt;      return ret;</span>
<span class="quote">&gt;             ^~~</span>

Fixed by http://lkml.kernel.org/r/20170724152703.GP25221@dhcp22.suse.cz
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 25, 2017, 2:17 p.m.</div>
<pre class="content">
On Mon, Jul 24, 2017 at 06:11:47PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Mon 24-07-17 17:51:42, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jul 24, 2017 at 04:15:26PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; What kind of scalability implication you have in mind? There is</span>
<span class="quote">&gt; &gt; &gt; basically a zero contention on the mmap_sem that late in the exit path</span>
<span class="quote">&gt; &gt; &gt; so this should be pretty much a fast path of the down_write. I agree it</span>
<span class="quote">&gt; &gt; &gt; is not 0 cost but the cost of the address space freeing should basically</span>
<span class="quote">&gt; &gt; &gt; make it a noise.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Even in fast path case, it adds two atomic operation per-process. If the</span>
<span class="quote">&gt; &gt; cache line is not exclusive to the core by the time of exit(2) it can be</span>
<span class="quote">&gt; &gt; noticible.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; ... but I guess it&#39;s not very hot scenario.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I guess I&#39;m just too cautious here. :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I definitely did not want to handwave your concern. I just think we can</span>
<span class="quote">&gt; rule out the slow path and didn&#39;t think about the fast path overhead.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; &gt; &gt; &gt; getting it applied?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; What kind of test(s) would you be interested in?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we at lest check that number of /bin/true we can spawn per second</span>
<span class="quote">&gt; &gt; wouldn&#39;t be harmed by the patch? ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, so measuring a single /bin/true doesn&#39;t tell anything so I&#39;ve done</span>
<span class="quote">&gt; root@test1:~# cat a.sh </span>
<span class="quote">&gt; #!/bin/sh</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NR=$1</span>
<span class="quote">&gt; for i in $(seq $NR)</span>
<span class="quote">&gt; do</span>
<span class="quote">&gt;         /bin/true</span>
<span class="quote">&gt; done</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; in my virtual machine (on a otherwise idle host) with 4 cpus and 2GB of</span>
<span class="quote">&gt; RAM</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unpatched kernel</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 53.57</span>
<span class="quote">&gt;         System time (seconds): 26.12</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.46</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 53.90</span>
<span class="quote">&gt;         System time (seconds): 26.23</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.77</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 54.02</span>
<span class="quote">&gt;         System time (seconds): 26.18</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.92</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; patched kernel</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 53.81</span>
<span class="quote">&gt;         System time (seconds): 26.55</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.99</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 53.78</span>
<span class="quote">&gt;         System time (seconds): 26.15</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:19.67</span>
<span class="quote">&gt; root@test1:~# /usr/bin/time -v ./a.sh 100000 </span>
<span class="quote">&gt;         Command being timed: &quot;./a.sh 100000&quot;</span>
<span class="quote">&gt;         User time (seconds): 54.08</span>
<span class="quote">&gt;         System time (seconds): 26.87</span>
<span class="quote">&gt;         Percent of CPU this job got: 100%</span>
<span class="quote">&gt;         Elapsed (wall clock) time (h:mm:ss or m:ss): 1:20.52</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; the results very quite a lot (have a look at the user time which</span>
<span class="quote">&gt; shouldn&#39;t have no reason to vary at all - maybe the virtual machine</span>
<span class="quote">&gt; aspect?). I would say that we are still reasonably close to a noise</span>
<span class="quote">&gt; here. Considering that /bin/true would close to the worst case I think</span>
<span class="quote">&gt; this looks reasonably. What do you think?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you absolutely insist, I can make the lock conditional only for oom</span>
<span class="quote">&gt; victims. That would still mean current-&gt;signal-&gt;oom_mm pointers fetches</span>
<span class="quote">&gt; and a 2 branches.</span>


Below are numbers for the same test case, but from bigger machine (48
threads, 64GiB of RAM).

v4.13-rc2:

 Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):

     159857.233790      task-clock:u (msec)       #    1.000 CPUs utilized            ( +-  3.21% )
                 0      context-switches:u        #    0.000 K/sec
                 0      cpu-migrations:u          #    0.000 K/sec
         8,761,843      page-faults:u             #    0.055 M/sec                    ( +-  0.64% )
    38,725,763,026      cycles:u                  #    0.242 GHz                      ( +-  0.18% )
   272,691,643,016      stalled-cycles-frontend:u #  704.16% frontend cycles idle     ( +-  3.16% )
    22,221,416,575      instructions:u            #    0.57  insn per cycle
                                                  #   12.27  stalled cycles per insn  ( +-  0.00% )
     5,306,829,649      branches:u                #   33.197 M/sec                    ( +-  0.00% )
       240,783,599      branch-misses:u           #    4.54% of all branches          ( +-  0.15% )

     159.808721098 seconds time elapsed                                          ( +-  3.15% )

v4.13-rc2 + the patch:

 Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):

     167628.094556      task-clock:u (msec)       #    1.007 CPUs utilized            ( +-  1.63% )
                 0      context-switches:u        #    0.000 K/sec
                 0      cpu-migrations:u          #    0.000 K/sec
         8,838,314      page-faults:u             #    0.053 M/sec                    ( +-  0.26% )
    38,862,240,137      cycles:u                  #    0.232 GHz                      ( +-  0.10% )
   282,105,057,553      stalled-cycles-frontend:u #  725.91% frontend cycles idle     ( +-  1.64% )
    22,219,273,623      instructions:u            #    0.57  insn per cycle
                                                  #   12.70  stalled cycles per insn  ( +-  0.00% )
     5,306,165,194      branches:u                #   31.654 M/sec                    ( +-  0.00% )
       240,473,075      branch-misses:u           #    4.53% of all branches          ( +-  0.07% )

     166.497005412 seconds time elapsed                                          ( +-  1.61% )

IMO, there is something to think about. ~4% slowdown is not insignificant.
I expect effect to be bigger for larger machines.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 25, 2017, 2:26 p.m.</div>
<pre class="content">
On Tue 25-07-17 17:17:23, Kirill A. Shutemov wrote:
[...]
<span class="quote">&gt; Below are numbers for the same test case, but from bigger machine (48</span>
<span class="quote">&gt; threads, 64GiB of RAM).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v4.13-rc2:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      159857.233790      task-clock:u (msec)       #    1.000 CPUs utilized            ( +-  3.21% )</span>
<span class="quote">&gt;                  0      context-switches:u        #    0.000 K/sec</span>
<span class="quote">&gt;                  0      cpu-migrations:u          #    0.000 K/sec</span>
<span class="quote">&gt;          8,761,843      page-faults:u             #    0.055 M/sec                    ( +-  0.64% )</span>
<span class="quote">&gt;     38,725,763,026      cycles:u                  #    0.242 GHz                      ( +-  0.18% )</span>
<span class="quote">&gt;    272,691,643,016      stalled-cycles-frontend:u #  704.16% frontend cycles idle     ( +-  3.16% )</span>
<span class="quote">&gt;     22,221,416,575      instructions:u            #    0.57  insn per cycle</span>
<span class="quote">&gt;                                                   #   12.27  stalled cycles per insn  ( +-  0.00% )</span>
<span class="quote">&gt;      5,306,829,649      branches:u                #   33.197 M/sec                    ( +-  0.00% )</span>
<span class="quote">&gt;        240,783,599      branch-misses:u           #    4.54% of all branches          ( +-  0.15% )</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      159.808721098 seconds time elapsed                                          ( +-  3.15% )</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v4.13-rc2 + the patch:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      167628.094556      task-clock:u (msec)       #    1.007 CPUs utilized            ( +-  1.63% )</span>
<span class="quote">&gt;                  0      context-switches:u        #    0.000 K/sec</span>
<span class="quote">&gt;                  0      cpu-migrations:u          #    0.000 K/sec</span>
<span class="quote">&gt;          8,838,314      page-faults:u             #    0.053 M/sec                    ( +-  0.26% )</span>
<span class="quote">&gt;     38,862,240,137      cycles:u                  #    0.232 GHz                      ( +-  0.10% )</span>
<span class="quote">&gt;    282,105,057,553      stalled-cycles-frontend:u #  725.91% frontend cycles idle     ( +-  1.64% )</span>
<span class="quote">&gt;     22,219,273,623      instructions:u            #    0.57  insn per cycle</span>
<span class="quote">&gt;                                                   #   12.70  stalled cycles per insn  ( +-  0.00% )</span>
<span class="quote">&gt;      5,306,165,194      branches:u                #   31.654 M/sec                    ( +-  0.00% )</span>
<span class="quote">&gt;        240,473,075      branch-misses:u           #    4.53% of all branches          ( +-  0.07% )</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;      166.497005412 seconds time elapsed                                          ( +-  1.61% )</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IMO, there is something to think about. ~4% slowdown is not insignificant.</span>
<span class="quote">&gt; I expect effect to be bigger for larger machines.</span>

Thanks for retesting Kirill. Are those numbers stable over runs? E.g.
the run without the patch has ~3% variance while the one with the patch
has it smaller. This sounds suspicious to me. There shouldn&#39;t be any
lock contention (except for the oom killer) so the lock shouldn&#39;t make
any difference wrt. variability.

Also I was about to post a more targeted test. Could you try it with it
as well, please?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 25, 2017, 2:26 p.m.</div>
<pre class="content">
On Mon 24-07-17 18:11:46, Michal Hocko wrote:
<span class="quote">&gt; On Mon 24-07-17 17:51:42, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jul 24, 2017 at 04:15:26PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; What kind of scalability implication you have in mind? There is</span>
<span class="quote">&gt; &gt; &gt; basically a zero contention on the mmap_sem that late in the exit path</span>
<span class="quote">&gt; &gt; &gt; so this should be pretty much a fast path of the down_write. I agree it</span>
<span class="quote">&gt; &gt; &gt; is not 0 cost but the cost of the address space freeing should basically</span>
<span class="quote">&gt; &gt; &gt; make it a noise.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Even in fast path case, it adds two atomic operation per-process. If the</span>
<span class="quote">&gt; &gt; cache line is not exclusive to the core by the time of exit(2) it can be</span>
<span class="quote">&gt; &gt; noticible.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; ... but I guess it&#39;s not very hot scenario.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I guess I&#39;m just too cautious here. :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I definitely did not want to handwave your concern. I just think we can</span>
<span class="quote">&gt; rule out the slow path and didn&#39;t think about the fast path overhead.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; &gt; &gt; &gt; getting it applied?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; What kind of test(s) would you be interested in?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we at lest check that number of /bin/true we can spawn per second</span>
<span class="quote">&gt; &gt; wouldn&#39;t be harmed by the patch? ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, so measuring a single /bin/true doesn&#39;t tell anything so I&#39;ve done</span>
<span class="quote">&gt; root@test1:~# cat a.sh </span>
<span class="quote">&gt; #!/bin/sh</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NR=$1</span>
<span class="quote">&gt; for i in $(seq $NR)</span>
<span class="quote">&gt; do</span>
<span class="quote">&gt;         /bin/true</span>
<span class="quote">&gt; done</span>

I wanted to reduce a potential shell side effects so I&#39;ve come with a
simple program which forks and saves the timestamp before child exit and
right after waitpid (see attached) and then measured it 100k times. Sure
this still measures waitpid overhead and the signal delivery but this
should be more or less constant on an idle system, right? See attached.

before the patch
min: 306300.00 max: 6731916.00 avg: 437962.07 std: 92898.30 nr: 100000

after
min: 303196.00 max: 5728080.00 avg: 436081.87 std: 96165.98 nr: 100000

The results are well withing noise as I would expect.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 25, 2017, 3:07 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 04:26:17PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Tue 25-07-17 17:17:23, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Below are numbers for the same test case, but from bigger machine (48</span>
<span class="quote">&gt; &gt; threads, 64GiB of RAM).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; v4.13-rc2:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;      159857.233790      task-clock:u (msec)       #    1.000 CPUs utilized            ( +-  3.21% )</span>
<span class="quote">&gt; &gt;                  0      context-switches:u        #    0.000 K/sec</span>
<span class="quote">&gt; &gt;                  0      cpu-migrations:u          #    0.000 K/sec</span>
<span class="quote">&gt; &gt;          8,761,843      page-faults:u             #    0.055 M/sec                    ( +-  0.64% )</span>
<span class="quote">&gt; &gt;     38,725,763,026      cycles:u                  #    0.242 GHz                      ( +-  0.18% )</span>
<span class="quote">&gt; &gt;    272,691,643,016      stalled-cycles-frontend:u #  704.16% frontend cycles idle     ( +-  3.16% )</span>
<span class="quote">&gt; &gt;     22,221,416,575      instructions:u            #    0.57  insn per cycle</span>
<span class="quote">&gt; &gt;                                                   #   12.27  stalled cycles per insn  ( +-  0.00% )</span>
<span class="quote">&gt; &gt;      5,306,829,649      branches:u                #   33.197 M/sec                    ( +-  0.00% )</span>
<span class="quote">&gt; &gt;        240,783,599      branch-misses:u           #    4.54% of all branches          ( +-  0.15% )</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;      159.808721098 seconds time elapsed                                          ( +-  3.15% )</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; v4.13-rc2 + the patch:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  Performance counter stats for &#39;./a.sh 100000&#39; (5 runs):</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;      167628.094556      task-clock:u (msec)       #    1.007 CPUs utilized            ( +-  1.63% )</span>
<span class="quote">&gt; &gt;                  0      context-switches:u        #    0.000 K/sec</span>
<span class="quote">&gt; &gt;                  0      cpu-migrations:u          #    0.000 K/sec</span>
<span class="quote">&gt; &gt;          8,838,314      page-faults:u             #    0.053 M/sec                    ( +-  0.26% )</span>
<span class="quote">&gt; &gt;     38,862,240,137      cycles:u                  #    0.232 GHz                      ( +-  0.10% )</span>
<span class="quote">&gt; &gt;    282,105,057,553      stalled-cycles-frontend:u #  725.91% frontend cycles idle     ( +-  1.64% )</span>
<span class="quote">&gt; &gt;     22,219,273,623      instructions:u            #    0.57  insn per cycle</span>
<span class="quote">&gt; &gt;                                                   #   12.70  stalled cycles per insn  ( +-  0.00% )</span>
<span class="quote">&gt; &gt;      5,306,165,194      branches:u                #   31.654 M/sec                    ( +-  0.00% )</span>
<span class="quote">&gt; &gt;        240,473,075      branch-misses:u           #    4.53% of all branches          ( +-  0.07% )</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;      166.497005412 seconds time elapsed                                          ( +-  1.61% )</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; IMO, there is something to think about. ~4% slowdown is not insignificant.</span>
<span class="quote">&gt; &gt; I expect effect to be bigger for larger machines.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for retesting Kirill. Are those numbers stable over runs? E.g.</span>
<span class="quote">&gt; the run without the patch has ~3% variance while the one with the patch</span>
<span class="quote">&gt; has it smaller. This sounds suspicious to me. There shouldn&#39;t be any</span>
<span class="quote">&gt; lock contention (except for the oom killer) so the lock shouldn&#39;t make</span>
<span class="quote">&gt; any difference wrt. variability.</span>

There&#39;s run-to-tun variability. I&#39;ll post new numbers for your new test.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 25, 2017, 3:15 p.m.</div>
<pre class="content">
On Tue 25-07-17 18:07:19, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Tue, Jul 25, 2017 at 04:26:17PM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; Thanks for retesting Kirill. Are those numbers stable over runs? E.g.</span>
<span class="quote">&gt; &gt; the run without the patch has ~3% variance while the one with the patch</span>
<span class="quote">&gt; &gt; has it smaller. This sounds suspicious to me. There shouldn&#39;t be any</span>
<span class="quote">&gt; &gt; lock contention (except for the oom killer) so the lock shouldn&#39;t make</span>
<span class="quote">&gt; &gt; any difference wrt. variability.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There&#39;s run-to-tun variability. I&#39;ll post new numbers for your new test.</span>

That&#39;s what I&#39;ve seen and the variance was quite large. I suspected
shell but if you look at the more dedicated test, the std over avg is
still quite large.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 25, 2017, 3:17 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 04:26:26PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Mon 24-07-17 18:11:46, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 24-07-17 17:51:42, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; On Mon, Jul 24, 2017 at 04:15:26PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; What kind of scalability implication you have in mind? There is</span>
<span class="quote">&gt; &gt; &gt; &gt; basically a zero contention on the mmap_sem that late in the exit path</span>
<span class="quote">&gt; &gt; &gt; &gt; so this should be pretty much a fast path of the down_write. I agree it</span>
<span class="quote">&gt; &gt; &gt; &gt; is not 0 cost but the cost of the address space freeing should basically</span>
<span class="quote">&gt; &gt; &gt; &gt; make it a noise.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Even in fast path case, it adds two atomic operation per-process. If the</span>
<span class="quote">&gt; &gt; &gt; cache line is not exclusive to the core by the time of exit(2) it can be</span>
<span class="quote">&gt; &gt; &gt; noticible.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; ... but I guess it&#39;s not very hot scenario.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I guess I&#39;m just too cautious here. :)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I definitely did not want to handwave your concern. I just think we can</span>
<span class="quote">&gt; &gt; rule out the slow path and didn&#39;t think about the fast path overhead.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Should we do performance/scalability evaluation of the patch before</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; getting it applied?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; What kind of test(s) would you be interested in?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Can we at lest check that number of /bin/true we can spawn per second</span>
<span class="quote">&gt; &gt; &gt; wouldn&#39;t be harmed by the patch? ;)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; OK, so measuring a single /bin/true doesn&#39;t tell anything so I&#39;ve done</span>
<span class="quote">&gt; &gt; root@test1:~# cat a.sh </span>
<span class="quote">&gt; &gt; #!/bin/sh</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; NR=$1</span>
<span class="quote">&gt; &gt; for i in $(seq $NR)</span>
<span class="quote">&gt; &gt; do</span>
<span class="quote">&gt; &gt;         /bin/true</span>
<span class="quote">&gt; &gt; done</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I wanted to reduce a potential shell side effects so I&#39;ve come with a</span>
<span class="quote">&gt; simple program which forks and saves the timestamp before child exit and</span>
<span class="quote">&gt; right after waitpid (see attached) and then measured it 100k times. Sure</span>
<span class="quote">&gt; this still measures waitpid overhead and the signal delivery but this</span>
<span class="quote">&gt; should be more or less constant on an idle system, right? See attached.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; before the patch</span>
<span class="quote">&gt; min: 306300.00 max: 6731916.00 avg: 437962.07 std: 92898.30 nr: 100000</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; after</span>
<span class="quote">&gt; min: 303196.00 max: 5728080.00 avg: 436081.87 std: 96165.98 nr: 100000</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The results are well withing noise as I would expect.</span>

I&#39;ve silightly modified your test case: replaced cpuid + rdtsc with
rdtscp. cpuid overhead is measurable in such tight loop.

3 runs before the patch:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 177200  205000  212900  217800  223700 2377000
 172400  201700  209700  214300  220600 1343000
 175700  203800  212300  217100  223000 1061000

3 runs after the patch:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 175900  204800  213000  216400  223600 1989000
 180300  210900  219600  223600  230200 3184000
 182100  212500  222000  226200  232700 1473000

The difference is still measuarble. Around 3%.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 25, 2017, 3:23 p.m.</div>
<pre class="content">
On Tue 25-07-17 18:17:54, Kirill A. Shutemov wrote:
<span class="quote">&gt; &gt; before the patch</span>
<span class="quote">&gt; &gt; min: 306300.00 max: 6731916.00 avg: 437962.07 std: 92898.30 nr: 100000</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; after</span>
<span class="quote">&gt; &gt; min: 303196.00 max: 5728080.00 avg: 436081.87 std: 96165.98 nr: 100000</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The results are well withing noise as I would expect.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve silightly modified your test case: replaced cpuid + rdtsc with</span>
<span class="quote">&gt; rdtscp. cpuid overhead is measurable in such tight loop.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3 runs before the patch:</span>
<span class="quote">&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</span>
<span class="quote">&gt;  177200  205000  212900  217800  223700 2377000</span>
<span class="quote">&gt;  172400  201700  209700  214300  220600 1343000</span>
<span class="quote">&gt;  175700  203800  212300  217100  223000 1061000</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3 runs after the patch:</span>
<span class="quote">&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</span>
<span class="quote">&gt;  175900  204800  213000  216400  223600 1989000</span>
<span class="quote">&gt;  180300  210900  219600  223600  230200 3184000</span>
<span class="quote">&gt;  182100  212500  222000  226200  232700 1473000</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The difference is still measuarble. Around 3%.</span>

what is stdev?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - July 25, 2017, 3:26 p.m.</div>
<pre class="content">
On Mon, Jul 24, 2017 at 09:23:32AM +0200, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; David has noticed that the oom killer might kill additional tasks while</span>
<span class="quote">&gt; the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks</span>
<span class="quote">&gt; the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down</span>
<span class="quote">&gt; to 0. The race is as follows</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; oom_reap_task				do_exit</span>
<span class="quote">&gt; 					  exit_mm</span>
<span class="quote">&gt;   __oom_reap_task_mm</span>
<span class="quote">&gt; 					    mmput</span>
<span class="quote">&gt; 					      __mmput</span>
<span class="quote">&gt;     mmget_not_zero # fails</span>
<span class="quote">&gt;     						exit_mmap # frees memory</span>
<span class="quote">&gt;   set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The victim is still visible to the OOM killer until it is unhashed.</span>

I think this is a very minor problem, in the worst case you get a
false positive oom kill, and it requires a race condition for it to
happen. I wouldn&#39;t add mmap_sem in exit_mmap just for this considering
the mmget_not_zero is already enough to leave exit_mmap alone.

Could you first clarify these points then I&#39;ll understand better what
the above is about:

1) if exit_mmap runs for a long time with terabytes of RAM with
   mmap_sem held for writing like your patch does, wouldn&#39;t then
   oom_reap_task_mm fail the same way after a few tries on
   down_read_trylock? Despite your patch got applied? Isn&#39;t that
   simply moving the failure that leads to set_bit(MMF_OOM_SKIP) from
   mmget_not_zero to down_read_trylock?

2) why isn&#39;t __oom_reap_task_mm returning different retvals in case
   mmget_not_zero fails? What is the point to schedule_timeout
   and retry MAX_OOM_REAP_RETRIES times if mmget_not_zero caused it to
   return null as it can&#39;t do anything about such task anymore? Why
   are we scheduling those RETRIES times if mm_users is 0?

3) if exit_mmap is freeing lots of memory already, why should there be
   another OOM immediately? I thought oom reaper only was needed when
   the task on the right column couldn&#39;t reach the final mmput to set
   mm_users to 0. Why exactly is a problem that MMF_OOM_SKIP gets set
   on the mm, if exit_mmap is already guaranteed to be running? Why
   isn&#39;t the oom reaper happy to just stop in such case and wait it to
   complete? exit_mmap doesn&#39;t even take the mmap_sem and it&#39;s running
   in R state, how would it block in a way that requires the OOM
   reaper to free memory from another process to complete?

4) how is it safe to overwrite a VM_FAULT_RETRY that returns without
   mmap_sem and then the arch code will release the mmap_sem despite
   it was already released by handle_mm_fault? Anonymous memory faults
   aren&#39;t common to return VM_FAULT_RETRY but an userfault
   can. Shouldn&#39;t there be a block that prevents overwriting if
   VM_FAULT_RETRY is set below? (not only VM_FAULT_ERROR)

	if (unlikely((current-&gt;flags &amp; PF_KTHREAD) &amp;&amp; !(ret &amp; VM_FAULT_ERROR)
				&amp;&amp; test_bit(MMF_UNSTABLE, &amp;vma-&gt;vm_mm-&gt;flags)))
		ret = VM_FAULT_SIGBUS;

Thanks,
Andrea
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 25, 2017, 3:31 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 05:23:00PM +0200, Michal Hocko wrote:
<span class="quote">&gt; what is stdev?</span>

Updated tables:

3 runs before the patch:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  Stdev
 177200  205000  212900  217800  223700 2377000  32868
 172400  201700  209700  214300  220600 1343000  31191
 175700  203800  212300  217100  223000 1061000  31195

3 runs after the patch:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  Stdev
 175900  204800  213000  216400  223600 1989000  27210
 180300  210900  219600  223600  230200 3184000  32609
 182100  212500  222000  226200  232700 1473000  32138
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 25, 2017, 3:45 p.m.</div>
<pre class="content">
On Tue 25-07-17 17:26:39, Andrea Arcangeli wrote:
<span class="quote">&gt; On Mon, Jul 24, 2017 at 09:23:32AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; David has noticed that the oom killer might kill additional tasks while</span>
<span class="quote">&gt; &gt; the exiting oom victim hasn&#39;t terminated yet because the oom_reaper marks</span>
<span class="quote">&gt; &gt; the curent victim MMF_OOM_SKIP too early when mm-&gt;mm_users dropped down</span>
<span class="quote">&gt; &gt; to 0. The race is as follows</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; oom_reap_task				do_exit</span>
<span class="quote">&gt; &gt; 					  exit_mm</span>
<span class="quote">&gt; &gt;   __oom_reap_task_mm</span>
<span class="quote">&gt; &gt; 					    mmput</span>
<span class="quote">&gt; &gt; 					      __mmput</span>
<span class="quote">&gt; &gt;     mmget_not_zero # fails</span>
<span class="quote">&gt; &gt;     						exit_mmap # frees memory</span>
<span class="quote">&gt; &gt;   set_bit(MMF_OOM_SKIP)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The victim is still visible to the OOM killer until it is unhashed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this is a very minor problem, in the worst case you get a</span>
<span class="quote">&gt; false positive oom kill, and it requires a race condition for it to</span>
<span class="quote">&gt; happen. I wouldn&#39;t add mmap_sem in exit_mmap just for this considering</span>
<span class="quote">&gt; the mmget_not_zero is already enough to leave exit_mmap alone.</span>

That problem is real though as reported by David.
<span class="quote">
&gt; Could you first clarify these points then I&#39;ll understand better what</span>
<span class="quote">&gt; the above is about:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) if exit_mmap runs for a long time with terabytes of RAM with</span>
<span class="quote">&gt;    mmap_sem held for writing like your patch does, wouldn&#39;t then</span>
<span class="quote">&gt;    oom_reap_task_mm fail the same way after a few tries on</span>
<span class="quote">&gt;    down_read_trylock? Despite your patch got applied? Isn&#39;t that</span>
<span class="quote">&gt;    simply moving the failure that leads to set_bit(MMF_OOM_SKIP) from</span>
<span class="quote">&gt;    mmget_not_zero to down_read_trylock?</span>

No, it&#39;s not because the exclusive lock in exit_mmap is taken _after_ we
unmapped the address space. unmap_vmas will happily race with the oom
reaper.
<span class="quote"> 
&gt; 2) why isn&#39;t __oom_reap_task_mm returning different retvals in case</span>
<span class="quote">&gt;    mmget_not_zero fails? What is the point to schedule_timeout</span>
<span class="quote">&gt;    and retry MAX_OOM_REAP_RETRIES times if mmget_not_zero caused it to</span>
<span class="quote">&gt;    return null as it can&#39;t do anything about such task anymore? Why</span>
<span class="quote">&gt;    are we scheduling those RETRIES times if mm_users is 0?</span>

We are not. __oom_reap_task_mm will return true if the mm_users is 0 and
bail out.
<span class="quote">
&gt; 3) if exit_mmap is freeing lots of memory already, why should there be</span>
<span class="quote">&gt;    another OOM immediately?</span>

Because the memory can be freed from a different oom domain (e.g. a
different NUMA node).
<span class="quote">
&gt;    I thought oom reaper only was needed when</span>
<span class="quote">&gt;    the task on the right column couldn&#39;t reach the final mmput to set</span>
<span class="quote">&gt;    mm_users to 0. Why exactly is a problem that MMF_OOM_SKIP gets set</span>
<span class="quote">&gt;    on the mm, if exit_mmap is already guaranteed to be running?</span>

MMF_OOM_SKIP will hide this task from the OOM killer and so we will
select another victim if we are still under oom. We _want_ to postpone
setting MMF_OOM_SKIP until we know that the oom victim no longer
interesting and we can go on to select another one.
<span class="quote">
&gt;    Why</span>
<span class="quote">&gt;    isn&#39;t the oom reaper happy to just stop in such case and wait it to</span>
<span class="quote">&gt;    complete?</span>

Because there is no _guarantee_ that the final __mmput will release the
memory in finite time. And we cannot guarantee that longterm.
<span class="quote">
&gt;    exit_mmap doesn&#39;t even take the mmap_sem and it&#39;s running</span>
<span class="quote">&gt;    in R state, how would it block in a way that requires the OOM</span>
<span class="quote">&gt;    reaper to free memory from another process to complete?</span>

it is not only about exit_mmap. __mmput calls into exit_aio and that can
wait for completion and there is no way to guarantee this will finish in
finite time.
<span class="quote">
&gt; 4) how is it safe to overwrite a VM_FAULT_RETRY that returns without</span>
<span class="quote">&gt;    mmap_sem and then the arch code will release the mmap_sem despite</span>
<span class="quote">&gt;    it was already released by handle_mm_fault? Anonymous memory faults</span>
<span class="quote">&gt;    aren&#39;t common to return VM_FAULT_RETRY but an userfault</span>
<span class="quote">&gt;    can. Shouldn&#39;t there be a block that prevents overwriting if</span>
<span class="quote">&gt;    VM_FAULT_RETRY is set below? (not only VM_FAULT_ERROR)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (unlikely((current-&gt;flags &amp; PF_KTHREAD) &amp;&amp; !(ret &amp; VM_FAULT_ERROR)</span>
<span class="quote">&gt; 				&amp;&amp; test_bit(MMF_UNSTABLE, &amp;vma-&gt;vm_mm-&gt;flags)))</span>
<span class="quote">&gt; 		ret = VM_FAULT_SIGBUS;</span>

I am not sure I understand what you mean and how this is related to the
patch?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 24e9261bdcc0..0eeb658caa30 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2993,6 +2993,11 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * oom reaper might race with exit_mmap so make sure we won&#39;t free</span>
<span class="p_add">+	 * page tables or unmap VMAs under its feet</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	down_write(&amp;mm-&gt;mmap_sem);</span>
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 
<span class="p_chunk">@@ -3005,7 +3010,9 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
<span class="p_add">+	mm-&gt;mmap = NULL;</span>
 	vm_unacct_memory(nr_accounted);
<span class="p_add">+	up_write(&amp;mm-&gt;mmap_sem);</span>
 }
 
 /* Insert vm structure into process list sorted by address
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f030c1c..a6dabe3691c1 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -470,40 +470,15 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
<span class="p_del">-	bool ret = true;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We have to make sure to not race with the victim exit path</span>
<span class="p_del">-	 * and cause premature new oom victim selection:</span>
<span class="p_del">-	 * __oom_reap_task_mm		exit_mm</span>
<span class="p_del">-	 *   mmget_not_zero</span>
<span class="p_del">-	 *				  mmput</span>
<span class="p_del">-	 *				    atomic_dec_and_test</span>
<span class="p_del">-	 *				  exit_oom_victim</span>
<span class="p_del">-	 *				[...]</span>
<span class="p_del">-	 *				out_of_memory</span>
<span class="p_del">-	 *				  select_bad_process</span>
<span class="p_del">-	 *				    # no TIF_MEMDIE task selects new victim</span>
<span class="p_del">-	 *  unmap_page_range # frees some memory</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mutex_lock(&amp;oom_lock);</span>
 
 	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {
<span class="p_del">-		ret = false;</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		return false;</span>
 	}
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * increase mm_users only after we know we will reap something so</span>
<span class="p_del">-	 * that the mmput_async is called only when we have reaped something</span>
<span class="p_del">-	 * and delayed __mmput doesn&#39;t matter that much</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!mmget_not_zero(mm)) {</span>
<span class="p_del">-		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_del">-		trace_skip_task_reaping(tsk-&gt;pid);</span>
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/* There is nothing to reap so bail out without signs in the log */</span>
<span class="p_add">+	if (!mm-&gt;mmap)</span>
<span class="p_add">+		goto unlock;</span>
 
 	trace_start_task_reaping(tsk-&gt;pid);
 
<span class="p_chunk">@@ -540,17 +515,11 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 			K(get_mm_counter(mm, MM_ANONPAGES)),
 			K(get_mm_counter(mm, MM_FILEPAGES)),
 			K(get_mm_counter(mm, MM_SHMEMPAGES)));
<span class="p_del">-	up_read(&amp;mm-&gt;mmap_sem);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Drop our reference but make sure the mmput slow path is called from a</span>
<span class="p_del">-	 * different context because we shouldn&#39;t risk we get stuck there and</span>
<span class="p_del">-	 * put the oom_reaper out of the way.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mmput_async(mm);</span>
 	trace_finish_task_reaping(tsk-&gt;pid);
<span class="p_del">-unlock_oom:</span>
<span class="p_del">-	mutex_unlock(&amp;oom_lock);</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
 	return ret;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



