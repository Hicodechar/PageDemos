
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3.10,268/268] mm: larger stack guard gap, between vmas - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3.10,268/268] mm: larger stack guard gap, between vmas</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 19, 2017, 6:32 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1497897167-14556-269-git-send-email-w@1wt.eu&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9797613/mbox/"
   >mbox</a>
|
   <a href="/patch/9797613/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9797613/">/patch/9797613/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F39956020B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 19 Jun 2017 19:28:25 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CEC9326256
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 19 Jun 2017 19:28:25 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C30EC26E46; Mon, 19 Jun 2017 19:28:25 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1357326256
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 19 Jun 2017 19:28:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752972AbdFST2V (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 19 Jun 2017 15:28:21 -0400
Received: from wtarreau.pck.nerim.net ([62.212.114.60]:51985 &quot;EHLO 1wt.eu&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752855AbdFSSfN (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 19 Jun 2017 14:35:13 -0400
Received: (from willy@localhost)
	by pcw.home.local (8.15.2/8.15.2/Submit) id v5JIY542015134;
	Mon, 19 Jun 2017 20:34:05 +0200
From: Willy Tarreau &lt;w@1wt.eu&gt;
To: linux-kernel@vger.kernel.org, stable@vger.kernel.org, linux@roeck-us.net
Cc: Hugh Dickins &lt;hughd@google.com&gt;, Willy Tarreau &lt;w@1wt.eu&gt;
Subject: [PATCH 3.10 268/268] mm: larger stack guard gap, between vmas
Date: Mon, 19 Jun 2017 20:32:47 +0200
Message-Id: &lt;1497897167-14556-269-git-send-email-w@1wt.eu&gt;
X-Mailer: git-send-email 2.8.0.rc2.1.gbe9624a
In-Reply-To: &lt;1497897167-14556-1-git-send-email-w@1wt.eu&gt;
References: &lt;1497897167-14556-1-git-send-email-w@1wt.eu&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=197">Willy Tarreau</a> - June 19, 2017, 6:32 p.m.</div>
<pre class="content">
<span class="from">From: Hugh Dickins &lt;hughd@google.com&gt;</span>

commit 1be7107fbe18eed3e319a6c3e83c78254b693acb upstream.

Stack guard page is a useful feature to reduce a risk of stack smashing
into a different mapping. We have been using a single page gap which
is sufficient to prevent having stack adjacent to a different mapping.
But this seems to be insufficient in the light of the stack usage in
userspace. E.g. glibc uses as large as 64kB alloca() in many commonly
used functions. Others use constructs liks gid_t buffer[NGROUPS_MAX]
which is 256kB or stack strings with MAX_ARG_STRLEN.

This will become especially dangerous for suid binaries and the default
no limit for the stack size limit because those applications can be
tricked to consume a large portion of the stack and a single glibc call
could jump over the guard page. These attacks are not theoretical,
unfortunatelly.

Make those attacks less probable by increasing the stack guard gap
to 1MB (on systems with 4k pages; but make it depend on the page size
because systems with larger base pages might cap stack allocations in
the PAGE_SIZE units) which should cover larger alloca() and VLA stack
allocations. It is obviously not a full fix because the problem is
somehow inherent, but it should reduce attack space a lot.

One could argue that the gap size should be configurable from userspace,
but that can be done later when somebody finds that the new 1MB is wrong
for some special case applications.  For now, add a kernel command line
option (stack_guard_gap) to specify the stack gap size (in page units).

Implementation wise, first delete all the old code for stack guard page:
because although we could get away with accounting one extra page in a
stack vma, accounting a larger gap can break userspace - case in point,
a program run with &quot;ulimit -S -v 20000&quot; failed when the 1MB gap was
counted for RLIMIT_AS; similar problems could come with RLIMIT_MLOCK
and strict non-overcommit mode.

Instead of keeping gap inside the stack vma, maintain the stack guard
gap as a gap between vmas: using vm_start_gap() in place of vm_start
(or vm_end_gap() in place of vm_end if VM_GROWSUP) in just those few
places which need to respect the gap - mainly arch_get_unmapped_area(),
and and the vma tree&#39;s subtree_gap support for that.

Original-patch-by: Oleg Nesterov &lt;oleg@redhat.com&gt;
Original-patch-by: Michal Hocko &lt;mhocko@suse.com&gt;
<span class="signed-off-by">Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
[wt: backport to 4.11: adjust context]
[wt: backport to 4.9: adjust context ; kernel doc was not in admin-guide]
[wt: backport to 4.4: adjust context ; drop ppc hugetlb_radix changes]
[wt: backport to 3.18: adjust context ; no FOLL_POPULATE ;
     s390 uses generic arch_get_unmapped_area()]
[wt: backport to 3.16: adjust context]
[wt: backport to 3.10: adjust context ; code logic in PARISC&#39;s
     arch_get_unmapped_area() wasn&#39;t found ; code inserted into
     expand_upwards() and expand_downwards() runs under anon_vma lock;
     changes for gup.c:faultin_page go to memory.c:__get_user_pages()]
<span class="signed-off-by">Signed-off-by: Willy Tarreau &lt;w@1wt.eu&gt;</span>
---
 Documentation/kernel-parameters.txt |   7 ++
 arch/arc/mm/mmap.c                  |   2 +-
 arch/arm/mm/mmap.c                  |   4 +-
 arch/frv/mm/elf-fdpic.c             |   2 +-
 arch/mips/mm/mmap.c                 |   2 +-
 arch/powerpc/mm/slice.c             |   2 +-
 arch/sh/mm/mmap.c                   |   4 +-
 arch/sparc/kernel/sys_sparc_64.c    |   4 +-
 arch/sparc/mm/hugetlbpage.c         |   2 +-
 arch/tile/mm/hugetlbpage.c          |   2 +-
 arch/x86/kernel/sys_x86_64.c        |   4 +-
 arch/x86/mm/hugetlbpage.c           |   2 +-
 arch/xtensa/kernel/syscall.c        |   2 +-
 fs/hugetlbfs/inode.c                |   2 +-
 fs/proc/task_mmu.c                  |   4 -
 include/linux/mm.h                  |  54 ++++++------
 mm/memory.c                         |  52 +-----------
 mm/mmap.c                           | 161 +++++++++++++++++++++++-------------
 18 files changed, 155 insertions(+), 157 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index daf8382..ed0c7e3 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2889,6 +2889,13 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 	spia_pedr=
 	spia_peddr=
 
<span class="p_add">+	stack_guard_gap=	[MM]</span>
<span class="p_add">+			override the default stack gap protection. The value</span>
<span class="p_add">+			is in page units and it defines how many pages prior</span>
<span class="p_add">+			to (for stacks growing down) resp. after (for stacks</span>
<span class="p_add">+			growing up) the main stack are reserved for no other</span>
<span class="p_add">+			mapping. Default value is 256 pages.</span>
<span class="p_add">+</span>
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
<span class="p_header">diff --git a/arch/arc/mm/mmap.c b/arch/arc/mm/mmap.c</span>
<span class="p_header">index 2e06d56..cf4ae69 100644</span>
<span class="p_header">--- a/arch/arc/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arc/mm/mmap.c</span>
<span class="p_chunk">@@ -64,7 +64,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/arm/mm/mmap.c b/arch/arm/mm/mmap.c</span>
<span class="p_header">index 5ef506c..984509e 100644</span>
<span class="p_header">--- a/arch/arm/mm/mmap.c</span>
<span class="p_header">+++ b/arch/arm/mm/mmap.c</span>
<span class="p_chunk">@@ -89,7 +89,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/frv/mm/elf-fdpic.c b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">index 836f1470..efa59f1 100644</span>
<span class="p_header">--- a/arch/frv/mm/elf-fdpic.c</span>
<span class="p_header">+++ b/arch/frv/mm/elf-fdpic.c</span>
<span class="p_chunk">@@ -74,7 +74,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(current-&gt;mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			goto success;
 	}
 
<span class="p_header">diff --git a/arch/mips/mm/mmap.c b/arch/mips/mm/mmap.c</span>
<span class="p_header">index 7e5fe27..0bb4295 100644</span>
<span class="p_header">--- a/arch/mips/mm/mmap.c</span>
<span class="p_header">+++ b/arch/mips/mm/mmap.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static unsigned long arch_get_unmapped_area_common(struct file *filp,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/powerpc/mm/slice.c b/arch/powerpc/mm/slice.c</span>
<span class="p_header">index 7ce9cf3..887365a 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/slice.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/slice.c</span>
<span class="p_chunk">@@ -103,7 +103,7 @@</span> <span class="p_context"> static int slice_area_is_free(struct mm_struct *mm, unsigned long addr,</span>
 	if ((mm-&gt;task_size - len) &lt; addr)
 		return 0;
 	vma = find_vma(mm, addr);
<span class="p_del">-	return (!vma || (addr + len) &lt;= vma-&gt;vm_start);</span>
<span class="p_add">+	return (!vma || (addr + len) &lt;= vm_start_gap(vma));</span>
 }
 
 static int slice_low_has_vma(struct mm_struct *mm, unsigned long slice)
<span class="p_header">diff --git a/arch/sh/mm/mmap.c b/arch/sh/mm/mmap.c</span>
<span class="p_header">index 6777177..7df7d59 100644</span>
<span class="p_header">--- a/arch/sh/mm/mmap.c</span>
<span class="p_header">+++ b/arch/sh/mm/mmap.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/sparc/kernel/sys_sparc_64.c b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">index 666510b..66e1047 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/sys_sparc_64.c</span>
<span class="p_chunk">@@ -119,7 +119,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsi</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index d2b5944..ce49370 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -118,7 +118,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, HPAGE_SIZE);
 		vma = find_vma(mm, addr);
 		if (task_size - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/tile/mm/hugetlbpage.c b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">index 650ccff..c75eac7 100644</span>
<span class="p_header">--- a/arch/tile/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/tile/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -297,7 +297,7 @@</span> <span class="p_context"> unsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (current-&gt;mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 30277e2..d050393 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -127,7 +127,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (end - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 		addr = PAGE_ALIGN(addr);
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index ae1aa71..6adf3d9 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -341,7 +341,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 	if (mm-&gt;get_unmapped_area == arch_get_unmapped_area)
<span class="p_header">diff --git a/arch/xtensa/kernel/syscall.c b/arch/xtensa/kernel/syscall.c</span>
<span class="p_header">index 5d3f7a1..1ff0b92 100644</span>
<span class="p_header">--- a/arch/xtensa/kernel/syscall.c</span>
<span class="p_header">+++ b/arch/xtensa/kernel/syscall.c</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		/* At this point:  (!vmm || addr &lt; vmm-&gt;vm_end). */
 		if (TASK_SIZE - len &lt; addr)
 			return -ENOMEM;
<span class="p_del">-		if (!vmm || addr + len &lt;= vmm-&gt;vm_start)</span>
<span class="p_add">+		if (!vmm || addr + len &lt;= vm_start_gap(vmm))</span>
 			return addr;
 		addr = vmm-&gt;vm_end;
 		if (flags &amp; MAP_SHARED)
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 4e5f332..db7d89c 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -169,7 +169,7 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 		addr = ALIGN(addr, huge_page_size(h));
 		vma = find_vma(mm, addr);
 		if (TASK_SIZE - len &gt;= addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)))</span>
 			return addr;
 	}
 
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index b86db12..972cdc2 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -279,11 +279,7 @@</span> <span class="p_context"> show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)</span>
 
 	/* We don&#39;t show the stack guard page in /proc/maps */
 	start = vma-&gt;vm_start;
<span class="p_del">-	if (stack_guard_page_start(vma, start))</span>
<span class="p_del">-		start += PAGE_SIZE;</span>
 	end = vma-&gt;vm_end;
<span class="p_del">-	if (stack_guard_page_end(vma, end))</span>
<span class="p_del">-		end -= PAGE_SIZE;</span>
 
 	seq_printf(m, &quot;%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n&quot;,
 			start,
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 55590f4..47f1604 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1069,34 +1069,6 @@</span> <span class="p_context"> int set_page_dirty(struct page *page);</span>
 int set_page_dirty_lock(struct page *page);
 int clear_page_dirty_for_io(struct page *page);
 
<span class="p_del">-/* Is the vma a continuation of the stack vma above it? */</span>
<span class="p_del">-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_end == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSDOWN);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_start(struct vm_area_struct *vma,</span>
<span class="p_del">-					     unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_start == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsdown(vma-&gt;vm_prev, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* Is the vma a continuation of the stack vma below it? */</span>
<span class="p_del">-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return vma &amp;&amp; (vma-&gt;vm_start == addr) &amp;&amp; (vma-&gt;vm_flags &amp; VM_GROWSUP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int stack_guard_page_end(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp;</span>
<span class="p_del">-		(vma-&gt;vm_end == addr) &amp;&amp;</span>
<span class="p_del">-		!vma_growsup(vma-&gt;vm_next, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 extern pid_t
 vm_is_stack(struct task_struct *task, struct vm_area_struct *vma, int in_group);
 
<span class="p_chunk">@@ -1622,6 +1594,8 @@</span> <span class="p_context"> unsigned long ra_submit(struct file_ra_state *ra,</span>
 			struct address_space *mapping,
 			struct file *filp);
 
<span class="p_add">+extern unsigned long stack_guard_gap;</span>
<span class="p_add">+</span>
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
 
<span class="p_chunk">@@ -1650,6 +1624,30 @@</span> <span class="p_context"> static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * m</span>
 	return vma;
 }
 
<span class="p_add">+static inline unsigned long vm_start_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_start = vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN) {</span>
<span class="p_add">+		vm_start -= stack_guard_gap;</span>
<span class="p_add">+		if (vm_start &gt; vma-&gt;vm_start)</span>
<span class="p_add">+			vm_start = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_start;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long vm_end_gap(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_GROWSUP) {</span>
<span class="p_add">+		vm_end += stack_guard_gap;</span>
<span class="p_add">+		if (vm_end &lt; vma-&gt;vm_end)</span>
<span class="p_add">+			vm_end = -PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return vm_end;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long vma_pages(struct vm_area_struct *vma)
 {
 	return (vma-&gt;vm_end - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 2ca2ee1..0508aef 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1654,12 +1654,6 @@</span> <span class="p_context"> no_page_table:</span>
 	return page;
 }
 
<span class="p_del">-static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return stack_guard_page_start(vma, addr) ||</span>
<span class="p_del">-	       stack_guard_page_end(vma, addr+PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /**
  * __get_user_pages() - pin user pages in memory
  * @tsk:	task_struct of target task
<span class="p_chunk">@@ -1827,11 +1821,9 @@</span> <span class="p_context"> long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 				int ret;
 				unsigned int fault_flags = 0;
 
<span class="p_del">-				/* For mlock, just skip the stack guard page. */</span>
<span class="p_del">-				if (foll_flags &amp; FOLL_MLOCK) {</span>
<span class="p_del">-					if (stack_guard_page(vma, start))</span>
<span class="p_del">-						goto next_page;</span>
<span class="p_del">-				}</span>
<span class="p_add">+				/* mlock all present pages, but do not fault in new pages */</span>
<span class="p_add">+				if (foll_flags &amp; FOLL_MLOCK)</span>
<span class="p_add">+					goto next_page;</span>
 				if (foll_flags &amp; FOLL_WRITE)
 					fault_flags |= FAULT_FLAG_WRITE;
 				if (nonblocking)
<span class="p_chunk">@@ -3192,40 +3184,6 @@</span> <span class="p_context"> out_release:</span>
 }
 
 /*
<span class="p_del">- * This is like a special single-page &quot;expand_{down|up}wards()&quot;,</span>
<span class="p_del">- * except we must first make sure that &#39;address{-|+}PAGE_SIZE&#39;</span>
<span class="p_del">- * doesn&#39;t hit another vma.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSDOWN) &amp;&amp; address == vma-&gt;vm_start) {</span>
<span class="p_del">-		struct vm_area_struct *prev = vma-&gt;vm_prev;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is there a mapping abutting this one below?</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * That&#39;s only ok if it&#39;s the same stack mapping</span>
<span class="p_del">-		 * that has gotten split..</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (prev &amp;&amp; prev-&gt;vm_end == address)</span>
<span class="p_del">-			return prev-&gt;vm_flags &amp; VM_GROWSDOWN ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_downwards(vma, address - PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_GROWSUP) &amp;&amp; address + PAGE_SIZE == vma-&gt;vm_end) {</span>
<span class="p_del">-		struct vm_area_struct *next = vma-&gt;vm_next;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* As VM_GROWSDOWN but s/below/above/ */</span>
<span class="p_del">-		if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE)</span>
<span class="p_del">-			return next-&gt;vm_flags &amp; VM_GROWSUP ? 0 : -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-		return expand_upwards(vma, address + PAGE_SIZE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
<span class="p_chunk">@@ -3244,10 +3202,6 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_flags &amp; VM_SHARED)
 		return VM_FAULT_SIGBUS;
 
<span class="p_del">-	/* Check if we need to add a guard page to the stack */</span>
<span class="p_del">-	if (check_stack_guard_page(vma, address) &lt; 0)</span>
<span class="p_del">-		return VM_FAULT_SIGSEGV;</span>
<span class="p_del">-</span>
 	/* Use the zero-page for reads */
 	if (!(flags &amp; FAULT_FLAG_WRITE)) {
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 70ff9b4..62d3208 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -263,6 +263,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long rlim, retval;
 	unsigned long newbrk, oldbrk;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	struct vm_area_struct *next;</span>
 	unsigned long min_brk;
 	bool populate;
 
<span class="p_chunk">@@ -308,7 +309,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	}
 
 	/* Check against existing mmap mappings. */
<span class="p_del">-	if (find_vma_intersection(mm, oldbrk, newbrk+PAGE_SIZE))</span>
<span class="p_add">+	next = find_vma(mm, oldbrk);</span>
<span class="p_add">+	if (next &amp;&amp; newbrk + PAGE_SIZE &gt; vm_start_gap(next))</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_chunk">@@ -331,10 +333,22 @@</span> <span class="p_context"> out:</span>
 
 static long vma_compute_subtree_gap(struct vm_area_struct *vma)
 {
<span class="p_del">-	unsigned long max, subtree_gap;</span>
<span class="p_del">-	max = vma-&gt;vm_start;</span>
<span class="p_del">-	if (vma-&gt;vm_prev)</span>
<span class="p_del">-		max -= vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_add">+	unsigned long max, prev_end, subtree_gap;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we</span>
<span class="p_add">+	 * allow two stack_guard_gaps between them here, and when choosing</span>
<span class="p_add">+	 * an unmapped area; whereas when expanding we only require one.</span>
<span class="p_add">+	 * That&#39;s a little inconsistent, but keeps the code here simpler.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max = vm_start_gap(vma);</span>
<span class="p_add">+	if (vma-&gt;vm_prev) {</span>
<span class="p_add">+		prev_end = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+		if (max &gt; prev_end)</span>
<span class="p_add">+			max -= prev_end;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			max = 0;</span>
<span class="p_add">+	}</span>
 	if (vma-&gt;vm_rb.rb_left) {
 		subtree_gap = rb_entry(vma-&gt;vm_rb.rb_left,
 				struct vm_area_struct, vm_rb)-&gt;rb_subtree_gap;
<span class="p_chunk">@@ -418,7 +432,7 @@</span> <span class="p_context"> void validate_mm(struct mm_struct *mm)</span>
 		list_for_each_entry(avc, &amp;vma-&gt;anon_vma_chain, same_vma)
 			anon_vma_interval_tree_verify(avc);
 		vma_unlock_anon_vma(vma);
<span class="p_del">-		highest_address = vma-&gt;vm_end;</span>
<span class="p_add">+		highest_address = vm_end_gap(vma);</span>
 		vma = vma-&gt;vm_next;
 		i++;
 	}
<span class="p_chunk">@@ -586,7 +600,7 @@</span> <span class="p_context"> void __vma_link_rb(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (vma-&gt;vm_next)
 		vma_gap_update(vma-&gt;vm_next);
 	else
<span class="p_del">-		mm-&gt;highest_vm_end = vma-&gt;vm_end;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 
 	/*
 	 * vma-&gt;vm_prev wasn&#39;t known when we followed the rbtree to find the
<span class="p_chunk">@@ -835,7 +849,7 @@</span> <span class="p_context"> again:			remove_next = 1 + (end &gt; next-&gt;vm_end);</span>
 			vma_gap_update(vma);
 		if (end_changed) {
 			if (!next)
<span class="p_del">-				mm-&gt;highest_vm_end = end;</span>
<span class="p_add">+				mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 			else if (!adjust_next)
 				vma_gap_update(next);
 		}
<span class="p_chunk">@@ -1670,7 +1684,7 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_unmapped_area_info *info)</span>
 
 	while (true) {
 		/* Visit left subtree if it looks promising */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &gt;= low_limit &amp;&amp; vma-&gt;vm_rb.rb_left) {
 			struct vm_area_struct *left =
 				rb_entry(vma-&gt;vm_rb.rb_left,
<span class="p_chunk">@@ -1681,7 +1695,7 @@</span> <span class="p_context"> unsigned long unmapped_area(struct vm_unmapped_area_info *info)</span>
 			}
 		}
 
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start &gt; high_limit)
<span class="p_chunk">@@ -1708,8 +1722,8 @@</span> <span class="p_context"> check_current:</span>
 			vma = rb_entry(rb_parent(prev),
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_left) {
<span class="p_del">-				gap_start = vma-&gt;vm_prev-&gt;vm_end;</span>
<span class="p_del">-				gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+				gap_start = vm_end_gap(vma-&gt;vm_prev);</span>
<span class="p_add">+				gap_end = vm_start_gap(vma);</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1773,7 +1787,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)</span>
 
 	while (true) {
 		/* Visit right subtree if it looks promising */
<span class="p_del">-		gap_start = vma-&gt;vm_prev ? vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+		gap_start = vma-&gt;vm_prev ? vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 		if (gap_start &lt;= high_limit &amp;&amp; vma-&gt;vm_rb.rb_right) {
 			struct vm_area_struct *right =
 				rb_entry(vma-&gt;vm_rb.rb_right,
<span class="p_chunk">@@ -1786,7 +1800,7 @@</span> <span class="p_context"> unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)</span>
 
 check_current:
 		/* Check if current node has a suitable gap */
<span class="p_del">-		gap_end = vma-&gt;vm_start;</span>
<span class="p_add">+		gap_end = vm_start_gap(vma);</span>
 		if (gap_end &lt; low_limit)
 			return -ENOMEM;
 		if (gap_start &lt;= high_limit &amp;&amp; gap_end - gap_start &gt;= length)
<span class="p_chunk">@@ -1812,7 +1826,7 @@</span> <span class="p_context"> check_current:</span>
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma-&gt;vm_rb.rb_right) {
 				gap_start = vma-&gt;vm_prev ?
<span class="p_del">-					vma-&gt;vm_prev-&gt;vm_end : 0;</span>
<span class="p_add">+					vm_end_gap(vma-&gt;vm_prev) : 0;</span>
 				goto check_current;
 			}
 		}
<span class="p_chunk">@@ -1850,7 +1864,7 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 		unsigned long len, unsigned long pgoff, unsigned long flags)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct vm_unmapped_area_info info;
 
 	if (len &gt; TASK_SIZE - mmap_min_addr)
<span class="p_chunk">@@ -1861,9 +1875,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-		    (!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+		    (!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+		    (!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -1895,7 +1910,7 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 			  const unsigned long len, const unsigned long pgoff,
 			  const unsigned long flags)
 {
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct vm_area_struct *vma, *prev;</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
<span class="p_chunk">@@ -1910,9 +1925,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	/* requesting a specific address */
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
<span class="p_del">-		vma = find_vma(mm, addr);</span>
<span class="p_add">+		vma = find_vma_prev(mm, addr, &amp;prev);</span>
 		if (TASK_SIZE - len &gt;= addr &amp;&amp; addr &gt;= mmap_min_addr &amp;&amp;
<span class="p_del">-				(!vma || addr + len &lt;= vma-&gt;vm_start))</span>
<span class="p_add">+				(!vma || addr + len &lt;= vm_start_gap(vma)) &amp;&amp;</span>
<span class="p_add">+				(!prev || addr &gt;= vm_end_gap(prev)))</span>
 			return addr;
 	}
 
<span class="p_chunk">@@ -2052,21 +2068,19 @@</span> <span class="p_context"> find_vma_prev(struct mm_struct *mm, unsigned long addr,</span>
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
<span class="p_del">-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)</span>
<span class="p_add">+static int acct_stack_growth(struct vm_area_struct *vma,</span>
<span class="p_add">+			     unsigned long size, unsigned long grow)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct rlimit *rlim = current-&gt;signal-&gt;rlim;
<span class="p_del">-	unsigned long new_start, actual_size;</span>
<span class="p_add">+	unsigned long new_start;</span>
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, grow))
 		return -ENOMEM;
 
 	/* Stack limit test */
<span class="p_del">-	actual_size = size;</span>
<span class="p_del">-	if (size &amp;&amp; (vma-&gt;vm_flags &amp; (VM_GROWSUP | VM_GROWSDOWN)))</span>
<span class="p_del">-		actual_size -= PAGE_SIZE;</span>
<span class="p_del">-	if (actual_size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
<span class="p_add">+	if (size &gt; ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur))</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_chunk">@@ -2107,6 +2121,8 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, uns</span>
  */
 int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *next;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
 	if (!(vma-&gt;vm_flags &amp; VM_GROWSUP))
<span class="p_chunk">@@ -2126,14 +2142,29 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 * Also guard against wrapping around to address 0.
 	 */
<span class="p_del">-	if (address &lt; PAGE_ALIGN(address+4))</span>
<span class="p_del">-		address = PAGE_ALIGN(address+4);</span>
<span class="p_del">-	else {</span>
<span class="p_add">+	address &amp;= PAGE_MASK;</span>
<span class="p_add">+	address += PAGE_SIZE;</span>
<span class="p_add">+	if (!address) {</span>
 		vma_unlock_anon_vma(vma);
 		return -ENOMEM;
 	}
 	error = 0;
 
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address + stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &lt; address) {</span>
<span class="p_add">+		vma_unlock_anon_vma(vma);</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	next = vma-&gt;vm_next;</span>
<span class="p_add">+	if (next &amp;&amp; next-&gt;vm_start &lt; gap_addr) {</span>
<span class="p_add">+		if (!(next-&gt;vm_flags &amp; VM_GROWSUP)) {</span>
<span class="p_add">+			vma_unlock_anon_vma(vma);</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Somebody else might have raced and expanded it already */
 	if (address &gt; vma-&gt;vm_end) {
 		unsigned long size, grow;
<span class="p_chunk">@@ -2163,7 +2194,7 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 				if (vma-&gt;vm_next)
 					vma_gap_update(vma-&gt;vm_next);
 				else
<span class="p_del">-					vma-&gt;vm_mm-&gt;highest_vm_end = address;</span>
<span class="p_add">+					vma-&gt;vm_mm-&gt;highest_vm_end = vm_end_gap(vma);</span>
 				spin_unlock(&amp;vma-&gt;vm_mm-&gt;page_table_lock);
 
 				perf_event_mmap(vma);
<span class="p_chunk">@@ -2183,6 +2214,8 @@</span> <span class="p_context"> int expand_upwards(struct vm_area_struct *vma, unsigned long address)</span>
 int expand_downwards(struct vm_area_struct *vma,
 				   unsigned long address)
 {
<span class="p_add">+	struct vm_area_struct *prev;</span>
<span class="p_add">+	unsigned long gap_addr;</span>
 	int error;
 
 	/*
<span class="p_chunk">@@ -2199,6 +2232,27 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 
 	vma_lock_anon_vma(vma);
 
<span class="p_add">+	/* Enforce stack_guard_gap */</span>
<span class="p_add">+	gap_addr = address - stack_guard_gap;</span>
<span class="p_add">+	if (gap_addr &gt; address) {</span>
<span class="p_add">+		vma_unlock_anon_vma(vma);</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	prev = vma-&gt;vm_prev;</span>
<span class="p_add">+	if (prev &amp;&amp; prev-&gt;vm_end &gt; gap_addr) {</span>
<span class="p_add">+		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN)) {</span>
<span class="p_add">+			vma_unlock_anon_vma(vma);</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* Check that both stack segments have the same anon_vma? */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We must make sure the anon_vma is allocated. */</span>
<span class="p_add">+	if (unlikely(anon_vma_prepare(vma))) {</span>
<span class="p_add">+		vma_unlock_anon_vma(vma);</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * vma-&gt;vm_start/vm_end cannot change under us because the caller
 	 * is required to hold the mmap_sem in read mode.  We need the
<span class="p_chunk">@@ -2245,28 +2299,25 @@</span> <span class="p_context"> int expand_downwards(struct vm_area_struct *vma,</span>
 	return error;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Note how expand_stack() refuses to expand the stack all the way to</span>
<span class="p_del">- * abut the next virtual mapping, *unless* that mapping itself is also</span>
<span class="p_del">- * a stack mapping. We want to leave room for a guard page, after all</span>
<span class="p_del">- * (the guard page itself is not added here, that is done by the</span>
<span class="p_del">- * actual page faulting logic)</span>
<span class="p_del">- *</span>
<span class="p_del">- * This matches the behavior of the guard page logic (see mm/memory.c:</span>
<span class="p_del">- * check_stack_guard_page()), which only allows the guard page to be</span>
<span class="p_del">- * removed under these circumstances.</span>
<span class="p_del">- */</span>
<span class="p_add">+/* enforced gap between the expanding stack and other mappings. */</span>
<span class="p_add">+unsigned long stack_guard_gap = 256UL&lt;&lt;PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init cmdline_parse_stack_guard_gap(char *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long val;</span>
<span class="p_add">+	char *endptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = simple_strtoul(p, &amp;endptr, 10);</span>
<span class="p_add">+	if (!*endptr)</span>
<span class="p_add">+		stack_guard_gap = val &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+__setup(&quot;stack_guard_gap=&quot;, cmdline_parse_stack_guard_gap);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_STACK_GROWSUP
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *next;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	next = vma-&gt;vm_next;</span>
<span class="p_del">-	if (next &amp;&amp; next-&gt;vm_start == address + PAGE_SIZE) {</span>
<span class="p_del">-		if (!(next-&gt;vm_flags &amp; VM_GROWSUP))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_upwards(vma, address);
 }
 
<span class="p_chunk">@@ -2288,14 +2339,6 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 #else
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
<span class="p_del">-	struct vm_area_struct *prev;</span>
<span class="p_del">-</span>
<span class="p_del">-	address &amp;= PAGE_MASK;</span>
<span class="p_del">-	prev = vma-&gt;vm_prev;</span>
<span class="p_del">-	if (prev &amp;&amp; prev-&gt;vm_end == address) {</span>
<span class="p_del">-		if (!(prev-&gt;vm_flags &amp; VM_GROWSDOWN))</span>
<span class="p_del">-			return -ENOMEM;</span>
<span class="p_del">-	}</span>
 	return expand_downwards(vma, address);
 }
 
<span class="p_chunk">@@ -2392,7 +2435,7 @@</span> <span class="p_context"> detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		vma-&gt;vm_prev = prev;
 		vma_gap_update(vma);
 	} else
<span class="p_del">-		mm-&gt;highest_vm_end = prev ? prev-&gt;vm_end : 0;</span>
<span class="p_add">+		mm-&gt;highest_vm_end = prev ? vm_end_gap(prev) : 0;</span>
 	tail_vma-&gt;vm_next = NULL;
 	if (mm-&gt;unmap_area == arch_unmap_area)
 		addr = prev ? prev-&gt;vm_end : mm-&gt;mmap_base;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



