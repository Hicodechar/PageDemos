
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,5/5] arm64: add KASan support - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,5/5] arm64: add KASan support</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 15, 2015, 1:59 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1431698344-28054-6-git-send-email-a.ryabinin@samsung.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6414301/mbox/"
   >mbox</a>
|
   <a href="/patch/6414301/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6414301/">/patch/6414301/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id F224C9F32E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 13:59:53 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 84E0720524
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 13:59:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id D009420529
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 15 May 2015 13:59:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S2992441AbbEON7m (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 15 May 2015 09:59:42 -0400
Received: from mailout2.w1.samsung.com ([210.118.77.12]:38370 &quot;EHLO
	mailout2.w1.samsung.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1946059AbbEON7W (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 15 May 2015 09:59:22 -0400
Received: from eucpsbgm1.samsung.com (unknown [203.254.199.244])
	by mailout2.w1.samsung.com
	(Oracle Communications Messaging Server 7.0.5.31.0 64bit (built May 5
	2014)) with ESMTP id &lt;0NOE00F4G9IWRE80@mailout2.w1.samsung.com&gt; for
	linux-kernel@vger.kernel.org; Fri, 15 May 2015 14:59:20 +0100 (BST)
X-AuditID: cbfec7f4-f79c56d0000012ee-7b-5555fbb868e1
Received: from eusync2.samsung.com ( [203.254.199.212])
	by eucpsbgm1.samsung.com (EUCPMTA) with SMTP id A0.1A.04846.8BBF5555;
	Fri, 15 May 2015 14:59:20 +0100 (BST)
Received: from localhost.localdomain ([106.109.129.143])
	by eusync2.samsung.com (Oracle Communications Messaging Server
	7.0.5.31.0 64bit (built May  5 2014))
	with ESMTPA id &lt;0NOE00IC69IMHU60@eusync2.samsung.com&gt;; Fri,
	15 May 2015 14:59:20 +0100 (BST)
From: Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt;
To: linux-kernel@vger.kernel.org
Cc: Dmitry Vyukov &lt;dvyukov@google.com&gt;,
	Alexander Potapenko &lt;glider@google.com&gt;,
	David Keitel &lt;dkeitel@codeaurora.org&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	linux-arm-kernel@lists.infradead.org, linux-mm@kvack.org,
	Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt;
Subject: [PATCH v2 5/5] arm64: add KASan support
Date: Fri, 15 May 2015 16:59:04 +0300
Message-id: &lt;1431698344-28054-6-git-send-email-a.ryabinin@samsung.com&gt;
X-Mailer: git-send-email 2.4.0
In-reply-to: &lt;1431698344-28054-1-git-send-email-a.ryabinin@samsung.com&gt;
References: &lt;1431698344-28054-1-git-send-email-a.ryabinin@samsung.com&gt;
X-Brightmail-Tracker: H4sIAAAAAAAAA+NgFtrFLMWRmVeSWpSXmKPExsVy+t/xK7o7foeGGvQ+ErbY9usRm8Wc9WvY
	LP5OOsZu8X5ZD6PF7kvPmC0mPGxjt2j/uJfZYtPja6wWl3fNYbO4t+Y/q8XLjydYHLg91sxb
	w+jx+9ckRo/Lfb1MHgs2lXps+jSJ3ePEjN8sHpuX1Hv0bVnF6PF5k1wAZxSXTUpqTmZZapG+
	XQJXxvOPvAUv8isOvexlaWB8E93FyMkhIWAi0dQziQXCFpO4cG89WxcjF4eQwFJGiWfdU1gh
	nCYmifY7t9hBqtgE9CT+zdrOBmKLCChIbO59BlbELPAPqGhhP9goYQEDiYU79jOC2CwCqhL3
	5jWA2bwCbhKvT32AWicncXbSArA4p4C7xNye42ALhIBq7u2ZxjiBkXcBI8MqRtHU0uSC4qT0
	XEO94sTc4tK8dL3k/NxNjJAQ/bKDcfExq0OMAhyMSjy8N1aGhgqxJpYVV+YeYpTgYFYS4f31
	AijEm5JYWZValB9fVJqTWnyIUZqDRUmcd+6u9yFCAumJJanZqakFqUUwWSYOTqkGRq6G6z/a
	1Bfe292xrSzN8+9NtUvNYYvO3G5nWx/47eMGTtHpERqPGXP2bP5fwqIhqKHFJMavfnff8zfB
	ziqimZMqn64oMtyh/dbjn/WfQ/67ImRVWq0cxYq5Mq8ZvBHgPLVVlHVjWPqlRHFdnlUrTJN2
	9lStPiT+R/mQ5vfFE6z0bdJuPb9zQomlOCPRUIu5qDgRAMyOwQxNAgAA
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - May 15, 2015, 1:59 p.m.</div>
<pre class="content">
This patch adds arch specific code for kernel address sanitizer
(see Documentation/kasan.txt).

1/8 of kernel addresses reserved for shadow memory. There was no
big enough hole for this, so virtual addresses for shadow were
stolen from vmalloc area.

At early boot stage the whole shadow region populated with just
one physical page (kasan_zero_page). Later, this page reused
as readonly zero shadow for some memory that KASan currently
don&#39;t track (vmalloc).
After mapping the physical memory, pages for shadow memory are
allocated and mapped.

KASan&#39;s stack instrumentation significantly increases stack&#39;s
consumption, so CONFIG_KASAN doubles THREAD_SIZE.

Functions like memset/memmove/memcpy do a lot of memory accesses.
If bad pointer passed to one of these function it is important
to catch this. Compiler&#39;s instrumentation cannot do this since
these functions are written in assembly.
KASan replaces memory functions with manually instrumented variants.
Original functions declared as weak symbols so strong definitions
in mm/kasan/kasan.c could replace them. Original functions have aliases
with &#39;__&#39; prefix in name, so we could call non-instrumented variant
if needed.
Some files built without kasan instrumentation (e.g. mm/slub.c).
Original mem* function replaced (via #define) with prefixed variants
to disable memory access checks for such files.
<span class="signed-off-by">
Signed-off-by: Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt;</span>
---
 arch/arm64/Kconfig                   |   7 ++
 arch/arm64/include/asm/kasan.h       |  24 ++++++
 arch/arm64/include/asm/pgtable.h     |   7 ++
 arch/arm64/include/asm/string.h      |  16 ++++
 arch/arm64/include/asm/thread_info.h |   8 ++
 arch/arm64/kernel/head.S             |   3 +
 arch/arm64/kernel/module.c           |  16 +++-
 arch/arm64/kernel/setup.c            |   2 +
 arch/arm64/lib/memcpy.S              |   3 +
 arch/arm64/lib/memmove.S             |   7 +-
 arch/arm64/lib/memset.S              |   3 +
 arch/arm64/mm/Makefile               |   3 +
 arch/arm64/mm/kasan_init.c           | 143 +++++++++++++++++++++++++++++++++++
 13 files changed, 237 insertions(+), 5 deletions(-)
 create mode 100644 arch/arm64/include/asm/kasan.h
 create mode 100644 arch/arm64/mm/kasan_init.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - May 27, 2015, 12:40 p.m.</div>
<pre class="content">
On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:
<span class="quote">
&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt; (see Documentation/kasan.txt).</span>

OK fixed a newer GCC (4.9.3, so still just KASAN_OUTLINE), compiled
and booted on the ARM Juno Development System:

kasan test: kmalloc_large_oob_rigth kmalloc large allocation:
out-of-bounds to right
==================================================================
BUG: KASan: out of bounds access in kmalloc_large_oob_rigth+0x60/0x78
at addr ffffffc06516a00a
Write of size 1 by task swapper/0/1
page:ffffffbdc3945a00 count:1 mapcount:0 mapping:          (null) index:0x0
flags: 0x4000(head)
page dumped because: kasan: bad access detected
CPU: 2 PID: 1 Comm: swapper/0 Tainted: G    B           4.1.0-rc4+ #9
Hardware name: ARM Juno development board (r0) (DT)
Call trace:
[&lt;ffffffc00008aea8&gt;] dump_backtrace+0x0/0x15c
[&lt;ffffffc00008b014&gt;] show_stack+0x10/0x1c
[&lt;ffffffc00080997c&gt;] dump_stack+0xac/0x104
[&lt;ffffffc0001ea4d8&gt;] kasan_report_error+0x3e4/0x400
[&lt;ffffffc0001ea5dc&gt;] kasan_report+0x40/0x4c
[&lt;ffffffc0001e9a8c&gt;] __asan_store1+0x70/0x78
[&lt;ffffffc000a5ae78&gt;] kmalloc_large_oob_rigth+0x5c/0x78
[&lt;ffffffc000a5b6c0&gt;] kmalloc_tests_init+0x14/0x4c
[&lt;ffffffc000082940&gt;] do_one_initcall+0xa0/0x1f4
[&lt;ffffffc000a3bdbc&gt;] kernel_init_freeable+0x1ec/0x294
[&lt;ffffffc000804c5c&gt;] kernel_init+0xc/0xec
Memory state around the buggy address:
 ffffffc065169f00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
 ffffffc065169f80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
<span class="quote">&gt;ffffffc06516a000: 00 02 fe fe fe fe fe fe fe fe fe fe fe fe fe fe</span>
                      ^
 ffffffc06516a080: fe fe fe fe fe fe fe fe fe fe fe fe fe fe fe fe
 ffffffc06516a100: fe fe fe fe fe fe fe fe fe fe fe fe fe fe fe fe
==================================================================
kasan test: kmalloc_oob_krealloc_more out-of-bounds after krealloc more
==================================================================
BUG: KASan: out of bounds access in
kmalloc_oob_krealloc_more+0xa0/0xc0 at addr ffffffc06501cd93
Write of size 1 by task swapper/0/1
=============================================================================
BUG kmalloc-64 (Tainted: G    B          ): kasan: bad access detected
-----------------------------------------------------------------------------

INFO: Allocated in kmalloc_oob_krealloc_more+0x48/0xc0 age=4 cpu=2 pid=1
        alloc_debug_processing+0x170/0x17c
        __slab_alloc.isra.59.constprop.61+0x34c/0x36c
        kmem_cache_alloc+0x1a4/0x1e0
        kmalloc_oob_krealloc_more+0x44/0xc0
        kmalloc_tests_init+0x18/0x4c
        do_one_initcall+0xa0/0x1f4
        kernel_init_freeable+0x1ec/0x294
        kernel_init+0xc/0xec
        ret_from_fork+0xc/0x50
INFO: Slab 0xffffffbdc3940700 objects=21 used=19 fp=0xffffffc06501d080
flags=0x4080
INFO: Object 0xffffffc06501cd80 @offset=3456 fp=0xffffffc06501cf00

Bytes b4 ffffffc06501cd70: 00 08 00 00 08 08 01 01 00 00 00 00 02 10
00 00  ................
Object ffffffc06501cd80: 00 cf 01 65 c0 ff ff ff 01 04 0c 00 01 04 10
c1  ...e............
Object ffffffc06501cd90: 00 82 60 28 58 01 04 43 98 48 48 24 01 81 b4
40  ..`(X..C.HH$...@
Object ffffffc06501cda0: 00 80 09 0a 69 a1 3d 82 08 01 34 65 21 31 b0
00  ....i.=...4e!1..
Object ffffffc06501cdb0: 04 42 4d a7 10 26 18 52 27 23 c2 1e 08 01 40
81  .BM..&amp;.R&#39;#....@.
Padding ffffffc06501cef0: 81 20 00 50 00 08 00 0b 00 0c 50 40 01 48 40
42  . .P......P@.H@B
CPU: 2 PID: 1 Comm: swapper/0 Tainted: G    B           4.1.0-rc4+ #9
Hardware name: ARM Juno development board (r0) (DT)
Call trace:
[&lt;ffffffc00008aea8&gt;] dump_backtrace+0x0/0x15c
[&lt;ffffffc00008b014&gt;] show_stack+0x10/0x1c
[&lt;ffffffc00080997c&gt;] dump_stack+0xac/0x104
[&lt;ffffffc0001e3940&gt;] print_trailer+0xdc/0x140
[&lt;ffffffc0001e8384&gt;] object_err+0x38/0x4c
[&lt;ffffffc0001ea2a4&gt;] kasan_report_error+0x1b0/0x400
[&lt;ffffffc0001ea5dc&gt;] kasan_report+0x40/0x4c
[&lt;ffffffc0001e9a8c&gt;] __asan_store1+0x70/0x78
[&lt;ffffffc000a5b3a4&gt;] kmalloc_oob_krealloc_more+0x9c/0xc0
[&lt;ffffffc000a5b6c4&gt;] kmalloc_tests_init+0x18/0x4c
[&lt;ffffffc000082940&gt;] do_one_initcall+0xa0/0x1f4
[&lt;ffffffc000a3bdbc&gt;] kernel_init_freeable+0x1ec/0x294
[&lt;ffffffc000804c5c&gt;] kernel_init+0xc/0xec
Memory state around the buggy address:
 ffffffc06501cc80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
 ffffffc06501cd00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
<span class="quote">&gt;ffffffc06501cd80: 00 00 03 fc fc fc fc fc fc fc fc fc fc fc fc fc</span>
                         ^
 ffffffc06501ce00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
 ffffffc06501ce80: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc

(etc)

This is how it should look I guess, so:
<span class="tested-by">Tested-by: Linus Walleij &lt;linus.walleij@linaro.org&gt;</span>

Now I have to fix all the naturally occuring KASan OOB bugs
that started to appear in my boot crawl :O

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - June 11, 2015, 1:39 p.m.</div>
<pre class="content">
On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:
<span class="quote">
&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt; (see Documentation/kasan.txt).</span>

I looked closer at this again ... I am trying to get KASan up for
ARM(32) with some tricks and hacks.
<span class="quote">
&gt; +config KASAN_SHADOW_OFFSET</span>
<span class="quote">&gt; +       hex</span>
<span class="quote">&gt; +       default 0xdfff200000000000 if ARM64_VA_BITS_48</span>
<span class="quote">&gt; +       default 0xdffffc8000000000 if ARM64_VA_BITS_42</span>
<span class="quote">&gt; +       default 0xdfffff9000000000 if ARM64_VA_BITS_39</span>

So IIUC these offsets are simply chosen to satisfy the equation

        SHADOW = (void *)((unsigned long)addr &gt;&gt; KASAN_SHADOW_SCALE_SHIFT)
                + KASAN_SHADOW_OFFSET;

For all memory that needs to be covered, i.e. kernel text+data,
modules text+data, any other kernel-mode running code+data.

And it needs to be statically assigned like this because the offset
is used at compile time.

Atleast that is how I understood it... correct me if wrong.
(Dunno if all is completely obvious to everyone else in the world...)
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="quote">&gt; + * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="quote">&gt; +#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>

Will this not mean that shadow start to end actually covers *all*
virtual addresses including userspace and what not? However a
large portion of this shadow memory will be unused because the
SHADOW_OFFSET only works for code compiled for the kernel
anyway.

When working on ARM32 I certainly could not map
(1UL &lt;&lt; (VA_BITS -3)) i.e. for 32 bit (1UL &lt;&lt; 29) bytes (512 MB) of
virtual memory for shadow, instead I had to restrict it to the size that
actually maps the memory used by the kernel.

I tried shrinking it down but it crashed on me so tell me how
wrong I am ... :)

But here comes the real tricks, where I need some help to
understand the patch set, maybe some comments should be
inserted here and there to ease understanding:
<span class="quote">
&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; @@ -0,0 +1,143 @@</span>
<span class="quote">&gt; +#include &lt;linux/kasan.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memblock.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/start_kernel.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/page.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;</span>
<span class="quote">&gt; +static pgd_t tmp_page_table[PTRS_PER_PGD] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt; +pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="quote">&gt; +pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="quote">&gt; +                                       unsigned long end, pud_t *pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long addr;</span>
<span class="quote">&gt; +       unsigned long next;</span>
<span class="quote">&gt; +       pmd_t *pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pmd = pmd_offset(pud, start);</span>
<span class="quote">&gt; +       for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; +               pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="quote">&gt; +               next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="quote">&gt; +                                       unsigned long end, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long addr;</span>
<span class="quote">&gt; +       unsigned long next;</span>
<span class="quote">&gt; +       pud_t *pud;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pud = pud_offset(pgd, start);</span>
<span class="quote">&gt; +       for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="quote">&gt; +               pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt; +               next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; +               kasan_early_pmd_populate(addr, next, pud);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       int i;</span>
<span class="quote">&gt; +       unsigned long start = KASAN_SHADOW_START;</span>
<span class="quote">&gt; +       unsigned long end = KASAN_SHADOW_END;</span>
<span class="quote">&gt; +       unsigned long addr;</span>
<span class="quote">&gt; +       unsigned long next;</span>
<span class="quote">&gt; +       pgd_t *pgd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="quote">&gt; +               set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="quote">&gt; +                               virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pgd = pgd_offset_k(start);</span>
<span class="quote">&gt; +       for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="quote">&gt; +               pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt; +               next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; +               kasan_early_pud_populate(addr, next, pgd);</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void __init kasan_early_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       kasan_map_early_shadow(swapper_pg_dir);</span>
<span class="quote">&gt; +       start_kernel();</span>
<span class="quote">&gt; +}</span>

So as far as I can see, kasan_early_init() needs to be called before
we even run start_kernel() because every memory access would
crash unless the MMU is set up for the shadow memory.

Is it correct that when the pte&#39;s, pgd&#39;s and pud&#39;s are populated
KASan really doesn&#39;t kick in, it&#39;s just done to have some scratch
memory with whatever contents so as to do dummy updates
for the __asan_loadN() and __asan_storeN() calls, and no checks
start until the shadow memory is populated in kasan_init()
i.e. there are no KASan checks for any code executing up
to that point, just random writes and reads?

Also, this code under kasan_early_init(), must that not be
written extremely carefully to avoid any loads and stores?
I.e. even if this file is obviously compiled with
KASAN_SANITIZE_kasan_init.o := n so that it is not
instrumented, I&#39;m thinking about the functions it is calling,
like set_pte(), pgd_populate(), pmd_offset() etc etc.

Are we just lucky that these functions never do any loads
and stores?

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100571">Andrey Ryabinin</a> - June 12, 2015, 6:14 p.m.</div>
<pre class="content">
2015-06-11 16:39 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:
<span class="quote">&gt; On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt;&gt; (see Documentation/kasan.txt).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I looked closer at this again ... I am trying to get KASan up for</span>
<span class="quote">&gt; ARM(32) with some tricks and hacks.</span>
<span class="quote">&gt;</span>

I have some patches for that. They still need some polishing, but works for me.
I could share after I get back to office on Tuesday.
<span class="quote">

&gt;&gt; +config KASAN_SHADOW_OFFSET</span>
<span class="quote">&gt;&gt; +       hex</span>
<span class="quote">&gt;&gt; +       default 0xdfff200000000000 if ARM64_VA_BITS_48</span>
<span class="quote">&gt;&gt; +       default 0xdffffc8000000000 if ARM64_VA_BITS_42</span>
<span class="quote">&gt;&gt; +       default 0xdfffff9000000000 if ARM64_VA_BITS_39</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So IIUC these offsets are simply chosen to satisfy the equation</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         SHADOW = (void *)((unsigned long)addr &gt;&gt; KASAN_SHADOW_SCALE_SHIFT)</span>
<span class="quote">&gt;                 + KASAN_SHADOW_OFFSET;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For all memory that needs to be covered, i.e. kernel text+data,</span>
<span class="quote">&gt; modules text+data, any other kernel-mode running code+data.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And it needs to be statically assigned like this because the offset</span>
<span class="quote">&gt; is used at compile time.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Atleast that is how I understood it... correct me if wrong.</span>
<span class="quote">&gt; (Dunno if all is completely obvious to everyone else in the world...)</span>
<span class="quote">&gt;</span>

I think you understood this right.
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="quote">&gt;&gt; + * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="quote">&gt;&gt; +#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will this not mean that shadow start to end actually covers *all*</span>
<span class="quote">&gt; virtual addresses including userspace and what not? However a</span>
<span class="quote">&gt; large portion of this shadow memory will be unused because the</span>
<span class="quote">&gt; SHADOW_OFFSET only works for code compiled for the kernel</span>
<span class="quote">&gt; anyway.</span>
<span class="quote">&gt;</span>

SHADOW_OFFSET:SHADOW_END - covers *all* 64bits of virtual addresses.
SHADOW_OFFSET:SHADOW_START - unused shadow.
SHADOW_START:SHADOW_END - covers only kernel virtual addresses (used shadow).
<span class="quote">

&gt; When working on ARM32 I certainly could not map</span>
<span class="quote">&gt; (1UL &lt;&lt; (VA_BITS -3)) i.e. for 32 bit (1UL &lt;&lt; 29) bytes (512 MB) of</span>

Why not? We can just take it from TASK_SIZE.
<span class="quote">
&gt; virtual memory for shadow, instead I had to restrict it to the size that</span>
<span class="quote">&gt; actually maps the memory used by the kernel.</span>
<span class="quote">&gt;</span>

That should work too.
<span class="quote">
&gt; I tried shrinking it down but it crashed on me so tell me how</span>
<span class="quote">&gt; wrong I am ... :)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But here comes the real tricks, where I need some help to</span>
<span class="quote">&gt; understand the patch set, maybe some comments should be</span>
<span class="quote">&gt; inserted here and there to ease understanding:</span>
<span class="quote">&gt;</span>

...
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void __init kasan_early_init(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       kasan_map_early_shadow(swapper_pg_dir);</span>
<span class="quote">&gt;&gt; +       start_kernel();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So as far as I can see, kasan_early_init() needs to be called before</span>
<span class="quote">&gt; we even run start_kernel() because every memory access would</span>
<span class="quote">&gt; crash unless the MMU is set up for the shadow memory.</span>
<span class="quote">&gt;</span>

 kasan_early_init() should be called before *any* instrumented function.
<span class="quote">
&gt; Is it correct that when the pte&#39;s, pgd&#39;s and pud&#39;s are populated</span>
<span class="quote">&gt; KASan really doesn&#39;t kick in, it&#39;s just done to have some scratch</span>
<span class="quote">&gt; memory with whatever contents so as to do dummy updates</span>
<span class="quote">&gt; for the __asan_loadN() and __asan_storeN() calls, and no checks</span>
<span class="quote">&gt; start until the shadow memory is populated in kasan_init()</span>
<span class="quote">&gt; i.e. there are no KASan checks for any code executing up</span>
<span class="quote">&gt; to that point, just random writes and reads?</span>
<span class="quote">&gt;</span>

Yes, kasan_early_init() setups scratch memory with whatever contents.
But  KASan checks shadow before kasan_init(), that&#39;s the reason why we
need scratch shadow.
So checks are performed, but KASan don&#39;t print any reports, because
init_task has non-zero kasan_depth flag (see include/linux/init_task.h)
We check that flag in kasan_report() and print report iff it have zero value.

In kasan_init() after shadow populated, we enable reports by setting
kasan_depth to zero.
<span class="quote">

&gt; Also, this code under kasan_early_init(), must that not be</span>
<span class="quote">&gt; written extremely carefully to avoid any loads and stores?</span>
<span class="quote">&gt; I.e. even if this file is obviously compiled with</span>
<span class="quote">&gt; KASAN_SANITIZE_kasan_init.o := n so that it is not</span>
<span class="quote">&gt; instrumented, I&#39;m thinking about the functions it is calling,</span>
<span class="quote">&gt; like set_pte(), pgd_populate(), pmd_offset() etc etc.</span>
<span class="quote">&gt;</span>

These functions are not instrumented as well, because they are static
and located in headers.
So these functions will be in kasan_init.o translation unit and will
be compiled without -fsanitize=kernel-address.
<span class="quote">
&gt; Are we just lucky that these functions never do any loads</span>
<span class="quote">&gt; and stores?</span>
<span class="quote">&gt;</span>

We relay on fact that these functions are static inline and do not call other
functions from other (instrumented) files.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - June 13, 2015, 3:25 p.m.</div>
<pre class="content">
On Fri, Jun 12, 2015 at 8:14 PM, Andrey Ryabinin &lt;ryabinin.a.a@gmail.com&gt; wrote:
<span class="quote">&gt; 2015-06-11 16:39 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt; On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt;&gt;&gt; (see Documentation/kasan.txt).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I looked closer at this again ... I am trying to get KASan up for</span>
<span class="quote">&gt;&gt; ARM(32) with some tricks and hacks.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I have some patches for that. They still need some polishing, but works for me.</span>
<span class="quote">&gt; I could share after I get back to office on Tuesday.</span>

OK! I&#39;d be happy to test!

I have a WIP patch too, but it was trying to do a physical carveout
at early boot because of misunderstandings as to how KASan works...
Yeah I&#39;m a slow learner.
<span class="quote">
&gt;&gt;&gt; +/*</span>
<span class="quote">&gt;&gt;&gt; + * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="quote">&gt;&gt;&gt; + * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="quote">&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt; +#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="quote">&gt;&gt;&gt; +#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Will this not mean that shadow start to end actually covers *all*</span>
<span class="quote">&gt;&gt; virtual addresses including userspace and what not? However a</span>
<span class="quote">&gt;&gt; large portion of this shadow memory will be unused because the</span>
<span class="quote">&gt;&gt; SHADOW_OFFSET only works for code compiled for the kernel</span>
<span class="quote">&gt;&gt; anyway.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; SHADOW_OFFSET:SHADOW_END - covers *all* 64bits of virtual addresses.</span>
<span class="quote">&gt; SHADOW_OFFSET:SHADOW_START - unused shadow.</span>
<span class="quote">&gt; SHADOW_START:SHADOW_END - covers only kernel virtual addresses (used shadow).</span>

Aha. I see now...
<span class="quote">
&gt;&gt; When working on ARM32 I certainly could not map</span>
<span class="quote">&gt;&gt; (1UL &lt;&lt; (VA_BITS -3)) i.e. for 32 bit (1UL &lt;&lt; 29) bytes (512 MB) of</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why not? We can just take it from TASK_SIZE.</span>

Yeah the idea to steal it from userspace occured to me too...
with ARM32 having a highmem split in the middle of vmalloc I was
quite stressed when trying to chisel it out from the vmalloc area.

I actually managed to remove all static iomaps from my platform
so I could allocate the KASan memory from high to log addresses
at 0xf7000000-0xff000000 but it puts requirements on all
the ARM32 platforms to rid their static maps :P
<span class="quote">
&gt;&gt; Is it correct that when the pte&#39;s, pgd&#39;s and pud&#39;s are populated</span>
<span class="quote">&gt;&gt; KASan really doesn&#39;t kick in, it&#39;s just done to have some scratch</span>
<span class="quote">&gt;&gt; memory with whatever contents so as to do dummy updates</span>
<span class="quote">&gt;&gt; for the __asan_loadN() and __asan_storeN() calls, and no checks</span>
<span class="quote">&gt;&gt; start until the shadow memory is populated in kasan_init()</span>
<span class="quote">&gt;&gt; i.e. there are no KASan checks for any code executing up</span>
<span class="quote">&gt;&gt; to that point, just random writes and reads?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, kasan_early_init() setups scratch memory with whatever contents.</span>
<span class="quote">&gt; But  KASan checks shadow before kasan_init(), that&#39;s the reason why we</span>
<span class="quote">&gt; need scratch shadow.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So checks are performed, but KASan don&#39;t print any reports, because</span>
<span class="quote">&gt; init_task has non-zero kasan_depth flag (see include/linux/init_task.h)</span>
<span class="quote">&gt; We check that flag in kasan_report() and print report iff it have zero value.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In kasan_init() after shadow populated, we enable reports by setting</span>
<span class="quote">&gt; kasan_depth to zero.</span>

Aha now I understand how this works! Now I understand
this init_task.kasan_depth = 0 too.
<span class="quote">
&gt;&gt; Are we just lucky that these functions never do any loads</span>
<span class="quote">&gt;&gt; and stores?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We relay on fact that these functions are static inline and do not call other</span>
<span class="quote">&gt; functions from other (instrumented) files.</span>

Aha, makes perfect sense.

I think I understand the code a bit now ... it maps all the KASan
shadow memory to the physical memory of the zero page and let all
updates hit that memory until the memory manager is up and running,
then you allocate physical memory backing the shadow in
kasan_populate_zero_shadow().

I misunderstood it such that the backing physical shadow memory
had to be available when we do the early call... no wonder I
got lost.

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100571">Andrey Ryabinin</a> - June 17, 2015, 9:32 p.m.</div>
<pre class="content">
2015-06-13 18:25 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:
<span class="quote">&gt;</span>
<span class="quote">&gt; On Fri, Jun 12, 2015 at 8:14 PM, Andrey Ryabinin &lt;ryabinin.a.a@gmail.com&gt; wrote:</span>
<span class="quote">&gt; &gt; 2015-06-11 16:39 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt; &gt;&gt; On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt; &gt;&gt;&gt; (see Documentation/kasan.txt).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I looked closer at this again ... I am trying to get KASan up for</span>
<span class="quote">&gt; &gt;&gt; ARM(32) with some tricks and hacks.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I have some patches for that. They still need some polishing, but works for me.</span>
<span class="quote">&gt; &gt; I could share after I get back to office on Tuesday.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; OK! I&#39;d be happy to test!</span>
<span class="quote">&gt;</span>

I&#39;ve pushed it here : git://github.com/aryabinin/linux.git kasan/arm_v0

It far from ready. Firstly I&#39;ve tried it only in qemu and it works.
Today, I&#39;ve tried to run it on bare metal (exynos5420), but it hangs
somewhere after early_irq_init().
So, it probably doesn&#39;t  worth for trying/testing yet.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 8, 2015, 3:48 p.m.</div>
<pre class="content">
On Fri, May 15, 2015 at 04:59:04PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="quote">&gt; index 7796af4..4cc73cc 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig</span>
<span class="quote">&gt; @@ -44,6 +44,7 @@ config ARM64</span>
<span class="quote">&gt;  	select HAVE_ARCH_AUDITSYSCALL</span>
<span class="quote">&gt;  	select HAVE_ARCH_BITREVERSE</span>
<span class="quote">&gt;  	select HAVE_ARCH_JUMP_LABEL</span>
<span class="quote">&gt; +	select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP</span>

Just curious, why the dependency?
<span class="quote">
&gt;  	select HAVE_ARCH_KGDB</span>
<span class="quote">&gt;  	select HAVE_ARCH_SECCOMP_FILTER</span>
<span class="quote">&gt;  	select HAVE_ARCH_TRACEHOOK</span>
<span class="quote">&gt; @@ -119,6 +120,12 @@ config GENERIC_CSUM</span>
<span class="quote">&gt;  config GENERIC_CALIBRATE_DELAY</span>
<span class="quote">&gt;  	def_bool y</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config KASAN_SHADOW_OFFSET</span>
<span class="quote">&gt; +	hex</span>
<span class="quote">&gt; +	default 0xdfff200000000000 if ARM64_VA_BITS_48</span>
<span class="quote">&gt; +	default 0xdffffc8000000000 if ARM64_VA_BITS_42</span>
<span class="quote">&gt; +	default 0xdfffff9000000000 if ARM64_VA_BITS_39</span>
<span class="quote">&gt; +</span>

How were these numbers generated? I can probably guess but we need a
comment in this file and a BUILD_BUG elsewhere (kasan_init.c) if we
change the memory map and they no longer match.
<span class="quote">
&gt; diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..65ac50d</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/kasan.h</span>
<span class="quote">&gt; @@ -0,0 +1,24 @@</span>
<span class="quote">&gt; +#ifndef __ASM_KASAN_H</span>
<span class="quote">&gt; +#define __ASM_KASAN_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_KASAN</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/memory.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="quote">&gt; + * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="quote">&gt; +#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>

Can you define a VA_START in asm/memory.h so that we avoid this long
list of f&#39;s here and in pgtable.h?

Another BUILD_BUG we need is to ensure that KASAN_SHADOW_START/END
covers an exact number of pgd entries, otherwise the logic in
kasan_init.c can go wrong (it seems to be the case in all VA_BITS
configurations but just in case we forget about this requirement in the
future).
<span class="quote">
&gt; diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; index bd5db28..8700f66 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt; @@ -40,7 +40,14 @@</span>
<span class="quote">&gt;   *	fixed mappings and modules</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define VMEMMAP_SIZE		ALIGN((1UL &lt;&lt; (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;  #define VMALLOC_START		(UL(0xffffffffffffffff) &lt;&lt; VA_BITS)</span>

And here we could just use VA_START.
<span class="quote">
&gt; +#else</span>
<span class="quote">&gt; +#include &lt;asm/kasan.h&gt;</span>
<span class="quote">&gt; +#define VMALLOC_START		KASAN_SHADOW_END</span>
<span class="quote">&gt; +#endif</span>

We could add a SZ_64K guard page here (just in case, the KASan shadow
probably never reaches KASAN_SHADOW_END).
<span class="quote">
&gt; diff --git a/arch/arm64/include/asm/string.h b/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt; index 64d2d48..bff522c 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt; @@ -36,17 +36,33 @@ extern __kernel_size_t strnlen(const char *, __kernel_size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __HAVE_ARCH_MEMCPY</span>
<span class="quote">&gt;  extern void *memcpy(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt; +extern void *__memcpy(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __HAVE_ARCH_MEMMOVE</span>
<span class="quote">&gt;  extern void *memmove(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt; +extern void *__memmove(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __HAVE_ARCH_MEMCHR</span>
<span class="quote">&gt;  extern void *memchr(const void *, int, __kernel_size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __HAVE_ARCH_MEMSET</span>
<span class="quote">&gt;  extern void *memset(void *, int, __kernel_size_t);</span>
<span class="quote">&gt; +extern void *__memset(void *, int, __kernel_size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __HAVE_ARCH_MEMCMP</span>
<span class="quote">&gt;  extern int memcmp(const void *, const void *, size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if defined(CONFIG_KASAN) &amp;&amp; !defined(__SANITIZE_ADDRESS__)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * For files that not instrumented (e.g. mm/slub.c) we</span>

Missing an &quot;are&quot;.
<span class="quote">
&gt; diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt; index dcd06d1..cfe5ea5 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt; @@ -24,10 +24,18 @@</span>
<span class="quote">&gt;  #include &lt;linux/compiler.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef CONFIG_ARM64_64K_PAGES</span>
<span class="quote">&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;  #define THREAD_SIZE_ORDER	2</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define THREAD_SIZE_ORDER	3</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;  #define THREAD_SIZE		16384</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define THREAD_SIZE		32768</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  #define THREAD_START_SP		(THREAD_SIZE - 16)</span>

Have you actually seen it failing with the 16KB THREAD_SIZE? You may run
into other problems with 8 4KB pages per stack.
<span class="quote">
&gt;  #ifndef __ASSEMBLY__</span>
<span class="quote">&gt; diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S</span>
<span class="quote">&gt; index 19f915e..650b1e8 100644</span>
<span class="quote">&gt; --- a/arch/arm64/kernel/head.S</span>
<span class="quote">&gt; +++ b/arch/arm64/kernel/head.S</span>
<span class="quote">&gt; @@ -486,6 +486,9 @@ __mmap_switched:</span>
<span class="quote">&gt;  	str_l	x21, __fdt_pointer, x5		// Save FDT pointer</span>
<span class="quote">&gt;  	str_l	x24, memstart_addr, x6		// Save PHYS_OFFSET</span>
<span class="quote">&gt;  	mov	x29, #0</span>
<span class="quote">&gt; +#ifdef CONFIG_KASAN</span>
<span class="quote">&gt; +	b	kasan_early_init</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  	b	start_kernel</span>
<span class="quote">&gt;  ENDPROC(__mmap_switched)</span>

I think we still have swapper_pg_dir in x26 at this point, could you
instead do:

	mov	x0, x26
	bl	kasan_map_early_shadow

Actually, I don&#39;t think kasan_map_early_shadow() even needs this
argument, it uses pgd_offset_k() anyway.
<span class="quote">
&gt; diff --git a/arch/arm64/lib/memcpy.S b/arch/arm64/lib/memcpy.S</span>
<span class="quote">&gt; index 8a9a96d..845e40a 100644</span>
<span class="quote">&gt; --- a/arch/arm64/lib/memcpy.S</span>
<span class="quote">&gt; +++ b/arch/arm64/lib/memcpy.S</span>
<span class="quote">&gt; @@ -56,6 +56,8 @@ C_h	.req	x12</span>
<span class="quote">&gt;  D_l	.req	x13</span>
<span class="quote">&gt;  D_h	.req	x14</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +.weak memcpy</span>

Nitpick: as with other such asm declarations, use some indentation:

	.weak	memcpy

(similarly for the others)
<span class="quote">
&gt; diff --git a/arch/arm64/mm/Makefile b/arch/arm64/mm/Makefile</span>
<span class="quote">&gt; index 773d37a..e17703c 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/Makefile</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/Makefile</span>
<span class="quote">&gt; @@ -4,3 +4,6 @@ obj-y				:= dma-mapping.o extable.o fault.o init.o \</span>
<span class="quote">&gt;  				   context.o proc.o pageattr.o</span>
<span class="quote">&gt;  obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o</span>
<span class="quote">&gt;  obj-$(CONFIG_ARM64_PTDUMP)	+= dump.o</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +KASAN_SANITIZE_kasan_init.o	:= n</span>
<span class="quote">&gt; +obj-$(CONFIG_KASAN)		+= kasan_init.o</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..35dbd84</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; @@ -0,0 +1,143 @@</span>
<span class="quote">&gt; +#include &lt;linux/kasan.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memblock.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/start_kernel.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/page.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;</span>

So that&#39;s needed because the shadow memory is mapped before paging_init
is called and we don&#39;t have the zero page set up yet. Please add a
comment.
<span class="quote">
&gt; +static pgd_t tmp_page_table[PTRS_PER_PGD] __initdata __aligned(PAGE_SIZE);</span>

This doesn&#39;t need a full PAGE_SIZE alignment, just PGD_SIZE. You could
also rename to tmp_pg_dir for consistency with swapper and idmap.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt; +pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="quote">&gt; +pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="quote">&gt; +					unsigned long end, pud_t *pud)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = pmd_offset(pud, start);</span>
<span class="quote">&gt; +	for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="quote">&gt; +					unsigned long end, pgd_t *pgd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_offset(pgd, start);</span>
<span class="quote">&gt; +	for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="quote">&gt; +		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; +		kasan_early_pmd_populate(addr, next, pud);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +	unsigned long start = KASAN_SHADOW_START;</span>
<span class="quote">&gt; +	unsigned long end = KASAN_SHADOW_END;</span>
<span class="quote">&gt; +	unsigned long addr;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="quote">&gt; +		set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="quote">&gt; +				virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>

Does this need to be writable? If yes, is there anything writing
non-zero values to it?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	pgd = pgd_offset_k(start);</span>
<span class="quote">&gt; +	for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="quote">&gt; +		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; +		kasan_early_pud_populate(addr, next, pgd);</span>
<span class="quote">&gt; +	}</span>

I prefer to use &quot;do ... while&quot; constructs similar to __create_mapping()
(or zero_{pgd,pud,pmd}_populate as you are more familiar with them).

But what I don&#39;t get here is that you repopulate the pud page for every
pgd (and so on for pmd). You don&#39;t need this recursive call all the way
to kasan_early_pmd_populate() but just sequential:

	kasan_early_pte_populate();
	kasan_early_pmd_populate(..., pte);
	kasan_early_pud_populate(..., pmd);
	kasan_early_pgd_populate(..., pud);

(or in reverse order)

That&#39;s because you don&#39;t have enough pte/pmd/pud pages to cover the
range (i.e. you need 512 pte pages for a pmd page) but you just reuse
the same table page to make all of them pointing to kasan_zero_page.
<span class="quote">
&gt; +void __init kasan_early_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	kasan_map_early_shadow(swapper_pg_dir);</span>
<span class="quote">&gt; +	start_kernel();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init clear_pgds(unsigned long start,</span>
<span class="quote">&gt; +			unsigned long end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Remove references to kasan page tables from</span>
<span class="quote">&gt; +	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt; +	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	for (; start &amp;&amp; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt; +		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init cpu_set_ttbr1(unsigned long ttbr1)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	asm(</span>
<span class="quote">&gt; +	&quot;	msr	ttbr1_el1, %0\n&quot;</span>
<span class="quote">&gt; +	&quot;	isb&quot;</span>
<span class="quote">&gt; +	:</span>
<span class="quote">&gt; +	: &quot;r&quot; (ttbr1));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void __init kasan_init(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct memblock_region *reg;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We are going to perform proper setup of shadow memory.</span>
<span class="quote">&gt; +	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="quote">&gt; +	 * However, instrumented code couldn&#39;t execute without shadow memory.</span>
<span class="quote">&gt; +	 * tmp_page_table used to keep early shadow mapped until full shadow</span>
<span class="quote">&gt; +	 * setup will be finished.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	memcpy(tmp_page_table, swapper_pg_dir, sizeof(tmp_page_table));</span>
<span class="quote">&gt; +	cpu_set_ttbr1(__pa(tmp_page_table));</span>
<span class="quote">&gt; +	flush_tlb_all();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,</span>
<span class="quote">&gt; +			kasan_mem_to_shadow((void *)MODULES_VADDR));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for_each_memblock(memory, reg) {</span>
<span class="quote">&gt; +		void *start = (void *)__phys_to_virt(reg-&gt;base);</span>
<span class="quote">&gt; +		void *end = (void *)__phys_to_virt(reg-&gt;base + reg-&gt;size);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (start &gt;= end)</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * end + 1 here is intentional. We check several shadow bytes in</span>
<span class="quote">&gt; +		 * advance to slightly speed up fastpath. In some rare cases</span>
<span class="quote">&gt; +		 * we could cross boundary of mapped shadow, so we just map</span>
<span class="quote">&gt; +		 * some more here.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		vmemmap_populate((unsigned long)kasan_mem_to_shadow(start),</span>
<span class="quote">&gt; +				(unsigned long)kasan_mem_to_shadow(end) + 1,</span>
<span class="quote">&gt; +				pfn_to_nid(virt_to_pfn(start)));</span>

Is the only reason for sparsemem vmemmap dependency to reuse this
function? Maybe at some point you could factor this out and not require
SPARSEMEM_VMEMMAP to be enabled.

About the &quot;end + 1&quot;, what you actually get is an additional full section
(PMD_SIZE) with the 4KB page configuration. Since the shadow is 1/8 of
the VA space, do we need a check for memblocks within 8 * 2MB of each
other?
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	memset(kasan_zero_page, 0, PAGE_SIZE);</span>

Has anyone written to this page? Actually, what&#39;s its use after we
enabled the proper KASan shadow mappings?
<span class="quote">
&gt; +	cpu_set_ttbr1(__pa(swapper_pg_dir));</span>
<span class="quote">&gt; +	flush_tlb_all();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* At this point kasan is fully initialized. Enable error messages */</span>
<span class="quote">&gt; +	init_task.kasan_depth = 0;</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 10, 2015, 5:11 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;  	select HAVE_ARCH_KGDB</span>
<span class="quote">&gt;&gt;  	select HAVE_ARCH_SECCOMP_FILTER</span>
<span class="quote">&gt;&gt;  	select HAVE_ARCH_TRACEHOOK</span>
<span class="quote">&gt;&gt; @@ -119,6 +120,12 @@ config GENERIC_CSUM</span>
<span class="quote">&gt;&gt;  config GENERIC_CALIBRATE_DELAY</span>
<span class="quote">&gt;&gt;  	def_bool y</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +config KASAN_SHADOW_OFFSET</span>
<span class="quote">&gt;&gt; +	hex</span>
<span class="quote">&gt;&gt; +	default 0xdfff200000000000 if ARM64_VA_BITS_48</span>
<span class="quote">&gt;&gt; +	default 0xdffffc8000000000 if ARM64_VA_BITS_42</span>
<span class="quote">&gt;&gt; +	default 0xdfffff9000000000 if ARM64_VA_BITS_39</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How were these numbers generated? I can probably guess but we need a</span>
<span class="quote">&gt; comment in this file and a BUILD_BUG elsewhere (kasan_init.c) if we</span>
<span class="quote">&gt; change the memory map and they no longer match.</span>
<span class="quote">&gt; </span>

Ok, will do.

Probably the simplest way to get this number is:
	KASAN_SHADOW_END - (1ULL &lt;&lt; (64 - 3))

64 is number of bits in pointer, 3 is KASAN_SHADOW_SCALE_SHIFT,
so [KASAN_SHADOW_OFFSET, KASAN_SHADOW_END] covers [0, -1ULL] addresses.
<span class="quote">

&gt;&gt; diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..65ac50d</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/kasan.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,24 @@</span>
<span class="quote">&gt;&gt; +#ifndef __ASM_KASAN_H</span>
<span class="quote">&gt;&gt; +#define __ASM_KASAN_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_KASAN</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/memory.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="quote">&gt;&gt; + * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="quote">&gt;&gt; +#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you define a VA_START in asm/memory.h so that we avoid this long</span>
<span class="quote">&gt; list of f&#39;s here and in pgtable.h?</span>
<span class="quote">&gt; </span>

Sure, will do.
<span class="quote">
&gt; Another BUILD_BUG we need is to ensure that KASAN_SHADOW_START/END</span>
<span class="quote">&gt; covers an exact number of pgd entries, otherwise the logic in</span>
<span class="quote">&gt; kasan_init.c can go wrong (it seems to be the case in all VA_BITS</span>
<span class="quote">&gt; configurations but just in case we forget about this requirement in the</span>
<span class="quote">&gt; future).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; index bd5db28..8700f66 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="quote">&gt;&gt; @@ -40,7 +40,14 @@</span>
<span class="quote">&gt;&gt;   *	fixed mappings and modules</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  #define VMEMMAP_SIZE		ALIGN((1UL &lt;&lt; (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;&gt;  #define VMALLOC_START		(UL(0xffffffffffffffff) &lt;&lt; VA_BITS)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And here we could just use VA_START.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +#include &lt;asm/kasan.h&gt;</span>
<span class="quote">&gt;&gt; +#define VMALLOC_START		KASAN_SHADOW_END</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We could add a SZ_64K guard page here (just in case, the KASan shadow</span>
<span class="quote">&gt; probably never reaches KASAN_SHADOW_END).</span>
<span class="quote">&gt; </span>

Ok.
<span class="quote">
&gt;&gt; diff --git a/arch/arm64/include/asm/string.h b/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt;&gt; index 64d2d48..bff522c 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/string.h</span>
<span class="quote">&gt;&gt; @@ -36,17 +36,33 @@ extern __kernel_size_t strnlen(const char *, __kernel_size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_MEMCPY</span>
<span class="quote">&gt;&gt;  extern void *memcpy(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;&gt; +extern void *__memcpy(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_MEMMOVE</span>
<span class="quote">&gt;&gt;  extern void *memmove(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;&gt; +extern void *__memmove(void *, const void *, __kernel_size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_MEMCHR</span>
<span class="quote">&gt;&gt;  extern void *memchr(const void *, int, __kernel_size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_MEMSET</span>
<span class="quote">&gt;&gt;  extern void *memset(void *, int, __kernel_size_t);</span>
<span class="quote">&gt;&gt; +extern void *__memset(void *, int, __kernel_size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #define __HAVE_ARCH_MEMCMP</span>
<span class="quote">&gt;&gt;  extern int memcmp(const void *, const void *, size_t);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#if defined(CONFIG_KASAN) &amp;&amp; !defined(__SANITIZE_ADDRESS__)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * For files that not instrumented (e.g. mm/slub.c) we</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Missing an &quot;are&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt;&gt; index dcd06d1..cfe5ea5 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/thread_info.h</span>
<span class="quote">&gt;&gt; @@ -24,10 +24,18 @@</span>
<span class="quote">&gt;&gt;  #include &lt;linux/compiler.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  #ifndef CONFIG_ARM64_64K_PAGES</span>
<span class="quote">&gt;&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;&gt;  #define THREAD_SIZE_ORDER	2</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +#define THREAD_SIZE_ORDER	3</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +#ifndef CONFIG_KASAN</span>
<span class="quote">&gt;&gt;  #define THREAD_SIZE		16384</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +#define THREAD_SIZE		32768</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  #define THREAD_START_SP		(THREAD_SIZE - 16)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Have you actually seen it failing with the 16KB THREAD_SIZE? You may run</span>
<span class="quote">&gt; into other problems with 8 4KB pages per stack.</span>
<span class="quote">&gt; </span>

Actually no, so I guess that we could try with 16K.

I&#39;ve seen it failing on ARM32 with 8K stack (we use some old version of kasan for our ARM kernels),
but that&#39;s a different story
<span class="quote">

&gt;&gt;  #ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S</span>
<span class="quote">&gt;&gt; index 19f915e..650b1e8 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/kernel/head.S</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/kernel/head.S</span>
<span class="quote">&gt;&gt; @@ -486,6 +486,9 @@ __mmap_switched:</span>
<span class="quote">&gt;&gt;  	str_l	x21, __fdt_pointer, x5		// Save FDT pointer</span>
<span class="quote">&gt;&gt;  	str_l	x24, memstart_addr, x6		// Save PHYS_OFFSET</span>
<span class="quote">&gt;&gt;  	mov	x29, #0</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_KASAN</span>
<span class="quote">&gt;&gt; +	b	kasan_early_init</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;  	b	start_kernel</span>
<span class="quote">&gt;&gt;  ENDPROC(__mmap_switched)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we still have swapper_pg_dir in x26 at this point, could you</span>
<span class="quote">&gt; instead do:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	mov	x0, x26</span>
<span class="quote">&gt; 	bl	kasan_map_early_shadow</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Actually, I don&#39;t think kasan_map_early_shadow() even needs this</span>
<span class="quote">&gt; argument, it uses pgd_offset_k() anyway.</span>
<span class="quote">&gt; </span>

Indeed, just &quot;bl	kasan_map_early_shadow&quot; would be enough.
<span class="quote">

&gt;&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 0000000..35dbd84</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,143 @@</span>
<span class="quote">&gt;&gt; +#include &lt;linux/kasan.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/kernel.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/memblock.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/start_kernel.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm/page.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So that&#39;s needed because the shadow memory is mapped before paging_init</span>
<span class="quote">&gt; is called and we don&#39;t have the zero page set up yet. Please add a</span>
<span class="quote">&gt; comment.</span>
<span class="quote">&gt; </span>

Actually this page has two purposes, so naming is bad here.
There was a debate (in kasan for x86_64 thread) about its name, but nobody
come up wit a good name.

So I&#39;ll add following comment:

/*
 * This page serves two purposes:
 *   - It used as early shadow memory. The entire shadow region populated with this
 *      page, before we will be able to setup normal shadow memory.
 *   - Latter it reused it as zero shadow to cover large ranges of memory
 *      that allowed to access, but not handled by kasan (vmalloc/vmemmap ...).
 */
<span class="quote">

&gt;&gt; +static pgd_t tmp_page_table[PTRS_PER_PGD] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This doesn&#39;t need a full PAGE_SIZE alignment, just PGD_SIZE. You could</span>
<span class="quote">&gt; also rename to tmp_pg_dir for consistency with swapper and idmap.</span>
<span class="quote">&gt; </span>

Ok.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt;&gt; +pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="quote">&gt;&gt; +pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="quote">&gt;&gt; +					unsigned long end, pud_t *pud)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pmd = pmd_offset(pud, start);</span>
<span class="quote">&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="quote">&gt;&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="quote">&gt;&gt; +					unsigned long end, pgd_t *pgd)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pud = pud_offset(pgd, start);</span>
<span class="quote">&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="quote">&gt;&gt; +		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt;&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; +		kasan_early_pmd_populate(addr, next, pud);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	int i;</span>
<span class="quote">&gt;&gt; +	unsigned long start = KASAN_SHADOW_START;</span>
<span class="quote">&gt;&gt; +	unsigned long end = KASAN_SHADOW_END;</span>
<span class="quote">&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="quote">&gt;&gt; +		set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="quote">&gt;&gt; +				virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does this need to be writable? If yes, is there anything writing</span>
<span class="quote">&gt; non-zero values to it?</span>
<span class="quote">&gt; </span>

Yes. Before kasan_init() this needs to be writable for stack instrumentation.
In function&#39;s prologue GCC generates some code that writes to shadow marking
redzones around stack variables.

So the page will contain some garbage, however it doesn&#39;t matter for early
stage of boot. Kasan will ignore any bad shadow value before kasan_init().
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pgd = pgd_offset_k(start);</span>
<span class="quote">&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="quote">&gt;&gt; +		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt;&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; +		kasan_early_pud_populate(addr, next, pgd);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I prefer to use &quot;do ... while&quot; constructs similar to __create_mapping()</span>
<span class="quote">&gt; (or zero_{pgd,pud,pmd}_populate as you are more familiar with them).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But what I don&#39;t get here is that you repopulate the pud page for every</span>
<span class="quote">&gt; pgd (and so on for pmd). You don&#39;t need this recursive call all the way</span>
<span class="quote">&gt; to kasan_early_pmd_populate() but just sequential:</span>
<span class="quote">&gt; </span>

This repopulation needed for 3,2 level page tables configurations.

E.g. for 3-level page tables we need to call pud_populate(&amp;init_mm, pud, kasan_zero_pmd)
for each pud in [KASAN_SHADOW_START, KASAN_SHADOW_END] range, this causes repopopulation for 4-level
page tables, since we need to pud_populate() only [KASAN_SHADOW_START, KASAN_SHADOW_START + PGDIR_SIZE] range.
<span class="quote">
&gt; 	kasan_early_pte_populate();</span>
<span class="quote">&gt; 	kasan_early_pmd_populate(..., pte);</span>
<span class="quote">&gt; 	kasan_early_pud_populate(..., pmd);</span>
<span class="quote">&gt; 	kasan_early_pgd_populate(..., pud);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (or in reverse order)</span>
<span class="quote">&gt; </span>

Unless, I&#39;m missing something, this will either work only with 4-level page tables.
We could do this without repopulation by using CONFIG_PGTABLE_LEVELS ifdefs.
<span class="quote">


&gt; That&#39;s because you don&#39;t have enough pte/pmd/pud pages to cover the</span>
<span class="quote">&gt; range (i.e. you need 512 pte pages for a pmd page) but you just reuse</span>
<span class="quote">&gt; the same table page to make all of them pointing to kasan_zero_page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +void __init kasan_early_init(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	kasan_map_early_shadow(swapper_pg_dir);</span>
<span class="quote">&gt;&gt; +	start_kernel();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void __init clear_pgds(unsigned long start,</span>
<span class="quote">&gt;&gt; +			unsigned long end)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Remove references to kasan page tables from</span>
<span class="quote">&gt;&gt; +	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt;&gt; +	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	for (; start &amp;&amp; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt;&gt; +		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void __init cpu_set_ttbr1(unsigned long ttbr1)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	asm(</span>
<span class="quote">&gt;&gt; +	&quot;	msr	ttbr1_el1, %0\n&quot;</span>
<span class="quote">&gt;&gt; +	&quot;	isb&quot;</span>
<span class="quote">&gt;&gt; +	:</span>
<span class="quote">&gt;&gt; +	: &quot;r&quot; (ttbr1));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +void __init kasan_init(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct memblock_region *reg;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * We are going to perform proper setup of shadow memory.</span>
<span class="quote">&gt;&gt; +	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="quote">&gt;&gt; +	 * However, instrumented code couldn&#39;t execute without shadow memory.</span>
<span class="quote">&gt;&gt; +	 * tmp_page_table used to keep early shadow mapped until full shadow</span>
<span class="quote">&gt;&gt; +	 * setup will be finished.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	memcpy(tmp_page_table, swapper_pg_dir, sizeof(tmp_page_table));</span>
<span class="quote">&gt;&gt; +	cpu_set_ttbr1(__pa(tmp_page_table));</span>
<span class="quote">&gt;&gt; +	flush_tlb_all();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,</span>
<span class="quote">&gt;&gt; +			kasan_mem_to_shadow((void *)MODULES_VADDR));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	for_each_memblock(memory, reg) {</span>
<span class="quote">&gt;&gt; +		void *start = (void *)__phys_to_virt(reg-&gt;base);</span>
<span class="quote">&gt;&gt; +		void *end = (void *)__phys_to_virt(reg-&gt;base + reg-&gt;size);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (start &gt;= end)</span>
<span class="quote">&gt;&gt; +			break;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * end + 1 here is intentional. We check several shadow bytes in</span>
<span class="quote">&gt;&gt; +		 * advance to slightly speed up fastpath. In some rare cases</span>
<span class="quote">&gt;&gt; +		 * we could cross boundary of mapped shadow, so we just map</span>
<span class="quote">&gt;&gt; +		 * some more here.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		vmemmap_populate((unsigned long)kasan_mem_to_shadow(start),</span>
<span class="quote">&gt;&gt; +				(unsigned long)kasan_mem_to_shadow(end) + 1,</span>
<span class="quote">&gt;&gt; +				pfn_to_nid(virt_to_pfn(start)));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is the only reason for sparsemem vmemmap dependency to reuse this</span>
<span class="quote">&gt; function? Maybe at some point you could factor this out and not require</span>
<span class="quote">&gt; SPARSEMEM_VMEMMAP to be enabled.</span>
<span class="quote">&gt; </span>

Yes, this is the only reason and I&#39;ll get rid of this dependency some day.
<span class="quote">
&gt; About the &quot;end + 1&quot;, what you actually get is an additional full section</span>
<span class="quote">&gt; (PMD_SIZE) with the 4KB page configuration. Since the shadow is 1/8 of</span>
<span class="quote">&gt; the VA space, do we need a check for memblocks within 8 * 2MB of each</span>
<span class="quote">&gt; other?</span>
<span class="quote">&gt; </span>

Overlaps should be ok. vmemmap_populate will handle it.
<span class="quote">
&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Has anyone written to this page? Actually, what&#39;s its use after we</span>
<span class="quote">&gt; enabled the proper KASan shadow mappings?</span>
<span class="quote">&gt; </span>

As said before, in function&#39;s prologue GCC generates code that writes to shadow
so it writes to this page.

After kasan_init() this page used as shadow and covers large portions of memory
which are not handled by kasan (vmalloc/vmemmap). We just assume that any access
to this memory region is good.
<span class="quote">
&gt;&gt; +	cpu_set_ttbr1(__pa(swapper_pg_dir));</span>
<span class="quote">&gt;&gt; +	flush_tlb_all();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* At this point kasan is fully initialized. Enable error messages */</span>
<span class="quote">&gt;&gt; +	init_task.kasan_depth = 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 14, 2015, 3:04 p.m.</div>
<pre class="content">
On Fri, Jul 10, 2015 at 08:11:03PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; &gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt; &gt;&gt; +pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt; &gt;&gt; +#endif</span>
<span class="quote">&gt; &gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="quote">&gt; &gt;&gt; +pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="quote">&gt; &gt;&gt; +#endif</span>
<span class="quote">&gt; &gt;&gt; +pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="quote">&gt; &gt;&gt; +					unsigned long end, pud_t *pud)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt;&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	pmd = pmd_offset(pud, start);</span>
<span class="quote">&gt; &gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt; &gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="quote">&gt; &gt;&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt;&gt; +	}</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="quote">&gt; &gt;&gt; +					unsigned long end, pgd_t *pgd)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	pud = pud_offset(pgd, start);</span>
<span class="quote">&gt; &gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="quote">&gt; &gt;&gt; +		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt; &gt;&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; &gt;&gt; +		kasan_early_pmd_populate(addr, next, pud);</span>
<span class="quote">&gt; &gt;&gt; +	}</span>
<span class="quote">&gt; &gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	int i;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long start = KASAN_SHADOW_START;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long end = KASAN_SHADOW_END;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt; &gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt; &gt;&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="quote">&gt; &gt;&gt; +		set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="quote">&gt; &gt;&gt; +				virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	pgd = pgd_offset_k(start);</span>
<span class="quote">&gt; &gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="quote">&gt; &gt;&gt; +		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt; &gt;&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt;&gt; +		kasan_early_pud_populate(addr, next, pgd);</span>
<span class="quote">&gt; &gt;&gt; +	}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I prefer to use &quot;do ... while&quot; constructs similar to __create_mapping()</span>
<span class="quote">&gt; &gt; (or zero_{pgd,pud,pmd}_populate as you are more familiar with them).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But what I don&#39;t get here is that you repopulate the pud page for every</span>
<span class="quote">&gt; &gt; pgd (and so on for pmd). You don&#39;t need this recursive call all the way</span>
<span class="quote">&gt; &gt; to kasan_early_pmd_populate() but just sequential:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This repopulation needed for 3,2 level page tables configurations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; E.g. for 3-level page tables we need to call pud_populate(&amp;init_mm,</span>
<span class="quote">&gt; pud, kasan_zero_pmd) for each pud in [KASAN_SHADOW_START,</span>
<span class="quote">&gt; KASAN_SHADOW_END] range, this causes repopopulation for 4-level page</span>
<span class="quote">&gt; tables, since we need to pud_populate() only [KASAN_SHADOW_START,</span>
<span class="quote">&gt; KASAN_SHADOW_START + PGDIR_SIZE] range.</span>

I&#39;m referring to writing the same information multiple times over the
same entry. kasan_map_early_shadow() goes over each pgd entry and writes
the address of kasan_zero_pud. That&#39;s fine so far. However, in the same
loop you call kasan_early_pud_populate(). The latter retrieves the pud
page via pud_offset(pgd, start) which would always be kasan_zero_pud
because that&#39;s what you wrote via pgd_populate() in each pgd entry. So
for each pgd entry, you keep populating the same kasan_zero_pud page
with pointers to kasan_zero_pmd. And so on for the pmd.
<span class="quote">
&gt; &gt; 	kasan_early_pte_populate();</span>
<span class="quote">&gt; &gt; 	kasan_early_pmd_populate(..., pte);</span>
<span class="quote">&gt; &gt; 	kasan_early_pud_populate(..., pmd);</span>
<span class="quote">&gt; &gt; 	kasan_early_pgd_populate(..., pud);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; (or in reverse order)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unless, I&#39;m missing something, this will either work only with 4-level</span>
<span class="quote">&gt; page tables. We could do this without repopulation by using</span>
<span class="quote">&gt; CONFIG_PGTABLE_LEVELS ifdefs.</span>

Or you could move kasan_early_*_populate outside the loop. You already
do this for the pte at the beginning of the kasan_map_early_shadow()
function (and it probably makes more sense to create a separate
kasan_early_pte_populate).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 15, 2015, 8:55 a.m.</div>
<pre class="content">
On 07/14/2015 06:04 PM, Catalin Marinas wrote:
<span class="quote">&gt; On Fri, Jul 10, 2015 at 08:11:03PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="quote">&gt;&gt;&gt;&gt; +pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;&gt;&gt; +#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="quote">&gt;&gt;&gt;&gt; +pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;&gt;&gt; +pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="quote">&gt;&gt;&gt;&gt; +					unsigned long end, pud_t *pud)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pmd = pmd_offset(pud, start);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="quote">&gt;&gt;&gt;&gt; +					unsigned long end, pgd_t *pgd)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pud = pud_offset(pgd, start);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		kasan_early_pmd_populate(addr, next, pud);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	int i;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long start = KASAN_SHADOW_START;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long end = KASAN_SHADOW_END;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long addr;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="quote">&gt;&gt;&gt;&gt; +		set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="quote">&gt;&gt;&gt;&gt; +				virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pgd = pgd_offset_k(start);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		kasan_early_pud_populate(addr, next, pgd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I prefer to use &quot;do ... while&quot; constructs similar to __create_mapping()</span>
<span class="quote">&gt;&gt;&gt; (or zero_{pgd,pud,pmd}_populate as you are more familiar with them).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; But what I don&#39;t get here is that you repopulate the pud page for every</span>
<span class="quote">&gt;&gt;&gt; pgd (and so on for pmd). You don&#39;t need this recursive call all the way</span>
<span class="quote">&gt;&gt;&gt; to kasan_early_pmd_populate() but just sequential:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This repopulation needed for 3,2 level page tables configurations.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; E.g. for 3-level page tables we need to call pud_populate(&amp;init_mm,</span>
<span class="quote">&gt;&gt; pud, kasan_zero_pmd) for each pud in [KASAN_SHADOW_START,</span>
<span class="quote">&gt;&gt; KASAN_SHADOW_END] range, this causes repopopulation for 4-level page</span>
<span class="quote">&gt;&gt; tables, since we need to pud_populate() only [KASAN_SHADOW_START,</span>
<span class="quote">&gt;&gt; KASAN_SHADOW_START + PGDIR_SIZE] range.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m referring to writing the same information multiple times over the</span>
<span class="quote">&gt; same entry. kasan_map_early_shadow() goes over each pgd entry and writes</span>
<span class="quote">&gt; the address of kasan_zero_pud. That&#39;s fine so far. However, in the same</span>
<span class="quote">&gt; loop you call kasan_early_pud_populate(). The latter retrieves the pud</span>
<span class="quote">&gt; page via pud_offset(pgd, start) which would always be kasan_zero_pud</span>

Not always. E.g. if we have 3-level page tables pud = pgd, pgd_populate() is nop, and
pud_populate in fact populates pgd.

pud_offset(pgd, start) will return (swapper_pg_dir + pgd_index(start)) and pud_populate()
will fill that entry with the address of kasan_zero_pmd. So we need to pud_populate() for
each pgd.
<span class="quote">
&gt; because that&#39;s what you wrote via pgd_populate() in each pgd entry. So</span>
<span class="quote">&gt; for each pgd entry, you keep populating the same kasan_zero_pud page</span>
<span class="quote">&gt; with pointers to kasan_zero_pmd. And so on for the pmd.</span>
<span class="quote">&gt; </span>

Yes, I&#39;m perfectly understand that. And this was done intentionally since I don&#39;t
see the way to make this work for all possible CONFIG_PGTABLE_LEVELS without rewrites
or without #ifdefs (and you didn&#39;t like them in v1).
<span class="quote">

&gt;&gt;&gt; 	kasan_early_pte_populate();</span>
<span class="quote">&gt;&gt;&gt; 	kasan_early_pmd_populate(..., pte);</span>
<span class="quote">&gt;&gt;&gt; 	kasan_early_pud_populate(..., pmd);</span>
<span class="quote">&gt;&gt;&gt; 	kasan_early_pgd_populate(..., pud);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; (or in reverse order)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Unless, I&#39;m missing something, this will either work only with 4-level</span>
<span class="quote">&gt;&gt; page tables. We could do this without repopulation by using</span>
<span class="quote">&gt;&gt; CONFIG_PGTABLE_LEVELS ifdefs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or you could move kasan_early_*_populate outside the loop. You already</span>
<span class="quote">&gt; do this for the pte at the beginning of the kasan_map_early_shadow()</span>
<span class="quote">&gt; function (and it probably makes more sense to create a separate</span>
<span class="quote">&gt; kasan_early_pte_populate).</span>
<span class="quote">&gt; </span>


Ok, let&#39;s try to implement that.
And for example, let&#39;s consider CONFIG_PGTABLE_LEVELS=3 case:

 * pgd_populate() is nop, so kasan_early_pgd_populate() won&#39;t do anything.

 * pud_populate() in kasan_early_pud_populate() actually will setup pgd entries in swapper_pg_dir,
   so pud_populate() should be called for the whole shadow range: [KASAN_SHADOW_START, KASAN_SHADOW_END]
	IOW: kasan_early_pud_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pmd);
	
	We will need to slightly change kasan_early_pud_populate() implementation for that
	(Current implementation implies that [start, end) addresses belong to one pgd)

	void kasan_early_pud_populate(unsigned long start, unsigned long end, pmd_t *pmd)
	{
		unsigned long addr;
		long next;

		for (addr = start; addr &lt; end; addr = next) {
			pud_t *pud = pud_offset(pgd_offset_k(addr), addr);
			pud_populate(&amp;init_mm, pud, pmd);
			next = pud_addr_end(addr, pgd_addr_end(addr, end));
		}
	}

	But, wait! In 4-level page tables case this will be the same repopulation as we had before!

See? The problem here is that pud_populate() but not pgd_populate() populates pgds (3-level page tables case).
So I still don&#39;t see the way to avoid repopulation without ifdefs.
Did I miss anything?


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 15, 2015, 4:37 p.m.</div>
<pre class="content">
On Wed, Jul 15, 2015 at 11:55:20AM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; On 07/14/2015 06:04 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Fri, Jul 10, 2015 at 08:11:03PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; 	kasan_early_pte_populate();</span>
<span class="quote">&gt; &gt;&gt;&gt; 	kasan_early_pmd_populate(..., pte);</span>
<span class="quote">&gt; &gt;&gt;&gt; 	kasan_early_pud_populate(..., pmd);</span>
<span class="quote">&gt; &gt;&gt;&gt; 	kasan_early_pgd_populate(..., pud);</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; (or in reverse order)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Unless, I&#39;m missing something, this will either work only with 4-level</span>
<span class="quote">&gt; &gt;&gt; page tables. We could do this without repopulation by using</span>
<span class="quote">&gt; &gt;&gt; CONFIG_PGTABLE_LEVELS ifdefs.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Or you could move kasan_early_*_populate outside the loop. You already</span>
<span class="quote">&gt; &gt; do this for the pte at the beginning of the kasan_map_early_shadow()</span>
<span class="quote">&gt; &gt; function (and it probably makes more sense to create a separate</span>
<span class="quote">&gt; &gt; kasan_early_pte_populate).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, let&#39;s try to implement that.</span>
<span class="quote">&gt; And for example, let&#39;s consider CONFIG_PGTABLE_LEVELS=3 case:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  * pgd_populate() is nop, so kasan_early_pgd_populate() won&#39;t do anything.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  * pud_populate() in kasan_early_pud_populate() actually will setup pgd entries in swapper_pg_dir,</span>
<span class="quote">&gt;    so pud_populate() should be called for the whole shadow range: [KASAN_SHADOW_START, KASAN_SHADOW_END]</span>
<span class="quote">&gt; 	IOW: kasan_early_pud_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pmd);</span>
<span class="quote">&gt; 	</span>
<span class="quote">&gt; 	We will need to slightly change kasan_early_pud_populate() implementation for that</span>
<span class="quote">&gt; 	(Current implementation implies that [start, end) addresses belong to one pgd)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	void kasan_early_pud_populate(unsigned long start, unsigned long end, pmd_t *pmd)</span>
<span class="quote">&gt; 	{</span>
<span class="quote">&gt; 		unsigned long addr;</span>
<span class="quote">&gt; 		long next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		for (addr = start; addr &lt; end; addr = next) {</span>
<span class="quote">&gt; 			pud_t *pud = pud_offset(pgd_offset_k(addr), addr);</span>
<span class="quote">&gt; 			pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; 			next = pud_addr_end(addr, pgd_addr_end(addr, end));</span>
<span class="quote">&gt; 		}</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	But, wait! In 4-level page tables case this will be the same repopulation as we had before!</span>

Ok, so simply taking the call out of the loop won&#39;t work unless we
conditionally define these functions (wouldn&#39;t be too bad since we have
some #if CONFIG_PGTABLE_LEVELS already introduced by this patch but it
would be nicer without).

Anyway, I think we can keep the current iterations but exit early if
!pud_none() because it means we already populated it (reworked to match
other such patterns throughout the kernel with pgd_populate called from
the pud function; and untested):

void kasan_early_pmd_populate(pud_t *pud, unsigned long addr, unsigned long end)
{
	pmd_t *pmd;
	unsigned long next;

	if (pud_none(*pud))
		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);

	pmd = pmd_offset(pud, addr);
	do {
		next = pmd_addr_end(addr, end);
		kasan_early_pte_populate(pmd, addr, next);
	} while (pmd++, addr = next, addr != end &amp;&amp; pmd_none(*pmd));
}

void kasan_early_pud_populate(pgd_t *pgd, unsigned long addr, unsigned long end)
{
	pud_t *pud;
	unsigned long next;

	if (pgd_none(*pgd))
		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);

	pud = pud_offset(pgd, addr);
	do {
		next = pud_addr_end(addr, end);
		kasan_early_pmd_populate(pud, addr, next);
	} while (pud++, addr = next, addr != end &amp;&amp; pud_none(*pud));
}

Given that we check pud_none() after the first iterations, it covers the
lower levels if needed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 16, 2015, 3:30 p.m.</div>
<pre class="content">
On 07/15/2015 07:37 PM, Catalin Marinas wrote:
<span class="quote">&gt; Ok, so simply taking the call out of the loop won&#39;t work unless we</span>
<span class="quote">&gt; conditionally define these functions (wouldn&#39;t be too bad since we have</span>
<span class="quote">&gt; some #if CONFIG_PGTABLE_LEVELS already introduced by this patch but it</span>
<span class="quote">&gt; would be nicer without).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway, I think we can keep the current iterations but exit early if</span>
<span class="quote">&gt; !pud_none() because it means we already populated it (reworked to match</span>
<span class="quote">&gt; other such patterns throughout the kernel with pgd_populate called from</span>
<span class="quote">&gt; the pud function; and untested):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void kasan_early_pmd_populate(pud_t *pud, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	pmd_t *pmd;</span>
<span class="quote">&gt; 	unsigned long next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (pud_none(*pud))</span>
<span class="quote">&gt; 		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; 	do {</span>
<span class="quote">&gt; 		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; 		kasan_early_pte_populate(pmd, addr, next);</span>
<span class="quote">&gt; 	} while (pmd++, addr = next, addr != end &amp;&amp; pmd_none(*pmd));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void kasan_early_pud_populate(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	pud_t *pud;</span>
<span class="quote">&gt; 	unsigned long next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (pgd_none(*pgd))</span>
<span class="quote">&gt; 		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; 	do {</span>
<span class="quote">&gt; 		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; 		kasan_early_pmd_populate(pud, addr, next);</span>
<span class="quote">&gt; 	} while (pud++, addr = next, addr != end &amp;&amp; pud_none(*pud));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that we check pud_none() after the first iterations, it covers the</span>
<span class="quote">&gt; lower levels if needed.</span>
<span class="quote">&gt; </span>

I think this may work, if pud_none(*pud) will be replaced with !pud_val(*pud).
We can&#39;t use pud_none() because with 2-level page tables it&#39;s always false, so
we will never go down to pmd level where swapper_pg_dir populated.

But you gave me another idea how we could use p?d_none() and avoid rewriting table entries:


void kasan_early_pmd_populate(unsigned long start, unsigned long end, pte_t *pte)
{
	unsigned long addr = start;
	long next;

	do {
		pgd_t *pgd = pgd_offset_k(addr);
		pud_t *pud = pud_offset(pgd, addr);
		pmd_t *pmd = pmd_offset(pud, addr);

		if (!pmd_none(*pmd))
			break;

		pmd_populate_kernel(&amp;init_mm, pmd, pte);
		next = pgd_addr_end(addr, end);
		next = pud_addr_end(addr, next)
		next = pmd_addr_end(addr, next);
	} while(addr = next, addr != end);
}

void kasan_early_pud_populate(unsigned long start, unsigned long end, pmd_t *pmd)
{
	unsigned long addr = start;
	long next;

	do {
		pgd_t *pgd = pgd_offset_k(addr);
		pud_t *pud = pud_offset(pgd, addr);

		if (!pud_none(*pud))
			break;

		pud_populate(&amp;init_mm, pud, pmd);
		next = pud_addr_end(addr, pgd_addr_end(addr, end));
	} while(addr = next, addr != end);
}


void kasan_early_pgd_populate(...)
{
	//something similar to above
	....
}

static void __init kasan_map_early_shadow(void)
{
	kasan_early_pgd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pud);
	kasan_early_pud_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pmd);
	kasan_early_pmd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pte);
	kasan_early_pte_populate();
}





--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - July 16, 2015, 4:03 p.m.</div>
<pre class="content">
On Thu, Jul 16, 2015 at 06:30:11PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; On 07/15/2015 07:37 PM, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; Ok, so simply taking the call out of the loop won&#39;t work unless we</span>
<span class="quote">&gt; &gt; conditionally define these functions (wouldn&#39;t be too bad since we have</span>
<span class="quote">&gt; &gt; some #if CONFIG_PGTABLE_LEVELS already introduced by this patch but it</span>
<span class="quote">&gt; &gt; would be nicer without).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Anyway, I think we can keep the current iterations but exit early if</span>
<span class="quote">&gt; &gt; !pud_none() because it means we already populated it (reworked to match</span>
<span class="quote">&gt; &gt; other such patterns throughout the kernel with pgd_populate called from</span>
<span class="quote">&gt; &gt; the pud function; and untested):</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; void kasan_early_pmd_populate(pud_t *pud, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt; 	pmd_t *pmd;</span>
<span class="quote">&gt; &gt; 	unsigned long next;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (pud_none(*pud))</span>
<span class="quote">&gt; &gt; 		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; &gt; 	do {</span>
<span class="quote">&gt; &gt; 		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; 		kasan_early_pte_populate(pmd, addr, next);</span>
<span class="quote">&gt; &gt; 	} while (pmd++, addr = next, addr != end &amp;&amp; pmd_none(*pmd));</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; void kasan_early_pud_populate(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt; 	pud_t *pud;</span>
<span class="quote">&gt; &gt; 	unsigned long next;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	if (pgd_none(*pgd))</span>
<span class="quote">&gt; &gt; 		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; &gt; 	do {</span>
<span class="quote">&gt; &gt; 		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; 		kasan_early_pmd_populate(pud, addr, next);</span>
<span class="quote">&gt; &gt; 	} while (pud++, addr = next, addr != end &amp;&amp; pud_none(*pud));</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Given that we check pud_none() after the first iterations, it covers the</span>
<span class="quote">&gt; &gt; lower levels if needed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this may work, if pud_none(*pud) will be replaced with !pud_val(*pud).</span>
<span class="quote">&gt; We can&#39;t use pud_none() because with 2-level page tables it&#39;s always false, so</span>
<span class="quote">&gt; we will never go down to pmd level where swapper_pg_dir populated.</span>

The reason I used &quot;do ... while&quot; vs &quot;while&quot; or &quot;for&quot; is so that it gets
down to the pmd level. The iteration over pgd is always done in the top
loop via pgd_addr_end while the loops for missing levels (nopud, nopmd)
are always a single iteration whether we check for pud_none or not. But
when the level is present, we avoid looping when !pud_none().
<span class="quote">
&gt; But you gave me another idea how we could use p?d_none() and avoid rewriting table entries:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void kasan_early_pmd_populate(unsigned long start, unsigned long end, pte_t *pte)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	unsigned long addr = start;</span>
<span class="quote">&gt; 	long next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	do {</span>
<span class="quote">&gt; 		pgd_t *pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; 		pud_t *pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; 		pmd_t *pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (!pmd_none(*pmd))</span>
<span class="quote">&gt; 			break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="quote">&gt; 		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; 		next = pud_addr_end(addr, next)</span>
<span class="quote">&gt; 		next = pmd_addr_end(addr, next);</span>
<span class="quote">&gt; 	} while(addr = next, addr != end);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void kasan_early_pud_populate(unsigned long start, unsigned long end, pmd_t *pmd)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	unsigned long addr = start;</span>
<span class="quote">&gt; 	long next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	do {</span>
<span class="quote">&gt; 		pgd_t *pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; 		pud_t *pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (!pud_none(*pud))</span>
<span class="quote">&gt; 			break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; 		next = pud_addr_end(addr, pgd_addr_end(addr, end));</span>
<span class="quote">&gt; 	} while(addr = next, addr != end);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; void kasan_early_pgd_populate(...)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	//something similar to above</span>
<span class="quote">&gt; 	....</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static void __init kasan_map_early_shadow(void)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	kasan_early_pgd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pud);</span>
<span class="quote">&gt; 	kasan_early_pud_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pmd);</span>
<span class="quote">&gt; 	kasan_early_pmd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, kasan_zero_pte);</span>
<span class="quote">&gt; 	kasan_early_pte_populate();</span>
<span class="quote">&gt; }</span>

While this would probably work, you still need #ifdef&#39;s since
kasan_zero_pud is not defined with 2 and 3 levels. That&#39;s what I
initially thought we should do but since you didn&#39;t like the #ifdef&#39;s, I
came up with another proposal.

So, I still prefer my suggestion above unless you find a problem with
it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 17, 2015, 1:13 p.m.</div>
<pre class="content">
On 07/16/2015 07:03 PM, Catalin Marinas wrote:
<span class="quote">&gt; On Thu, Jul 16, 2015 at 06:30:11PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think this may work, if pud_none(*pud) will be replaced with !pud_val(*pud).</span>
<span class="quote">&gt;&gt; We can&#39;t use pud_none() because with 2-level page tables it&#39;s always false, so</span>
<span class="quote">&gt;&gt; we will never go down to pmd level where swapper_pg_dir populated.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The reason I used &quot;do ... while&quot; vs &quot;while&quot; or &quot;for&quot; is so that it gets</span>
<span class="quote">&gt; down to the pmd level. The iteration over pgd is always done in the top</span>
<span class="quote">&gt; loop via pgd_addr_end while the loops for missing levels (nopud, nopmd)</span>
<span class="quote">&gt; are always a single iteration whether we check for pud_none or not. But</span>
<span class="quote">&gt; when the level is present, we avoid looping when !pud_none().</span>
<span class="quote">&gt; </span>

Right, dunno what I was thinking.
It seems to work. Lightly tested with every possible CONFIG_PGTABLE_LEVELS.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - July 21, 2015, 10:36 a.m.</div>
<pre class="content">
On Wed, Jun 17, 2015 at 11:32 PM, Andrey Ryabinin
&lt;ryabinin.a.a@gmail.com&gt; wrote:
<span class="quote">&gt; 2015-06-13 18:25 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Fri, Jun 12, 2015 at 8:14 PM, Andrey Ryabinin &lt;ryabinin.a.a@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; 2015-06-11 16:39 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt; &gt;&gt; On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; (see Documentation/kasan.txt).</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; I looked closer at this again ... I am trying to get KASan up for</span>
<span class="quote">&gt;&gt; &gt;&gt; ARM(32) with some tricks and hacks.</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; I have some patches for that. They still need some polishing, but works for me.</span>
<span class="quote">&gt;&gt; &gt; I could share after I get back to office on Tuesday.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; OK! I&#39;d be happy to test!</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;ve pushed it here : git://github.com/aryabinin/linux.git kasan/arm_v0</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It far from ready. Firstly I&#39;ve tried it only in qemu and it works.</span>

Hm what QEMU model are you using? I tried to test it with Versatile
(the most common) and it kinda boots and hangs:

Memory: 106116K/131072K available (3067K kernel code, 166K rwdata,
864K rodata, 3072K init, 130K bss, 24956K reserved, 0K cma-reserved)
Virtual kernel memory layout:
    vector  : 0xffff0000 - 0xffff1000   (   4 kB)
    fixmap  : 0xffc00000 - 0xfff00000   (3072 kB)
    kasan   : 0x9f000000 - 0xbf000000   ( 512 MB)
    vmalloc : 0xc8800000 - 0xff000000   ( 872 MB)
(...)

Looks correct, no highmem on this beast.

Then I get this.

Unable to handle kernel NULL pointer dereference at virtual address 00000130
pgd = c5ea8000
[00000130] *pgd=00000000
Internal error: Oops: 5 [#1] ARM
Modules linked in:
CPU: 0 PID: 19 Comm: modprobe Not tainted 4.1.0-rc8+ #7
Hardware name: ARM-Versatile (Device Tree Support)
task: c5e0b5a0 ti: c5ea0000 task.ti: c5ea0000
PC is at v4wbi_flush_user_tlb_range+0x10/0x4c
LR is at move_page_tables+0x218/0x308
pc : [&lt;c001e870&gt;]    lr : [&lt;c008f230&gt;]    psr: 20000153
sp : c5ea7df0  ip : c5e8c000  fp : ff8ec000
r10: 00bf334f  r9 : c5ead3b0  r8 : 9e8ec000
r7 : 00000001  r6 : 00002000  r5 : 9f000000  r4 : 9effe000
r3 : 00000000  r2 : c5e8a000  r1 : 9f000000  r0 : 9effe000
Flags: nzCv  IRQs on  FIQs off  Mode SVC_32  ISA ARM  Segment user
Control: 00093177  Table: 05ea8000  DAC: 00000015
Process modprobe (pid: 19, stack limit = 0xc5ea0190)
Stack: (0xc5ea7df0 to 0xc5ea8000)
7de0:                                     00000000 9f000000 c5e8a000 00000000
7e00: 00000000 00000000 c5e8a000 9effe000 000000c0 c5e8a000 c68c5700 9e8ec000
7e20: c5e8c034 9effe000 9e8ea000 9f000000 00002000 c00a9384 00002000 00000000
7e40: c5e8c000 00000000 00000000 c5e8a000 00000000 00000000 00000000 00000000
7e60: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
7e80: c5e870e0 5e5e8830 c0701808 00000013 c5ea7ec0 c5eac000 c5e870e0 c5e26140
7ea0: c68c5700 00000000 c5ea7ec0 c5eac000 c5e870e0 c6a2d8c0 00000001 c00e4530
7ec0: c68c5700 00000080 c5df2480 9f000000 00000000 c5ea0000 0000000a c00a99ec
7ee0: 00000017 c5ea7ef4 00000000 00000000 00000000 c7f10e60 c0377bc0 c0bf3f99
7f00: 0000000f 00000000 00000000 c00a9c28 9efff000 c06fa2c8 c68c5700 c06f9eb8
7f20: 00000001 fffffff8 c68c5700 00000000 00000001 c00a9770 c5e0b5a0 00000013
7f40: c5e87ee0 c06ef0c8 c6a60000 c00aabd0 c5e8c034 00000000 00000000 c5e0b790
7f60: c06e8190 c5ddcc60 c5d4c300 0000003f ffffffff 00000000 00000000 00000000
7f80: 00000000 c00aad00 00000000 00000000 00000000 c0030f48 c5d4c300 c0030e54
7fa0: 00000000 00000000 00000000 c0014960 00000000 00000000 00000000 00000000
7fc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
7fe0: 00000000 00000000 00000000 00000000 00000013 00000000 00000000 00000000
[&lt;c001e870&gt;] (v4wbi_flush_user_tlb_range) from [&lt;00000000&gt;] (  (null))
Code: e592c020 e3cd3d7f e3c3303f e593300c (e5933130)
---[ end trace b3c4eba35670ba77 ]---

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 21, 2015, 2:27 p.m.</div>
<pre class="content">
On 07/21/2015 01:36 PM, Linus Walleij wrote:
<span class="quote">&gt; On Wed, Jun 17, 2015 at 11:32 PM, Andrey Ryabinin</span>
<span class="quote">&gt; &lt;ryabinin.a.a@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; 2015-06-13 18:25 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Fri, Jun 12, 2015 at 8:14 PM, Andrey Ryabinin &lt;ryabinin.a.a@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; 2015-06-11 16:39 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Fri, May 15, 2015 at 3:59 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; This patch adds arch specific code for kernel address sanitizer</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; (see Documentation/kasan.txt).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I looked closer at this again ... I am trying to get KASan up for</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; ARM(32) with some tricks and hacks.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; I have some patches for that. They still need some polishing, but works for me.</span>
<span class="quote">&gt;&gt;&gt;&gt; I could share after I get back to office on Tuesday.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; OK! I&#39;d be happy to test!</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;ve pushed it here : git://github.com/aryabinin/linux.git kasan/arm_v0</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It far from ready. Firstly I&#39;ve tried it only in qemu and it works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm what QEMU model are you using? I tried to test it with Versatile</span>
<span class="quote">&gt; (the most common) and it kinda boots and hangs:</span>
<span class="quote">&gt; </span>

I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update
with a lot of stuff fixed, and it works on hardware.
I still need to do some work on it and tomorrow, probably, I will share.




--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - July 21, 2015, 9:27 p.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:
<span class="quote">
&gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt; I still need to do some work on it and tomorrow, probably, I will share.</span>

Ah awesome. I have a stash of ARM boards so I can test it on a
range of hardware once you feel it&#39;s ready.

Sorry for pulling stuff out of your hands, people are excited about
KASan ARM32 as it turns out.

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=86891">Andrey Ryabinin</a> - July 22, 2015, 5:54 p.m.</div>
<pre class="content">
On 07/22/2015 12:27 AM, Linus Walleij wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt;&gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt;&gt; I still need to do some work on it and tomorrow, probably, I will share.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah awesome. I have a stash of ARM boards so I can test it on a</span>
<span class="quote">&gt; range of hardware once you feel it&#39;s ready.</span>
<span class="quote">&gt; </span>

So here is updated version:
	git://github.com/aryabinin/linux.git kasan/arm_v0_1

The code is still ugly in some places and it probably have some bugs.
Lightly tested on exynos 5410/5420.
<span class="quote">

&gt; Sorry for pulling stuff out of your hands, people are excited about</span>
<span class="quote">&gt; KASan ARM32 as it turns out.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yours,</span>
<span class="quote">&gt; Linus Walleij</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - Aug. 19, 2015, 12:14 p.m.</div>
<pre class="content">
On Wed, Jul 22, 2015 at 7:54 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:
<span class="quote">
&gt; So here is updated version:</span>
<span class="quote">&gt;         git://github.com/aryabinin/linux.git kasan/arm_v0_1</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The code is still ugly in some places and it probably have some bugs.</span>
<span class="quote">&gt; Lightly tested on exynos 5410/5420.</span>

I compiled this for various ARM platforms and tested to boot.
I used GCC version 4.9.3 20150113 (prerelease) (Linaro).

I get these compilation warnings no matter what I compile,
I chose to ignore them:

WARNING: vmlinux.o(.meminit.text+0x2c):
Section mismatch in reference from the function kasan_pte_populate()
to the function
.init.text:kasan_alloc_block.constprop.7()
The function __meminit kasan_pte_populate() references
a function __init kasan_alloc_block.constprop.7().
If kasan_alloc_block.constprop.7 is only used by kasan_pte_populate then
annotate kasan_alloc_block.constprop.7 with a matching annotation.

WARNING: vmlinux.o(.meminit.text+0x98):
Section mismatch in reference from the function kasan_pmd_populate()
to the function
.init.text:kasan_alloc_block.constprop.7()
The function __meminit kasan_pmd_populate() references
a function __init kasan_alloc_block.constprop.7().
If kasan_alloc_block.constprop.7 is only used by kasan_pmd_populate then
annotate kasan_alloc_block.constprop.7 with a matching annotation.

These KASan outline tests run fine:

kasan test: kmalloc_oob_right out-of-bounds to right
kasan test: kmalloc_oob_left out-of-bounds to left
kasan test: kmalloc_node_oob_right kmalloc_node(): out-of-bounds to right
kasan test: kmalloc_large_oob_rigth kmalloc large allocation:
out-of-bounds to right
kasan test: kmalloc_oob_krealloc_more out-of-bounds after krealloc more
kasan test: kmalloc_oob_krealloc_less out-of-bounds after krealloc less
kasan test: kmalloc_oob_16 kmalloc out-of-bounds for 16-bytes access
kasan test: kmalloc_oob_in_memset out-of-bounds in memset
kasan test: kmalloc_uaf use-after-free
kasan test: kmalloc_uaf_memset use-after-free in memset
kasan test: kmalloc_uaf2 use-after-free after another kmalloc
kasan test: kmem_cache_oob out-of-bounds in kmem_cache_alloc

These two tests seems to not trigger KASan BUG()s, and seemse to
be like so on all hardware, so I guess it is this kind of test
that requires GCC 5.0:

kasan test: kasan_stack_oob out-of-bounds on stack
kasan test: kasan_global_oob out-of-bounds global variable


Hardware test targets:

Ux500 (ARMv7):

On Ux500 I get a real slow boot (as exepected) and after
enabling the test cases produce KASan warnings
expectedly.

MSM APQ8060 (ARMv7):

Also a real slow boot and the expected KASan warnings when
running the tests.

Integrator/AP (ARMv5):

This one mounted with an ARMv5 ARM926 tile. It boots nicely
(but takes forever) with KASan and run all test cases (!) just like
for the other platforms but before reaching userspace this happens:

Unable to handle kernel paging request at virtual address 00021144
pgd = c5a74000
[00021144] *pgd=00000000
Internal error: Oops: 5 [#1] PREEMPT ARM
Modules linked in:
CPU: 0 PID: 24 Comm: modprobe Tainted: G    B
4.2.0-rc2-77613-g11c2df68e4a8 #1
Hardware name: ARM Integrator/AP (Device Tree)
task: c69f8cc0 ti: c5a68000 task.ti: c5a68000
PC is at v4wbi_flush_user_tlb_range+0x10/0x4c
LR is at move_page_tables+0x320/0x46c
pc : [&lt;c00182d0&gt;]    lr : [&lt;c00ce9d0&gt;]    psr: 60000013
sp : c5a6bd78  ip : c5a70000  fp : 9f000000
r10: 9eaab000  r9 : ffaab000  r8 : 0093d34f
r7 : c5a782ac  r6 : c5a68000  r5 : 9effe000  r4 : c0900044
r3 : 00021000  r2 : c5a4c000  r1 : 9f000000  r0 : 9effe000
Flags: nZCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment user
Control: 0005317f  Table: 05a74000  DAC: 00000015
Process modprobe (pid: 24, stack limit = 0xc5a68190)
Stack: (0xc5a6bd78 to 0xc5a6c000)
bd60:                                                       9f000000 00002000
bd80: 00000000 9f000000 c5a4c000 00000000 00000000 c5a4c020 c5a68000 c5a68004
bda0: c5a4c000 9effe000 00000000 c5a4c000 c59a6540 c5a4c004 c5a70034 c59a65cc
bdc0: 9effe000 00000000 00002000 c00f4558 00002000 00000000 00000000 9f000000
bde0: 9eaa9000 c5a70000 9eaab000 c5a4c04c 00000000 c5a4c000 00000000 00000000
be00: 00000000 00000000 00000000 0f4a4438 b0a5485b 8881364f c0910aec 00000000
be20: 00000000 00000000 c69f8cc0 0f4a4438 c0910aec 00000018 9f000000 c59a68c0
be40: c69f8cc0 c592db40 c59a6540 c592db50 9f000000 c59a68c0 c69f8cc0 00c00000
be60: c5a6be68 c015694c c59a6540 00000080 c5a4aa80 c59a65d8 c5e9bc00 c592db6c
be80: c6a0da00 c592db40 00000001 00000000 c59a6540 c00f4f10 00000000 00002000
bea0: 9f000000 c5a4c000 c69f8cc0 c00f5008 00000017 c5a6bec4 00000000 00000008
bec0: 9efff000 c00b5160 c7e937a0 c00a4684 00000000 c7e937a0 9efff000 c09089a8
bee0: c09089b0 c59a6540 c0908590 fffffff8 c59a65d4 c5a68000 c5a68004 c00f4b28
bf00: c59a65c8 00000001 00000018 c69f8cc0 c5a4ab60 00000018 c6a38000 c59a6540
bf20: 00000001 c59a65e8 c59a65cc c00f6fec c090476c 00000000 c59a65c8 0000038f
bf40: c5a4c028 c59a65c0 c59a65f0 c5a4c004 00000000 c69f8ec0 00000000 c6a38000
bf60: c5a11a00 c5a4ab60 00000000 00000000 ffffffff 00000000 00000000 c00f71a4
bf80: 00000000 ffffffff 00000000 c00361fc c5a11a00 c0036044 00000000 00000000
bfa0: 00000000 00000000 00000000 c000aa60 00000000 00000000 00000000 00000000
bfc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
bfe0: 00000000 00000000 00000000 00000000 00000013 00000000 00002000 41000001
[&lt;c00182d0&gt;] (v4wbi_flush_user_tlb_range) from [&lt;9f000000&gt;] (0x9f000000)
Code: e592c020 e3cd3d7f e3c3303f e593300c (e5933144)
---[ end trace 7fa2f634c630ab6d ]---


Compaq iPAQ H3600 (ARMv4):

This is an ARMv4 machine, SA1100-based. It boots nicely
with KASan and run all test cases (!) just like for the other platforms
but before reaching userspace this happens:

Unable to handle kernel paging request at virtual address e3e57b43
pgd = c08e8000
[e3e57b43] *pgd=00000000
Internal error: Oops: 408eb003 [#1] PREEMPT ARM
Modules linked in:
CPU: 0 PID: 944 Comm: modprobe Tainted: G    B
4.2.0-rc2-77612-g66bb8b6c242c #1
Hardware name: Compaq iPAQ H3600
task: c14d4880 ti: c08dc000 task.ti: c08dc000
PC is at v4wb_flush_user_tlb_range+0x10/0x48
LR is at change_protection_range+0x3c8/0x464
pc : [&lt;c001cb10&gt;]    lr : [&lt;c00c9d58&gt;]    psr: 20000013
sp : c08dfd20  ip : c08e6000  fp : 00000000
r10: c08dc000  r9 : c08ea7c8  r8 : 00000001
r7 : 9f000000  r6 : 9f000000  r5 : c08ed800  r4 : c1ff834f
r3 : e3e579ff  r2 : c08ec000  r1 : 9f000000  r0 : 9effe000
Flags: nzCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment user
Control: c08eb17f  Table: c08eb17f  DAC: 00000015
Process modprobe (pid: 944, stack limit = 0xc08dc190)
Stack: (0xc08dfd20 to 0xc08e0000)
fd20: 00000001 9f000000 9effffff c08ec000 00000181 c08dc004 c14bc050 c08ea7c0
fd40: c08dc000 c08e6000 9effe000 c08e6170 c0800d90 00000000 c08ec000 00118177
fd60: 9effe000 00000000 00118173 9f000000 c08e6000 c00c9f50 00000000 00000000
fd80: 00000000 0009effe 00000000 c00e9868 00000000 00000002 c08ef000 00000000
fda0: 00000000 c08ec000 c12d4000 c08ec004 c08e6034 c12d408c 00118177 9effe000
fdc0: 001b8000 c00f0a24 00118177 ffffa1d9 00000000 00100177 00000000 00000000
fde0: 00000000 00000000 00000000 c08ec000 00000000 00000000 00000000 00000000
fe00: 00000000 673bd3e6 80a09842 cd271319 c08087f8 00000000 00000000 00000000
fe20: c14d4880 673bd3e6 c08087f8 000003b0 9f000000 c08f0000 c14d4880 c0ace8c0
fe40: c12d4000 c0ace8d0 9f000000 c08f0000 c14d4880 00c00000 c08dfe60 c015474c
fe60: c12d4000 00000080 c08e5d20 c12d4098 c08f0d80 c0ace8ec c14bc000 c0ace8c0
fe80: 00000002 00000000 9f000000 00000000 c12d4000 c00f14dc 00000000 00002000
fea0: 9f000000 c08ec000 c14d4880 c00f15d4 00000017 c08dfec4 00000000 9efff000
fec0: 00000000 c00b0eb4 c1f38f00 c00a3590 00000000 c0801f28 c12d4000 c08dc000
fee0: c08dc004 fffffff8 c0801b10 c12d4094 00000000 c00f118c c12d4098 c12d4088
ff00: 00000001 c12d4001 000003b0 c14d4880 c08e5e00 000003b0 c1494300 c12d4000
ff20: 00000001 c12d40a8 c12d408c c00f35e8 c07fdd8c 00000000 c12d4088 0000038f
ff40: c08ec028 c12d4080 c12d40b0 c08ec004 00000000 c14d4a80 00000000 c1494300
ff60: c08db300 c08e5e00 00000000 00000000 ffffffff 00000000 00000000 c00f3778
ff80: 00000000 ffffffff 00000000 c0038f3c c08db300 c0038d40 00000000 00000000
ffa0: 00000000 00000000 00000000 c00108a0 00000000 00000000 00000000 00000000
ffc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
ffe0: 00000000 00000000 00000000 00000000 00000013 00000000 e85b38dd 6e46c80d
[&lt;c001cb10&gt;] (v4wb_flush_user_tlb_range) from [&lt;c08ec000&gt;] (0xc08ec000)
Code: e592c020 e3cd3d7f e3c3303f e593300c (e5933144)
---[ end trace 66be5d28dde42c9f ]---
random: nonblocking pool is initialized

So it seems v4wb_flush_user_tlb_range+0x10/0x48 is the culprit
on these two systems. It is in arch/arm/mm/tlb-v4wb.S.

Uninstrumenting these files with ASAN_SANITIZE_tlb-v4wb.o := n does
not help.

I then tested on the Footbridge, another ARMv4 system, the oldest I have
SA110-based. This passes decompression and then you may *think* it hangs.
But it doesn&#39;t. It just takes a few minutes to boot with KASan
instrumentation, then all tests run fine also on this hardware.
The crash logs scroll by on the physical console.

They keep scrolling forever however, and are still scrolling as I
write this. I suspect some real memory usage bugs to be causing it,
as it is exercising some ages old code that didn&#39;t see much scrutiny
in recent years.


Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=709">Russell King - ARM Linux</a> - Aug. 24, 2015, 1:15 p.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 11:27:56PM +0200, Linus Walleij wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt; &gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt; &gt; I still need to do some work on it and tomorrow, probably, I will share.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah awesome. I have a stash of ARM boards so I can test it on a</span>
<span class="quote">&gt; range of hardware once you feel it&#39;s ready.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry for pulling stuff out of your hands, people are excited about</span>
<span class="quote">&gt; KASan ARM32 as it turns out.</span>

People may be excited about it because it&#39;s a new feature, but we really
need to consider whether gobbling up 512MB of userspace for it is a good
idea or not.  There are programs around which like to map large amounts
of memory into their process space, and the more we steal from them, the
more likely these programs are to fail.

The other thing which I&#39;m not happy about is having a 16K allocation per
thread - the 16K allocation for the PGD is already prone to invoking the
OOM killer after memory fragmentation has set in, we don&#39;t need another
16K allocation.  We&#39;re going from one 16K allocation per process to that
_and_ one 16K allocation per thread.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=18191">Linus Walleij</a> - Aug. 24, 2015, 1:45 p.m.</div>
<pre class="content">
On Mon, Aug 24, 2015 at 3:15 PM, Russell King - ARM Linux
&lt;linux@arm.linux.org.uk&gt; wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 11:27:56PM +0200, Linus Walleij wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt;&gt; &gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt;&gt; &gt; I still need to do some work on it and tomorrow, probably, I will share.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ah awesome. I have a stash of ARM boards so I can test it on a</span>
<span class="quote">&gt;&gt; range of hardware once you feel it&#39;s ready.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sorry for pulling stuff out of your hands, people are excited about</span>
<span class="quote">&gt;&gt; KASan ARM32 as it turns out.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; People may be excited about it because it&#39;s a new feature, but we really</span>
<span class="quote">&gt; need to consider whether gobbling up 512MB of userspace for it is a good</span>
<span class="quote">&gt; idea or not.  There are programs around which like to map large amounts</span>
<span class="quote">&gt; of memory into their process space, and the more we steal from them, the</span>
<span class="quote">&gt; more likely these programs are to fail.</span>

I looked at some different approaches over the last weeks for this
when playing around with KASan.

It seems since KASan was developed on 64bit systems, this was
not much of an issue for them as they could take their shadow
memory from the vmalloc space.

I think it is possible to actually just steal as much memory as is
needed to cover the kernel, and not 1/8 of the entire addressable
32bit space. So instead of covering all from 0x0-0xffffffff
at least just MODULES_VADDR thru 0xffffffff should be enough.
So if that is 0xbf000000-0xffffffff in most cases, 0x41000000
bytes, then 1/8 of that, 0x8200000, 130MB should be enough.
(Andrey need to say if this is possible.)

That will probably miss some usecases I&#39;m not familiar with, where
the kernel is actually executing something below 0xbf000000...

I looked at taking memory from vmalloc instead, but ran into
problems since this is subject to the highmem split and KASan
need to have it&#39;s address offset at compile time. On
Ux500 I managed to remove all the static maps and steal memory
from the top of the vmalloc area instead of the beginning, but
that is probably not generally feasible.

I suspect you have better ideas than what I can come up
with though.

Yours,
Linus Walleij
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100571">Andrey Ryabinin</a> - Aug. 24, 2015, 2:15 p.m.</div>
<pre class="content">
2015-08-24 16:45 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:
<span class="quote">&gt; On Mon, Aug 24, 2015 at 3:15 PM, Russell King - ARM Linux</span>
<span class="quote">&gt; &lt;linux@arm.linux.org.uk&gt; wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 11:27:56PM +0200, Linus Walleij wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt;&gt;&gt; &gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt;&gt;&gt; &gt; I still need to do some work on it and tomorrow, probably, I will share.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Ah awesome. I have a stash of ARM boards so I can test it on a</span>
<span class="quote">&gt;&gt;&gt; range of hardware once you feel it&#39;s ready.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Sorry for pulling stuff out of your hands, people are excited about</span>
<span class="quote">&gt;&gt;&gt; KASan ARM32 as it turns out.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; People may be excited about it because it&#39;s a new feature, but we really</span>
<span class="quote">&gt;&gt; need to consider whether gobbling up 512MB of userspace for it is a good</span>
<span class="quote">&gt;&gt; idea or not.  There are programs around which like to map large amounts</span>
<span class="quote">&gt;&gt; of memory into their process space, and the more we steal from them, the</span>
<span class="quote">&gt;&gt; more likely these programs are to fail.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I looked at some different approaches over the last weeks for this</span>
<span class="quote">&gt; when playing around with KASan.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It seems since KASan was developed on 64bit systems, this was</span>
<span class="quote">&gt; not much of an issue for them as they could take their shadow</span>
<span class="quote">&gt; memory from the vmalloc space.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think it is possible to actually just steal as much memory as is</span>
<span class="quote">&gt; needed to cover the kernel, and not 1/8 of the entire addressable</span>
<span class="quote">&gt; 32bit space. So instead of covering all from 0x0-0xffffffff</span>
<span class="quote">&gt; at least just MODULES_VADDR thru 0xffffffff should be enough.</span>
<span class="quote">&gt; So if that is 0xbf000000-0xffffffff in most cases, 0x41000000</span>
<span class="quote">&gt; bytes, then 1/8 of that, 0x8200000, 130MB should be enough.</span>
<span class="quote">&gt; (Andrey need to say if this is possible.)</span>
<span class="quote">&gt;</span>

Yes, ~130Mb (3G/1G split) should work. 512Mb shadow is optional.
The only advantage of 512Mb shadow is better handling of user memory
accesses bugs
(access to user memory without copy_from_user/copy_to_user/strlen_user etc API).
In case of 512Mb shadow we could to not map anything in shadow for
user addresses, so such bug will
guarantee  to crash the kernel.
In case of 130Mb, the behavior will depend on memory layout of the
current process.
So, I think it&#39;s fine to keep shadow only for kernel addresses.
<span class="quote">
&gt; That will probably miss some usecases I&#39;m not familiar with, where</span>
<span class="quote">&gt; the kernel is actually executing something below 0xbf000000...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I looked at taking memory from vmalloc instead, but ran into</span>
<span class="quote">&gt; problems since this is subject to the highmem split and KASan</span>
<span class="quote">&gt; need to have it&#39;s address offset at compile time. On</span>
<span class="quote">&gt; Ux500 I managed to remove all the static maps and steal memory</span>
<span class="quote">&gt; from the top of the vmalloc area instead of the beginning, but</span>
<span class="quote">&gt; that is probably not generally feasible.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I suspect you have better ideas than what I can come up</span>
<span class="quote">&gt; with though.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yours,</span>
<span class="quote">&gt; Linus Walleij</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=109631">Vladimir Murzin</a> - Aug. 24, 2015, 3:44 p.m.</div>
<pre class="content">
On 24/08/15 15:15, Andrey Ryabinin wrote:
<span class="quote">&gt; 2015-08-24 16:45 GMT+03:00 Linus Walleij &lt;linus.walleij@linaro.org&gt;:</span>
<span class="quote">&gt;&gt; On Mon, Aug 24, 2015 at 3:15 PM, Russell King - ARM Linux</span>
<span class="quote">&gt;&gt; &lt;linux@arm.linux.org.uk&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jul 21, 2015 at 11:27:56PM +0200, Linus Walleij wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, Jul 21, 2015 at 4:27 PM, Andrey Ryabinin &lt;a.ryabinin@samsung.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I used vexpress. Anyway, it doesn&#39;t matter now, since I have an update</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; with a lot of stuff fixed, and it works on hardware.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I still need to do some work on it and tomorrow, probably, I will share.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Ah awesome. I have a stash of ARM boards so I can test it on a</span>
<span class="quote">&gt;&gt;&gt;&gt; range of hardware once you feel it&#39;s ready.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Sorry for pulling stuff out of your hands, people are excited about</span>
<span class="quote">&gt;&gt;&gt;&gt; KASan ARM32 as it turns out.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; People may be excited about it because it&#39;s a new feature, but we really</span>
<span class="quote">&gt;&gt;&gt; need to consider whether gobbling up 512MB of userspace for it is a good</span>
<span class="quote">&gt;&gt;&gt; idea or not.  There are programs around which like to map large amounts</span>
<span class="quote">&gt;&gt;&gt; of memory into their process space, and the more we steal from them, the</span>
<span class="quote">&gt;&gt;&gt; more likely these programs are to fail.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I looked at some different approaches over the last weeks for this</span>
<span class="quote">&gt;&gt; when playing around with KASan.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It seems since KASan was developed on 64bit systems, this was</span>
<span class="quote">&gt;&gt; not much of an issue for them as they could take their shadow</span>
<span class="quote">&gt;&gt; memory from the vmalloc space.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think it is possible to actually just steal as much memory as is</span>
<span class="quote">&gt;&gt; needed to cover the kernel, and not 1/8 of the entire addressable</span>
<span class="quote">&gt;&gt; 32bit space. So instead of covering all from 0x0-0xffffffff</span>
<span class="quote">&gt;&gt; at least just MODULES_VADDR thru 0xffffffff should be enough.</span>
<span class="quote">&gt;&gt; So if that is 0xbf000000-0xffffffff in most cases, 0x41000000</span>
<span class="quote">&gt;&gt; bytes, then 1/8 of that, 0x8200000, 130MB should be enough.</span>
<span class="quote">&gt;&gt; (Andrey need to say if this is possible.)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, ~130Mb (3G/1G split) should work. 512Mb shadow is optional.</span>
<span class="quote">&gt; The only advantage of 512Mb shadow is better handling of user memory</span>
<span class="quote">&gt; accesses bugs</span>
<span class="quote">&gt; (access to user memory without copy_from_user/copy_to_user/strlen_user etc API).</span>
<span class="quote">&gt; In case of 512Mb shadow we could to not map anything in shadow for</span>
<span class="quote">&gt; user addresses, so such bug will</span>
<span class="quote">&gt; guarantee  to crash the kernel.</span>
<span class="quote">&gt; In case of 130Mb, the behavior will depend on memory layout of the</span>
<span class="quote">&gt; current process.</span>
<span class="quote">&gt; So, I think it&#39;s fine to keep shadow only for kernel addresses.</span>

Another option would be having &quot;sparse&quot; shadow memory based on page
extension. I did play with that some time ago based on ideas from
original v1 KASan support for x86/arm - it is how 614be38 &quot;irqchip:
gic-v3: Fix out of bounds access to cpu_logical_map&quot; was caught.
It doesn&#39;t require any VA reservations, only some contiguous memory for
the page_ext itself, which serves as indirection level for the 0-order
shadow pages.
In theory such design can be reused by others 32-bit arches and, I
think, nommu too. Additionally, the shadow pages might be movable with
help of driver-page migration patch series [1].
The cost is obvious - performance drop, although I didn&#39;t bother
measuring it.

[1] https://lwn.net/Articles/650917/

Cheers
Vladimir
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; That will probably miss some usecases I&#39;m not familiar with, where</span>
<span class="quote">&gt;&gt; the kernel is actually executing something below 0xbf000000...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I looked at taking memory from vmalloc instead, but ran into</span>
<span class="quote">&gt;&gt; problems since this is subject to the highmem split and KASan</span>
<span class="quote">&gt;&gt; need to have it&#39;s address offset at compile time. On</span>
<span class="quote">&gt;&gt; Ux500 I managed to remove all the static maps and steal memory</span>
<span class="quote">&gt;&gt; from the top of the vmalloc area instead of the beginning, but</span>
<span class="quote">&gt;&gt; that is probably not generally feasible.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I suspect you have better ideas than what I can come up</span>
<span class="quote">&gt;&gt; with though.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Yours,</span>
<span class="quote">&gt;&gt; Linus Walleij</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100571">Andrey Ryabinin</a> - Aug. 24, 2015, 4 p.m.</div>
<pre class="content">
2015-08-24 18:44 GMT+03:00 Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;:
<span class="quote">&gt;</span>
<span class="quote">&gt; Another option would be having &quot;sparse&quot; shadow memory based on page</span>
<span class="quote">&gt; extension. I did play with that some time ago based on ideas from</span>
<span class="quote">&gt; original v1 KASan support for x86/arm - it is how 614be38 &quot;irqchip:</span>
<span class="quote">&gt; gic-v3: Fix out of bounds access to cpu_logical_map&quot; was caught.</span>
<span class="quote">&gt; It doesn&#39;t require any VA reservations, only some contiguous memory for</span>
<span class="quote">&gt; the page_ext itself, which serves as indirection level for the 0-order</span>
<span class="quote">&gt; shadow pages.</span>

We won&#39;t be able to use inline instrumentation (I could live with that),
and most importantly, we won&#39;t be able to use stack instrumentation.
GCC needs to know shadow address for inline and/or stack instrumentation
to generate correct code.
<span class="quote">
&gt; In theory such design can be reused by others 32-bit arches and, I</span>
<span class="quote">&gt; think, nommu too. Additionally, the shadow pages might be movable with</span>
<span class="quote">&gt; help of driver-page migration patch series [1].</span>
<span class="quote">&gt; The cost is obvious - performance drop, although I didn&#39;t bother</span>
<span class="quote">&gt; measuring it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [1] https://lwn.net/Articles/650917/</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Cheers</span>
<span class="quote">&gt; Vladimir</span>
<span class="quote">&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=109631">Vladimir Murzin</a> - Aug. 24, 2015, 4:16 p.m.</div>
<pre class="content">
On 24/08/15 17:00, Andrey Ryabinin wrote:
<span class="quote">&gt; 2015-08-24 18:44 GMT+03:00 Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Another option would be having &quot;sparse&quot; shadow memory based on page</span>
<span class="quote">&gt;&gt; extension. I did play with that some time ago based on ideas from</span>
<span class="quote">&gt;&gt; original v1 KASan support for x86/arm - it is how 614be38 &quot;irqchip:</span>
<span class="quote">&gt;&gt; gic-v3: Fix out of bounds access to cpu_logical_map&quot; was caught.</span>
<span class="quote">&gt;&gt; It doesn&#39;t require any VA reservations, only some contiguous memory for</span>
<span class="quote">&gt;&gt; the page_ext itself, which serves as indirection level for the 0-order</span>
<span class="quote">&gt;&gt; shadow pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We won&#39;t be able to use inline instrumentation (I could live with that),</span>
<span class="quote">&gt; and most importantly, we won&#39;t be able to use stack instrumentation.</span>
<span class="quote">&gt; GCC needs to know shadow address for inline and/or stack instrumentation</span>
<span class="quote">&gt; to generate correct code.</span>

It&#39;s definitely a trade-off ;)

Just for my understanding does that stack instrumentation is controlled
via -asan-stack?

Thanks
Vladimir
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; In theory such design can be reused by others 32-bit arches and, I</span>
<span class="quote">&gt;&gt; think, nommu too. Additionally, the shadow pages might be movable with</span>
<span class="quote">&gt;&gt; help of driver-page migration patch series [1].</span>
<span class="quote">&gt;&gt; The cost is obvious - performance drop, although I didn&#39;t bother</span>
<span class="quote">&gt;&gt; measuring it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; [1] https://lwn.net/Articles/650917/</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Cheers</span>
<span class="quote">&gt;&gt; Vladimir</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=100571">Andrey Ryabinin</a> - Aug. 24, 2015, 4:18 p.m.</div>
<pre class="content">
2015-08-24 19:16 GMT+03:00 Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;:
<span class="quote">&gt; On 24/08/15 17:00, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt; 2015-08-24 18:44 GMT+03:00 Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Another option would be having &quot;sparse&quot; shadow memory based on page</span>
<span class="quote">&gt;&gt;&gt; extension. I did play with that some time ago based on ideas from</span>
<span class="quote">&gt;&gt;&gt; original v1 KASan support for x86/arm - it is how 614be38 &quot;irqchip:</span>
<span class="quote">&gt;&gt;&gt; gic-v3: Fix out of bounds access to cpu_logical_map&quot; was caught.</span>
<span class="quote">&gt;&gt;&gt; It doesn&#39;t require any VA reservations, only some contiguous memory for</span>
<span class="quote">&gt;&gt;&gt; the page_ext itself, which serves as indirection level for the 0-order</span>
<span class="quote">&gt;&gt;&gt; shadow pages.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; We won&#39;t be able to use inline instrumentation (I could live with that),</span>
<span class="quote">&gt;&gt; and most importantly, we won&#39;t be able to use stack instrumentation.</span>
<span class="quote">&gt;&gt; GCC needs to know shadow address for inline and/or stack instrumentation</span>
<span class="quote">&gt;&gt; to generate correct code.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s definitely a trade-off ;)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Just for my understanding does that stack instrumentation is controlled</span>
<span class="quote">&gt; via -asan-stack?</span>
<span class="quote">&gt;</span>

Yup.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=709">Russell King - ARM Linux</a> - Aug. 24, 2015, 5:47 p.m.</div>
<pre class="content">
On Mon, Aug 24, 2015 at 05:15:22PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; Yes, ~130Mb (3G/1G split) should work. 512Mb shadow is optional.</span>
<span class="quote">&gt; The only advantage of 512Mb shadow is better handling of user memory</span>
<span class="quote">&gt; accesses bugs</span>
<span class="quote">&gt; (access to user memory without copy_from_user/copy_to_user/strlen_user etc API).</span>

No need for that to be handed by KASan.  I have patches in linux-next,
now acked by Will, which prevent the kernel accessing userspace with
zero memory footprint.  No need for remapping, we have a way to quickly
turn off access to userspace mapped pages on non-LPAE 32-bit CPUs.
(LPAE is not supported yet - Catalin will be working on that using the
hooks I&#39;m providing once he returns.)

This isn&#39;t a debugging thing, it&#39;s a security hardening thing.  Some
use-after-free bugs are potentially exploitable from userspace.  See
the recent blackhat conference paper.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Aug. 25, 2015, 9:15 a.m.</div>
<pre class="content">
On Mon, Aug 24, 2015 at 06:47:36PM +0100, Russell King - ARM Linux wrote:
<span class="quote">&gt; On Mon, Aug 24, 2015 at 05:15:22PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt; &gt; Yes, ~130Mb (3G/1G split) should work. 512Mb shadow is optional.</span>
<span class="quote">&gt; &gt; The only advantage of 512Mb shadow is better handling of user memory</span>
<span class="quote">&gt; &gt; accesses bugs</span>
<span class="quote">&gt; &gt; (access to user memory without copy_from_user/copy_to_user/strlen_user etc API).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No need for that to be handed by KASan.  I have patches in linux-next,</span>
<span class="quote">&gt; now acked by Will, which prevent the kernel accessing userspace with</span>
<span class="quote">&gt; zero memory footprint.  No need for remapping, we have a way to quickly</span>
<span class="quote">&gt; turn off access to userspace mapped pages on non-LPAE 32-bit CPUs.</span>
<span class="quote">&gt; (LPAE is not supported yet - Catalin will be working on that using the</span>
<span class="quote">&gt; hooks I&#39;m providing once he returns.)</span>

Hey, I only acked the &quot;Efficiency cleanups&quot; series so far! The PAN emulation
is still on my list.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index 7796af4..4cc73cc 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"> config ARM64</span>
 	select HAVE_ARCH_AUDITSYSCALL
 	select HAVE_ARCH_BITREVERSE
 	select HAVE_ARCH_JUMP_LABEL
<span class="p_add">+	select HAVE_ARCH_KASAN if SPARSEMEM_VMEMMAP</span>
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_SECCOMP_FILTER
 	select HAVE_ARCH_TRACEHOOK
<span class="p_chunk">@@ -119,6 +120,12 @@</span> <span class="p_context"> config GENERIC_CSUM</span>
 config GENERIC_CALIBRATE_DELAY
 	def_bool y
 
<span class="p_add">+config KASAN_SHADOW_OFFSET</span>
<span class="p_add">+	hex</span>
<span class="p_add">+	default 0xdfff200000000000 if ARM64_VA_BITS_48</span>
<span class="p_add">+	default 0xdffffc8000000000 if ARM64_VA_BITS_42</span>
<span class="p_add">+	default 0xdfffff9000000000 if ARM64_VA_BITS_39</span>
<span class="p_add">+</span>
 config ZONE_DMA
 	def_bool y
 
<span class="p_header">diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h</span>
new file mode 100644
<span class="p_header">index 0000000..65ac50d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kasan.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef __ASM_KASAN_H</span>
<span class="p_add">+#define __ASM_KASAN_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KASAN</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/memory.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * KASAN_SHADOW_START: beginning of the kernel virtual addresses.</span>
<span class="p_add">+ * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/8 of kernel virtual addresses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define KASAN_SHADOW_START      (UL(0xffffffffffffffff) &lt;&lt; (VA_BITS))</span>
<span class="p_add">+#define KASAN_SHADOW_END        (KASAN_SHADOW_START + (1UL &lt;&lt; (VA_BITS - 3)))</span>
<span class="p_add">+</span>
<span class="p_add">+void kasan_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void kasan_init(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">index bd5db28..8700f66 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -40,7 +40,14 @@</span> <span class="p_context"></span>
  *	fixed mappings and modules
  */
 #define VMEMMAP_SIZE		ALIGN((1UL &lt;&lt; (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_KASAN</span>
 #define VMALLOC_START		(UL(0xffffffffffffffff) &lt;&lt; VA_BITS)
<span class="p_add">+#else</span>
<span class="p_add">+#include &lt;asm/kasan.h&gt;</span>
<span class="p_add">+#define VMALLOC_START		KASAN_SHADOW_END</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
<span class="p_header">diff --git a/arch/arm64/include/asm/string.h b/arch/arm64/include/asm/string.h</span>
<span class="p_header">index 64d2d48..bff522c 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/string.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/string.h</span>
<span class="p_chunk">@@ -36,17 +36,33 @@</span> <span class="p_context"> extern __kernel_size_t strnlen(const char *, __kernel_size_t);</span>
 
 #define __HAVE_ARCH_MEMCPY
 extern void *memcpy(void *, const void *, __kernel_size_t);
<span class="p_add">+extern void *__memcpy(void *, const void *, __kernel_size_t);</span>
 
 #define __HAVE_ARCH_MEMMOVE
 extern void *memmove(void *, const void *, __kernel_size_t);
<span class="p_add">+extern void *__memmove(void *, const void *, __kernel_size_t);</span>
 
 #define __HAVE_ARCH_MEMCHR
 extern void *memchr(const void *, int, __kernel_size_t);
 
 #define __HAVE_ARCH_MEMSET
 extern void *memset(void *, int, __kernel_size_t);
<span class="p_add">+extern void *__memset(void *, int, __kernel_size_t);</span>
 
 #define __HAVE_ARCH_MEMCMP
 extern int memcmp(const void *, const void *, size_t);
 
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_KASAN) &amp;&amp; !defined(__SANITIZE_ADDRESS__)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For files that not instrumented (e.g. mm/slub.c) we</span>
<span class="p_add">+ * should use not instrumented version of mem* functions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define memcpy(dst, src, len) __memcpy(dst, src, len)</span>
<span class="p_add">+#define memmove(dst, src, len) __memmove(dst, src, len)</span>
<span class="p_add">+#define memset(s, c, n) __memset(s, c, n)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h</span>
<span class="p_header">index dcd06d1..cfe5ea5 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -24,10 +24,18 @@</span> <span class="p_context"></span>
 #include &lt;linux/compiler.h&gt;
 
 #ifndef CONFIG_ARM64_64K_PAGES
<span class="p_add">+#ifndef CONFIG_KASAN</span>
 #define THREAD_SIZE_ORDER	2
<span class="p_add">+#else</span>
<span class="p_add">+#define THREAD_SIZE_ORDER	3</span>
<span class="p_add">+#endif</span>
 #endif
 
<span class="p_add">+#ifndef CONFIG_KASAN</span>
 #define THREAD_SIZE		16384
<span class="p_add">+#else</span>
<span class="p_add">+#define THREAD_SIZE		32768</span>
<span class="p_add">+#endif</span>
 #define THREAD_START_SP		(THREAD_SIZE - 16)
 
 #ifndef __ASSEMBLY__
<span class="p_header">diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S</span>
<span class="p_header">index 19f915e..650b1e8 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/head.S</span>
<span class="p_header">+++ b/arch/arm64/kernel/head.S</span>
<span class="p_chunk">@@ -486,6 +486,9 @@</span> <span class="p_context"> __mmap_switched:</span>
 	str_l	x21, __fdt_pointer, x5		// Save FDT pointer
 	str_l	x24, memstart_addr, x6		// Save PHYS_OFFSET
 	mov	x29, #0
<span class="p_add">+#ifdef CONFIG_KASAN</span>
<span class="p_add">+	b	kasan_early_init</span>
<span class="p_add">+#endif</span>
 	b	start_kernel
 ENDPROC(__mmap_switched)
 
<span class="p_header">diff --git a/arch/arm64/kernel/module.c b/arch/arm64/kernel/module.c</span>
<span class="p_header">index 67bf410..7d90c0f 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/module.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/module.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/bitops.h&gt;
 #include &lt;linux/elf.h&gt;
 #include &lt;linux/gfp.h&gt;
<span class="p_add">+#include &lt;linux/kasan.h&gt;</span>
 #include &lt;linux/kernel.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/moduleloader.h&gt;
<span class="p_chunk">@@ -34,9 +35,18 @@</span> <span class="p_context"></span>
 
 void *module_alloc(unsigned long size)
 {
<span class="p_del">-	return __vmalloc_node_range(size, 1, MODULES_VADDR, MODULES_END,</span>
<span class="p_del">-				    GFP_KERNEL, PAGE_KERNEL_EXEC, 0,</span>
<span class="p_del">-				    NUMA_NO_NODE, __builtin_return_address(0));</span>
<span class="p_add">+	void *p;</span>
<span class="p_add">+</span>
<span class="p_add">+	p = __vmalloc_node_range(size, MODULE_ALIGN, MODULES_VADDR, MODULES_END,</span>
<span class="p_add">+				GFP_KERNEL, PAGE_KERNEL_EXEC, 0,</span>
<span class="p_add">+				NUMA_NO_NODE, __builtin_return_address(0));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p &amp;&amp; (kasan_module_alloc(p, size) &lt; 0)) {</span>
<span class="p_add">+		vfree(p);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return p;</span>
 }
 
 enum aarch64_reloc_op {
<span class="p_header">diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c</span>
<span class="p_header">index 7475313..963b53a 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/setup.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/setup.c</span>
<span class="p_chunk">@@ -54,6 +54,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/elf.h&gt;
 #include &lt;asm/cpufeature.h&gt;
 #include &lt;asm/cpu_ops.h&gt;
<span class="p_add">+#include &lt;asm/kasan.h&gt;</span>
 #include &lt;asm/sections.h&gt;
 #include &lt;asm/setup.h&gt;
 #include &lt;asm/smp_plat.h&gt;
<span class="p_chunk">@@ -401,6 +402,7 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 	acpi_boot_table_init();
 
 	paging_init();
<span class="p_add">+	kasan_init();</span>
 	request_standard_resources();
 
 	early_ioremap_reset();
<span class="p_header">diff --git a/arch/arm64/lib/memcpy.S b/arch/arm64/lib/memcpy.S</span>
<span class="p_header">index 8a9a96d..845e40a 100644</span>
<span class="p_header">--- a/arch/arm64/lib/memcpy.S</span>
<span class="p_header">+++ b/arch/arm64/lib/memcpy.S</span>
<span class="p_chunk">@@ -56,6 +56,8 @@</span> <span class="p_context"> C_h	.req	x12</span>
 D_l	.req	x13
 D_h	.req	x14
 
<span class="p_add">+.weak memcpy</span>
<span class="p_add">+ENTRY(__memcpy)</span>
 ENTRY(memcpy)
 	mov	dst, dstin
 	cmp	count, #16
<span class="p_chunk">@@ -199,3 +201,4 @@</span> <span class="p_context"> ENTRY(memcpy)</span>
 	b.ne	.Ltail63
 	ret
 ENDPROC(memcpy)
<span class="p_add">+ENDPROC(__memcpy)</span>
<span class="p_header">diff --git a/arch/arm64/lib/memmove.S b/arch/arm64/lib/memmove.S</span>
<span class="p_header">index 57b19ea..48074d2 100644</span>
<span class="p_header">--- a/arch/arm64/lib/memmove.S</span>
<span class="p_header">+++ b/arch/arm64/lib/memmove.S</span>
<span class="p_chunk">@@ -57,12 +57,14 @@</span> <span class="p_context"> C_h	.req	x12</span>
 D_l	.req	x13
 D_h	.req	x14
 
<span class="p_add">+.weak memmove</span>
<span class="p_add">+ENTRY(__memmove)</span>
 ENTRY(memmove)
 	cmp	dstin, src
<span class="p_del">-	b.lo	memcpy</span>
<span class="p_add">+	b.lo	__memcpy</span>
 	add	tmp1, src, count
 	cmp	dstin, tmp1
<span class="p_del">-	b.hs	memcpy		/* No overlap.  */</span>
<span class="p_add">+	b.hs	__memcpy		/* No overlap.  */</span>
 
 	add	dst, dstin, count
 	add	src, src, count
<span class="p_chunk">@@ -195,3 +197,4 @@</span> <span class="p_context"> ENTRY(memmove)</span>
 	b.ne	.Ltail63
 	ret
 ENDPROC(memmove)
<span class="p_add">+ENDPROC(__memmove)</span>
<span class="p_header">diff --git a/arch/arm64/lib/memset.S b/arch/arm64/lib/memset.S</span>
<span class="p_header">index 7c72dfd..4ab2594 100644</span>
<span class="p_header">--- a/arch/arm64/lib/memset.S</span>
<span class="p_header">+++ b/arch/arm64/lib/memset.S</span>
<span class="p_chunk">@@ -54,6 +54,8 @@</span> <span class="p_context"> dst		.req	x8</span>
 tmp3w		.req	w9
 tmp3		.req	x9
 
<span class="p_add">+.weak memset</span>
<span class="p_add">+ENTRY(__memset)</span>
 ENTRY(memset)
 	mov	dst, dstin	/* Preserve return value.  */
 	and	A_lw, val, #255
<span class="p_chunk">@@ -214,3 +216,4 @@</span> <span class="p_context"> ENTRY(memset)</span>
 	b.ne	.Ltail_maybe_long
 	ret
 ENDPROC(memset)
<span class="p_add">+ENDPROC(__memset)</span>
<span class="p_header">diff --git a/arch/arm64/mm/Makefile b/arch/arm64/mm/Makefile</span>
<span class="p_header">index 773d37a..e17703c 100644</span>
<span class="p_header">--- a/arch/arm64/mm/Makefile</span>
<span class="p_header">+++ b/arch/arm64/mm/Makefile</span>
<span class="p_chunk">@@ -4,3 +4,6 @@</span> <span class="p_context"> obj-y				:= dma-mapping.o extable.o fault.o init.o \</span>
 				   context.o proc.o pageattr.o
 obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o
 obj-$(CONFIG_ARM64_PTDUMP)	+= dump.o
<span class="p_add">+</span>
<span class="p_add">+KASAN_SANITIZE_kasan_init.o	:= n</span>
<span class="p_add">+obj-$(CONFIG_KASAN)		+= kasan_init.o</span>
<span class="p_header">diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
new file mode 100644
<span class="p_header">index 0000000..35dbd84</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/arm64/mm/kasan_init.c</span>
<span class="p_chunk">@@ -0,0 +1,143 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/kasan.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/memblock.h&gt;</span>
<span class="p_add">+#include &lt;linux/start_kernel.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;</span>
<span class="p_add">+static pgd_t tmp_page_table[PTRS_PER_PGD] __initdata __aligned(PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 3</span>
<span class="p_add">+pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt; 2</span>
<span class="p_add">+pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_early_pmd_populate(unsigned long start,</span>
<span class="p_add">+					unsigned long end, pud_t *pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, start);</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr = next, pmd++) {</span>
<span class="p_add">+		pmd_populate_kernel(&amp;init_mm, pmd, kasan_zero_pte);</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_early_pud_populate(unsigned long start,</span>
<span class="p_add">+					unsigned long end, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, start);</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr = next, pud++) {</span>
<span class="p_add">+		pud_populate(&amp;init_mm, pud, kasan_zero_pmd);</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		kasan_early_pmd_populate(addr, next, pud);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init kasan_map_early_shadow(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	unsigned long start = KASAN_SHADOW_START;</span>
<span class="p_add">+	unsigned long end = KASAN_SHADOW_END;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PTE; i++)</span>
<span class="p_add">+		set_pte(&amp;kasan_zero_pte[i], pfn_pte(</span>
<span class="p_add">+				virt_to_pfn(kasan_zero_page), PAGE_KERNEL));</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(start);</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr = next, pgd++) {</span>
<span class="p_add">+		pgd_populate(&amp;init_mm, pgd, kasan_zero_pud);</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		kasan_early_pud_populate(addr, next, pgd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init kasan_early_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kasan_map_early_shadow(swapper_pg_dir);</span>
<span class="p_add">+	start_kernel();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init clear_pgds(unsigned long start,</span>
<span class="p_add">+			unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Remove references to kasan page tables from</span>
<span class="p_add">+	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="p_add">+	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (; start &amp;&amp; start &lt; end; start += PGDIR_SIZE)</span>
<span class="p_add">+		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init cpu_set_ttbr1(unsigned long ttbr1)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm(</span>
<span class="p_add">+	&quot;	msr	ttbr1_el1, %0\n&quot;</span>
<span class="p_add">+	&quot;	isb&quot;</span>
<span class="p_add">+	:</span>
<span class="p_add">+	: &quot;r&quot; (ttbr1));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init kasan_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct memblock_region *reg;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are going to perform proper setup of shadow memory.</span>
<span class="p_add">+	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="p_add">+	 * However, instrumented code couldn&#39;t execute without shadow memory.</span>
<span class="p_add">+	 * tmp_page_table used to keep early shadow mapped until full shadow</span>
<span class="p_add">+	 * setup will be finished.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	memcpy(tmp_page_table, swapper_pg_dir, sizeof(tmp_page_table));</span>
<span class="p_add">+	cpu_set_ttbr1(__pa(tmp_page_table));</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+</span>
<span class="p_add">+	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
<span class="p_add">+</span>
<span class="p_add">+	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,</span>
<span class="p_add">+			kasan_mem_to_shadow((void *)MODULES_VADDR));</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_memblock(memory, reg) {</span>
<span class="p_add">+		void *start = (void *)__phys_to_virt(reg-&gt;base);</span>
<span class="p_add">+		void *end = (void *)__phys_to_virt(reg-&gt;base + reg-&gt;size);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (start &gt;= end)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * end + 1 here is intentional. We check several shadow bytes in</span>
<span class="p_add">+		 * advance to slightly speed up fastpath. In some rare cases</span>
<span class="p_add">+		 * we could cross boundary of mapped shadow, so we just map</span>
<span class="p_add">+		 * some more here.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		vmemmap_populate((unsigned long)kasan_mem_to_shadow(start),</span>
<span class="p_add">+				(unsigned long)kasan_mem_to_shadow(end) + 1,</span>
<span class="p_add">+				pfn_to_nid(virt_to_pfn(start)));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="p_add">+	cpu_set_ttbr1(__pa(swapper_pg_dir));</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* At this point kasan is fully initialized. Enable error messages */</span>
<span class="p_add">+	init_task.kasan_depth = 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



