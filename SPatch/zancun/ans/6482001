
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,3/3] memcg: get rid of mm_struct::owner - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,3/3] memcg: get rid of mm_struct::owner</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 26, 2015, 11:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1432641006-8025-4-git-send-email-mhocko@suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6482001/mbox/"
   >mbox</a>
|
   <a href="/patch/6482001/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6482001/">/patch/6482001/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 7B157C0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 26 May 2015 14:14:48 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id E782D205C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 26 May 2015 14:14:46 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 32A9B20624
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 26 May 2015 14:14:45 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755626AbbEZOOn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 26 May 2015 10:14:43 -0400
Received: from cantor2.suse.de ([195.135.220.15]:44162 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754135AbbEZONl (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 26 May 2015 10:13:41 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 27D9AADD4;
	Tue, 26 May 2015 11:50:24 +0000 (UTC)
From: Michal Hocko &lt;mhocko@suse.cz&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;, Tejun Heo &lt;tj@kernel.org&gt;,
	Vladimir Davydov &lt;vdavydov@parallels.com&gt;,
	KAMEZAWA Hiroyuki &lt;kamezawa.hiroyu@jp.fujitsu.com&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: [RFC 3/3] memcg: get rid of mm_struct::owner
Date: Tue, 26 May 2015 13:50:06 +0200
Message-Id: &lt;1432641006-8025-4-git-send-email-mhocko@suse.cz&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1432641006-8025-1-git-send-email-mhocko@suse.cz&gt;
References: &lt;1432641006-8025-1-git-send-email-mhocko@suse.cz&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a> - May 26, 2015, 11:50 a.m.</div>
<pre class="content">
mm_struct::owner keeps track of the task which is in charge for the
specific mm. This is usually the thread group leader of the task but
there are more exotic cases where this doesn&#39;t hold.

The most prominent one is when separate tasks (not in the same thread
group) share the address space (by using clone with CLONE_VM without
CLONE_THREAD). The first task will be the owner until it exits.
mm_update_next_owner will then try to find a new owner - a task which
points to the same mm_struct. There is no guarantee a new owner will
be the thread group leader though because the leader might have
exited. Even though such a thread will be still around waiting for the
remaining threads from its group, it&#39;s mm will be NULL so it cannot be
chosen.

cgroup migration code, however assumes only group leaders when migrating
via cgroup.procs (which will be the only mode in the unified hierarchy
API) while mem_cgroup_can_attach only those tasks which are owner
of the mm. So we might end up with tasks which cannot be migrated.
mm_update_next_owner could be tweaked to try harder and use a group
leader whenever possible but this will never be 100% because all the
leaders might be dead.  It seems that getting rid of the mm-&gt;owner
sounds like a better option.

The whole concept of the mm owner is a bit artificial and too tricky to
get right. All the memcg code needs is to find struct mem_cgroup from
a given mm_struct and there are only two events when the association
is either built or changed
	- a new mm is created - dup_mm - when the memcg is inherited
	  from the oldmm
	- task associated with the mm is moved to another memcg
So it is much more easier to bind mm_struct with the mem_cgroup directly
rather than indirectly via a task. This is exactly what this patch does.

mm_set_memcg and mm_drop_memcg are exported for the core kernel to bind
an old memcg during dup_mm and releasing that memcg in mmput after the
last reference is dropped and no task sees the mm anymore. We have to be
careful and take a reference to the memcg-&gt;css so that it doesn&#39;t vanish
from under our feet.
mm_move_memcg is then used during the task migration to change the
association. This is done in mem_cgroup_move_task before charges get
moved because mem_cgroup_can_attach is too early and other controllers
might fail and we would have to handle the rollback. The race between
can_attach and attach is harmless and it existed even before.

mm-&gt;memcg conforms to standard mem_cgroup locking rules. It has to be
used inside rcu_read_{un}lock() and a reference has to be taken before the
unlock if the memcg is supposed to be used outside. mm_move_memcg will
make sure that all the preexisting users will finish before it drops the
reference to the old memcg.

Finally mem_cgroup_can_attach will allow task migration only for the
thread group leaders to conform with cgroup core requirements.

Please note that this patch introduces a USER VISIBLE CHANGE OF BEHAVIOR.
Without mm-&gt;owner _all_ tasks associated with the mm_struct would
initiate memcg migration while previously only owner of the mm_struct
could do that. The original behavior was awkward though because the user
task didn&#39;t have any means to find out the current owner (esp. after
mm_update_next_owner) so the migration behavior was not well defined
in general.
New cgroup API (unified hierarchy) will discontinue tasks file which
means that migrating threads will no longer be possible. In such a case
having CLONE_VM without CLONE_THREAD could emulate the thread behavior
but this patch prevents from isolating memcg controllers from others.
Nevertheless I am not convinced such a use case would really deserve
complications on the memcg code side.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.cz&gt;</span>
---
 fs/exec.c                  |  1 -
 include/linux/memcontrol.h | 14 ++++++-
 include/linux/mm_types.h   | 12 +-----
 kernel/exit.c              | 89 -----------------------------------------
 kernel/fork.c              | 10 +----
 mm/debug.c                 |  4 +-
 mm/memcontrol.c            | 99 +++++++++++++++++++++++++++++++++++-----------
 7 files changed, 93 insertions(+), 136 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=12">Oleg Nesterov</a> - May 26, 2015, 4:36 p.m.</div>
<pre class="content">
On 05/26, Michal Hocko wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -426,17 +426,7 @@ struct mm_struct {</span>
<span class="quote">&gt;  	struct kioctx_table __rcu	*ioctx_table;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * &quot;owner&quot; points to a task that is regarded as the canonical</span>
<span class="quote">&gt; -	 * user/owner of this mm. All of the following must be true in</span>
<span class="quote">&gt; -	 * order for it to be changed:</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * current == mm-&gt;owner</span>
<span class="quote">&gt; -	 * current-&gt;mm != mm</span>
<span class="quote">&gt; -	 * new_owner-&gt;mm == mm</span>
<span class="quote">&gt; -	 * new_owner-&gt;alloc_lock is held</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	struct task_struct __rcu *owner;</span>
<span class="quote">&gt; +	struct mem_cgroup __rcu *memcg;</span>

Yes, thanks, this is what I tried to suggest ;)

But I can&#39;t review this series. Simply because I know nothing about
memcs. I don&#39;t even know how to use it.

Just one question,
<span class="quote">
&gt; +static struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!p-&gt;mm)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	return rcu_dereference(p-&gt;mm-&gt;memcg);</span>
<span class="quote">&gt; +}</span>

Probably I missed something, but it seems that the callers do not
expect it can return NULL. Perhaps sock_update_memcg() is fine, but
task_in_mem_cgroup() calls it when find_lock_task_mm() fails, and in
this case -&gt;mm is NULL.

And in fact I can&#39;t understand what mem_cgroup_from_task() actually
means, with or without these changes.

And another question. I can&#39;t understand what happens when a task
execs... IOW, could you confirm that exec_mmap() does not need
mm_set_memcg(mm, oldmm-&gt;memcg) ?

Oleg.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=482">Tejun Heo</a> - May 28, 2015, 9:07 p.m.</div>
<pre class="content">
Hello, Johannes, Michal.

On Tue, May 26, 2015 at 10:10:11AM -0400, Johannes Weiner wrote:
<span class="quote">&gt; On Tue, May 26, 2015 at 01:50:06PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; Please note that this patch introduces a USER VISIBLE CHANGE OF BEHAVIOR.</span>
<span class="quote">&gt; &gt; Without mm-&gt;owner _all_ tasks associated with the mm_struct would</span>
<span class="quote">&gt; &gt; initiate memcg migration while previously only owner of the mm_struct</span>
<span class="quote">&gt; &gt; could do that. The original behavior was awkward though because the user</span>
<span class="quote">&gt; &gt; task didn&#39;t have any means to find out the current owner (esp. after</span>
<span class="quote">&gt; &gt; mm_update_next_owner) so the migration behavior was not well defined</span>
<span class="quote">&gt; &gt; in general.</span>
<span class="quote">&gt; &gt; New cgroup API (unified hierarchy) will discontinue tasks file which</span>
<span class="quote">&gt; &gt; means that migrating threads will no longer be possible. In such a case</span>
<span class="quote">&gt; &gt; having CLONE_VM without CLONE_THREAD could emulate the thread behavior</span>
<span class="quote">&gt; &gt; but this patch prevents from isolating memcg controllers from others.</span>
<span class="quote">&gt; &gt; Nevertheless I am not convinced such a use case would really deserve</span>
<span class="quote">&gt; &gt; complications on the memcg code side.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think such a change is okay.  The memcg semantics of moving threads</span>
<span class="quote">&gt; with the same mm into separate groups have always been arbitrary.  No</span>
<span class="quote">&gt; reasonable behavior can be expected of this, so what sane real life</span>
<span class="quote">&gt; usecase would rely on it?</span>

I suppose that making mm always follow the threadgroup leader should
be fine, right?  While this wouldn&#39;t make any difference in the
unified hierarchy, I think this would make more sense for traditional
hierarchies.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a> - May 29, 2015, 12:08 p.m.</div>
<pre class="content">
On Thu 28-05-15 17:07:42, Tejun Heo wrote:
<span class="quote">&gt; Hello, Johannes, Michal.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue, May 26, 2015 at 10:10:11AM -0400, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Tue, May 26, 2015 at 01:50:06PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; Please note that this patch introduces a USER VISIBLE CHANGE OF BEHAVIOR.</span>
<span class="quote">&gt; &gt; &gt; Without mm-&gt;owner _all_ tasks associated with the mm_struct would</span>
<span class="quote">&gt; &gt; &gt; initiate memcg migration while previously only owner of the mm_struct</span>
<span class="quote">&gt; &gt; &gt; could do that. The original behavior was awkward though because the user</span>
<span class="quote">&gt; &gt; &gt; task didn&#39;t have any means to find out the current owner (esp. after</span>
<span class="quote">&gt; &gt; &gt; mm_update_next_owner) so the migration behavior was not well defined</span>
<span class="quote">&gt; &gt; &gt; in general.</span>
<span class="quote">&gt; &gt; &gt; New cgroup API (unified hierarchy) will discontinue tasks file which</span>
<span class="quote">&gt; &gt; &gt; means that migrating threads will no longer be possible. In such a case</span>
<span class="quote">&gt; &gt; &gt; having CLONE_VM without CLONE_THREAD could emulate the thread behavior</span>
<span class="quote">&gt; &gt; &gt; but this patch prevents from isolating memcg controllers from others.</span>
<span class="quote">&gt; &gt; &gt; Nevertheless I am not convinced such a use case would really deserve</span>
<span class="quote">&gt; &gt; &gt; complications on the memcg code side.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think such a change is okay.  The memcg semantics of moving threads</span>
<span class="quote">&gt; &gt; with the same mm into separate groups have always been arbitrary.  No</span>
<span class="quote">&gt; &gt; reasonable behavior can be expected of this, so what sane real life</span>
<span class="quote">&gt; &gt; usecase would rely on it?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I suppose that making mm always follow the threadgroup leader should</span>
<span class="quote">&gt; be fine, right? </span>

That is the plan.
<span class="quote">
&gt; While this wouldn&#39;t make any difference in the unified hierarchy,</span>

Just to make sure I understand. &quot;wouldn&#39;t make any difference&quot; because
the API is not backward compatible right?
<span class="quote">
&gt; I think this would make more sense for traditional hierarchies.</span>

Yes I believe so.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=482">Tejun Heo</a> - May 29, 2015, 1:10 p.m.</div>
<pre class="content">
On Fri, May 29, 2015 at 02:08:38PM +0200, Michal Hocko wrote:
<span class="quote">&gt; &gt; I suppose that making mm always follow the threadgroup leader should</span>
<span class="quote">&gt; &gt; be fine, right? </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That is the plan.</span>

Cool.
<span class="quote">
&gt; &gt; While this wouldn&#39;t make any difference in the unified hierarchy,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just to make sure I understand. &quot;wouldn&#39;t make any difference&quot; because</span>
<span class="quote">&gt; the API is not backward compatible right?</span>

Hmm... because it&#39;s always per-process.  If any thread is going, the
whole process is going together.
<span class="quote">
&gt; &gt; I think this would make more sense for traditional hierarchies.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes I believe so.</span>

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a> - May 29, 2015, 1:45 p.m.</div>
<pre class="content">
On Fri 29-05-15 09:10:55, Tejun Heo wrote:
<span class="quote">&gt; On Fri, May 29, 2015 at 02:08:38PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; I suppose that making mm always follow the threadgroup leader should</span>
<span class="quote">&gt; &gt; &gt; be fine, right? </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That is the plan.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cool.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; While this wouldn&#39;t make any difference in the unified hierarchy,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Just to make sure I understand. &quot;wouldn&#39;t make any difference&quot; because</span>
<span class="quote">&gt; &gt; the API is not backward compatible right?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm... because it&#39;s always per-process.  If any thread is going, the</span>
<span class="quote">&gt; whole process is going together.</span>

Sure but we are talking about processes here. They just happen to share
mm. And this is exactly the behavior change I am talking about... With
the owner you could emulate &quot;threads&quot; with this patch you cannot
anymore. IMO we shouldn&#39;t allow for that but just reading the original
commit message (cf475ad28ac35) which has added mm-&gt;owner:
&quot;
It also allows several control groups that are virtually grouped by
mm_struct, to exist independent of the memory controller i.e., without
adding mem_cgroup&#39;s for each controller, to mm_struct.
&quot;
suggests it might have been intentional. That being said, I think it was
a mistake back at the time and we should move on to a saner model. But I
also believe we should be really vocal when the user visible behavior
changes. If somebody really asks for the previous behavior I would
insist on a _strong_ usecase.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=482">Tejun Heo</a> - May 29, 2015, 2:07 p.m.</div>
<pre class="content">
Hello,

On Fri, May 29, 2015 at 03:45:53PM +0200, Michal Hocko wrote:
<span class="quote">&gt; Sure but we are talking about processes here. They just happen to share</span>
<span class="quote">&gt; mm. And this is exactly the behavior change I am talking about... With</span>

Are we talking about CLONE_VM w/o CLONE_THREAD?  ie. two threadgroups
sharing the same VM?
<span class="quote">
&gt; the owner you could emulate &quot;threads&quot; with this patch you cannot</span>
<span class="quote">&gt; anymore. IMO we shouldn&#39;t allow for that but just reading the original</span>
<span class="quote">&gt; commit message (cf475ad28ac35) which has added mm-&gt;owner:</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; It also allows several control groups that are virtually grouped by</span>
<span class="quote">&gt; mm_struct, to exist independent of the memory controller i.e., without</span>
<span class="quote">&gt; adding mem_cgroup&#39;s for each controller, to mm_struct.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; suggests it might have been intentional. That being said, I think it was</span>

I think he&#39;s talking about implmenting different controllers which may
want to add their own css pointer in mm_struct now wouldn&#39;t need to as
the mm is tagged with the owning task from which membership of all
controllers can be derived.  I don&#39;t think that&#39;s something we need to
worry about.  We haven&#39;t seen even a suggestion for such a controller
and even if that happens we&#39;d be better off adding a separate field
for the new controller.
<span class="quote">
&gt; a mistake back at the time and we should move on to a saner model. But I</span>
<span class="quote">&gt; also believe we should be really vocal when the user visible behavior</span>
<span class="quote">&gt; changes. If somebody really asks for the previous behavior I would</span>
<span class="quote">&gt; insist on a _strong_ usecase.</span>

I&#39;m a bit lost on what&#39;s cleared defined is actually changing.  It&#39;s
not like userland had firm control over mm-&gt;owner.  It was already a
crapshoot, no?

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a> - May 29, 2015, 2:57 p.m.</div>
<pre class="content">
On Fri 29-05-15 10:07:37, Tejun Heo wrote:
<span class="quote">&gt; Hello,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Fri, May 29, 2015 at 03:45:53PM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; Sure but we are talking about processes here. They just happen to share</span>
<span class="quote">&gt; &gt; mm. And this is exactly the behavior change I am talking about... With</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Are we talking about CLONE_VM w/o CLONE_THREAD?  ie. two threadgroups</span>
<span class="quote">&gt; sharing the same VM?</span>

yes.
<span class="quote">
&gt; &gt; the owner you could emulate &quot;threads&quot; with this patch you cannot</span>
<span class="quote">&gt; &gt; anymore. IMO we shouldn&#39;t allow for that but just reading the original</span>
<span class="quote">&gt; &gt; commit message (cf475ad28ac35) which has added mm-&gt;owner:</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; It also allows several control groups that are virtually grouped by</span>
<span class="quote">&gt; &gt; mm_struct, to exist independent of the memory controller i.e., without</span>
<span class="quote">&gt; &gt; adding mem_cgroup&#39;s for each controller, to mm_struct.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; suggests it might have been intentional. That being said, I think it was</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think he&#39;s talking about implmenting different controllers which may</span>
<span class="quote">&gt; want to add their own css pointer in mm_struct now wouldn&#39;t need to as</span>
<span class="quote">&gt; the mm is tagged with the owning task from which membership of all</span>
<span class="quote">&gt; controllers can be derived.  I don&#39;t think that&#39;s something we need to</span>
<span class="quote">&gt; worry about.  We haven&#39;t seen even a suggestion for such a controller</span>
<span class="quote">&gt; and even if that happens we&#39;d be better off adding a separate field</span>
<span class="quote">&gt; for the new controller.</span>

Maybe I&#39;ve just misunderstood. My understandig was that tasks sharing
the mm could live in different cgroups while the memory would be bound
by a shared memcg.
<span class="quote">
&gt; &gt; a mistake back at the time and we should move on to a saner model. But I</span>
<span class="quote">&gt; &gt; also believe we should be really vocal when the user visible behavior</span>
<span class="quote">&gt; &gt; changes. If somebody really asks for the previous behavior I would</span>
<span class="quote">&gt; &gt; insist on a _strong_ usecase.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m a bit lost on what&#39;s cleared defined is actually changing.  It&#39;s</span>
<span class="quote">&gt; not like userland had firm control over mm-&gt;owner.  It was already a</span>
<span class="quote">&gt; crapshoot, no?</span>

OK so you creat a task A (leader) which clones several tasks Pn with
CLONE_VM without CLONE_THREAD. Moving A around would control memcg
membership while Pn could be moved around freely to control membership
in other controllers (e.g. cpu to control shares). So it is something
like moving threads separately.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=482">Tejun Heo</a> - May 29, 2015, 3:23 p.m.</div>
<pre class="content">
Hello,

On Fri, May 29, 2015 at 04:57:39PM +0200, Michal Hocko wrote:
<span class="quote">&gt; &gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; &gt; It also allows several control groups that are virtually grouped by</span>
<span class="quote">&gt; &gt; &gt; mm_struct, to exist independent of the memory controller i.e., without</span>
<span class="quote">&gt; &gt; &gt; adding mem_cgroup&#39;s for each controller, to mm_struct.</span>
<span class="quote">&gt; &gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; &gt; suggests it might have been intentional. That being said, I think it was</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think he&#39;s talking about implmenting different controllers which may</span>
<span class="quote">&gt; &gt; want to add their own css pointer in mm_struct now wouldn&#39;t need to as</span>
<span class="quote">&gt; &gt; the mm is tagged with the owning task from which membership of all</span>
<span class="quote">&gt; &gt; controllers can be derived.  I don&#39;t think that&#39;s something we need to</span>
<span class="quote">&gt; &gt; worry about.  We haven&#39;t seen even a suggestion for such a controller</span>
<span class="quote">&gt; &gt; and even if that happens we&#39;d be better off adding a separate field</span>
<span class="quote">&gt; &gt; for the new controller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe I&#39;ve just misunderstood. My understandig was that tasks sharing</span>
<span class="quote">&gt; the mm could live in different cgroups while the memory would be bound</span>
<span class="quote">&gt; by a shared memcg.</span>

Hmm.... it specifically goes into explaining that it&#39;s about having
different controllers sharing the owner field.

 &quot;i.e., without adding mem_cgroup&#39;s for each controller, to mm_struct.&quot;

It seems fairly clear to me.
<span class="quote">
&gt; &gt; I&#39;m a bit lost on what&#39;s cleared defined is actually changing.  It&#39;s</span>
<span class="quote">&gt; &gt; not like userland had firm control over mm-&gt;owner.  It was already a</span>
<span class="quote">&gt; &gt; crapshoot, no?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK so you creat a task A (leader) which clones several tasks Pn with</span>
<span class="quote">&gt; CLONE_VM without CLONE_THREAD. Moving A around would control memcg</span>
<span class="quote">&gt; membership while Pn could be moved around freely to control membership</span>
<span class="quote">&gt; in other controllers (e.g. cpu to control shares). So it is something</span>
<span class="quote">&gt; like moving threads separately.</span>

Sure, it&#39;d behave clearly in certain cases but then again you&#39;d have
cases where how mm-&gt;owner changes isn&#39;t clear at all when seen from
the userland.  e.g. When the original owner goes away, the assignment
of the next owner is essentially arbitrary.  That&#39;s what I meant by
saying it was already a crapshoot.  We should definitely document the
change but this isn&#39;t likely to be an issue.  CLONE_VM &amp;&amp;
!CLONE_THREAD is an extreme corner case to begin with and even the
behavior there wasn&#39;t all that clearly defined.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1091">Michal Hocko</a> - May 29, 2015, 3:26 p.m.</div>
<pre class="content">
On Fri 29-05-15 11:23:28, Tejun Heo wrote:
<span class="quote">&gt; Hello,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Fri, May 29, 2015 at 04:57:39PM +0200, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; OK so you creat a task A (leader) which clones several tasks Pn with</span>
<span class="quote">&gt; &gt; CLONE_VM without CLONE_THREAD. Moving A around would control memcg</span>
<span class="quote">&gt; &gt; membership while Pn could be moved around freely to control membership</span>
<span class="quote">&gt; &gt; in other controllers (e.g. cpu to control shares). So it is something</span>
<span class="quote">&gt; &gt; like moving threads separately.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, it&#39;d behave clearly in certain cases but then again you&#39;d have</span>
<span class="quote">&gt; cases where how mm-&gt;owner changes isn&#39;t clear at all when seen from</span>
<span class="quote">&gt; the userland. </span>

Sure. I am definitely _not_ advocating this use case! As said before, I
consider it abuse. It is just fair to point out this is a user visible
change IMO.
<span class="quote">
&gt; e.g. When the original owner goes away, the assignment</span>
<span class="quote">&gt; of the next owner is essentially arbitrary.  That&#39;s what I meant by</span>
<span class="quote">&gt; saying it was already a crapshoot.  We should definitely document the</span>
<span class="quote">&gt; change but this isn&#39;t likely to be an issue.  CLONE_VM &amp;&amp;</span>
<span class="quote">&gt; !CLONE_THREAD is an extreme corner case to begin with and even the</span>
<span class="quote">&gt; behavior there wasn&#39;t all that clearly defined.</span>

That is the line of argumentation in my changelog ;)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 02bfd980a40c..2cd4def4b1d6 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -867,7 +867,6 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 		up_read(&amp;old_mm-&gt;mmap_sem);
 		BUG_ON(active_mm != old_mm);
 		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);
<span class="p_del">-		mm_update_next_owner(old_mm);</span>
 		mmput(old_mm);
 		return 0;
 	}
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 6c8918114804..315ec1e58acb 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -67,6 +67,8 @@</span> <span class="p_context"> enum mem_cgroup_events_index {</span>
 };
 
 #ifdef CONFIG_MEMCG
<span class="p_add">+void mm_drop_memcg(struct mm_struct *mm);</span>
<span class="p_add">+void mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg);</span>
 void mem_cgroup_events(struct mem_cgroup *memcg,
 		       enum mem_cgroup_events_index idx,
 		       unsigned int nr);
<span class="p_chunk">@@ -92,7 +94,6 @@</span> <span class="p_context"> bool mem_cgroup_is_descendant(struct mem_cgroup *memcg,</span>
 bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);
 
 extern struct mem_cgroup *try_get_mem_cgroup_from_page(struct page *page);
<span class="p_del">-extern struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);</span>
 
 extern struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg);
 extern struct mem_cgroup *mem_cgroup_from_css(struct cgroup_subsys_state *css);
<span class="p_chunk">@@ -104,7 +105,12 @@</span> <span class="p_context"> static inline bool mm_match_cgroup(struct mm_struct *mm,</span>
 	bool match = false;
 
 	rcu_read_lock();
<span class="p_del">-	task_memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * rcu_dereference would be better but mem_cgroup is not a complete</span>
<span class="p_add">+	 * type here</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	task_memcg = READ_ONCE(mm-&gt;memcg);</span>
<span class="p_add">+	smp_read_barrier_depends();</span>
 	if (task_memcg)
 		match = mem_cgroup_is_descendant(task_memcg, memcg);
 	rcu_read_unlock();
<span class="p_chunk">@@ -195,6 +201,10 @@</span> <span class="p_context"> void mem_cgroup_split_huge_fixup(struct page *head);</span>
 #else /* CONFIG_MEMCG */
 struct mem_cgroup;
 
<span class="p_add">+void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="p_add">+{}</span>
<span class="p_add">+void mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="p_add">+{}</span>
 static inline void mem_cgroup_events(struct mem_cgroup *memcg,
 				     enum mem_cgroup_events_index idx,
 				     unsigned int nr)
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index f6266742ce1f..93dc8cb9c636 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -426,17 +426,7 @@</span> <span class="p_context"> struct mm_struct {</span>
 	struct kioctx_table __rcu	*ioctx_table;
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-	/*</span>
<span class="p_del">-	 * &quot;owner&quot; points to a task that is regarded as the canonical</span>
<span class="p_del">-	 * user/owner of this mm. All of the following must be true in</span>
<span class="p_del">-	 * order for it to be changed:</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * current == mm-&gt;owner</span>
<span class="p_del">-	 * current-&gt;mm != mm</span>
<span class="p_del">-	 * new_owner-&gt;mm == mm</span>
<span class="p_del">-	 * new_owner-&gt;alloc_lock is held</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct task_struct __rcu *owner;</span>
<span class="p_add">+	struct mem_cgroup __rcu *memcg;</span>
 #endif
 
 	/* store ref to file /proc/&lt;pid&gt;/exe symlink points to */
<span class="p_header">diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="p_header">index 4089c2fd373e..8f3e5b4c58ce 100644</span>
<span class="p_header">--- a/kernel/exit.c</span>
<span class="p_header">+++ b/kernel/exit.c</span>
<span class="p_chunk">@@ -292,94 +292,6 @@</span> <span class="p_context"> kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)</span>
 	}
 }
 
<span class="p_del">-#ifdef CONFIG_MEMCG</span>
<span class="p_del">-/*</span>
<span class="p_del">- * A task is exiting.   If it owned this mm, find a new owner for the mm.</span>
<span class="p_del">- */</span>
<span class="p_del">-void mm_update_next_owner(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct task_struct *c, *g, *p = current;</span>
<span class="p_del">-</span>
<span class="p_del">-retry:</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If the exiting or execing task is not the owner, it&#39;s</span>
<span class="p_del">-	 * someone else&#39;s problem.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (mm-&gt;owner != p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The current owner is exiting/execing and there are no other</span>
<span class="p_del">-	 * candidates.  Do not leave the mm pointing to a possibly</span>
<span class="p_del">-	 * freed task structure.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (atomic_read(&amp;mm-&gt;mm_users) &lt;= 1) {</span>
<span class="p_del">-		mm-&gt;owner = NULL;</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	read_lock(&amp;tasklist_lock);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search in the children</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	list_for_each_entry(c, &amp;p-&gt;children, sibling) {</span>
<span class="p_del">-		if (c-&gt;mm == mm)</span>
<span class="p_del">-			goto assign_new_owner;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search in the siblings</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	list_for_each_entry(c, &amp;p-&gt;real_parent-&gt;children, sibling) {</span>
<span class="p_del">-		if (c-&gt;mm == mm)</span>
<span class="p_del">-			goto assign_new_owner;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Search through everything else, we should not get here often.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	for_each_process(g) {</span>
<span class="p_del">-		if (g-&gt;flags &amp; PF_KTHREAD)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		for_each_thread(g, c) {</span>
<span class="p_del">-			if (c-&gt;mm == mm)</span>
<span class="p_del">-				goto assign_new_owner;</span>
<span class="p_del">-			if (c-&gt;mm)</span>
<span class="p_del">-				break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	read_unlock(&amp;tasklist_lock);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We found no owner yet mm_users &gt; 1: this implies that we are</span>
<span class="p_del">-	 * most likely racing with swapoff (try_to_unuse()) or /proc or</span>
<span class="p_del">-	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mm-&gt;owner = NULL;</span>
<span class="p_del">-	return;</span>
<span class="p_del">-</span>
<span class="p_del">-assign_new_owner:</span>
<span class="p_del">-	BUG_ON(c == p);</span>
<span class="p_del">-	get_task_struct(c);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The task_lock protects c-&gt;mm from changing.</span>
<span class="p_del">-	 * We always want mm-&gt;owner-&gt;mm == mm</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	task_lock(c);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Delay read_unlock() till we have the task_lock()</span>
<span class="p_del">-	 * to ensure that c does not slip away underneath us</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	read_unlock(&amp;tasklist_lock);</span>
<span class="p_del">-	if (c-&gt;mm != mm) {</span>
<span class="p_del">-		task_unlock(c);</span>
<span class="p_del">-		put_task_struct(c);</span>
<span class="p_del">-		goto retry;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	mm-&gt;owner = c;</span>
<span class="p_del">-	task_unlock(c);</span>
<span class="p_del">-	put_task_struct(c);</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_MEMCG */</span>
<span class="p_del">-</span>
 /*
  * Turn us into a lazy TLB process if we
  * aren&#39;t already..
<span class="p_chunk">@@ -433,7 +345,6 @@</span> <span class="p_context"> static void exit_mm(struct task_struct *tsk)</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 	enter_lazy_tlb(mm, current);
 	task_unlock(tsk);
<span class="p_del">-	mm_update_next_owner(mm);</span>
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 556cc64ae0c4..075688b2cae5 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -570,13 +570,6 @@</span> <span class="p_context"> static void mm_init_aio(struct mm_struct *mm)</span>
 #endif
 }
 
<span class="p_del">-static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-#ifdef CONFIG_MEMCG</span>
<span class="p_del">-	mm-&gt;owner = p;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 {
 	mm-&gt;mmap = NULL;
<span class="p_chunk">@@ -596,7 +589,6 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
 	spin_lock_init(&amp;mm-&gt;page_table_lock);
 	mm_init_cpumask(mm);
 	mm_init_aio(mm);
<span class="p_del">-	mm_init_owner(mm, p);</span>
 	mmu_notifier_mm_init(mm);
 	clear_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
<span class="p_chunk">@@ -702,6 +694,7 @@</span> <span class="p_context"> void mmput(struct mm_struct *mm)</span>
 		}
 		if (mm-&gt;binfmt)
 			module_put(mm-&gt;binfmt-&gt;module);
<span class="p_add">+		mm_drop_memcg(mm);</span>
 		mmdrop(mm);
 	}
 }
<span class="p_chunk">@@ -925,6 +918,7 @@</span> <span class="p_context"> static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
 	if (mm-&gt;binfmt &amp;&amp; !try_module_get(mm-&gt;binfmt-&gt;module))
 		goto free_pt;
 
<span class="p_add">+	mm_set_memcg(mm, oldmm-&gt;memcg);</span>
 	return mm;
 
 free_pt:
<span class="p_header">diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="p_header">index 3eb3ac2fcee7..d0347a168651 100644</span>
<span class="p_header">--- a/mm/debug.c</span>
<span class="p_header">+++ b/mm/debug.c</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> void dump_mm(const struct mm_struct *mm)</span>
 		&quot;ioctx_table %p\n&quot;
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-		&quot;owner %p &quot;</span>
<span class="p_add">+		&quot;memcg %p &quot;</span>
 #endif
 		&quot;exe_file %p\n&quot;
 #ifdef CONFIG_MMU_NOTIFIER
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void dump_mm(const struct mm_struct *mm)</span>
 		mm-&gt;ioctx_table,
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-		mm-&gt;owner,</span>
<span class="p_add">+		mm-&gt;memcg,</span>
 #endif
 		mm-&gt;exe_file,
 #ifdef CONFIG_MMU_NOTIFIER
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 4d905209f00f..950875eb7d89 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -469,6 +469,46 @@</span> <span class="p_context"> static inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)</span>
 	return mem_cgroup_from_css(css);
 }
 
<span class="p_add">+static struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!p-&gt;mm)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return rcu_dereference(p-&gt;mm-&gt;memcg);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mm_set_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (memcg)</span>
<span class="p_add">+		css_get(&amp;memcg-&gt;css);</span>
<span class="p_add">+	rcu_assign_pointer(mm-&gt;memcg, memcg);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mm_drop_memcg(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is the last reference to mm so nobody can see</span>
<span class="p_add">+	 * this memcg</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm-&gt;memcg)</span>
<span class="p_add">+		css_put(&amp;mm-&gt;memcg-&gt;css);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mm_move_memcg(struct mm_struct *mm, struct mem_cgroup *memcg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mem_cgroup *old_memcg;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_set_memcg(mm, memcg);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * wait for all current users of the old memcg before we</span>
<span class="p_add">+	 * release the reference.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	old_memcg = mm-&gt;memcg;</span>
<span class="p_add">+	synchronize_rcu();</span>
<span class="p_add">+	if (old_memcg)</span>
<span class="p_add">+		css_put(&amp;old_memcg-&gt;css);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Writing them here to avoid exposing memcg&#39;s inner layout */
 #if defined(CONFIG_INET) &amp;&amp; defined(CONFIG_MEMCG_KMEM)
 
<span class="p_chunk">@@ -953,19 +993,6 @@</span> <span class="p_context"> static void memcg_check_events(struct mem_cgroup *memcg, struct page *page)</span>
 	}
 }
 
<span class="p_del">-struct mem_cgroup *mem_cgroup_from_task(struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * mm_update_next_owner() may clear mm-&gt;owner to NULL</span>
<span class="p_del">-	 * if it races with swapoff, page migration, etc.</span>
<span class="p_del">-	 * So this can be called with p == NULL.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(!p))</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	return mem_cgroup_from_css(task_css(p, memory_cgrp_id));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)
 {
 	struct mem_cgroup *memcg = NULL;
<span class="p_chunk">@@ -980,7 +1007,7 @@</span> <span class="p_context"> static struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)</span>
 		if (unlikely(!mm))
 			memcg = root_mem_cgroup;
 		else {
<span class="p_del">-			memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+			memcg = rcu_dereference(mm-&gt;memcg);</span>
 			if (unlikely(!memcg))
 				memcg = root_mem_cgroup;
 		}
<span class="p_chunk">@@ -1157,7 +1184,7 @@</span> <span class="p_context"> void __mem_cgroup_count_vm_event(struct mm_struct *mm, enum vm_event_item idx)</span>
 	struct mem_cgroup *memcg;
 
 	rcu_read_lock();
<span class="p_del">-	memcg = mem_cgroup_from_task(rcu_dereference(mm-&gt;owner));</span>
<span class="p_add">+	memcg = rcu_dereference(mm-&gt;memcg);</span>
 	if (unlikely(!memcg))
 		goto out;
 
<span class="p_chunk">@@ -2674,7 +2701,7 @@</span> <span class="p_context"> void __memcg_kmem_put_cache(struct kmem_cache *cachep)</span>
 }
 
 /*
<span class="p_del">- * We need to verify if the allocation against current-&gt;mm-&gt;owner&#39;s memcg is</span>
<span class="p_add">+ * We need to verify if the allocation against current-&gt;mm-&gt;memcg is</span>
  * possible for the given order. But the page is not allocated yet, so we&#39;ll
  * need a further commit step to do the final arrangements.
  *
<span class="p_chunk">@@ -4993,6 +5020,7 @@</span> <span class="p_context"> static bool mc_move_charge(void)</span>
 static void mem_cgroup_clear_mc(void)
 {
 	bool move_charge = mc_move_charge();
<span class="p_add">+	struct mem_cgroup *from;</span>
 	/*
 	 * we must clear moving_task before waking up waiters at the end of
 	 * task migration.
<span class="p_chunk">@@ -5000,16 +5028,21 @@</span> <span class="p_context"> static void mem_cgroup_clear_mc(void)</span>
 	mc.moving_task = NULL;
 	if (move_charge)
 		__mem_cgroup_clear_mc();
<span class="p_add">+</span>
 	spin_lock(&amp;mc.lock);
<span class="p_add">+	from = mc.from;</span>
 	mc.from = NULL;
 	mc.to = NULL;
 	spin_unlock(&amp;mc.lock);
<span class="p_add">+</span>
<span class="p_add">+	/* drops the reference from mem_cgroup_can_attach */</span>
<span class="p_add">+	css_put(&amp;from-&gt;css);</span>
 }
 
 static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,
 				 struct cgroup_taskset *tset)
 {
<span class="p_del">-	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="p_add">+	struct mem_cgroup *to = mem_cgroup_from_css(css);</span>
 	struct mem_cgroup *from;
 	struct task_struct *p;
 	struct mm_struct *mm;
<span class="p_chunk">@@ -5017,14 +5050,27 @@</span> <span class="p_context"> static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
 	int ret = 0;
 
 	p = cgroup_taskset_first(tset);
<span class="p_del">-	from = mem_cgroup_from_task(p);</span>
<span class="p_del">-</span>
<span class="p_del">-	VM_BUG_ON(from == memcg);</span>
<span class="p_add">+	if (!thread_group_leader(p))</span>
<span class="p_add">+		return 0;</span>
 
 	mm = get_task_mm(p);
 	if (!mm)
 		return 0;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * tasks&#39; cgroup might be different from the one p-&gt;mm is associated</span>
<span class="p_add">+	 * with because CLONE_VM is allowed without CLONE_THREAD. The task is</span>
<span class="p_add">+	 * moving so we have to migrate from the memcg associated with its</span>
<span class="p_add">+	 * address space.</span>
<span class="p_add">+	 * Keep the reference until the whole migration is done - until</span>
<span class="p_add">+	 * mem_cgroup_clear_mc</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	from = get_mem_cgroup_from_mm(mm);</span>
<span class="p_add">+	if (from == to) {</span>
<span class="p_add">+		css_put(&amp;from-&gt;css);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	VM_BUG_ON(mc.from);
 	VM_BUG_ON(mc.to);
 	VM_BUG_ON(mc.precharge);
<span class="p_chunk">@@ -5033,7 +5079,7 @@</span> <span class="p_context"> static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
 
 	spin_lock(&amp;mc.lock);
 	mc.from = from;
<span class="p_del">-	mc.to = memcg;</span>
<span class="p_add">+	mc.to = to;</span>
 	mc.flags = move_flags;
 	spin_unlock(&amp;mc.lock);
 	/* We set mc.moving_task later */
<span class="p_chunk">@@ -5043,14 +5089,15 @@</span> <span class="p_context"> static int mem_cgroup_can_attach(struct cgroup_subsys_state *css,</span>
 	 * tunable will only affect upcoming migrations, not the current one.
 	 * So we need to save it, and keep it going.
 	 */
<span class="p_del">-	move_flags = READ_ONCE(memcg-&gt;move_charge_at_immigrate);</span>
<span class="p_add">+	move_flags = READ_ONCE(to-&gt;move_charge_at_immigrate);</span>
 
 	/* We move charges only when we move a owner of the mm */
<span class="p_del">-	if (move_flags &amp;&amp; mm-&gt;owner == p) {</span>
<span class="p_add">+	if (move_flags) {</span>
 		ret = mem_cgroup_precharge_mc(mm);
 		if (ret)
 			mem_cgroup_clear_mc();
 	}
<span class="p_add">+out:</span>
 	mmput(mm);
 	return ret;
 }
<span class="p_chunk">@@ -5204,6 +5251,12 @@</span> <span class="p_context"> static void mem_cgroup_move_task(struct cgroup_subsys_state *css,</span>
 	struct mm_struct *mm = get_task_mm(p);
 
 	if (mm) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Commit to a new memcg. mc.to points to the destination</span>
<span class="p_add">+		 * memcg even when the current charges are not moved.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mm_move_memcg(mm, mc.to);</span>
<span class="p_add">+</span>
 		if (mc_move_charge())
 			mem_cgroup_move_charge(mm);
 		mmput(mm);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



