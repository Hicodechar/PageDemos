
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,3/5] arm64: Create and use __tlbi_dsb() macros - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,3/5] arm64: Create and use __tlbi_dsb() macros</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=41311">Christopher Covington</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 11, 2017, 2:41 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170111144118.17062-3-cov@codeaurora.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9510207/mbox/"
   >mbox</a>
|
   <a href="/patch/9510207/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9510207/">/patch/9510207/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	900246075C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Jan 2017 14:42:59 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 855A42857D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Jan 2017 14:42:59 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7A3F128613; Wed, 11 Jan 2017 14:42:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0A2542857D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 11 Jan 2017 14:42:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S967528AbdAKOm5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 11 Jan 2017 09:42:57 -0500
Received: from smtp.codeaurora.org ([198.145.29.96]:39686 &quot;EHLO
	smtp.codeaurora.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S967390AbdAKOll (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 11 Jan 2017 09:41:41 -0500
Received: by smtp.codeaurora.org (Postfix, from userid 1000)
	id 588AA61503; Wed, 11 Jan 2017 14:41:39 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1484145700;
	bh=IZtfH1S1OC7Vrsdd0Kn6mbYwi2xBhsJAuKoYg7fFJ+Y=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=LWQX7t3DKTxULIDN5if4VuA8RTKXITy7hVu8sI0SMhoTHi2P4DtVet3DpabUQzak6
	dKPpNyJCZWPH+Yrm5qnbNQT7kwTaSDw226Tq6ixCUPrFAoJzvmQOhBh/grXaYZwXD/
	wG8s49qKB756j2OJUO6U7FTdss0u9QkcLdUf4Gc8=
Received: from illium.qualcomm.com (global_nat1_iad_fw.qualcomm.com
	[129.46.232.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-SHA256 (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: cov@smtp.codeaurora.org)
	by smtp.codeaurora.org (Postfix) with ESMTPSA id 2B9F660F94;
	Wed, 11 Jan 2017 14:41:37 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1484145698;
	bh=IZtfH1S1OC7Vrsdd0Kn6mbYwi2xBhsJAuKoYg7fFJ+Y=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=RELeceUekdahdd7NSRrAJ3ckHI+S8BxP6a4Ri4FhQudX7lIYw2abwoDKMq3oKfQyv
	3FVPO7xdrHxf2SJu0rLMlkJclOZWi/7yIimgslPB6xeP8jNaRjWmKg3+BwFO5KYx81
	+noeIaEigkiATmNZnsLJZFnP0bSvF0veP6e90IHY=
DMARC-Filter: OpenDMARC Filter v1.3.1 smtp.codeaurora.org 2B9F660F94
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	dmarc=none header.from=codeaurora.org
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	spf=pass smtp.mailfrom=cov@codeaurora.org
From: Christopher Covington &lt;cov@codeaurora.org&gt;
To: Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	=?UTF-8?q?Radim=20Kr=C4=8Dm=C3=A1=C5=99?= &lt;rkrcmar@redhat.com&gt;,
	Christoffer Dall &lt;christoffer.dall@linaro.org&gt;,
	Marc Zyngier &lt;marc.zyngier@arm.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, kvm@vger.kernel.org,
	linux-arm-kernel@lists.infradead.org, kvmarm@lists.cs.columbia.edu,
	linux-kernel@vger.kernel.org, shankerd@codeaurora.org,
	timur@codeaurora.org
Cc: Mark Langsdorf &lt;mlangsdo@redhat.com&gt;,
	Mark Salter &lt;msalter@redhat.com&gt;, Jon Masters &lt;jcm@redhat.com&gt;,
	Christopher Covington &lt;cov@codeaurora.org&gt;
Subject: [PATCH v3 3/5] arm64: Create and use __tlbi_dsb() macros
Date: Wed, 11 Jan 2017 09:41:16 -0500
Message-Id: &lt;20170111144118.17062-3-cov@codeaurora.org&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;20170111144118.17062-1-cov@codeaurora.org&gt;
References: &lt;20170111144118.17062-1-cov@codeaurora.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41311">Christopher Covington</a> - Jan. 11, 2017, 2:41 p.m.</div>
<pre class="content">
This refactoring will allow an errata workaround that repeats tlbi dsb
sequences to only change one location. This is not intended to change the
generated assembly and comparison of before and after preprocessor output
of arch/arm64/mm/mmu.c and vmlinux objdump shows no functional changes.
<span class="signed-off-by">
Signed-off-by: Christopher Covington &lt;cov@codeaurora.org&gt;</span>
---
 arch/arm64/include/asm/tlbflush.h | 104 +++++++++++++++++++++++++-------------
 1 file changed, 69 insertions(+), 35 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Jan. 12, 2017, 4:58 p.m.</div>
<pre class="content">
Hi Christopher,

On Wed, Jan 11, 2017 at 09:41:16AM -0500, Christopher Covington wrote:
<span class="quote">&gt; This refactoring will allow an errata workaround that repeats tlbi dsb</span>
<span class="quote">&gt; sequences to only change one location. This is not intended to change the</span>
<span class="quote">&gt; generated assembly and comparison of before and after preprocessor output</span>
<span class="quote">&gt; of arch/arm64/mm/mmu.c and vmlinux objdump shows no functional changes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Christopher Covington &lt;cov@codeaurora.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/include/asm/tlbflush.h | 104 +++++++++++++++++++++++++-------------</span>
<span class="quote">&gt;  1 file changed, 69 insertions(+), 35 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt; index deab523..f28813c 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt; @@ -25,22 +25,69 @@</span>
<span class="quote">&gt;  #include &lt;asm/cputype.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Raw TLBI operations.</span>
<span class="quote">&gt; + * Raw TLBI, DSB operations</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * Where necessary, use the __tlbi() macro to avoid asm()</span>
<span class="quote">&gt; - * boilerplate. Drivers and most kernel code should use the TLB</span>
<span class="quote">&gt; - * management routines in preference to the macro below.</span>
<span class="quote">&gt; + * Where necessary, use __tlbi_*dsb() macros to avoid asm() boilerplate.</span>
<span class="quote">&gt; + * Drivers and most kernel code should use the TLB management routines in</span>
<span class="quote">&gt; + * preference to the macros below.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The macro can be used as __tlbi(op) or __tlbi(op, arg), depending</span>
<span class="quote">&gt; - * on whether a particular TLBI operation takes an argument or</span>
<span class="quote">&gt; - * not. The macros handles invoking the asm with or without the</span>
<span class="quote">&gt; - * register argument as appropriate.</span>
<span class="quote">&gt; + * The __tlbi_dsb() macro handles invoking the asm without any register</span>
<span class="quote">&gt; + * argument, with a single register argument, and with start (included)</span>
<span class="quote">&gt; + * and end (excluded) range of register arguments. For example:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * __tlbi_dsb(op, attr)</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * 	tlbi op</span>
<span class="quote">&gt; + *	dsb attr</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * __tlbi_dsb(op, attr, addr)</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *	mov %[addr], =addr</span>
<span class="quote">&gt; + *	tlbi op, %[addr]</span>
<span class="quote">&gt; + *	dsb attr</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * __tlbi_range_dsb(op, attr, start, end)</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * 	mov %[arg], =start</span>
<span class="quote">&gt; + *	mov %[end], =end</span>
<span class="quote">&gt; + * for:</span>
<span class="quote">&gt; + * 	tlbi op, %[addr]</span>
<span class="quote">&gt; + * 	add %[addr], %[addr], #(1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="quote">&gt; + * 	cmp %[addr], %[end]</span>
<span class="quote">&gt; + * 	b.ne for</span>
<span class="quote">&gt; + * 	dsb attr</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define __TLBI_0(op, arg)		asm (&quot;tlbi &quot; #op)</span>
<span class="quote">&gt; -#define __TLBI_1(op, arg)		asm (&quot;tlbi &quot; #op &quot;, %0&quot; : : &quot;r&quot; (arg))</span>
<span class="quote">&gt; -#define __TLBI_N(op, arg, n, ...)	__TLBI_##n(op, arg)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define __tlbi(op, ...)		__TLBI_N(op, ##__VA_ARGS__, 1, 0)</span>
<span class="quote">&gt; +#define __TLBI_FOR_0(ig0, ig1, ig2)</span>
<span class="quote">&gt; +#define __TLBI_INSTR_0(op, ig1, ig2)	&quot;tlbi &quot; #op</span>
<span class="quote">&gt; +#define __TLBI_IO_0(ig0, ig1, ig2)	: :</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __TLBI_FOR_1(ig0, ig1, ig2)</span>
<span class="quote">&gt; +#define __TLBI_INSTR_1(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="quote">&gt; +#define __TLBI_IO_1(ig0, arg, ig1)	: : &quot;r&quot; (arg)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __TLBI_FOR_2(ig0, start, ig1)	unsigned long addr;		       \</span>
<span class="quote">&gt; +					for (addr = start; addr &lt; end;	       \</span>
<span class="quote">&gt; +						addr += 1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="quote">&gt; +#define __TLBI_INSTR_2(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="quote">&gt; +#define __TLBI_IO_2(ig0, ig1, ig2)	: : &quot;r&quot; (addr)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __TLBI_FOR_N(op, a1, a2, n, ...)	__TLBI_FOR_##n(op, a1, a2)</span>
<span class="quote">&gt; +#define __TLBI_INSTR_N(op, a1, a2, n, ...)	__TLBI_INSTR_##n(op, a1, a2)</span>
<span class="quote">&gt; +#define __TLBI_IO_N(op, a1, a2, n, ...)	__TLBI_IO_##n(op, a1, a2)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __TLBI_FOR(op, ...)		__TLBI_FOR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt; +#define __TLBI_INSTR(op, ...)		__TLBI_INSTR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt; +#define __TLBI_IO(op, ...)		__TLBI_IO_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __tlbi_asm_dsb(as, op, attr, ...) do {				       \</span>
<span class="quote">&gt; +		__TLBI_FOR(op, ##__VA_ARGS__)				       \</span>
<span class="quote">&gt; +			asm (__TLBI_INSTR(op, ##__VA_ARGS__)		       \</span>
<span class="quote">&gt; +			__TLBI_IO(op, ##__VA_ARGS__));			       \</span>
<span class="quote">&gt; +		asm volatile (	     as			&quot;\ndsb &quot; #attr &quot;\n&quot;    \</span>
<span class="quote">&gt; +		: : : &quot;memory&quot;); } while (0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __tlbi_dsb(...)	__tlbi_asm_dsb(&quot;&quot;, ##__VA_ARGS__)</span>

I can&#39;t deny that this is cool, but ultimately it&#39;s completely unreadable.
What I was thinking you&#39;d do would be make __tlbi expand to:

  tlbi
  dsb
  tlbi
  dsb

for Falkor, and:

  tlbi
  nop
  nop
  nop

for everybody else.

Wouldn&#39;t that localise this change sufficiently that you wouldn&#39;t need
to change all the callers and encode the looping in your cpp macros?

I realise you get an extra dsb in some places with that change, but I&#39;d
like to see numbers for the impact of that on top of the workaround. If
it&#39;s an issue, then an alternative sequence would be:

  tlbi
  dsb
  tlbi

and you&#39;d rely on the existing dsb to complete that.

Having said that, I don&#39;t understand how your current loop code works
when the workaround is applied. AFAICT, you end up emitting something
like:

dsb ishst
for i in 0 to n
	tlbi va+i
dsb
tlbi va+n
dsb

which looks wrong to me. Am I misreading something here?

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41311">Christopher Covington</a> - Jan. 13, 2017, 3:12 p.m.</div>
<pre class="content">
Hi Will,

On 01/12/2017 11:58 AM, Will Deacon wrote:
<span class="quote">&gt; Hi Christopher,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Wed, Jan 11, 2017 at 09:41:16AM -0500, Christopher Covington wrote:</span>
<span class="quote">&gt;&gt; This refactoring will allow an errata workaround that repeats tlbi dsb</span>
<span class="quote">&gt;&gt; sequences to only change one location. This is not intended to change the</span>
<span class="quote">&gt;&gt; generated assembly and comparison of before and after preprocessor output</span>
<span class="quote">&gt;&gt; of arch/arm64/mm/mmu.c and vmlinux objdump shows no functional changes.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Christopher Covington &lt;cov@codeaurora.org&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/arm64/include/asm/tlbflush.h | 104 +++++++++++++++++++++++++-------------</span>
<span class="quote">&gt;&gt;  1 file changed, 69 insertions(+), 35 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt;&gt; index deab523..f28813c 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/include/asm/tlbflush.h</span>
<span class="quote">&gt;&gt; @@ -25,22 +25,69 @@</span>
<span class="quote">&gt;&gt;  #include &lt;asm/cputype.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; - * Raw TLBI operations.</span>
<span class="quote">&gt;&gt; + * Raw TLBI, DSB operations</span>
<span class="quote">&gt;&gt;   *</span>
<span class="quote">&gt;&gt; - * Where necessary, use the __tlbi() macro to avoid asm()</span>
<span class="quote">&gt;&gt; - * boilerplate. Drivers and most kernel code should use the TLB</span>
<span class="quote">&gt;&gt; - * management routines in preference to the macro below.</span>
<span class="quote">&gt;&gt; + * Where necessary, use __tlbi_*dsb() macros to avoid asm() boilerplate.</span>
<span class="quote">&gt;&gt; + * Drivers and most kernel code should use the TLB management routines in</span>
<span class="quote">&gt;&gt; + * preference to the macros below.</span>
<span class="quote">&gt;&gt;   *</span>
<span class="quote">&gt;&gt; - * The macro can be used as __tlbi(op) or __tlbi(op, arg), depending</span>
<span class="quote">&gt;&gt; - * on whether a particular TLBI operation takes an argument or</span>
<span class="quote">&gt;&gt; - * not. The macros handles invoking the asm with or without the</span>
<span class="quote">&gt;&gt; - * register argument as appropriate.</span>
<span class="quote">&gt;&gt; + * The __tlbi_dsb() macro handles invoking the asm without any register</span>
<span class="quote">&gt;&gt; + * argument, with a single register argument, and with start (included)</span>
<span class="quote">&gt;&gt; + * and end (excluded) range of register arguments. For example:</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * __tlbi_dsb(op, attr)</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * 	tlbi op</span>
<span class="quote">&gt;&gt; + *	dsb attr</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * __tlbi_dsb(op, attr, addr)</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *	mov %[addr], =addr</span>
<span class="quote">&gt;&gt; + *	tlbi op, %[addr]</span>
<span class="quote">&gt;&gt; + *	dsb attr</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * __tlbi_range_dsb(op, attr, start, end)</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * 	mov %[arg], =start</span>
<span class="quote">&gt;&gt; + *	mov %[end], =end</span>
<span class="quote">&gt;&gt; + * for:</span>
<span class="quote">&gt;&gt; + * 	tlbi op, %[addr]</span>
<span class="quote">&gt;&gt; + * 	add %[addr], %[addr], #(1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="quote">&gt;&gt; + * 	cmp %[addr], %[end]</span>
<span class="quote">&gt;&gt; + * 	b.ne for</span>
<span class="quote">&gt;&gt; + * 	dsb attr</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt; -#define __TLBI_0(op, arg)		asm (&quot;tlbi &quot; #op)</span>
<span class="quote">&gt;&gt; -#define __TLBI_1(op, arg)		asm (&quot;tlbi &quot; #op &quot;, %0&quot; : : &quot;r&quot; (arg))</span>
<span class="quote">&gt;&gt; -#define __TLBI_N(op, arg, n, ...)	__TLBI_##n(op, arg)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -#define __tlbi(op, ...)		__TLBI_N(op, ##__VA_ARGS__, 1, 0)</span>
<span class="quote">&gt;&gt; +#define __TLBI_FOR_0(ig0, ig1, ig2)</span>
<span class="quote">&gt;&gt; +#define __TLBI_INSTR_0(op, ig1, ig2)	&quot;tlbi &quot; #op</span>
<span class="quote">&gt;&gt; +#define __TLBI_IO_0(ig0, ig1, ig2)	: :</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __TLBI_FOR_1(ig0, ig1, ig2)</span>
<span class="quote">&gt;&gt; +#define __TLBI_INSTR_1(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="quote">&gt;&gt; +#define __TLBI_IO_1(ig0, arg, ig1)	: : &quot;r&quot; (arg)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __TLBI_FOR_2(ig0, start, ig1)	unsigned long addr;		       \</span>
<span class="quote">&gt;&gt; +					for (addr = start; addr &lt; end;	       \</span>
<span class="quote">&gt;&gt; +						addr += 1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="quote">&gt;&gt; +#define __TLBI_INSTR_2(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="quote">&gt;&gt; +#define __TLBI_IO_2(ig0, ig1, ig2)	: : &quot;r&quot; (addr)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __TLBI_FOR_N(op, a1, a2, n, ...)	__TLBI_FOR_##n(op, a1, a2)</span>
<span class="quote">&gt;&gt; +#define __TLBI_INSTR_N(op, a1, a2, n, ...)	__TLBI_INSTR_##n(op, a1, a2)</span>
<span class="quote">&gt;&gt; +#define __TLBI_IO_N(op, a1, a2, n, ...)	__TLBI_IO_##n(op, a1, a2)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __TLBI_FOR(op, ...)		__TLBI_FOR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt;&gt; +#define __TLBI_INSTR(op, ...)		__TLBI_INSTR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt;&gt; +#define __TLBI_IO(op, ...)		__TLBI_IO_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __tlbi_asm_dsb(as, op, attr, ...) do {				       \</span>
<span class="quote">&gt;&gt; +		__TLBI_FOR(op, ##__VA_ARGS__)				       \</span>
<span class="quote">&gt;&gt; +			asm (__TLBI_INSTR(op, ##__VA_ARGS__)		       \</span>
<span class="quote">&gt;&gt; +			__TLBI_IO(op, ##__VA_ARGS__));			       \</span>
<span class="quote">&gt;&gt; +		asm volatile (	     as			&quot;\ndsb &quot; #attr &quot;\n&quot;    \</span>
<span class="quote">&gt;&gt; +		: : : &quot;memory&quot;); } while (0)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __tlbi_dsb(...)	__tlbi_asm_dsb(&quot;&quot;, ##__VA_ARGS__)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can&#39;t deny that this is cool, but ultimately it&#39;s completely unreadable.</span>
<span class="quote">&gt; What I was thinking you&#39;d do would be make __tlbi expand to:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   tlbi</span>
<span class="quote">&gt;   dsb</span>
<span class="quote">&gt;   tlbi</span>
<span class="quote">&gt;   dsb</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; for Falkor, and:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   tlbi</span>
<span class="quote">&gt;   nop</span>
<span class="quote">&gt;   nop</span>
<span class="quote">&gt;   nop</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; for everybody else.</span>

Thanks for the suggestion. So would __tlbi take a dsb sharability argument in
your proposal? Or would it be communicated in some other fashion, maybe inferred
from the tlbi argument? Or would the workaround dsbs all be the worst/broadest
case?
<span class="quote">
&gt; Wouldn&#39;t that localise this change sufficiently that you wouldn&#39;t need</span>
<span class="quote">&gt; to change all the callers and encode the looping in your cpp macros?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I realise you get an extra dsb in some places with that change, but I&#39;d</span>
<span class="quote">&gt; like to see numbers for the impact of that on top of the workaround. If</span>
<span class="quote">&gt; it&#39;s an issue, then an alternative sequence would be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   tlbi</span>
<span class="quote">&gt;   dsb</span>
<span class="quote">&gt;   tlbi</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; and you&#39;d rely on the existing dsb to complete that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having said that, I don&#39;t understand how your current loop code works</span>
<span class="quote">&gt; when the workaround is applied. AFAICT, you end up emitting something</span>
<span class="quote">&gt; like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; dsb ishst</span>
<span class="quote">&gt; for i in 0 to n</span>
<span class="quote">&gt; 	tlbi va+i</span>
<span class="quote">&gt; dsb</span>
<span class="quote">&gt; tlbi va+n</span>
<span class="quote">&gt; dsb</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; which looks wrong to me. Am I misreading something here?</span>

You&#39;re right, I am off by 1 &lt;&lt; (PAGE_SHIFT - 12) here. I would need to
increment, compare, not take the loop branch (regular for loop stuff),
then decrement (missing) and perform TLB invalidation again (present but
using incorrect value).

Thanks,
Cov
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Jan. 13, 2017, 4:12 p.m.</div>
<pre class="content">
On Fri, Jan 13, 2017 at 10:12:36AM -0500, Christopher Covington wrote:
<span class="quote">&gt; On 01/12/2017 11:58 AM, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; On Wed, Jan 11, 2017 at 09:41:16AM -0500, Christopher Covington wrote:</span>
<span class="quote">&gt; &gt;&gt; +#define __tlbi_asm_dsb(as, op, attr, ...) do {				       \</span>
<span class="quote">&gt; &gt;&gt; +		__TLBI_FOR(op, ##__VA_ARGS__)				       \</span>
<span class="quote">&gt; &gt;&gt; +			asm (__TLBI_INSTR(op, ##__VA_ARGS__)		       \</span>
<span class="quote">&gt; &gt;&gt; +			__TLBI_IO(op, ##__VA_ARGS__));			       \</span>
<span class="quote">&gt; &gt;&gt; +		asm volatile (	     as			&quot;\ndsb &quot; #attr &quot;\n&quot;    \</span>
<span class="quote">&gt; &gt;&gt; +		: : : &quot;memory&quot;); } while (0)</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +#define __tlbi_dsb(...)	__tlbi_asm_dsb(&quot;&quot;, ##__VA_ARGS__)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I can&#39;t deny that this is cool, but ultimately it&#39;s completely unreadable.</span>
<span class="quote">&gt; &gt; What I was thinking you&#39;d do would be make __tlbi expand to:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   tlbi</span>
<span class="quote">&gt; &gt;   dsb</span>
<span class="quote">&gt; &gt;   tlbi</span>
<span class="quote">&gt; &gt;   dsb</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; for Falkor, and:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   tlbi</span>
<span class="quote">&gt; &gt;   nop</span>
<span class="quote">&gt; &gt;   nop</span>
<span class="quote">&gt; &gt;   nop</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; for everybody else.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for the suggestion. So would __tlbi take a dsb sharability argument in</span>
<span class="quote">&gt; your proposal? Or would it be communicated in some other fashion, maybe inferred</span>
<span class="quote">&gt; from the tlbi argument? Or would the workaround dsbs all be the worst/broadest</span>
<span class="quote">&gt; case?</span>

I think always using inner-shareable should be ok. If you wanted to optimise
this, you&#39;d want to avoid the workaround altogether for non-shareable
invalidation, but that&#39;s fairly rare and I doubt you&#39;d be able to measure
the impact.
<span class="quote">
&gt; &gt; Wouldn&#39;t that localise this change sufficiently that you wouldn&#39;t need</span>
<span class="quote">&gt; &gt; to change all the callers and encode the looping in your cpp macros?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I realise you get an extra dsb in some places with that change, but I&#39;d</span>
<span class="quote">&gt; &gt; like to see numbers for the impact of that on top of the workaround. If</span>
<span class="quote">&gt; &gt; it&#39;s an issue, then an alternative sequence would be:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   tlbi</span>
<span class="quote">&gt; &gt;   dsb</span>
<span class="quote">&gt; &gt;   tlbi</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; and you&#39;d rely on the existing dsb to complete that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Having said that, I don&#39;t understand how your current loop code works</span>
<span class="quote">&gt; &gt; when the workaround is applied. AFAICT, you end up emitting something</span>
<span class="quote">&gt; &gt; like:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; dsb ishst</span>
<span class="quote">&gt; &gt; for i in 0 to n</span>
<span class="quote">&gt; &gt; 	tlbi va+i</span>
<span class="quote">&gt; &gt; dsb</span>
<span class="quote">&gt; &gt; tlbi va+n</span>
<span class="quote">&gt; &gt; dsb</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; which looks wrong to me. Am I misreading something here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You&#39;re right, I am off by 1 &lt;&lt; (PAGE_SHIFT - 12) here. I would need to</span>
<span class="quote">&gt; increment, compare, not take the loop branch (regular for loop stuff),</span>
<span class="quote">&gt; then decrement (missing) and perform TLB invalidation again (present but</span>
<span class="quote">&gt; using incorrect value).</span>

It also strikes me as odd that you only need one extra TLBI after the loop
has finished, as opposed to a tlbi; dsb; tlbi loop body (which is what you&#39;d
get if you modified __tlbi as I suggest).

Is it sufficient to have one extra TLBI after the loop and, if so, is the
performance impact of my suggestion therefore unacceptable?

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41311">Christopher Covington</a> - Jan. 24, 2017, 10:03 p.m.</div>
<pre class="content">
Hi Will,

On 01/13/2017 11:12 AM, Will Deacon wrote:
<span class="quote">&gt; On Fri, Jan 13, 2017 at 10:12:36AM -0500, Christopher Covington wrote:</span>
<span class="quote">&gt;&gt; On 01/12/2017 11:58 AM, Will Deacon wrote:</span>
<span class="quote">&gt;&gt;&gt; On Wed, Jan 11, 2017 at 09:41:16AM -0500, Christopher Covington wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define __tlbi_asm_dsb(as, op, attr, ...) do {				       \</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__TLBI_FOR(op, ##__VA_ARGS__)				       \</span>
<span class="quote">&gt;&gt;&gt;&gt; +			asm (__TLBI_INSTR(op, ##__VA_ARGS__)		       \</span>
<span class="quote">&gt;&gt;&gt;&gt; +			__TLBI_IO(op, ##__VA_ARGS__));			       \</span>
<span class="quote">&gt;&gt;&gt;&gt; +		asm volatile (	     as			&quot;\ndsb &quot; #attr &quot;\n&quot;    \</span>
<span class="quote">&gt;&gt;&gt;&gt; +		: : : &quot;memory&quot;); } while (0)</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define __tlbi_dsb(...)	__tlbi_asm_dsb(&quot;&quot;, ##__VA_ARGS__)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I can&#39;t deny that this is cool, but ultimately it&#39;s completely unreadable.</span>
<span class="quote">&gt;&gt;&gt; What I was thinking you&#39;d do would be make __tlbi expand to:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   tlbi</span>
<span class="quote">&gt;&gt;&gt;   dsb</span>
<span class="quote">&gt;&gt;&gt;   tlbi</span>
<span class="quote">&gt;&gt;&gt;   dsb</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; for Falkor, and:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   tlbi</span>
<span class="quote">&gt;&gt;&gt;   nop</span>
<span class="quote">&gt;&gt;&gt;   nop</span>
<span class="quote">&gt;&gt;&gt;   nop</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; for everybody else.</span>

I&#39;ve implemented this (minus the last dsb / nop) in the next revision.
<span class="quote">
&gt;&gt; Thanks for the suggestion. So would __tlbi take a dsb sharability argument in</span>
<span class="quote">&gt;&gt; your proposal? Or would it be communicated in some other fashion, maybe inferred</span>
<span class="quote">&gt;&gt; from the tlbi argument? Or would the workaround dsbs all be the worst/broadest</span>
<span class="quote">&gt;&gt; case?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think always using inner-shareable should be ok. If you wanted to optimise</span>
<span class="quote">&gt; this, you&#39;d want to avoid the workaround altogether for non-shareable</span>
<span class="quote">&gt; invalidation, but that&#39;s fairly rare and I doubt you&#39;d be able to measure</span>
<span class="quote">&gt; the impact.</span>

I did not originally notice that Shanker&#39;s original workaround implementation
unnecessarily applies the workaround to non-shareable invalidations. They&#39;re
not affected by the erratum. But as you say, it&#39;s simpler to modify __tlbi for
all cases. I&#39;m not currently worried about that performance impact.
<span class="quote">
&gt;&gt;&gt; Wouldn&#39;t that localise this change sufficiently that you wouldn&#39;t need</span>
<span class="quote">&gt;&gt;&gt; to change all the callers and encode the looping in your cpp macros?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I realise you get an extra dsb in some places with that change, but I&#39;d</span>
<span class="quote">&gt;&gt;&gt; like to see numbers for the impact of that on top of the workaround. If</span>
<span class="quote">&gt;&gt;&gt; it&#39;s an issue, then an alternative sequence would be:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   tlbi</span>
<span class="quote">&gt;&gt;&gt;   dsb</span>
<span class="quote">&gt;&gt;&gt;   tlbi</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; and you&#39;d rely on the existing dsb to complete that.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Having said that, I don&#39;t understand how your current loop code works</span>
<span class="quote">&gt;&gt;&gt; when the workaround is applied. AFAICT, you end up emitting something</span>
<span class="quote">&gt;&gt;&gt; like:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; dsb ishst</span>
<span class="quote">&gt;&gt;&gt; for i in 0 to n</span>
<span class="quote">&gt;&gt;&gt; 	tlbi va+i</span>
<span class="quote">&gt;&gt;&gt; dsb</span>
<span class="quote">&gt;&gt;&gt; tlbi va+n</span>
<span class="quote">&gt;&gt;&gt; dsb</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; which looks wrong to me. Am I misreading something here?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; You&#39;re right, I am off by 1 &lt;&lt; (PAGE_SHIFT - 12) here. I would need to</span>
<span class="quote">&gt;&gt; increment, compare, not take the loop branch (regular for loop stuff),</span>
<span class="quote">&gt;&gt; then decrement (missing) and perform TLB invalidation again (present but</span>
<span class="quote">&gt;&gt; using incorrect value).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It also strikes me as odd that you only need one extra TLBI after the loop</span>
<span class="quote">&gt; has finished, as opposed to a tlbi; dsb; tlbi loop body (which is what you&#39;d</span>
<span class="quote">&gt; get if you modified __tlbi as I suggest).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it sufficient to have one extra TLBI after the loop and, if so, is the</span>
<span class="quote">&gt; performance impact of my suggestion therefore unacceptable?</span>

One is sufficient according to the errata documentation. I&#39;ve described that
aspect in the commit message of the next revision.

I&#39;ve suggested colleagues follow up regarding performance. But reliable
functionality comes first.

Thanks,
Cov
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_header">index deab523..f28813c 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -25,22 +25,69 @@</span> <span class="p_context"></span>
 #include &lt;asm/cputype.h&gt;
 
 /*
<span class="p_del">- * Raw TLBI operations.</span>
<span class="p_add">+ * Raw TLBI, DSB operations</span>
  *
<span class="p_del">- * Where necessary, use the __tlbi() macro to avoid asm()</span>
<span class="p_del">- * boilerplate. Drivers and most kernel code should use the TLB</span>
<span class="p_del">- * management routines in preference to the macro below.</span>
<span class="p_add">+ * Where necessary, use __tlbi_*dsb() macros to avoid asm() boilerplate.</span>
<span class="p_add">+ * Drivers and most kernel code should use the TLB management routines in</span>
<span class="p_add">+ * preference to the macros below.</span>
  *
<span class="p_del">- * The macro can be used as __tlbi(op) or __tlbi(op, arg), depending</span>
<span class="p_del">- * on whether a particular TLBI operation takes an argument or</span>
<span class="p_del">- * not. The macros handles invoking the asm with or without the</span>
<span class="p_del">- * register argument as appropriate.</span>
<span class="p_add">+ * The __tlbi_dsb() macro handles invoking the asm without any register</span>
<span class="p_add">+ * argument, with a single register argument, and with start (included)</span>
<span class="p_add">+ * and end (excluded) range of register arguments. For example:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __tlbi_dsb(op, attr)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * 	tlbi op</span>
<span class="p_add">+ *	dsb attr</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __tlbi_dsb(op, attr, addr)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	mov %[addr], =addr</span>
<span class="p_add">+ *	tlbi op, %[addr]</span>
<span class="p_add">+ *	dsb attr</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __tlbi_range_dsb(op, attr, start, end)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * 	mov %[arg], =start</span>
<span class="p_add">+ *	mov %[end], =end</span>
<span class="p_add">+ * for:</span>
<span class="p_add">+ * 	tlbi op, %[addr]</span>
<span class="p_add">+ * 	add %[addr], %[addr], #(1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="p_add">+ * 	cmp %[addr], %[end]</span>
<span class="p_add">+ * 	b.ne for</span>
<span class="p_add">+ * 	dsb attr</span>
  */
<span class="p_del">-#define __TLBI_0(op, arg)		asm (&quot;tlbi &quot; #op)</span>
<span class="p_del">-#define __TLBI_1(op, arg)		asm (&quot;tlbi &quot; #op &quot;, %0&quot; : : &quot;r&quot; (arg))</span>
<span class="p_del">-#define __TLBI_N(op, arg, n, ...)	__TLBI_##n(op, arg)</span>
 
<span class="p_del">-#define __tlbi(op, ...)		__TLBI_N(op, ##__VA_ARGS__, 1, 0)</span>
<span class="p_add">+#define __TLBI_FOR_0(ig0, ig1, ig2)</span>
<span class="p_add">+#define __TLBI_INSTR_0(op, ig1, ig2)	&quot;tlbi &quot; #op</span>
<span class="p_add">+#define __TLBI_IO_0(ig0, ig1, ig2)	: :</span>
<span class="p_add">+</span>
<span class="p_add">+#define __TLBI_FOR_1(ig0, ig1, ig2)</span>
<span class="p_add">+#define __TLBI_INSTR_1(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="p_add">+#define __TLBI_IO_1(ig0, arg, ig1)	: : &quot;r&quot; (arg)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __TLBI_FOR_2(ig0, start, ig1)	unsigned long addr;		       \</span>
<span class="p_add">+					for (addr = start; addr &lt; end;	       \</span>
<span class="p_add">+						addr += 1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="p_add">+#define __TLBI_INSTR_2(op, ig0, ig1)	&quot;tlbi &quot; #op &quot;, %0&quot;</span>
<span class="p_add">+#define __TLBI_IO_2(ig0, ig1, ig2)	: : &quot;r&quot; (addr)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __TLBI_FOR_N(op, a1, a2, n, ...)	__TLBI_FOR_##n(op, a1, a2)</span>
<span class="p_add">+#define __TLBI_INSTR_N(op, a1, a2, n, ...)	__TLBI_INSTR_##n(op, a1, a2)</span>
<span class="p_add">+#define __TLBI_IO_N(op, a1, a2, n, ...)	__TLBI_IO_##n(op, a1, a2)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __TLBI_FOR(op, ...)		__TLBI_FOR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="p_add">+#define __TLBI_INSTR(op, ...)		__TLBI_INSTR_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="p_add">+#define __TLBI_IO(op, ...)		__TLBI_IO_N(op, ##__VA_ARGS__, 2, 1, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __tlbi_asm_dsb(as, op, attr, ...) do {				       \</span>
<span class="p_add">+		__TLBI_FOR(op, ##__VA_ARGS__)				       \</span>
<span class="p_add">+			asm (__TLBI_INSTR(op, ##__VA_ARGS__)		       \</span>
<span class="p_add">+			__TLBI_IO(op, ##__VA_ARGS__));			       \</span>
<span class="p_add">+		asm volatile (	     as			&quot;\ndsb &quot; #attr &quot;\n&quot;    \</span>
<span class="p_add">+		: : : &quot;memory&quot;); } while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __tlbi_dsb(...)	__tlbi_asm_dsb(&quot;&quot;, ##__VA_ARGS__)</span>
 
 /*
  *	TLB Management
<span class="p_chunk">@@ -84,16 +131,14 @@</span> <span class="p_context"></span>
 static inline void local_flush_tlb_all(void)
 {
 	dsb(nshst);
<span class="p_del">-	__tlbi(vmalle1);</span>
<span class="p_del">-	dsb(nsh);</span>
<span class="p_add">+	__tlbi_dsb(vmalle1, nsh);</span>
 	isb();
 }
 
 static inline void flush_tlb_all(void)
 {
 	dsb(ishst);
<span class="p_del">-	__tlbi(vmalle1is);</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	__tlbi_dsb(vmalle1is, ish);</span>
 	isb();
 }
 
<span class="p_chunk">@@ -102,8 +147,7 @@</span> <span class="p_context"> static inline void flush_tlb_mm(struct mm_struct *mm)</span>
 	unsigned long asid = ASID(mm) &lt;&lt; 48;
 
 	dsb(ishst);
<span class="p_del">-	__tlbi(aside1is, asid);</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	__tlbi_dsb(aside1is, ish, asid);</span>
 }
 
 static inline void flush_tlb_page(struct vm_area_struct *vma,
<span class="p_chunk">@@ -112,8 +156,7 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct vm_area_struct *vma,</span>
 	unsigned long addr = uaddr &gt;&gt; 12 | (ASID(vma-&gt;vm_mm) &lt;&lt; 48);
 
 	dsb(ishst);
<span class="p_del">-	__tlbi(vale1is, addr);</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	__tlbi_dsb(vale1is, ish, addr);</span>
 }
 
 /*
<span class="p_chunk">@@ -127,7 +170,6 @@</span> <span class="p_context"> static inline void __flush_tlb_range(struct vm_area_struct *vma,</span>
 				     bool last_level)
 {
 	unsigned long asid = ASID(vma-&gt;vm_mm) &lt;&lt; 48;
<span class="p_del">-	unsigned long addr;</span>
 
 	if ((end - start) &gt; MAX_TLB_RANGE) {
 		flush_tlb_mm(vma-&gt;vm_mm);
<span class="p_chunk">@@ -138,13 +180,10 @@</span> <span class="p_context"> static inline void __flush_tlb_range(struct vm_area_struct *vma,</span>
 	end = asid | (end &gt;&gt; 12);
 
 	dsb(ishst);
<span class="p_del">-	for (addr = start; addr &lt; end; addr += 1 &lt;&lt; (PAGE_SHIFT - 12)) {</span>
<span class="p_del">-		if (last_level)</span>
<span class="p_del">-			__tlbi(vale1is, addr);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			__tlbi(vae1is, addr);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	if (last_level)</span>
<span class="p_add">+		__tlbi_dsb(vale1is, ish, start, end);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		__tlbi_dsb(vae1is, ish, start, end);</span>
 }
 
 static inline void flush_tlb_range(struct vm_area_struct *vma,
<span class="p_chunk">@@ -155,8 +194,6 @@</span> <span class="p_context"> static inline void flush_tlb_range(struct vm_area_struct *vma,</span>
 
 static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 {
<span class="p_del">-	unsigned long addr;</span>
<span class="p_del">-</span>
 	if ((end - start) &gt; MAX_TLB_RANGE) {
 		flush_tlb_all();
 		return;
<span class="p_chunk">@@ -166,9 +203,7 @@</span> <span class="p_context"> static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end</span>
 	end &gt;&gt;= 12;
 
 	dsb(ishst);
<span class="p_del">-	for (addr = start; addr &lt; end; addr += 1 &lt;&lt; (PAGE_SHIFT - 12))</span>
<span class="p_del">-		__tlbi(vaae1is, addr);</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	__tlbi_dsb(vaae1is, ish, start, end);</span>
 	isb();
 }
 
<span class="p_chunk">@@ -181,8 +216,7 @@</span> <span class="p_context"> static inline void __flush_tlb_pgtable(struct mm_struct *mm,</span>
 {
 	unsigned long addr = uaddr &gt;&gt; 12 | (ASID(mm) &lt;&lt; 48);
 
<span class="p_del">-	__tlbi(vae1is, addr);</span>
<span class="p_del">-	dsb(ish);</span>
<span class="p_add">+	__tlbi_dsb(vae1is, ish, addr);</span>
 }
 
 #endif

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



