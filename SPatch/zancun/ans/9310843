
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,1/3] Add support for eXclusive Page Frame Ownership (XPFO) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,1/3] Add support for eXclusive Page Frame Ownership (XPFO)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 2, 2016, 11:39 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160902113909.32631-2-juerg.haefliger@hpe.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9310843/mbox/"
   >mbox</a>
|
   <a href="/patch/9310843/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9310843/">/patch/9310843/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5308860760 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Sep 2016 11:39:57 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 38D832978B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Sep 2016 11:39:57 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 229CB212BE; Fri,  2 Sep 2016 11:39:57 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D42D42978B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Sep 2016 11:39:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752979AbcIBLju (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 2 Sep 2016 07:39:50 -0400
Received: from g2t1383g.austin.hpe.com ([15.233.16.89]:43184 &quot;EHLO
	g2t1383g.austin.hpe.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752541AbcIBLjr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 2 Sep 2016 07:39:47 -0400
Received: from g9t5009.houston.hpe.com (g9t5009.houston.hpe.com
	[15.241.48.73])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by g2t1383g.austin.hpe.com (Postfix) with ESMTPS id 96842C23;
	Fri,  2 Sep 2016 11:39:46 +0000 (UTC)
Received: from smtp1.hpe.com (unknown [16.29.166.28])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128
	bits)) (No client certificate requested)
	by g9t5009.houston.hpe.com (Postfix) with ESMTPS id CB5815B;
	Fri,  2 Sep 2016 11:39:43 +0000 (UTC)
From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	kernel-hardening@lists.openwall.com, linux-x86_64@vger.kernel.org
Cc: juerg.haefliger@hpe.com, vpk@cs.columbia.edu
Subject: [RFC PATCH v2 1/3] Add support for eXclusive Page Frame Ownership
	(XPFO)
Date: Fri,  2 Sep 2016 13:39:07 +0200
Message-Id: &lt;20160902113909.32631-2-juerg.haefliger@hpe.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;20160902113909.32631-1-juerg.haefliger@hpe.com&gt;
References: &lt;1456496467-14247-1-git-send-email-juerg.haefliger@hpe.com&gt;
	&lt;20160902113909.32631-1-juerg.haefliger@hpe.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153531">Juerg Haefliger</a> - Sept. 2, 2016, 11:39 a.m.</div>
<pre class="content">
This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel
attacks. The basic idea is to enforce exclusive ownership of page frames
by either the kernel or userspace, unless explicitly requested by the
kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping.
Specifically two flags to distinguish user vs. kernel pages and to tag
unmapped pages and a reference counter to balance kmap/kunmap operations
and a lock to serialize access to the XPFO fields.

Known issues/limitations:
  - Only supports x86-64 (for now)
  - Only supports 4k pages (for now)
  - There are most likely some legitimate uses cases where the kernel needs
    to access userspace which need to be made XPFO-aware
  - Performance penalty

Reference paper by the original patch authors:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;
<span class="signed-off-by">Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
---
 arch/x86/Kconfig         |   3 +-
 arch/x86/mm/init.c       |   2 +-
 include/linux/highmem.h  |  15 +++-
 include/linux/page_ext.h |   7 ++
 include/linux/xpfo.h     |  39 +++++++++
 lib/swiotlb.c            |   3 +-
 mm/Makefile              |   1 +
 mm/page_alloc.c          |   2 +
 mm/page_ext.c            |   4 +
 mm/xpfo.c                | 205 +++++++++++++++++++++++++++++++++++++++++++++++
 security/Kconfig         |  20 +++++
 11 files changed, 296 insertions(+), 5 deletions(-)
 create mode 100644 include/linux/xpfo.h
 create mode 100644 mm/xpfo.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index c580d8c33562..dc5604a710c6 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -165,6 +165,7 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_STACK_VALIDATION		if X86_64
 	select ARCH_USES_HIGH_VMA_FLAGS		if X86_INTEL_MEMORY_PROTECTION_KEYS
 	select ARCH_HAS_PKEYS			if X86_INTEL_MEMORY_PROTECTION_KEYS
<span class="p_add">+	select ARCH_SUPPORTS_XPFO		if X86_64</span>
 
 config INSTRUCTION_DECODER
 	def_bool y
<span class="p_chunk">@@ -1350,7 +1351,7 @@</span> <span class="p_context"> config ARCH_DMA_ADDR_T_64BIT</span>
 
 config X86_DIRECT_GBPAGES
 	def_bool y
<span class="p_del">-	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; !DEBUG_PAGEALLOC &amp;&amp; !KMEMCHECK &amp;&amp; !XPFO</span>
 	---help---
 	  Certain kernel features effectively disable kernel
 	  linear 1 GB mappings (even if the CPU otherwise
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index d28a2d741f9e..426427b54639 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -161,7 +161,7 @@</span> <span class="p_context"> static int page_size_mask;</span>
 
 static void __init probe_page_size_mask(void)
 {
<span class="p_del">-#if !defined(CONFIG_KMEMCHECK)</span>
<span class="p_add">+#if !defined(CONFIG_KMEMCHECK) &amp;&amp; !defined(CONFIG_XPFO)</span>
 	/*
 	 * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will
 	 * use small pages.
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index bb3f3297062a..7a17c166532f 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_chunk">@@ -55,24 +56,34 @@</span> <span class="p_context"> static inline struct page *kmap_to_page(void *addr)</span>
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 
 static inline void kunmap(struct page *page)
 {
<span class="p_add">+	xpfo_kunmap(page_address(page), page);</span>
 }
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	preempt_disable();
 	pagefault_disable();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
 
 static inline void __kunmap_atomic(void *addr)
 {
<span class="p_add">+	xpfo_kunmap(addr, virt_to_page(addr));</span>
 	pagefault_enable();
 	preempt_enable();
 }
<span class="p_header">diff --git a/include/linux/page_ext.h b/include/linux/page_ext.h</span>
<span class="p_header">index 03f2a3e7d76d..fdf63dcc399e 100644</span>
<span class="p_header">--- a/include/linux/page_ext.h</span>
<span class="p_header">+++ b/include/linux/page_ext.h</span>
<span class="p_chunk">@@ -27,6 +27,8 @@</span> <span class="p_context"> enum page_ext_flags {</span>
 	PAGE_EXT_DEBUG_POISON,		/* Page is poisoned */
 	PAGE_EXT_DEBUG_GUARD,
 	PAGE_EXT_OWNER,
<span class="p_add">+	PAGE_EXT_XPFO_KERNEL,		/* Page is a kernel page */</span>
<span class="p_add">+	PAGE_EXT_XPFO_UNMAPPED,		/* Page is unmapped */</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	PAGE_EXT_YOUNG,
 	PAGE_EXT_IDLE,
<span class="p_chunk">@@ -48,6 +50,11 @@</span> <span class="p_context"> struct page_ext {</span>
 	int last_migrate_reason;
 	depot_stack_handle_t handle;
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	int inited;		/* Map counter and lock initialized */</span>
<span class="p_add">+	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="p_add">+	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="p_add">+#endif</span>
 };
 
 extern void pgdat_page_ext_init(struct pglist_data *pgdat);
<span class="p_header">diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..77187578ca33</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/xpfo.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_XPFO_H</span>
<span class="p_add">+#define _LINUX_XPFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct page_ext_operations page_xpfo_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="p_add">+extern void xpfo_alloc_page(struct page *page, int order, gfp_t gfp);</span>
<span class="p_add">+extern void xpfo_free_page(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+extern bool xpfo_page_is_unmapped(struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_alloc_page(struct page *page, int order, gfp_t gfp) { }</span>
<span class="p_add">+static inline void xpfo_free_page(struct page *page, int order) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool xpfo_page_is_unmapped(struct page *page) { return false; }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _LINUX_XPFO_H */</span>
<span class="p_header">diff --git a/lib/swiotlb.c b/lib/swiotlb.c</span>
<span class="p_header">index 22e13a0e19d7..455eff44604e 100644</span>
<span class="p_header">--- a/lib/swiotlb.c</span>
<span class="p_header">+++ b/lib/swiotlb.c</span>
<span class="p_chunk">@@ -390,8 +390,9 @@</span> <span class="p_context"> static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,</span>
 {
 	unsigned long pfn = PFN_DOWN(orig_addr);
 	unsigned char *vaddr = phys_to_virt(tlb_addr);
<span class="p_add">+	struct page *page = pfn_to_page(pfn);</span>
 
<span class="p_del">-	if (PageHighMem(pfn_to_page(pfn))) {</span>
<span class="p_add">+	if (PageHighMem(page) || xpfo_page_is_unmapped(page)) {</span>
 		/* The buffer does not have a mapping.  Map it in and copy */
 		unsigned int offset = orig_addr &amp; ~PAGE_MASK;
 		char *buffer;
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 2ca1faf3fa09..e6f8894423da 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -103,3 +103,4 @@</span> <span class="p_context"> obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o</span>
 obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
<span class="p_add">+obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 3fbe73a6fe4b..0241c8a7e72a 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1029,6 +1029,7 @@</span> <span class="p_context"> static __always_inline bool free_pages_prepare(struct page *page,</span>
 	kernel_poison_pages(page, 1 &lt;&lt; order, 0);
 	kernel_map_pages(page, 1 &lt;&lt; order, 0);
 	kasan_free_pages(page, order);
<span class="p_add">+	xpfo_free_page(page, order);</span>
 
 	return true;
 }
<span class="p_chunk">@@ -1726,6 +1727,7 @@</span> <span class="p_context"> inline void post_alloc_hook(struct page *page, unsigned int order,</span>
 	kernel_map_pages(page, 1 &lt;&lt; order, 1);
 	kernel_poison_pages(page, 1 &lt;&lt; order, 1);
 	kasan_alloc_pages(page, order);
<span class="p_add">+	xpfo_alloc_page(page, order, gfp_flags);</span>
 	set_page_owner(page, order, gfp_flags);
 }
 
<span class="p_header">diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="p_header">index 44a4c029c8e7..1cd7d7f460cc 100644</span>
<span class="p_header">--- a/mm/page_ext.c</span>
<span class="p_header">+++ b/mm/page_ext.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kmemleak.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/page_idle.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 /*
  * struct page extension
<span class="p_chunk">@@ -63,6 +64,9 @@</span> <span class="p_context"> static struct page_ext_operations *page_ext_ops[] = {</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	&amp;page_idle_ops,
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	&amp;page_xpfo_ops,</span>
<span class="p_add">+#endif</span>
 };
 
 static unsigned long total_usage;
<span class="p_header">diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..ddb1be05485d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,205 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/page_ext.h&gt;</span>
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool need_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="p_add">+	static_branch_enable(&amp;xpfo_inited);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page_ext_operations page_xpfo_ops = {</span>
<span class="p_add">+	.need = need_xpfo,</span>
<span class="p_add">+	.init = init_xpfo,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Update a single kernel page table entry</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_kpte(struct page *page, unsigned long kaddr,</span>
<span class="p_add">+			    pgprot_t prot) {</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+	pte_t *kpte = lookup_address(kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We only support 4k pages for now */</span>
<span class="p_add">+	BUG_ON(!kpte || level != PG_LEVEL_4K);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pte_atomic(kpte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_alloc_page(struct page *page, int order, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, flush_tlb = 0;</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="p_add">+		page_ext = lookup_page_ext(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Initialize the map lock and map counter */</span>
<span class="p_add">+		if (!page_ext-&gt;inited) {</span>
<span class="p_add">+			spin_lock_init(&amp;page_ext-&gt;maplock);</span>
<span class="p_add">+			atomic_set(&amp;page_ext-&gt;mapcount, 0);</span>
<span class="p_add">+			page_ext-&gt;inited = 1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		BUG_ON(atomic_read(&amp;page_ext-&gt;mapcount));</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Flush the TLB if the page was previously allocated</span>
<span class="p_add">+			 * to the kernel.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (test_and_clear_bit(PAGE_EXT_XPFO_KERNEL,</span>
<span class="p_add">+					       &amp;page_ext-&gt;flags))</span>
<span class="p_add">+				flush_tlb = 1;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Tag the page as a kernel page */</span>
<span class="p_add">+			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flush_tlb) {</span>
<span class="p_add">+		kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) *</span>
<span class="p_add">+				       PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_free_page(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="p_add">+		page_ext = lookup_page_ext(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page_ext-&gt;inited) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * The page was allocated before page_ext was</span>
<span class="p_add">+			 * initialized, so it is a kernel page and it needs to</span>
<span class="p_add">+			 * be tagged accordingly.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map the page back into the kernel if it was previously</span>
<span class="p_add">+		 * allocated to user space.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED,</span>
<span class="p_add">+				       &amp;page_ext-&gt;flags)) {</span>
<span class="p_add">+			kaddr = (unsigned long)page_address(page + i);</span>
<span class="p_add">+			set_kpte(page + i,  kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	page_ext = lookup_page_ext(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!page_ext-&gt;inited ||</span>
<span class="p_add">+	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was previously allocated to user space, so map it back</span>
<span class="p_add">+	 * into the kernel. No TLB flush required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_inc_return(&amp;page_ext-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="p_add">+	    test_and_clear_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(__PAGE_KERNEL));</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_ext *page_ext;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	page_ext = lookup_page_ext(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!page_ext-&gt;inited ||</span>
<span class="p_add">+	    test_bit(PAGE_EXT_XPFO_KERNEL, &amp;page_ext-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="p_add">+	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (atomic_dec_return(&amp;page_ext-&gt;mapcount) == 0) {</span>
<span class="p_add">+		BUG_ON(test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags));</span>
<span class="p_add">+		set_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;page_ext-&gt;flags);</span>
<span class="p_add">+		set_kpte(page, (unsigned long)kaddr, __pgprot(0));</span>
<span class="p_add">+		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;page_ext-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="p_add">+</span>
<span class="p_add">+inline bool xpfo_page_is_unmapped(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return test_bit(PAGE_EXT_XPFO_UNMAPPED, &amp;lookup_page_ext(page)-&gt;flags);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index da10d9b573a4..1eac37a9bec2 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -6,6 +6,26 @@</span> <span class="p_context"> menu &quot;Security options&quot;</span>
 
 source security/keys/Kconfig
 
<span class="p_add">+config ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+config XPFO</span>
<span class="p_add">+	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	depends on DEBUG_KERNEL &amp;&amp; ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	select DEBUG_TLBFLUSH</span>
<span class="p_add">+	select PAGE_EXTENSION</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="p_add">+	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="p_add">+	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="p_add">+	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="p_add">+	  mapped back to physmap.</span>
<span class="p_add">+</span>
<span class="p_add">+	  There is a slight performance impact when this option is enabled.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say &quot;N&quot;.</span>
<span class="p_add">+</span>
 config SECURITY_DMESG_RESTRICT
 	bool &quot;Restrict unprivileged access to the kernel syslog&quot;
 	default n

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



