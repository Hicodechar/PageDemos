
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,3/6] iommu: add ARM short descriptor page table allocator - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,3/6] iommu: add ARM short descriptor page table allocator</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=123111">Yong Wu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 9, 2015, 2:23 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1444357388-30257-4-git-send-email-yong.wu@mediatek.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7358821/mbox/"
   >mbox</a>
|
   <a href="/patch/7358821/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7358821/">/patch/7358821/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 733EDBEEA4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  9 Oct 2015 02:24:31 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6B94B20819
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  9 Oct 2015 02:24:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id E46F92081E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  9 Oct 2015 02:24:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934334AbbJICYW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 8 Oct 2015 22:24:22 -0400
Received: from mailgw02.mediatek.com ([210.61.82.184]:47156 &quot;EHLO
	mailgw02.mediatek.com&quot; rhost-flags-OK-FAIL-OK-FAIL) by
	vger.kernel.org with ESMTP id S1756401AbbJICYO (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 8 Oct 2015 22:24:14 -0400
X-Listener-Flag: 11101
Received: from mtkhts09.mediatek.inc [(172.21.101.70)] by
	mailgw02.mediatek.com (envelope-from &lt;yong.wu@mediatek.com&gt;)
	(mhqrelay.mediatek.com ESMTP with TLS)
	with ESMTP id 535015488; Fri, 09 Oct 2015 10:24:12 +0800
Received: from localhost.localdomain (10.17.3.153) by mtkhts09.mediatek.inc
	(172.21.101.73) with Microsoft SMTP Server id 14.3.181.6;
	Fri, 9 Oct 2015 10:24:10 +0800
From: Yong Wu &lt;yong.wu@mediatek.com&gt;
To: Joerg Roedel &lt;joro@8bytes.org&gt;, Thierry Reding &lt;treding@nvidia.com&gt;,
	Mark Rutland &lt;mark.rutland@arm.com&gt;,
	Matthias Brugger &lt;matthias.bgg@gmail.com&gt;
CC: Robin Murphy &lt;robin.murphy@arm.com&gt;, Will Deacon &lt;will.deacon@arm.com&gt;,
	Daniel Kurtz &lt;djkurtz@google.com&gt;, Tomasz Figa &lt;tfiga@google.com&gt;,
	Lucas Stach &lt;l.stach@pengutronix.de&gt;, Rob Herring &lt;robh+dt@kernel.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	&lt;linux-mediatek@lists.infradead.org&gt;,
	Sasha Hauer &lt;kernel@pengutronix.de&gt;,
	&lt;srv_heupstream@mediatek.com&gt;, &lt;devicetree@vger.kernel.org&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, &lt;linux-arm-kernel@lists.infradead.org&gt;,
	&lt;iommu@lists.linux-foundation.org&gt;, &lt;pebolle@tiscali.nl&gt;,
	&lt;arnd@arndb.de&gt;, &lt;mitchelh@codeaurora.org&gt;,
	Sricharan R &lt;sricharan@codeaurora.org&gt;,
	&lt;youhua.li@mediatek.com&gt;, &lt;k.zhang@mediatek.com&gt;,
	&lt;kendrick.hsu@mediatek.com&gt;, Yong Wu &lt;yong.wu@mediatek.com&gt;
Subject: [PATCH v5 3/6] iommu: add ARM short descriptor page table allocator
Date: Fri, 9 Oct 2015 10:23:05 +0800
Message-ID: &lt;1444357388-30257-4-git-send-email-yong.wu@mediatek.com&gt;
X-Mailer: git-send-email 1.8.1.1.dirty
In-Reply-To: &lt;1444357388-30257-1-git-send-email-yong.wu@mediatek.com&gt;
References: &lt;1444357388-30257-1-git-send-email-yong.wu@mediatek.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
X-MTK: N
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - Oct. 9, 2015, 2:23 a.m.</div>
<pre class="content">
This patch is for ARM Short Descriptor Format.
<span class="signed-off-by">
Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
---
 drivers/iommu/Kconfig                |  18 +
 drivers/iommu/Makefile               |   1 +
 drivers/iommu/io-pgtable-arm-short.c | 827 +++++++++++++++++++++++++++++++++++
 drivers/iommu/io-pgtable-arm.c       |   3 -
 drivers/iommu/io-pgtable.c           |   3 +
 drivers/iommu/io-pgtable.h           |  18 +-
 6 files changed, 866 insertions(+), 4 deletions(-)
 create mode 100644 drivers/iommu/io-pgtable-arm-short.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a> - Oct. 14, 2015, 12:54 p.m.</div>
<pre class="content">
On Fri, Oct 09, 2015 at 10:23:05AM +0800, Yong Wu wrote:
<span class="quote">&gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>

I think it would be good if Will Deacon could have a look on that.

Will, any comments on this patch?



	Joerg


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Oct. 15, 2015, 5:20 p.m.</div>
<pre class="content">
On Wed, Oct 14, 2015 at 02:54:19PM +0200, Joerg Roedel wrote:
<span class="quote">&gt; On Fri, Oct 09, 2015 at 10:23:05AM +0800, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it would be good if Will Deacon could have a look on that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will, any comments on this patch?</span>

Unfortunately, I haven&#39;t found time to take a good look at this version
(this patch usually takes the best part of a day to review properly)
and I&#39;m away on holiday all next week.

I can review it when I&#39;m back and have got back on top of email, but
that probably doesn&#39;t work well with 4.4, unfortunately. Robin and/or
Catalin may be able to review it in my absence, but it&#39;s hard work...

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123111">Yong Wu</a> - Nov. 6, 2015, 8:42 a.m.</div>
<pre class="content">
On Fri, 2015-10-09 at 10:23 +0800, Yong Wu wrote:
<span class="quote">&gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="quote">&gt; ---</span>

Hi Will, Robin,
   Is there any comment about this patch?
   As our project request, We are going to prepare the next version.
Thanks very much.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Nov. 6, 2015, 11:42 a.m.</div>
<pre class="content">
On Fri, Nov 06, 2015 at 04:42:52PM +0800, Yong Wu wrote:
<span class="quote">&gt; On Fri, 2015-10-09 at 10:23 +0800, Yong Wu wrote:</span>
<span class="quote">&gt; &gt; This patch is for ARM Short Descriptor Format.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Will, Robin,</span>
<span class="quote">&gt;    Is there any comment about this patch?</span>
<span class="quote">&gt;    As our project request, We are going to prepare the next version.</span>

Robin has been looking at the code after he ran into problems using it
in conjunction with arm-smmu. I&#39;d expect him to post something after the
merge window, so you may as well hold-off until then.

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig</span>
<span class="p_header">index 4664c2a..a7920fb 100644</span>
<span class="p_header">--- a/drivers/iommu/Kconfig</span>
<span class="p_header">+++ b/drivers/iommu/Kconfig</span>
<span class="p_chunk">@@ -40,6 +40,24 @@</span> <span class="p_context"> config IOMMU_IO_PGTABLE_LPAE_SELFTEST</span>
 
 	  If unsure, say N here.
 
<span class="p_add">+config IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	bool &quot;ARMv7/v8 Short Descriptor Format&quot;</span>
<span class="p_add">+	select IOMMU_IO_PGTABLE</span>
<span class="p_add">+	depends on HAS_DMA &amp;&amp; (ARM || ARM64 || COMPILE_TEST)</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable support for the ARM Short-descriptor pagetable format.</span>
<span class="p_add">+	  This allocator supports 2 levels of translation tables, which</span>
<span class="p_add">+	  enables a 32-bit memory map based on memory sections or pages.</span>
<span class="p_add">+</span>
<span class="p_add">+config IOMMU_IO_PGTABLE_SHORT_SELFTEST</span>
<span class="p_add">+	bool &quot;Short Descriptor selftests&quot;</span>
<span class="p_add">+	depends on IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable self-tests for Short-descriptor page table allocator.</span>
<span class="p_add">+	  This performs a series of page-table consistency checks during boot.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N here.</span>
<span class="p_add">+</span>
 endmenu
 
 config IOMMU_IOVA
<span class="p_header">diff --git a/drivers/iommu/Makefile b/drivers/iommu/Makefile</span>
<span class="p_header">index c6dcc51..06df3e6 100644</span>
<span class="p_header">--- a/drivers/iommu/Makefile</span>
<span class="p_header">+++ b/drivers/iommu/Makefile</span>
<span class="p_chunk">@@ -3,6 +3,7 @@</span> <span class="p_context"> obj-$(CONFIG_IOMMU_API) += iommu-traces.o</span>
 obj-$(CONFIG_IOMMU_API) += iommu-sysfs.o
 obj-$(CONFIG_IOMMU_IO_PGTABLE) += io-pgtable.o
 obj-$(CONFIG_IOMMU_IO_PGTABLE_LPAE) += io-pgtable-arm.o
<span class="p_add">+obj-$(CONFIG_IOMMU_IO_PGTABLE_SHORT) += io-pgtable-arm-short.o</span>
 obj-$(CONFIG_IOMMU_IOVA) += iova.o
 obj-$(CONFIG_OF_IOMMU)	+= of_iommu.o
 obj-$(CONFIG_MSM_IOMMU) += msm_iommu.o msm_iommu_dev.o
<span class="p_header">diff --git a/drivers/iommu/io-pgtable-arm-short.c b/drivers/iommu/io-pgtable-arm-short.c</span>
new file mode 100644
<span class="p_header">index 0000000..6337c61</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable-arm-short.c</span>
<span class="p_chunk">@@ -0,0 +1,827 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (c) 2014-2015 MediaTek Inc.</span>
<span class="p_add">+ * Author: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/err.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &quot;io-pgtable.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef u32 arm_short_iopte;</span>
<span class="p_add">+</span>
<span class="p_add">+struct arm_short_io_pgtable {</span>
<span class="p_add">+	struct io_pgtable	iop;</span>
<span class="p_add">+	struct kmem_cache	*pgtable_cached;</span>
<span class="p_add">+	size_t			pgd_size;</span>
<span class="p_add">+	void			*pgd;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_to_data(x)			\</span>
<span class="p_add">+	container_of((x), struct arm_short_io_pgtable, iop)</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_ops_to_data(x)		\</span>
<span class="p_add">+	io_pgtable_to_data(io_pgtable_ops_to_pgtable(x))</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_cfg_to_pgtable(x)		\</span>
<span class="p_add">+	container_of((x), struct io_pgtable, cfg)</span>
<span class="p_add">+</span>
<span class="p_add">+#define io_pgtable_cfg_to_data(x)		\</span>
<span class="p_add">+	io_pgtable_to_data(io_pgtable_cfg_to_pgtable(x))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGDIR_SHIFT			20</span>
<span class="p_add">+#define ARM_SHORT_PAGE_SHIFT			12</span>
<span class="p_add">+#define ARM_SHORT_PTRS_PER_PTE			\</span>
<span class="p_add">+	(1 &lt;&lt; (ARM_SHORT_PGDIR_SHIFT - ARM_SHORT_PAGE_SHIFT))</span>
<span class="p_add">+#define ARM_SHORT_BYTES_PER_PTE			\</span>
<span class="p_add">+	(ARM_SHORT_PTRS_PER_PTE * sizeof(arm_short_iopte))</span>
<span class="p_add">+</span>
<span class="p_add">+/* level 1 pagetable */</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_PGTABLE		BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_SECTION		BIT(1)</span>
<span class="p_add">+#define ARM_SHORT_PGD_B				BIT(2)</span>
<span class="p_add">+#define ARM_SHORT_PGD_C				BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_NS		BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_XN		BIT(4)</span>
<span class="p_add">+#define ARM_SHORT_PGD_IMPLE			BIT(9)</span>
<span class="p_add">+#define ARM_SHORT_PGD_RD_WR			(3 &lt;&lt; 10)</span>
<span class="p_add">+#define ARM_SHORT_PGD_RDONLY			BIT(15)</span>
<span class="p_add">+#define ARM_SHORT_PGD_S				BIT(16)</span>
<span class="p_add">+#define ARM_SHORT_PGD_nG			BIT(17)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SUPERSECTION		BIT(18)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_NS		BIT(19)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_SUPERSECTION		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_TYPE_MSK		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_TYPE_MSK		\</span>
<span class="p_add">+	(ARM_SHORT_PGD_TYPE_SECTION | ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_PGTABLE(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_PGTABLE_TYPE_MSK) == ARM_SHORT_PGD_TYPE_PGTABLE)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_SECTION(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == ARM_SHORT_PGD_TYPE_SECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(pgd)	\</span>
<span class="p_add">+	(((pgd) &amp; ARM_SHORT_PGD_SECTION_TYPE_MSK) == \</span>
<span class="p_add">+	ARM_SHORT_PGD_TYPE_SUPERSECTION)</span>
<span class="p_add">+#define ARM_SHORT_PGD_PGTABLE_MSK		(~(ARM_SHORT_BYTES_PER_PTE - 1))</span>
<span class="p_add">+#define ARM_SHORT_PGD_SECTION_MSK		(~(SZ_1M - 1))</span>
<span class="p_add">+#define ARM_SHORT_PGD_SUPERSECTION_MSK		(~(SZ_16M - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* level 2 pagetable */</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_LARGE		BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PTE_SMALL_XN			BIT(0)</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_SMALL		BIT(1)</span>
<span class="p_add">+#define ARM_SHORT_PTE_B				BIT(2)</span>
<span class="p_add">+#define ARM_SHORT_PTE_C				BIT(3)</span>
<span class="p_add">+#define ARM_SHORT_PTE_RD_WR			(3 &lt;&lt; 4)</span>
<span class="p_add">+#define ARM_SHORT_PTE_RDONLY			BIT(9)</span>
<span class="p_add">+#define ARM_SHORT_PTE_S				BIT(10)</span>
<span class="p_add">+#define ARM_SHORT_PTE_nG			BIT(11)</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_XN			BIT(15)</span>
<span class="p_add">+#define ARM_SHORT_PTE_LARGE_MSK			(~(SZ_64K - 1))</span>
<span class="p_add">+#define ARM_SHORT_PTE_SMALL_MSK			(~(SZ_4K - 1))</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_MSK			\</span>
<span class="p_add">+	(ARM_SHORT_PTE_TYPE_LARGE | ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="p_add">+/* Bit[0] in small page is the XN bit */</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(pte)	\</span>
<span class="p_add">+	(((pte) &amp; ARM_SHORT_PTE_TYPE_SMALL) == ARM_SHORT_PTE_TYPE_SMALL)</span>
<span class="p_add">+#define ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(pte)	\</span>
<span class="p_add">+	(((pte) &amp; ARM_SHORT_PTE_TYPE_MSK) == ARM_SHORT_PTE_TYPE_LARGE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_IDX(a)			((a) &gt;&gt; ARM_SHORT_PGDIR_SHIFT)</span>
<span class="p_add">+#define ARM_SHORT_PTE_IDX(a)			\</span>
<span class="p_add">+	(((a) &gt;&gt; ARM_SHORT_PAGE_SHIFT) &amp; (ARM_SHORT_PTRS_PER_PTE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_GET_PGTABLE_VA(pgd)		\</span>
<span class="p_add">+	(phys_to_virt((pgd) &amp; ARM_SHORT_PGD_PGTABLE_MSK))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Get the prot of large page for split */</span>
<span class="p_add">+#define ARM_SHORT_PTE_GET_PROT_LARGE(pte)	\</span>
<span class="p_add">+	(((pte) &amp; (~ARM_SHORT_PTE_LARGE_MSK)) &amp; ~ARM_SHORT_PTE_TYPE_MSK)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARM_SHORT_PGD_GET_PROT(pgd)		\</span>
<span class="p_add">+	(((pgd) &amp; (~ARM_SHORT_PGD_SECTION_MSK)) &amp; ~ARM_SHORT_PGD_SUPERSECTION)</span>
<span class="p_add">+</span>
<span class="p_add">+static bool selftest_running;</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte *</span>
<span class="p_add">+arm_short_get_pte_in_pgd(arm_short_iopte pgd, unsigned int iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = ARM_SHORT_GET_PGTABLE_VA(pgd);</span>
<span class="p_add">+	pte += ARM_SHORT_PTE_IDX(iova);</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static dma_addr_t __arm_short_dma_addr(void *va)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (dma_addr_t)virt_to_phys(va);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+__arm_short_set_pte(arm_short_iopte *ptep, arm_short_iopte pte,</span>
<span class="p_add">+		    unsigned int ptenr, struct io_pgtable_cfg *cfg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; ptenr; i++) {</span>
<span class="p_add">+		if (ptep[i] &amp;&amp; pte) {</span>
<span class="p_add">+			/* Someone else may have allocated for this pte */</span>
<span class="p_add">+			WARN_ON(!selftest_running);</span>
<span class="p_add">+			goto err_exist_pte;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		ptep[i] = pte;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (selftest_running)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	dma_sync_single_for_device(cfg-&gt;iommu_dev, __arm_short_dma_addr(ptep),</span>
<span class="p_add">+				   sizeof(*ptep) * ptenr, DMA_TO_DEVICE);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+err_exist_pte:</span>
<span class="p_add">+	while (i--) {</span>
<span class="p_add">+		ptep[i] = 0;</span>
<span class="p_add">+		if (!selftest_running)</span>
<span class="p_add">+			dma_sync_single_for_device(</span>
<span class="p_add">+				cfg-&gt;iommu_dev, __arm_short_dma_addr(ptep + i),</span>
<span class="p_add">+				sizeof(*ptep), DMA_TO_DEVICE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return -EEXIST;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *</span>
<span class="p_add">+__arm_short_alloc_pgtable(size_t size, gfp_t gfp, bool pgd,</span>
<span class="p_add">+			  struct io_pgtable_cfg *cfg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data;</span>
<span class="p_add">+	struct device *dev = cfg-&gt;iommu_dev;</span>
<span class="p_add">+	dma_addr_t dma;</span>
<span class="p_add">+	void *va;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd) {/* lvl1 pagetable */</span>
<span class="p_add">+		va = alloc_pages_exact(size, gfp);</span>
<span class="p_add">+	} else {  /* lvl2 pagetable */</span>
<span class="p_add">+		data = io_pgtable_cfg_to_data(cfg);</span>
<span class="p_add">+		va = kmem_cache_zalloc(data-&gt;pgtable_cached, gfp);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!va)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (selftest_running)</span>
<span class="p_add">+		return va;</span>
<span class="p_add">+</span>
<span class="p_add">+	dma = dma_map_single(dev, va, size, DMA_TO_DEVICE);</span>
<span class="p_add">+	if (dma_mapping_error(dev, dma))</span>
<span class="p_add">+		goto out_free;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dma != virt_to_phys(va))</span>
<span class="p_add">+		goto out_unmap;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pgd)</span>
<span class="p_add">+		kmemleak_ignore(va);</span>
<span class="p_add">+</span>
<span class="p_add">+	return va;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unmap:</span>
<span class="p_add">+	dev_err(dev, &quot;Cannot accommodate DMA translation for IOMMU page tables\n&quot;);</span>
<span class="p_add">+	dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	if (pgd)</span>
<span class="p_add">+		free_pages_exact(va, size);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		kmem_cache_free(data-&gt;pgtable_cached, va);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+__arm_short_free_pgtable(void *va, size_t size, bool pgd,</span>
<span class="p_add">+			 struct io_pgtable_cfg *cfg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!selftest_running)</span>
<span class="p_add">+		dma_unmap_single(cfg-&gt;iommu_dev, __arm_short_dma_addr(va),</span>
<span class="p_add">+				 size, DMA_TO_DEVICE);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd) {</span>
<span class="p_add">+		free_pages_exact(va, size);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		data = io_pgtable_cfg_to_data(cfg);</span>
<span class="p_add">+		kmem_cache_free(data-&gt;pgtable_cached, va);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pte_prot(struct arm_short_io_pgtable *data, int prot, bool large)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pteprot;</span>
<span class="p_add">+	int quirk = data-&gt;iop.cfg.quirks;</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="p_add">+				ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="p_add">+	if (prot &amp; IOMMU_CACHE)</span>
<span class="p_add">+		pteprot |=  ARM_SHORT_PTE_B | ARM_SHORT_PTE_C;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(quirk &amp; IO_PGTABLE_QUIRK_NO_PERMS)) {</span>
<span class="p_add">+		if (prot &amp; IOMMU_NOEXEC) {</span>
<span class="p_add">+			pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="p_add">+				ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_RD_WR;</span>
<span class="p_add">+		if (!(prot &amp; IOMMU_WRITE) &amp;&amp; (prot &amp; IOMMU_READ))</span>
<span class="p_add">+			pteprot |= ARM_SHORT_PTE_RDONLY;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pteprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pgd_prot(struct arm_short_io_pgtable *data, int prot, bool super)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pgdprot;</span>
<span class="p_add">+	int quirk = data-&gt;iop.cfg.quirks;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdprot = ARM_SHORT_PGD_S | ARM_SHORT_PGD_nG;</span>
<span class="p_add">+	pgdprot |= super ? ARM_SHORT_PGD_TYPE_SUPERSECTION :</span>
<span class="p_add">+				ARM_SHORT_PGD_TYPE_SECTION;</span>
<span class="p_add">+	if (prot &amp; IOMMU_CACHE)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_C | ARM_SHORT_PGD_B;</span>
<span class="p_add">+	if (quirk &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_SECTION_NS;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(quirk &amp; IO_PGTABLE_QUIRK_NO_PERMS)) {</span>
<span class="p_add">+		if (prot &amp; IOMMU_NOEXEC)</span>
<span class="p_add">+			pgdprot |= ARM_SHORT_PGD_SECTION_XN;</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_RD_WR;</span>
<span class="p_add">+		if (!(prot &amp; IOMMU_WRITE) &amp;&amp; (prot &amp; IOMMU_READ))</span>
<span class="p_add">+			pgdprot |= ARM_SHORT_PGD_RDONLY;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pgdprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pte_prot_split(struct arm_short_io_pgtable *data,</span>
<span class="p_add">+			   arm_short_iopte pgdprot,</span>
<span class="p_add">+			   arm_short_iopte pteprot_large,</span>
<span class="p_add">+			   bool large)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pteprot = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot = ARM_SHORT_PTE_S | ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	pteprot |= large ? ARM_SHORT_PTE_TYPE_LARGE :</span>
<span class="p_add">+				ARM_SHORT_PTE_TYPE_SMALL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Get the pte prot while large page split to small page */</span>
<span class="p_add">+	if (!pgdprot &amp;&amp; !large) {</span>
<span class="p_add">+		pteprot |= pteprot_large &amp; ~ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="p_add">+		if (pteprot_large &amp; ARM_SHORT_PTE_LARGE_XN)</span>
<span class="p_add">+			pteprot |= ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Section to pte prot */</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_C)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_C;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_B)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_B;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_nG)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_nG;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_SECTION_XN)</span>
<span class="p_add">+		pteprot |= large ? ARM_SHORT_PTE_LARGE_XN :</span>
<span class="p_add">+				ARM_SHORT_PTE_SMALL_XN;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_RD_WR)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_RD_WR;</span>
<span class="p_add">+	if (pgdprot &amp; ARM_SHORT_PGD_RDONLY)</span>
<span class="p_add">+		pteprot |= ARM_SHORT_PTE_RDONLY;</span>
<span class="p_add">+</span>
<span class="p_add">+	return pteprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static arm_short_iopte</span>
<span class="p_add">+__arm_short_pgtable_prot(struct arm_short_io_pgtable *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte pgdprot = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgdprot = ARM_SHORT_PGD_TYPE_PGTABLE;</span>
<span class="p_add">+	if (data-&gt;iop.cfg.quirks &amp; IO_PGTABLE_QUIRK_ARM_NS)</span>
<span class="p_add">+		pgdprot |= ARM_SHORT_PGD_PGTABLE_NS;</span>
<span class="p_add">+	return pgdprot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+_arm_short_map(struct arm_short_io_pgtable *data,</span>
<span class="p_add">+	       unsigned int iova, size_t size, phys_addr_t paddr,</span>
<span class="p_add">+	       arm_short_iopte pgdprot, arm_short_iopte pteprot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="p_add">+	const struct iommu_gather_ops *tlb = cfg-&gt;tlb;</span>
<span class="p_add">+	arm_short_iopte *pgd = data-&gt;pgd, *pte;</span>
<span class="p_add">+	void *cookie = data-&gt;iop.cookie, *pgtable_new = NULL;</span>
<span class="p_add">+	unsigned int pte_nr;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pteprot) { /* section or supersection */</span>
<span class="p_add">+		pte = pgd;</span>
<span class="p_add">+		pteprot = pgdprot;</span>
<span class="p_add">+		pte_nr = (size == SZ_1M) ? 1 : 16;</span>
<span class="p_add">+	} else {        /* page or largepage */</span>
<span class="p_add">+		if (!(*pgd)) {</span>
<span class="p_add">+			pgtable_new = __arm_short_alloc_pgtable(</span>
<span class="p_add">+					ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+					GFP_ATOMIC, false, cfg);</span>
<span class="p_add">+			if (unlikely(!pgtable_new))</span>
<span class="p_add">+				return -ENOMEM;</span>
<span class="p_add">+			pgdprot |= virt_to_phys(pgtable_new);</span>
<span class="p_add">+			__arm_short_set_pte(pgd, pgdprot, 1, cfg);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+		pte_nr = (size == SZ_4K) ? 1 : 16;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pteprot |= (arm_short_iopte)paddr;</span>
<span class="p_add">+	ret = __arm_short_set_pte(pte, pteprot, pte_nr, cfg);</span>
<span class="p_add">+	if (ret &amp;&amp; pgtable_new)</span>
<span class="p_add">+		goto err_unmap_pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cfg-&gt;quirks &amp; IO_PGTABLE_QUIRK_TLBI_ON_MAP) {</span>
<span class="p_add">+		tlb-&gt;tlb_add_flush(iova, size, true, cookie);</span>
<span class="p_add">+		tlb-&gt;tlb_sync(cookie);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+err_unmap_pgd:</span>
<span class="p_add">+	__arm_short_set_pte(pgd, 0, 1, cfg);</span>
<span class="p_add">+	tlb-&gt;tlb_add_flush(iova, SZ_1M, false, cookie);/* Flush whole the pgd */</span>
<span class="p_add">+	tlb-&gt;tlb_sync(cookie);</span>
<span class="p_add">+	__arm_short_free_pgtable(pgtable_new, ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+				 false, cfg);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int arm_short_map(struct io_pgtable_ops *ops, unsigned long iova,</span>
<span class="p_add">+			 phys_addr_t paddr, size_t size, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	arm_short_iopte pgdprot = 0, pteprot = 0;</span>
<span class="p_add">+	bool large;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If no access, then nothing to do */</span>
<span class="p_add">+	if (!(prot &amp; (IOMMU_READ | IOMMU_WRITE)))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON((iova | paddr) &amp; (size - 1)))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (size) {</span>
<span class="p_add">+	case SZ_4K:</span>
<span class="p_add">+	case SZ_64K:</span>
<span class="p_add">+		large = (size == SZ_64K) ? true : false;</span>
<span class="p_add">+		pteprot = __arm_short_pte_prot(data, prot, large);</span>
<span class="p_add">+		pgdprot = __arm_short_pgtable_prot(data);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+</span>
<span class="p_add">+	case SZ_1M:</span>
<span class="p_add">+	case SZ_16M:</span>
<span class="p_add">+		large = (size == SZ_16M) ? true : false;</span>
<span class="p_add">+		pgdprot = __arm_short_pgd_prot(data, prot, large);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return _arm_short_map(data, iova, size, paddr, pgdprot, pteprot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static phys_addr_t arm_short_iova_to_phys(struct io_pgtable_ops *ops,</span>
<span class="p_add">+					  unsigned long iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	arm_short_iopte *pte, *pgd = data-&gt;pgd;</span>
<span class="p_add">+	phys_addr_t pa = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd += ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="p_add">+		pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte)) {</span>
<span class="p_add">+			pa = (*pte) &amp; ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="p_add">+			pa |= iova &amp; ~ARM_SHORT_PTE_LARGE_MSK;</span>
<span class="p_add">+		} else if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte)) {</span>
<span class="p_add">+			pa = (*pte) &amp; ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="p_add">+			pa |= iova &amp; ~ARM_SHORT_PTE_SMALL_MSK;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="p_add">+		pa = (*pgd) &amp; ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="p_add">+		pa |= iova &amp; ~ARM_SHORT_PGD_SECTION_MSK;</span>
<span class="p_add">+	} else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="p_add">+		pa = (*pgd) &amp; ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="p_add">+		pa |= iova &amp; ~ARM_SHORT_PGD_SUPERSECTION_MSK;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pa;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool __arm_short_pgtable_empty(arm_short_iopte *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arm_short_iopte *pte;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = ARM_SHORT_GET_PGTABLE_VA(*pgd);</span>
<span class="p_add">+	for (i = 0; i &lt; ARM_SHORT_PTRS_PER_PTE; i++) {</span>
<span class="p_add">+		if (pte[i])</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+arm_short_split_blk_unmap(struct io_pgtable_ops *ops, unsigned int iova,</span>
<span class="p_add">+			  size_t size, size_t blk_size,</span>
<span class="p_add">+			  arm_short_iopte pgdprotup, arm_short_iopte pteprotup)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="p_add">+	unsigned long *pgbitmap = &amp;cfg-&gt;pgsize_bitmap;</span>
<span class="p_add">+	unsigned int blk_base, blk_start, blk_end, i;</span>
<span class="p_add">+	arm_short_iopte pgdprot, pteprot;</span>
<span class="p_add">+	phys_addr_t blk_paddr;</span>
<span class="p_add">+	size_t mapsize = 0, nextmapsize;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Find the nearest mapsize */</span>
<span class="p_add">+	for (i = find_first_bit(pgbitmap, BITS_PER_LONG);</span>
<span class="p_add">+	     i &lt; BITS_PER_LONG &amp;&amp; ((1 &lt;&lt; i) &lt; blk_size) &amp;&amp;</span>
<span class="p_add">+	     IS_ALIGNED(size, 1 &lt;&lt; i);</span>
<span class="p_add">+	     i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1))</span>
<span class="p_add">+		mapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON(!mapsize))</span>
<span class="p_add">+		return 0; /* Bytes unmapped */</span>
<span class="p_add">+	nextmapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+	blk_base = iova &amp; ~(blk_size - 1);</span>
<span class="p_add">+	blk_start = blk_base;</span>
<span class="p_add">+	blk_end = blk_start + blk_size;</span>
<span class="p_add">+	blk_paddr = arm_short_iova_to_phys(ops, blk_base);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; blk_start &lt; blk_end;</span>
<span class="p_add">+	     blk_start += mapsize, blk_paddr += mapsize) {</span>
<span class="p_add">+		/* Unmap! */</span>
<span class="p_add">+		if (blk_start == iova)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Try to upper map */</span>
<span class="p_add">+		if (blk_base != blk_start &amp;&amp;</span>
<span class="p_add">+		    IS_ALIGNED(blk_start | blk_paddr, nextmapsize) &amp;&amp;</span>
<span class="p_add">+		    mapsize != nextmapsize) {</span>
<span class="p_add">+			mapsize = nextmapsize;</span>
<span class="p_add">+			i = find_next_bit(pgbitmap, BITS_PER_LONG, i + 1);</span>
<span class="p_add">+			if (i &lt; BITS_PER_LONG)</span>
<span class="p_add">+				nextmapsize = 1 &lt;&lt; i;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (mapsize == SZ_1M) {</span>
<span class="p_add">+			pgdprot = pgdprotup;</span>
<span class="p_add">+			pgdprot |= __arm_short_pgd_prot(data, 0, false);</span>
<span class="p_add">+			pteprot = 0;</span>
<span class="p_add">+		} else { /* small or large page */</span>
<span class="p_add">+			pgdprot = (blk_size == SZ_64K) ? 0 : pgdprotup;</span>
<span class="p_add">+			pteprot = __arm_short_pte_prot_split(</span>
<span class="p_add">+					data, pgdprot, pteprotup,</span>
<span class="p_add">+					mapsize == SZ_64K);</span>
<span class="p_add">+			pgdprot = __arm_short_pgtable_prot(data);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = _arm_short_map(data, blk_start, mapsize,</span>
<span class="p_add">+				     blk_paddr, pgdprot, pteprot);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return 0;/* Bytes unmapped */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int arm_short_unmap(struct io_pgtable_ops *ops,</span>
<span class="p_add">+			   unsigned long iova,</span>
<span class="p_add">+			   size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_ops_to_data(ops);</span>
<span class="p_add">+	struct io_pgtable_cfg *cfg = &amp;data-&gt;iop.cfg;</span>
<span class="p_add">+	void *cookie = data-&gt;iop.cookie;</span>
<span class="p_add">+	arm_short_iopte *pgd_base = data-&gt;pgd;</span>
<span class="p_add">+	arm_short_iopte *pgd, *pte = NULL;</span>
<span class="p_add">+	arm_short_iopte pgd_tmp, pte_tmp = 0;</span>
<span class="p_add">+	unsigned int blk_base, blk_size;</span>
<span class="p_add">+	int unmap_size = 0;</span>
<span class="p_add">+	bool pgtempty;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd = pgd_base + ARM_SHORT_PGD_IDX(iova);</span>
<span class="p_add">+		blk_size = 0;</span>
<span class="p_add">+		pgtempty = false;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Get block size */</span>
<span class="p_add">+		if (ARM_SHORT_PGD_TYPE_IS_PGTABLE(*pgd)) {</span>
<span class="p_add">+			pte = arm_short_get_pte_in_pgd(*pgd, iova);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (ARM_SHORT_PTE_TYPE_IS_SMALLPAGE(*pte))</span>
<span class="p_add">+				blk_size = SZ_4K;</span>
<span class="p_add">+			else if (ARM_SHORT_PTE_TYPE_IS_LARGEPAGE(*pte))</span>
<span class="p_add">+				blk_size = SZ_64K;</span>
<span class="p_add">+		} else if (ARM_SHORT_PGD_TYPE_IS_SECTION(*pgd)) {</span>
<span class="p_add">+			blk_size = SZ_1M;</span>
<span class="p_add">+		} else if (ARM_SHORT_PGD_TYPE_IS_SUPERSECTION(*pgd)) {</span>
<span class="p_add">+			blk_size = SZ_16M;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (WARN_ON(!blk_size))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Unmap the pgd/pte of the block base */</span>
<span class="p_add">+		blk_base = iova &amp; ~(blk_size - 1);</span>
<span class="p_add">+		pgd = pgd_base + ARM_SHORT_PGD_IDX(blk_base);</span>
<span class="p_add">+		pgd_tmp = *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (blk_size == SZ_4K || blk_size == SZ_64K) {</span>
<span class="p_add">+			pte = arm_short_get_pte_in_pgd(*pgd, blk_base);</span>
<span class="p_add">+			pte_tmp = *pte;</span>
<span class="p_add">+			__arm_short_set_pte(pte, 0, blk_size / SZ_4K, cfg);</span>
<span class="p_add">+</span>
<span class="p_add">+			pgtempty = __arm_short_pgtable_empty(pgd);</span>
<span class="p_add">+			if (pgtempty)</span>
<span class="p_add">+				__arm_short_set_pte(pgd, 0, 1, cfg);</span>
<span class="p_add">+		} else if (blk_size == SZ_1M || blk_size == SZ_16M) {</span>
<span class="p_add">+			__arm_short_set_pte(pgd, 0, blk_size / SZ_1M, cfg);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		cfg-&gt;tlb-&gt;tlb_add_flush(blk_base, blk_size, true, cookie);</span>
<span class="p_add">+		cfg-&gt;tlb-&gt;tlb_sync(cookie);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pgtempty)/* Free lvl2 pgtable after tlb-flush */</span>
<span class="p_add">+			__arm_short_free_pgtable(</span>
<span class="p_add">+					ARM_SHORT_GET_PGTABLE_VA(pgd_tmp),</span>
<span class="p_add">+					ARM_SHORT_BYTES_PER_PTE, false, cfg);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If the unmap size that from the pgsize_bitmap is more</span>
<span class="p_add">+		 * than the current blk_size, unmap it continuously.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (blk_size &lt;= size) {</span>
<span class="p_add">+			iova += blk_size;</span>
<span class="p_add">+			size -= blk_size;</span>
<span class="p_add">+			unmap_size += blk_size;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		} else { /* Split this block */</span>
<span class="p_add">+			return arm_short_split_blk_unmap(</span>
<span class="p_add">+					ops, iova, size, blk_size,</span>
<span class="p_add">+					ARM_SHORT_PGD_GET_PROT(pgd_tmp),</span>
<span class="p_add">+					ARM_SHORT_PTE_GET_PROT_LARGE(pte_tmp));</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (size);</span>
<span class="p_add">+</span>
<span class="p_add">+	return unmap_size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct io_pgtable *</span>
<span class="p_add">+arm_short_alloc_pgtable(struct io_pgtable_cfg *cfg, void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cfg-&gt;ias &gt; 32 || cfg-&gt;oas &gt; 32)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg-&gt;pgsize_bitmap &amp;=</span>
<span class="p_add">+		(cfg-&gt;quirks &amp; IO_PGTABLE_QUIRK_SHORT_SUPERSECTION) ?</span>
<span class="p_add">+		(SZ_4K | SZ_64K | SZ_1M | SZ_16M) : (SZ_4K | SZ_64K | SZ_1M);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!selftest_running &amp;&amp; cfg-&gt;iommu_dev-&gt;dma_pfn_offset) {</span>
<span class="p_add">+		dev_err(cfg-&gt;iommu_dev, &quot;Cannot accommodate DMA offset for IOMMU page tables\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	data = kzalloc(sizeof(*data), GFP_KERNEL);</span>
<span class="p_add">+	if (!data)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;pgd_size = SZ_16K;</span>
<span class="p_add">+	data-&gt;pgd = __arm_short_alloc_pgtable(</span>
<span class="p_add">+					data-&gt;pgd_size,</span>
<span class="p_add">+					GFP_KERNEL | __GFP_ZERO | GFP_DMA,</span>
<span class="p_add">+					true, cfg);</span>
<span class="p_add">+	if (!data-&gt;pgd)</span>
<span class="p_add">+		goto out_free_data;</span>
<span class="p_add">+	wmb();/* Ensure the empty pgd is visible before any actual TTBR write */</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;pgtable_cached = kmem_cache_create(</span>
<span class="p_add">+					&quot;io-pgtable-arm-short&quot;,</span>
<span class="p_add">+					 ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+					 ARM_SHORT_BYTES_PER_PTE,</span>
<span class="p_add">+					 SLAB_CACHE_DMA, NULL);</span>
<span class="p_add">+	if (!data-&gt;pgtable_cached)</span>
<span class="p_add">+		goto out_free_pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* TTBRs */</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.ttbr[0] = virt_to_phys(data-&gt;pgd);</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.ttbr[1] = 0;</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.tcr = 0;</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.nmrr = 0;</span>
<span class="p_add">+	cfg-&gt;arm_short_cfg.prrr = 0;</span>
<span class="p_add">+	/* The access flag is not supported as SCTLR isn&#39;t implemented */</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;iop.ops = (struct io_pgtable_ops) {</span>
<span class="p_add">+		.map		= arm_short_map,</span>
<span class="p_add">+		.unmap		= arm_short_unmap,</span>
<span class="p_add">+		.iova_to_phys	= arm_short_iova_to_phys,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;data-&gt;iop;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free_pgd:</span>
<span class="p_add">+	__arm_short_free_pgtable(data-&gt;pgd, data-&gt;pgd_size, true, cfg);</span>
<span class="p_add">+out_free_data:</span>
<span class="p_add">+	kfree(data);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void arm_short_free_pgtable(struct io_pgtable *iop)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arm_short_io_pgtable *data = io_pgtable_to_data(iop);</span>
<span class="p_add">+</span>
<span class="p_add">+	kmem_cache_destroy(data-&gt;pgtable_cached);</span>
<span class="p_add">+	__arm_short_free_pgtable(data-&gt;pgd, data-&gt;pgd_size,</span>
<span class="p_add">+				 true, &amp;data-&gt;iop.cfg);</span>
<span class="p_add">+	kfree(data);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct io_pgtable_init_fns io_pgtable_arm_short_init_fns = {</span>
<span class="p_add">+	.alloc	= arm_short_alloc_pgtable,</span>
<span class="p_add">+	.free	= arm_short_free_pgtable,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_IOMMU_IO_PGTABLE_SHORT_SELFTEST</span>
<span class="p_add">+</span>
<span class="p_add">+static struct io_pgtable_cfg *cfg_cookie;</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_flush_all(void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_add_flush(unsigned long iova, size_t size, bool leaf,</span>
<span class="p_add">+				void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+	WARN_ON(!(size &amp; cfg_cookie-&gt;pgsize_bitmap));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dummy_tlb_sync(void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN_ON(cookie != cfg_cookie);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct iommu_gather_ops dummy_tlb_ops = {</span>
<span class="p_add">+	.tlb_flush_all	= dummy_tlb_flush_all,</span>
<span class="p_add">+	.tlb_add_flush	= dummy_tlb_add_flush,</span>
<span class="p_add">+	.tlb_sync	= dummy_tlb_sync,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define __FAIL(ops)	({				\</span>
<span class="p_add">+		WARN(1, &quot;selftest: test failed\n&quot;);	\</span>
<span class="p_add">+		selftest_running = false;		\</span>
<span class="p_add">+		-EFAULT;				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init arm_short_do_selftests(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct io_pgtable_ops *ops;</span>
<span class="p_add">+	struct io_pgtable_cfg cfg = {</span>
<span class="p_add">+		.tlb = &amp;dummy_tlb_ops,</span>
<span class="p_add">+		.oas = 32,</span>
<span class="p_add">+		.ias = 32,</span>
<span class="p_add">+		.quirks = IO_PGTABLE_QUIRK_ARM_NS |</span>
<span class="p_add">+			IO_PGTABLE_QUIRK_SHORT_SUPERSECTION,</span>
<span class="p_add">+		.pgsize_bitmap = SZ_4K | SZ_64K | SZ_1M | SZ_16M,</span>
<span class="p_add">+	};</span>
<span class="p_add">+	unsigned int iova, size, iova_start;</span>
<span class="p_add">+	unsigned int i, loopnr = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	selftest_running = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	cfg_cookie = &amp;cfg;</span>
<span class="p_add">+</span>
<span class="p_add">+	ops = alloc_io_pgtable_ops(ARM_SHORT_DESC, &amp;cfg, &amp;cfg);</span>
<span class="p_add">+	if (!ops) {</span>
<span class="p_add">+		pr_err(&quot;Failed to alloc short desc io pgtable\n&quot;);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initial sanity checks.</span>
<span class="p_add">+	 * Empty page tables shouldn&#39;t provide any translations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, SZ_1G + 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ops-&gt;iova_to_phys(ops, SZ_2G + 42))</span>
<span class="p_add">+		return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Distinct mappings of different granule sizes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	iova = 0;</span>
<span class="p_add">+	i = find_first_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG);</span>
<span class="p_add">+	while (i != BITS_PER_LONG) {</span>
<span class="p_add">+		size = 1UL &lt;&lt; i;</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova, iova, size, IOMMU_READ |</span>
<span class="p_add">+						    IOMMU_WRITE |</span>
<span class="p_add">+						    IOMMU_NOEXEC |</span>
<span class="p_add">+						    IOMMU_CACHE))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Overlapping mappings */</span>
<span class="p_add">+		if (!ops-&gt;map(ops, iova, iova + size, size,</span>
<span class="p_add">+			      IOMMU_READ | IOMMU_NOEXEC))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42) != (iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		iova += SZ_16M;</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+		i = find_next_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG, i);</span>
<span class="p_add">+		loopnr++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Partial unmap */</span>
<span class="p_add">+	i = 1;</span>
<span class="p_add">+	size = 1UL &lt;&lt; __ffs(cfg.pgsize_bitmap);</span>
<span class="p_add">+	while (i &lt; loopnr) {</span>
<span class="p_add">+		iova_start = i * SZ_16M;</span>
<span class="p_add">+		if (ops-&gt;unmap(ops, iova_start + size, size) != size)</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Remap of partial unmap */</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova_start + size, size, size, IOMMU_READ))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova_start + size + 42)</span>
<span class="p_add">+		    != (size + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Full unmap */</span>
<span class="p_add">+	iova = 0;</span>
<span class="p_add">+	i = find_first_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG);</span>
<span class="p_add">+	while (i != BITS_PER_LONG) {</span>
<span class="p_add">+		size = 1UL &lt;&lt; i;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;unmap(ops, iova, size) != size)</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Remap full block */</span>
<span class="p_add">+		if (ops-&gt;map(ops, iova, iova, size, IOMMU_WRITE))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ops-&gt;iova_to_phys(ops, iova + 42) != (iova + 42))</span>
<span class="p_add">+			return __FAIL(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+		iova += SZ_16M;</span>
<span class="p_add">+		i++;</span>
<span class="p_add">+		i = find_next_bit(&amp;cfg.pgsize_bitmap, BITS_PER_LONG, i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	free_io_pgtable_ops(ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	selftest_running = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;arm-short io-pgtable: self test ok\n&quot;);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+subsys_initcall(arm_short_do_selftests);</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/drivers/iommu/io-pgtable-arm.c b/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_header">index 73c0748..0d7bcfc 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable-arm.c</span>
<span class="p_chunk">@@ -38,9 +38,6 @@</span> <span class="p_context"></span>
 #define io_pgtable_to_data(x)						\
 	container_of((x), struct arm_lpae_io_pgtable, iop)
 
<span class="p_del">-#define io_pgtable_ops_to_pgtable(x)					\</span>
<span class="p_del">-	container_of((x), struct io_pgtable, ops)</span>
<span class="p_del">-</span>
 #define io_pgtable_ops_to_data(x)					\
 	io_pgtable_to_data(io_pgtable_ops_to_pgtable(x))
 
<span class="p_header">diff --git a/drivers/iommu/io-pgtable.c b/drivers/iommu/io-pgtable.c</span>
<span class="p_header">index 6f2e319..e7b0b1a 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable.c</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable.c</span>
<span class="p_chunk">@@ -33,6 +33,9 @@</span> <span class="p_context"> io_pgtable_init_table[IO_PGTABLE_NUM_FMTS] =</span>
 	[ARM_64_LPAE_S1] = &amp;io_pgtable_arm_64_lpae_s1_init_fns,
 	[ARM_64_LPAE_S2] = &amp;io_pgtable_arm_64_lpae_s2_init_fns,
 #endif
<span class="p_add">+#ifdef CONFIG_IOMMU_IO_PGTABLE_SHORT</span>
<span class="p_add">+	[ARM_SHORT_DESC] = &amp;io_pgtable_arm_short_init_fns,</span>
<span class="p_add">+#endif</span>
 };
 
 struct io_pgtable_ops *alloc_io_pgtable_ops(enum io_pgtable_fmt fmt,
<span class="p_header">diff --git a/drivers/iommu/io-pgtable.h b/drivers/iommu/io-pgtable.h</span>
<span class="p_header">index ac9e234..af09467 100644</span>
<span class="p_header">--- a/drivers/iommu/io-pgtable.h</span>
<span class="p_header">+++ b/drivers/iommu/io-pgtable.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"> enum io_pgtable_fmt {</span>
 	ARM_32_LPAE_S2,
 	ARM_64_LPAE_S1,
 	ARM_64_LPAE_S2,
<span class="p_add">+	ARM_SHORT_DESC,</span>
 	IO_PGTABLE_NUM_FMTS,
 };
 
<span class="p_chunk">@@ -45,7 +46,10 @@</span> <span class="p_context"> struct iommu_gather_ops {</span>
  *                 page table walker.
  */
 struct io_pgtable_cfg {
<span class="p_del">-	#define IO_PGTABLE_QUIRK_ARM_NS	(1 &lt;&lt; 0)	/* Set NS bit in PTEs */</span>
<span class="p_add">+	#define IO_PGTABLE_QUIRK_ARM_NS		BIT(0) /* Set NS bit in PTEs */</span>
<span class="p_add">+	#define IO_PGTABLE_QUIRK_NO_PERMS	BIT(1) /* No AP/XN bits */</span>
<span class="p_add">+	#define IO_PGTABLE_QUIRK_TLBI_ON_MAP	BIT(2) /* TLB Inv. on map */</span>
<span class="p_add">+	#define IO_PGTABLE_QUIRK_SHORT_SUPERSECTION	BIT(3)</span>
 	int				quirks;
 	unsigned long			pgsize_bitmap;
 	unsigned int			ias;
<span class="p_chunk">@@ -65,6 +69,14 @@</span> <span class="p_context"> struct io_pgtable_cfg {</span>
 			u64	vttbr;
 			u64	vtcr;
 		} arm_lpae_s2_cfg;
<span class="p_add">+</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u32	ttbr[2];</span>
<span class="p_add">+			u32	tcr;</span>
<span class="p_add">+			u32	nmrr;</span>
<span class="p_add">+			u32	prrr;</span>
<span class="p_add">+			u32	sctlr;</span>
<span class="p_add">+		} arm_short_cfg;</span>
 	};
 };
 
<span class="p_chunk">@@ -131,6 +143,9 @@</span> <span class="p_context"> struct io_pgtable {</span>
 	struct io_pgtable_ops	ops;
 };
 
<span class="p_add">+#define io_pgtable_ops_to_pgtable(x)		\</span>
<span class="p_add">+	container_of((x), struct io_pgtable, ops)</span>
<span class="p_add">+</span>
 /**
  * struct io_pgtable_init_fns - Alloc/free a set of page tables for a
  *                              particular format.
<span class="p_chunk">@@ -147,5 +162,6 @@</span> <span class="p_context"> extern struct io_pgtable_init_fns io_pgtable_arm_32_lpae_s1_init_fns;</span>
 extern struct io_pgtable_init_fns io_pgtable_arm_32_lpae_s2_init_fns;
 extern struct io_pgtable_init_fns io_pgtable_arm_64_lpae_s1_init_fns;
 extern struct io_pgtable_init_fns io_pgtable_arm_64_lpae_s2_init_fns;
<span class="p_add">+extern struct io_pgtable_init_fns io_pgtable_arm_short_init_fns;</span>
 
 #endif /* __IO_PGTABLE_H */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



