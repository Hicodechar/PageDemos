
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/4] vfio/type1: Gather TLB-syncs and pages to unpin - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/4] vfio/type1: Gather TLB-syncs and pages to unpin</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 13, 2017, 2:40 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1507905613-30695-5-git-send-email-joro@8bytes.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10004921/mbox/"
   >mbox</a>
|
   <a href="/patch/10004921/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10004921/">/patch/10004921/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6350860360 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 13 Oct 2017 14:41:29 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 540DE290B2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 13 Oct 2017 14:41:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4936F290BF; Fri, 13 Oct 2017 14:41:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B2D15290BF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 13 Oct 2017 14:41:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758493AbdJMOl0 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 13 Oct 2017 10:41:26 -0400
Received: from 8bytes.org ([81.169.241.247]:58126 &quot;EHLO theia.8bytes.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1757931AbdJMOkS (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 13 Oct 2017 10:40:18 -0400
Received: by theia.8bytes.org (Postfix, from userid 1000)
	id DD945438; Fri, 13 Oct 2017 16:40:16 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple; d=8bytes.org; s=mail-1; 
	t=1507905616; bh=tcbIi7VBXRjSdhn7XVRu3EVFz/UuB8DRGonmmtWAhRs=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=pdYaY352SvRAl6V6+exL92EPDdXzBQsO99TSKYPFYCbk96YHCVxWJ8XR7lyEvWAf+
	StYCRFSjC3uKQZWaXnLDXUgDmy+8ipwuffwNepZlPuznyRytSm/1iRCDSplRcUVY6y
	hAVuT5pA8AM8EpN2fU6ZyS9Q9iAcZFryNxaDLnZe6Zf8nKt+hXbYXRe1Cpac9ASNVN
	yFq2KthbKuEbDDx6SmIvw4qCQ5ju28A149bszn51aQmZb/PjPBLZn/QrClnKi9evKw
	8A+x5jkuF3zsGB/VAe0Fiw7Tgk6Hfm7E60y1k+hvhqZ5qdL6XRuviwPhQYegsagcFG
	fYZGWs1Zo5tQg==
From: Joerg Roedel &lt;joro@8bytes.org&gt;
To: iommu@lists.linux-foundation.org,
	Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;,
	kvm@vger.kernel.org, linux-kernel@vger.kernel.org,
	Joerg Roedel &lt;jroedel@suse.de&gt;
Subject: [PATCH 4/4] vfio/type1: Gather TLB-syncs and pages to unpin
Date: Fri, 13 Oct 2017 16:40:13 +0200
Message-Id: &lt;1507905613-30695-5-git-send-email-joro@8bytes.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1507905613-30695-1-git-send-email-joro@8bytes.org&gt;
References: &lt;1507905613-30695-1-git-send-email-joro@8bytes.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=317">Joerg Roedel</a> - Oct. 13, 2017, 2:40 p.m.</div>
<pre class="content">
<span class="from">From: Joerg Roedel &lt;jroedel@suse.de&gt;</span>

After every unmap VFIO unpins the pages that where mapped by
the IOMMU. This requires an IOTLB flush after every unmap
and puts a high load on the IOMMU hardware and the device
TLBs.

Gather up to 32 ranges to flush and unpin and do the IOTLB
flush once for all these ranges. This significantly reduces
the number of IOTLB flushes in the unmapping path.
<span class="signed-off-by">
Signed-off-by: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
---
 drivers/vfio/vfio_iommu_type1.c | 106 ++++++++++++++++++++++++++++++++++++++--
 1 file changed, 101 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Oct. 13, 2017, 5:02 p.m.</div>
<pre class="content">
On Fri, 13 Oct 2017 16:40:13 +0200
Joerg Roedel &lt;joro@8bytes.org&gt; wrote:
<span class="quote">
&gt; From: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After every unmap VFIO unpins the pages that where mapped by</span>
<span class="quote">&gt; the IOMMU. This requires an IOTLB flush after every unmap</span>
<span class="quote">&gt; and puts a high load on the IOMMU hardware and the device</span>
<span class="quote">&gt; TLBs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Gather up to 32 ranges to flush and unpin and do the IOTLB</span>
<span class="quote">&gt; flush once for all these ranges. This significantly reduces</span>
<span class="quote">&gt; the number of IOTLB flushes in the unmapping path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/vfio/vfio_iommu_type1.c | 106 ++++++++++++++++++++++++++++++++++++++--</span>
<span class="quote">&gt;  1 file changed, 101 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; index 2b1e81f..86fc1da 100644</span>
<span class="quote">&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; @@ -107,6 +107,92 @@ struct vfio_pfn {</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int put_pfn(unsigned long pfn, int prot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt; +				    unsigned long pfn, long npage,</span>
<span class="quote">&gt; +				    bool do_accounting);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define GATHER_ENTRIES	32</span>

What heuristics make us arrive at this number and how would we evaluate
changing it in the future?  Ideally we&#39;d only want to do a single
flush, but we can&#39;t unpin pages until after the iommu sync and we need
the iommu to track iova-phys mappings, so it&#39;s a matter of how much do
we want to allocate to buffer those translations.  I wonder if a cache
pool would help here, but this is probably fine for a first pass with
some comment about this trade-off and why 32 was chosen.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Gather TLB flushes before unpinning pages</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct vfio_gather_entry {</span>
<span class="quote">&gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; +	size_t size;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct vfio_gather {</span>
<span class="quote">&gt; +	unsigned fill;</span>
<span class="quote">&gt; +	struct vfio_gather_entry entries[GATHER_ENTRIES];</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * The vfio_gather* functions below keep track of flushing the IOMMU TLB</span>
<span class="quote">&gt; + * and unpinning the pages. It is safe to call them gather == NULL, in</span>
<span class="quote">&gt; + * which case they will fall-back to flushing the TLB and unpinning the</span>
<span class="quote">&gt; + * pages at every call.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static long vfio_gather_flush(struct iommu_domain *domain,</span>
<span class="quote">&gt; +			      struct vfio_dma *dma,</span>
<span class="quote">&gt; +			      struct vfio_gather *gather)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long unlocked = 0;</span>
<span class="quote">&gt; +	unsigned i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!gather)</span>

|| !gather-&gt;fill

We might have gotten lucky that our last add triggered a flush.
<span class="quote">
&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* First flush unmapped TLB entries */</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; gather-&gt;fill; i++) {</span>
<span class="quote">&gt; +		dma_addr_t iova = gather-&gt;entries[i].iova;</span>
<span class="quote">&gt; +		phys_addr_t phys = gather-&gt;entries[i].phys;</span>
<span class="quote">&gt; +		size_t size = gather-&gt;entries[i].size;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="quote">&gt; +						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    size &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    false);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	gather-&gt;fill = 0;</span>


A struct vfio_gather_entry* would clean this up and eliminate some
variables, including i.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	return unlocked;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static long vfio_gather_add(struct iommu_domain *domain,</span>
<span class="quote">&gt; +			    struct vfio_dma *dma,</span>
<span class="quote">&gt; +			    struct vfio_gather *gather,</span>
<span class="quote">&gt; +			    dma_addr_t iova, phys_addr_t phys, size_t size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long unlocked = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (gather) {</span>
<span class="quote">&gt; +		unsigned index;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (gather-&gt;fill == GATHER_ENTRIES)</span>
<span class="quote">&gt; +			unlocked = vfio_gather_flush(domain, dma, gather);</span>

                        unlocked += vfio_unpin_pages_remote(...);
                } else {

IOW, vfio_gather_flush() has already done the iommu_tlb_sync() for the
mapping that called us, there&#39;s no point in adding these to our list,
unpin them immediate.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		index = gather-&gt;fill++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		gather-&gt;entries[index].iova = iova;</span>
<span class="quote">&gt; +		gather-&gt;entries[index].phys = phys;</span>
<span class="quote">&gt; +		gather-&gt;entries[index].size = size;</span>

Alternatively, do the test and flush here instead.

Thanks,
Alex
<span class="quote">
&gt; +	} else {</span>
<span class="quote">&gt; +		iommu_tlb_sync(domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		unlocked = vfio_unpin_pages_remote(dma, iova,</span>
<span class="quote">&gt; +						   phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						   size &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						   false);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return unlocked;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * This code handles mapping and unmapping of user data buffers</span>
<span class="quote">&gt;   * into DMA&#39;ble space using the IOMMU</span>
<span class="quote">&gt; @@ -653,6 +739,7 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;</span>
<span class="quote">&gt;  	struct vfio_domain *domain, *d;</span>
<span class="quote">&gt; +	struct vfio_gather *gather;</span>
<span class="quote">&gt;  	long unlocked = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!dma-&gt;size)</span>
<span class="quote">&gt; @@ -662,6 +749,12 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; +	 * No need to check return value - It is safe to continue with a</span>
<span class="quote">&gt; +	 * NULL pointer.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	gather = kzalloc(sizeof(*gather), GFP_KERNEL);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt;  	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d</span>
<span class="quote">&gt;  	 * need a much more complicated tracking system.  Unfortunately that</span>
<span class="quote">&gt;  	 * means we need to use one of the iommu domains to figure out the</span>
<span class="quote">&gt; @@ -706,17 +799,20 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; -		iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="quote">&gt; -						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; -						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; -						    false);</span>
<span class="quote">&gt; +		unlocked += vfio_gather_add(domain-&gt;domain, dma, gather,</span>
<span class="quote">&gt; +					    iova, phys, unmapped);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		iova += unmapped;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	unlocked += vfio_gather_flush(domain-&gt;domain, dma, gather);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	kfree(gather);</span>
<span class="quote">&gt; +	gather = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	dma-&gt;iommu_mapped = false;</span>
<span class="quote">&gt;  	if (do_accounting) {</span>
<span class="quote">&gt;  		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index 2b1e81f..86fc1da 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -107,6 +107,92 @@</span> <span class="p_context"> struct vfio_pfn {</span>
 
 static int put_pfn(unsigned long pfn, int prot);
 
<span class="p_add">+static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="p_add">+				    unsigned long pfn, long npage,</span>
<span class="p_add">+				    bool do_accounting);</span>
<span class="p_add">+</span>
<span class="p_add">+#define GATHER_ENTRIES	32</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Gather TLB flushes before unpinning pages</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct vfio_gather_entry {</span>
<span class="p_add">+	dma_addr_t iova;</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+	size_t size;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct vfio_gather {</span>
<span class="p_add">+	unsigned fill;</span>
<span class="p_add">+	struct vfio_gather_entry entries[GATHER_ENTRIES];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The vfio_gather* functions below keep track of flushing the IOMMU TLB</span>
<span class="p_add">+ * and unpinning the pages. It is safe to call them gather == NULL, in</span>
<span class="p_add">+ * which case they will fall-back to flushing the TLB and unpinning the</span>
<span class="p_add">+ * pages at every call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static long vfio_gather_flush(struct iommu_domain *domain,</span>
<span class="p_add">+			      struct vfio_dma *dma,</span>
<span class="p_add">+			      struct vfio_gather *gather)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long unlocked = 0;</span>
<span class="p_add">+	unsigned i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!gather)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* First flush unmapped TLB entries */</span>
<span class="p_add">+	iommu_tlb_sync(domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; gather-&gt;fill; i++) {</span>
<span class="p_add">+		dma_addr_t iova = gather-&gt;entries[i].iova;</span>
<span class="p_add">+		phys_addr_t phys = gather-&gt;entries[i].phys;</span>
<span class="p_add">+		size_t size = gather-&gt;entries[i].size;</span>
<span class="p_add">+</span>
<span class="p_add">+		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="p_add">+						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    size &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	gather-&gt;fill = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return unlocked;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static long vfio_gather_add(struct iommu_domain *domain,</span>
<span class="p_add">+			    struct vfio_dma *dma,</span>
<span class="p_add">+			    struct vfio_gather *gather,</span>
<span class="p_add">+			    dma_addr_t iova, phys_addr_t phys, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long unlocked = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (gather) {</span>
<span class="p_add">+		unsigned index;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (gather-&gt;fill == GATHER_ENTRIES)</span>
<span class="p_add">+			unlocked = vfio_gather_flush(domain, dma, gather);</span>
<span class="p_add">+</span>
<span class="p_add">+		index = gather-&gt;fill++;</span>
<span class="p_add">+</span>
<span class="p_add">+		gather-&gt;entries[index].iova = iova;</span>
<span class="p_add">+		gather-&gt;entries[index].phys = phys;</span>
<span class="p_add">+		gather-&gt;entries[index].size = size;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		iommu_tlb_sync(domain);</span>
<span class="p_add">+</span>
<span class="p_add">+		unlocked = vfio_unpin_pages_remote(dma, iova,</span>
<span class="p_add">+						   phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						   size &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						   false);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return unlocked;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * This code handles mapping and unmapping of user data buffers
  * into DMA&#39;ble space using the IOMMU
<span class="p_chunk">@@ -653,6 +739,7 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 {
 	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;
 	struct vfio_domain *domain, *d;
<span class="p_add">+	struct vfio_gather *gather;</span>
 	long unlocked = 0;
 
 	if (!dma-&gt;size)
<span class="p_chunk">@@ -662,6 +749,12 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 		return 0;
 
 	/*
<span class="p_add">+	 * No need to check return value - It is safe to continue with a</span>
<span class="p_add">+	 * NULL pointer.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	gather = kzalloc(sizeof(*gather), GFP_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d
 	 * need a much more complicated tracking system.  Unfortunately that
 	 * means we need to use one of the iommu domains to figure out the
<span class="p_chunk">@@ -706,17 +799,20 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 			break;
 
 		iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);
<span class="p_del">-		iommu_tlb_sync(domain-&gt;domain);</span>
 
<span class="p_del">-		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="p_del">-						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    false);</span>
<span class="p_add">+		unlocked += vfio_gather_add(domain-&gt;domain, dma, gather,</span>
<span class="p_add">+					    iova, phys, unmapped);</span>
<span class="p_add">+</span>
 		iova += unmapped;
 
 		cond_resched();
 	}
 
<span class="p_add">+	unlocked += vfio_gather_flush(domain-&gt;domain, dma, gather);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(gather);</span>
<span class="p_add">+	gather = NULL;</span>
<span class="p_add">+</span>
 	dma-&gt;iommu_mapped = false;
 	if (do_accounting) {
 		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



