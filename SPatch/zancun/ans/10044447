
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,for,4.15,08/14] Provide cpu_opv system call - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,for,4.15,08/14] Provide cpu_opv system call</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 6, 2017, 8:56 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171106205644.29386-9-mathieu.desnoyers@efficios.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10044447/mbox/"
   >mbox</a>
|
   <a href="/patch/10044447/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10044447/">/patch/10044447/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1A4D6602BF for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Nov 2017 20:57:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0863829FB6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Nov 2017 20:57:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EEEE42A00F; Mon,  6 Nov 2017 20:57:57 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B696B29FB6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Nov 2017 20:57:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933298AbdKFU5y (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Nov 2017 15:57:54 -0500
Received: from mail.efficios.com ([167.114.142.141]:36945 &quot;EHLO
	mail.efficios.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S932186AbdKFU5j (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Nov 2017 15:57:39 -0500
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id 344D3340206;
	Mon,  6 Nov 2017 20:57:59 +0000 (UTC)
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10032)
	with ESMTP id vuyVrxUVG43s; Mon,  6 Nov 2017 20:57:45 +0000 (UTC)
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id BF2693403CB;
	Mon,  6 Nov 2017 20:57:34 +0000 (UTC)
X-Virus-Scanned: amavisd-new at efficios.com
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10026)
	with ESMTP id GvKoa15gopt0; Mon,  6 Nov 2017 20:57:34 +0000 (UTC)
Received: from thinkos.internal.efficios.com
	(192-222-157-41.qc.cable.ebox.net [192.222.157.41])
	by mail.efficios.com (Postfix) with ESMTPSA id 56E493403C0;
	Mon,  6 Nov 2017 20:57:34 +0000 (UTC)
From: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;
To: Peter Zijlstra &lt;peterz@infradead.org&gt;,
	&quot;Paul E . McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;,
	Boqun Feng &lt;boqun.feng@gmail.com&gt;, Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Dave Watson &lt;davejwatson@fb.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-api@vger.kernel.org,
	Paul Turner &lt;pjt@google.com&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Russell King &lt;linux@arm.linux.org.uk&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Andrew Hunter &lt;ahh@google.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, Chris Lameter &lt;cl@linux.com&gt;,
	Ben Maurer &lt;bmaurer@fb.com&gt;, Steven Rostedt &lt;rostedt@goodmis.org&gt;,
	Josh Triplett &lt;josh@joshtriplett.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;
Subject: [RFC PATCH v2 for 4.15 08/14] Provide cpu_opv system call
Date: Mon,  6 Nov 2017 15:56:38 -0500
Message-Id: &lt;20171106205644.29386-9-mathieu.desnoyers@efficios.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20171106205644.29386-1-mathieu.desnoyers@efficios.com&gt;
References: &lt;20171106205644.29386-1-mathieu.desnoyers@efficios.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Nov. 6, 2017, 8:56 p.m.</div>
<pre class="content">
This new cpu_opv system call executes a vector of operations on behalf
of user-space on a specific CPU with preemption disabled. It is inspired
from readv() and writev() system calls which take a &quot;struct iovec&quot; array
as argument.

The operations available are: comparison, memcpy, add, or, and, xor,
left shift, right shift, and mb. The system call receives a CPU number
from user-space as argument, which is the CPU on which those operations
need to be performed. All preparation steps such as loading pointers,
and applying offsets to arrays, need to be performed by user-space
before invoking the system call. The &quot;comparison&quot; operation can be used
to check that the data used in the preparation step did not change
between preparation of system call inputs and operation execution within
the preempt-off critical section.

The reason why we require all pointer offsets to be calculated by
user-space beforehand is because we need to use get_user_pages_fast() to
first pin all pages touched by each operation. This takes care of
faulting-in the pages. Then, preemption is disabled, and the operations
are performed atomically with respect to other thread execution on that
CPU, without generating any page fault.

A maximum limit of 16 operations per cpu_opv syscall invocation is
enforced, so user-space cannot generate a too long preempt-off critical
section. Each operation is also limited a length of PAGE_SIZE bytes,
meaning that an operation can touch a maximum of 4 pages (memcpy: 2
pages for source, 2 pages for destination if addresses are not aligned
on page boundaries). Moreover, a total limit of 4216 bytes is applied
to operation lengths.

If the thread is not running on the requested CPU, a new
push_task_to_cpu() is invoked to migrate the task to the requested CPU.
If the requested CPU is not part of the cpus allowed mask of the thread,
the system call fails with EINVAL. After the migration has been
performed, preemption is disabled, and the current CPU number is checked
again and compared to the requested CPU number. If it still differs, it
means the scheduler migrated us away from that CPU. Return EAGAIN to
user-space in that case, and let user-space retry (either requesting the
same CPU number, or a different one, depending on the user-space
algorithm constraints).
<span class="signed-off-by">
Signed-off-by: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
CC: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
CC: Peter Zijlstra &lt;peterz@infradead.org&gt;
CC: Paul Turner &lt;pjt@google.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;
CC: Andrew Hunter &lt;ahh@google.com&gt;
CC: Andy Lutomirski &lt;luto@amacapital.net&gt;
CC: Andi Kleen &lt;andi@firstfloor.org&gt;
CC: Dave Watson &lt;davejwatson@fb.com&gt;
CC: Chris Lameter &lt;cl@linux.com&gt;
CC: Ingo Molnar &lt;mingo@redhat.com&gt;
CC: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
CC: Ben Maurer &lt;bmaurer@fb.com&gt;
CC: Steven Rostedt &lt;rostedt@goodmis.org&gt;
CC: Josh Triplett &lt;josh@joshtriplett.org&gt;
CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
CC: Andrew Morton &lt;akpm@linux-foundation.org&gt;
CC: Russell King &lt;linux@arm.linux.org.uk&gt;
CC: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
CC: Will Deacon &lt;will.deacon@arm.com&gt;
CC: Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;
CC: Boqun Feng &lt;boqun.feng@gmail.com&gt;
CC: linux-api@vger.kernel.org
---

Changes since v1:
- handle CPU hotplug,
- cleanup implementation using function pointers: We can use function
  pointers to implement the operations rather than duplicating all the
  user-access code.
- refuse device pages: Performing cpu_opv operations on io map&#39;d pages
  with preemption disabled could generate long preempt-off critical
  sections, which leads to unwanted scheduler latency. Return EFAULT if
  a device page is received as parameter
- restrict op vector to 4216 bytes length sum: Restrict the operation
  vector to length sum of:
  - 4096 bytes (typical page size on most architectures, should be
    enough for a string, or structures)
  - 15 * 8 bytes (typical operations on integers or pointers).
  The goal here is to keep the duration of preempt off critical section
  short, so we don&#39;t add significant scheduler latency.
- Add INIT_ONSTACK macro: Introduce the
  CPU_OP_FIELD_u32_u64_INIT_ONSTACK() macros to ensure that users
  correctly initialize the upper bits of CPU_OP_FIELD_u32_u64() on their
  stack to 0 on 32-bit architectures.
- Add CPU_MB_OP operation:
  Use-cases with:
  - two consecutive stores,
  - a mempcy followed by a store,
  require a memory barrier before the final store operation. A typical
  use-case is a store-release on the final store. Given that this is a
  slow path, just providing an explicit full barrier instruction should
  be sufficient.
- Add expect fault field:
  The use-case of list_pop brings interesting challenges. With rseq, we
  can use rseq_cmpnev_storeoffp_load(), and therefore load a pointer,
  compare it against NULL, add an offset, and load the target &quot;next&quot;
  pointer from the object, all within a single req critical section.

  Life is not so easy for cpu_opv in this use-case, mainly because we
  need to pin all pages we are going to touch in the preempt-off
  critical section beforehand. So we need to know the target object (in
  which we apply an offset to fetch the next pointer) when we pin pages
  before disabling preemption.

  So the approach is to load the head pointer and compare it against
  NULL in user-space, before doing the cpu_opv syscall. User-space can
  then compute the address of the head-&gt;next field, *without loading it*.

  The cpu_opv system call will first need to pin all pages associated
  with input data. This includes the page backing the head-&gt;next object,
  which may have been concurrently deallocated and unmapped. Therefore,
  in this case, getting -EFAULT when trying to pin those pages may
  happen: it just means they have been concurrently unmapped. This is
  an expected situation, and should just return -EAGAIN to user-space,
  to user-space can distinguish between &quot;should retry&quot; type of
  situations and actual errors that should be handled with extreme
  prejudice to the program (e.g. abort()).

  Therefore, add &quot;expect_fault&quot; fields along with op input address
  pointers, so user-space can identify whether a fault when getting a
  field should return EAGAIN rather than EFAULT.
- Add compiler barrier between operations: Adding a compiler barrier
  between store operations in a cpu_opv sequence can be useful when
  paired with membarrier system call.

  An algorithm with a paired slow path and fast path can use
  sys_membarrier on the slow path to replace fast-path memory barriers
  by compiler barrier.

  Adding an explicit compiler barrier between operations allows
  cpu_opv to be used as fallback for operations meant to match
  the membarrier system call.
---
 MAINTAINERS                  |   7 +
 include/uapi/linux/cpu_opv.h | 117 ++++++
 init/Kconfig                 |  14 +
 kernel/Makefile              |   1 +
 kernel/cpu_opv.c             | 952 +++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/core.c          |  37 ++
 kernel/sched/sched.h         |   2 +
 kernel/sys_ni.c              |   1 +
 8 files changed, 1131 insertions(+)
 create mode 100644 include/uapi/linux/cpu_opv.h
 create mode 100644 kernel/cpu_opv.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - Nov. 7, 2017, 2:07 a.m.</div>
<pre class="content">
On Mon, Nov 06, 2017 at 03:56:38PM -0500, Mathieu Desnoyers wrote:
[...]
<span class="quote">&gt; +static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		struct page ***pinned_pages_ptr, size_t *nr_pinned,</span>
<span class="quote">&gt; +		int write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *pages[2];</span>
<span class="quote">&gt; +	int ret, nr_pages;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!len)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="quote">&gt; +	BUG_ON(nr_pages &gt; 2);</span>
<span class="quote">&gt; +	if (*nr_pinned + nr_pages &gt; NR_PINNED_PAGES_ON_STACK) {</span>

Is this a bug? Seems you will kzalloc() every time if *nr_pinned is
bigger than NR_PINNED_PAGES_ON_STACK, which will result in memory
leaking.

I think the logic here is complex enough for us to introduce a
structure, like:

	struct cpu_opv_page_pinner {
		int nr_pinned;
		bool is_kmalloc;
		struct page **pinned_pages;
	};

Thoughts?

Regards,
Boqun
<span class="quote">
&gt; +		struct page **pinned_pages =</span>
<span class="quote">&gt; +			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="quote">&gt; +				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="quote">&gt; +		if (!pinned_pages)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +		memcpy(pinned_pages, *pinned_pages_ptr,</span>
<span class="quote">&gt; +			*nr_pinned * sizeof(struct page *));</span>
<span class="quote">&gt; +		*pinned_pages_ptr = pinned_pages;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +again:</span>
<span class="quote">&gt; +	ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="quote">&gt; +	if (ret &lt; nr_pages) {</span>
<span class="quote">&gt; +		if (ret &gt; 0)</span>
<span class="quote">&gt; +			put_page(pages[0]);</span>
<span class="quote">&gt; +		return -EFAULT;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="quote">&gt; +	 * and special mappings.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="quote">&gt; +	if (ret == -EAGAIN) {</span>
<span class="quote">&gt; +		put_page(pages[0]);</span>
<span class="quote">&gt; +		if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +			put_page(pages[1]);</span>
<span class="quote">&gt; +		goto again;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	if (ret)</span>
<span class="quote">&gt; +		goto error;</span>
<span class="quote">&gt; +	(*pinned_pages_ptr)[(*nr_pinned)++] = pages[0];</span>
<span class="quote">&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +		(*pinned_pages_ptr)[(*nr_pinned)++] = pages[1];</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +error:</span>
<span class="quote">&gt; +	put_page(pages[0]);</span>
<span class="quote">&gt; +	if (nr_pages &gt; 1)</span>
<span class="quote">&gt; +		put_page(pages[1]);</span>
<span class="quote">&gt; +	return -EFAULT;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="quote">&gt; +		struct page ***pinned_pages_ptr, size_t *nr_pinned)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int ret, i;</span>
<span class="quote">&gt; +	bool expect_fault = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check access, pin pages. */</span>
<span class="quote">&gt; +	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="quote">&gt; +		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		switch (op-&gt;op) {</span>
<span class="quote">&gt; +		case CPU_COMPARE_EQ_OP:</span>
<span class="quote">&gt; +		case CPU_COMPARE_NE_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.compare_op.expect_fault_b;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.compare_op.b,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case CPU_MEMCPY_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.memcpy_op.expect_fault_dst;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_WRITE, op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.memcpy_op.expect_fault_src;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_READ, op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case CPU_ADD_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.arithmetic_op.expect_fault_p;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_WRITE, op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case CPU_OR_OP:</span>
<span class="quote">&gt; +		case CPU_AND_OP:</span>
<span class="quote">&gt; +		case CPU_XOR_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.bitwise_op.expect_fault_p;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_WRITE, op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case CPU_LSHIFT_OP:</span>
<span class="quote">&gt; +		case CPU_RSHIFT_OP:</span>
<span class="quote">&gt; +			ret = -EFAULT;</span>
<span class="quote">&gt; +			expect_fault = op-&gt;u.shift_op.expect_fault_p;</span>
<span class="quote">&gt; +			if (!access_ok(VERIFY_WRITE, op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +					op-&gt;len))</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			ret = cpu_op_pin_pages(</span>
<span class="quote">&gt; +					(unsigned long)op-&gt;u.shift_op.p,</span>
<span class="quote">&gt; +					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="quote">&gt; +			if (ret)</span>
<span class="quote">&gt; +				goto error;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case CPU_MB_OP:</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		default:</span>
<span class="quote">&gt; +			return -EINVAL;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +error:</span>
<span class="quote">&gt; +	for (i = 0; i &lt; *nr_pinned; i++)</span>
<span class="quote">&gt; +		put_page((*pinned_pages_ptr)[i]);</span>
<span class="quote">&gt; +	*nr_pinned = 0;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="quote">&gt; +	 * It allows user-space to distinguish between a fault caused by</span>
<span class="quote">&gt; +	 * an access which is expect to fault (e.g. due to concurrent</span>
<span class="quote">&gt; +	 * unmapping of underlying memory) from an unexpected fault from</span>
<span class="quote">&gt; +	 * which a retry would not recover.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="quote">&gt; +		return -EAGAIN;</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
[...]
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index 353366928ae8..6a428d6cf494 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -3675,6 +3675,13 @@</span> <span class="p_context"> B:	https://bugzilla.kernel.org</span>
 F:	drivers/cpuidle/*
 F:	include/linux/cpuidle.h
 
<span class="p_add">+CPU NON-PREEMPTIBLE OPERATION VECTOR SUPPORT</span>
<span class="p_add">+M:	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+L:	linux-kernel@vger.kernel.org</span>
<span class="p_add">+S:	Supported</span>
<span class="p_add">+F:	kernel/cpu_opv.c</span>
<span class="p_add">+F:	include/uapi/linux/cpu_opv.h</span>
<span class="p_add">+</span>
 CRAMFS FILESYSTEM
 W:	http://sourceforge.net/projects/cramfs/
 S:	Orphan / Obsolete
<span class="p_header">diff --git a/include/uapi/linux/cpu_opv.h b/include/uapi/linux/cpu_opv.h</span>
new file mode 100644
<span class="p_header">index 000000000000..17f7d46e053b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/uapi/linux/cpu_opv.h</span>
<span class="p_chunk">@@ -0,0 +1,117 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+#define _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * linux/cpu_opv.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * CPU preempt-off operation vector system call API</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2017 Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="p_add">+ * of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="p_add">+ * in the Software without restriction, including without limitation the rights</span>
<span class="p_add">+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="p_add">+ * copies of the Software, and to permit persons to whom the Software is</span>
<span class="p_add">+ * furnished to do so, subject to the following conditions:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The above copyright notice and this permission notice shall be included in</span>
<span class="p_add">+ * all copies or substantial portions of the Software.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="p_add">+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="p_add">+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="p_add">+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="p_add">+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="p_add">+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="p_add">+ * SOFTWARE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+# include &lt;linux/types.h&gt;</span>
<span class="p_add">+#else	/* #ifdef __KERNEL__ */</span>
<span class="p_add">+# include &lt;stdint.h&gt;</span>
<span class="p_add">+#endif	/* #else #ifdef __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __LP64__</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)			uint64_t field</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	field = (intptr_t)v</span>
<span class="p_add">+#elif defined(__BYTE_ORDER) ? \</span>
<span class="p_add">+	__BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t field ## _padding, field</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="p_add">+	field ## _padding = 0, field = (intptr_t)v</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t field, field ## _padding</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64_INIT_ONSTACK(field, v)	\</span>
<span class="p_add">+	field = (intptr_t)v, field ## _padding = 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_OP_VEC_LEN_MAX		16</span>
<span class="p_add">+#define CPU_OP_ARG_LEN_MAX		24</span>
<span class="p_add">+/* Max. data len per operation. */</span>
<span class="p_add">+#define CPU_OP_DATA_LEN_MAX		PAGE_SIZE</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Max. data len for overall vector. We to restrict the amount of</span>
<span class="p_add">+ * user-space data touched by the kernel in non-preemptible context so</span>
<span class="p_add">+ * we do not introduce long scheduler latencies.</span>
<span class="p_add">+ * This allows one copy of up to 4096 bytes, and 15 operations touching</span>
<span class="p_add">+ * 8 bytes each.</span>
<span class="p_add">+ * This limit is applied to the sum of length specified for all</span>
<span class="p_add">+ * operations in a vector.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define CPU_OP_VEC_DATA_LEN_MAX		(4096 + 15*8)</span>
<span class="p_add">+#define CPU_OP_MAX_PAGES		4	/* Max. pages per op. */</span>
<span class="p_add">+</span>
<span class="p_add">+enum cpu_op_type {</span>
<span class="p_add">+	CPU_COMPARE_EQ_OP,	/* compare */</span>
<span class="p_add">+	CPU_COMPARE_NE_OP,	/* compare */</span>
<span class="p_add">+	CPU_MEMCPY_OP,		/* memcpy */</span>
<span class="p_add">+	CPU_ADD_OP,		/* arithmetic */</span>
<span class="p_add">+	CPU_OR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_AND_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_XOR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_LSHIFT_OP,		/* shift */</span>
<span class="p_add">+	CPU_RSHIFT_OP,		/* shift */</span>
<span class="p_add">+	CPU_MB_OP,		/* memory barrier */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Vector of operations to perform. Limited to 16. */</span>
<span class="p_add">+struct cpu_op {</span>
<span class="p_add">+	int32_t op;	/* enum cpu_op_type. */</span>
<span class="p_add">+	uint32_t len;	/* data length, in bytes. */</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(a);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(b);</span>
<span class="p_add">+			uint8_t expect_fault_a;</span>
<span class="p_add">+			uint8_t expect_fault_b;</span>
<span class="p_add">+		} compare_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(dst);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(src);</span>
<span class="p_add">+			uint8_t expect_fault_dst;</span>
<span class="p_add">+			uint8_t expect_fault_src;</span>
<span class="p_add">+		} memcpy_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			int64_t count;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} arithmetic_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint64_t mask;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} bitwise_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint32_t bits;</span>
<span class="p_add">+			uint8_t expect_fault_p;</span>
<span class="p_add">+		} shift_op;</span>
<span class="p_add">+		char __padding[CPU_OP_ARG_LEN_MAX];</span>
<span class="p_add">+	} u;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_LINUX_CPU_OPV_H */</span>
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index cbedfb91b40a..e4fbb5dd6a24 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -1404,6 +1404,7 @@</span> <span class="p_context"> config RSEQ</span>
 	bool &quot;Enable rseq() system call&quot; if EXPERT
 	default y
 	depends on HAVE_RSEQ
<span class="p_add">+	select CPU_OPV</span>
 	select MEMBARRIER
 	help
 	  Enable the restartable sequences system call. It provides a
<span class="p_chunk">@@ -1414,6 +1415,19 @@</span> <span class="p_context"> config RSEQ</span>
 
 	  If unsure, say Y.
 
<span class="p_add">+config CPU_OPV</span>
<span class="p_add">+	bool &quot;Enable cpu_opv() system call&quot; if EXPERT</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable the CPU preempt-off operation vector system call.</span>
<span class="p_add">+	  It allows user-space to perform a sequence of operations on</span>
<span class="p_add">+	  per-cpu data with preemption disabled. Useful as</span>
<span class="p_add">+	  single-stepping fall-back for restartable sequences, and for</span>
<span class="p_add">+	  performing more complex operations on per-cpu data that would</span>
<span class="p_add">+	  not be otherwise possible to do with restartable sequences.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say Y.</span>
<span class="p_add">+</span>
 config EMBEDDED
 	bool &quot;Embedded system&quot;
 	option allnoconfig_y
<span class="p_header">diff --git a/kernel/Makefile b/kernel/Makefile</span>
<span class="p_header">index 3574669dafd9..cac8855196ff 100644</span>
<span class="p_header">--- a/kernel/Makefile</span>
<span class="p_header">+++ b/kernel/Makefile</span>
<span class="p_chunk">@@ -113,6 +113,7 @@</span> <span class="p_context"> obj-$(CONFIG_TORTURE_TEST) += torture.o</span>
 
 obj-$(CONFIG_HAS_IOMEM) += memremap.o
 obj-$(CONFIG_RSEQ) += rseq.o
<span class="p_add">+obj-$(CONFIG_CPU_OPV) += cpu_opv.o</span>
 
 $(obj)/configs.o: $(obj)/config_data.h
 
<span class="p_header">diff --git a/kernel/cpu_opv.c b/kernel/cpu_opv.c</span>
new file mode 100644
<span class="p_header">index 000000000000..09754bbe6a4f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/kernel/cpu_opv.c</span>
<span class="p_chunk">@@ -0,0 +1,952 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * CPU preempt-off operation vector system call</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It allows user-space to perform a sequence of operations on per-cpu</span>
<span class="p_add">+ * data with preemption disabled. Useful as single-stepping fall-back</span>
<span class="p_add">+ * for restartable sequences, and for performing more complex operations</span>
<span class="p_add">+ * on per-cpu data that would not be otherwise possible to do with</span>
<span class="p_add">+ * restartable sequences.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2017, EfficiOS Inc.,</span>
<span class="p_add">+ * Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/syscalls.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpu_opv.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mutex.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;sched/sched.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define TMP_BUFLEN			64</span>
<span class="p_add">+#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="p_add">+</span>
<span class="p_add">+union op_fn_data {</span>
<span class="p_add">+	uint8_t _u8;</span>
<span class="p_add">+	uint16_t _u16;</span>
<span class="p_add">+	uint32_t _u32;</span>
<span class="p_add">+	uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+	uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+typedef int (*op_fn_t)(union op_fn_data *data, uint64_t v, uint32_t len);</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_MUTEX(cpu_opv_offline_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="p_add">+ * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="p_add">+ * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="p_add">+ * array as argument.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="p_add">+ * left shift, and right shift. The system call receives a CPU number</span>
<span class="p_add">+ * from user-space as argument, which is the CPU on which those</span>
<span class="p_add">+ * operations need to be performed. All preparation steps such as</span>
<span class="p_add">+ * loading pointers, and applying offsets to arrays, need to be</span>
<span class="p_add">+ * performed by user-space before invoking the system call. The</span>
<span class="p_add">+ * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="p_add">+ * preparation step did not change between preparation of system call</span>
<span class="p_add">+ * inputs and operation execution within the preempt-off critical</span>
<span class="p_add">+ * section.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * The reason why we require all pointer offsets to be calculated by</span>
<span class="p_add">+ * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="p_add">+ * to first pin all pages touched by each operation. This takes care of</span>
<span class="p_add">+ * faulting-in the pages. Then, preemption is disabled, and the</span>
<span class="p_add">+ * operations are performed atomically with respect to other thread</span>
<span class="p_add">+ * execution on that CPU, without generating any page fault.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="p_add">+ * enforced, and a overall maximum length sum, so user-space cannot</span>
<span class="p_add">+ * generate a too long preempt-off critical section. Each operation is</span>
<span class="p_add">+ * also limited a length of PAGE_SIZE bytes, meaning that an operation</span>
<span class="p_add">+ * can touch a maximum of 4 pages (memcpy: 2 pages for source, 2 pages</span>
<span class="p_add">+ * for destination if addresses are not aligned on page boundaries).</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * If the thread is not running on the requested CPU, a new</span>
<span class="p_add">+ * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="p_add">+ * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="p_add">+ * the thread, the system call fails with EINVAL. After the migration</span>
<span class="p_add">+ * has been performed, preemption is disabled, and the current CPU</span>
<span class="p_add">+ * number is checked again and compared to the requested CPU number. If</span>
<span class="p_add">+ * it still differs, it means the scheduler migrated us away from that</span>
<span class="p_add">+ * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="p_add">+ * retry (either requesting the same CPU number, or a different one,</span>
<span class="p_add">+ * depending on the user-space algorithm constraints).</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check operation types and length parameters.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	uint32_t sum = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			sum += op-&gt;len;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			if (op-&gt;len &gt; CPU_OP_DATA_LEN_MAX)</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 7)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 15)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 31)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 63)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (sum &gt; CPU_OP_VEC_DATA_LEN_MAX)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="p_add">+		unsigned long len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_op_check_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_zone_device_page(page))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	page = compound_head(page);</span>
<span class="p_add">+	mapping = READ_ONCE(page-&gt;mapping);</span>
<span class="p_add">+	if (!mapping) {</span>
<span class="p_add">+		int shmem_swizzled;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Check again with page lock held to guard against</span>
<span class="p_add">+		 * memory pressure making shmem_writepage move the page</span>
<span class="p_add">+		 * from filecache to swapcache.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		lock_page(page);</span>
<span class="p_add">+		shmem_swizzled = PageSwapCache(page) || page-&gt;mapping;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		if (shmem_swizzled)</span>
<span class="p_add">+			return -EAGAIN;</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Refusing device pages, the zero page, pages in the gate area, and</span>
<span class="p_add">+ * special mappings. Inspired from futex.c checks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpu_op_check_pages(struct page **pages,</span>
<span class="p_add">+		unsigned long nr_pages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nr_pages; i++) {</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = cpu_op_check_page(pages[i]);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		struct page ***pinned_pages_ptr, size_t *nr_pinned,</span>
<span class="p_add">+		int write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *pages[2];</span>
<span class="p_add">+	int ret, nr_pages;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!len)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="p_add">+	BUG_ON(nr_pages &gt; 2);</span>
<span class="p_add">+	if (*nr_pinned + nr_pages &gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="p_add">+		struct page **pinned_pages =</span>
<span class="p_add">+			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="p_add">+				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="p_add">+		if (!pinned_pages)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		memcpy(pinned_pages, *pinned_pages_ptr,</span>
<span class="p_add">+			*nr_pinned * sizeof(struct page *));</span>
<span class="p_add">+		*pinned_pages_ptr = pinned_pages;</span>
<span class="p_add">+	}</span>
<span class="p_add">+again:</span>
<span class="p_add">+	ret = get_user_pages_fast(addr, nr_pages, write, pages);</span>
<span class="p_add">+	if (ret &lt; nr_pages) {</span>
<span class="p_add">+		if (ret &gt; 0)</span>
<span class="p_add">+			put_page(pages[0]);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Refuse device pages, the zero page, pages in the gate area,</span>
<span class="p_add">+	 * and special mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ret = cpu_op_check_pages(pages, nr_pages);</span>
<span class="p_add">+	if (ret == -EAGAIN) {</span>
<span class="p_add">+		put_page(pages[0]);</span>
<span class="p_add">+		if (nr_pages &gt; 1)</span>
<span class="p_add">+			put_page(pages[1]);</span>
<span class="p_add">+		goto again;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto error;</span>
<span class="p_add">+	(*pinned_pages_ptr)[(*nr_pinned)++] = pages[0];</span>
<span class="p_add">+	if (nr_pages &gt; 1)</span>
<span class="p_add">+		(*pinned_pages_ptr)[(*nr_pinned)++] = pages[1];</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	put_page(pages[0]);</span>
<span class="p_add">+	if (nr_pages &gt; 1)</span>
<span class="p_add">+		put_page(pages[1]);</span>
<span class="p_add">+	return -EFAULT;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="p_add">+		struct page ***pinned_pages_ptr, size_t *nr_pinned)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+	bool expect_fault = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check access, pin pages. */</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.compare_op.expect_fault_a;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.compare_op.expect_fault_b;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.memcpy_op.expect_fault_dst;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.memcpy_op.expect_fault_src;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 0);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.arithmetic_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.bitwise_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			expect_fault = op-&gt;u.shift_op.expect_fault_p;</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned, 1);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	for (i = 0; i &lt; *nr_pinned; i++)</span>
<span class="p_add">+		put_page((*pinned_pages_ptr)[i]);</span>
<span class="p_add">+	*nr_pinned = 0;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If faulting access is expected, return EAGAIN to user-space.</span>
<span class="p_add">+	 * It allows user-space to distinguish between a fault caused by</span>
<span class="p_add">+	 * an access which is expect to fault (e.g. due to concurrent</span>
<span class="p_add">+	 * unmapping of underlying memory) from an unexpected fault from</span>
<span class="p_add">+	 * which a retry would not recover.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (ret == -EFAULT &amp;&amp; expect_fault)</span>
<span class="p_add">+		return -EAGAIN;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t compared = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (compared != len) {</span>
<span class="p_add">+		unsigned long to_compare;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (memcmp(bufa, bufb, to_compare))</span>
<span class="p_add">+			return 1;	/* different */</span>
<span class="p_add">+		compared += to_compare;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;	/* same */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_compare_iter(a, b, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy_iter(void __user *dst, void __user *src,</span>
<span class="p_add">+		uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t copied = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (copied != len) {</span>
<span class="p_add">+		unsigned long to_copy;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_copy = min_t(uint32_t, TMP_BUFLEN, len - copied);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(buf, src + copied, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_to_user_inatomic(dst + copied, buf, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		copied += to_copy;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy(void __user *dst, void __user *src, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)src + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)dst + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_memcpy_iter(dst, src, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_add_fn(union op_fn_data *data, uint64_t count, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 += (uint8_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 += (uint16_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 += (uint32_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 += (uint64_t)count;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_or_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 |= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 |= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 |= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 |= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_and_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &amp;= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &amp;= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &amp;= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &amp;= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_xor_fn(union op_fn_data *data, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 ^= (uint8_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 ^= (uint16_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 ^= (uint32_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 ^= (uint64_t)mask;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_lshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &lt;&lt;= (uint8_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &lt;&lt;= (uint16_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &lt;&lt;= (uint32_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &lt;&lt;= (uint64_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int op_rshift_fn(union op_fn_data *data, uint64_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		data-&gt;_u8 &gt;&gt;= (uint8_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		data-&gt;_u16 &gt;&gt;= (uint16_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		data-&gt;_u32 &gt;&gt;= (uint32_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+		data-&gt;_u64 &gt;&gt;= (uint64_t)bits;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_fn(op_fn_t op_fn, void __user *p, uint64_t v,</span>
<span class="p_add">+		uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union op_fn_data tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		if (op_fn(&amp;tmp, v, len))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Guarantee a compiler barrier between each operation. */</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * differs.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret &gt; 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * is identical.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret == 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			ret = do_cpu_op_memcpy(</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_add_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;u.arithmetic_op.count, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_or_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_and_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_xor_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_lshift_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_fn(op_rshift_fn,</span>
<span class="p_add">+					(void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MB_OP:</span>
<span class="p_add">+			smp_mb();</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu != raw_smp_processor_id()) {</span>
<span class="p_add">+		ret = push_task_to_cpu(current, cpu);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto check_online;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	if (cpu != smp_processor_id()) {</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+check_online:</span>
<span class="p_add">+	if (!cpu_possible(cpu))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	get_online_cpus();</span>
<span class="p_add">+	if (cpu_online(cpu)) {</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		goto put_online_cpus;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * CPU is offline. Perform operation from the current CPU with</span>
<span class="p_add">+	 * cpu_online read lock held, preventing that CPU from coming online,</span>
<span class="p_add">+	 * and with mutex held, providing mutual exclusion against other</span>
<span class="p_add">+	 * CPUs also finding out about an offline CPU.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mutex_lock(&amp;cpu_opv_offline_lock);</span>
<span class="p_add">+	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="p_add">+	mutex_unlock(&amp;cpu_opv_offline_lock);</span>
<span class="p_add">+put_online_cpus:</span>
<span class="p_add">+	put_online_cpus();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="p_add">+ * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="p_add">+ */</span>
<span class="p_add">+SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="p_add">+		int, cpu, int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="p_add">+	struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="p_add">+	struct page **pinned_pages = pinned_pages_on_stack;</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+	size_t nr_pinned = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(flags))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (unlikely(cpu &lt; 0))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	ret = cpu_opv_pin_pages(cpuopv, cpuopcnt,</span>
<span class="p_add">+				&amp;pinned_pages, &amp;nr_pinned);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="p_add">+	for (i = 0; i &lt; nr_pinned; i++)</span>
<span class="p_add">+		put_page(pinned_pages[i]);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	if (pinned_pages != pinned_pages_on_stack)</span>
<span class="p_add">+		kfree(pinned_pages);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 6bba05f47e51..e547f93a46c2 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -1052,6 +1052,43 @@</span> <span class="p_context"> void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
 		set_curr_task(rq, p);
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
<span class="p_add">+	update_rq_clock(rq);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpumask_test_cpu(dest_cpu, &amp;p-&gt;cpus_allowed)) {</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_cpu(p) == dest_cpu)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {</span>
<span class="p_add">+		struct migration_arg arg = { p, dest_cpu };</span>
<span class="p_add">+		/* Need help from migration thread: drop lock and wait. */</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);</span>
<span class="p_add">+		tlb_migrate_finish(p-&gt;mm);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	} else if (task_on_rq_queued(p)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * OK, since we&#39;re going to drop the lock immediately</span>
<span class="p_add">+		 * afterwards anyway.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		rq = move_queued_task(rq, &amp;rf, p, dest_cpu);</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Change a given task&#39;s CPU affinity. Migrate the thread to a
  * proper CPU and schedule it away if the CPU it&#39;s executing on
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index 3b448ba82225..cab256c1720a 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -1209,6 +1209,8 @@</span> <span class="p_context"> static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)</span>
 #endif
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>
<span class="p_add">+</span>
 /*
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
<span class="p_header">diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c</span>
<span class="p_header">index bfa1ee1bf669..59e622296dc3 100644</span>
<span class="p_header">--- a/kernel/sys_ni.c</span>
<span class="p_header">+++ b/kernel/sys_ni.c</span>
<span class="p_chunk">@@ -262,3 +262,4 @@</span> <span class="p_context"> cond_syscall(sys_pkey_free);</span>
 
 /* restartable sequence */
 cond_syscall(sys_rseq);
<span class="p_add">+cond_syscall(sys_cpu_opv);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



