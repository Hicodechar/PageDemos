
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,1/2] s390x: mm: allow mixed page table types (2k and 4k) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,1/2] s390x: mm: allow mixed page table types (2k and 4k)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171221">David Hildenbrand</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 29, 2017, 4:32 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170529163202.13077-2-david@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9753459/mbox/"
   >mbox</a>
|
   <a href="/patch/9753459/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9753459/">/patch/9753459/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EA3B26038E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 29 May 2017 16:32:17 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D8D1226E4E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 29 May 2017 16:32:17 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id CD7D0283C0; Mon, 29 May 2017 16:32:17 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E56D826E4E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 29 May 2017 16:32:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751144AbdE2QcM (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 29 May 2017 12:32:12 -0400
Received: from mx1.redhat.com ([209.132.183.28]:42160 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750846AbdE2QcJ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 29 May 2017 12:32:09 -0400
Received: from smtp.corp.redhat.com
	(int-mx05.intmail.prod.int.phx2.redhat.com [10.5.11.15])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 8A55785545;
	Mon, 29 May 2017 16:32:08 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 8A55785545
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx04.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=david@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 8A55785545
Received: from t460s.redhat.com (ovpn-117-161.ams2.redhat.com
	[10.36.117.161])
	by smtp.corp.redhat.com (Postfix) with ESMTP id CA4E81714A;
	Mon, 29 May 2017 16:32:06 +0000 (UTC)
From: David Hildenbrand &lt;david@redhat.com&gt;
To: kvm@vger.kernel.org
Cc: linux-kernel@vger.kernel.org,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;,
	Thomas Huth &lt;thuth@redhat.com&gt;, david@redhat.com,
	Christian Borntraeger &lt;borntraeger@de.ibm.com&gt;
Subject: [PATCH RFC 1/2] s390x: mm: allow mixed page table types (2k and 4k)
Date: Mon, 29 May 2017 18:32:01 +0200
Message-Id: &lt;20170529163202.13077-2-david@redhat.com&gt;
In-Reply-To: &lt;20170529163202.13077-1-david@redhat.com&gt;
References: &lt;20170529163202.13077-1-david@redhat.com&gt;
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.15
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.28]);
	Mon, 29 May 2017 16:32:08 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171221">David Hildenbrand</a> - May 29, 2017, 4:32 p.m.</div>
<pre class="content">
For now, one has to globally enable vm.alloc_pgste in order to run
KVM guests. This results in any process getting 4k page tables instead of
only 2k page tables.

As the decision about which page table type to use has to be made at fork
time, there isn&#39;t much we can do to avoid this global system setting.
However, by allowing mixed page table types for one process, we can
simply turn on allocation of 4k page tables with pgstes at one point
and rely on the caller to make sure that everything getting mapped into
the gmap will be based on 4k page tables allocated/mmaped after the
switch to 4k page table allocation. If we detect some incompatible
page table, we will handle this as an ordinary fault, indicating -EFAULT.

Background: QEMU will in general create the KVM VM before mmap-ing memory
that will get part of the guest address space. Every mmap will create
new page tables. To not break some legacy/other user space doing
this in a different order, this will have to be enabled explicitly by
the caller. So the vm.alloc_pgste option has to stay.
<span class="signed-off-by">
Signed-off-by: David Hildenbrand &lt;david@redhat.com&gt;</span>
---
 arch/s390/include/asm/pgtable.h | 15 +++++++++--
 arch/s390/kvm/kvm-s390.c        |  4 +--
 arch/s390/mm/gmap.c             | 11 +++++---
 arch/s390/mm/pgalloc.c          | 14 +++++-----
 arch/s390/mm/pgtable.c          | 59 ++++++++++++++++++++++++++++++++++-------
 5 files changed, 81 insertions(+), 22 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=447">Christian Borntraeger</a> - June 1, 2017, 11:39 a.m.</div>
<pre class="content">
On 05/29/2017 06:32 PM, David Hildenbrand wrote:
<span class="quote">
&gt;  	new = old = pgste_get_lock(ptep);</span>
<span class="quote">&gt;  	pgste_val(new) &amp;= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |</span>
<span class="quote">&gt; @@ -748,6 +764,11 @@ int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt;  	ptep = get_locked_pte(mm, addr, &amp;ptl);</span>
<span class="quote">&gt;  	if (unlikely(!ptep))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; +	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt; +		pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; +		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>

All these WARN_ONCE. Is there a way how a malicious user can trigger this or is this checked
everywhere and triggered would be indeed a bug?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171221">David Hildenbrand</a> - June 1, 2017, 12:44 p.m.</div>
<pre class="content">
On 01.06.2017 13:39, Christian Borntraeger wrote:
<span class="quote">&gt; On 05/29/2017 06:32 PM, David Hildenbrand wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;  	new = old = pgste_get_lock(ptep);</span>
<span class="quote">&gt;&gt;  	pgste_val(new) &amp;= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |</span>
<span class="quote">&gt;&gt; @@ -748,6 +764,11 @@ int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt;&gt;  	ptep = get_locked_pte(mm, addr, &amp;ptl);</span>
<span class="quote">&gt;&gt;  	if (unlikely(!ptep))</span>
<span class="quote">&gt;&gt;  		return -EFAULT;</span>
<span class="quote">&gt;&gt; +	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt;&gt; +		pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;&gt; +		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; All these WARN_ONCE. Is there a way how a malicious user can trigger this or is this checked</span>
<span class="quote">&gt; everywhere and triggered would be indeed a bug?</span>

Very good question I added these for testing purposes, but leaving the
WARN_ONCE here is wrong.

The user can create memslots with &quot;wrong&quot; memory. Whenever such memory
is linked into the gmap, we return -EFAULT. So we will only have page
table with &quot;pgstes&quot; in our GMAP at any time.

However, all these functions here go via memslots:

test_and_clear_guest_dirty
-&gt; via memslot from memslot list

set_guest_storage_key
reset_guest_reference_bit
get_guest_storage_key
pgste_perform_essa
set_pgste_bits
get_pgste
-&gt;  come via gfn_to_hva() -&gt; gfn_to_memslot() -&gt; search_memslots -&gt; via
memslot list


And then use the calculated host address to just walk the ordinary
process page tables (get_locked_pte) and not the pgste.

So simply returning -EFAULT here is the right thing to, dropping the
WARN_ONCE.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171221">David Hildenbrand</a> - June 1, 2017, 12:59 p.m.</div>
<pre class="content">
<span class="quote">&gt; diff --git a/arch/s390/mm/pgtable.c b/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt; index d4d409b..b22c2b6 100644</span>
<span class="quote">&gt; --- a/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt; +++ b/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt; @@ -196,7 +196,7 @@ static inline pgste_t ptep_xchg_start(struct mm_struct *mm,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgste_t pgste = __pgste(0);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (mm_has_pgste(mm)) {</span>
<span class="quote">&gt; +	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt;  		pgste = pgste_get_lock(ptep);</span>
<span class="quote">&gt;  		pgste = pgste_pte_notify(mm, addr, ptep, pgste);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -207,7 +207,7 @@ static inline pte_t ptep_xchg_commit(struct mm_struct *mm,</span>
<span class="quote">&gt;  				    unsigned long addr, pte_t *ptep,</span>
<span class="quote">&gt;  				    pgste_t pgste, pte_t old, pte_t new)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (mm_has_pgste(mm)) {</span>
<span class="quote">&gt; +	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt;  		if (pte_val(old) &amp; _PAGE_INVALID)</span>
<span class="quote">&gt;  			pgste_set_key(ptep, pgste, new, mm);</span>
<span class="quote">&gt;  		if (pte_val(new) &amp; _PAGE_INVALID) </span>

I think these two checks are wrong. We really have to test here the
mapcount bit only (relying on mm_has_pgste(mm) is wrong in case global
vm.allocate_pgste ist set).

But before I continue working on this, I think it makes sense to clarify
if something like that would be acceptable at all.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=447">Christian Borntraeger</a> - June 2, 2017, 7:11 a.m.</div>
<pre class="content">
On 06/01/2017 02:59 PM, David Hildenbrand wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/s390/mm/pgtable.c b/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt;&gt; index d4d409b..b22c2b6 100644</span>
<span class="quote">&gt;&gt; --- a/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt;&gt; +++ b/arch/s390/mm/pgtable.c</span>
<span class="quote">&gt;&gt; @@ -196,7 +196,7 @@ static inline pgste_t ptep_xchg_start(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	pgste_t pgste = __pgste(0);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (mm_has_pgste(mm)) {</span>
<span class="quote">&gt;&gt; +	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt;&gt;  		pgste = pgste_get_lock(ptep);</span>
<span class="quote">&gt;&gt;  		pgste = pgste_pte_notify(mm, addr, ptep, pgste);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt; @@ -207,7 +207,7 @@ static inline pte_t ptep_xchg_commit(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;  				    unsigned long addr, pte_t *ptep,</span>
<span class="quote">&gt;&gt;  				    pgste_t pgste, pte_t old, pte_t new)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -	if (mm_has_pgste(mm)) {</span>
<span class="quote">&gt;&gt; +	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="quote">&gt;&gt;  		if (pte_val(old) &amp; _PAGE_INVALID)</span>
<span class="quote">&gt;&gt;  			pgste_set_key(ptep, pgste, new, mm);</span>
<span class="quote">&gt;&gt;  		if (pte_val(new) &amp; _PAGE_INVALID) </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think these two checks are wrong. We really have to test here the</span>
<span class="quote">&gt; mapcount bit only (relying on mm_has_pgste(mm) is wrong in case global</span>
<span class="quote">&gt; vm.allocate_pgste ist set).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But before I continue working on this, I think it makes sense to clarify</span>
<span class="quote">&gt; if something like that would be acceptable at all.</span>

I think that is up to Martin to decide. Given the fact that Fedora, SUSE, Ubuntu always
enable this sysctl when the qemu package is installed (other distros as well?) I think 
that we should really think about changing things. I see 2 options:

1. dropping 2k page tables completely
pro:	- simplifies pagetable code (e.g. look at page_table_alloc code)
	- we could get rid of a lock in the pgtable allocation path (mm-&gt;context.pgtable_lock)
	- I am not aware of any performance impact due to the 4k page tables
	- transparent for old QEMUs
	- KVM will work out of the box
con: 	- higher page table memory usage for non-KVM processes

2. go with your approach
pro: 	- lower page table memory usage for non-KVM processes
	- KVM will work out of the box
	- no addtl overhead for non-KVM processes
con:	- higher overhead for KVM processes during paging (since we are going to use IPTE
	or friends anyway, the question is: does it matter?)
	- needs QEMU change

Christian
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/s390/include/asm/pgtable.h b/arch/s390/include/asm/pgtable.h</span>
<span class="p_header">index 3effb26..0fb6d29 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -1089,6 +1089,17 @@</span> <span class="p_context"> int get_pgste(struct mm_struct *mm, unsigned long hva, unsigned long *pgstep);</span>
 int pgste_perform_essa(struct mm_struct *mm, unsigned long hva, int orc,
 			unsigned long *oldpte, unsigned long *oldpgste);
 
<span class="p_add">+static inline int pgtable_has_pgste(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm_has_pgste(mm))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pfn_to_page(addr &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+	return atomic_read(&amp;page-&gt;_mapcount) &amp; 0x4U;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Certain architectures need to do special things when PTEs
  * within a page table are directly modified.  Thus, the following
<span class="p_chunk">@@ -1101,7 +1112,7 @@</span> <span class="p_context"> static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 		pte_val(entry) &amp;= ~_PAGE_NOEXEC;
 	if (pte_present(entry))
 		pte_val(entry) &amp;= ~_PAGE_UNUSED;
<span class="p_del">-	if (mm_has_pgste(mm))</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(ptep)))</span>
 		ptep_set_pte_at(mm, addr, ptep, entry);
 	else
 		*ptep = entry;
<span class="p_chunk">@@ -1541,7 +1552,7 @@</span> <span class="p_context"> static inline swp_entry_t __swp_entry(unsigned long type, unsigned long offset)</span>
 
 extern int vmem_add_mapping(unsigned long start, unsigned long size);
 extern int vmem_remove_mapping(unsigned long start, unsigned long size);
<span class="p_del">-extern int s390_enable_sie(void);</span>
<span class="p_add">+extern int s390_enable_sie(bool mixed_pgtables);</span>
 extern int s390_enable_skey(void);
 extern void s390_reset_cmma(struct mm_struct *mm);
 
<span class="p_header">diff --git a/arch/s390/kvm/kvm-s390.c b/arch/s390/kvm/kvm-s390.c</span>
<span class="p_header">index 4ef3035..89684bb 100644</span>
<span class="p_header">--- a/arch/s390/kvm/kvm-s390.c</span>
<span class="p_header">+++ b/arch/s390/kvm/kvm-s390.c</span>
<span class="p_chunk">@@ -354,7 +354,7 @@</span> <span class="p_context"> long kvm_arch_dev_ioctl(struct file *filp,</span>
 			unsigned int ioctl, unsigned long arg)
 {
 	if (ioctl == KVM_S390_ENABLE_SIE)
<span class="p_del">-		return s390_enable_sie();</span>
<span class="p_add">+		return s390_enable_sie(false);</span>
 	return -EINVAL;
 }
 
<span class="p_chunk">@@ -1808,7 +1808,7 @@</span> <span class="p_context"> int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)</span>
 		goto out_err;
 #endif
 
<span class="p_del">-	rc = s390_enable_sie();</span>
<span class="p_add">+	rc = s390_enable_sie(false);</span>
 	if (rc)
 		goto out_err;
 
<span class="p_header">diff --git a/arch/s390/mm/gmap.c b/arch/s390/mm/gmap.c</span>
<span class="p_header">index 4fb3d3c..ff24ada 100644</span>
<span class="p_header">--- a/arch/s390/mm/gmap.c</span>
<span class="p_header">+++ b/arch/s390/mm/gmap.c</span>
<span class="p_chunk">@@ -586,6 +586,9 @@</span> <span class="p_context"> int __gmap_link(struct gmap *gmap, unsigned long gaddr, unsigned long vmaddr)</span>
 	/* large pmds cannot yet be handled */
 	if (pmd_large(*pmd))
 		return -EFAULT;
<span class="p_add">+	/* only page tables with pgstes can be linked into a gmap */</span>
<span class="p_add">+	if (!pgtable_has_pgste(mm, pmd_pfn(*pmd) &lt;&lt; PAGE_SHIFT))</span>
<span class="p_add">+		return -EFAULT;</span>
 	/* Link gmap segment table entry location to page table. */
 	rc = radix_tree_preload(GFP_KERNEL);
 	if (rc)
<span class="p_chunk">@@ -2123,17 +2126,19 @@</span> <span class="p_context"> static inline void thp_split_mm(struct mm_struct *mm)</span>
 /*
  * switch on pgstes for its userspace process (for kvm)
  */
<span class="p_del">-int s390_enable_sie(void)</span>
<span class="p_add">+int s390_enable_sie(bool mixed_pgtables)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 
 	/* Do we have pgstes? if yes, we are done */
 	if (mm_has_pgste(mm))
 		return 0;
<span class="p_del">-	/* Fail if the page tables are 2K */</span>
<span class="p_del">-	if (!mm_alloc_pgste(mm))</span>
<span class="p_add">+	/* Fail if the page tables are 2K and mixed pgtables are disabled */</span>
<span class="p_add">+	if (!mixed_pgtables &amp;&amp; !mm_alloc_pgste(mm))</span>
 		return -EINVAL;
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+	/* allocate page tables with pgste from now on if not already done */</span>
<span class="p_add">+	mm-&gt;context.alloc_pgste = 1;</span>
 	mm-&gt;context.has_pgste = 1;
 	/* split thp mappings and disable thp for future mappings */
 	thp_split_mm(mm);
<span class="p_header">diff --git a/arch/s390/mm/pgalloc.c b/arch/s390/mm/pgalloc.c</span>
<span class="p_header">index 18918e3..2790473 100644</span>
<span class="p_header">--- a/arch/s390/mm/pgalloc.c</span>
<span class="p_header">+++ b/arch/s390/mm/pgalloc.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> unsigned long *page_table_alloc(struct mm_struct *mm)</span>
 	table = (unsigned long *) page_to_phys(page);
 	if (mm_alloc_pgste(mm)) {
 		/* Return 4K page table with PGSTEs */
<span class="p_del">-		atomic_set(&amp;page-&gt;_mapcount, 3);</span>
<span class="p_add">+		atomic_set(&amp;page-&gt;_mapcount, 4);</span>
 		clear_table(table, _PAGE_INVALID, PAGE_SIZE/2);
 		clear_table(table + PTRS_PER_PTE, 0, PAGE_SIZE/2);
 	} else {
<span class="p_chunk">@@ -238,7 +238,7 @@</span> <span class="p_context"> void page_table_free(struct mm_struct *mm, unsigned long *table)</span>
 	unsigned int bit, mask;
 
 	page = pfn_to_page(__pa(table) &gt;&gt; PAGE_SHIFT);
<span class="p_del">-	if (!mm_alloc_pgste(mm)) {</span>
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(table))) {</span>
 		/* Free 2K page table fragment of a 4K page */
 		bit = (__pa(table) &amp; ~PAGE_MASK)/(PTRS_PER_PTE*sizeof(pte_t));
 		spin_lock_bh(&amp;mm-&gt;context.pgtable_lock);
<span class="p_chunk">@@ -266,9 +266,9 @@</span> <span class="p_context"> void page_table_free_rcu(struct mmu_gather *tlb, unsigned long *table,</span>
 
 	mm = tlb-&gt;mm;
 	page = pfn_to_page(__pa(table) &gt;&gt; PAGE_SHIFT);
<span class="p_del">-	if (mm_alloc_pgste(mm)) {</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(table))) {</span>
 		gmap_unlink(mm, table, vmaddr);
<span class="p_del">-		table = (unsigned long *) (__pa(table) | 3);</span>
<span class="p_add">+		table = (unsigned long *) (__pa(table) | 4);</span>
 		tlb_remove_table(tlb, table);
 		return;
 	}
<span class="p_chunk">@@ -286,7 +286,7 @@</span> <span class="p_context"> void page_table_free_rcu(struct mmu_gather *tlb, unsigned long *table,</span>
 
 static void __tlb_remove_table(void *_table)
 {
<span class="p_del">-	unsigned int mask = (unsigned long) _table &amp; 3;</span>
<span class="p_add">+	unsigned int mask = (unsigned long) _table &amp; 7;</span>
 	void *table = (void *)((unsigned long) _table ^ mask);
 	struct page *page = pfn_to_page(__pa(table) &gt;&gt; PAGE_SHIFT);
 
<span class="p_chunk">@@ -299,11 +299,13 @@</span> <span class="p_context"> static void __tlb_remove_table(void *_table)</span>
 		if (atomic_xor_bits(&amp;page-&gt;_mapcount, mask &lt;&lt; 4) != 0)
 			break;
 		/* fallthrough */
<span class="p_del">-	case 3:		/* 4K page table with pgstes */</span>
<span class="p_add">+	case 4:		/* 4K page table with pgstes */</span>
 		pgtable_page_dtor(page);
 		atomic_set(&amp;page-&gt;_mapcount, -1);
 		__free_page(page);
 		break;
<span class="p_add">+	default:</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Unknown table type: %x&quot;, mask);</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/s390/mm/pgtable.c b/arch/s390/mm/pgtable.c</span>
<span class="p_header">index d4d409b..b22c2b6 100644</span>
<span class="p_header">--- a/arch/s390/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/s390/mm/pgtable.c</span>
<span class="p_chunk">@@ -196,7 +196,7 @@</span> <span class="p_context"> static inline pgste_t ptep_xchg_start(struct mm_struct *mm,</span>
 {
 	pgste_t pgste = __pgste(0);
 
<span class="p_del">-	if (mm_has_pgste(mm)) {</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
 		pgste = pgste_get_lock(ptep);
 		pgste = pgste_pte_notify(mm, addr, ptep, pgste);
 	}
<span class="p_chunk">@@ -207,7 +207,7 @@</span> <span class="p_context"> static inline pte_t ptep_xchg_commit(struct mm_struct *mm,</span>
 				    unsigned long addr, pte_t *ptep,
 				    pgste_t pgste, pte_t old, pte_t new)
 {
<span class="p_del">-	if (mm_has_pgste(mm)) {</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
 		if (pte_val(old) &amp; _PAGE_INVALID)
 			pgste_set_key(ptep, pgste, new, mm);
 		if (pte_val(new) &amp; _PAGE_INVALID) {
<span class="p_chunk">@@ -263,7 +263,7 @@</span> <span class="p_context"> pte_t ptep_modify_prot_start(struct mm_struct *mm, unsigned long addr,</span>
 	preempt_disable();
 	pgste = ptep_xchg_start(mm, addr, ptep);
 	old = ptep_flush_lazy(mm, addr, ptep);
<span class="p_del">-	if (mm_has_pgste(mm)) {</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
 		pgste = pgste_update_all(old, pgste, mm);
 		pgste_set(ptep, pgste);
 	}
<span class="p_chunk">@@ -278,7 +278,7 @@</span> <span class="p_context"> void ptep_modify_prot_commit(struct mm_struct *mm, unsigned long addr,</span>
 
 	if (!MACHINE_HAS_NX)
 		pte_val(pte) &amp;= ~_PAGE_NOEXEC;
<span class="p_del">-	if (mm_has_pgste(mm)) {</span>
<span class="p_add">+	if (pgtable_has_pgste(mm, __pa(ptep))) {</span>
 		pgste = pgste_get(ptep);
 		pgste_set_key(ptep, pgste, pte, mm);
 		pgste = pgste_set_pte(ptep, pgste, pte);
<span class="p_chunk">@@ -445,7 +445,7 @@</span> <span class="p_context"> void ptep_set_pte_at(struct mm_struct *mm, unsigned long addr,</span>
 {
 	pgste_t pgste;
 
<span class="p_del">-	/* the mm_has_pgste() check is done in set_pte_at() */</span>
<span class="p_add">+	/* the pgtable_has_pgste() check is done in set_pte_at() */</span>
 	preempt_disable();
 	pgste = pgste_get_lock(ptep);
 	pgste_val(pgste) &amp;= ~_PGSTE_GPS_ZERO;
<span class="p_chunk">@@ -569,6 +569,9 @@</span> <span class="p_context"> void ptep_zap_unused(struct mm_struct *mm, unsigned long addr,</span>
 	pgste_t pgste;
 	pte_t pte;
 
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep)))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	/* Zap unused and logically-zero pages */
 	preempt_disable();
 	pgste = pgste_get_lock(ptep);
<span class="p_chunk">@@ -588,18 +591,22 @@</span> <span class="p_context"> void ptep_zap_unused(struct mm_struct *mm, unsigned long addr,</span>
 
 void ptep_zap_key(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
 {
<span class="p_add">+	const bool has_pgste = pgtable_has_pgste(mm, __pa(ptep));</span>
 	unsigned long ptev;
 	pgste_t pgste;
 
 	/* Clear storage key */
 	preempt_disable();
<span class="p_del">-	pgste = pgste_get_lock(ptep);</span>
<span class="p_del">-	pgste_val(pgste) &amp;= ~(PGSTE_ACC_BITS | PGSTE_FP_BIT |</span>
<span class="p_del">-			      PGSTE_GR_BIT | PGSTE_GC_BIT);</span>
<span class="p_add">+	if (has_pgste) {</span>
<span class="p_add">+		pgste = pgste_get_lock(ptep);</span>
<span class="p_add">+		pgste_val(pgste) &amp;= ~(PGSTE_ACC_BITS | PGSTE_FP_BIT |</span>
<span class="p_add">+				      PGSTE_GR_BIT | PGSTE_GC_BIT);</span>
<span class="p_add">+	}</span>
 	ptev = pte_val(*ptep);
 	if (!(ptev &amp; _PAGE_INVALID) &amp;&amp; (ptev &amp; _PAGE_WRITE))
 		page_set_storage_key(ptev &amp; PAGE_MASK, PAGE_DEFAULT_KEY, 1);
<span class="p_del">-	pgste_set_unlock(ptep, pgste);</span>
<span class="p_add">+	if (has_pgste)</span>
<span class="p_add">+		pgste_set_unlock(ptep, pgste);</span>
 	preempt_enable();
 }
 
<span class="p_chunk">@@ -634,6 +641,10 @@</span> <span class="p_context"> bool test_and_clear_guest_dirty(struct mm_struct *mm, unsigned long addr)</span>
 	 */
 	if (pmd_large(*pmd))
 		return true;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(pmd))) {</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
 
 	ptep = pte_alloc_map_lock(mm, pmd, addr, &amp;ptl);
 	if (unlikely(!ptep))
<span class="p_chunk">@@ -670,6 +681,11 @@</span> <span class="p_context"> int set_guest_storage_key(struct mm_struct *mm, unsigned long addr,</span>
 	ptep = get_locked_pte(mm, addr, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 
 	new = old = pgste_get_lock(ptep);
 	pgste_val(new) &amp;= ~(PGSTE_GR_BIT | PGSTE_GC_BIT |
<span class="p_chunk">@@ -748,6 +764,11 @@</span> <span class="p_context"> int reset_guest_reference_bit(struct mm_struct *mm, unsigned long addr)</span>
 	ptep = get_locked_pte(mm, addr, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 
 	new = old = pgste_get_lock(ptep);
 	/* Reset guest reference bit only */
<span class="p_chunk">@@ -780,6 +801,11 @@</span> <span class="p_context"> int get_guest_storage_key(struct mm_struct *mm, unsigned long addr,</span>
 	ptep = get_locked_pte(mm, addr, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 
 	pgste = pgste_get_lock(ptep);
 	*key = (pgste_val(pgste) &amp; (PGSTE_ACC_BITS | PGSTE_FP_BIT)) &gt;&gt; 56;
<span class="p_chunk">@@ -820,6 +846,11 @@</span> <span class="p_context"> int pgste_perform_essa(struct mm_struct *mm, unsigned long hva, int orc,</span>
 	ptep = get_locked_pte(mm, hva, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 	pgste = pgste_get_lock(ptep);
 	pgstev = pgste_val(pgste);
 	if (oldpte)
<span class="p_chunk">@@ -912,6 +943,11 @@</span> <span class="p_context"> int set_pgste_bits(struct mm_struct *mm, unsigned long hva,</span>
 	ptep = get_locked_pte(mm, hva, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 	new = pgste_get_lock(ptep);
 
 	pgste_val(new) &amp;= ~bits;
<span class="p_chunk">@@ -939,6 +975,11 @@</span> <span class="p_context"> int get_pgste(struct mm_struct *mm, unsigned long hva, unsigned long *pgstep)</span>
 	ptep = get_locked_pte(mm, hva, &amp;ptl);
 	if (unlikely(!ptep))
 		return -EFAULT;
<span class="p_add">+	if (!pgtable_has_pgste(mm, __pa(ptep))) {</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+		WARN_ONCE(true, &quot;Guest address on page table without pgste&quot;);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
 	*pgstep = pgste_val(pgste_get(ptep));
 	pte_unmap_unlock(ptep, ptl);
 	return 0;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



