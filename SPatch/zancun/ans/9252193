
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2] sparc64: Trim page tables for 8M hugepages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2] sparc64: Trim page tables for 8M hugepages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=145751">Nitin Gupta</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 29, 2016, 7:54 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1469778930-242969-1-git-send-email-nitin.m.gupta@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9252193/mbox/"
   >mbox</a>
|
   <a href="/patch/9252193/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9252193/">/patch/9252193/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2EAC56075F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Jul 2016 07:56:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 16DE627F88
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Jul 2016 07:56:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0AE5727F9A; Fri, 29 Jul 2016 07:56:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AE1A527F88
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Jul 2016 07:56:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752505AbcG2H4V (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 29 Jul 2016 03:56:21 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:19235 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751575AbcG2H4S (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 29 Jul 2016 03:56:18 -0400
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id u6T7trbm019109
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 29 Jul 2016 07:55:53 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by aserv0022.oracle.com (8.13.8/8.13.8) with ESMTP id u6T7tqrs024971
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 29 Jul 2016 07:55:52 GMT
Received: from abhmp0001.oracle.com (abhmp0001.oracle.com [141.146.116.7])
	by userv0121.oracle.com (8.13.8/8.13.8) with ESMTP id u6T7tmIO007449; 
	Fri, 29 Jul 2016 07:55:50 GMT
Received: from ca-qasparc20.us.oracle.com (/10.147.24.73)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Fri, 29 Jul 2016 00:55:48 -0700
From: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;
To: &quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;
Cc: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Julian Calaby &lt;julian.calaby@gmail.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Adam Buchbinder &lt;adam.buchbinder@gmail.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;,
	Sudeep Holla &lt;sudeep.holla@arm.com&gt;,
	Chris Hyser &lt;chris.hyser@oracle.com&gt;,
	Khalid Aziz &lt;khalid.aziz@oracle.com&gt;, Toshi Kani &lt;toshi.kani@hpe.com&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, sparclinux@vger.kernel.org,
	linux-kernel@vger.kernel.org
Subject: [PATCH v2] sparc64: Trim page tables for 8M hugepages
Date: Fri, 29 Jul 2016 00:54:21 -0700
Message-Id: &lt;1469778930-242969-1-git-send-email-nitin.m.gupta@oracle.com&gt;
X-Mailer: git-send-email 1.7.1
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=145751">Nitin Gupta</a> - July 29, 2016, 7:54 a.m.</div>
<pre class="content">
For PMD aligned (8M) hugepages, we currently allocate
all four page table levels which is wasteful. We now
allocate till PMD level only which saves memory usage
from page tables.

Also, when freeing page table for 8M hugepage backed region,
make sure we don&#39;t try to access non-existent PTE level.

Orabug: 22630259
<span class="signed-off-by">
Signed-off-by: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>
---
Changelog v2 vs v1
 - Combine fix for page table freeing with the main trimming patch (Dave)

arch/sparc/include/asm/hugetlb.h    |   12 +--
 arch/sparc/include/asm/pgtable_64.h |    7 ++-
 arch/sparc/include/asm/tsb.h        |    2 +-
 arch/sparc/mm/fault_64.c            |    4 +-
 arch/sparc/mm/hugetlbpage.c         |  166 +++++++++++++++++++++++-----------
 arch/sparc/mm/init_64.c             |    5 +-
 6 files changed, 129 insertions(+), 67 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - July 29, 2016, 6:09 p.m.</div>
<pre class="content">
<span class="from">From: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>
Date: Fri, 29 Jul 2016 00:54:21 -0700
<span class="quote">
&gt; For PMD aligned (8M) hugepages, we currently allocate</span>
<span class="quote">&gt; all four page table levels which is wasteful. We now</span>
<span class="quote">&gt; allocate till PMD level only which saves memory usage</span>
<span class="quote">&gt; from page tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, when freeing page table for 8M hugepage backed region,</span>
<span class="quote">&gt; make sure we don&#39;t try to access non-existent PTE level.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Orabug: 22630259</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Nitin Gupta &lt;nitin.m.gupta@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Changelog v2 vs v1</span>
<span class="quote">&gt;  - Combine fix for page table freeing with the main trimming patch (Dave)</span>

Applied, thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/sparc/include/asm/hugetlb.h b/arch/sparc/include/asm/hugetlb.h</span>
<span class="p_header">index 139e711..dcbf985 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/hugetlb.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/hugetlb.h</span>
<span class="p_chunk">@@ -31,14 +31,6 @@</span> <span class="p_context"> static inline int prepare_hugepage_range(struct file *file,</span>
 	return 0;
 }
 
<span class="p_del">-static inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,</span>
<span class="p_del">-					  unsigned long addr, unsigned long end,</span>
<span class="p_del">-					  unsigned long floor,</span>
<span class="p_del">-					  unsigned long ceiling)</span>
<span class="p_del">-{</span>
<span class="p_del">-	free_pgd_range(tlb, addr, end, floor, ceiling);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
 					 unsigned long addr, pte_t *ptep)
 {
<span class="p_chunk">@@ -82,4 +74,8 @@</span> <span class="p_context"> static inline void arch_clear_hugepage_flags(struct page *page)</span>
 {
 }
 
<span class="p_add">+void hugetlb_free_pgd_range(struct mmu_gather *tlb, unsigned long addr,</span>
<span class="p_add">+			    unsigned long end, unsigned long floor,</span>
<span class="p_add">+			    unsigned long ceiling);</span>
<span class="p_add">+</span>
 #endif /* _ASM_SPARC64_HUGETLB_H */
<span class="p_header">diff --git a/arch/sparc/include/asm/pgtable_64.h b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">index e7d8280..1fb317f 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -395,7 +395,7 @@</span> <span class="p_context"> static inline unsigned long __pte_huge_mask(void)</span>
 
 static inline pte_t pte_mkhuge(pte_t pte)
 {
<span class="p_del">-	return __pte(pte_val(pte) | __pte_huge_mask());</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_PMD_HUGE | __pte_huge_mask());</span>
 }
 
 static inline bool is_hugetlb_pte(pte_t pte)
<span class="p_chunk">@@ -403,6 +403,11 @@</span> <span class="p_context"> static inline bool is_hugetlb_pte(pte_t pte)</span>
 	return !!(pte_val(pte) &amp; __pte_huge_mask());
 }
 
<span class="p_add">+static inline bool is_hugetlb_pmd(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !!(pmd_val(pmd) &amp; _PAGE_PMD_HUGE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline pmd_t pmd_mkhuge(pmd_t pmd)
 {
<span class="p_header">diff --git a/arch/sparc/include/asm/tsb.h b/arch/sparc/include/asm/tsb.h</span>
<span class="p_header">index c6a155c..32258e0 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/tsb.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/tsb.h</span>
<span class="p_chunk">@@ -203,7 +203,7 @@</span> <span class="p_context"> extern struct tsb_phys_patch_entry __tsb_phys_patch, __tsb_phys_patch_end;</span>
 	 * We have to propagate the 4MB bit of the virtual address
 	 * because we are fabricating 8MB pages using 4MB hw pages.
 	 */
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_add">+#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)</span>
 #define USER_PGTABLE_CHECK_PMD_HUGE(VADDR, REG1, REG2, FAIL_LABEL, PTE_LABEL) \
 	brz,pn		REG1, FAIL_LABEL;		\
 	 sethi		%uhi(_PAGE_PMD_HUGE), REG2;	\
<span class="p_header">diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c</span>
<span class="p_header">index 6c43b92..575ecfe 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_64.c</span>
<span class="p_chunk">@@ -111,8 +111,8 @@</span> <span class="p_context"> static unsigned int get_user_insn(unsigned long tpc)</span>
 	if (pmd_none(*pmdp) || unlikely(pmd_bad(*pmdp)))
 		goto out_irq_enable;
 
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-	if (pmd_trans_huge(*pmdp)) {</span>
<span class="p_add">+#if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)</span>
<span class="p_add">+	if (is_hugetlb_pmd(*pmdp)) {</span>
 		pa  = pmd_pfn(*pmdp) &lt;&lt; PAGE_SHIFT;
 		pa += tpc &amp; ~HPAGE_MASK;
 
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index ba52e64..494c390 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/mman.h&gt;
 #include &lt;asm/pgalloc.h&gt;
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
 #include &lt;asm/tlb.h&gt;
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/cacheflush.h&gt;
<span class="p_chunk">@@ -131,23 +132,13 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm,</span>
 {
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd;</span>
 	pte_t *pte = NULL;
 
<span class="p_del">-	/* We must align the address, because our caller will run</span>
<span class="p_del">-	 * set_huge_pte_at() on whatever we return, which writes out</span>
<span class="p_del">-	 * all of the sub-ptes for the hugepage range.  So we have</span>
<span class="p_del">-	 * to give it the first such sub-pte.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	addr &amp;= HPAGE_MASK;</span>
<span class="p_del">-</span>
 	pgd = pgd_offset(mm, addr);
 	pud = pud_alloc(mm, pgd, addr);
<span class="p_del">-	if (pud) {</span>
<span class="p_del">-		pmd = pmd_alloc(mm, pud, addr);</span>
<span class="p_del">-		if (pmd)</span>
<span class="p_del">-			pte = pte_alloc_map(mm, pmd, addr);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (pud)</span>
<span class="p_add">+		pte = (pte_t *)pmd_alloc(mm, pud, addr);</span>
<span class="p_add">+</span>
 	return pte;
 }
 
<span class="p_chunk">@@ -155,19 +146,13 @@</span> <span class="p_context"> pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)</span>
 {
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_del">-	pmd_t *pmd;</span>
 	pte_t *pte = NULL;
 
<span class="p_del">-	addr &amp;= HPAGE_MASK;</span>
<span class="p_del">-</span>
 	pgd = pgd_offset(mm, addr);
 	if (!pgd_none(*pgd)) {
 		pud = pud_offset(pgd, addr);
<span class="p_del">-		if (!pud_none(*pud)) {</span>
<span class="p_del">-			pmd = pmd_offset(pud, addr);</span>
<span class="p_del">-			if (!pmd_none(*pmd))</span>
<span class="p_del">-				pte = pte_offset_map(pmd, addr);</span>
<span class="p_del">-		}</span>
<span class="p_add">+		if (!pud_none(*pud))</span>
<span class="p_add">+			pte = (pte_t *)pmd_offset(pud, addr);</span>
 	}
 	return pte;
 }
<span class="p_chunk">@@ -175,70 +160,143 @@</span> <span class="p_context"> pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)</span>
 void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
 		     pte_t *ptep, pte_t entry)
 {
<span class="p_del">-	int i;</span>
<span class="p_del">-	pte_t orig[2];</span>
<span class="p_del">-	unsigned long nptes;</span>
<span class="p_add">+	pte_t orig;</span>
 
 	if (!pte_present(*ptep) &amp;&amp; pte_present(entry))
 		mm-&gt;context.huge_pte_count++;
 
 	addr &amp;= HPAGE_MASK;
<span class="p_del">-</span>
<span class="p_del">-	nptes = 1 &lt;&lt; HUGETLB_PAGE_ORDER;</span>
<span class="p_del">-	orig[0] = *ptep;</span>
<span class="p_del">-	orig[1] = *(ptep + nptes / 2);</span>
<span class="p_del">-	for (i = 0; i &lt; nptes; i++) {</span>
<span class="p_del">-		*ptep = entry;</span>
<span class="p_del">-		ptep++;</span>
<span class="p_del">-		addr += PAGE_SIZE;</span>
<span class="p_del">-		pte_val(entry) += PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	orig = *ptep;</span>
<span class="p_add">+	*ptep = entry;</span>
 
 	/* Issue TLB flush at REAL_HPAGE_SIZE boundaries */
<span class="p_del">-	addr -= REAL_HPAGE_SIZE;</span>
<span class="p_del">-	ptep -= nptes / 2;</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, orig[1], 0);</span>
<span class="p_del">-	addr -= REAL_HPAGE_SIZE;</span>
<span class="p_del">-	ptep -= nptes / 2;</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, orig[0], 0);</span>
<span class="p_add">+	maybe_tlb_batch_add(mm, addr, ptep, orig, 0);</span>
<span class="p_add">+	maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, orig, 0);</span>
 }
 
 pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep)
 {
 	pte_t entry;
<span class="p_del">-	int i;</span>
<span class="p_del">-	unsigned long nptes;</span>
 
 	entry = *ptep;
 	if (pte_present(entry))
 		mm-&gt;context.huge_pte_count--;
 
 	addr &amp;= HPAGE_MASK;
<span class="p_del">-	nptes = 1 &lt;&lt; HUGETLB_PAGE_ORDER;</span>
<span class="p_del">-	for (i = 0; i &lt; nptes; i++) {</span>
<span class="p_del">-		*ptep = __pte(0UL);</span>
<span class="p_del">-		addr += PAGE_SIZE;</span>
<span class="p_del">-		ptep++;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	*ptep = __pte(0UL);</span>
 
 	/* Issue TLB flush at REAL_HPAGE_SIZE boundaries */
<span class="p_del">-	addr -= REAL_HPAGE_SIZE;</span>
<span class="p_del">-	ptep -= nptes / 2;</span>
<span class="p_del">-	maybe_tlb_batch_add(mm, addr, ptep, entry, 0);</span>
<span class="p_del">-	addr -= REAL_HPAGE_SIZE;</span>
<span class="p_del">-	ptep -= nptes / 2;</span>
 	maybe_tlb_batch_add(mm, addr, ptep, entry, 0);
<span class="p_add">+	maybe_tlb_batch_add(mm, addr + REAL_HPAGE_SIZE, ptep, entry, 0);</span>
 
 	return entry;
 }
 
 int pmd_huge(pmd_t pmd)
 {
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return !pmd_none(pmd) &amp;&amp;</span>
<span class="p_add">+		(pmd_val(pmd) &amp; (_PAGE_VALID|_PAGE_PMD_HUGE)) != _PAGE_VALID;</span>
 }
 
 int pud_huge(pud_t pud)
 {
 	return 0;
 }
<span class="p_add">+</span>
<span class="p_add">+static void hugetlb_free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,</span>
<span class="p_add">+			   unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgtable_t token = pmd_pgtable(*pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_clear(pmd);</span>
<span class="p_add">+	pte_free_tlb(tlb, token, addr);</span>
<span class="p_add">+	atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hugetlb_free_pmd_range(struct mmu_gather *tlb, pud_t *pud,</span>
<span class="p_add">+				   unsigned long addr, unsigned long end,</span>
<span class="p_add">+				   unsigned long floor, unsigned long ceiling)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = addr;</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		if (pmd_none(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (is_hugetlb_pmd(*pmd))</span>
<span class="p_add">+			pmd_clear(pmd);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			hugetlb_free_pte_range(tlb, pmd, addr);</span>
<span class="p_add">+	} while (pmd++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	start &amp;= PUD_MASK;</span>
<span class="p_add">+	if (start &lt; floor)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (ceiling) {</span>
<span class="p_add">+		ceiling &amp;= PUD_MASK;</span>
<span class="p_add">+		if (!ceiling)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (end - 1 &gt; ceiling - 1)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, start);</span>
<span class="p_add">+	pud_clear(pud);</span>
<span class="p_add">+	pmd_free_tlb(tlb, pmd, start);</span>
<span class="p_add">+	mm_dec_nr_pmds(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hugetlb_free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
<span class="p_add">+				   unsigned long addr, unsigned long end,</span>
<span class="p_add">+				   unsigned long floor, unsigned long ceiling)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+</span>
<span class="p_add">+	start = addr;</span>
<span class="p_add">+	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (pud_none_or_clear_bad(pud))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		hugetlb_free_pmd_range(tlb, pud, addr, next, floor,</span>
<span class="p_add">+				       ceiling);</span>
<span class="p_add">+	} while (pud++, addr = next, addr != end);</span>
<span class="p_add">+</span>
<span class="p_add">+	start &amp;= PGDIR_MASK;</span>
<span class="p_add">+	if (start &lt; floor)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	if (ceiling) {</span>
<span class="p_add">+		ceiling &amp;= PGDIR_MASK;</span>
<span class="p_add">+		if (!ceiling)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (end - 1 &gt; ceiling - 1)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, start);</span>
<span class="p_add">+	pgd_clear(pgd);</span>
<span class="p_add">+	pud_free_tlb(tlb, pud, start);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hugetlb_free_pgd_range(struct mmu_gather *tlb,</span>
<span class="p_add">+			    unsigned long addr, unsigned long end,</span>
<span class="p_add">+			    unsigned long floor, unsigned long ceiling)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset(tlb-&gt;mm, addr);</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		if (pgd_none_or_clear_bad(pgd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		hugetlb_free_pud_range(tlb, pgd, addr, next, floor, ceiling);</span>
<span class="p_add">+	} while (pgd++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index aec508e..9510051 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -346,9 +346,12 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *</span>
 	spin_lock_irqsave(&amp;mm-&gt;context.lock, flags);
 
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
<span class="p_del">-	if (mm-&gt;context.huge_pte_count &amp;&amp; is_hugetlb_pte(pte))</span>
<span class="p_add">+	if (mm-&gt;context.huge_pte_count &amp;&amp; is_hugetlb_pte(pte)) {</span>
<span class="p_add">+		/* We are fabricating 8MB pages using 4MB real hw pages.  */</span>
<span class="p_add">+		pte_val(pte) |= (address &amp; (1UL &lt;&lt; REAL_HPAGE_SHIFT));</span>
 		__update_mmu_tsb_insert(mm, MM_TSB_HUGE, REAL_HPAGE_SHIFT,
 					address, pte_val(pte));
<span class="p_add">+	}</span>
 	else
 #endif
 		__update_mmu_tsb_insert(mm, MM_TSB_BASE, PAGE_SHIFT,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



