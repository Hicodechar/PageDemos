
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv11,32/37] thp: reintroduce split_huge_page() - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv11,32/37] thp: reintroduce split_huge_page()</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 18, 2015, 3:01 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1442588500-77331-33-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7217571/mbox/"
   >mbox</a>
|
   <a href="/patch/7217571/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7217571/">/patch/7217571/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 9B5C89F372
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Sep 2015 15:02:49 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 18A3320830
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Sep 2015 15:02:47 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0278120842
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Sep 2015 15:02:40 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754999AbbIRPCe (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 18 Sep 2015 11:02:34 -0400
Received: from mga02.intel.com ([134.134.136.20]:10522 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754904AbbIRPCT (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 18 Sep 2015 11:02:19 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by orsmga101.jf.intel.com with ESMTP; 18 Sep 2015 08:02:05 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,553,1437462000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;807459677&quot;
Received: from black.fi.intel.com ([10.237.72.93])
	by orsmga002.jf.intel.com with ESMTP; 18 Sep 2015 08:02:01 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 59F5B607; Fri, 18 Sep 2015 18:01:44 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Christoph Lameter &lt;cl@gentwo.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Steve Capper &lt;steve.capper@linaro.org&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Jerome Marchand &lt;jmarchan@redhat.com&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv11 32/37] thp: reintroduce split_huge_page()
Date: Fri, 18 Sep 2015 18:01:35 +0300
Message-Id: &lt;1442588500-77331-33-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.5.1
In-Reply-To: &lt;1442588500-77331-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1442588500-77331-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Sept. 18, 2015, 3:01 p.m.</div>
<pre class="content">
This patch adds implementation of split_huge_page() for new
refcountings.

Unlike previous implementation, new split_huge_page() can fail if
somebody holds GUP pin on the page. It also means that pin on page
would prevent it from bening split under you. It makes situation in
many places much cleaner.

The basic scheme of split_huge_page():

  - Check that sum of mapcounts of all subpage is equal to page_count()
    plus one (caller pin). Foll off with -EBUSY. This way we can avoid
    useless PMD-splits.

  - Freeze the page counters by splitting all PMD and setup migration
    PTEs.

  - Re-check sum of mapcounts against page_count(). Page&#39;s counts are
    stable now. -EBUSY if page is pinned.

  - Split compound page.

  - Unfreeze the page by removing migration entries.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="tested-by">Tested-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
<span class="tested-by">Tested-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="acked-by">Acked-by: Jerome Marchand &lt;jmarchan@redhat.com&gt;</span>
---
 include/linux/huge_mm.h |   7 +-
 include/linux/pagemap.h |  13 +-
 mm/huge_memory.c        | 317 ++++++++++++++++++++++++++++++++++++++++++++++++
 mm/internal.h           |  26 +++-
 mm/rmap.c               |  21 ----
 5 files changed, 356 insertions(+), 28 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index 60b5448bf37d..156290523a05 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -94,8 +94,11 @@</span> <span class="p_context"> extern bool is_vma_temporary_stack(struct vm_area_struct *vma);</span>
 
 extern unsigned long transparent_hugepage_flags;
 
<span class="p_del">-#define split_huge_page_to_list(page, list) BUILD_BUG()</span>
<span class="p_del">-#define split_huge_page(page) BUILD_BUG()</span>
<span class="p_add">+int split_huge_page_to_list(struct page *page, struct list_head *list);</span>
<span class="p_add">+static inline int split_huge_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return split_huge_page_to_list(page, NULL);</span>
<span class="p_add">+}</span>
 
 void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
 		unsigned long address);
<span class="p_header">diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h</span>
<span class="p_header">index 3e95fb6a77af..0f4f10dd7080 100644</span>
<span class="p_header">--- a/include/linux/pagemap.h</span>
<span class="p_header">+++ b/include/linux/pagemap.h</span>
<span class="p_chunk">@@ -387,10 +387,21 @@</span> <span class="p_context"> static inline struct page *read_mapping_page(struct address_space *mapping,</span>
  */
 static inline pgoff_t page_to_pgoff(struct page *page)
 {
<span class="p_add">+	pgoff_t pgoff;</span>
<span class="p_add">+</span>
 	if (unlikely(PageHeadHuge(page)))
 		return page-&gt;index &lt;&lt; compound_order(page);
<span class="p_del">-	else</span>
<span class="p_add">+</span>
<span class="p_add">+	if (likely(!PageTransTail(page)))</span>
 		return page-&gt;index &lt;&lt; (PAGE_CACHE_SHIFT - PAGE_SHIFT);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 *  We don&#39;t initialize -&gt;index for tail pages: calculate based on</span>
<span class="p_add">+	 *  head page</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgoff = compound_head(page)-&gt;index &lt;&lt; (PAGE_CACHE_SHIFT - PAGE_SHIFT);</span>
<span class="p_add">+	pgoff += page - compound_head(page);</span>
<span class="p_add">+	return pgoff;</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 4e722c85f2bc..7cacd8a956f0 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2980,3 +2980,320 @@</span> <span class="p_context"> void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 			split_huge_pmd_address(next, nstart);
 	}
 }
<span class="p_add">+</span>
<span class="p_add">+static void freeze_page_vma(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+		unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset(vma-&gt;vm_mm, address);</span>
<span class="p_add">+	if (!pgd_present(*pgd))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	pud = pud_offset(pgd, address);</span>
<span class="p_add">+	if (!pud_present(*pud))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	pmd = pmd_offset(pud, address);</span>
<span class="p_add">+	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="p_add">+	if (!pmd_present(*pmd)) {</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pmd_trans_huge(*pmd)) {</span>
<span class="p_add">+		if (page == pmd_page(*pmd))</span>
<span class="p_add">+			__split_huge_pmd_locked(vma, pmd, address, true);</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_map_lock(vma-&gt;vm_mm, pmd, address, &amp;ptl);</span>
<span class="p_add">+	for (i = 0; i &lt; HPAGE_PMD_NR; i++, address += PAGE_SIZE, page++) {</span>
<span class="p_add">+		pte_t entry, swp_pte;</span>
<span class="p_add">+		swp_entry_t swp_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(pte[i]))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (page_to_pfn(page) != pte_pfn(pte[i]))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		flush_cache_page(vma, address, page_to_pfn(page));</span>
<span class="p_add">+		entry = ptep_clear_flush(vma, address, pte + i);</span>
<span class="p_add">+		swp_entry = make_migration_entry(page, pte_write(entry));</span>
<span class="p_add">+		swp_pte = swp_entry_to_pte(swp_entry);</span>
<span class="p_add">+		if (pte_soft_dirty(entry))</span>
<span class="p_add">+			swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="p_add">+		set_pte_at(vma-&gt;vm_mm, address, pte + i, swp_pte);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void freeze_page(struct anon_vma *anon_vma, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct anon_vma_chain *avc;</span>
<span class="p_add">+	pgoff_t pgoff = page_to_pgoff(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+	anon_vma_interval_tree_foreach(avc, &amp;anon_vma-&gt;rb_root, pgoff,</span>
<span class="p_add">+			pgoff + HPAGE_PMD_NR - 1) {</span>
<span class="p_add">+		unsigned long haddr;</span>
<span class="p_add">+</span>
<span class="p_add">+		haddr = __vma_address(page, avc-&gt;vma) &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(avc-&gt;vma-&gt;vm_mm,</span>
<span class="p_add">+				haddr, haddr + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		freeze_page_vma(avc-&gt;vma, page, haddr);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(avc-&gt;vma-&gt;vm_mm,</span>
<span class="p_add">+				haddr, haddr + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void unfreeze_page_vma(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+		unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte, entry;</span>
<span class="p_add">+	swp_entry_t swp_entry;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = mm_find_pmd(vma-&gt;vm_mm, address);</span>
<span class="p_add">+	if (!pmd)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	pte = pte_offset_map_lock(vma-&gt;vm_mm, pmd, address, &amp;ptl);</span>
<span class="p_add">+	for (i = 0; i &lt; HPAGE_PMD_NR; i++, address += PAGE_SIZE, page++) {</span>
<span class="p_add">+		if (!page_mapped(page))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!is_swap_pte(pte[i]))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		swp_entry = pte_to_swp_entry(pte[i]);</span>
<span class="p_add">+		if (!is_migration_entry(swp_entry))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (migration_entry_to_page(swp_entry) != page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pte_mkold(mk_pte(page, vma-&gt;vm_page_prot));</span>
<span class="p_add">+		if (is_write_migration_entry(swp_entry))</span>
<span class="p_add">+			entry = maybe_mkwrite(entry, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_dcache_page(page);</span>
<span class="p_add">+		set_pte_at(vma-&gt;vm_mm, address, pte + i, entry);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* No need to invalidate - it was non-present before */</span>
<span class="p_add">+		update_mmu_cache(vma, address, pte + i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pte_unmap_unlock(pte, ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void unfreeze_page(struct anon_vma *anon_vma, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct anon_vma_chain *avc;</span>
<span class="p_add">+	pgoff_t pgoff = page_to_pgoff(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	anon_vma_interval_tree_foreach(avc, &amp;anon_vma-&gt;rb_root,</span>
<span class="p_add">+			pgoff, pgoff + HPAGE_PMD_NR - 1) {</span>
<span class="p_add">+		unsigned long address = __vma_address(page, avc-&gt;vma);</span>
<span class="p_add">+</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(avc-&gt;vma-&gt;vm_mm,</span>
<span class="p_add">+				address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		unfreeze_page_vma(avc-&gt;vma, page, address);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(avc-&gt;vma-&gt;vm_mm,</span>
<span class="p_add">+				address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int total_mapcount(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = compound_mapcount(page);</span>
<span class="p_add">+	for (i = 0; i &lt; HPAGE_PMD_NR; i++)</span>
<span class="p_add">+		ret += atomic_read(&amp;page[i]._mapcount) + 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PageDoubleMap(page))</span>
<span class="p_add">+		ret -= HPAGE_PMD_NR;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __split_huge_page_tail(struct page *head, int tail,</span>
<span class="p_add">+		struct lruvec *lruvec, struct list_head *list)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int mapcount;</span>
<span class="p_add">+	struct page *page_tail = head + tail;</span>
<span class="p_add">+</span>
<span class="p_add">+	mapcount = atomic_read(&amp;page_tail-&gt;_mapcount) + 1;</span>
<span class="p_add">+	VM_BUG_ON_PAGE(atomic_read(&amp;page_tail-&gt;_count) != 0, page_tail);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * tail_page-&gt;_count is zero and not changing from under us. But</span>
<span class="p_add">+	 * get_page_unless_zero() may be running from under us on the</span>
<span class="p_add">+	 * tail_page. If we used atomic_set() below instead of atomic_add(), we</span>
<span class="p_add">+	 * would then run atomic_set() concurrently with</span>
<span class="p_add">+	 * get_page_unless_zero(), and atomic_set() is implemented in C not</span>
<span class="p_add">+	 * using locked ops. spin_unlock on x86 sometime uses locked ops</span>
<span class="p_add">+	 * because of PPro errata 66, 92, so unless somebody can guarantee</span>
<span class="p_add">+	 * atomic_set() here would be safe on all archs (and not only on x86),</span>
<span class="p_add">+	 * it&#39;s safer to use atomic_add().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	atomic_add(mapcount + 1, &amp;page_tail-&gt;_count);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* after clearing PageTail the gup refcount can be released */</span>
<span class="p_add">+	smp_mb__after_atomic();</span>
<span class="p_add">+</span>
<span class="p_add">+	page_tail-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP;</span>
<span class="p_add">+	page_tail-&gt;flags |= (head-&gt;flags &amp;</span>
<span class="p_add">+			((1L &lt;&lt; PG_referenced) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_swapbacked) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_mlocked) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_uptodate) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_active) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_locked) |</span>
<span class="p_add">+			 (1L &lt;&lt; PG_unevictable)));</span>
<span class="p_add">+	page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="p_add">+</span>
<span class="p_add">+	clear_compound_head(page_tail);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (page_is_young(head))</span>
<span class="p_add">+		set_page_young(page_tail);</span>
<span class="p_add">+	if (page_is_idle(head))</span>
<span class="p_add">+		set_page_idle(page_tail);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* -&gt;mapping in first tail page is compound_mapcount */</span>
<span class="p_add">+	VM_BUG_ON_PAGE(tail != 1 &amp;&amp; page_tail-&gt;mapping != TAIL_MAPPING,</span>
<span class="p_add">+			page_tail);</span>
<span class="p_add">+	page_tail-&gt;mapping = head-&gt;mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	page_tail-&gt;index = head-&gt;index + tail;</span>
<span class="p_add">+	page_cpupid_xchg_last(page_tail, page_cpupid_last(head));</span>
<span class="p_add">+	lru_add_page_tail(head, page_tail, lruvec, list);</span>
<span class="p_add">+</span>
<span class="p_add">+	return mapcount;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __split_huge_page(struct page *page, struct list_head *list)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *head = compound_head(page);</span>
<span class="p_add">+	struct zone *zone = page_zone(head);</span>
<span class="p_add">+	struct lruvec *lruvec;</span>
<span class="p_add">+	int i, tail_mapcount;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* prevent PageLRU to go away from under us, and freeze lru stats */</span>
<span class="p_add">+	spin_lock_irq(&amp;zone-&gt;lru_lock);</span>
<span class="p_add">+	lruvec = mem_cgroup_page_lruvec(head, zone);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* complete memcg works before add pages to LRU */</span>
<span class="p_add">+	mem_cgroup_split_huge_fixup(head);</span>
<span class="p_add">+</span>
<span class="p_add">+	tail_mapcount = 0;</span>
<span class="p_add">+	for (i = HPAGE_PMD_NR - 1; i &gt;= 1; i--)</span>
<span class="p_add">+		tail_mapcount += __split_huge_page_tail(head, i, lruvec, list);</span>
<span class="p_add">+	atomic_sub(tail_mapcount, &amp;head-&gt;_count);</span>
<span class="p_add">+</span>
<span class="p_add">+	ClearPageCompound(head);</span>
<span class="p_add">+	spin_unlock_irq(&amp;zone-&gt;lru_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	unfreeze_page(page_anon_vma(head), head);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {</span>
<span class="p_add">+		struct page *subpage = head + i;</span>
<span class="p_add">+		if (subpage == page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		unlock_page(subpage);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Subpages may be freed if there wasn&#39;t any mapping</span>
<span class="p_add">+		 * like if add_to_swap() is running on a lru page that</span>
<span class="p_add">+		 * had its mapping zapped. And freeing these pages</span>
<span class="p_add">+		 * requires taking the lru_lock so we do the put_page</span>
<span class="p_add">+		 * of the tail pages after the split is complete.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		put_page(subpage);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This function splits huge page into normal pages. @page can point to any</span>
<span class="p_add">+ * subpage of huge page to split. Split doesn&#39;t change the position of @page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only caller must hold pin on the @page, otherwise split fails with -EBUSY.</span>
<span class="p_add">+ * The huge page must be locked.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If @list is null, tail pages will be added to LRU list, otherwise, to @list.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Both head page and tail pages will inherit mapping, flags, and so on from</span>
<span class="p_add">+ * the hugepage.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * GUP pin and PG_locked transferred to @page. Rest subpages can be freed if</span>
<span class="p_add">+ * they are not mapped.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns 0 if the hugepage is split successfully.</span>
<span class="p_add">+ * Returns -EBUSY if the page is pinned or if anon_vma disappeared from under</span>
<span class="p_add">+ * us.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int split_huge_page_to_list(struct page *page, struct list_head *list)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *head = compound_head(page);</span>
<span class="p_add">+	struct anon_vma *anon_vma;</span>
<span class="p_add">+	int count, mapcount, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON_PAGE(is_huge_zero_page(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageAnon(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageSwapBacked(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageCompound(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The caller does not necessarily hold an mmap_sem that would prevent</span>
<span class="p_add">+	 * the anon_vma disappearing so we first we take a reference to it</span>
<span class="p_add">+	 * and then lock the anon_vma for write. This is similar to</span>
<span class="p_add">+	 * page_lock_anon_vma_read except the write lock is taken to serialise</span>
<span class="p_add">+	 * against parallel split or collapse operations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	anon_vma = page_get_anon_vma(head);</span>
<span class="p_add">+	if (!anon_vma) {</span>
<span class="p_add">+		ret = -EBUSY;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	anon_vma_lock_write(anon_vma);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Racy check if we can split the page, before freeze_page() will</span>
<span class="p_add">+	 * split PMDs</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (total_mapcount(head) != page_count(head) - 1) {</span>
<span class="p_add">+		ret = -EBUSY;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	freeze_page(anon_vma, head);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(compound_mapcount(head), head);</span>
<span class="p_add">+</span>
<span class="p_add">+	count = page_count(head);</span>
<span class="p_add">+	mapcount = total_mapcount(head);</span>
<span class="p_add">+	if (mapcount == count - 1) {</span>
<span class="p_add">+		__split_huge_page(page, list);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+	} else if (IS_ENABLED(CONFIG_DEBUG_VM) &amp;&amp; mapcount &gt; count - 1) {</span>
<span class="p_add">+		pr_alert(&quot;total_mapcount: %u, page_count(): %u\n&quot;,</span>
<span class="p_add">+				mapcount, count);</span>
<span class="p_add">+		if (PageTail(page))</span>
<span class="p_add">+			dump_page(head, NULL);</span>
<span class="p_add">+		dump_page(page, &quot;total_mapcount(head) &gt; page_count(head) - 1&quot;);</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		unfreeze_page(anon_vma, head);</span>
<span class="p_add">+		ret = -EBUSY;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	anon_vma_unlock_write(anon_vma);</span>
<span class="p_add">+	put_anon_vma(anon_vma);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	count_vm_event(!ret ? THP_SPLIT_PAGE : THP_SPLIT_PAGE_FAILED);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 6c1ba744f8f5..c29fea7060e7 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
 
 extern int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			unsigned long address, pte_t *page_table, pmd_t *pmd,
<span class="p_chunk">@@ -250,10 +251,27 @@</span> <span class="p_context"> static inline void mlock_migrate_page(struct page *newpage, struct page *page)</span>
 
 extern pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);
 
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-extern unsigned long vma_address(struct page *page,</span>
<span class="p_del">-				 struct vm_area_struct *vma);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * At what user virtual address is page expected in @vma?</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+__vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgoff_t pgoff = page_to_pgoff(page);</span>
<span class="p_add">+	return vma-&gt;vm_start + ((pgoff - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long address = __vma_address(page, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* page should be within @vma mapping range */</span>
<span class="p_add">+	VM_BUG_ON_VMA(address &lt; vma-&gt;vm_start || address &gt;= vma-&gt;vm_end, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+	return address;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #else /* !CONFIG_MMU */
 static inline void clear_page_mlock(struct page *page) { }
 static inline void mlock_vma_page(struct page *page) { }
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 6dab22355abb..8541b4ed850d 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -565,27 +565,6 @@</span> <span class="p_context"> void page_unlock_anon_vma_read(struct anon_vma *anon_vma)</span>
 	anon_vma_unlock_read(anon_vma);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * At what user virtual address is page expected in @vma?</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline unsigned long</span>
<span class="p_del">-__vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgoff_t pgoff = page_to_pgoff(page);</span>
<span class="p_del">-	return vma-&gt;vm_start + ((pgoff - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-inline unsigned long</span>
<span class="p_del">-vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long address = __vma_address(page, vma);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* page should be within @vma mapping range */</span>
<span class="p_del">-	VM_BUG_ON_VMA(address &lt; vma-&gt;vm_start || address &gt;= vma-&gt;vm_end, vma);</span>
<span class="p_del">-</span>
<span class="p_del">-	return address;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 static void percpu_flush_tlb_batch_pages(void *data)
 {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



