
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[03/10] thp: Prepare for DAX huge pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [03/10] thp: Prepare for DAX huge pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 10, 2015, 8:29 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436560165-8943-4-git-send-email-matthew.r.wilcox@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6768301/mbox/"
   >mbox</a>
|
   <a href="/patch/6768301/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6768301/">/patch/6768301/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 055119F38C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Jul 2015 20:30:21 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 2115820782
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Jul 2015 20:30:20 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1CE8020773
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 10 Jul 2015 20:30:19 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933260AbbGJUaH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 10 Jul 2015 16:30:07 -0400
Received: from mga03.intel.com ([134.134.136.65]:24082 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932889AbbGJU3f (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 10 Jul 2015 16:29:35 -0400
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga103.jf.intel.com with ESMTP; 10 Jul 2015 13:29:36 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.15,449,1432623600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;603928944&quot;
Received: from qwang4-mobl2.ccr.corp.intel.com (HELO thog.int.wil.cx)
	([10.252.194.226])
	by orsmga003.jf.intel.com with SMTP; 10 Jul 2015 13:29:34 -0700
Received: by thog.int.wil.cx (Postfix, from userid 1000)
	id 9D41F60A1A; Fri, 10 Jul 2015 16:29:33 -0400 (EDT)
From: Matthew Wilcox &lt;matthew.r.wilcox@intel.com&gt;
To: linux-fsdevel@vger.kernel.org, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org
Cc: Matthew Wilcox &lt;willy@linux.intel.com&gt;
Subject: [PATCH 03/10] thp: Prepare for DAX huge pages
Date: Fri, 10 Jul 2015 16:29:18 -0400
Message-Id: &lt;1436560165-8943-4-git-send-email-matthew.r.wilcox@intel.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1436560165-8943-1-git-send-email-matthew.r.wilcox@intel.com&gt;
References: &lt;1436560165-8943-1-git-send-email-matthew.r.wilcox@intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66491">Wilcox, Matthew R</a> - July 10, 2015, 8:29 p.m.</div>
<pre class="content">
<span class="from">From: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>

Add a vma_is_dax() helper macro to test whether the VMA is DAX, and use
it in zap_huge_pmd() and __split_huge_page_pmd().
<span class="signed-off-by">
Signed-off-by: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>
---
 include/linux/dax.h |  4 ++++
 mm/huge_memory.c    | 46 ++++++++++++++++++++++++++++------------------
 2 files changed, 32 insertions(+), 18 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - July 19, 2015, 11:03 a.m.</div>
<pre class="content">
On Fri, Jul 10, 2015 at 04:29:18PM -0400, Matthew Wilcox wrote:
<span class="quote">&gt; From: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Add a vma_is_dax() helper macro to test whether the VMA is DAX, and use</span>
<span class="quote">&gt; it in zap_huge_pmd() and __split_huge_page_pmd().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Matthew Wilcox &lt;willy@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/dax.h |  4 ++++</span>
<span class="quote">&gt;  mm/huge_memory.c    | 46 ++++++++++++++++++++++++++++------------------</span>
<span class="quote">&gt;  2 files changed, 32 insertions(+), 18 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/dax.h b/include/linux/dax.h</span>
<span class="quote">&gt; index 4f27d3d..9b51f9d 100644</span>
<span class="quote">&gt; --- a/include/linux/dax.h</span>
<span class="quote">&gt; +++ b/include/linux/dax.h</span>
<span class="quote">&gt; @@ -18,4 +18,8 @@ int dax_pfn_mkwrite(struct vm_area_struct *, struct vm_fault *);</span>
<span class="quote">&gt;  #define dax_mkwrite(vma, vmf, gb, iod)		dax_fault(vma, vmf, gb, iod)</span>
<span class="quote">&gt;  #define __dax_mkwrite(vma, vmf, gb, iod)	__dax_fault(vma, vmf, gb, iod)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool vma_is_dax(struct vm_area_struct *vma)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return vma-&gt;vm_file &amp;&amp; IS_DAX(vma-&gt;vm_file-&gt;f_mapping-&gt;host);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 911071b..b7bd855 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -23,6 +23,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/migrate.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/hashtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/dax.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlb.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt; @@ -1391,7 +1392,6 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (__pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="quote">&gt; -		struct page *page;</span>
<span class="quote">&gt;  		pgtable_t pgtable;</span>
<span class="quote">&gt;  		pmd_t orig_pmd;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; @@ -1403,13 +1403,22 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,</span>
<span class="quote">&gt;  							tlb-&gt;fullmm);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt; -		pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +		if (vma_is_dax(vma)) {</span>
<span class="quote">&gt; +			if (is_huge_zero_pmd(orig_pmd)) {</span>
<span class="quote">&gt; +				pgtable = NULL;</span>

pgtable_t is not always a pointer. See arch/arc.
<span class="quote">
&gt; +			} else {</span>
<span class="quote">&gt; +				spin_unlock(ptl);</span>
<span class="quote">&gt; +				return 1;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		if (is_huge_zero_pmd(orig_pmd)) {</span>
<span class="quote">&gt;  			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;  			spin_unlock(ptl);</span>
<span class="quote">&gt;  			put_huge_zero_page();</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; -			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; +			struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  			page_remove_rmap(page);</span>
<span class="quote">&gt;  			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt;  			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; @@ -1418,7 +1427,8 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			spin_unlock(ptl);</span>
<span class="quote">&gt;  			tlb_remove_page(tlb, page);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +		if (pgtable)</span>
<span class="quote">&gt; +			pte_free(tlb-&gt;mm, pgtable);</span>

It&#39;s better to drop &quot;pgtable = NULL;&quot; above and use &quot;if (vma_is_dax(vma))&quot;
here.
<span class="quote">
&gt;  		ret = 1;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; @@ -2887,7 +2897,7 @@ void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		pmd_t *pmd)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt; -	struct page *page;</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;  	unsigned long mmun_start;	/* For mmu_notifiers */</span>
<span class="quote">&gt; @@ -2900,25 +2910,25 @@ void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; -	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt; -		spin_unlock(ptl);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	if (is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt; +	if (unlikely(!pmd_trans_huge(*pmd)))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	if (vma_is_dax(vma)) {</span>
<span class="quote">&gt; +		pmdp_huge_clear_flush(vma, haddr, pmd);</span>

pmdp_huge_clear_flush_notify()
<span class="quote">
&gt; +	} else if (is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt;  		__split_huge_zero_page_pmd(vma, haddr, pmd);</span>
<span class="quote">&gt; -		spin_unlock(ptl);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt; +		VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="quote">&gt; +		get_page(page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; -	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="quote">&gt; -	get_page(page);</span>
<span class="quote">&gt; + unlock:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	split_huge_page(page);</span>
<span class="quote">&gt; +	if (!page)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	split_huge_page(page);</span>
<span class="quote">&gt;  	put_page(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
<span class="quote">&gt; Please read the FAQ at  http://www.tux.org/lkml/</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/dax.h b/include/linux/dax.h</span>
<span class="p_header">index 4f27d3d..9b51f9d 100644</span>
<span class="p_header">--- a/include/linux/dax.h</span>
<span class="p_header">+++ b/include/linux/dax.h</span>
<span class="p_chunk">@@ -18,4 +18,8 @@</span> <span class="p_context"> int dax_pfn_mkwrite(struct vm_area_struct *, struct vm_fault *);</span>
 #define dax_mkwrite(vma, vmf, gb, iod)		dax_fault(vma, vmf, gb, iod)
 #define __dax_mkwrite(vma, vmf, gb, iod)	__dax_fault(vma, vmf, gb, iod)
 
<span class="p_add">+static inline bool vma_is_dax(struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return vma-&gt;vm_file &amp;&amp; IS_DAX(vma-&gt;vm_file-&gt;f_mapping-&gt;host);</span>
<span class="p_add">+}</span>
 #endif
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 911071b..b7bd855 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/pagemap.h&gt;
 #include &lt;linux/migrate.h&gt;
 #include &lt;linux/hashtable.h&gt;
<span class="p_add">+#include &lt;linux/dax.h&gt;</span>
 
 #include &lt;asm/tlb.h&gt;
 #include &lt;asm/pgalloc.h&gt;
<span class="p_chunk">@@ -1391,7 +1392,6 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	int ret = 0;
 
 	if (__pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {
<span class="p_del">-		struct page *page;</span>
 		pgtable_t pgtable;
 		pmd_t orig_pmd;
 		/*
<span class="p_chunk">@@ -1403,13 +1403,22 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,
 							tlb-&gt;fullmm);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
<span class="p_del">-		pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_add">+		if (vma_is_dax(vma)) {</span>
<span class="p_add">+			if (is_huge_zero_pmd(orig_pmd)) {</span>
<span class="p_add">+				pgtable = NULL;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				spin_unlock(ptl);</span>
<span class="p_add">+				return 1;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_add">+		}</span>
 		if (is_huge_zero_pmd(orig_pmd)) {
 			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);
 			spin_unlock(ptl);
 			put_huge_zero_page();
 		} else {
<span class="p_del">-			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			struct page *page = pmd_page(orig_pmd);</span>
 			page_remove_rmap(page);
 			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);
 			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);
<span class="p_chunk">@@ -1418,7 +1427,8 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 			spin_unlock(ptl);
 			tlb_remove_page(tlb, page);
 		}
<span class="p_del">-		pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+		if (pgtable)</span>
<span class="p_add">+			pte_free(tlb-&gt;mm, pgtable);</span>
 		ret = 1;
 	}
 	return ret;
<span class="p_chunk">@@ -2887,7 +2897,7 @@</span> <span class="p_context"> void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
 		pmd_t *pmd)
 {
 	spinlock_t *ptl;
<span class="p_del">-	struct page *page;</span>
<span class="p_add">+	struct page *page = NULL;</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	unsigned long haddr = address &amp; HPAGE_PMD_MASK;
 	unsigned long mmun_start;	/* For mmu_notifiers */
<span class="p_chunk">@@ -2900,25 +2910,25 @@</span> <span class="p_context"> void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
 again:
 	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 	ptl = pmd_lock(mm, pmd);
<span class="p_del">-	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if (is_huge_zero_pmd(*pmd)) {</span>
<span class="p_add">+	if (unlikely(!pmd_trans_huge(*pmd)))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	if (vma_is_dax(vma)) {</span>
<span class="p_add">+		pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="p_add">+	} else if (is_huge_zero_pmd(*pmd)) {</span>
 		__split_huge_zero_page_pmd(vma, haddr, pmd);
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_add">+		get_page(page);</span>
 	}
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_del">-	get_page(page);</span>
<span class="p_add">+ unlock:</span>
 	spin_unlock(ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 
<span class="p_del">-	split_huge_page(page);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return;</span>
 
<span class="p_add">+	split_huge_page(page);</span>
 	put_page(page);
 
 	/*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



