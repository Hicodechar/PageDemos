
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6] x86/mm: Improve TLB flush documentation - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6] x86/mm: Improve TLB flush documentation</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 25, 2017, 2:10 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;b994bd38fd8dbed15e3bf8a0a23dde207b2297c0.1500991817.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9862089/mbox/"
   >mbox</a>
|
   <a href="/patch/9862089/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9862089/">/patch/9862089/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EA89D600F5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 14:10:52 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DEDB326E97
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 14:10:52 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D381028589; Tue, 25 Jul 2017 14:10:52 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5CF3926E97
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 25 Jul 2017 14:10:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752100AbdGYOKt (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 25 Jul 2017 10:10:49 -0400
Received: from mail.kernel.org ([198.145.29.99]:48608 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1750948AbdGYOKs (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 25 Jul 2017 10:10:48 -0400
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 82ACB219AA;
	Tue, 25 Jul 2017 14:10:47 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 82ACB219AA
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH v6] x86/mm: Improve TLB flush documentation
Date: Tue, 25 Jul 2017 07:10:44 -0700
Message-Id: &lt;b994bd38fd8dbed15e3bf8a0a23dde207b2297c0.1500991817.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.9.4
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - July 25, 2017, 2:10 p.m.</div>
<pre class="content">
Improve comments as requested by PeterZ and also add some
documentation at the top of the file.

This adds and removes some smp_mb__after_atomic() calls to make the
code correct even in the absence of x86&#39;s extra-strong atomics.
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---

Changes from v5:
 - Fix blatantly wrong docs (PeterZ, Nadav)
 - Remove the smp_mb__...._atomic() I was supposed to remove, not the one
   I did remove (found by turning on brain and re-reading PeterZ&#39;s email)

arch/x86/include/asm/tlbflush.h |  2 --
 arch/x86/mm/tlb.c               | 45 ++++++++++++++++++++++++++++++++---------
 2 files changed, 35 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 25, 2017, 2:44 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 07:10:44AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; Improve comments as requested by PeterZ and also add some</span>
<span class="quote">&gt; documentation at the top of the file.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This adds and removes some smp_mb__after_atomic() calls to make the</span>
<span class="quote">&gt; code correct even in the absence of x86&#39;s extra-strong atomics.</span>

The main point being that this better documents on which specific
ordering we rely.
<span class="quote">
&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changes from v5:</span>
<span class="quote">&gt;  - Fix blatantly wrong docs (PeterZ, Nadav)</span>
<span class="quote">&gt;  - Remove the smp_mb__...._atomic() I was supposed to remove, not the one</span>
<span class="quote">&gt;    I did remove (found by turning on brain and re-reading PeterZ&#39;s email)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; arch/x86/include/asm/tlbflush.h |  2 --</span>
<span class="quote">&gt;  arch/x86/mm/tlb.c               | 45 ++++++++++++++++++++++++++++++++---------</span>
<span class="quote">&gt;  2 files changed, 35 insertions(+), 12 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; index d23e61dc0640..eb2b44719d57 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; @@ -67,9 +67,7 @@ static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="quote">&gt;  	 * their read of mm_cpumask after their writes to the paging</span>
<span class="quote">&gt;  	 * structures.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	smp_mb__before_atomic();</span>
<span class="quote">&gt;  	new_tlb_gen = atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="quote">&gt; -	smp_mb__after_atomic();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return new_tlb_gen;</span>
<span class="quote">&gt;  }</span>

Right, as atomic*_inc_return() already implies a MB on either side.
<span class="quote">
&gt; diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; index ce104b962a17..0a2e9d0b5503 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; @@ -15,17 +15,24 @@</span>
<span class="quote">&gt;  #include &lt;linux/debugfs.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * The code in this file handles mm switches and TLB flushes.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; + * An mm&#39;s TLB state is logically represented by a totally ordered sequence</span>
<span class="quote">&gt; + * of TLB flushes.  Each flush increments the mm&#39;s tlb_gen.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; + * Each CPU that might have an mm in its TLB (and that might ever use</span>
<span class="quote">&gt; + * those TLB entries) will have an entry for it in its cpu_tlbstate.ctxs</span>
<span class="quote">&gt; + * array.  The kernel maintains the following invariant: for each CPU and</span>
<span class="quote">&gt; + * for each mm in its cpu_tlbstate.ctxs array, the CPU has performed all</span>
<span class="quote">&gt; + * flushes in that mms history up to the tlb_gen in cpu_tlbstate.ctxs</span>
<span class="quote">&gt; + * or the CPU has performed an equivalent set of flushes.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; + * For this purpose, an equivalent set is a set that is at least as strong.</span>
<span class="quote">&gt; + * So, for example, if the flush history is a full flush at time 1,</span>
<span class="quote">&gt; + * a full flush after time 1 is sufficient, but a full flush before time 1</span>
<span class="quote">&gt; + * is not.  Similarly, any number of flushes can be replaced by a single</span>
<span class="quote">&gt; + * full flush so long as that replacement flush is after all the flushes</span>
<span class="quote">&gt; + * that it&#39;s replacing.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);</span>
<span class="quote">&gt; @@ -138,8 +145,18 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;  			return;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Resume remote flushes and then read tlb_gen.  The</span>
<span class="quote">&gt; +		 * barrier synchronizes with inc_mm_tlb_gen() like</span>
<span class="quote">&gt; +		 * this:</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * switch_mm_irqs_off():	flush request:</span>
<span class="quote">&gt; +		 *  cpumask_set_cpu(...);	 inc_mm_tlb_gen();</span>
<span class="quote">&gt; +		 *  MB				 MB</span>
<span class="quote">&gt; +		 *  atomic64_read(.tlb_gen);	 flush_tlb_others(mm_cpumask());</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt;  		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; +		smp_mb__after_atomic();</span>
<span class="quote">&gt;  		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) &lt;</span>
<span class="quote">&gt; @@ -186,9 +203,17 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;  		VM_WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; +		 * Start remote flushes and then read tlb_gen.  As</span>
<span class="quote">&gt; +		 * above, the barrier synchronizes with</span>
<span class="quote">&gt; +		 * inc_mm_tlb_gen() like this:</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * switch_mm_irqs_off():	flush request:</span>
<span class="quote">&gt; +		 *  cpumask_set_cpu(...);	 inc_mm_tlb_gen();</span>
<span class="quote">&gt; +		 *  MB				 MB</span>
<span class="quote">&gt; +		 *  atomic64_read(.tlb_gen);	 flush_tlb_others(mm_cpumask());</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; +		smp_mb__after_atomic();</span>
<span class="quote">&gt;  		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		choose_new_asid(next, next_tlb_gen, &amp;new_asid, &amp;need_flush);</span>

Arguably one could make a helper function of those few lines, not sure
it makes sense, but this duplication seems wasteful.

So we either see the increment or the CPU set, but can not have neither.

Should not arch_tlbbatch_add_mm() also have this same comment? It too
seems to increment and then read the mask.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - July 26, 2017, 1:52 p.m.</div>
<pre class="content">
On Tue, Jul 25, 2017 at 7:44 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Tue, Jul 25, 2017 at 07:10:44AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; Improve comments as requested by PeterZ and also add some</span>
<span class="quote">&gt;&gt; documentation at the top of the file.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This adds and removes some smp_mb__after_atomic() calls to make the</span>
<span class="quote">&gt;&gt; code correct even in the absence of x86&#39;s extra-strong atomics.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The main point being that this better documents on which specific</span>
<span class="quote">&gt; ordering we rely.</span>

Indeed.
<span class="quote">
&gt;&gt;               /*</span>
<span class="quote">&gt;&gt; +              * Start remote flushes and then read tlb_gen.  As</span>
<span class="quote">&gt;&gt; +              * above, the barrier synchronizes with</span>
<span class="quote">&gt;&gt; +              * inc_mm_tlb_gen() like this:</span>
<span class="quote">&gt;&gt; +              *</span>
<span class="quote">&gt;&gt; +              * switch_mm_irqs_off():        flush request:</span>
<span class="quote">&gt;&gt; +              *  cpumask_set_cpu(...);        inc_mm_tlb_gen();</span>
<span class="quote">&gt;&gt; +              *  MB                           MB</span>
<span class="quote">&gt;&gt; +              *  atomic64_read(.tlb_gen);     flush_tlb_others(mm_cpumask());</span>
<span class="quote">&gt;&gt;                */</span>
<span class="quote">&gt;&gt;               cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt;&gt; +             smp_mb__after_atomic();</span>
<span class="quote">&gt;&gt;               next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;               choose_new_asid(next, next_tlb_gen, &amp;new_asid, &amp;need_flush);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Arguably one could make a helper function of those few lines, not sure</span>
<span class="quote">&gt; it makes sense, but this duplication seems wasteful.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So we either see the increment or the CPU set, but can not have neither.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Should not arch_tlbbatch_add_mm() also have this same comment? It too</span>
<span class="quote">&gt; seems to increment and then read the mask.</span>

Hmm.  There&#39;s already this comment in inc_mm_tlb_gen():

        /*
         * Bump the generation count.  This also serves as a full barrier
         * that synchronizes with switch_mm(): callers are required to order
         * their read of mm_cpumask after their writes to the paging
         * structures.
         */

is that not adequate?

FWIW, I have followup patches in the works to further de-deduplicate a
bunch of this code.  I wanted to get the main bits all landed first,
though.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 26, 2017, 2:03 p.m.</div>
<pre class="content">
On Wed, Jul 26, 2017 at 06:52:06AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; On Tue, Jul 25, 2017 at 7:44 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt; &gt; On Tue, Jul 25, 2017 at 07:10:44AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt; Improve comments as requested by PeterZ and also add some</span>
<span class="quote">&gt; &gt;&gt; documentation at the top of the file.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This adds and removes some smp_mb__after_atomic() calls to make the</span>
<span class="quote">&gt; &gt;&gt; code correct even in the absence of x86&#39;s extra-strong atomics.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The main point being that this better documents on which specific</span>
<span class="quote">&gt; &gt; ordering we rely.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;               /*</span>
<span class="quote">&gt; &gt;&gt; +              * Start remote flushes and then read tlb_gen.  As</span>
<span class="quote">&gt; &gt;&gt; +              * above, the barrier synchronizes with</span>
<span class="quote">&gt; &gt;&gt; +              * inc_mm_tlb_gen() like this:</span>
<span class="quote">&gt; &gt;&gt; +              *</span>
<span class="quote">&gt; &gt;&gt; +              * switch_mm_irqs_off():        flush request:</span>
<span class="quote">&gt; &gt;&gt; +              *  cpumask_set_cpu(...);        inc_mm_tlb_gen();</span>
<span class="quote">&gt; &gt;&gt; +              *  MB                           MB</span>
<span class="quote">&gt; &gt;&gt; +              *  atomic64_read(.tlb_gen);     flush_tlb_others(mm_cpumask());</span>
<span class="quote">&gt; &gt;&gt;                */</span>
<span class="quote">&gt; &gt;&gt;               cpumask_set_cpu(cpu, mm_cpumask(next));</span>
<span class="quote">&gt; &gt;&gt; +             smp_mb__after_atomic();</span>
<span class="quote">&gt; &gt;&gt;               next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;               choose_new_asid(next, next_tlb_gen, &amp;new_asid, &amp;need_flush);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Arguably one could make a helper function of those few lines, not sure</span>
<span class="quote">&gt; &gt; it makes sense, but this duplication seems wasteful.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So we either see the increment or the CPU set, but can not have neither.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Should not arch_tlbbatch_add_mm() also have this same comment? It too</span>
<span class="quote">&gt; &gt; seems to increment and then read the mask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm.  There&#39;s already this comment in inc_mm_tlb_gen():</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Bump the generation count.  This also serves as a full barrier</span>
<span class="quote">&gt;          * that synchronizes with switch_mm(): callers are required to order</span>
<span class="quote">&gt;          * their read of mm_cpumask after their writes to the paging</span>
<span class="quote">&gt;          * structures.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; is that not adequate?</span>

Yeah, I suppose so.

Thanks!
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index d23e61dc0640..eb2b44719d57 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -67,9 +67,7 @@</span> <span class="p_context"> static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
 	 * their read of mm_cpumask after their writes to the paging
 	 * structures.
 	 */
<span class="p_del">-	smp_mb__before_atomic();</span>
 	new_tlb_gen = atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);
<span class="p_del">-	smp_mb__after_atomic();</span>
 
 	return new_tlb_gen;
 }
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index ce104b962a17..0a2e9d0b5503 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -15,17 +15,24 @@</span> <span class="p_context"></span>
 #include &lt;linux/debugfs.h&gt;
 
 /*
<span class="p_del">- *	TLB flushing, formerly SMP-only</span>
<span class="p_del">- *		c/o Linus Torvalds.</span>
<span class="p_add">+ * The code in this file handles mm switches and TLB flushes.</span>
  *
<span class="p_del">- *	These mean you can really definitely utterly forget about</span>
<span class="p_del">- *	writing to user space from interrupts. (Its not allowed anyway).</span>
<span class="p_add">+ * An mm&#39;s TLB state is logically represented by a totally ordered sequence</span>
<span class="p_add">+ * of TLB flushes.  Each flush increments the mm&#39;s tlb_gen.</span>
  *
<span class="p_del">- *	Optimizations Manfred Spraul &lt;manfred@colorfullife.com&gt;</span>
<span class="p_add">+ * Each CPU that might have an mm in its TLB (and that might ever use</span>
<span class="p_add">+ * those TLB entries) will have an entry for it in its cpu_tlbstate.ctxs</span>
<span class="p_add">+ * array.  The kernel maintains the following invariant: for each CPU and</span>
<span class="p_add">+ * for each mm in its cpu_tlbstate.ctxs array, the CPU has performed all</span>
<span class="p_add">+ * flushes in that mms history up to the tlb_gen in cpu_tlbstate.ctxs</span>
<span class="p_add">+ * or the CPU has performed an equivalent set of flushes.</span>
  *
<span class="p_del">- *	More scalable flush, from Andi Kleen</span>
<span class="p_del">- *</span>
<span class="p_del">- *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi</span>
<span class="p_add">+ * For this purpose, an equivalent set is a set that is at least as strong.</span>
<span class="p_add">+ * So, for example, if the flush history is a full flush at time 1,</span>
<span class="p_add">+ * a full flush after time 1 is sufficient, but a full flush before time 1</span>
<span class="p_add">+ * is not.  Similarly, any number of flushes can be replaced by a single</span>
<span class="p_add">+ * full flush so long as that replacement flush is after all the flushes</span>
<span class="p_add">+ * that it&#39;s replacing.</span>
  */
 
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
<span class="p_chunk">@@ -138,8 +145,18 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			return;
 		}
 
<span class="p_del">-		/* Resume remote flushes and then read tlb_gen. */</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Resume remote flushes and then read tlb_gen.  The</span>
<span class="p_add">+		 * barrier synchronizes with inc_mm_tlb_gen() like</span>
<span class="p_add">+		 * this:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * switch_mm_irqs_off():	flush request:</span>
<span class="p_add">+		 *  cpumask_set_cpu(...);	 inc_mm_tlb_gen();</span>
<span class="p_add">+		 *  MB				 MB</span>
<span class="p_add">+		 *  atomic64_read(.tlb_gen);	 flush_tlb_others(mm_cpumask());</span>
<span class="p_add">+		 */</span>
 		cpumask_set_cpu(cpu, mm_cpumask(next));
<span class="p_add">+		smp_mb__after_atomic();</span>
 		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);
 
 		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) &lt;
<span class="p_chunk">@@ -186,9 +203,17 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		VM_WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));
 
 		/*
<span class="p_del">-		 * Start remote flushes and then read tlb_gen.</span>
<span class="p_add">+		 * Start remote flushes and then read tlb_gen.  As</span>
<span class="p_add">+		 * above, the barrier synchronizes with</span>
<span class="p_add">+		 * inc_mm_tlb_gen() like this:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * switch_mm_irqs_off():	flush request:</span>
<span class="p_add">+		 *  cpumask_set_cpu(...);	 inc_mm_tlb_gen();</span>
<span class="p_add">+		 *  MB				 MB</span>
<span class="p_add">+		 *  atomic64_read(.tlb_gen);	 flush_tlb_others(mm_cpumask());</span>
 		 */
 		cpumask_set_cpu(cpu, mm_cpumask(next));
<span class="p_add">+		smp_mb__after_atomic();</span>
 		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);
 
 		choose_new_asid(next, next_tlb_gen, &amp;new_asid, &amp;need_flush);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



