
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFT] arm64: kasan: Make KASAN work with 16K pages + 48 bit VA - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFT] arm64: kasan: Make KASAN work with 16K pages + 48 bit VA</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=141821">Andrey Ryabinin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 26, 2015, 1:14 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1448543686-31869-1-git-send-email-aryabinin@virtuozzo.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7706671/mbox/"
   >mbox</a>
|
   <a href="/patch/7706671/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7706671/">/patch/7706671/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id ACB74BF90C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 26 Nov 2015 13:14:39 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id C159A2077D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 26 Nov 2015 13:14:38 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A09BB20779
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 26 Nov 2015 13:14:37 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751995AbbKZNOf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 26 Nov 2015 08:14:35 -0500
Received: from relay.parallels.com ([195.214.232.42]:33728 &quot;EHLO
	relay.parallels.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751016AbbKZNOc (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 26 Nov 2015 08:14:32 -0500
Received: from [10.67.48.55] (helo=mail.parallels.net)
	by relay.parallels.com with esmtps
	(TLSv1.2:ECDHE-RSA-AES256-SHA384:256) (Exim 4.86)
	(envelope-from &lt;aryabinin@odin.com&gt;)
	id 1a1wNX-0001p8-Tt; Thu, 26 Nov 2015 16:14:28 +0300
Received: from localhost.sw.ru (10.30.25.228) by MSK-EXCH1.sw.swsoft.com
	(10.67.48.55) with Microsoft SMTP Server (TLS) id 15.0.1130.7;
	Thu, 26 Nov 2015 14:14:17 +0100
From: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;
To: Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, &lt;linux-arm-kernel@lists.infradead.org&gt;
CC: Yury &lt;yury.norov@gmail.com&gt;, Alexey Klimov &lt;klimov.linux@gmail.com&gt;,
	Arnd Bergmann &lt;arnd@arndb.de&gt;, &lt;linux-mm@kvack.org&gt;,
	Linus Walleij &lt;linus.walleij@linaro.org&gt;,
	Ard Biesheuvel &lt;ard.biesheuvel@linaro.org&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, David Keitel &lt;dkeitel@codeaurora.org&gt;,
	Alexander Potapenko &lt;glider@google.com&gt;,
	Dmitry Vyukov &lt;dvyukov@google.com&gt;,
	&quot;Suzuki K. Poulose&quot; &lt;Suzuki.Poulose@arm.com&gt;,
	Mark Rutland &lt;mark.rutland@arm.com&gt;,
	Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;
Subject: [PATCH RFT] arm64: kasan: Make KASAN work with 16K pages + 48 bit VA
Date: Thu, 26 Nov 2015 16:14:46 +0300
Message-ID: &lt;1448543686-31869-1-git-send-email-aryabinin@virtuozzo.com&gt;
X-Mailer: git-send-email 2.4.10
MIME-Version: 1.0
Content-Type: text/plain
X-ClientProxiedBy: US-EXCH2.sw.swsoft.com (10.255.249.46) To
	MSK-EXCH1.sw.swsoft.com (10.67.48.55)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=141821">Andrey Ryabinin</a> - Nov. 26, 2015, 1:14 p.m.</div>
<pre class="content">
Currently kasan assumes that shadow memory covers one or more entire PGDs.
That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger
than the whole shadow memory.

This patch tries to fix that case.
clear_page_tables() is a new replacement of clear_pgs(). Instead of always
clearing pgds it clears top level page table entries that entirely belongs
to shadow memory.
In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store
puds that now might be cleared by clear_page_tables.

Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;
<span class="signed-off-by">Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
---

 *** THIS is not tested with 16k pages ***

 arch/arm64/mm/kasan_init.c | 87 ++++++++++++++++++++++++++++++++++++++++------
 1 file changed, 76 insertions(+), 11 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Nov. 26, 2015, 2:48 p.m.</div>
<pre class="content">
Hi,

On Thu, Nov 26, 2015 at 04:14:46PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; than the whole shadow memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; to shadow memory.</span>
<span class="quote">&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  *** THIS is not tested with 16k pages ***</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  arch/arm64/mm/kasan_init.c | 87 ++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  1 file changed, 76 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; index cf038c7..ea9f92a 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; @@ -22,6 +22,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);</span>
<span class="quote">&gt; +static pud_t tmp_pud[PAGE_SIZE/sizeof(pud_t)] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  					unsigned long end)</span>
<span class="quote">&gt; @@ -92,20 +93,84 @@ asmlinkage void __init kasan_early_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	BUILD_BUG_ON(KASAN_SHADOW_OFFSET != KASAN_SHADOW_END - (1UL &lt;&lt; 61));</span>
<span class="quote">&gt;  	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));</span>
<span class="quote">&gt; -	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));</span>
<span class="quote">&gt; +	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE));</span>

We also assume that even in the shared PUD case, the shadow region falls
within the same PGD entry, or we would need more than a single tmp_pud.

It would be good to test for that.
<span class="quote">
&gt;  	kasan_map_early_shadow();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void __init clear_pgds(unsigned long start,</span>
<span class="quote">&gt; -			unsigned long end)</span>
<span class="quote">&gt; +static void __init clear_pmds(pud_t *pud, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	do {</span>
<span class="quote">&gt; +		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (IS_ALIGNED(addr, PMD_SIZE) &amp;&amp; end - addr &gt;= PMD_SIZE)</span>
<span class="quote">&gt; +			pmd_clear(pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	} while (pmd++, addr = next, addr != end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init clear_puds(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	do {</span>
<span class="quote">&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (IS_ALIGNED(addr, PUD_SIZE) &amp;&amp; end - addr &gt;= PUD_SIZE)</span>
<span class="quote">&gt; +			pud_clear(pud);</span>

I think this can just be:

		if (next - addr == PUD_SIZE)
			pud_clear(pud);

Given that next can at most be PUD_SIZE from addr, and if so we knwo
addr is aligned.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		if (!pud_none(*pud))</span>
<span class="quote">&gt; +			clear_pmds(pud, addr, next);</span>

I don&#39;t understand this. The KASAN shadow region is PUD_SIZE aligned at
either end, so KASAN should never own a partial pud entry like this.

Regardless, were this case to occur, surely we&#39;d be clearing pmd entries
in the active page tables? We didn&#39;t copy anything at the pmd level.

That doesn&#39;t seem right.
<span class="quote">
&gt; +	} while (pud++, addr = next, addr != end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void __init clear_page_tables(unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	unsigned long next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	do {</span>
<span class="quote">&gt; +		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (IS_ALIGNED(addr, PGDIR_SIZE) &amp;&amp; end - addr &gt;= PGDIR_SIZE)</span>
<span class="quote">&gt; +			pgd_clear(pgd);</span>

As with clear_puds, I think this can be:

		if (next - addr == PGDIR_SIZE)
			pgd_clear(pgd);
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		if (!pgd_none(*pgd))</span>
<span class="quote">&gt; +			clear_puds(pgd, addr, next);</span>
<span class="quote">&gt; +	} while (pgd++, addr = next, addr != end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void copy_pagetables(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * Remove references to kasan page tables from</span>
<span class="quote">&gt; -	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt; -	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt; +	 * If kasan shadow shares PGD with other mappings,</span>
<span class="quote">&gt; +	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="quote">&gt; +	 * so we need temporary pud table to keep early shadow mapped.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt; -		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt; +	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="quote">&gt; +		pud_t *pud;</span>
<span class="quote">&gt; +		pmd_t *pmd;</span>
<span class="quote">&gt; +		pte_t *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="quote">&gt; +		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; +		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="quote">&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; +		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>

I don&#39;t understand why we need to do anything below the pud level here.
We only copy down to the pud level, and we already initialised the
shared ptes and pmds earlier.

Regardless of this patch, we currently initialise the shared tables
repeatedly, which is redundant after the first time we initialise them.
We could improve that.
<span class="quote">
&gt; +	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __init cpu_set_ttbr1(unsigned long ttbr1)</span>
<span class="quote">&gt; @@ -123,16 +188,16 @@ void __init kasan_init(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * We are going to perform proper setup of shadow memory.</span>
<span class="quote">&gt; -	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="quote">&gt; +	 * At first we should unmap early shadow (clear_page_tables()).</span>
<span class="quote">&gt;  	 * However, instrumented code couldn&#39;t execute without shadow memory.</span>
<span class="quote">&gt;  	 * tmp_pg_dir used to keep early shadow mapped until full shadow</span>
<span class="quote">&gt;  	 * setup will be finished.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt; +	copy_pagetables();</span>
<span class="quote">&gt;  	cpu_set_ttbr1(__pa(tmp_pg_dir));</span>
<span class="quote">&gt;  	flush_tlb_all();</span>

As a heads-up, I will shortly have patches altering the swap of TTBR1,
as it risks conflicting TLB entries and misses barriers.

Otherwise, we need a dsb(ishst) between the memcpy and writing to the
TTBR to ensure that the tables are visible to the MMU.

Thanks,
Mark.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=141821">Andrey Ryabinin</a> - Nov. 26, 2015, 3:47 p.m.</div>
<pre class="content">
On 11/26/2015 05:48 PM, Mark Rutland wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, Nov 26, 2015 at 04:14:46PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt;&gt; to shadow memory.</span>
<span class="quote">&gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  *** THIS is not tested with 16k pages ***</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  arch/arm64/mm/kasan_init.c | 87 ++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;  1 file changed, 76 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt; index cf038c7..ea9f92a 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt; @@ -22,6 +22,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);</span>
<span class="quote">&gt;&gt; +static pud_t tmp_pud[PAGE_SIZE/sizeof(pud_t)] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;&gt;  					unsigned long end)</span>
<span class="quote">&gt;&gt; @@ -92,20 +93,84 @@ asmlinkage void __init kasan_early_init(void)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	BUILD_BUG_ON(KASAN_SHADOW_OFFSET != KASAN_SHADOW_END - (1UL &lt;&lt; 61));</span>
<span class="quote">&gt;&gt;  	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));</span>
<span class="quote">&gt;&gt; -	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));</span>
<span class="quote">&gt;&gt; +	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We also assume that even in the shared PUD case, the shadow region falls</span>
<span class="quote">&gt; within the same PGD entry, or we would need more than a single tmp_pud.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be good to test for that.</span>
<span class="quote">&gt; </span>

Something like this:

	#define KASAN_SHADOW_SIZE (KASAN_SHADOW_END - KASAN_SHADOW_START)

	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGD_SIZE)
			 &amp;&amp; !((PGDIR_SIZE &gt; KASAN_SHADOW_SIZE)
				 &amp;&amp; IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE)));
<span class="quote">


&gt;&gt; +static void __init clear_puds(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pud_t *pud;</span>
<span class="quote">&gt;&gt; +	unsigned long next;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	do {</span>
<span class="quote">&gt;&gt; +		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; +		if (IS_ALIGNED(addr, PUD_SIZE) &amp;&amp; end - addr &gt;= PUD_SIZE)</span>
<span class="quote">&gt;&gt; +			pud_clear(pud);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this can just be:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (next - addr == PUD_SIZE)</span>
<span class="quote">&gt; 			pud_clear(pud);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that next can at most be PUD_SIZE from addr, and if so we knwo</span>
<span class="quote">&gt; addr is aligned.</span>
<span class="quote">&gt; </span>

Right.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (!pud_none(*pud))</span>
<span class="quote">&gt;&gt; +			clear_pmds(pud, addr, next);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t understand this. The KASAN shadow region is PUD_SIZE aligned at</span>
<span class="quote">&gt; either end, so KASAN should never own a partial pud entry like this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Regardless, were this case to occur, surely we&#39;d be clearing pmd entries</span>
<span class="quote">&gt; in the active page tables? We didn&#39;t copy anything at the pmd level.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That doesn&#39;t seem right.</span>
<span class="quote">&gt; </span>

Just take a look at p?d_clear() macroses, under CONFIG_PGTABLE_LEVELS=2 for example.
pgd_clear() and pud_clear() is nops, and pmd_clear() is actually clears pgd.

I could replace p?d_clear() with set_p?d(p?d, __p?d(0)).
In that case going down to pmds is not needed, set_p?d() macro will do it for us.


...
<span class="quote">
&gt;&gt; +static void copy_pagetables(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt; -	 * Remove references to kasan page tables from</span>
<span class="quote">&gt;&gt; -	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt;&gt; -	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt;&gt; +	 * If kasan shadow shares PGD with other mappings,</span>
<span class="quote">&gt;&gt; +	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="quote">&gt;&gt; +	 * so we need temporary pud table to keep early shadow mapped.</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt;&gt; -		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt;&gt; +	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="quote">&gt;&gt; +		pud_t *pud;</span>
<span class="quote">&gt;&gt; +		pmd_t *pmd;</span>
<span class="quote">&gt;&gt; +		pte_t *pte;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="quote">&gt;&gt; +		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt; +		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt;&gt; +		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t understand why we need to do anything below the pud level here.</span>
<span class="quote">&gt; We only copy down to the pud level, and we already initialised the</span>
<span class="quote">&gt; shared ptes and pmds earlier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Regardless of this patch, we currently initialise the shared tables</span>
<span class="quote">&gt; repeatedly, which is redundant after the first time we initialise them.</span>
<span class="quote">&gt; We could improve that.</span>
<span class="quote">&gt; </span>

Sure, just pgd_populate() will work here, because this code is only for 16K+48-bit,
which has 4-level pagetables.
But it wouldn&#39;t work if 16k+48-bit would have &gt; 4-level.
Because pgd_populate() in nop in such case, so we need to go down to actually set &#39;tmp_pud&#39;

I just tried to avoid assumptions about number of pagetables levels in that code.
<span class="quote">



&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  static void __init cpu_set_ttbr1(unsigned long ttbr1)</span>
<span class="quote">&gt;&gt; @@ -123,16 +188,16 @@ void __init kasan_init(void)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * We are going to perform proper setup of shadow memory.</span>
<span class="quote">&gt;&gt; -	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="quote">&gt;&gt; +	 * At first we should unmap early shadow (clear_page_tables()).</span>
<span class="quote">&gt;&gt;  	 * However, instrumented code couldn&#39;t execute without shadow memory.</span>
<span class="quote">&gt;&gt;  	 * tmp_pg_dir used to keep early shadow mapped until full shadow</span>
<span class="quote">&gt;&gt;  	 * setup will be finished.</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; -	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt;&gt; +	copy_pagetables();</span>
<span class="quote">&gt;&gt;  	cpu_set_ttbr1(__pa(tmp_pg_dir));</span>
<span class="quote">&gt;&gt;  	flush_tlb_all();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As a heads-up, I will shortly have patches altering the swap of TTBR1,</span>
<span class="quote">&gt; as it risks conflicting TLB entries and misses barriers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Otherwise, we need a dsb(ishst) between the memcpy and writing to the</span>
<span class="quote">&gt; TTBR to ensure that the tables are visible to the MMU.</span>
<span class="quote">&gt; </span>

Thanks.
<span class="quote">
&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt; </span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Nov. 26, 2015, 4:21 p.m.</div>
<pre class="content">
On Thu, Nov 26, 2015 at 06:47:36PM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 11/26/2015 05:48 PM, Mark Rutland wrote:</span>
<span class="quote">&gt; &gt; Hi,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Thu, Nov 26, 2015 at 04:14:46PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt; &gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; &gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; &gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; &gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; &gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; &gt;&gt; to shadow memory.</span>
<span class="quote">&gt; &gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; &gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;  *** THIS is not tested with 16k pages ***</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;  arch/arm64/mm/kasan_init.c | 87 ++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt; &gt;&gt;  1 file changed, 76 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; &gt;&gt; index cf038c7..ea9f92a 100644</span>
<span class="quote">&gt; &gt;&gt; --- a/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; &gt;&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt; &gt;&gt; @@ -22,6 +22,7 @@</span>
<span class="quote">&gt; &gt;&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;  static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);</span>
<span class="quote">&gt; &gt;&gt; +static pud_t tmp_pud[PAGE_SIZE/sizeof(pud_t)] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;  static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt;&gt;  					unsigned long end)</span>
<span class="quote">&gt; &gt;&gt; @@ -92,20 +93,84 @@ asmlinkage void __init kasan_early_init(void)</span>
<span class="quote">&gt; &gt;&gt;  {</span>
<span class="quote">&gt; &gt;&gt;  	BUILD_BUG_ON(KASAN_SHADOW_OFFSET != KASAN_SHADOW_END - (1UL &lt;&lt; 61));</span>
<span class="quote">&gt; &gt;&gt;  	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));</span>
<span class="quote">&gt; &gt;&gt; -	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));</span>
<span class="quote">&gt; &gt;&gt; +	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE));</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We also assume that even in the shared PUD case, the shadow region falls</span>
<span class="quote">&gt; &gt; within the same PGD entry, or we would need more than a single tmp_pud.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It would be good to test for that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Something like this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	#define KASAN_SHADOW_SIZE (KASAN_SHADOW_END - KASAN_SHADOW_START)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGD_SIZE)</span>
<span class="quote">&gt; 			 &amp;&amp; !((PGDIR_SIZE &gt; KASAN_SHADOW_SIZE)</span>
<span class="quote">&gt; 				 &amp;&amp; IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE)));</span>

I was thinking something more like:

	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE);
	BUILD_BUG_ON(KASAN_SHADOW_START &gt;&gt; PGDIR_SHIFT !=
		     KASAN_SHADOW_END &gt;&gt; PGDIR_SHIFT);
<span class="quote">
&gt; &gt;&gt; +		if (!pud_none(*pud))</span>
<span class="quote">&gt; &gt;&gt; +			clear_pmds(pud, addr, next);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t understand this. The KASAN shadow region is PUD_SIZE aligned at</span>
<span class="quote">&gt; &gt; either end, so KASAN should never own a partial pud entry like this.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Regardless, were this case to occur, surely we&#39;d be clearing pmd entries</span>
<span class="quote">&gt; &gt; in the active page tables? We didn&#39;t copy anything at the pmd level.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That doesn&#39;t seem right.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just take a look at p?d_clear() macroses, under CONFIG_PGTABLE_LEVELS=2 for example.</span>
<span class="quote">&gt; pgd_clear() and pud_clear() is nops, and pmd_clear() is actually clears pgd.</span>

I see. Thanks for pointing that out.

I detest the weird folding behaviour we have in the p??_* macros. It
violates least surprise almost every time.
<span class="quote">
&gt; I could replace p?d_clear() with set_p?d(p?d, __p?d(0)).</span>
<span class="quote">&gt; In that case going down to pmds is not needed, set_p?d() macro will do it for us.</span>

I think it would be simpler to rely on the fact that we only use puds
with 4 levels of table (and hence the p??_* macros will operate at the
levels their names imply).

We can verify that at build time with:

BUILD_BUG_ON(CONFIG_PGTABLE_LEVELS != 4 &amp;&amp;
	     (!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE) ||
	      !IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE)));
<span class="quote">
&gt; &gt;&gt; +static void copy_pagetables(void)</span>
<span class="quote">&gt; &gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt; +	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;  	/*</span>
<span class="quote">&gt; &gt;&gt; -	 * Remove references to kasan page tables from</span>
<span class="quote">&gt; &gt;&gt; -	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt; &gt;&gt; -	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt; &gt;&gt; +	 * If kasan shadow shares PGD with other mappings,</span>
<span class="quote">&gt; &gt;&gt; +	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="quote">&gt; &gt;&gt; +	 * so we need temporary pud table to keep early shadow mapped.</span>
<span class="quote">&gt; &gt;&gt;  	 */</span>
<span class="quote">&gt; &gt;&gt; -	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt; &gt;&gt; -		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt; &gt;&gt; +	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="quote">&gt; &gt;&gt; +		pud_t *pud;</span>
<span class="quote">&gt; &gt;&gt; +		pmd_t *pmd;</span>
<span class="quote">&gt; &gt;&gt; +		pte_t *pte;</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="quote">&gt; &gt;&gt; +		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt; +		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; &gt;&gt; +		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t understand why we need to do anything below the pud level here.</span>
<span class="quote">&gt; &gt; We only copy down to the pud level, and we already initialised the</span>
<span class="quote">&gt; &gt; shared ptes and pmds earlier.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Regardless of this patch, we currently initialise the shared tables</span>
<span class="quote">&gt; &gt; repeatedly, which is redundant after the first time we initialise them.</span>
<span class="quote">&gt; &gt; We could improve that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, just pgd_populate() will work here, because this code is only for 16K+48-bit,</span>
<span class="quote">&gt; which has 4-level pagetables.</span>
<span class="quote">&gt; But it wouldn&#39;t work if 16k+48-bit would have &gt; 4-level.</span>
<span class="quote">&gt; Because pgd_populate() in nop in such case, so we need to go down to actually set &#39;tmp_pud&#39;</span>

I don&#39;t follow.

16K + 48-bit will always require 4 levels given the page table format.
We never have more than 4 levels.

Thanks,
Mark.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=141821">Andrey Ryabinin</a> - Nov. 26, 2015, 4:40 p.m.</div>
<pre class="content">
On 11/26/2015 07:21 PM, Mark Rutland wrote:
<span class="quote">&gt; On Thu, Nov 26, 2015 at 06:47:36PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On 11/26/2015 05:48 PM, Mark Rutland wrote:</span>
<span class="quote">&gt;&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Thu, Nov 26, 2015 at 04:14:46PM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt;&gt;&gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt;&gt;&gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt;&gt;&gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt;&gt;&gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt;&gt;&gt;&gt; to shadow memory.</span>
<span class="quote">&gt;&gt;&gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt;&gt;&gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  *** THIS is not tested with 16k pages ***</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  arch/arm64/mm/kasan_init.c | 87 ++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;&gt;&gt;  1 file changed, 76 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt;&gt;&gt; index cf038c7..ea9f92a 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/arch/arm64/mm/kasan_init.c</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -22,6 +22,7 @@</span>
<span class="quote">&gt;&gt;&gt;&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;  static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt; +static pud_t tmp_pud[PAGE_SIZE/sizeof(pud_t)] __initdata __aligned(PAGE_SIZE);</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;  static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;&gt;&gt;&gt;  					unsigned long end)</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -92,20 +93,84 @@ asmlinkage void __init kasan_early_init(void)</span>
<span class="quote">&gt;&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;&gt;  	BUILD_BUG_ON(KASAN_SHADOW_OFFSET != KASAN_SHADOW_END - (1UL &lt;&lt; 61));</span>
<span class="quote">&gt;&gt;&gt;&gt;  	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));</span>
<span class="quote">&gt;&gt;&gt;&gt; -	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE));</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We also assume that even in the shared PUD case, the shadow region falls</span>
<span class="quote">&gt;&gt;&gt; within the same PGD entry, or we would need more than a single tmp_pud.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It would be good to test for that.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Something like this:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 	#define KASAN_SHADOW_SIZE (KASAN_SHADOW_END - KASAN_SHADOW_START)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGD_SIZE)</span>
<span class="quote">&gt;&gt; 			 &amp;&amp; !((PGDIR_SIZE &gt; KASAN_SHADOW_SIZE)</span>
<span class="quote">&gt;&gt; 				 &amp;&amp; IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE)));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I was thinking something more like:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE);</span>
<span class="quote">&gt; 	BUILD_BUG_ON(KASAN_SHADOW_START &gt;&gt; PGDIR_SHIFT !=</span>
<span class="quote">&gt; 		     KASAN_SHADOW_END &gt;&gt; PGDIR_SHIFT);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (!pud_none(*pud))</span>
<span class="quote">&gt;&gt;&gt;&gt; +			clear_pmds(pud, addr, next);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I don&#39;t understand this. The KASAN shadow region is PUD_SIZE aligned at</span>
<span class="quote">&gt;&gt;&gt; either end, so KASAN should never own a partial pud entry like this.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Regardless, were this case to occur, surely we&#39;d be clearing pmd entries</span>
<span class="quote">&gt;&gt;&gt; in the active page tables? We didn&#39;t copy anything at the pmd level.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That doesn&#39;t seem right.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Just take a look at p?d_clear() macroses, under CONFIG_PGTABLE_LEVELS=2 for example.</span>
<span class="quote">&gt;&gt; pgd_clear() and pud_clear() is nops, and pmd_clear() is actually clears pgd.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see. Thanks for pointing that out.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I detest the weird folding behaviour we have in the p??_* macros. It</span>
<span class="quote">&gt; violates least surprise almost every time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; I could replace p?d_clear() with set_p?d(p?d, __p?d(0)).</span>
<span class="quote">&gt;&gt; In that case going down to pmds is not needed, set_p?d() macro will do it for us.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it would be simpler to rely on the fact that we only use puds</span>
<span class="quote">&gt; with 4 levels of table (and hence the p??_* macros will operate at the</span>
<span class="quote">&gt; levels their names imply).</span>
<span class="quote">&gt; </span>

It&#39;s not only about puds.
E.g. if we need to clear PGD with 2-level page tables, than we need to call pmd_clear().

So we should either leave this code as is, or switch to set_pgd/set_pud.
<span class="quote">

&gt; We can verify that at build time with:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; BUILD_BUG_ON(CONFIG_PGTABLE_LEVELS != 4 &amp;&amp;</span>
<span class="quote">&gt; 	     (!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE) ||</span>
<span class="quote">&gt; 	      !IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE)));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; +static void copy_pagetables(void)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;&gt; -	 * Remove references to kasan page tables from</span>
<span class="quote">&gt;&gt;&gt;&gt; -	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt;&gt;&gt;&gt; -	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt;&gt;&gt;&gt; +	 * If kasan shadow shares PGD with other mappings,</span>
<span class="quote">&gt;&gt;&gt;&gt; +	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="quote">&gt;&gt;&gt;&gt; +	 * so we need temporary pud table to keep early shadow mapped.</span>
<span class="quote">&gt;&gt;&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt;&gt;&gt; -	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt;&gt;&gt;&gt; -		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pud_t *pud;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pmd_t *pmd;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pte_t *pte;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="quote">&gt;&gt;&gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I don&#39;t understand why we need to do anything below the pud level here.</span>
<span class="quote">&gt;&gt;&gt; We only copy down to the pud level, and we already initialised the</span>
<span class="quote">&gt;&gt;&gt; shared ptes and pmds earlier.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Regardless of this patch, we currently initialise the shared tables</span>
<span class="quote">&gt;&gt;&gt; repeatedly, which is redundant after the first time we initialise them.</span>
<span class="quote">&gt;&gt;&gt; We could improve that.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure, just pgd_populate() will work here, because this code is only for 16K+48-bit,</span>
<span class="quote">&gt;&gt; which has 4-level pagetables.</span>
<span class="quote">&gt;&gt; But it wouldn&#39;t work if 16k+48-bit would have &gt; 4-level.</span>
<span class="quote">&gt;&gt; Because pgd_populate() in nop in such case, so we need to go down to actually set &#39;tmp_pud&#39;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t follow.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 16K + 48-bit will always require 4 levels given the page table format.</span>
<span class="quote">&gt; We never have more than 4 levels.</span>
<span class="quote">&gt; </span>

Oh, it should be &#39;&lt; 4&#39; of course.
Yes, 16K + 48-bit is always 4-levels, but I tried to not rely on this here.

But since we can rely on 4-levels here, I&#39;m gonna leave only pgd_populate() and add you BUILD_BUG_ON().
<span class="quote">

&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt; </span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66681">Ard Biesheuvel</a> - Nov. 26, 2015, 4:40 p.m.</div>
<pre class="content">
On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:
<span class="quote">&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; than the whole shadow memory.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; to shadow memory.</span>
<span class="quote">&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>

I would argue that the Kasan code is complicated enough, and we should
avoid complicating it even further for a configuration that is highly
theoretical in nature.

In a 16k configuration, the 4th level only adds a single bit of VA
space (which is, as I understand it, exactly the issue you need to
address here since the top level page table has only 2 entries and
hence does not divide by 8 cleanly), which means you are better off
using 3 levels unless you *really* need more than 128 TB of VA space.

So can&#39;t we just live with the limitation, and keep the current code?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Nov. 26, 2015, 5:08 p.m.</div>
<pre class="content">
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		if (!pud_none(*pud))</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +			clear_pmds(pud, addr, next);</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I don&#39;t understand this. The KASAN shadow region is PUD_SIZE aligned at</span>
<span class="quote">&gt; &gt;&gt;&gt; either end, so KASAN should never own a partial pud entry like this.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Regardless, were this case to occur, surely we&#39;d be clearing pmd entries</span>
<span class="quote">&gt; &gt;&gt;&gt; in the active page tables? We didn&#39;t copy anything at the pmd level.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; That doesn&#39;t seem right.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Just take a look at p?d_clear() macroses, under CONFIG_PGTABLE_LEVELS=2 for example.</span>
<span class="quote">&gt; &gt;&gt; pgd_clear() and pud_clear() is nops, and pmd_clear() is actually clears pgd.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I see. Thanks for pointing that out.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I detest the weird folding behaviour we have in the p??_* macros. It</span>
<span class="quote">&gt; &gt; violates least surprise almost every time.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; I could replace p?d_clear() with set_p?d(p?d, __p?d(0)).</span>
<span class="quote">&gt; &gt;&gt; In that case going down to pmds is not needed, set_p?d() macro will do it for us.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think it would be simpler to rely on the fact that we only use puds</span>
<span class="quote">&gt; &gt; with 4 levels of table (and hence the p??_* macros will operate at the</span>
<span class="quote">&gt; &gt; levels their names imply).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s not only about puds.</span>
<span class="quote">&gt; E.g. if we need to clear PGD with 2-level page tables, than we need to call pmd_clear().</span>

Ah. Yes :(

I will reiterate that I hate the folding behaviour.
<span class="quote">
&gt; So we should either leave this code as is, or switch to set_pgd/set_pud.</span>

I think set_p?d is preferable.
<span class="quote">
&gt; &gt; We can verify that at build time with:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; BUILD_BUG_ON(CONFIG_PGTABLE_LEVELS != 4 &amp;&amp;</span>
<span class="quote">&gt; &gt; 	     (!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE) ||</span>
<span class="quote">&gt; &gt; 	      !IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE)));</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +static void copy_pagetables(void)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; -	 * Remove references to kasan page tables from</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; -	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; -	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	 * If kasan shadow shares PGD with other mappings,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	 * so we need temporary pud table to keep early shadow mapped.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;  	 */</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; -	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; -		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pud_t *pud;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pmd_t *pmd;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pte_t *pte;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; +		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I don&#39;t understand why we need to do anything below the pud level here.</span>
<span class="quote">&gt; &gt;&gt;&gt; We only copy down to the pud level, and we already initialised the</span>
<span class="quote">&gt; &gt;&gt;&gt; shared ptes and pmds earlier.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Regardless of this patch, we currently initialise the shared tables</span>
<span class="quote">&gt; &gt;&gt;&gt; repeatedly, which is redundant after the first time we initialise them.</span>
<span class="quote">&gt; &gt;&gt;&gt; We could improve that.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Sure, just pgd_populate() will work here, because this code is only for 16K+48-bit,</span>
<span class="quote">&gt; &gt;&gt; which has 4-level pagetables.</span>
<span class="quote">&gt; &gt;&gt; But it wouldn&#39;t work if 16k+48-bit would have &gt; 4-level.</span>
<span class="quote">&gt; &gt;&gt; Because pgd_populate() in nop in such case, so we need to go down to actually set &#39;tmp_pud&#39;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t follow.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 16K + 48-bit will always require 4 levels given the page table format.</span>
<span class="quote">&gt; &gt; We never have more than 4 levels.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh, it should be &#39;&lt; 4&#39; of course.</span>
<span class="quote">&gt; Yes, 16K + 48-bit is always 4-levels, but I tried to not rely on this here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But since we can rely on 4-levels here, I&#39;m gonna leave only pgd_populate() and add you BUILD_BUG_ON().</span>

Ok. That sounds good to me.

Thanks,
Mark.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=141821">Andrey Ryabinin</a> - Nov. 27, 2015, 8:12 a.m.</div>
<pre class="content">
On 11/26/2015 07:40 PM, Ard Biesheuvel wrote:
<span class="quote">&gt; On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt;&gt; to shadow memory.</span>
<span class="quote">&gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would argue that the Kasan code is complicated enough, and we should</span>
<span class="quote">&gt; avoid complicating it even further for a configuration that is highly</span>
<span class="quote">&gt; theoretical in nature.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In a 16k configuration, the 4th level only adds a single bit of VA</span>
<span class="quote">&gt; space (which is, as I understand it, exactly the issue you need to</span>
<span class="quote">&gt; address here since the top level page table has only 2 entries and</span>
<span class="quote">&gt; hence does not divide by 8 cleanly), which means you are better off</span>
<span class="quote">&gt; using 3 levels unless you *really* need more than 128 TB of VA space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So can&#39;t we just live with the limitation, and keep the current code?</span>
 

No objections from my side. Let&#39;s keep the current code.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Nov. 27, 2015, 9:35 a.m.</div>
<pre class="content">
On Fri, Nov 27, 2015 at 11:12:28AM +0300, Andrey Ryabinin wrote:
<span class="quote">&gt; On 11/26/2015 07:40 PM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt; &gt; On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; &gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; &gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; &gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; &gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; &gt;&gt; to shadow memory.</span>
<span class="quote">&gt; &gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; &gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I would argue that the Kasan code is complicated enough, and we should</span>
<span class="quote">&gt; &gt; avoid complicating it even further for a configuration that is highly</span>
<span class="quote">&gt; &gt; theoretical in nature.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In a 16k configuration, the 4th level only adds a single bit of VA</span>
<span class="quote">&gt; &gt; space (which is, as I understand it, exactly the issue you need to</span>
<span class="quote">&gt; &gt; address here since the top level page table has only 2 entries and</span>
<span class="quote">&gt; &gt; hence does not divide by 8 cleanly), which means you are better off</span>
<span class="quote">&gt; &gt; using 3 levels unless you *really* need more than 128 TB of VA space.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So can&#39;t we just live with the limitation, and keep the current code?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No objections from my side. Let&#39;s keep the current code.</span>

Ard had a good point, so fine by me as well.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Nov. 27, 2015, 10:02 a.m.</div>
<pre class="content">
On Fri, Nov 27, 2015 at 09:35:29AM +0000, Catalin Marinas wrote:
<span class="quote">&gt; On Fri, Nov 27, 2015 at 11:12:28AM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt; &gt; On 11/26/2015 07:40 PM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt; &gt; &gt; On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; &gt; &gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; &gt; &gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; &gt; &gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; &gt; &gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; &gt; &gt;&gt; to shadow memory.</span>
<span class="quote">&gt; &gt; &gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; &gt; &gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; &gt; &gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I would argue that the Kasan code is complicated enough, and we should</span>
<span class="quote">&gt; &gt; &gt; avoid complicating it even further for a configuration that is highly</span>
<span class="quote">&gt; &gt; &gt; theoretical in nature.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In a 16k configuration, the 4th level only adds a single bit of VA</span>
<span class="quote">&gt; &gt; &gt; space (which is, as I understand it, exactly the issue you need to</span>
<span class="quote">&gt; &gt; &gt; address here since the top level page table has only 2 entries and</span>
<span class="quote">&gt; &gt; &gt; hence does not divide by 8 cleanly), which means you are better off</span>
<span class="quote">&gt; &gt; &gt; using 3 levels unless you *really* need more than 128 TB of VA space.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; So can&#39;t we just live with the limitation, and keep the current code?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No objections from my side. Let&#39;s keep the current code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ard had a good point, so fine by me as well.</span>

Ok, so obvious follow-up question: why do we even support 48-bit + 16k
pages in the kernel? Either it&#39;s useful, and we make things work with it,
or it&#39;s not and we can drop it (or, at least, hide it behind EXPERT like
we do for 36-bit).

Will
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66681">Ard Biesheuvel</a> - Nov. 27, 2015, 10:39 a.m.</div>
<pre class="content">
On 27 November 2015 at 11:02, Will Deacon &lt;will.deacon@arm.com&gt; wrote:
<span class="quote">&gt; On Fri, Nov 27, 2015 at 09:35:29AM +0000, Catalin Marinas wrote:</span>
<span class="quote">&gt;&gt; On Fri, Nov 27, 2015 at 11:12:28AM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt;&gt; &gt; On 11/26/2015 07:40 PM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt; On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; to shadow memory.</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt;&gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; I would argue that the Kasan code is complicated enough, and we should</span>
<span class="quote">&gt;&gt; &gt; &gt; avoid complicating it even further for a configuration that is highly</span>
<span class="quote">&gt;&gt; &gt; &gt; theoretical in nature.</span>
<span class="quote">&gt;&gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; In a 16k configuration, the 4th level only adds a single bit of VA</span>
<span class="quote">&gt;&gt; &gt; &gt; space (which is, as I understand it, exactly the issue you need to</span>
<span class="quote">&gt;&gt; &gt; &gt; address here since the top level page table has only 2 entries and</span>
<span class="quote">&gt;&gt; &gt; &gt; hence does not divide by 8 cleanly), which means you are better off</span>
<span class="quote">&gt;&gt; &gt; &gt; using 3 levels unless you *really* need more than 128 TB of VA space.</span>
<span class="quote">&gt;&gt; &gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; &gt; So can&#39;t we just live with the limitation, and keep the current code?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; No objections from my side. Let&#39;s keep the current code.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ard had a good point, so fine by me as well.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok, so obvious follow-up question: why do we even support 48-bit + 16k</span>
<span class="quote">&gt; pages in the kernel? Either it&#39;s useful, and we make things work with it,</span>
<span class="quote">&gt; or it&#39;s not and we can drop it (or, at least, hide it behind EXPERT like</span>
<span class="quote">&gt; we do for 36-bit).</span>
<span class="quote">&gt;</span>

So there&#39;s 10 kinds of features in the world, useful ones and !useful ones? :-)

I think 48-bit/16k is somewhat useful, and I think we should support
it. But I also think we should be pragmatic, and not go out of our way
to support the combinatorial expansion of all niche features enabled
together. I think it is perfectly fine to limit kasan support to
configurations whose top level translation table divides by 8 cleanly
(which only excludes 16k/48-bit anyway)

However, I think it deserves being hidden behind CONFIG_EXPERT more
than 36-bit/16k does.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - Nov. 27, 2015, 2:11 p.m.</div>
<pre class="content">
On Fri, Nov 27, 2015 at 10:02:11AM +0000, Will Deacon wrote:
<span class="quote">&gt; On Fri, Nov 27, 2015 at 09:35:29AM +0000, Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Fri, Nov 27, 2015 at 11:12:28AM +0300, Andrey Ryabinin wrote:</span>
<span class="quote">&gt; &gt; &gt; On 11/26/2015 07:40 PM, Ard Biesheuvel wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On 26 November 2015 at 14:14, Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; Currently kasan assumes that shadow memory covers one or more entire PGDs.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; That&#39;s not true for 16K pages + 48bit VA space, where PGDIR_SIZE is bigger</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; than the whole shadow memory.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; This patch tries to fix that case.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; clear_page_tables() is a new replacement of clear_pgs(). Instead of always</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; clearing pgds it clears top level page table entries that entirely belongs</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; to shadow memory.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; In addition to &#39;tmp_pg_dir&#39; we now have &#39;tmp_pud&#39; which is used to store</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; puds that now might be cleared by clear_page_tables.</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; Reported-by: Suzuki K. Poulose &lt;Suzuki.Poulose@arm.com&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;&gt; Signed-off-by: Andrey Ryabinin &lt;aryabinin@virtuozzo.com&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I would argue that the Kasan code is complicated enough, and we should</span>
<span class="quote">&gt; &gt; &gt; &gt; avoid complicating it even further for a configuration that is highly</span>
<span class="quote">&gt; &gt; &gt; &gt; theoretical in nature.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; In a 16k configuration, the 4th level only adds a single bit of VA</span>
<span class="quote">&gt; &gt; &gt; &gt; space (which is, as I understand it, exactly the issue you need to</span>
<span class="quote">&gt; &gt; &gt; &gt; address here since the top level page table has only 2 entries and</span>
<span class="quote">&gt; &gt; &gt; &gt; hence does not divide by 8 cleanly), which means you are better off</span>
<span class="quote">&gt; &gt; &gt; &gt; using 3 levels unless you *really* need more than 128 TB of VA space.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; So can&#39;t we just live with the limitation, and keep the current code?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; No objections from my side. Let&#39;s keep the current code.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ard had a good point, so fine by me as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, so obvious follow-up question: why do we even support 48-bit + 16k</span>
<span class="quote">&gt; pages in the kernel? Either it&#39;s useful, and we make things work with it,</span>
<span class="quote">&gt; or it&#39;s not and we can drop it (or, at least, hide it behind EXPERT like</span>
<span class="quote">&gt; we do for 36-bit).</span>

One reason is hardware validation (I guess that may be the only reason
for 16KB in general ;)). For each of the page sizes we support two VA
ranges: 48-bit (maximum) and a recommended one for the corresponding
granule. With 16K, the difference is not significant (47 to 48), so we
could follow Ard&#39;s suggestion and make it depend on EXPERT (we already
do this for 16KB and 36-bit VA).
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c</span>
<span class="p_header">index cf038c7..ea9f92a 100644</span>
<span class="p_header">--- a/arch/arm64/mm/kasan_init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/kasan_init.c</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 
 static pgd_t tmp_pg_dir[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);
<span class="p_add">+static pud_t tmp_pud[PAGE_SIZE/sizeof(pud_t)] __initdata __aligned(PAGE_SIZE);</span>
 
 static void __init kasan_early_pte_populate(pmd_t *pmd, unsigned long addr,
 					unsigned long end)
<span class="p_chunk">@@ -92,20 +93,84 @@</span> <span class="p_context"> asmlinkage void __init kasan_early_init(void)</span>
 {
 	BUILD_BUG_ON(KASAN_SHADOW_OFFSET != KASAN_SHADOW_END - (1UL &lt;&lt; 61));
 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));
<span class="p_del">-	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));</span>
<span class="p_add">+	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PUD_SIZE));</span>
 	kasan_map_early_shadow();
 }
 
<span class="p_del">-static void __init clear_pgds(unsigned long start,</span>
<span class="p_del">-			unsigned long end)</span>
<span class="p_add">+static void __init clear_pmds(pud_t *pud, unsigned long addr, unsigned long end)</span>
 {
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		if (IS_ALIGNED(addr, PMD_SIZE) &amp;&amp; end - addr &gt;= PMD_SIZE)</span>
<span class="p_add">+			pmd_clear(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	} while (pmd++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init clear_puds(pgd_t *pgd, unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pud_addr_end(addr, end);</span>
<span class="p_add">+		if (IS_ALIGNED(addr, PUD_SIZE) &amp;&amp; end - addr &gt;= PUD_SIZE)</span>
<span class="p_add">+			pud_clear(pud);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pud_none(*pud))</span>
<span class="p_add">+			clear_pmds(pud, addr, next);</span>
<span class="p_add">+	} while (pud++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init clear_page_tables(unsigned long addr, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	unsigned long next;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset_k(addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		next = pgd_addr_end(addr, end);</span>
<span class="p_add">+		if (IS_ALIGNED(addr, PGDIR_SIZE) &amp;&amp; end - addr &gt;= PGDIR_SIZE)</span>
<span class="p_add">+			pgd_clear(pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pgd_none(*pgd))</span>
<span class="p_add">+			clear_puds(pgd, addr, next);</span>
<span class="p_add">+	} while (pgd++, addr = next, addr != end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void copy_pagetables(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd = tmp_pg_dir + pgd_index(KASAN_SHADOW_START);</span>
<span class="p_add">+</span>
<span class="p_add">+	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * Remove references to kasan page tables from</span>
<span class="p_del">-	 * swapper_pg_dir. pgd_clear() can&#39;t be used</span>
<span class="p_del">-	 * here because it&#39;s nop on 2,3-level pagetable setups</span>
<span class="p_add">+	 * If kasan shadow shares PGD with other mappings,</span>
<span class="p_add">+	 * clear_page_tables() will clear puds instead of pgd,</span>
<span class="p_add">+	 * so we need temporary pud table to keep early shadow mapped.</span>
 	 */
<span class="p_del">-	for (; start &lt; end; start += PGDIR_SIZE)</span>
<span class="p_del">-		set_pgd(pgd_offset_k(start), __pgd(0));</span>
<span class="p_add">+	if (PGDIR_SIZE &gt; KASAN_SHADOW_END - KASAN_SHADOW_START) {</span>
<span class="p_add">+		pud_t *pud;</span>
<span class="p_add">+		pmd_t *pmd;</span>
<span class="p_add">+		pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		memcpy(tmp_pud, pgd_page_vaddr(*pgd), sizeof(tmp_pud));</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd_populate(&amp;init_mm, pgd, tmp_pud);</span>
<span class="p_add">+		pud = pud_offset(pgd, KASAN_SHADOW_START);</span>
<span class="p_add">+		pmd = pmd_offset(pud, KASAN_SHADOW_START);</span>
<span class="p_add">+		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="p_add">+		pte = pte_offset_kernel(pmd, KASAN_SHADOW_START);</span>
<span class="p_add">+		pmd_populate_kernel(&amp;init_mm, pmd, pte);</span>
<span class="p_add">+	}</span>
 }
 
 static void __init cpu_set_ttbr1(unsigned long ttbr1)
<span class="p_chunk">@@ -123,16 +188,16 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 
 	/*
 	 * We are going to perform proper setup of shadow memory.
<span class="p_del">-	 * At first we should unmap early shadow (clear_pgds() call bellow).</span>
<span class="p_add">+	 * At first we should unmap early shadow (clear_page_tables()).</span>
 	 * However, instrumented code couldn&#39;t execute without shadow memory.
 	 * tmp_pg_dir used to keep early shadow mapped until full shadow
 	 * setup will be finished.
 	 */
<span class="p_del">-	memcpy(tmp_pg_dir, swapper_pg_dir, sizeof(tmp_pg_dir));</span>
<span class="p_add">+	copy_pagetables();</span>
 	cpu_set_ttbr1(__pa(tmp_pg_dir));
 	flush_tlb_all();
 
<span class="p_del">-	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
<span class="p_add">+	clear_page_tables(KASAN_SHADOW_START, KASAN_SHADOW_END);</span>
 
 	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,
 			kasan_mem_to_shadow((void *)MODULES_VADDR));

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



