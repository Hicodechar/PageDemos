
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm: rename and document alloc_pages_exact_node - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm: rename and document alloc_pages_exact_node</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 21, 2015, 1:55 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437486951-19898-1-git-send-email-vbabka@suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6836361/mbox/"
   >mbox</a>
|
   <a href="/patch/6836361/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6836361/">/patch/6836361/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id B51BC9F380
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 13:56:45 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 251C62069A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 13:56:41 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5E525206A4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 13:56:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932944AbbGUN4V (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 21 Jul 2015 09:56:21 -0400
Received: from cantor2.suse.de ([195.135.220.15]:40112 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932187AbbGUN4S (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 21 Jul 2015 09:56:18 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 50B6EAAC8;
	Tue, 21 Jul 2015 13:56:13 +0000 (UTC)
From: Vlastimil Babka &lt;vbabka@suse.cz&gt;
To: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	linux-ia64@vger.kernel.org, linuxppc-dev@lists.ozlabs.org,
	cbe-oss-dev@lists.ozlabs.org, kvm@vger.kernel.org,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Greg Thelen &lt;gthelen@google.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Christoph Lameter &lt;cl@linux.com&gt;, Pekka Enberg &lt;penberg@kernel.org&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Tony Luck &lt;tony.luck@intel.com&gt;, Fenghua Yu &lt;fenghua.yu@intel.com&gt;,
	Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Paul Mackerras &lt;paulus@samba.org&gt;, Michael Ellerman &lt;mpe@ellerman.id.au&gt;,
	Gleb Natapov &lt;gleb@kernel.org&gt;, Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Cliff Whickman &lt;cpw@sgi.com&gt;,
	Robin Holt &lt;robinmholt@gmail.com&gt;
Subject: [PATCH] mm: rename and document alloc_pages_exact_node
Date: Tue, 21 Jul 2015 15:55:51 +0200
Message-Id: &lt;1437486951-19898-1-git-send-email-vbabka@suse.cz&gt;
X-Mailer: git-send-email 2.4.5
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-3.1 required=5.0 tests=BAYES_00,RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 21, 2015, 1:55 p.m.</div>
<pre class="content">
The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page
allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)
as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id
to be -1. Unfortunately the name of the function can easily suggest that the
allocation is restricted to the given node. In truth, the node is only
preferred, unless __GFP_THISNODE is among the gfp flags.

The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,
thp: really limit transparent hugepage allocation to local node&quot;) and
b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).

To prevent further mistakes, this patch renames the function to
alloc_pages_prefer_node() and documents it together with alloc_pages_node().
<span class="signed-off-by">
Signed-off-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
Cc: Mel Gorman &lt;mgorman@suse.de&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: Greg Thelen &lt;gthelen@google.com&gt;
Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Cc: Christoph Lameter &lt;cl@linux.com&gt;
Cc: Pekka Enberg &lt;penberg@kernel.org&gt;
Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: Tony Luck &lt;tony.luck@intel.com&gt;
Cc: Fenghua Yu &lt;fenghua.yu@intel.com&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;
Cc: Paul Mackerras &lt;paulus@samba.org&gt;
Cc: Michael Ellerman &lt;mpe@ellerman.id.au&gt;
Cc: Gleb Natapov &lt;gleb@kernel.org&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Ingo Molnar &lt;mingo@redhat.com&gt;
Cc: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Cliff Whickman &lt;cpw@sgi.com&gt;
Cc: Robin Holt &lt;robinmholt@gmail.com&gt;
---
 I hope the new name will be OK. Although it doesn&#39;t fully convey the
 difference from alloc_pages_node(), renaming the latter would be a much
 larger patch (some 60 callsites) and hopefully the new comments are enough to
 prevent further confusion.
 
 I&#39;m CC&#39;ing also maintainers of the callsites so they can verify that the
 callsites that don&#39;t pass __GFP_THISNODE are really not intended to restrict
 allocation to the given node. I went through them myself and each looked like
 it&#39;s better off if it can successfully allocate on a fallback node rather
 than fail. DavidR checked them also I think, but it&#39;s better if maintainers
 can verify that. I&#39;m not completely sure about all the usages in sl*b due to
 multiple layers through which gfp flags are being passed.

 arch/ia64/hp/common/sba_iommu.c   |  2 +-
 arch/ia64/kernel/uncached.c       |  2 +-
 arch/ia64/sn/pci/pci_dma.c        |  2 +-
 arch/powerpc/platforms/cell/ras.c |  2 +-
 arch/x86/kvm/vmx.c                |  2 +-
 drivers/misc/sgi-xp/xpc_uv.c      |  2 +-
 include/linux/gfp.h               | 14 ++++++++++++--
 kernel/profile.c                  |  8 ++++----
 mm/filemap.c                      |  2 +-
 mm/huge_memory.c                  |  2 +-
 mm/hugetlb.c                      |  4 ++--
 mm/memory-failure.c               |  2 +-
 mm/mempolicy.c                    |  4 ++--
 mm/migrate.c                      |  4 ++--
 mm/page_alloc.c                   |  2 --
 mm/slab.c                         |  2 +-
 mm/slob.c                         |  4 ++--
 mm/slub.c                         |  2 +-
 18 files changed, 35 insertions(+), 27 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1731">Christoph Lameter</a> - July 21, 2015, 2:05 p.m.</div>
<pre class="content">
On Tue, 21 Jul 2015, Vlastimil Babka wrote:
<span class="quote">
&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id</span>
<span class="quote">&gt; to be -1. Unfortunately the name of the function can easily suggest that the</span>
<span class="quote">&gt; allocation is restricted to the given node. In truth, the node is only</span>
<span class="quote">&gt; preferred, unless __GFP_THISNODE is among the gfp flags.</span>

Yup. I complained about this when this was introduced. Glad to see this
fixed. Initially this was alloc_pages_node() which just means that a node
is specified. The exact behavior of the allocation is determined by flags
such as GFP_THISNODE. I&#39;d rather have that restored because otherwise we
get into weird code like the one below. And such an arrangement also
leaves the way open to add more flags in the future that may change the
allocation behavior.
<span class="quote">

&gt;  	area-&gt;nid = nid;</span>
<span class="quote">&gt;  	area-&gt;order = order;</span>
<span class="quote">&gt; -	area-&gt;pages = alloc_pages_exact_node(area-&gt;nid,</span>
<span class="quote">&gt; +	area-&gt;pages = alloc_pages_prefer_node(area-&gt;nid,</span>
<span class="quote">&gt;  						GFP_KERNEL|__GFP_THISNODE,</span>
<span class="quote">&gt;  						area-&gt;order);</span>

This is not preferring a node but requiring alloction on that node.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72830">Robin Holt</a> - July 21, 2015, 2:14 p.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 8:55 AM, Vlastimil Babka &lt;vbabka@suse.cz&gt; wrote:
<span class="quote">&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id</span>
<span class="quote">&gt; to be -1. Unfortunately the name of the function can easily suggest that the</span>
<span class="quote">&gt; allocation is restricted to the given node. In truth, the node is only</span>
<span class="quote">&gt; preferred, unless __GFP_THISNODE is among the gfp flags.</span>
<span class="quote">&gt;</span>
...
<span class="quote">&gt; Cc: Robin Holt &lt;robinmholt@gmail.com&gt;</span>
<span class="acked-by">
Acked-by: Robin Holt &lt;robinmholt@gmail.com&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - July 21, 2015, 9:31 p.m.</div>
<pre class="content">
On Tue, 21 Jul 2015, Vlastimil Babka wrote:
<span class="quote">
&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id</span>
<span class="quote">&gt; to be -1. Unfortunately the name of the function can easily suggest that the</span>
<span class="quote">&gt; allocation is restricted to the given node. In truth, the node is only</span>
<span class="quote">&gt; preferred, unless __GFP_THISNODE is among the gfp flags.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,</span>
<span class="quote">&gt; thp: really limit transparent hugepage allocation to local node&quot;) and</span>
<span class="quote">&gt; b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To prevent further mistakes, this patch renames the function to</span>
<span class="quote">&gt; alloc_pages_prefer_node() and documents it together with alloc_pages_node().</span>
<span class="quote">&gt; </span>

alloc_pages_exact_node(), as you said, connotates that the allocation will 
take place on that node or will fail.  So why not go beyond this patch and 
actually make alloc_pages_exact_node() set __GFP_THISNODE and then call 
into a new alloc_pages_prefer_node(), which would be the current 
alloc_pages_exact_node() implementation, and then fix up the callers?
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=81661">Michael Ellerman</a> - July 22, 2015, 1:23 a.m.</div>
<pre class="content">
On Tue, 2015-07-21 at 15:55 +0200, Vlastimil Babka wrote:
<span class="quote">&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id</span>
<span class="quote">&gt; to be -1. Unfortunately the name of the function can easily suggest that the</span>
<span class="quote">&gt; allocation is restricted to the given node. In truth, the node is only</span>
<span class="quote">&gt; preferred, unless __GFP_THISNODE is among the gfp flags.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,</span>
<span class="quote">&gt; thp: really limit transparent hugepage allocation to local node&quot;) and</span>
<span class="quote">&gt; b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To prevent further mistakes, this patch renames the function to</span>
<span class="quote">&gt; alloc_pages_prefer_node() and documents it together with alloc_pages_node().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote"> 
&gt;  I&#39;m CC&#39;ing also maintainers of the callsites so they can verify that the</span>
<span class="quote">&gt;  callsites that don&#39;t pass __GFP_THISNODE are really not intended to restrict</span>
<span class="quote">&gt;  allocation to the given node. I went through them myself and each looked like</span>
<span class="quote">&gt;  it&#39;s better off if it can successfully allocate on a fallback node rather</span>
<span class="quote">&gt;  than fail. DavidR checked them also I think, but it&#39;s better if maintainers</span>
<span class="quote">&gt;  can verify that. I&#39;m not completely sure about all the usages in sl*b due to</span>
<span class="quote">&gt;  multiple layers through which gfp flags are being passed.</span>
<span class="quote">

&gt; diff --git a/arch/powerpc/platforms/cell/ras.c b/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; index e865d74..646a310 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; @@ -123,7 +123,7 @@ static int __init cbe_ptcal_enable_on_node(int nid, int order)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	area-&gt;nid = nid;</span>
<span class="quote">&gt;  	area-&gt;order = order;</span>
<span class="quote">&gt; -	area-&gt;pages = alloc_pages_exact_node(area-&gt;nid,</span>
<span class="quote">&gt; +	area-&gt;pages = alloc_pages_prefer_node(area-&gt;nid,</span>
<span class="quote">&gt;  						GFP_KERNEL|__GFP_THISNODE,</span>
<span class="quote">&gt;  						area-&gt;order);</span>

Yeah that looks right to me.

This code enables some firmware memory calibration so I think it really does
want to get memory on the specified node, or else fail.
<span class="acked-by">
Acked-by: Michael Ellerman &lt;mpe@ellerman.id.au&gt;</span>

cheers


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - July 22, 2015, 11:31 a.m.</div>
<pre class="content">
On 21/07/2015 15:55, Vlastimil Babka wrote:
<span class="quote">&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; index 2d73807..a8723a8 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; @@ -3158,7 +3158,7 @@ static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
<span class="quote">&gt;  	struct page *pages;</span>
<span class="quote">&gt;  	struct vmcs *vmcs;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt; +	pages = alloc_pages_prefer_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt;  	if (!pages)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	vmcs = page_address(pages);</span>

Even though there&#39;s a pretty strong preference for the &quot;right&quot; node,
things can work if the node is the wrong one.  The order is always zero
in practice, so the allocation should succeed.

Paolo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 22, 2015, 11:32 a.m.</div>
<pre class="content">
On 07/21/2015 11:31 PM, David Rientjes wrote:
<span class="quote">&gt; On Tue, 21 Jul 2015, Vlastimil Babka wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt;&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt;&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t allow the node id</span>
<span class="quote">&gt;&gt; to be -1. Unfortunately the name of the function can easily suggest that the</span>
<span class="quote">&gt;&gt; allocation is restricted to the given node. In truth, the node is only</span>
<span class="quote">&gt;&gt; preferred, unless __GFP_THISNODE is among the gfp flags.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,</span>
<span class="quote">&gt;&gt; thp: really limit transparent hugepage allocation to local node&quot;) and</span>
<span class="quote">&gt;&gt; b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To prevent further mistakes, this patch renames the function to</span>
<span class="quote">&gt;&gt; alloc_pages_prefer_node() and documents it together with alloc_pages_node().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_pages_exact_node(), as you said, connotates that the allocation will</span>
<span class="quote">&gt; take place on that node or will fail.  So why not go beyond this patch and</span>
<span class="quote">&gt; actually make alloc_pages_exact_node() set __GFP_THISNODE and then call</span>
<span class="quote">&gt; into a new alloc_pages_prefer_node(), which would be the current</span>
<span class="quote">&gt; alloc_pages_exact_node() implementation, and then fix up the callers?</span>

OK, but then we have alloc_pages_node(), alloc_pages_prefer_node() and
alloc_pages_exact_node(). Isn&#39;t that a bit too much? The first two
differ only in tiny bit:

static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
                                                unsigned int order)
{
        /* Unknown node is current node */
        if (nid &lt; 0)
                nid = numa_node_id();

        return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));
}

static inline struct page *alloc_pages_prefer_node(int nid, gfp_t gfp_mask,
                                                unsigned int order)
{
        VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));

        return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));
}

So _prefer_node is just a tiny optimization over the other one. It
should be maybe called __alloc_pages_node() then? This would perhaps
discourage users outside of mm/arch code (where it may matter). The
savings of a skipped branch is likely dubious anyway... It would be also
nice if alloc_pages_node() could use __alloc_pages_node() internally, but
I&#39;m not sure if all callers are safe wrt the
VM_BUG_ON(!node_online(nid)) part.

So when the alloc_pages_prefer_node is diminished as __alloc_pages_node
or outright removed, then maybe alloc_pages_exact_node() which adds
__GFP_THISNODE on its own, might be a useful wrapper. But I agree with
Christoph it&#39;s a duplication of the gfp_flags functionality and I don&#39;t
think there would be many users left anyway.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - July 22, 2015, 9:44 p.m.</div>
<pre class="content">
On Wed, 22 Jul 2015, Paolo Bonzini wrote:
<span class="quote">
&gt; &gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; &gt; index 2d73807..a8723a8 100644</span>
<span class="quote">&gt; &gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; &gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; &gt; @@ -3158,7 +3158,7 @@ static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
<span class="quote">&gt; &gt;  	struct page *pages;</span>
<span class="quote">&gt; &gt;  	struct vmcs *vmcs;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt; &gt; +	pages = alloc_pages_prefer_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt; &gt;  	if (!pages)</span>
<span class="quote">&gt; &gt;  		return NULL;</span>
<span class="quote">&gt; &gt;  	vmcs = page_address(pages);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Even though there&#39;s a pretty strong preference for the &quot;right&quot; node,</span>
<span class="quote">&gt; things can work if the node is the wrong one.  The order is always zero</span>
<span class="quote">&gt; in practice, so the allocation should succeed.</span>
<span class="quote">&gt; </span>

You&#39;re code is fine both before and after the patch since __GFP_THISNODE 
isn&#39;t set.  The allocation will eventually succeed but, as you said, may 
be from remote memory (and the success of allocating on node may be 
influenced by the global setting of zone_reclaim_mode).
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - July 22, 2015, 9:52 p.m.</div>
<pre class="content">
On Wed, 22 Jul 2015, Vlastimil Babka wrote:
<span class="quote">
&gt; &gt; alloc_pages_exact_node(), as you said, connotates that the allocation will</span>
<span class="quote">&gt; &gt; take place on that node or will fail.  So why not go beyond this patch and</span>
<span class="quote">&gt; &gt; actually make alloc_pages_exact_node() set __GFP_THISNODE and then call</span>
<span class="quote">&gt; &gt; into a new alloc_pages_prefer_node(), which would be the current</span>
<span class="quote">&gt; &gt; alloc_pages_exact_node() implementation, and then fix up the callers?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, but then we have alloc_pages_node(), alloc_pages_prefer_node() and</span>
<span class="quote">&gt; alloc_pages_exact_node(). Isn&#39;t that a bit too much? The first two</span>
<span class="quote">&gt; differ only in tiny bit:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
<span class="quote">&gt;                                                 unsigned int order)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         /* Unknown node is current node */</span>
<span class="quote">&gt;         if (nid &lt; 0)</span>
<span class="quote">&gt;                 nid = numa_node_id();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static inline struct page *alloc_pages_prefer_node(int nid, gfp_t gfp_mask,</span>
<span class="quote">&gt;                                                 unsigned int order)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>

Eek, yeah, that does look bad.  I&#39;m not even sure the

	if (nid &lt; 0)
		nid = numa_node_id();

is correct; I think this should be comparing to NUMA_NO_NODE rather than
all negative numbers, otherwise we silently ignore overflow and nobody 
ever knows.
<span class="quote">
&gt; So _prefer_node is just a tiny optimization over the other one. It</span>
<span class="quote">&gt; should be maybe called __alloc_pages_node() then? This would perhaps</span>
<span class="quote">&gt; discourage users outside of mm/arch code (where it may matter). The</span>
<span class="quote">&gt; savings of a skipped branch is likely dubious anyway... It would be also</span>
<span class="quote">&gt; nice if alloc_pages_node() could use __alloc_pages_node() internally, but</span>
<span class="quote">&gt; I&#39;m not sure if all callers are safe wrt the</span>
<span class="quote">&gt; VM_BUG_ON(!node_online(nid)) part.</span>
<span class="quote">&gt; </span>

I&#39;m not sure how large you want to make your patch :)  In a perfect world 
I would think that we wouldn&#39;t have an alloc_pages_prefer_node() at all 
and everything would be converted to alloc_pages_node() which would do

	if (nid == NUMA_NO_NODE)
		nid = numa_mem_id();

	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));
	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));

and then alloc_pages_exact_node() would do

	return alloc_pages_node(nid, gfp_mask | __GFP_THISNODE, order);

and existing alloc_pages_exact_node() callers fixed up depending on 
whether they set the bit or not.

The only possible downside would be existing users of 
alloc_pages_node() that are calling it with an offline node.  Since it&#39;s a 
VM_BUG_ON() that would catch that, I think it should be changed to a 
VM_WARN_ON() and eventually fixed up because it&#39;s nonsensical.  
VM_BUG_ON() here should be avoided.

Or just go with a single alloc_pages_node() and rename __GFP_THISNODE to 
__GFP_EXACT_NODE which may accomplish the same thing :)
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1731">Christoph Lameter</a> - July 23, 2015, 2:11 p.m.</div>
<pre class="content">
On Wed, 22 Jul 2015, David Rientjes wrote:
<span class="quote">
&gt; Eek, yeah, that does look bad.  I&#39;m not even sure the</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	if (nid &lt; 0)</span>
<span class="quote">&gt; 		nid = numa_node_id();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; is correct; I think this should be comparing to NUMA_NO_NODE rather than</span>
<span class="quote">&gt; all negative numbers, otherwise we silently ignore overflow and nobody</span>
<span class="quote">&gt; ever knows.</span>

Comparing to NUMA_NO_NODE would be better. Also use numa_mem_id() instead
to support memoryless nodes better?
<span class="quote">
&gt; The only possible downside would be existing users of</span>
<span class="quote">&gt; alloc_pages_node() that are calling it with an offline node.  Since it&#39;s a</span>
<span class="quote">&gt; VM_BUG_ON() that would catch that, I think it should be changed to a</span>
<span class="quote">&gt; VM_WARN_ON() and eventually fixed up because it&#39;s nonsensical.</span>
<span class="quote">&gt; VM_BUG_ON() here should be avoided.</span>

The offline node thing could be addresses by using numa_mem_id()?

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - July 23, 2015, 8:27 p.m.</div>
<pre class="content">
On Thu, 23 Jul 2015, Christoph Lameter wrote:
<span class="quote">
&gt; &gt; The only possible downside would be existing users of</span>
<span class="quote">&gt; &gt; alloc_pages_node() that are calling it with an offline node.  Since it&#39;s a</span>
<span class="quote">&gt; &gt; VM_BUG_ON() that would catch that, I think it should be changed to a</span>
<span class="quote">&gt; &gt; VM_WARN_ON() and eventually fixed up because it&#39;s nonsensical.</span>
<span class="quote">&gt; &gt; VM_BUG_ON() here should be avoided.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The offline node thing could be addresses by using numa_mem_id()?</span>
<span class="quote">&gt; </span>

I was concerned about any callers that were passing an offline node, not 
NUMA_NO_NODE, today.  One of the alloc-node functions has a VM_BUG_ON() 
for it, the other silently calls node_zonelist() on it.

I suppose the final alloc_pages_node() implementation could be

        if (nid == NUMA_NO_NODE || VM_WARN_ON(!node_online(nid)))
                nid = numa_mem_id();

        VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES);
        return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));

though.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 24, 2015, 2:52 p.m.</div>
<pre class="content">
On 07/23/2015 10:27 PM, David Rientjes wrote:
<span class="quote">&gt; On Thu, 23 Jul 2015, Christoph Lameter wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt; The only possible downside would be existing users of</span>
<span class="quote">&gt;&gt;&gt; alloc_pages_node() that are calling it with an offline node.  Since it&#39;s a</span>
<span class="quote">&gt;&gt;&gt; VM_BUG_ON() that would catch that, I think it should be changed to a</span>
<span class="quote">&gt;&gt;&gt; VM_WARN_ON() and eventually fixed up because it&#39;s nonsensical.</span>
<span class="quote">&gt;&gt;&gt; VM_BUG_ON() here should be avoided.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The offline node thing could be addresses by using numa_mem_id()?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I was concerned about any callers that were passing an offline node, not</span>
<span class="quote">&gt; NUMA_NO_NODE, today.  One of the alloc-node functions has a VM_BUG_ON()</span>
<span class="quote">&gt; for it, the other silently calls node_zonelist() on it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I suppose the final alloc_pages_node() implementation could be</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;          if (nid == NUMA_NO_NODE || VM_WARN_ON(!node_online(nid)))</span>
<span class="quote">&gt;                  nid = numa_mem_id();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;          VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES);</span>
<span class="quote">&gt;          return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; though.</span>

I&#39;ve posted v2 based on David&#39;s and Christoph&#39;s suggestions (thanks) but 
to avoid spamming everyone until we agree on the final interface, it&#39;s 
marked as RFC and excludes the arch people from CC:

http://marc.info/?l=linux-kernel&amp;m=143774920902608&amp;w=2
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/ia64/hp/common/sba_iommu.c b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_header">index 344387a..17762af 100644</span>
<span class="p_header">--- a/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_header">+++ b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_chunk">@@ -1146,7 +1146,7 @@</span> <span class="p_context"> sba_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
 		if (node == NUMA_NO_NODE)
 			node = numa_node_id();
 
<span class="p_del">-		page = alloc_pages_exact_node(node, flags, get_order(size));</span>
<span class="p_add">+		page = alloc_pages_prefer_node(node, flags, get_order(size));</span>
 		if (unlikely(!page))
 			return NULL;
 
<span class="p_header">diff --git a/arch/ia64/kernel/uncached.c b/arch/ia64/kernel/uncached.c</span>
<span class="p_header">index 20e8a9b..1451961 100644</span>
<span class="p_header">--- a/arch/ia64/kernel/uncached.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/uncached.c</span>
<span class="p_chunk">@@ -97,7 +97,7 @@</span> <span class="p_context"> static int uncached_add_chunk(struct uncached_pool *uc_pool, int nid)</span>
 
 	/* attempt to allocate a granule&#39;s worth of cached memory pages */
 
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = alloc_pages_prefer_node(nid,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				IA64_GRANULE_SHIFT-PAGE_SHIFT);
 	if (!page) {
<span class="p_header">diff --git a/arch/ia64/sn/pci/pci_dma.c b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_header">index d0853e8..dcab82f 100644</span>
<span class="p_header">--- a/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_header">+++ b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static void *sn_dma_alloc_coherent(struct device *dev, size_t size,</span>
 	 */
 	node = pcibus_to_node(pdev-&gt;bus);
 	if (likely(node &gt;=0)) {
<span class="p_del">-		struct page *p = alloc_pages_exact_node(node,</span>
<span class="p_add">+		struct page *p = alloc_pages_prefer_node(node,</span>
 						flags, get_order(size));
 
 		if (likely(p))
<span class="p_header">diff --git a/arch/powerpc/platforms/cell/ras.c b/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_header">index e865d74..646a310 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_chunk">@@ -123,7 +123,7 @@</span> <span class="p_context"> static int __init cbe_ptcal_enable_on_node(int nid, int order)</span>
 
 	area-&gt;nid = nid;
 	area-&gt;order = order;
<span class="p_del">-	area-&gt;pages = alloc_pages_exact_node(area-&gt;nid,</span>
<span class="p_add">+	area-&gt;pages = alloc_pages_prefer_node(area-&gt;nid,</span>
 						GFP_KERNEL|__GFP_THISNODE,
 						area-&gt;order);
 
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 2d73807..a8723a8 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -3158,7 +3158,7 @@</span> <span class="p_context"> static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
 	struct page *pages;
 	struct vmcs *vmcs;
 
<span class="p_del">-	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="p_add">+	pages = alloc_pages_prefer_node(node, GFP_KERNEL, vmcs_config.order);</span>
 	if (!pages)
 		return NULL;
 	vmcs = page_address(pages);
<span class="p_header">diff --git a/drivers/misc/sgi-xp/xpc_uv.c b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_header">index 95c8944..3ff0a24 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_chunk">@@ -239,7 +239,7 @@</span> <span class="p_context"> xpc_create_gru_mq_uv(unsigned int mq_size, int cpu, char *irq_name,</span>
 	mq-&gt;mmr_blade = uv_cpu_to_blade_id(cpu);
 
 	nid = cpu_to_node(cpu);
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = alloc_pages_prefer_node(nid,</span>
 				      GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				      pg_order);
 	if (page == NULL) {
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index 15928f0..ff892d7 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -300,6 +300,10 @@</span> <span class="p_context"> __alloc_pages(gfp_t gfp_mask, unsigned int order,</span>
 	return __alloc_pages_nodemask(gfp_mask, order, zonelist, NULL);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate pages, preferring the node given as nid. When nid equals -1,</span>
<span class="p_add">+ * prefer the current CPU&#39;s node.</span>
<span class="p_add">+ */</span>
 static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
 						unsigned int order)
 {
<span class="p_chunk">@@ -310,7 +314,14 @@</span> <span class="p_context"> static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
 	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));
 }
 
<span class="p_del">-static inline struct page *alloc_pages_exact_node(int nid, gfp_t gfp_mask,</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate pages, preferring the node given as nid. Unlike alloc_pages_node(),</span>
<span class="p_add">+ * nid must be a valid online node, there is no fallback to current CPU&#39;s node.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * In order to completely restrict allocation to the preferred node, include</span>
<span class="p_add">+ * __GFP_THISNODE in the gfp_mask.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct page *alloc_pages_prefer_node(int nid, gfp_t gfp_mask,</span>
 						unsigned int order)
 {
 	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));
<span class="p_chunk">@@ -354,7 +365,6 @@</span> <span class="p_context"> extern unsigned long get_zeroed_page(gfp_t gfp_mask);</span>
 
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask);
 void free_pages_exact(void *virt, size_t size);
<span class="p_del">-/* This is different from alloc_pages_exact_node !!! */</span>
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask);
 
 #define __get_free_page(gfp_mask) \
<span class="p_header">diff --git a/kernel/profile.c b/kernel/profile.c</span>
<span class="p_header">index a7bcd28..6ee695e 100644</span>
<span class="p_header">--- a/kernel/profile.c</span>
<span class="p_header">+++ b/kernel/profile.c</span>
<span class="p_chunk">@@ -339,7 +339,7 @@</span> <span class="p_context"> static int profile_cpu_callback(struct notifier_block *info,</span>
 		node = cpu_to_mem(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
<span class="p_del">-			page = alloc_pages_exact_node(node,</span>
<span class="p_add">+			page = alloc_pages_prefer_node(node,</span>
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
<span class="p_chunk">@@ -347,7 +347,7 @@</span> <span class="p_context"> static int profile_cpu_callback(struct notifier_block *info,</span>
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
<span class="p_del">-			page = alloc_pages_exact_node(node,</span>
<span class="p_add">+			page = alloc_pages_prefer_node(node,</span>
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
<span class="p_chunk">@@ -543,14 +543,14 @@</span> <span class="p_context"> static int create_hash_tables(void)</span>
 		int node = cpu_to_mem(cpu);
 		struct page *page;
 
<span class="p_del">-		page = alloc_pages_exact_node(node,</span>
<span class="p_add">+		page = alloc_pages_prefer_node(node,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
<span class="p_del">-		page = alloc_pages_exact_node(node,</span>
<span class="p_add">+		page = alloc_pages_prefer_node(node,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 6bf5e42..52e3b55 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -648,7 +648,7 @@</span> <span class="p_context"> struct page *__page_cache_alloc(gfp_t gfp)</span>
 		do {
 			cpuset_mems_cookie = read_mems_allowed_begin();
 			n = cpuset_mem_spread_node();
<span class="p_del">-			page = alloc_pages_exact_node(n, gfp, 0);</span>
<span class="p_add">+			page = alloc_pages_prefer_node(n, gfp, 0);</span>
 		} while (!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie));
 
 		return page;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 078832c..7130e97 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2350,7 +2350,7 @@</span> <span class="p_context"> khugepaged_alloc_page(struct page **hpage, gfp_t gfp, struct mm_struct *mm,</span>
 	 */
 	up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_del">-	*hpage = alloc_pages_exact_node(node, gfp, HPAGE_PMD_ORDER);</span>
<span class="p_add">+	*hpage = alloc_pages_prefer_node(node, gfp, HPAGE_PMD_ORDER);</span>
 	if (unlikely(!*hpage)) {
 		count_vm_event(THP_COLLAPSE_ALLOC_FAILED);
 		*hpage = ERR_PTR(-ENOMEM);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 271e443..4c6c0c1 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1088,7 +1088,7 @@</span> <span class="p_context"> static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
 {
 	struct page *page;
 
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = alloc_pages_prefer_node(nid,</span>
 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
 						__GFP_REPEAT|__GFP_NOWARN,
 		huge_page_order(h));
<span class="p_chunk">@@ -1250,7 +1250,7 @@</span> <span class="p_context"> static struct page *alloc_buddy_huge_page(struct hstate *h, int nid)</span>
 				   __GFP_REPEAT|__GFP_NOWARN,
 				   huge_page_order(h));
 	else
<span class="p_del">-		page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+		page = alloc_pages_prefer_node(nid,</span>
 			htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
 			__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));
 
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index 501820c..132f043 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -1503,7 +1503,7 @@</span> <span class="p_context"> static struct page *new_page(struct page *p, unsigned long private, int **x)</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 						   nid);
 	else
<span class="p_del">-		return alloc_pages_exact_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="p_add">+		return alloc_pages_prefer_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 7477432..2e2a876 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -945,7 +945,7 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long node, int **x</span>
 		return alloc_huge_page_node(page_hstate(compound_head(page)),
 					node);
 	else
<span class="p_del">-		return alloc_pages_exact_node(node, GFP_HIGHUSER_MOVABLE |</span>
<span class="p_add">+		return alloc_pages_prefer_node(node, GFP_HIGHUSER_MOVABLE |</span>
 						    __GFP_THISNODE, 0);
 }
 
<span class="p_chunk">@@ -1986,7 +1986,7 @@</span> <span class="p_context"> alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,</span>
 		nmask = policy_nodemask(gfp, pol);
 		if (!nmask || node_isset(node, *nmask)) {
 			mpol_cond_put(pol);
<span class="p_del">-			page = alloc_pages_exact_node(node,</span>
<span class="p_add">+			page = alloc_pages_prefer_node(node,</span>
 						gfp | __GFP_THISNODE, order);
 			goto out;
 		}
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index f53838f..b51e106 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1187,7 +1187,7 @@</span> <span class="p_context"> static struct page *new_page_node(struct page *p, unsigned long private,</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 					pm-&gt;node);
 	else
<span class="p_del">-		return alloc_pages_exact_node(pm-&gt;node,</span>
<span class="p_add">+		return alloc_pages_prefer_node(pm-&gt;node,</span>
 				GFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);
 }
 
<span class="p_chunk">@@ -1553,7 +1553,7 @@</span> <span class="p_context"> static struct page *alloc_misplaced_dst_page(struct page *page,</span>
 	int nid = (int) data;
 	struct page *newpage;
 
<span class="p_del">-	newpage = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	newpage = alloc_pages_prefer_node(nid,</span>
 					 (GFP_HIGHUSER_MOVABLE |
 					  __GFP_THISNODE | __GFP_NOMEMALLOC |
 					  __GFP_NORETRY | __GFP_NOWARN) &amp;
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index ebffa0e..e6eef56 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3062,8 +3062,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(alloc_pages_exact);</span>
  *
  * Like alloc_pages_exact(), but try to allocate on node nid first before falling
  * back.
<span class="p_del">- * Note this is not alloc_pages_exact_node() which allocates on a specific node,</span>
<span class="p_del">- * but is not exact.</span>
  */
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="p_header">index 7eb38dd..471fde8 100644</span>
<span class="p_header">--- a/mm/slab.c</span>
<span class="p_header">+++ b/mm/slab.c</span>
<span class="p_chunk">@@ -1594,7 +1594,7 @@</span> <span class="p_context"> static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,</span>
 	if (memcg_charge_slab(cachep, flags, cachep-&gt;gfporder))
 		return NULL;
 
<span class="p_del">-	page = alloc_pages_exact_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
<span class="p_add">+	page = alloc_pages_prefer_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
 	if (!page) {
 		memcg_uncharge_slab(cachep, cachep-&gt;gfporder);
 		slab_out_of_memory(cachep, flags, nodeid);
<span class="p_header">diff --git a/mm/slob.c b/mm/slob.c</span>
<span class="p_header">index 4765f65..cfda5e2 100644</span>
<span class="p_header">--- a/mm/slob.c</span>
<span class="p_header">+++ b/mm/slob.c</span>
<span class="p_chunk">@@ -45,7 +45,7 @@</span> <span class="p_context"></span>
  * NUMA support in SLOB is fairly simplistic, pushing most of the real
  * logic down to the page allocator, and simply doing the node accounting
  * on the upper levels. In the event that a node id is explicitly
<span class="p_del">- * provided, alloc_pages_exact_node() with the specified node id is used</span>
<span class="p_add">+ * provided, alloc_pages_prefer_node() with the specified node id is used</span>
  * instead. The common case (or when the node id isn&#39;t explicitly provided)
  * will default to the current node, as per numa_node_id().
  *
<span class="p_chunk">@@ -193,7 +193,7 @@</span> <span class="p_context"> static void *slob_new_pages(gfp_t gfp, int order, int node)</span>
 
 #ifdef CONFIG_NUMA
 	if (node != NUMA_NO_NODE)
<span class="p_del">-		page = alloc_pages_exact_node(node, gfp, order);</span>
<span class="p_add">+		page = alloc_pages_prefer_node(node, gfp, order);</span>
 	else
 #endif
 		page = alloc_pages(gfp, order);
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index 54c0876..8b10404 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -1323,7 +1323,7 @@</span> <span class="p_context"> static inline struct page *alloc_slab_page(struct kmem_cache *s,</span>
 	if (node == NUMA_NO_NODE)
 		page = alloc_pages(flags, order);
 	else
<span class="p_del">-		page = alloc_pages_exact_node(node, flags, order);</span>
<span class="p_add">+		page = alloc_pages_prefer_node(node, flags, order);</span>
 
 	if (!page)
 		memcg_uncharge_slab(s, order);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



