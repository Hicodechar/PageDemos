
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[10/17] RISC-V: Atomic and Locking Code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [10/17] RISC-V: Atomic and Locking Code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 12, 2017, 1:31 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170712013130.14792-11-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9835715/mbox/"
   >mbox</a>
|
   <a href="/patch/9835715/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9835715/">/patch/9835715/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	329E860363 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 12 Jul 2017 01:32:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1C7DC28567
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 12 Jul 2017 01:32:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0F2312857F; Wed, 12 Jul 2017 01:32:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B939928567
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 12 Jul 2017 01:32:37 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756467AbdGLBcd (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 11 Jul 2017 21:32:33 -0400
Received: from mail-pg0-f67.google.com ([74.125.83.67]:32777 &quot;EHLO
	mail-pg0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756388AbdGLBc1 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 11 Jul 2017 21:32:27 -0400
Received: by mail-pg0-f67.google.com with SMTP id u62so1039824pgb.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 11 Jul 2017 18:32:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=subject:date:message-id:in-reply-to:references:cc:from:to;
	bh=dshpl/OGbEswjlm4rVJsyevzhUFBBvdeIwFu7zUOMwg=;
	b=UQL9rhocYVSLbJnUk8ywpVKJQ9vBzs8MDWjWIO5MOR3YY+33ZypJRzprW4uWrdpgi/
	YpXLcYrvx83q/ylCeJya9o48w3Bd9ZAjnyaQ1fjNEwJyfoS7J5jeAznNp9czTHO0hbCb
	KwzUFBcO+1yzKr4ediHbUTC/FBcrwUPupTtggLMJJbF0zLe6TPM+I7i91sdApvg1u0n6
	CotnLj7SwlmTKqTz/uKTnPG3tRh7eRIu/qq2KRVOVYN99rIWqqyxFfn/pXig2WiXc8PW
	qG+d9JAEFXrZ8nAqRBKjDI7QCu7bkhYp+avmhI/mdPU/S2SA6a5NLE8pYZ9tp8FTd/ns
	z2dg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:subject:date:message-id:in-reply-to:references
	:cc:from:to;
	bh=dshpl/OGbEswjlm4rVJsyevzhUFBBvdeIwFu7zUOMwg=;
	b=K19kECn5zcDmtE5BwfW75Mg00eCWoA8bjCv9hK6Dv/uz2KAV/Osw7coFrFVMpESTxc
	CdbLkw8bYnDSdzRgKcHLgIuZGZhB/CV/tbmAIyEuX9ldMPAO7MP9DgYYpG9Prpgssm0C
	yd+MC6UEceC6Uw5lkxjT4PbH2L/ewnWmoRQxwViX8IqH4vaPpBoCFYkQ3+Q9zWZ9yh3f
	/YDANipUWr3s4MYQEDrqrnMNiImtQ+/PTo21KKfnqjgX5O5YfOV68O8C7GY3nrav2EOy
	GgeA8xPYAHI6qyE338L8fl6pGlX+4bpU8zIk5335a8uUb29mIsyd1AXxt2ZEqVnOow2B
	Gntw==
X-Gm-Message-State: AIVw111R8f4Mkvk4VYVgcFs6TzBffAS9EaU341m/r94Cfp4gIh/QbXQZ
	BayevCop+P9JXK1M
X-Received: by 10.98.87.195 with SMTP id i64mr47207047pfj.175.1499823145449; 
	Tue, 11 Jul 2017 18:32:25 -0700 (PDT)
Received: from localhost ([216.38.154.21]) by smtp.gmail.com with ESMTPSA id
	68sm1030994pfi.69.2017.07.11.18.32.24
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 11 Jul 2017 18:32:24 -0700 (PDT)
Subject: [PATCH 10/17] RISC-V: Atomic and Locking Code
Date: Tue, 11 Jul 2017 18:31:23 -0700
Message-Id: &lt;20170712013130.14792-11-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170712013130.14792-1-palmer@dabbelt.com&gt;
References: &lt;20170712013130.14792-1-palmer@dabbelt.com&gt;
Cc: albert@sifive.com, yamada.masahiro@socionext.com, mmarek@suse.com,
	will.deacon@arm.com, peterz@infradead.org, boqun.feng@gmail.com,
	mingo@redhat.com, daniel.lezcano@linaro.org, tglx@linutronix.de,
	jason@lakedaemon.net, marc.zyngier@arm.com,
	gregkh@linuxfoundation.org, jslaby@suse.com, davem@davemloft.net,
	mchehab@kernel.org, sfr@canb.auug.org.au, fweisbec@gmail.com,
	viro@zeniv.linux.org.uk, mcgrof@kernel.org, dledford@redhat.com,
	bart.vanassche@sandisk.com, sstabellini@kernel.org,
	daniel.vetter@ffwll.ch, mpe@ellerman.id.au, msalter@redhat.com,
	nicolas.dichtel@6wind.com, james.hogan@imgtec.com,
	paul.gortmaker@windriver.com, linux@roeck-us.net,
	heiko.carstens@de.ibm.com, schwidefsky@de.ibm.com,
	linux-kernel@vger.kernel.org, patches@groups.riscv.org,
	Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: Olof Johansson &lt;olof@lixom.net&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	akpm@linux-foundation.org
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 12, 2017, 1:31 a.m.</div>
<pre class="content">
This contains all the code that directly interfaces with the RISC-V
memory model.  While this code corforms to the current RISC-V ISA
specifications (user 2.2 and priv 1.10), the memory model is somewhat
underspecified in those documents.  There is a working group that hopes
to produce a formal memory model by the end of the year, but my
understanding is that the basic definitions we&#39;re relying on here won&#39;t
change significantly.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/include/asm/atomic.h         | 328 ++++++++++++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h        |  68 +++++++
 arch/riscv/include/asm/bitops.h         | 216 +++++++++++++++++++++
 arch/riscv/include/asm/cacheflush.h     |  39 ++++
 arch/riscv/include/asm/cmpxchg.h        | 134 +++++++++++++
 arch/riscv/include/asm/io.h             | 180 ++++++++++++++++++
 arch/riscv/include/asm/spinlock.h       | 165 ++++++++++++++++
 arch/riscv/include/asm/spinlock_types.h |  33 ++++
 arch/riscv/include/asm/tlb.h            |  24 +++
 arch/riscv/include/asm/tlbflush.h       |  64 +++++++
 10 files changed, 1251 insertions(+)
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - July 12, 2017, 12:40 p.m.</div>
<pre class="content">
On Tue, Jul 11, 2017 at 06:31:23PM -0700, Palmer Dabbelt wrote:
[...]
<span class="quote">&gt; diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..b0a0c76e966a</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/bitops.h</span>
<span class="quote">&gt; @@ -0,0 +1,216 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2012 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_BITOPS_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _LINUX_BITOPS_H</span>
<span class="quote">&gt; +#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="quote">&gt; +#endif /* _LINUX_BITOPS_H */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/compiler.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/irqflags.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/barrier.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/bitsperlong.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef smp_mb__before_clear_bit</span>
<span class="quote">&gt; +#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="quote">&gt; +#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="quote">&gt; +#endif /* smp_mb__before_clear_bit */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if (BITS_PER_LONG == 64)</span>
<span class="quote">&gt; +#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="quote">&gt; +#elif (BITS_PER_LONG == 32)</span>
<span class="quote">&gt; +#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>

So the test_and_{set,change,clear}_bit() have the similar semantics as
cmpxchg(), so
<span class="quote">
&gt; +#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="quote">&gt; +({								\</span>
<span class="quote">&gt; +	unsigned long __res, __mask;				\</span>
<span class="quote">&gt; +	__mask = BIT_MASK(nr);					\</span>
<span class="quote">&gt; +	__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		__AMO(op) &quot; %0, %2, %1&quot;				\</span>

... &quot;aqrl&quot; bit is needed here, and
<span class="quote">
&gt; +		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="quote">&gt; +		: &quot;r&quot; (mod(__mask)));				\</span>

... &quot;memory&quot; clobber is needed here.
<span class="quote">
&gt; +	((__res &amp; __mask) != 0);				\</span>
<span class="quote">&gt; +})</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __op_bit(op, mod, nr, addr)				\</span>
<span class="quote">&gt; +	__asm__ __volatile__ (					\</span>
<span class="quote">&gt; +		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="quote">&gt; +		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="quote">&gt; +		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Bitmask modifiers */</span>
<span class="quote">&gt; +#define __NOP(x)	(x)</span>
<span class="quote">&gt; +#define __NOT(x)	(~(x))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * test_and_set_bit - Set a bit and return its old value</span>
<span class="quote">&gt; + * @nr: Bit to set</span>
<span class="quote">&gt; + * @addr: Address to count from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt; + * It may be reordered on other architectures than x86.</span>
<span class="quote">&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="quote">&gt; + * @nr: Bit to clear</span>
<span class="quote">&gt; + * @addr: Address to count from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt; + * It can be reordered on other architectures other than x86.</span>
<span class="quote">&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * test_and_change_bit - Change a bit and return its old value</span>
<span class="quote">&gt; + * @nr: Bit to change</span>
<span class="quote">&gt; + * @addr: Address to count from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * set_bit - Atomically set a bit in memory</span>
<span class="quote">&gt; + * @nr: the bit to set</span>
<span class="quote">&gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This function is atomic and may not be reordered.  See __set_bit()</span>

This is incorrect, {set,change,clear}_bit() can be reordered, see
Documentation/memory-barriers.txt, they are just relaxed atomics. But I
think you just copy this from x86 code, so maybe x86 code needs help
too, at least claim that&#39;s only x86-specific guarantee.
<span class="quote">
&gt; + * if you do not require the atomic guarantees.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note: there are no guarantees that this function will not be reordered</span>
<span class="quote">&gt; + * on non x86 architectures, so if you are writing portable code,</span>
<span class="quote">&gt; + * make sure not to rely on its reordering guarantees.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="quote">&gt; + * restricted to acting on a single-word quantity.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__op_bit(or, __NOP, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * clear_bit - Clears a bit in memory</span>
<span class="quote">&gt; + * @nr: Bit to clear</span>
<span class="quote">&gt; + * @addr: Address to start counting from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="quote">&gt; + * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="quote">&gt; + * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="quote">&gt; + * in order to ensure changes are visible on other processors.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__op_bit(and, __NOT, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * change_bit - Toggle a bit in memory</span>
<span class="quote">&gt; + * @nr: Bit to change</span>
<span class="quote">&gt; + * @addr: Address to start counting from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * change_bit() is atomic and may not be reordered. It may be</span>
<span class="quote">&gt; + * reordered on other architectures than x86.</span>
<span class="quote">&gt; + * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="quote">&gt; + * restricted to acting on a single-word quantity.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	__op_bit(xor, __NOP, nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="quote">&gt; + * @nr: Bit to set</span>
<span class="quote">&gt; + * @addr: Address to count from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is atomic and provides acquire barrier semantics.</span>
<span class="quote">&gt; + * It can be used to implement bit locks.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int test_and_set_bit_lock(</span>
<span class="quote">&gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return test_and_set_bit(nr, addr);</span>

If you want, you can open code an &quot;amoor.aq&quot; here, because
test_and_set_bit_lock() only needs an acquire barrier.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="quote">&gt; + * @nr: the bit to set</span>
<span class="quote">&gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is atomic and provides release barrier semantics.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void clear_bit_unlock(</span>
<span class="quote">&gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>

You need a smp_mb__before_atomic() here, because clear_bit() is only
relaxed atomic. And clear_bit_unlock() is a release.

Regards,
Boqun
<span class="quote">
&gt; +	clear_bit(nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="quote">&gt; + * @nr: the bit to set</span>
<span class="quote">&gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="quote">&gt; + * It does provide release barrier semantics so it can be used to unlock</span>
<span class="quote">&gt; + * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="quote">&gt; + * any bits in the memory until the lock is released (a good example is</span>
<span class="quote">&gt; + * if the bit lock itself protects access to the other bits in the word).</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void __clear_bit_unlock(</span>
<span class="quote">&gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	clear_bit(nr, addr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#undef __test_and_op_bit</span>
<span class="quote">&gt; +#undef __op_bit</span>
<span class="quote">&gt; +#undef __NOP</span>
<span class="quote">&gt; +#undef __NOT</span>
<span class="quote">&gt; +#undef __AMO</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _ASM_RISCV_BITOPS_H */</span>
[...]
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124171">Boqun Feng</a> - July 12, 2017, 12:44 p.m.</div>
<pre class="content">
On Wed, Jul 12, 2017 at 08:40:49PM +0800, Boqun Feng wrote:
[...]
<span class="quote">&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * set_bit - Atomically set a bit in memory</span>
<span class="quote">&gt; &gt; + * @nr: the bit to set</span>
<span class="quote">&gt; &gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is incorrect, {set,change,clear}_bit() can be reordered, see</span>
<span class="quote">&gt; Documentation/memory-barriers.txt, they are just relaxed atomics. But I</span>
<span class="quote">&gt; think you just copy this from x86 code, so maybe x86 code needs help</span>
<span class="quote">&gt; too, at least claim that&#39;s only x86-specific guarantee.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; + * if you do not require the atomic guarantees.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Note: there are no guarantees that this function will not be reordered</span>
<span class="quote">&gt; &gt; + * on non x86 architectures, so if you are writing portable code,</span>
<span class="quote">&gt; &gt; + * make sure not to rely on its reordering guarantees.</span>
<span class="quote">&gt; &gt; + *</span>

Hmmm.. the claim is right here ;-/

As your implementation is relax semantics, you&#39;d better rewrite the
comment ;-)

Regards,
Boqun
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - July 12, 2017, 12:49 p.m.</div>
<pre class="content">
On Wed, Jul 12, 2017 at 08:40:49PM +0800, Boqun Feng wrote:
<span class="quote">&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * set_bit - Atomically set a bit in memory</span>
<span class="quote">&gt; &gt; + * @nr: the bit to set</span>
<span class="quote">&gt; &gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is incorrect, {set,change,clear}_bit() can be reordered, see</span>
<span class="quote">&gt; Documentation/memory-barriers.txt, they are just relaxed atomics. But I</span>
<span class="quote">&gt; think you just copy this from x86 code, so maybe x86 code needs help</span>
<span class="quote">&gt; too, at least claim that&#39;s only x86-specific guarantee.</span>

Yeah, I suspect that&#39;s an x86 special (all our atomics are fully
ordered).
<span class="quote">
&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="quote">&gt; &gt; + * @nr: Bit to set</span>
<span class="quote">&gt; &gt; + * @addr: Address to count from</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This operation is atomic and provides acquire barrier semantics.</span>
<span class="quote">&gt; &gt; + * It can be used to implement bit locks.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static inline int test_and_set_bit_lock(</span>
<span class="quote">&gt; &gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return test_and_set_bit(nr, addr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you want, you can open code an &quot;amoor.aq&quot; here, because</span>
<span class="quote">&gt; test_and_set_bit_lock() only needs an acquire barrier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/**</span>
<span class="quote">&gt; &gt; + * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="quote">&gt; &gt; + * @nr: the bit to set</span>
<span class="quote">&gt; &gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This operation is atomic and provides release barrier semantics.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static inline void clear_bit_unlock(</span>
<span class="quote">&gt; &gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You need a smp_mb__before_atomic() here, because clear_bit() is only</span>
<span class="quote">&gt; relaxed atomic. And clear_bit_unlock() is a release.</span>

alternatively you can do &quot;amoand.rl&quot;.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - July 12, 2017, 1:13 p.m.</div>
<pre class="content">
On Wed, Jul 12, 2017 at 3:31 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * FIXME: I&#39;m flip-flopping on whether or not we should keep this or enforce</span>
<span class="quote">&gt; + * the ordering with I/O on spinlocks.  The worry is that drivers won&#39;t get</span>
<span class="quote">&gt; + * this correct, but I also don&#39;t want to introduce a fence into the lock code</span>
<span class="quote">&gt; + * that otherwise only uses AMOs and LR/SC.   For now I&#39;m leaving this here:</span>
<span class="quote">&gt; + * &quot;w,o&quot; is sufficient to ensure that all writes to the device has completed</span>
<span class="quote">&gt; + * before the write to the spinlock is allowed to commit.  I surmised this from</span>
<span class="quote">&gt; + * reading &quot;ACQUIRES VS I/O ACCESSES&quot; in memory-barriers.txt.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define mmiowb()       __asm__ __volatile__ (&quot;fence o,w&quot; : : : &quot;memory&quot;);</span>

Not sure if this has been discussed before, but have you considered doing
it like powerpc with a per-cpu flag that gets checked by spin_unlock()?
<span class="quote">


&gt; +/*</span>
<span class="quote">&gt; + * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="quote">&gt; + * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="quote">&gt; + * accesses.  The memory barriers here are necessary as RISC-V doesn&#39;t define</span>
<span class="quote">&gt; + * any ordering constraints on accesses to the device I/O space.  These are</span>
<span class="quote">&gt; + * defined to order the indicated access (either a read or write) with all</span>
<span class="quote">&gt; + * other I/O memory accesses.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * FIXME: The platform spec will define the default Linux-capable platform to</span>
<span class="quote">&gt; + * have some extra IO ordering constraints that will make these fences</span>
<span class="quote">&gt; + * unnecessary.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __iorrmb()     __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +#define __iorwmb()     __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define readb_relaxed(c)       ({ u8  __v = readb_cpu(c); __iorrmb(); __v; })</span>
<span class="quote">&gt; +#define readw_relaxed(c)       ({ u16 __v = readw_cpu(c); __iorrmb(); __v; })</span>
<span class="quote">&gt; +#define readl_relaxed(c)       ({ u32 __v = readl_cpu(c); __iorrmb(); __v; })</span>
<span class="quote">&gt; +#define readq_relaxed(c)       ({ u64 __v = readq_cpu(c); __iorrmb(); __v; })</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define writeb_relaxed(v,c)    ({ __iorwmb(); writeb_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writew_relaxed(v,c)    ({ __iorwmb(); writew_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writel_relaxed(v,c)    ({ __iorwmb(); writel_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writeq_relaxed(v,c)    ({ __iorwmb(); writeq_cpu((v),(c)); })</span>

What are the ordering rules for a write followed by a read? You say
that there are no ordering constraints on I/O access at all, but you don&#39;t
add any barriers between that pair, which is probably the most important
to get right.

Also, can you say here why the architecture is defined like this?
I&#39;m sure that there have been very careful considerations on what
the semantics should be, yet it appears that MMIO access is defined
to be so relaxed purely as an optimization that as a consequence
leads to much slower I/O than a more typical definition as the operating
system has to add much heavier barriers than would otherwise be
needed.
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="quote">&gt; + * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="quote">&gt; + * Normal memory access.  The memory barriers here are necessary as RISC-V</span>
<span class="quote">&gt; + * doesn&#39;t define any ordering between the memory space and the I/O space.</span>
<span class="quote">&gt; + * They may be stronger than necessary (&quot;i,r&quot; and &quot;w,o&quot; might be sufficient),</span>
<span class="quote">&gt; + * but I feel kind of queasy making these weaker in any manner than the relaxed</span>
<span class="quote">&gt; + * versions above.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __iormb()      __asm__ __volatile__ (&quot;fence i,ior&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +#define __iowmb()      __asm__ __volatile__ (&quot;fence iow,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define readb(c)               ({ u8  __v = readb_cpu(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readw(c)               ({ u16 __v = readw_cpu(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readl(c)               ({ u32 __v = readl_cpu(c); __iormb(); __v; })</span>
<span class="quote">&gt; +#define readq(c)               ({ u64 __v = readq_cpu(c); __iormb(); __v; })</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define writeb(v,c)            ({ __iowmb(); writeb_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writew(v,c)            ({ __iowmb(); writew_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writel(v,c)            ({ __iowmb(); writel_cpu((v),(c)); })</span>
<span class="quote">&gt; +#define writeq(v,c)            ({ __iowmb(); writeq_cpu((v),(c)); })</span>

These look good to me (once the _relaxed version has been clarified, the
same question applies here as well). I see that you don&#39;t override the
PCI I/O space accessors (inb/outb etc), which have even stricter
ordering requirements.
I think you also need a barrier after outb/outw/outl to ensure that the write
has completed on the bus before anything else happens.

Finally, I think you also need to override the string functions
(readsl/writesl/insl/outsl) that are lacking barriers in the asm-generic
version.

         Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - July 12, 2017, 5:17 p.m.</div>
<pre class="content">
On Wed, 12 Jul 2017 05:40:49 PDT (-0700), boqun.feng@gmail.com wrote:
<span class="quote">&gt; On Tue, Jul 11, 2017 at 06:31:23PM -0700, Palmer Dabbelt wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..b0a0c76e966a</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/bitops.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,216 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2012 Regents of the University of California</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt;&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt;&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt;&gt; + *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt;&gt; + *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt;&gt; + *   GNU General Public License for more details.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_BITOPS_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _LINUX_BITOPS_H</span>
<span class="quote">&gt;&gt; +#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="quote">&gt;&gt; +#endif /* _LINUX_BITOPS_H */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;linux/compiler.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;linux/irqflags.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/barrier.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/bitsperlong.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef smp_mb__before_clear_bit</span>
<span class="quote">&gt;&gt; +#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="quote">&gt;&gt; +#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="quote">&gt;&gt; +#endif /* smp_mb__before_clear_bit */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#if (BITS_PER_LONG == 64)</span>
<span class="quote">&gt;&gt; +#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="quote">&gt;&gt; +#elif (BITS_PER_LONG == 32)</span>
<span class="quote">&gt;&gt; +#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="quote">&gt;&gt; +#else</span>
<span class="quote">&gt;&gt; +#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So the test_and_{set,change,clear}_bit() have the similar semantics as</span>
<span class="quote">&gt; cmpxchg(), so</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="quote">&gt;&gt; +({								\</span>
<span class="quote">&gt;&gt; +	unsigned long __res, __mask;				\</span>
<span class="quote">&gt;&gt; +	__mask = BIT_MASK(nr);					\</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ... &quot;aqrl&quot; bit is needed here, and</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="quote">&gt;&gt; +		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ... &quot;memory&quot; clobber is needed here.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +	((__res &amp; __mask) != 0);				\</span>
<span class="quote">&gt;&gt; +})</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __op_bit(op, mod, nr, addr)				\</span>
<span class="quote">&gt;&gt; +	__asm__ __volatile__ (					\</span>
<span class="quote">&gt;&gt; +		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="quote">&gt;&gt; +		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="quote">&gt;&gt; +		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* Bitmask modifiers */</span>
<span class="quote">&gt;&gt; +#define __NOP(x)	(x)</span>
<span class="quote">&gt;&gt; +#define __NOT(x)	(~(x))</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * test_and_set_bit - Set a bit and return its old value</span>
<span class="quote">&gt;&gt; + * @nr: Bit to set</span>
<span class="quote">&gt;&gt; + * @addr: Address to count from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt;&gt; + * It may be reordered on other architectures than x86.</span>
<span class="quote">&gt;&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="quote">&gt;&gt; + * @nr: Bit to clear</span>
<span class="quote">&gt;&gt; + * @addr: Address to count from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt;&gt; + * It can be reordered on other architectures other than x86.</span>
<span class="quote">&gt;&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * test_and_change_bit - Change a bit and return its old value</span>
<span class="quote">&gt;&gt; + * @nr: Bit to change</span>
<span class="quote">&gt;&gt; + * @addr: Address to count from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This operation is atomic and cannot be reordered.</span>
<span class="quote">&gt;&gt; + * It also implies a memory barrier.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * set_bit - Atomically set a bit in memory</span>
<span class="quote">&gt;&gt; + * @nr: the bit to set</span>
<span class="quote">&gt;&gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is incorrect, {set,change,clear}_bit() can be reordered, see</span>
<span class="quote">&gt; Documentation/memory-barriers.txt, they are just relaxed atomics. But I</span>
<span class="quote">&gt; think you just copy this from x86 code, so maybe x86 code needs help</span>
<span class="quote">&gt; too, at least claim that&#39;s only x86-specific guarantee.</span>

I went ahead and fixed our comments.

  https://github.com/riscv/riscv-linux/commit/38d727b99b9eb76fac533cebc23f89d364b7d60d
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; + * if you do not require the atomic guarantees.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Note: there are no guarantees that this function will not be reordered</span>
<span class="quote">&gt;&gt; + * on non x86 architectures, so if you are writing portable code,</span>
<span class="quote">&gt;&gt; + * make sure not to rely on its reordering guarantees.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="quote">&gt;&gt; + * restricted to acting on a single-word quantity.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	__op_bit(or, __NOP, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * clear_bit - Clears a bit in memory</span>
<span class="quote">&gt;&gt; + * @nr: Bit to clear</span>
<span class="quote">&gt;&gt; + * @addr: Address to start counting from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="quote">&gt;&gt; + * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="quote">&gt;&gt; + * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="quote">&gt;&gt; + * in order to ensure changes are visible on other processors.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	__op_bit(and, __NOT, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * change_bit - Toggle a bit in memory</span>
<span class="quote">&gt;&gt; + * @nr: Bit to change</span>
<span class="quote">&gt;&gt; + * @addr: Address to start counting from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * change_bit() is atomic and may not be reordered. It may be</span>
<span class="quote">&gt;&gt; + * reordered on other architectures than x86.</span>
<span class="quote">&gt;&gt; + * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="quote">&gt;&gt; + * restricted to acting on a single-word quantity.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	__op_bit(xor, __NOP, nr, addr);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="quote">&gt;&gt; + * @nr: Bit to set</span>
<span class="quote">&gt;&gt; + * @addr: Address to count from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This operation is atomic and provides acquire barrier semantics.</span>
<span class="quote">&gt;&gt; + * It can be used to implement bit locks.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int test_and_set_bit_lock(</span>
<span class="quote">&gt;&gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return test_and_set_bit(nr, addr);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If you want, you can open code an &quot;amoor.aq&quot; here, because</span>
<span class="quote">&gt; test_and_set_bit_lock() only needs an acquire barrier.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="quote">&gt;&gt; + * @nr: the bit to set</span>
<span class="quote">&gt;&gt; + * @addr: the address to start counting from</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * This operation is atomic and provides release barrier semantics.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void clear_bit_unlock(</span>
<span class="quote">&gt;&gt; +	unsigned long nr, volatile unsigned long *addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You need a smp_mb__before_atomic() here, because clear_bit() is only</span>
<span class="quote">&gt; relaxed atomic. And clear_bit_unlock() is a release.</span>

Makes sense.  I went ahead and added the aq and rl bits to the AMOs already in
these two:

  https://github.com/riscv/riscv-linux/commit/f3903a2b403522a8dafd5cbe850caa22755d6b5b
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..ee3ab06e492b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,328 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+# include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+# if (__riscv_xlen &lt; 64)</span>
<span class="p_add">+#  error &quot;64-bit atomics require XLEN to be at least 64&quot;</span>
<span class="p_add">+# endif</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+static __always_inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC64_INIT(i) { (i) }</span>
<span class="p_add">+static __always_inline int atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return READ_ONCE(v-&gt;counter);</span>
<span class="p_add">+}</span>
<span class="p_add">+static __always_inline void atomic64_set(atomic64_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WRITE_ONCE(v-&gt;counter, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * First, the atomic ops that have no ordering constraints and therefor don&#39;t</span>
<span class="p_add">+ * have the AQ or RL bits set.  These don&#39;t return anything, so there&#39;s only</span>
<span class="p_add">+ * one version to worry about.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, asm_op, c_op, I, asm_type, c_type, prefix)				\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(c_type i, atomic##prefix##_t *v)		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	__asm__ __volatile__ (									\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type &quot; zero, %1, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)								\</span>
<span class="p_add">+		: &quot;r&quot; (I)									\</span>
<span class="p_add">+		: &quot;memory&quot;);									\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP (op, asm_op, c_op, I, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic ops that have ordered, relaxed, acquire, and relese variants.</span>
<span class="p_add">+ * There&#39;s two flavors of these: the arithmatic ops have both fetch and return</span>
<span class="p_add">+ * versions, while the logical ops only have fetch versions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_fetch_##op##c_or(c_type i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+	register c_type ret;										\</span>
<span class="p_add">+	__asm__ __volatile__ (										\</span>
<span class="p_add">+		&quot;amo&quot; #asm_op &quot;.&quot; #asm_type #asm_or &quot; %1, %2, %0&quot;					\</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (ret)								\</span>
<span class="p_add">+		: &quot;r&quot; (I)										\</span>
<span class="p_add">+		: &quot;memory&quot;);										\</span>
<span class="p_add">+	return ret;											\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, asm_type, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline c_type atomic##prefix##_##op##_return##c_or(int i, atomic##prefix##_t *v)	\</span>
<span class="p_add">+{													\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op##c_or(i, v) c_op I;						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )	\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, asm_or, c_or, d, long, 64)	\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(add, add, +,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(sub, add, +, -i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I, asm_or, c_or)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, w,  int,   )		\</span>
<span class="p_add">+        ATOMIC_FETCH_OP(op, asm_op, c_op, I, asm_or, c_or, d, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(and, and, &amp;,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS( or,  or, |,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i,      , _relaxed)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aq  , _acquire)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .rl  , _release)</span>
<span class="p_add">+ATOMIC_OPS(xor, xor, ^,  i, .aqrl,         )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * AMO-based operations above (aside from sub, which is easier to fit above).</span>
<span class="p_add">+ * These are required to perform a barrier, but they&#39;re OK this way because</span>
<span class="p_add">+ * atomic_*_return is also required to perform a barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, c_type, prefix)			\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(c_type i, atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(i, v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, func_op, comp_op, I)			\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I,  int,   )		\</span>
<span class="p_add">+        ATOMIC_OP (op, func_op, comp_op, I, long, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(add_and_test, add, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(sub_and_test, sub, ==, 0)</span>
<span class="p_add">+ATOMIC_OPS(add_negative, add,  &lt;, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, c_op, I, prefix)					\</span>
<span class="p_add">+static __always_inline void atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	atomic##prefix##_##func_op(I, v);					\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_FETCH_OP(op, func_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_fetch_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_fetch_##func_op(I, v);				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP_RETURN(op, asm_op, c_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline int atomic##prefix##_##op##_return(atomic##prefix##_t *v) \</span>
<span class="p_add">+{										\</span>
<span class="p_add">+        return atomic##prefix##_fetch_##op(v) c_op I;				\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(op, asm_op, c_op, I)						\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I,   )				\</span>
<span class="p_add">+        ATOMIC_OP       (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_FETCH_OP (op, asm_op, c_op, I, 64)				\</span>
<span class="p_add">+        ATOMIC_OP_RETURN(op, asm_op, c_op, I, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(inc, add, +,  1)</span>
<span class="p_add">+ATOMIC_OPS(dec, add, +, -1)</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+#undef ATOMIC_FETCH_OP</span>
<span class="p_add">+#undef ATOMIC_OP_RETURN</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_OP(op, func_op, comp_op, I, prefix)				\</span>
<span class="p_add">+static __always_inline bool atomic##prefix##_##op(atomic##prefix##_t *v)	\</span>
<span class="p_add">+{										\</span>
<span class="p_add">+	return atomic##prefix##_##func_op##_return(v) comp_op I;		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0,   )</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0,   )</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+ATOMIC_OP(inc_and_test, inc, ==, 0, 64)</span>
<span class="p_add">+ATOMIC_OP(dec_and_test, dec, ==, 0, 64)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+/* This is required to provide a barrier on success. */</span>
<span class="p_add">+static __always_inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.w.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.w.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline long atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+{</span>
<span class="p_add">+       register long prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;0:\n\t&quot;</span>
<span class="p_add">+		&quot;lr.d.aqrl %0, %2\n\t&quot;</span>
<span class="p_add">+		&quot;beq       %0, %4, 1f\n\t&quot;</span>
<span class="p_add">+		&quot;add       %1, %0, %3\n\t&quot;</span>
<span class="p_add">+		&quot;sc.d.aqrl %1, %1, %2\n\t&quot;</span>
<span class="p_add">+		&quot;bnez      %1, 0b\n\t&quot;</span>
<span class="p_add">+		&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The extra atomic operations that are constructed from one of the core</span>
<span class="p_add">+ * LR/SC-based operations above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int atomic_inc_not_zero(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return __atomic_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+static __always_inline int atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * atomic_{cmp,}xchg is required to have exactly the same ordering semantics as</span>
<span class="p_add">+ * {cmp,}xchg and the operations that return, so they need a barrier.  We just</span>
<span class="p_add">+ * use the other implementations directly.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ATOMIC_OP(c_t, prefix, c_or, size, asm_or)						\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_cmpxchg##c_or(atomic##prefix##_t *v, c_t o, c_t n) 	\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __cmpxchg(&amp;(v-&gt;counter), o, n, size, asm_or, asm_or);				\</span>
<span class="p_add">+}												\</span>
<span class="p_add">+static __always_inline c_t atomic##prefix##_xchg##c_or(atomic##prefix##_t *v, c_t n) 		\</span>
<span class="p_add">+{												\</span>
<span class="p_add">+	return __xchg(n, &amp;(v-&gt;counter), size, asm_or);						\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ATOMIC_OPS(c_or, asm_or)			\</span>
<span class="p_add">+	ATOMIC_OP( int,   , c_or, 4, asm_or)		\</span>
<span class="p_add">+	ATOMIC_OP(long, 64, c_or, 8, asm_or)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+ATOMIC_OPS(        , .aqrl)</span>
<span class="p_add">+ATOMIC_OPS(_acquire,   .aq)</span>
<span class="p_add">+ATOMIC_OPS(_release,   .rl)</span>
<span class="p_add">+ATOMIC_OPS(_relaxed,      )</span>
<span class="p_add">+</span>
<span class="p_add">+#undef ATOMIC_OPS</span>
<span class="p_add">+#undef ATOMIC_OP</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..183534b7c39b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,68 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()		__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_FENCE(p, s) \</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;fence &quot; #p &quot;,&quot; #s : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barriers need to enforce ordering on both devices or memory. */</span>
<span class="p_add">+#define mb()		RISCV_FENCE(iorw,iorw)</span>
<span class="p_add">+#define rmb()		RISCV_FENCE(ir,ir)</span>
<span class="p_add">+#define wmb()		RISCV_FENCE(ow,ow)</span>
<span class="p_add">+</span>
<span class="p_add">+/* These barriers do not need to enforce ordering on devices, just memory. */</span>
<span class="p_add">+#define smp_mb()	RISCV_FENCE(rw,rw)</span>
<span class="p_add">+#define smp_rmb()	RISCV_FENCE(r,r)</span>
<span class="p_add">+#define smp_wmb()	RISCV_FENCE(w,w)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These fences exist to enforce ordering around the relaxed AMOs.  The</span>
<span class="p_add">+ * documentation defines that</span>
<span class="p_add">+ * &quot;</span>
<span class="p_add">+ *     atomic_fetch_add();</span>
<span class="p_add">+ *   is equivalent to:</span>
<span class="p_add">+ *     smp_mb__before_atomic();</span>
<span class="p_add">+ *     atomic_fetch_add_relaxed();</span>
<span class="p_add">+ *     smp_mb__after_atomic();</span>
<span class="p_add">+ * &quot;</span>
<span class="p_add">+ * So we emit full fences on both sides.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __smb_mb__before_atomic()	smp_mb()</span>
<span class="p_add">+#define __smb_mb__after_atomic()	smp_mb()</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These barriers prevent accesses performed outside a spinlock from being moved</span>
<span class="p_add">+ * inside a spinlock.  Since RISC-V sets the aq/rl bits on our spinlock only</span>
<span class="p_add">+ * enforce release consistency, we need full fences here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define smb_mb__before_spinlock()	smp_mb()</span>
<span class="p_add">+#define smb_mb__after_spinlock()	smp_mb()</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b0a0c76e966a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,216 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It may be reordered on other architectures than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It can be reordered on other architectures other than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="p_add">+ * if you do not require the atomic guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="p_add">+ * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="p_add">+ * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="p_add">+ * in order to ensure changes are visible on other processors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit() is atomic and may not be reordered. It may be</span>
<span class="p_add">+ * reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return test_and_set_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0595585013b0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..db249dbc7b97</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,134 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size, asm_or)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new)				\</span>
<span class="p_add">+			: &quot;memory&quot;);				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d&quot; #asm_or &quot; %0, %2, %1&quot;	\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new)				\</span>
<span class="p_add">+			: &quot;memory&quot;);				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr)), .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg32(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg64(ptr, x)				\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	xchg((ptr), (x));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size, lrb, scb)			\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __old = (old);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.w&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new)			\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d&quot; #scb &quot; %0, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bne         %0, %z3, 1f\n&quot;			\</span>
<span class="p_add">+			&quot;sc.d&quot; #lrb &quot; %1, %z4, %2\n&quot;			\</span>
<span class="p_add">+			&quot;bnez        %1, 0b\n&quot;				\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new)			\</span>
<span class="p_add">+			: &quot;memory&quot;);					\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), .aqrl, .aqrl))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr)), , ))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg32_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 4);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f7d63320ff8e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,180 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * {read,write}{b,w,l,q} based on arch/arm64/include/asm/io.h</span>
<span class="p_add">+ *   which was based on arch/arm/include/io.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1996-2000 Russell King</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The RISC-V ISA doesn&#39;t yet specify how to query of modify PMAs, so we can&#39;t</span>
<span class="p_add">+ * change the properties of memory regions.  This should be fixed by the</span>
<span class="p_add">+ * upcoming platform spec.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Generic IO read/write.  These perform native-endian accesses. */</span>
<span class="p_add">+#define __raw_writeb __raw_writeb</span>
<span class="p_add">+static inline void __raw_writeb(u8 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sb %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writew __raw_writew</span>
<span class="p_add">+static inline void __raw_writew(u16 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sh %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_writel __raw_writel</span>
<span class="p_add">+static inline void __raw_writel(u32 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sw %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_writeq __raw_writeq</span>
<span class="p_add">+static inline void __raw_writeq(u64 val, volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;sd %0, 0(%1)&quot; : : &quot;r&quot; (val), &quot;r&quot; (addr));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readb __raw_readb</span>
<span class="p_add">+static inline u8 __raw_readb(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u8 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lb %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readw __raw_readw</span>
<span class="p_add">+static inline u16 __raw_readw(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lh %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __raw_readl __raw_readl</span>
<span class="p_add">+static inline u32 __raw_readl(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;lw %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __riscv64</span>
<span class="p_add">+#define __raw_readq __raw_readq</span>
<span class="p_add">+static inline u64 __raw_readq(const volatile void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;ld %0, 0(%1)&quot; : &quot;=r&quot; (val) : &quot;r&quot; (addr));</span>
<span class="p_add">+	return val;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: I&#39;m flip-flopping on whether or not we should keep this or enforce</span>
<span class="p_add">+ * the ordering with I/O on spinlocks.  The worry is that drivers won&#39;t get</span>
<span class="p_add">+ * this correct, but I also don&#39;t want to introduce a fence into the lock code</span>
<span class="p_add">+ * that otherwise only uses AMOs and LR/SC.   For now I&#39;m leaving this here:</span>
<span class="p_add">+ * &quot;w,o&quot; is sufficient to ensure that all writes to the device has completed</span>
<span class="p_add">+ * before the write to the spinlock is allowed to commit.  I surmised this from</span>
<span class="p_add">+ * reading &quot;ACQUIRES VS I/O ACCESSES&quot; in memory-barriers.txt.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define mmiowb()	__asm__ __volatile__ (&quot;fence o,w&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Unordered I/O memory access primitives.  These are even more relaxed than</span>
<span class="p_add">+ * the relaxed versions, as they don&#39;t even order accesses between successive</span>
<span class="p_add">+ * operations to the I/O regions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define readb_cpu(c)		({ u8  __r = __raw_readb(c); __r; })</span>
<span class="p_add">+#define readw_cpu(c)		({ u16 __r = le16_to_cpu((__force __le16)__raw_readw(c)); __r; })</span>
<span class="p_add">+#define readl_cpu(c)		({ u32 __r = le32_to_cpu((__force __le32)__raw_readl(c)); __r; })</span>
<span class="p_add">+#define readq_cpu(c)		({ u64 __r = le64_to_cpu((__force __le64)__raw_readq(c)); __r; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_cpu(v,c)		((void)__raw_writeb((v),(c)))</span>
<span class="p_add">+#define writew_cpu(v,c)		((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))</span>
<span class="p_add">+#define writel_cpu(v,c)		((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))</span>
<span class="p_add">+#define writeq_cpu(v,c)		((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Relaxed I/O memory access primitives. These follow the Device memory</span>
<span class="p_add">+ * ordering rules but do not guarantee any ordering relative to Normal memory</span>
<span class="p_add">+ * accesses.  The memory barriers here are necessary as RISC-V doesn&#39;t define</span>
<span class="p_add">+ * any ordering constraints on accesses to the device I/O space.  These are</span>
<span class="p_add">+ * defined to order the indicated access (either a read or write) with all</span>
<span class="p_add">+ * other I/O memory accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: The platform spec will define the default Linux-capable platform to</span>
<span class="p_add">+ * have some extra IO ordering constraints that will make these fences</span>
<span class="p_add">+ * unnecessary.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iorrmb()	__asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iorwmb()	__asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb_relaxed(c)	({ u8  __v = readb_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readw_relaxed(c)	({ u16 __v = readw_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readl_relaxed(c)	({ u32 __v = readl_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+#define readq_relaxed(c)	({ u64 __v = readq_cpu(c); __iorrmb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb_relaxed(v,c)	({ __iorwmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew_relaxed(v,c)	({ __iorwmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel_relaxed(v,c)	({ __iorwmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq_relaxed(v,c)	({ __iorwmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * I/O memory access primitives. Reads are ordered relative to any</span>
<span class="p_add">+ * following Normal memory access. Writes are ordered relative to any prior</span>
<span class="p_add">+ * Normal memory access.  The memory barriers here are necessary as RISC-V</span>
<span class="p_add">+ * doesn&#39;t define any ordering between the memory space and the I/O space.</span>
<span class="p_add">+ * They may be stronger than necessary (&quot;i,r&quot; and &quot;w,o&quot; might be sufficient),</span>
<span class="p_add">+ * but I feel kind of queasy making these weaker in any manner than the relaxed</span>
<span class="p_add">+ * versions above.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __iormb()	__asm__ __volatile__ (&quot;fence i,ior&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+#define __iowmb()	__asm__ __volatile__ (&quot;fence iow,o&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+#define readb(c)		({ u8  __v = readb_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readw(c)		({ u16 __v = readw_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readl(c)		({ u32 __v = readl_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+#define readq(c)		({ u64 __v = readq_cpu(c); __iormb(); __v; })</span>
<span class="p_add">+</span>
<span class="p_add">+#define writeb(v,c)		({ __iowmb(); writeb_cpu((v),(c)); })</span>
<span class="p_add">+#define writew(v,c)		({ __iowmb(); writew_cpu((v),(c)); })</span>
<span class="p_add">+#define writel(v,c)		({ __iowmb(); writel_cpu((v),(c)); })</span>
<span class="p_add">+#define writeq(v,c)		({ __iowmb(); writeq_cpu((v),(c)); })</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b3b394ffaf7e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,165 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: Replace this with a ticket lock, like MIPS. */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_rmb();</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+	} while (arch_spin_is_locked(lock));</span>
<span class="p_add">+	smp_acquire__after_ctrl_dep();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83ac4ac9e2ac</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c229509288ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ee4ae370b5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



