
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RESEND,1/1] mm: vmstat: Add OOM victims count in vmstat counter - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RESEND,1/1] mm: vmstat: Add OOM victims count in vmstat counter</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 12, 2015, 2:28 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1444660139-30125-1-git-send-email-pintu.k@samsung.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7376051/mbox/"
   >mbox</a>
|
   <a href="/patch/7376051/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7376051/">/patch/7376051/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 6555EBEEA4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Oct 2015 14:45:54 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 3613720489
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Oct 2015 14:45:53 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1C9B4204AB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Oct 2015 14:45:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752000AbbJLOps (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 12 Oct 2015 10:45:48 -0400
Received: from mailout2.samsung.com ([203.254.224.25]:54441 &quot;EHLO
	mailout2.samsung.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751571AbbJLOpr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 12 Oct 2015 10:45:47 -0400
Received: from epcpsbgr4.samsung.com
	(u144.gpu120.samsung.co.kr [203.254.230.144])
	by mailout2.samsung.com (Oracle Communications Messaging Server
	7.0.5.31.0 64bit (built May  5 2014))
	with ESMTP id &lt;0NW4008JY3O9QMB0@mailout2.samsung.com&gt; for
	linux-kernel@vger.kernel.org; Mon, 12 Oct 2015 23:45:45 +0900 (KST)
Received: from epcpsbgm2new.samsung.com ( [172.20.52.122])
	by epcpsbgr4.samsung.com (EPCPMTA) with SMTP id BA.B6.05342.997CB165;
	Mon, 12 Oct 2015 23:45:45 +0900 (KST)
X-AuditID: cbfee690-f794e6d0000014de-b7-561bc799929a
Received: from epmmp2 ( [203.254.227.17])	by epcpsbgm2new.samsung.com
	(EPCPMTA) with SMTP id 31.26.18629.997CB165;
	Mon, 12 Oct 2015 23:45:45 +0900 (KST)
Received: from pintu-ubuntu.sisodomain.com ([107.108.86.218])
	by mmp2.samsung.com
	(Oracle Communications Messaging Server 7.0.5.31.0 64bit (built May 5
	2014)) with ESMTPA id &lt;0NW400DDI3HPR200@mmp2.samsung.com&gt;; Mon,
	12 Oct 2015 23:45:45 +0900 (KST)
From: Pintu Kumar &lt;pintu.k@samsung.com&gt;
To: akpm@linux-foundation.org, minchan@kernel.org, dave@stgolabs.net,
	pintu.k@samsung.com, mhocko@suse.cz, koct9i@gmail.com,
	rientjes@google.com, hannes@cmpxchg.org,
	penguin-kernel@i-love.sakura.ne.jp, bywxiaobai@163.com,
	mgorman@suse.de, vbabka@suse.cz, js1304@gmail.com,
	kirill.shutemov@linux.intel.com, alexander.h.duyck@redhat.com,
	sasha.levin@oracle.com, cl@linux.com, fengguang.wu@intel.com,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: cpgs@samsung.com, pintu_agarwal@yahoo.com, pintu.ping@gmail.com,
	vishnu.ps@samsung.com, rohit.kr@samsung.com,
	c.rajkumar@samsung.com, sreenathd@samsung.com
Subject: [RESEND PATCH 1/1] mm: vmstat: Add OOM victims count in vmstat
	counter
Date: Mon, 12 Oct 2015 19:58:59 +0530
Message-id: &lt;1444660139-30125-1-git-send-email-pintu.k@samsung.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-reply-to: &lt;1444656800-29915-1-git-send-email-pintu.k@samsung.com&gt;
References: &lt;1444656800-29915-1-git-send-email-pintu.k@samsung.com&gt;
X-Brightmail-Tracker: H4sIAAAAAAAAAzWSSUxTURSGue+9joo+C+ilRMAaJZCADAUuEQ0JLt4CA4aEBRqw4AsQpqYF
	gmyYRAKYyjy1GkKJVCiilaCAYC3InDigYZJCgrYioyIgYtDSxt0Zvv//z+Kwcd4mwWcnpKTR
	khRRkoDJJdS2wiz3uiGHCM+xVRekaFcz0Z/9GgLJFC6opLICQ5PbKwAt6VxR67QaoHVjO45a
	NZdRXe89Bpo2Kgj0sGiBgSa6FUykV/9loIo1A0Df8ncY6MHWOgvJb60xkGynn4Uml6oJtL06
	y0K3m55gyJhfQCDlwCyOShbzCSTPlQFUKZsDQQ6UUlXJoPpX1nGqq36ORTVo0qmuthaMUr5Y
	wihNSxGT0vwoZ1HDtXsE9flDDUbdH7lCff8yQ1DrfR+ZlKyjBVBV8mxqvGGAFWYTyQ28QScl
	ZNCScxevc+PzCiaAuN85c6WmA+SAVn4x4LAhKYRNA9WYpT4O3+rbmcWAy+aRKgBlj7Ws/5Cq
	cQRYFvUAKienMEuTi8GhZhlxQDFJF/hrZc9M2ZKjOKxR9+EHDU4qASzcrTZ72ZChsDfvrllB
	kGdg06DSPLcmg+Fm85zJlm3Kc4aK8sCDMYe8BMd6X5txngl5o5WbPSGZz4HLj6owiw8Jtyt0
	hEV7Emq0uOVse/hKNUWUApsGYNUC7GhxrFgaEycRekhFydL0lDiP2NRkDTD9wti+ofQ50GvP
	6wDJBoLD1uwOfgSPIcqQ3kzWAV9TQhnOt4tNNb1PSlq0l4+fN/IV+vp4+wf4CU5YL/N3w3lk
	nCiNTqRpMS2JlqQn0VIdwNgcfg5IPPLuZVnI2cFmeShPXLeREe5OnDJaBUQMg9PdHLztZ5XI
	KbHnaevRLaI2KkjIWQBc/XtrwbyjeCNsqz4z5oLdp0L+PB1ln8ayl1th9e6dJYav/tdcF72y
	nUKO9fjkFY3HHJmZ1zfekXhqHTsPbRim97OMzxpHr/4OdrMbMUQKCGm8yMsNl0hF/wDdE5Nu
	BgMAAA==
X-Brightmail-Tracker: H4sIAAAAAAAAA+NgFjrAKsWRmVeSWpSXmKPExsVy+t9jQd2Zx6XDDN6fkrSYs34Nm8Wff9NZ
	LPrmqFt0T5nMZHH92xtGi5eHNC1W31zDaPH++Xpmi9WbfC1m7p3LanHz+RwWi5WdD1gtLu+a
	w2Zxb81/VovJ754xWrxq/s5qsezre3aL2S3vWC36vh9mt7j+chqLxbe3t9kt2pZsZLJ43tzK
	YrH4yG1mi+7HzSwWsxv7GC2m9N1ldJD2WLxiCqvH4TfvmT12zrrL7rFgU6nHzrWrmDwW73nJ
	5LFpVSebx6ZPk9g9Tsz4zeLx5Mp0Jo95JwM9Pj69xeLxft9VNo++LasYPabOrvc4s+AIe4Bw
	VAOjTUZqYkpqkUJqXnJ+SmZeuq2Sd3C8c7ypmYGhrqGlhbmSQl5ibqqtkotPgK5bZg4wdJQU
	yhJzSoFCAYnFxUr6dpgmhIa46VrANEbo+oYEwfUYGaCBhDWMGU2tlxkLDitUvJm+hbGBcbVU
	FyMnh4SAicSKRScZIWwxiQv31rN1MXJxCAnMYpRYfP0GE4TTyCRxfHkfC0gVm4C6xI83vxlB
	EiICp5glpq/ZxwziMAssZpRo/zmNHaRKWMBfYm9TP1gHi4CqxJJji8HivALOEp+X3wUaywG0
	T0FiziQbkDCngIvE6b1HwcqFgErOH5jNPIGRdwEjwypGidSC5ILipPRco7zUcr3ixNzi0rx0
	veT83E2M4NTzTHoH4+Fd7ocYBTgYlXh4ObZIhQmxJpYVV+YeYpTgYFYS4VWaKB0mxJuSWFmV
	WpQfX1Sak1p8iNEU6K6JzFKiyfnAtJhXEm9obGJuamxqaWJhYmapJM574xBDmJBAemJJanZq
	akFqEUwfEwenVAMjl/DrgPjrwe5iZoKsj5nEtlsG/sxadvrJgYuv8zWbH6jc+z5j69XPSScc
	rzQWXnB93fnf6OLmho7In3utzii7btrfdLZ+08ofqzQ4DZI+qiaEv9ph/sGS91eTmMALP4bW
	tX8rd26dvbLix47+kONdS+bw3/DO+qu7KWFzd2zGapcbJ2IcQs3WKLEUZyQaajEXFScCAC3F
	45RTAwAA
DLP-Filter: Pass
X-MTR: 20000000000000000@CPGS
X-CFilter-Loop: Reflected
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 12, 2015, 2:28 p.m.</div>
<pre class="content">
This patch maintains the number of oom victims kill count in
/proc/vmstat.
Currently, we are dependent upon kernel logs when the kernel OOM occurs.
But kernel OOM can went passed unnoticed by the developer as it can
silently kill some background applications/services.
In some small embedded system, it might be possible that OOM is captured
in the logs but it was over-written due to ring-buffer.
Thus this interface can quickly help the user in analyzing, whether there
were any OOM kill happened in the past, or whether the system have ever
entered the oom kill stage till date.

Thus, it can be beneficial under following cases:
1. User can monitor kernel oom kill scenario without looking into the
   kernel logs.
2. It can help in tuning the watermark level in the system.
3. It can help in tuning the low memory killer behavior in user space.
4. It can be helpful on a logless system or if klogd logging
   (/var/log/messages) are disabled.

A snapshot of the result of 3 days of over night test is shown below:
System: ARM Cortex A7, 1GB RAM, 8GB EMMC
Linux: 3.10.xx
Category: reference smart phone device
Loglevel: 7
Conditions: Fully loaded, BT/WiFi/GPS ON
Tests: auto launching of ~30+ apps using test scripts, in a loop for
3 days.
At the end of tests, check:
$ cat /proc/vmstat
nr_oom_victims 6

As we noticed, there were around 6 oom kill victims.

The OOM is bad for any system. So, this counter can help in quickly
tuning the OOM behavior of the system, without depending on the logs.
<span class="signed-off-by">
Signed-off-by: Pintu Kumar &lt;pintu.k@samsung.com&gt;</span>
---
V2: Removed oom_stall, Suggested By: Michal Hocko &lt;mhocko@kernel.org&gt;
    Renamed oom_kill_count to nr_oom_victims,
    Suggested By: Michal Hocko &lt;mhocko@kernel.org&gt;
    Suggested By: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;

 include/linux/vm_event_item.h |    1 +
 mm/oom_kill.c                 |    2 ++
 mm/page_alloc.c               |    1 -
 mm/vmstat.c                   |    1 +
 4 files changed, 4 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Oct. 14, 2015, 3:05 a.m.</div>
<pre class="content">
On Mon, 12 Oct 2015, Pintu Kumar wrote:
<span class="quote">
&gt; This patch maintains the number of oom victims kill count in</span>
<span class="quote">&gt; /proc/vmstat.</span>
<span class="quote">&gt; Currently, we are dependent upon kernel logs when the kernel OOM occurs.</span>
<span class="quote">&gt; But kernel OOM can went passed unnoticed by the developer as it can</span>
<span class="quote">&gt; silently kill some background applications/services.</span>
<span class="quote">&gt; In some small embedded system, it might be possible that OOM is captured</span>
<span class="quote">&gt; in the logs but it was over-written due to ring-buffer.</span>
<span class="quote">&gt; Thus this interface can quickly help the user in analyzing, whether there</span>
<span class="quote">&gt; were any OOM kill happened in the past, or whether the system have ever</span>
<span class="quote">&gt; entered the oom kill stage till date.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thus, it can be beneficial under following cases:</span>
<span class="quote">&gt; 1. User can monitor kernel oom kill scenario without looking into the</span>
<span class="quote">&gt;    kernel logs.</span>

I&#39;m not sure how helpful that would be since we don&#39;t know anything about 
the oom kill itself, only that at some point during the uptime there were 
oom kills.
<span class="quote">
&gt; 2. It can help in tuning the watermark level in the system.</span>

I disagree with this one, because we can encounter oom kills due to 
fragmentation rather than low memory conditions for high-order 
allocations.  The amount of free memory may be substantially higher than 
all zone watermarks.
<span class="quote">
&gt; 3. It can help in tuning the low memory killer behavior in user space.</span>

Same reason as above.
<span class="quote">
&gt; 4. It can be helpful on a logless system or if klogd logging</span>
<span class="quote">&gt;    (/var/log/messages) are disabled.</span>
<span class="quote">&gt; </span>

This would be similar to point (1) above, and I question how helpful it 
would be.  I notice that all oom kills (system, cpuset, mempolicy, and 
memcg) are treated equally in this case and there&#39;s no way to 
differentiate them.  That would lead me to believe that you are targeting 
this change for systems that don&#39;t use mempolicies or cgroups.  That&#39;s 
fine, but I doubt it will be helpful for anybody else.
<span class="quote">
&gt; A snapshot of the result of 3 days of over night test is shown below:</span>
<span class="quote">&gt; System: ARM Cortex A7, 1GB RAM, 8GB EMMC</span>
<span class="quote">&gt; Linux: 3.10.xx</span>
<span class="quote">&gt; Category: reference smart phone device</span>
<span class="quote">&gt; Loglevel: 7</span>
<span class="quote">&gt; Conditions: Fully loaded, BT/WiFi/GPS ON</span>
<span class="quote">&gt; Tests: auto launching of ~30+ apps using test scripts, in a loop for</span>
<span class="quote">&gt; 3 days.</span>
<span class="quote">&gt; At the end of tests, check:</span>
<span class="quote">&gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; nr_oom_victims 6</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As we noticed, there were around 6 oom kill victims.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The OOM is bad for any system. So, this counter can help in quickly</span>
<span class="quote">&gt; tuning the OOM behavior of the system, without depending on the logs.</span>
<span class="quote">&gt; </span>

NACK to the patch since it isn&#39;t justified.

We&#39;ve long had a desire to have a better oom reporting mechanism rather 
than just the kernel log.  It seems like you&#39;re feeling the same pain.  I 
think it would be better to have an eventfd notifier for system oom 
conditions so we can track kernel oom kills (and conditions) in 
userspace.  I have a patch for that, and it works quite well when 
userspace is mlocked with a buffer in memory.

If you are only interested in a strict count of system oom kills, this 
could then easily be implemented without adding vmstat counters.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 14, 2015, 1:41 p.m.</div>
<pre class="content">
Hi,

Thank you very much for your review and comments.
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: David Rientjes [mailto:rientjes@google.com]</span>
<span class="quote">&gt; Sent: Wednesday, October 14, 2015 8:36 AM</span>
<span class="quote">&gt; To: Pintu Kumar</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; mhocko@suse.cz; koct9i@gmail.com; hannes@cmpxchg.org; penguin-kernel@i-</span>
<span class="quote">&gt; love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de; vbabka@suse.cz;</span>
<span class="quote">&gt; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com;</span>
<span class="quote">&gt; sreenathd@samsung.com</span>
<span class="quote">&gt; Subject: Re: [RESEND PATCH 1/1] mm: vmstat: Add OOM victims count in vmstat</span>
<span class="quote">&gt; counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, 12 Oct 2015, Pintu Kumar wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; This patch maintains the number of oom victims kill count in</span>
<span class="quote">&gt; &gt; /proc/vmstat.</span>
<span class="quote">&gt; &gt; Currently, we are dependent upon kernel logs when the kernel OOM occurs.</span>
<span class="quote">&gt; &gt; But kernel OOM can went passed unnoticed by the developer as it can</span>
<span class="quote">&gt; &gt; silently kill some background applications/services.</span>
<span class="quote">&gt; &gt; In some small embedded system, it might be possible that OOM is</span>
<span class="quote">&gt; &gt; captured in the logs but it was over-written due to ring-buffer.</span>
<span class="quote">&gt; &gt; Thus this interface can quickly help the user in analyzing, whether</span>
<span class="quote">&gt; &gt; there were any OOM kill happened in the past, or whether the system</span>
<span class="quote">&gt; &gt; have ever entered the oom kill stage till date.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Thus, it can be beneficial under following cases:</span>
<span class="quote">&gt; &gt; 1. User can monitor kernel oom kill scenario without looking into the</span>
<span class="quote">&gt; &gt;    kernel logs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure how helpful that would be since we don&#39;t know anything about the</span>
<span class="quote">&gt; oom kill itself, only that at some point during the uptime there were oom</span>
kills.
<span class="quote">&gt; </span>
Not sure about others.
For me it was very helpful during sluggish and long duration ageing tests.
With this, I don&#39;t have to look into the logs manually.
I just monitor this count in a script. 
The moment I get nr_oom_victims &gt; 1, I know that kernel OOM would have happened
and I need to take the log dump.
So, then I do: dmesg &gt;&gt; oom_logs.txt
Or, even stop the tests for further tuning.
<span class="quote">
&gt; &gt; 2. It can help in tuning the watermark level in the system.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I disagree with this one, because we can encounter oom kills due to</span>
<span class="quote">&gt; fragmentation rather than low memory conditions for high-order allocations.</span>
<span class="quote">&gt; The amount of free memory may be substantially higher than all zone</span>
<span class="quote">&gt; watermarks.</span>
<span class="quote">&gt; </span>
AFAIK, kernel oom happens only for lower-order (PAGE_ALLOC_COSTLY_ORDER).
For higher-order we get page allocation failure.
<span class="quote">
&gt; &gt; 3. It can help in tuning the low memory killer behavior in user space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same reason as above.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; 4. It can be helpful on a logless system or if klogd logging</span>
<span class="quote">&gt; &gt;    (/var/log/messages) are disabled.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This would be similar to point (1) above, and I question how helpful it would</span>
be.
<span class="quote">&gt; I notice that all oom kills (system, cpuset, mempolicy, and</span>
<span class="quote">&gt; memcg) are treated equally in this case and there&#39;s no way to differentiate</span>
them.
<span class="quote">&gt; That would lead me to believe that you are targeting this change for systems</span>
<span class="quote">&gt; that don&#39;t use mempolicies or cgroups.  That&#39;s fine, but I doubt it will be</span>
helpful
<span class="quote">&gt; for anybody else.</span>
<span class="quote">&gt; </span>
No, we are not targeting any specific category.
Our goal is simple, track and report kernel oom kill as soon as it occurs.
<span class="quote">
&gt; &gt; A snapshot of the result of 3 days of over night test is shown below:</span>
<span class="quote">&gt; &gt; System: ARM Cortex A7, 1GB RAM, 8GB EMMC</span>
<span class="quote">&gt; &gt; Linux: 3.10.xx</span>
<span class="quote">&gt; &gt; Category: reference smart phone device</span>
<span class="quote">&gt; &gt; Loglevel: 7</span>
<span class="quote">&gt; &gt; Conditions: Fully loaded, BT/WiFi/GPS ON</span>
<span class="quote">&gt; &gt; Tests: auto launching of ~30+ apps using test scripts, in a loop for</span>
<span class="quote">&gt; &gt; 3 days.</span>
<span class="quote">&gt; &gt; At the end of tests, check:</span>
<span class="quote">&gt; &gt; $ cat /proc/vmstat</span>
<span class="quote">&gt; &gt; nr_oom_victims 6</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; As we noticed, there were around 6 oom kill victims.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The OOM is bad for any system. So, this counter can help in quickly</span>
<span class="quote">&gt; &gt; tuning the OOM behavior of the system, without depending on the logs.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NACK to the patch since it isn&#39;t justified.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;ve long had a desire to have a better oom reporting mechanism rather than</span>
<span class="quote">&gt; just the kernel log.  It seems like you&#39;re feeling the same pain.  I think it</span>
would be
<span class="quote">&gt; better to have an eventfd notifier for system oom conditions so we can track</span>
<span class="quote">&gt; kernel oom kills (and conditions) in userspace.  I have a patch for that, and</span>
it
<span class="quote">&gt; works quite well when userspace is mlocked with a buffer in memory.</span>
<span class="quote">&gt; </span>
Ok, this would be interesting.
Can you point me to the patches?
I will quickly check if it is useful for us.
<span class="quote">
&gt; If you are only interested in a strict count of system oom kills, this could</span>
then
<span class="quote">&gt; easily be implemented without adding vmstat counters.</span>
<span class="quote">&gt;</span>
We are interested only to know when kernel OOM occurs and not even the oom
victim count. So that we can tune something is user space to avoid or delay it
as far as possible.

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Oct. 14, 2015, 10:04 p.m.</div>
<pre class="content">
On Wed, 14 Oct 2015, PINTU KUMAR wrote:
<span class="quote">
&gt; For me it was very helpful during sluggish and long duration ageing tests.</span>
<span class="quote">&gt; With this, I don&#39;t have to look into the logs manually.</span>
<span class="quote">&gt; I just monitor this count in a script. </span>
<span class="quote">&gt; The moment I get nr_oom_victims &gt; 1, I know that kernel OOM would have happened</span>
<span class="quote">&gt; and I need to take the log dump.</span>
<span class="quote">&gt; So, then I do: dmesg &gt;&gt; oom_logs.txt</span>
<span class="quote">&gt; Or, even stop the tests for further tuning.</span>
<span class="quote">&gt; </span>

I think eventfd(2) was created for that purpose, to avoid the constant 
polling that you would have to do to check nr_oom_victims and then take a 
snapshot.
<span class="quote">
&gt; &gt; I disagree with this one, because we can encounter oom kills due to</span>
<span class="quote">&gt; &gt; fragmentation rather than low memory conditions for high-order allocations.</span>
<span class="quote">&gt; &gt; The amount of free memory may be substantially higher than all zone</span>
<span class="quote">&gt; &gt; watermarks.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; AFAIK, kernel oom happens only for lower-order (PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="quote">&gt; For higher-order we get page allocation failure.</span>
<span class="quote">&gt; </span>

Order-3 is included.  I&#39;ve seen machines with _gigabytes_ of free memory 
in ZONE_NORMAL on a node and have an order-3 page allocation failure that 
called the oom killer.
<span class="quote">
&gt; &gt; We&#39;ve long had a desire to have a better oom reporting mechanism rather than</span>
<span class="quote">&gt; &gt; just the kernel log.  It seems like you&#39;re feeling the same pain.  I think it</span>
<span class="quote">&gt; would be</span>
<span class="quote">&gt; &gt; better to have an eventfd notifier for system oom conditions so we can track</span>
<span class="quote">&gt; &gt; kernel oom kills (and conditions) in userspace.  I have a patch for that, and</span>
<span class="quote">&gt; it</span>
<span class="quote">&gt; &gt; works quite well when userspace is mlocked with a buffer in memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Ok, this would be interesting.</span>
<span class="quote">&gt; Can you point me to the patches?</span>
<span class="quote">&gt; I will quickly check if it is useful for us.</span>
<span class="quote">&gt; </span>

https://lwn.net/Articles/589404.  It&#39;s invasive and isn&#39;t upstream.  I 
would like to restructure that patchset to avoid the memcg trickery and 
allow for a root-only eventfd(2) notification through procfs on system 
oom.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72438">Pintu Kumar</a> - Oct. 15, 2015, 2:35 p.m.</div>
<pre class="content">
Hi,
<span class="quote">
&gt; -----Original Message-----</span>
<span class="quote">&gt; From: David Rientjes [mailto:rientjes@google.com]</span>
<span class="quote">&gt; Sent: Thursday, October 15, 2015 3:35 AM</span>
<span class="quote">&gt; To: PINTU KUMAR</span>
<span class="quote">&gt; Cc: akpm@linux-foundation.org; minchan@kernel.org; dave@stgolabs.net;</span>
<span class="quote">&gt; mhocko@suse.cz; koct9i@gmail.com; hannes@cmpxchg.org; penguin-kernel@i-</span>
<span class="quote">&gt; love.sakura.ne.jp; bywxiaobai@163.com; mgorman@suse.de; vbabka@suse.cz;</span>
<span class="quote">&gt; js1304@gmail.com; kirill.shutemov@linux.intel.com;</span>
<span class="quote">&gt; alexander.h.duyck@redhat.com; sasha.levin@oracle.com; cl@linux.com;</span>
<span class="quote">&gt; fengguang.wu@intel.com; linux-kernel@vger.kernel.org; linux-mm@kvack.org;</span>
<span class="quote">&gt; cpgs@samsung.com; pintu_agarwal@yahoo.com; pintu.ping@gmail.com;</span>
<span class="quote">&gt; vishnu.ps@samsung.com; rohit.kr@samsung.com; c.rajkumar@samsung.com</span>
<span class="quote">&gt; Subject: RE: [RESEND PATCH 1/1] mm: vmstat: Add OOM victims count in vmstat</span>
<span class="quote">&gt; counter</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Wed, 14 Oct 2015, PINTU KUMAR wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; For me it was very helpful during sluggish and long duration ageing tests.</span>
<span class="quote">&gt; &gt; With this, I don&#39;t have to look into the logs manually.</span>
<span class="quote">&gt; &gt; I just monitor this count in a script.</span>
<span class="quote">&gt; &gt; The moment I get nr_oom_victims &gt; 1, I know that kernel OOM would have</span>
<span class="quote">&gt; &gt; happened and I need to take the log dump.</span>
<span class="quote">&gt; &gt; So, then I do: dmesg &gt;&gt; oom_logs.txt</span>
<span class="quote">&gt; &gt; Or, even stop the tests for further tuning.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think eventfd(2) was created for that purpose, to avoid the constant polling</span>
<span class="quote">&gt; that you would have to do to check nr_oom_victims and then take a snapshot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; &gt; I disagree with this one, because we can encounter oom kills due to</span>
<span class="quote">&gt; &gt; &gt; fragmentation rather than low memory conditions for high-order</span>
allocations.
<span class="quote">&gt; &gt; &gt; The amount of free memory may be substantially higher than all zone</span>
<span class="quote">&gt; &gt; &gt; watermarks.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; AFAIK, kernel oom happens only for lower-order</span>
<span class="quote">&gt; (PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="quote">&gt; &gt; For higher-order we get page allocation failure.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Order-3 is included.  I&#39;ve seen machines with _gigabytes_ of free memory in</span>
<span class="quote">&gt; ZONE_NORMAL on a node and have an order-3 page allocation failure that</span>
<span class="quote">&gt; called the oom killer.</span>
<span class="quote">&gt; </span>
Yes, if PAGE_ALLOC_COSTLY_ORDER is defined as 3, then order-3 will be included
for OOM. But that&#39;s fine. We are just interested to know if system entered oom
state.
That&#39;s the reason, earlier I added even _oom_stall_ to know if system ever
entered oom but resulted into page allocation failure instead of oom killing.
<span class="quote">
&gt; &gt; &gt; We&#39;ve long had a desire to have a better oom reporting mechanism</span>
<span class="quote">&gt; &gt; &gt; rather than just the kernel log.  It seems like you&#39;re feeling the</span>
<span class="quote">&gt; &gt; &gt; same pain.  I think it</span>
<span class="quote">&gt; &gt; would be</span>
<span class="quote">&gt; &gt; &gt; better to have an eventfd notifier for system oom conditions so we</span>
<span class="quote">&gt; &gt; &gt; can track kernel oom kills (and conditions) in userspace.  I have a</span>
<span class="quote">&gt; &gt; &gt; patch for that, and</span>
<span class="quote">&gt; &gt; it</span>
<span class="quote">&gt; &gt; &gt; works quite well when userspace is mlocked with a buffer in memory.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; Ok, this would be interesting.</span>
<span class="quote">&gt; &gt; Can you point me to the patches?</span>
<span class="quote">&gt; &gt; I will quickly check if it is useful for us.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; https://lwn.net/Articles/589404.  It&#39;s invasive and isn&#39;t upstream.  I would</span>
like to
<span class="quote">&gt; restructure that patchset to avoid the memcg trickery and allow for a</span>
root-only
<span class="quote">&gt; eventfd(2) notification through procfs on system oom.</span>

I am interested only in global oom case and not memcg. We have memcg enabled but
I think even memcg_oom will finally invoke _oom_kill_process_.
So, I am interested in a patchset that can trigger notifications from
oom_kill_process, as soon as any victim is killed.
Sorry, from your patchset, I could not actually local the system_oom
notification patch.
If you have similar patchset please point me to it.
It will be really helpful.
Thank you!

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 2b1cef8..dd2600d 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -57,6 +57,7 @@</span> <span class="p_context"> enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,</span>
 #ifdef CONFIG_HUGETLB_PAGE
 		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
 #endif
<span class="p_add">+		NR_OOM_VICTIMS,</span>
 		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
 		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
 		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 03b612b..802b8a1 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -570,6 +570,7 @@</span> <span class="p_context"> void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
 	 * space under its control.
 	 */
 	do_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);
<span class="p_add">+	count_vm_event(NR_OOM_VICTIMS);</span>
 	mark_oom_victim(victim);
 	pr_err(&quot;Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB\n&quot;,
 		task_pid_nr(victim), victim-&gt;comm, K(victim-&gt;mm-&gt;total_vm),
<span class="p_chunk">@@ -600,6 +601,7 @@</span> <span class="p_context"> void oom_kill_process(struct oom_control *oc, struct task_struct *p,</span>
 				task_pid_nr(p), p-&gt;comm);
 			task_unlock(p);
 			do_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);
<span class="p_add">+			count_vm_event(NR_OOM_VICTIMS);</span>
 		}
 	rcu_read_unlock();
 
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 9bcfd70..fafb09d 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -2761,7 +2761,6 @@</span> <span class="p_context"> __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,</span>
 		schedule_timeout_uninterruptible(1);
 		return NULL;
 	}
<span class="p_del">-</span>
 	/*
 	 * Go through the zonelist yet one more time, keep very high watermark
 	 * here, this is only to catch a parallel oom killing, we must fail if
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 1fd0886..8503a2e 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -808,6 +808,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;htlb_buddy_alloc_success&quot;,
 	&quot;htlb_buddy_alloc_fail&quot;,
 #endif
<span class="p_add">+	&quot;nr_oom_victims&quot;,</span>
 	&quot;unevictable_pgs_culled&quot;,
 	&quot;unevictable_pgs_scanned&quot;,
 	&quot;unevictable_pgs_rescued&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



