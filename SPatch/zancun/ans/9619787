
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[26/26] x86/mm: allow to have userspace mappings above 47-bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [26/26] x86/mm: allow to have userspace mappings above 47-bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 13, 2017, 5:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170313055020.69655-27-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9619787/mbox/"
   >mbox</a>
|
   <a href="/patch/9619787/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9619787/">/patch/9619787/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C31FD60244 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:54:49 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B0F2B28423
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:54:49 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A5E3D28451; Mon, 13 Mar 2017 05:54:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E08D928446
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:54:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751843AbdCMFyq (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Mar 2017 01:54:46 -0400
Received: from mga06.intel.com ([134.134.136.31]:47912 &quot;EHLO mga06.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751094AbdCMFvk (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Mar 2017 01:51:40 -0400
Received: from orsmga001.jf.intel.com ([10.7.209.18])
	by orsmga104.jf.intel.com with ESMTP; 12 Mar 2017 22:50:45 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.36,157,1486454400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1107759712&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by orsmga001.jf.intel.com with ESMTP; 12 Mar 2017 22:50:41 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id D574F58A; Mon, 13 Mar 2017 07:50:23 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, linux-arch@vger.kernel.org,
	linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCH 26/26] x86/mm: allow to have userspace mappings above 47-bits
Date: Mon, 13 Mar 2017 08:50:20 +0300
Message-Id: &lt;20170313055020.69655-27-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170313055020.69655-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170313055020.69655-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - March 13, 2017, 5:50 a.m.</div>
<pre class="content">
On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

To mitigate this, we are not going to allocate virtual address space
above 47-bit by default.

But userspace can ask for allocation from full address space by
specifying hint address (with or without MAP_FIXED) above 47-bits.

If hint address set above 47-bit, but MAP_FIXED is not specified, we try
to look for unmapped area by specified address. If it&#39;s already
occupied, we look for unmapped area in *full* address space, rather than
from 47-bit window.

This approach helps to easily make application&#39;s memory allocator aware
about large address space without manually tracking allocated virtual
address space.

One important case we need to handle here is interaction with MPX.
MPX (without MAWA( extension cannot handle addresses above 47-bit, so we
need to make sure that MPX cannot be enabled we already have VMA above
the boundary and forbid creating such VMAs once MPX is enabled.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/x86/include/asm/elf.h       |  2 +-
 arch/x86/include/asm/mpx.h       |  9 +++++++++
 arch/x86/include/asm/processor.h |  9 ++++++---
 arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-
 arch/x86/mm/hugetlbpage.c        | 31 +++++++++++++++++++++++++++----
 arch/x86/mm/mmap.c               |  4 ++--
 arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-
 7 files changed, 104 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - March 17, 2017, 5:53 p.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:
<span class="quote">
&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; leads to crashes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; above 47-bit by default.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt; from 47-bit window.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt; address space.</span>
<span class="quote">&gt;</span>

So if I have done a successful mmap which returned &gt; 128TB what should a
following mmap(0,...) return ? Should that now search the *full* address
space or below 128TB ?

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 17, 2017, 5:57 p.m.</div>
<pre class="content">
On Fri, Mar 17, 2017 at 11:23:54PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; &gt; leads to crashes.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt; &gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt; &gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt; &gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt; &gt; from 47-bit window.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt; &gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt; &gt; address space.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So if I have done a successful mmap which returned &gt; 128TB what should a</span>
<span class="quote">&gt; following mmap(0,...) return ? Should that now search the *full* address</span>
<span class="quote">&gt; space or below 128TB ?</span>

No, I don&#39;t think so. And this implementation doesn&#39;t do this.

It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s
better not to switch it automagically to full address space if other part
of the process requested high address.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - March 19, 2017, 8:24 a.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:
<span class="quote">
&gt; On Fri, Mar 17, 2017 at 11:23:54PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt;&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt;&gt; &gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt;&gt; &gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt;&gt; &gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt;&gt; &gt; from 47-bit window.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt;&gt; &gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt;&gt; &gt; address space.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So if I have done a successful mmap which returned &gt; 128TB what should a</span>
<span class="quote">&gt;&gt; following mmap(0,...) return ? Should that now search the *full* address</span>
<span class="quote">&gt;&gt; space or below 128TB ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No, I don&#39;t think so. And this implementation doesn&#39;t do this.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s</span>
<span class="quote">&gt; better not to switch it automagically to full address space if other part</span>
<span class="quote">&gt; of the process requested high address.</span>
<span class="quote">&gt;</span>

What is the epectation when the hint addr is below 128TB but addr + len &gt;
128TB ? Should such mmap request fail ?

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - March 19, 2017, 8:55 a.m.</div>
<pre class="content">
&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; writes:
<span class="quote">
&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Fri, Mar 17, 2017 at 11:23:54PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt;&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt;&gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt;&gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt;&gt;&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt;&gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt;&gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt;&gt;&gt; &gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt;&gt;&gt; &gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt;&gt;&gt; &gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt;&gt;&gt; &gt; from 47-bit window.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt;&gt;&gt; &gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt;&gt;&gt; &gt; address space.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; So if I have done a successful mmap which returned &gt; 128TB what should a</span>
<span class="quote">&gt;&gt;&gt; following mmap(0,...) return ? Should that now search the *full* address</span>
<span class="quote">&gt;&gt;&gt; space or below 128TB ?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; No, I don&#39;t think so. And this implementation doesn&#39;t do this.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s</span>
<span class="quote">&gt;&gt; better not to switch it automagically to full address space if other part</span>
<span class="quote">&gt;&gt; of the process requested high address.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What is the epectation when the hint addr is below 128TB but addr + len &gt;</span>
<span class="quote">&gt; 128TB ? Should such mmap request fail ?</span>

Considering that we have stack at the top (around 128TB) we may not be
able to get a free area for such a request. But I guess the idea here is
that if hint address is below 128TB, we behave as though our TASK_SIZE
is 128TB ? Is that correct ?
 
-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - March 20, 2017, 5:10 a.m.</div>
<pre class="content">
&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:
 @@ -168,6 +182,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,
<span class="quote">&gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; @@ -192,6 +210,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = mm-&gt;mmap_base;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; +</span>

Is this ok for 32 bit application ?
<span class="quote">

&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>


-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=81661">Michael Ellerman</a> - March 20, 2017, 9:15 a.m.</div>
<pre class="content">
&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; writes:
<span class="quote">&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:</span>
<span class="quote">&gt;&gt; On Fri, Mar 17, 2017 at 11:23:54PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt;&gt; So if I have done a successful mmap which returned &gt; 128TB what should a</span>
<span class="quote">&gt;&gt;&gt; following mmap(0,...) return ? Should that now search the *full* address</span>
<span class="quote">&gt;&gt;&gt; space or below 128TB ?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; No, I don&#39;t think so. And this implementation doesn&#39;t do this.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s</span>
<span class="quote">&gt;&gt; better not to switch it automagically to full address space if other part</span>
<span class="quote">&gt;&gt; of the process requested high address.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What is the epectation when the hint addr is below 128TB but addr + len &gt;</span>
<span class="quote">&gt; 128TB ? Should such mmap request fail ?</span>

Yeah I think that makes sense, it retains the existing behaviour unless
the hint itself is &gt;= 128TB.

cheers
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - March 20, 2017, 6:08 p.m.</div>
<pre class="content">
On March 19, 2017 1:26:58 AM PDT, &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; wrote:
<span class="quote">&gt;On Mar 19, 2017 09:25, &quot;Aneesh Kumar K.V&quot;</span>
<span class="quote">&gt;&lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Fri, Mar 17, 2017 at 11:23:54PM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt;&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address</span>
<span class="quote">&gt;space.</span>
<span class="quote">&gt;&gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known</span>
<span class="quote">&gt;that</span>
<span class="quote">&gt;&gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode</span>
<span class="quote">&gt;their</span>
<span class="quote">&gt;&gt;&gt; &gt; information. It collides with valid pointers with 5-level paging</span>
<span class="quote">&gt;and</span>
<span class="quote">&gt;&gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address</span>
<span class="quote">&gt;space</span>
<span class="quote">&gt;&gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt;&gt;&gt; &gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; If hint address set above 47-bit, but MAP_FIXED is not specified,</span>
<span class="quote">&gt;we</span>
<span class="quote">&gt;try</span>
<span class="quote">&gt;&gt;&gt; &gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt;&gt;&gt; &gt; occupied, we look for unmapped area in *full* address space,</span>
<span class="quote">&gt;rather</span>
<span class="quote">&gt;than</span>
<span class="quote">&gt;&gt;&gt; &gt; from 47-bit window.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; This approach helps to easily make application&#39;s memory allocator</span>
<span class="quote">&gt;aware</span>
<span class="quote">&gt;&gt;&gt; &gt; about large address space without manually tracking allocated</span>
<span class="quote">&gt;virtual</span>
<span class="quote">&gt;&gt;&gt; &gt; address space.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So if I have done a successful mmap which returned &gt; 128TB what</span>
<span class="quote">&gt;should a</span>
<span class="quote">&gt;&gt;&gt; following mmap(0,...) return ? Should that now search the *full*</span>
<span class="quote">&gt;address</span>
<span class="quote">&gt;&gt;&gt; space or below 128TB ?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; No, I don&#39;t think so. And this implementation doesn&#39;t do this.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s</span>
<span class="quote">&gt;&gt; better not to switch it automagically to full address space if other</span>
<span class="quote">&gt;part</span>
<span class="quote">&gt;&gt; of the process requested high address.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;What is the epectation when the hint addr is below 128TB but addr + len</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;128TB ? Should such mmap request fail ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Yes, I believe so.</span>

This *better* be conditional on some kind of settable limit.  Having a barrier in the middle of the address space for no apparent reason to &quot;clean&quot; software is insane.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172187">willy@infradead.org</a> - March 20, 2017, 6:38 p.m.</div>
<pre class="content">
On Mon, Mar 20, 2017 at 11:08:41AM -0700, hpa@zytor.com wrote:
<span class="quote">&gt; On March 19, 2017 1:26:58 AM PDT, &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; wrote:</span>
<span class="quote">&gt; &gt;On Mar 19, 2017 09:25, &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; What is the epectation when the hint addr is below 128TB but addr + len</span>
<span class="quote">&gt; &gt; &gt; 128TB ? Should such mmap request fail ?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Yes, I believe so.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This *better* be conditional on some kind of settable limit.  Having a</span>
<span class="quote">&gt; barrier in the middle of the address space for no apparent reason to</span>
<span class="quote">&gt; &quot;clean&quot; software is insane.</span>

I disagree with Kirill here.  If addr+len &gt; 128TB, I think we should
assume the application is 57-bit aware.

Specifying hint addresses is such a rare thing to do anyway.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 24, 2017, 8:59 a.m.</div>
<pre class="content">
On Mon, Mar 20, 2017 at 11:08:41AM -0700, hpa@zytor.com wrote:
<span class="quote">&gt; This *better* be conditional on some kind of settable limit.  Having a</span>
<span class="quote">&gt; barrier in the middle of the address space for no apparent reason to</span>
<span class="quote">&gt; &quot;clean&quot; software is insane.</span>

I had the same argument (on your side) before, but if you look on numbers
it&#39;s far from the middle of address space. The barrier is around 0.2% from
the start 56-bit address space.

And it&#39;s we have vdso/vvar/stack just below the barier anyway.

I don&#39;t think we would loose much if wouldn&#39;t not allow VMA to sit
across it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 24, 2017, 9:03 a.m.</div>
<pre class="content">
On Sun, Mar 19, 2017 at 02:25:08PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; &gt;&gt;&gt; So if I have done a successful mmap which returned &gt; 128TB what should a</span>
<span class="quote">&gt; &gt;&gt;&gt; following mmap(0,...) return ? Should that now search the *full* address</span>
<span class="quote">&gt; &gt;&gt;&gt; space or below 128TB ?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; No, I don&#39;t think so. And this implementation doesn&#39;t do this.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; It&#39;s safer this way: if an library can&#39;t handle high addresses, it&#39;s</span>
<span class="quote">&gt; &gt;&gt; better not to switch it automagically to full address space if other part</span>
<span class="quote">&gt; &gt;&gt; of the process requested high address.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; What is the epectation when the hint addr is below 128TB but addr + len &gt;</span>
<span class="quote">&gt; &gt; 128TB ? Should such mmap request fail ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Considering that we have stack at the top (around 128TB) we may not be</span>
<span class="quote">&gt; able to get a free area for such a request. But I guess the idea here is</span>
<span class="quote">&gt; that if hint address is below 128TB, we behave as though our TASK_SIZE</span>
<span class="quote">&gt; is 128TB ? Is that correct ?</span>

Right.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 24, 2017, 9:04 a.m.</div>
<pre class="content">
On Mon, Mar 20, 2017 at 10:40:20AM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt;  @@ -168,6 +182,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt; &gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; &gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; &gt; +		return addr;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt; &gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt; &gt;  		return -ENOMEM;</span>
<span class="quote">&gt; &gt; @@ -192,6 +210,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt;  	info.high_limit = mm-&gt;mmap_base;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; &gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this ok for 32 bit application ?</span>

DEFAULT_MAP_WINDOW is equal to TASK_SIZE on 32-bit, so it&#39;s nop and will
be compile out.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - March 24, 2017, 9:14 a.m.</div>
<pre class="content">
On Friday 24 March 2017 02:34 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Mar 20, 2017 at 10:40:20AM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt;&gt;  @@ -168,6 +182,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt;&gt;&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt;&gt;&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt;&gt;&gt; +		return addr;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt;&gt;&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;&gt;&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;&gt;&gt; @@ -192,6 +210,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;&gt;&gt;  	info.length = len;</span>
<span class="quote">&gt;&gt;&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;&gt;&gt;  	info.high_limit = mm-&gt;mmap_base;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt;&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt;&gt;&gt; +	 * in the full address space.</span>
<span class="quote">&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;&gt;&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is this ok for 32 bit application ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; DEFAULT_MAP_WINDOW is equal to TASK_SIZE on 32-bit, so it&#39;s nop and will</span>
<span class="quote">&gt; be compile out.</span>
<span class="quote">&gt;</span>

That is not about CONFIG_X86_32 but about 32 bit application on a 64 bit 
kernel. I guess we will never find addr &gt; DEFAULT_MAP_WINDOW with
a 32 bit app ?

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 24, 2017, 9:30 a.m.</div>
<pre class="content">
On Fri, Mar 24, 2017 at 02:44:10PM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Friday 24 March 2017 02:34 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Mon, Mar 20, 2017 at 10:40:20AM +0530, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt; &gt; &gt; &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt; writes:</span>
<span class="quote">&gt; &gt; &gt;  @@ -168,6 +182,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt; &gt; &gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; &gt; &gt; &gt; +		return addr;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt; &gt; &gt; &gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt; &gt; &gt; &gt;  		return -ENOMEM;</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -192,6 +210,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.length = len;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt; &gt; &gt; &gt;  	info.high_limit = mm-&gt;mmap_base;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * in the full address space.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; &gt; &gt; &gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Is this ok for 32 bit application ?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; DEFAULT_MAP_WINDOW is equal to TASK_SIZE on 32-bit, so it&#39;s nop and will</span>
<span class="quote">&gt; &gt; be compile out.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That is not about CONFIG_X86_32 but about 32 bit application on a 64 bit</span>
<span class="quote">&gt; kernel. I guess we will never find addr &gt; DEFAULT_MAP_WINDOW with</span>
<span class="quote">&gt; a 32 bit app ?</span>

I have local change to avoid this within 32-bit syscall, but I&#39;ll need to
recheck everthing.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index 9d49c18b5ea9..265625b0d6cb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(DEFAULT_MAP_WINDOW / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_header">diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="p_header">index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mpx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mpx.h</span>
<span class="p_chunk">@@ -73,6 +73,9 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 }
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
 		      unsigned long start, unsigned long end);
<span class="p_add">+</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags);</span>
 #else
 static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)
 {
<span class="p_chunk">@@ -94,6 +97,12 @@</span> <span class="p_context"> static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
 				    unsigned long start, unsigned long end)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="p_add">+		unsigned long len, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_X86_INTEL_MPX */
 
 #endif /* _ASM_X86_MPX_H */
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index f385eca5407a..da8ab4f2d0c7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -799,6 +799,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  */
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
<span class="p_add">+#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
<span class="p_chunk">@@ -838,7 +839,9 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
<span class="p_del">-#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="p_add">+#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFAULT_MAP_WINDOW	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
<span class="p_chunk">@@ -851,7 +854,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		DEFAULT_MAP_WINDOW</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -873,7 +876,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
  */
<span class="p_del">-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 3))</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(DEFAULT_MAP_WINDOW / 3))</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 50215a4b9347..bae3706130a6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/ia32.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 /*
  * Align a virtual address to avoid aliasing in the I$ on AMD F15h.
<span class="p_chunk">@@ -129,6 +130,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	struct vm_unmapped_area_info info;
 	unsigned long begin, end;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (flags &amp; MAP_FIXED)
 		return addr;
 
<span class="p_chunk">@@ -148,7 +153,16 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = begin;
<span class="p_del">-	info.high_limit = end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = min(end, TASK_SIZE);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_chunk">@@ -168,6 +182,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
<span class="p_chunk">@@ -192,6 +210,14 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = mm-&gt;mmap_base;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index c5066a260803..94f41a39d8fe 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlb.h&gt;
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/pgalloc.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 #if 0	/* This is just for testing */
 struct page *
<span class="p_chunk">@@ -83,24 +84,41 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = current-&gt;mm-&gt;mmap_legacy_base;
<span class="p_del">-	info.high_limit = TASK_SIZE;</span>
<span class="p_add">+	info.high_limit = DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		info.high_limit = DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
 }
 
 static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,
<span class="p_del">-		unsigned long addr0, unsigned long len,</span>
<span class="p_add">+		unsigned long addr, unsigned long len,</span>
 		unsigned long pgoff, unsigned long flags)
 {
 	struct hstate *h = hstate_file(file);
 	struct vm_unmapped_area_info info;
<span class="p_del">-	unsigned long addr;</span>
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = current-&gt;mm-&gt;mmap_base;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -115,7 +133,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = DEFAULT_MAP_WINDOW;</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -132,6 +150,11 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_add">+</span>
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index 7940166c799b..2fbfcabd098a 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> static unsigned long stack_maxrandom_size(void)</span>
  * Leave an at least ~128 MB hole with possible stack randomization.
  */
 #define MIN_GAP (128*1024*1024UL + stack_maxrandom_size())
<span class="p_del">-#define MAX_GAP (TASK_SIZE/6*5)</span>
<span class="p_add">+#define MAX_GAP (DEFAULT_MAP_WINDOW/6*5)</span>
 
 static int mmap_is_legacy(void)
 {
<span class="p_chunk">@@ -91,7 +91,7 @@</span> <span class="p_context"> static unsigned long mmap_base(unsigned long rnd)</span>
 	else if (gap &gt; MAX_GAP)
 		gap = MAX_GAP;
 
<span class="p_del">-	return PAGE_ALIGN(TASK_SIZE - gap - rnd);</span>
<span class="p_add">+	return PAGE_ALIGN(DEFAULT_MAP_WINDOW - gap - rnd);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index 5126dfd52b18..cc318817ce7c 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -355,10 +355,19 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 */
 	bd_base = mpx_get_bounds_dir();
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+</span>
<span class="p_add">+	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="p_add">+	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="p_add">+		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="p_add">+				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="p_add">+				current-&gt;comm, current-&gt;pid);</span>
<span class="p_add">+		ret = -ENXIO;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
<span class="p_del">-</span>
<span class="p_add">+out:</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -1038,3 +1047,25 @@</span> <span class="p_context"> void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (ret)
 		force_sig(SIGSEGV, current);
 }
<span class="p_add">+</span>
<span class="p_add">+/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (flags &amp; MAP_FIXED)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="p_add">+	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



