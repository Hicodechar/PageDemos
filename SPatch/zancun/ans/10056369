
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm: show stats for non-default hugepage sizes in /proc/meminfo - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm: show stats for non-default hugepage sizes in /proc/meminfo</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 13, 2017, 4:03 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171113160302.14409-1-guro@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10056369/mbox/"
   >mbox</a>
|
   <a href="/patch/10056369/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10056369/">/patch/10056369/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	442C560586 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 16:05:01 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 31E3128E34
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 16:05:01 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2343E28EC5; Mon, 13 Nov 2017 16:05:01 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4591528CA6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Nov 2017 16:05:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753517AbdKMQE6 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Nov 2017 11:04:58 -0500
Received: from mx0b-00082601.pphosted.com ([67.231.153.30]:55196 &quot;EHLO
	mx0a-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1753209AbdKMQEz (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Nov 2017 11:04:55 -0500
Received: from pps.filterd (m0001303.ppops.net [127.0.0.1])
	by m0001303.ppops.net (8.16.0.21/8.16.0.21) with SMTP id
	vADG2FIv004899; Mon, 13 Nov 2017 08:03:31 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : mime-version : content-type; s=facebook;
	bh=D6EfITCs6lw8l0TIwf8JF/Ph/1V6D5hz3SGuX0q75pQ=;
	b=K/eE2rVU/OnyENCUF6gZz9c42pICHGCLaFBmv2Jk3gRzu9XwdTUzFpDdcj1engc3WqrA
	dR0LY3grOdaldbN+U8IKJ7mO9E8qpd/4kE2fAIQsROAzYmdULIjh6V3t64Tr/sWNu7Rb
	Ic57EYb871/M4Th5uWVcAe5bfwKO2DkraCY= 
Received: from maileast.thefacebook.com ([199.201.65.23])
	by m0001303.ppops.net with ESMTP id 2e6tjq2ee1-1
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT);
	Mon, 13 Nov 2017 08:03:31 -0800
Received: from NAM02-CY1-obe.outbound.protection.outlook.com (192.168.183.28)
	by o365-in.thefacebook.com (192.168.177.32) with Microsoft SMTP
	Server (TLS) id 14.3.361.1; Mon, 13 Nov 2017 11:03:30 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.onmicrosoft.com; 
	s=selector1-fb-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version; 
	bh=D6EfITCs6lw8l0TIwf8JF/Ph/1V6D5hz3SGuX0q75pQ=;
	b=aBgdWdqUYCMySBBPUsT7KIitCaFx8DLwGkbLWsebMW5/OsHRo7OV4ERdvtvuAjgvQdgxQL7i/jLXMA7HwbsONNdUM02eRYs+rM3PVzTNPYPB3+9H911WAbKPVrNa+aOcjOGqW4VXQdXDMpNWfvwlQkTtEM7XzapPL99vztyOqto=
Received: from castle.thefacebook.com (2620:10d:c092:200::1:dd3e) by
	BL2PR15MB1075.namprd15.prod.outlook.com (2603:10b6:201:17::9) with
	Microsoft SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256) id 15.20.218.12;
	Mon, 13 Nov 2017 16:03:25 +0000
From: Roman Gushchin &lt;guro@fb.com&gt;
To: &lt;linux-mm@kvack.org&gt;
CC: Roman Gushchin &lt;guro@fb.com&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, &lt;kernel-team@fb.com&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;
Subject: [PATCH] mm: show stats for non-default hugepage sizes in
	/proc/meminfo
Date: Mon, 13 Nov 2017 16:03:02 +0000
Message-ID: &lt;20171113160302.14409-1-guro@fb.com&gt;
X-Mailer: git-send-email 2.13.6
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [2620:10d:c092:200::1:dd3e]
X-ClientProxiedBy: DB6P18901CA0002.EURP189.PROD.OUTLOOK.COM
	(2603:10a6:4:16::12) To BL2PR15MB1075.namprd15.prod.outlook.com
	(2603:10b6:201:17::9)
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: 434ed77e-83c9-4c51-6fba-08d52ab01356
X-Microsoft-Antispam: UriScan:; BCL:0; PCL:0;
	RULEID:(22001)(4534020)(4602075)(4627115)(201703031133081)(201702281549075)(2017052603199);
	SRVR:BL2PR15MB1075; 
X-Microsoft-Exchange-Diagnostics: 1; BL2PR15MB1075;
	3:F/wQRjXfyKGuIM3Zfq7WuFCej/mH2wNT8tZLou8D9tCYB/w6NKtoN7Ffnti3oIpzt/w1XmNZBDz/WMTX8pjrUB50GNxlba9U1tz5OirhsdAoHsh/JS9JqtEFD0bISxZm+TOwnAZ8gK7n/DZMpHVL/LiVCoIAMu+9zoFj5gJfaXh3mnM2OENgTKQIlIcIW6/oEPEfKXo6XehG+rYZuyYxI0OjzpRWSzTKHkl2mT3U6mI5sE5tSvI0xZrSdG5Wnhn2;
	25:CVqZOSJed3WIRc3oJ8UM8AWc9hcjv0I+Q08YQC9C1+t4R8UYU3SpPr2NcyVxGhdGJjqE4GlBgCgjHCnBUk9mCsWzz5CkAjl47MAMZAGM5jeeIEnJDEgayo4leB3tyMG1qG3TTXaRZlnTCmdXlm5DI2rAUmbkIhf3rz6BEONsZBo7cnRazKEJphv0VHWMIQpvD+rdoZ5CX4zjNYOezmmkuCOA7/ROaIJiBgVXOLa/PptNd348X7lqZDiuxRqsAsIP2RXAvD0r4su1Oxv8L1RsYDyWY7sMkLLBvALmotzJyNbzatJa6PhFv3wlkNDW3oEtMVIWhWfpx6fnQbrLnyZrFg==;
	31:k/+9PBuLe5Ay6RvnmYfi/V0QhwKUqi4EgpcC/vFRAOG9jVRpdo/YcARM5awm7i/Z42CwJMfTpxTwK3ctn8rvjJkG2upancKXZnWvyGnpBzkAx/VisSbnhVCZLu1n6b5olvj/L8MSUlYHCgCzDNkizZsw2T1tLC4D98nW9pgz96QkVfIO2ft0qY8YQ7WFFI9aeowv5iUrLMUUohqJNlfQGu81bwyijhGaH7lxi5jna98=
X-MS-TrafficTypeDiagnostic: BL2PR15MB1075:
X-Microsoft-Exchange-Diagnostics: 1; BL2PR15MB1075;
	20:0ewst3ze6OxZ4+scLYDrCeAqF229i8G7DYZvxh7pv1MH7emGi/H+2AjvSwDcJSsA+a2S2VaE7dbhdiCK5eN/9B/fQePNnzDZ6APBVhypRVJFP8DMdQLY+JvV97kGEmjVa7EGP7lXyZoabKM8KbunkYXN9a2hUtc4cVKRgHvQGIFzSQDfjAf2qNVoXgjntU6N+UDbCwI5rjotHkQhmPekCivSDdpCeApKO0t4DK845xcet/ySYkKIRUYqq4r1YGcZNGC3RSTaBqx2YPd77T2SRGGltJ/txvypfNPY/HGbR3vEUD1sFU2Dd9wP/IQkCZNJB5KcEYwCOH2gH5lldUY5hegs80m+DqBjYcnWnm1apwBQMYY5S8UGJlvf8Jj2NFnG9trRELVeV1xvjNLKyVYPwhpUWFlBriuTlYyK2+P77YnYsfryKAP3mYmm3N0bZG9P99s7qPgfImwc/9upC2bnj3BQ+cPCr5wlAfWLRT4ynnO7i14IBy9oase4gsEQA+9F;
	4:xyHlt0AeJM0NlKpR4ii/UyqDO+jBtTGekwHw6XadrWHV5RxDwWHYQohbvODk55FASbYO+cYkt+/9xT0gthVGtrRaHS4JMjhRjYQDOwe4dBWKl/knjgt2ZpAN8R5CsaS4fdNB93J7NZ+jSWfq+q81LreFspdZ4Z/x96REX0OLdBYj3KGRMj4/cTkmZD/Wdopn26Sxpe4OSqBkOnrNIQ9rN5v8dG46VMiNnrWBolrKHlalDvIOwwpXPO3L/XcQV6+C1Z1OXoUhNdvtHh/X/lMnW1WPvf5Xr3+r+QTZRz7jhKrxg3UkdPPblxhse8153bKgHbUoXI8rgRsQgoOs/1/OLc3i3Ql2G/byTMIukdQ27Hjv0GL0R8rTovFu8m1SPSu4T7HOm+P6CFngh3eKyH7k8C4Ztl5zZX7kfwid+b
	yqZ+o=
X-Microsoft-Antispam-PRVS: &lt;BL2PR15MB1075E4D1EBD8595FF534ADE3BE2B0@BL2PR15MB1075.namprd15.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(9452136761055)(67672495146484)(104084551191319)(146099531331640);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(100000700101)(100105000095)(100000701101)(100105300095)(100000702101)(100105100095)(11241501159)(6040450)(2401047)(5005006)(8121501046)(100000703101)(100105400095)(93006095)(93001095)(3002001)(10201501046)(3231022)(6041248)(20161123564025)(20161123562025)(20161123558100)(20161123560025)(20161123555025)(201703131423075)(201702281528075)(201703061421075)(201703061406153)(6072148)(201708071742011)(100000704101)(100105200095)(100000705101)(100105500095);
	SRVR:BL2PR15MB1075; BCL:0; PCL:0;
	RULEID:(100000800101)(100110000095)(100000801101)(100110300095)(100000802101)(100110100095)(100000803101)(100110400095)(100000804101)(100110200095)(100000805101)(100110500095);
	SRVR:BL2PR15MB1075; 
X-Forefront-PRVS: 0490BBA1F0
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10019020)(6009001)(376002)(346002)(199003)(189002)(2361001)(5003940100001)(478600001)(106356001)(6506006)(4326008)(47776003)(1076002)(2906002)(105586002)(2351001)(6512007)(33646002)(6666003)(6116002)(6916009)(53416004)(101416001)(189998001)(68736007)(53936002)(6486002)(50986999)(50466002)(69596002)(97736004)(86362001)(54906003)(81166006)(50226002)(5660300001)(16586007)(316002)(81156014)(305945005)(8676002)(7736002)(36756003)(8936002)(25786009)(48376002)(42262002);
	DIR:OUT; SFP:1102; SCL:1; SRVR:BL2PR15MB1075;
	H:castle.thefacebook.com; FPR:; SPF:None; PTR:InfoNoRecords;
	A:1; MX:1; LANG:en; 
Received-SPF: None (protection.outlook.com: fb.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; BL2PR15MB1075;
	23:RcHgSaMs68gJpGSGHmn67CGnpfjR8nPsIymF1vGQo?=
	=?us-ascii?Q?g5SmV/1S2pTcwA9fRUyMHo3h5PbzcMgnr6FtsaYImsswFu2huZKgpXxeGjTB?=
	=?us-ascii?Q?ZfNS4a0WQJUw96R1jaz5oSOWs6ITst+fhRPbNkHHhQq8E0gWDZACNRzpUsU4?=
	=?us-ascii?Q?4pSuth5iGqJQe4WaspSz3DneIwL6SXT3SGK9ydMmAlm6sCdwKEwmnxWVdeQt?=
	=?us-ascii?Q?nA3WlD3Jg4Im/mrA8aBkRYZ3zjLb3S9/cTXnRUWkcUEcqSvjI0Zn5hK7wc81?=
	=?us-ascii?Q?L4BtH/p4P1uhFZJ5QHg2OJMiT1mxQIP+Y8xnG8cAQFGmpCpxEs/Ef43olKTH?=
	=?us-ascii?Q?BUu95YIhY7/5k9z52/tcycrYxS4XdtytUBOF4I0PdP6Cypt5YVZkZYTOE8se?=
	=?us-ascii?Q?0/M/8y6t0rHLFf7w7zqPmSPKC4FM+rRDrUTgE8u330uFCbPr+NCO/N8jjnHU?=
	=?us-ascii?Q?gGE6XpPqWSrDi9zkx14e6jrSUVmK1DVXk6DrjeBmuhEsc9N7upLCh3WUhe8X?=
	=?us-ascii?Q?vIZlqEPZJjUU0bxcMekq2rCeTE0knzBznA5uvSFeOLsb3hKvy9Cw2pzuh2vv?=
	=?us-ascii?Q?60L6mBRwO2+uT7Io7SQszwHqqvXkD27oJl/TgvdhZ45BWxWGPEmL9WRWwFYA?=
	=?us-ascii?Q?WXPnkCa4fE9Prbz9ziihDQGS2kA5EGgM4T6RL+K3KV8/i6yi4JcMMMaBV5bV?=
	=?us-ascii?Q?S/hUIP+fC5ulIQIaTUnmLidcDGpoZXfwMmmty/wN2+M8x9r/K5Eb5P2LN6oD?=
	=?us-ascii?Q?VOtMtY3Vw/KRxD1VIJpO/0LbtJR+fu8YtbJrIJa40NQ0+Q9v2KboZ8QBGAc+?=
	=?us-ascii?Q?JF+/eBO5JKecu+Zc9DDDIkp2aWR6wQ/+CTCdqxHfNsNAzDttgqvVGekSNhpm?=
	=?us-ascii?Q?eqTpNvzF29C5PRLGeoYASxaz75qlLzrbctwNo9iauHOE4ynNO9JdE3l8rehb?=
	=?us-ascii?Q?UDxGglTft0tXUWBcylUArHNNbwDp+pG6ZxOIApnK9RCsMHhkndCLCF/WFttv?=
	=?us-ascii?Q?E5bYsH+8JoIbCmB+cBnc9wf5Tv9ANVBJBNzCBvCHJZ7R6lLs0yvV/CT4ypOt?=
	=?us-ascii?Q?Jm593NKTFvwP6UPEa8QZD5GPbjxr50aSsyXVSZJcPc1CmPkpw=3D=3D?=
X-Microsoft-Exchange-Diagnostics: 1; BL2PR15MB1075;
	6:DuQ4Voh3eMZBgqvEpPoOwtEFKe2h0t/HmJ49Stg2Gf1IPXXYN3/4hvHgB7iR+Q44HXRcwZHbFDdArzKUmFkOlGNjGol8Ci5aCNssaqI+VF3p5pbHCipnfPJJ135xyvmd/JpJNijLQyRZ8UuaxuJhNKIqoNuCVZ3B4Q1q8lJbrLq0Hd9a4e4sqgPc7FASSVuDvKq87i6QIhwk1gfFEnYlEqJqfZJ7wMi2nt8PBQoXrYAMnnPyxEWvFKebe3lt9svnqiyDYs/vBAwrNDnPr2GXyelIlCUsTooxJyO8hrjbhj2Tbzv4uOGTnTaf38Bgv9IGePN+9gZ1bZv2wx7x1xiKO7OMAg31wHtrkEohvZZQKT8=;
	5:5Kikc3qcsc4U2H/ZRR2gumqqrXjL0KeFRnM/LWtbZH/krlWjCCq02YhO/LGoaleDbJ9qu4wImnHRsum0wsLpvQQ/+Zx8zOHe/F06qaVHAN7v5LGeFhOrdV7l9Lt5W36rBwU4PYKOxOhabF6tdQjcsNXKr/5YNwsJdnD2B0nfo5c=;
	24:6GnemZjvx7Qu9jFLmDBTN2lDp06FliFIFWWc9bEZ/xWazvVIUZnqZxY8NtiO4ylFJRrTausCFe6ZwzmjrsfExYSJL60vH4ptAsnhIKGPwNI=;
	7:QYfjk6X8YIeaLjHDOXxU7do8nPL0U0NTsgSO370U40VbIOWpS+LKZBH2ra0L6i1tBIuVJODspN6vcHVJMXNuzWvlg95P72/juIrSagsV96IzYo9mYr9YqYZQwhRIZu6lwDOlTWAkQn+Qlag9eXm4gjHDirgf9urJHmlmeYlMomD6MUSqw/kYRZZPz82Yx6kG6bSMzGePKIA9T+Tk7/JVrzWnoqnoWc1R04eV7f/9axsYRnXP/b+WPbuKdizpMBgb
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; BL2PR15MB1075;
	20:e33000AHGeq2fYcUMdV0mR0zp4S86EzUjKBlmJ3hlpZrHqxvgswmw7Gf3N7PeGC0PnZxs0iD4Oi9Kt459+5/Tr3UWfTH4jAX0JuRL4L3P5cgsYkwI0QyyApKFZs0agxbwZevw9pIqzMwcoDbQaoUv+sKssj9OYsdYsU9OdatIxc=
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 13 Nov 2017 16:03:25.7178
	(UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 434ed77e-83c9-4c51-6fba-08d52ab01356
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-CrossTenant-Id: 8ae927fe-1255-47a7-a2af-5f3a069daaa2
X-MS-Exchange-Transport-CrossTenantHeadersStamped: BL2PR15MB1075
X-OriginatorOrg: fb.com
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-11-13_08:, , signatures=0
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 13, 2017, 4:03 p.m.</div>
<pre class="content">
Currently we display some hugepage statistics (total, free, etc)
in /proc/meminfo, but only for default hugepage size (e.g. 2Mb).

If hugepages of different sizes are used (like 2Mb and 1Gb on x86-64),
/proc/meminfo output can be confusing, as non-default sized hugepages
are not reflected at all, and there are no signs that they are
existing and consuming system memory.

To solve this problem, let&#39;s display stats for all hugepage sizes.
To provide the backward compatibility let&#39;s save the existing format
for the default size, and add a prefix (e.g. 1G_) for non-default sizes.

For example (100 2Mb pages and 2 1Gb pages are pre-allocated):
  $ cat /proc/meminfo
  MemTotal:        8168976 kB
  MemFree:         5664792 kB
  &lt;...&gt;
  CmaFree:               0 kB
  HugePages_1G_Total:       2
  HugePages_1G_Free:        2
  HugePages_1G_Rsvd:        0
  HugePages_1G_Surp:        0
  Hugepagesize_1G:    1048576 kB
  HugePages_Total:     100
  HugePages_Free:      100
  HugePages_Rsvd:        0
  HugePages_Surp:        0
  Hugepagesize:       2048 kB
  DirectMap4k:       30584 kB
  DirectMap2M:     3115008 kB
  DirectMap1G:     7340032 kB
<span class="signed-off-by">
Signed-off-by: Roman Gushchin &lt;guro@fb.com&gt;</span>
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: kernel-team@fb.com
Cc: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org
---
 mm/hugetlb.c | 49 +++++++++++++++++++++++++++++++++++++------------
 1 file changed, 37 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 13, 2017, 4:11 p.m.</div>
<pre class="content">
On Mon 13-11-17 16:03:02, Roman Gushchin wrote:
<span class="quote">&gt; Currently we display some hugepage statistics (total, free, etc)</span>
<span class="quote">&gt; in /proc/meminfo, but only for default hugepage size (e.g. 2Mb).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If hugepages of different sizes are used (like 2Mb and 1Gb on x86-64),</span>
<span class="quote">&gt; /proc/meminfo output can be confusing, as non-default sized hugepages</span>
<span class="quote">&gt; are not reflected at all, and there are no signs that they are</span>
<span class="quote">&gt; existing and consuming system memory.</span>

Yes this sucks but we do have per numa node per h-state stats in sysfs
already /sys/devices/system/node/node*/hugepages

I know it is another source of the information but is there any reason
you cannot use it?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 13, 2017, 4:33 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 05:11:02PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Mon 13-11-17 16:03:02, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt; Currently we display some hugepage statistics (total, free, etc)</span>
<span class="quote">&gt; &gt; in /proc/meminfo, but only for default hugepage size (e.g. 2Mb).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If hugepages of different sizes are used (like 2Mb and 1Gb on x86-64),</span>
<span class="quote">&gt; &gt; /proc/meminfo output can be confusing, as non-default sized hugepages</span>
<span class="quote">&gt; &gt; are not reflected at all, and there are no signs that they are</span>
<span class="quote">&gt; &gt; existing and consuming system memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes this sucks but we do have per numa node per h-state stats in sysfs</span>
<span class="quote">&gt; already /sys/devices/system/node/node*/hugepages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I know it is another source of the information but is there any reason</span>
<span class="quote">&gt; you cannot use it?</span>

Hi, Michal!

In my case, I didn&#39;t know in advance, that hugetlb pages are preallocated,
and spent some time trying to find &quot;magically disappeared&quot; several Gb of memory,
which are not reflected in any /proc/[meminfo,vmstat] counters.

IMO, /proc/meminfo should give a user a high-level overview of memory usage
in the system, without a need to look into other places. Of course, we always
have some amount of unaccounted memory, but it shouldn&#39;t be measured in Gb,
as in this case.

If you&#39;re worried about adding counters which will be 0 most of the time
for most users, we can print them conditionally, only if total number of
corresponding pages is not 0.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Nov. 13, 2017, 5:06 p.m.</div>
<pre class="content">
On 11/13/2017 08:03 AM, Roman Gushchin wrote:
<span class="quote">&gt; To solve this problem, let&#39;s display stats for all hugepage sizes.</span>
<span class="quote">&gt; To provide the backward compatibility let&#39;s save the existing format</span>
<span class="quote">&gt; for the default size, and add a prefix (e.g. 1G_) for non-default sizes.</span>

Is there something keeping you from using the sysfs version of this
information?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 13, 2017, 6:11 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 09:06:32AM -0800, Dave Hansen wrote:
<span class="quote">&gt; On 11/13/2017 08:03 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt; To solve this problem, let&#39;s display stats for all hugepage sizes.</span>
<span class="quote">&gt; &gt; To provide the backward compatibility let&#39;s save the existing format</span>
<span class="quote">&gt; &gt; for the default size, and add a prefix (e.g. 1G_) for non-default sizes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is there something keeping you from using the sysfs version of this</span>
<span class="quote">&gt; information?</span>

Just answered the same question to Michal.

In two words: it would be nice to have a high-level overview of
memory usage in the system in /proc/meminfo. 

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Nov. 13, 2017, 6:17 p.m.</div>
<pre class="content">
On 11/13/2017 10:11 AM, Roman Gushchin wrote:
<span class="quote">&gt; On Mon, Nov 13, 2017 at 09:06:32AM -0800, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt; On 11/13/2017 08:03 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt;&gt;&gt; To solve this problem, let&#39;s display stats for all hugepage sizes.</span>
<span class="quote">&gt;&gt;&gt; To provide the backward compatibility let&#39;s save the existing format</span>
<span class="quote">&gt;&gt;&gt; for the default size, and add a prefix (e.g. 1G_) for non-default sizes.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is there something keeping you from using the sysfs version of this</span>
<span class="quote">&gt;&gt; information?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just answered the same question to Michal.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In two words: it would be nice to have a high-level overview of</span>
<span class="quote">&gt; memory usage in the system in /proc/meminfo. </span>

I don&#39;t think it&#39;s worth cluttering up meminfo for this, imnho.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Nov. 13, 2017, 6:30 p.m.</div>
<pre class="content">
On 11/13/2017 10:17 AM, Dave Hansen wrote:
<span class="quote">&gt; On 11/13/2017 10:11 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt;&gt; On Mon, Nov 13, 2017 at 09:06:32AM -0800, Dave Hansen wrote:</span>
<span class="quote">&gt;&gt;&gt; On 11/13/2017 08:03 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; To solve this problem, let&#39;s display stats for all hugepage sizes.</span>
<span class="quote">&gt;&gt;&gt;&gt; To provide the backward compatibility let&#39;s save the existing format</span>
<span class="quote">&gt;&gt;&gt;&gt; for the default size, and add a prefix (e.g. 1G_) for non-default sizes.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Is there something keeping you from using the sysfs version of this</span>
<span class="quote">&gt;&gt;&gt; information?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Just answered the same question to Michal.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In two words: it would be nice to have a high-level overview of</span>
<span class="quote">&gt;&gt; memory usage in the system in /proc/meminfo. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t think it&#39;s worth cluttering up meminfo for this, imnho.</span>

I tend to agree that it would be better not to add additional huge page
sizes here.  It may not seem too intrusive to (potentially) add one extra
set of entries for GB huge pages on x86.  However, other architectures
such as powerpc or sparc have several several huge pages sizes that could
potentially be added here as well.  Although, in practice one does tend
to use a single huge pages size.  If you change the default huge page
size, then those entries will be in /proc/meminfo.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 13, 2017, 6:45 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 10:30:10AM -0800, Mike Kravetz wrote:
<span class="quote">&gt; On 11/13/2017 10:17 AM, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt; On 11/13/2017 10:11 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt;&gt; On Mon, Nov 13, 2017 at 09:06:32AM -0800, Dave Hansen wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; On 11/13/2017 08:03 AM, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; To solve this problem, let&#39;s display stats for all hugepage sizes.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; To provide the backward compatibility let&#39;s save the existing format</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; for the default size, and add a prefix (e.g. 1G_) for non-default sizes.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Is there something keeping you from using the sysfs version of this</span>
<span class="quote">&gt; &gt;&gt;&gt; information?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Just answered the same question to Michal.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; In two words: it would be nice to have a high-level overview of</span>
<span class="quote">&gt; &gt;&gt; memory usage in the system in /proc/meminfo. </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t think it&#39;s worth cluttering up meminfo for this, imnho.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I tend to agree that it would be better not to add additional huge page</span>
<span class="quote">&gt; sizes here.  It may not seem too intrusive to (potentially) add one extra</span>
<span class="quote">&gt; set of entries for GB huge pages on x86.  However, other architectures</span>
<span class="quote">&gt; such as powerpc or sparc have several several huge pages sizes that could</span>
<span class="quote">&gt; potentially be added here as well.  Although, in practice one does tend</span>
<span class="quote">&gt; to use a single huge pages size.  If you change the default huge page</span>
<span class="quote">&gt; size, then those entries will be in /proc/meminfo.</span>

I do agree that it might add some unnecessary verbosity if these sizes
are not used, but if they are, this information is super-useful.
So, might be a conditional printing will work here?

Or, at least, some total counter, e.g. how much memory is consumed
by hugetlb pages?

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Nov. 13, 2017, 7:10 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 06:45:01PM +0000, Roman Gushchin wrote:
<span class="quote">&gt; Or, at least, some total counter, e.g. how much memory is consumed</span>
<span class="quote">&gt; by hugetlb pages?</span>

I&#39;m not a big fan of the verbose breakdown for every huge page size.
As others have pointed out such detail exists elswhere.

But I do think we should have a summary counter for memory consumed by
hugetlb that lets you know how much is missing from MemTotal. This can
be large parts of overall memory, and right now /proc/meminfo will
give the impression we are leaking those pages.

Maybe a simple summary counter for everything set aside by the hugetlb
subsystem - default and non-default page sizes, whether they&#39;re used
or only reserved etc.?

Hugetlb 12345 kB
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Nov. 13, 2017, 7:25 p.m.</div>
<pre class="content">
On 11/13/2017 11:10 AM, Johannes Weiner wrote:
<span class="quote">&gt; On Mon, Nov 13, 2017 at 06:45:01PM +0000, Roman Gushchin wrote:</span>
<span class="quote">&gt;&gt; Or, at least, some total counter, e.g. how much memory is consumed</span>
<span class="quote">&gt;&gt; by hugetlb pages?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not a big fan of the verbose breakdown for every huge page size.</span>
<span class="quote">&gt; As others have pointed out such detail exists elswhere.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But I do think we should have a summary counter for memory consumed by</span>
<span class="quote">&gt; hugetlb that lets you know how much is missing from MemTotal. This can</span>
<span class="quote">&gt; be large parts of overall memory, and right now /proc/meminfo will</span>
<span class="quote">&gt; give the impression we are leaking those pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe a simple summary counter for everything set aside by the hugetlb</span>
<span class="quote">&gt; subsystem - default and non-default page sizes, whether they&#39;re used</span>
<span class="quote">&gt; or only reserved etc.?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hugetlb 12345 kB</span>

I would prefer this approach.  The &#39;trick&#39; is coming up with a name or
description that is not confusing.  Unfortunately, we have to leave the
existing entries.  So, this new entry will be greater than or equal to
HugePages_Total. :(  I guess Hugetlb is as good of a name as any?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Nov. 13, 2017, 7:31 p.m.</div>
<pre class="content">
On 11/13/2017 11:10 AM, Johannes Weiner wrote:
<span class="quote">&gt; Maybe a simple summary counter for everything set aside by the hugetlb</span>
<span class="quote">&gt; subsystem - default and non-default page sizes, whether they&#39;re used</span>
<span class="quote">&gt; or only reserved etc.?</span>

Yeah, one line is a lot more sane than 5 lines times all the extra
sizes.  It&#39;ll just be a matter of bikeshedding the name and whether it
should include the default pages being consumed or not.  I vote for:

	Hugetlb: &quot;/sysfs FTW!&quot; kB
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 14, 2017, 8:21 a.m.</div>
<pre class="content">
On Mon 13-11-17 16:33:05, Roman Gushchin wrote:
[...]
<span class="quote">&gt; IMO, /proc/meminfo should give a user a high-level overview of memory usage</span>
<span class="quote">&gt; in the system, without a need to look into other places. Of course, we always</span>
<span class="quote">&gt; have some amount of unaccounted memory, but it shouldn&#39;t be measured in Gb,</span>
<span class="quote">&gt; as in this case.</span>

Well, this is not so easy. There can be _a lot_ of unaccounted memory.
Gbs is not something unheard of (fs metadata directly allocated by the
page allocator, network buffers, you name it). Unlike those the hugetlb
requires an explicit admin interaction. Especially for the non-default
hugetlb page sizes.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 14, 2017, 12:48 p.m.</div>
<pre class="content">
On Mon, Nov 13, 2017 at 11:25:21AM -0800, Mike Kravetz wrote:
<span class="quote">&gt; On 11/13/2017 11:10 AM, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Mon, Nov 13, 2017 at 06:45:01PM +0000, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt;&gt; Or, at least, some total counter, e.g. how much memory is consumed</span>
<span class="quote">&gt; &gt;&gt; by hugetlb pages?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;m not a big fan of the verbose breakdown for every huge page size.</span>
<span class="quote">&gt; &gt; As others have pointed out such detail exists elswhere.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But I do think we should have a summary counter for memory consumed by</span>
<span class="quote">&gt; &gt; hugetlb that lets you know how much is missing from MemTotal. This can</span>
<span class="quote">&gt; &gt; be large parts of overall memory, and right now /proc/meminfo will</span>
<span class="quote">&gt; &gt; give the impression we are leaking those pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Maybe a simple summary counter for everything set aside by the hugetlb</span>
<span class="quote">&gt; &gt; subsystem - default and non-default page sizes, whether they&#39;re used</span>
<span class="quote">&gt; &gt; or only reserved etc.?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hugetlb 12345 kB</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would prefer this approach.  The &#39;trick&#39; is coming up with a name or</span>
<span class="quote">&gt; description that is not confusing.  Unfortunately, we have to leave the</span>
<span class="quote">&gt; existing entries.  So, this new entry will be greater than or equal to</span>
<span class="quote">&gt; HugePages_Total. :(  I guess Hugetlb is as good of a name as any?</span>

Yes, I like this approach too, and Hugetlb (in kB) sounds reasonable.
I&#39;ll post a new patch soon.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 4b3bbd2980bb..abd37999f5da 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -2973,20 +2973,45 @@</span> <span class="p_context"> int hugetlb_overcommit_handler(struct ctl_table *table, int write,</span>
 
 void hugetlb_report_meminfo(struct seq_file *m)
 {
<span class="p_del">-	struct hstate *h = &amp;default_hstate;</span>
<span class="p_add">+	struct hstate *h;</span>
<span class="p_add">+</span>
 	if (!hugepages_supported())
 		return;
<span class="p_del">-	seq_printf(m,</span>
<span class="p_del">-			&quot;HugePages_Total:   %5lu\n&quot;</span>
<span class="p_del">-			&quot;HugePages_Free:    %5lu\n&quot;</span>
<span class="p_del">-			&quot;HugePages_Rsvd:    %5lu\n&quot;</span>
<span class="p_del">-			&quot;HugePages_Surp:    %5lu\n&quot;</span>
<span class="p_del">-			&quot;Hugepagesize:   %8lu kB\n&quot;,</span>
<span class="p_del">-			h-&gt;nr_huge_pages,</span>
<span class="p_del">-			h-&gt;free_huge_pages,</span>
<span class="p_del">-			h-&gt;resv_huge_pages,</span>
<span class="p_del">-			h-&gt;surplus_huge_pages,</span>
<span class="p_del">-			1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_hstate(h) {</span>
<span class="p_add">+		char prefix[16] = &quot;&quot;;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (h != &amp;default_hstate) {</span>
<span class="p_add">+			unsigned int order = huge_page_order(h) + PAGE_SHIFT;</span>
<span class="p_add">+			char suffix = &#39;_&#39;;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (order &gt;= 30) {</span>
<span class="p_add">+				order -= 30;</span>
<span class="p_add">+				suffix = &#39;G&#39;;</span>
<span class="p_add">+			} else if (order &gt;= 20) {</span>
<span class="p_add">+				order -= 20;</span>
<span class="p_add">+				suffix = &#39;M&#39;;</span>
<span class="p_add">+			} else if (order &gt;= 10) {</span>
<span class="p_add">+				order -= 10;</span>
<span class="p_add">+				suffix = &#39;k&#39;;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			snprintf(prefix, sizeof(prefix), &quot;_%lu%c&quot;,</span>
<span class="p_add">+				 1UL &lt;&lt; order, suffix);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		seq_printf(m,</span>
<span class="p_add">+			&quot;HugePages%s_Total:   %5lu\n&quot;</span>
<span class="p_add">+			&quot;HugePages%s_Free:    %5lu\n&quot;</span>
<span class="p_add">+			&quot;HugePages%s_Rsvd:    %5lu\n&quot;</span>
<span class="p_add">+			&quot;HugePages%s_Surp:    %5lu\n&quot;</span>
<span class="p_add">+			&quot;Hugepagesize%s:   %8lu kB\n&quot;,</span>
<span class="p_add">+			prefix, h-&gt;nr_huge_pages,</span>
<span class="p_add">+			prefix, h-&gt;free_huge_pages,</span>
<span class="p_add">+			prefix, h-&gt;resv_huge_pages,</span>
<span class="p_add">+			prefix, h-&gt;surplus_huge_pages,</span>
<span class="p_add">+			prefix, 1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));</span>
<span class="p_add">+	}</span>
 }
 
 int hugetlb_report_node_meminfo(int nid, char *buf)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



