
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[01/36] mmu_notifier: add event information to address invalidation v7 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [01/36] mmu_notifier: add event information to address invalidation v7</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 21, 2015, 7:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1432236705-4209-2-git-send-email-j.glisse@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6458431/mbox/"
   >mbox</a>
|
   <a href="/patch/6458431/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6458431/">/patch/6458431/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id D7EF0C0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:33:44 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4792020513
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:33:42 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 72FF920534
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:33:38 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756374AbbEUTdX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 21 May 2015 15:33:23 -0400
Received: from mail-qk0-f169.google.com ([209.85.220.169]:36171 &quot;EHLO
	mail-qk0-f169.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756286AbbEUTdT (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 21 May 2015 15:33:19 -0400
Received: by qkx62 with SMTP id 62so17323938qkx.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 21 May 2015 12:33:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:mime-version:content-type:content-transfer-encoding;
	bh=9fiTF/5xEva6Htm2NGGYYo8X75J8N396iWnP8JRa+r8=;
	b=vR9lbo+E/DnTMVhqbIsThHGcgdB2v5oBr42IFKH+Ct10v2wyN7fSiQ+TfMW20uD5VM
	j/8GAW+NpnLjunmuvLWpRgIyDabcyP8u+SGT3KS1ZLp3q2InCKKXpJD9oNIQeLozkjAA
	8/zvEBhvDt979knwKooa43J+GDyHqKuCcbQ+F8QRVakaaulx7bzrA8FDT8JPnSyl7Duu
	oNpDG1qxp2MreJR/2aBl6serRjWMD/HMqlM6I0DXSedj+7Vo18MfVVGM4/iNipnPuW3r
	JfKYUs3llzQfw9OXFnzETjh00e2WmgxMZjHlSWZzg4kXPwEtzrzNv1dobHsj0aaF0je8
	ArcQ==
X-Received: by 10.55.18.9 with SMTP id c9mr5862454qkh.50.1432236797779;
	Thu, 21 May 2015 12:33:17 -0700 (PDT)
Received: from localhost.localdomain.com (nat-pool-bos-t.redhat.com.
	[66.187.233.206]) by mx.google.com with ESMTPSA id
	6sm13922601qks.37.2015.05.21.12.33.14
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 21 May 2015 12:33:17 -0700 (PDT)
From: j.glisse@gmail.com
To: akpm@linux-foundation.org
Cc: &lt;linux-kernel@vger.kernel.org&gt;, linux-mm@kvack.org,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	&lt;joro@8bytes.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Johannes Weiner &lt;jweiner@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Airlie &lt;airlied@redhat.com&gt;, Brendan Conoboy &lt;blc@redhat.com&gt;,
	Joe Donohue &lt;jdonohue@redhat.com&gt;, Duncan Poole &lt;dpoole@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;,
	John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Lucien Dunning &lt;ldunning@nvidia.com&gt;,
	Cameron Buschardt &lt;cabuschardt@nvidia.com&gt;,
	Arvind Gopalakrishnan &lt;arvindg@nvidia.com&gt;,
	Haggai Eran &lt;haggaie@mellanox.com&gt;,
	Shachar Raindel &lt;raindel@mellanox.com&gt;, Liran Liss &lt;liranl@mellanox.com&gt;,
	Roland Dreier &lt;roland@purestorage.com&gt;, Ben Sander &lt;ben.sander@amd.com&gt;,
	Greg Stoner &lt;Greg.Stoner@amd.com&gt;, John Bridgman &lt;John.Bridgman@amd.com&gt;,
	Michael Mantor &lt;Michael.Mantor@amd.com&gt;,
	Paul Blinzer &lt;Paul.Blinzer@amd.com&gt;,
	Laurent Morichetti &lt;Laurent.Morichetti@amd.com&gt;,
	Alexander Deucher &lt;Alexander.Deucher@amd.com&gt;,
	Oded Gabbay &lt;Oded.Gabbay@amd.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
Subject: [PATCH 01/36] mmu_notifier: add event information to address
	invalidation v7
Date: Thu, 21 May 2015 15:31:10 -0400
Message-Id: &lt;1432236705-4209-2-git-send-email-j.glisse@gmail.com&gt;
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: &lt;1432236705-4209-1-git-send-email-j.glisse@gmail.com&gt;
References: &lt;1432236705-4209-1-git-send-email-j.glisse@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,
	DKIM_ADSP_CUSTOM_MED, 
	DKIM_SIGNED, FREEMAIL_FROM, RCVD_IN_DNSWL_HI, T_DKIM_INVALID,
	T_RP_MATCHES_RCVD, 
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - May 21, 2015, 7:31 p.m.</div>
<pre class="content">
<span class="from">From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

The event information will be useful for new user of mmu_notifier API.
The event argument differentiate between a vma disappearing, a page
being write protected or simply a page being unmaped. This allow new
user to take different path for different event for instance on unmap
the resource used to track a vma are still valid and should stay around.
While if the event is saying that a vma is being destroy it means that any
resources used to track this vma can be free.

Changed since v1:
  - renamed action into event (updated commit message too).
  - simplified the event names and clarified their usage
    also documenting what exceptation the listener can have in
    respect to each event.

Changed since v2:
  - Avoid crazy name.
  - Do not move code that do not need to move.

Changed since v3:
  - Separate hugue page split from mlock/munlock and softdirty.

Changed since v4:
  - Rebase (no other changes).

Changed since v5:
  - Typo fix.
  - Changed zap_page_range from MMU_MUNMAP to MMU_MIGRATE to reflect the
    fact that the address range is still valid just the page backing it
    are no longer.

Changed since v6:
  - try_to_unmap_one() only invalidate when doing migration.
  - Differentiate fork from other case.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="reviewed-by">Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
---
 drivers/gpu/drm/i915/i915_gem_userptr.c |   3 +-
 drivers/gpu/drm/radeon/radeon_mn.c      |   3 +-
 drivers/infiniband/core/umem_odp.c      |   9 ++-
 drivers/iommu/amd_iommu_v2.c            |   3 +-
 drivers/misc/sgi-gru/grutlbpurge.c      |   9 ++-
 drivers/xen/gntdev.c                    |   9 ++-
 fs/proc/task_mmu.c                      |   6 +-
 include/linux/mmu_notifier.h            | 135 ++++++++++++++++++++++++++------
 kernel/events/uprobes.c                 |  10 ++-
 mm/huge_memory.c                        |  39 ++++++---
 mm/hugetlb.c                            |  23 +++---
 mm/ksm.c                                |  18 +++--
 mm/madvise.c                            |   4 +-
 mm/memory.c                             |  27 ++++---
 mm/migrate.c                            |   9 ++-
 mm/mmu_notifier.c                       |  28 ++++---
 mm/mprotect.c                           |   6 +-
 mm/mremap.c                             |   6 +-
 mm/rmap.c                               |   4 +-
 virt/kvm/kvm_main.c                     |  12 ++-
 20 files changed, 261 insertions(+), 102 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101231">John Hubbard</a> - May 30, 2015, 3:43 a.m.</div>
<pre class="content">
On Thu, 21 May 2015, j.glisse@gmail.com wrote:
<span class="quote">
&gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The event information will be useful for new user of mmu_notifier API.</span>
<span class="quote">&gt; The event argument differentiate between a vma disappearing, a page</span>
<span class="quote">&gt; being write protected or simply a page being unmaped. This allow new</span>
<span class="quote">&gt; user to take different path for different event for instance on unmap</span>
<span class="quote">&gt; the resource used to track a vma are still valid and should stay around.</span>
<span class="quote">&gt; While if the event is saying that a vma is being destroy it means that any</span>
<span class="quote">&gt; resources used to track this vma can be free.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - renamed action into event (updated commit message too).</span>
<span class="quote">&gt;   - simplified the event names and clarified their usage</span>
<span class="quote">&gt;     also documenting what exceptation the listener can have in</span>
<span class="quote">&gt;     respect to each event.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v2:</span>
<span class="quote">&gt;   - Avoid crazy name.</span>
<span class="quote">&gt;   - Do not move code that do not need to move.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v3:</span>
<span class="quote">&gt;   - Separate hugue page split from mlock/munlock and softdirty.</span>

Do we care about fixing up patch comments? If so:

s/hugue/huge/
<span class="quote">
&gt; </span>
<span class="quote">&gt; Changed since v4:</span>
<span class="quote">&gt;   - Rebase (no other changes).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v5:</span>
<span class="quote">&gt;   - Typo fix.</span>
<span class="quote">&gt;   - Changed zap_page_range from MMU_MUNMAP to MMU_MIGRATE to reflect the</span>
<span class="quote">&gt;     fact that the address range is still valid just the page backing it</span>
<span class="quote">&gt;     are no longer.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v6:</span>
<span class="quote">&gt;   - try_to_unmap_one() only invalidate when doing migration.</span>
<span class="quote">&gt;   - Differentiate fork from other case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Reviewed-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/gpu/drm/i915/i915_gem_userptr.c |   3 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/radeon/radeon_mn.c      |   3 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/umem_odp.c      |   9 ++-</span>
<span class="quote">&gt;  drivers/iommu/amd_iommu_v2.c            |   3 +-</span>
<span class="quote">&gt;  drivers/misc/sgi-gru/grutlbpurge.c      |   9 ++-</span>
<span class="quote">&gt;  drivers/xen/gntdev.c                    |   9 ++-</span>
<span class="quote">&gt;  fs/proc/task_mmu.c                      |   6 +-</span>
<span class="quote">&gt;  include/linux/mmu_notifier.h            | 135 ++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  kernel/events/uprobes.c                 |  10 ++-</span>
<span class="quote">&gt;  mm/huge_memory.c                        |  39 ++++++---</span>
<span class="quote">&gt;  mm/hugetlb.c                            |  23 +++---</span>
<span class="quote">&gt;  mm/ksm.c                                |  18 +++--</span>
<span class="quote">&gt;  mm/madvise.c                            |   4 +-</span>
<span class="quote">&gt;  mm/memory.c                             |  27 ++++---</span>
<span class="quote">&gt;  mm/migrate.c                            |   9 ++-</span>
<span class="quote">&gt;  mm/mmu_notifier.c                       |  28 ++++---</span>
<span class="quote">&gt;  mm/mprotect.c                           |   6 +-</span>
<span class="quote">&gt;  mm/mremap.c                             |   6 +-</span>
<span class="quote">&gt;  mm/rmap.c                               |   4 +-</span>
<span class="quote">&gt;  virt/kvm/kvm_main.c                     |  12 ++-</span>
<span class="quote">&gt;  20 files changed, 261 insertions(+), 102 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; index 4039ede..452e9b1 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; @@ -132,7 +132,8 @@ restart:</span>
<span class="quote">&gt;  static void i915_gem_userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,</span>
<span class="quote">&gt;  						       struct mm_struct *mm,</span>
<span class="quote">&gt;  						       unsigned long start,</span>
<span class="quote">&gt; -						       unsigned long end)</span>
<span class="quote">&gt; +						       unsigned long end,</span>
<span class="quote">&gt; +						       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct i915_mmu_notifier *mn = container_of(_mn, struct i915_mmu_notifier, mn);</span>
<span class="quote">&gt;  	struct interval_tree_node *it = NULL;</span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="quote">&gt; index eef006c..3a9615b 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="quote">&gt; @@ -121,7 +121,8 @@ static void radeon_mn_release(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  static void radeon_mn_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  					     struct mm_struct *mm,</span>
<span class="quote">&gt;  					     unsigned long start,</span>
<span class="quote">&gt; -					     unsigned long end)</span>
<span class="quote">&gt; +					     unsigned long end,</span>
<span class="quote">&gt; +					     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct radeon_mn *rmn = container_of(mn, struct radeon_mn, mn);</span>
<span class="quote">&gt;  	struct interval_tree_node *it;</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; index 40becdb..6ed69fa 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; @@ -165,7 +165,8 @@ static int invalidate_page_trampoline(struct ib_umem *item, u64 start,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void ib_umem_notifier_invalidate_page(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  					     struct mm_struct *mm,</span>
<span class="quote">&gt; -					     unsigned long address)</span>
<span class="quote">&gt; +					     unsigned long address,</span>
<span class="quote">&gt; +					     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -192,7 +193,8 @@ static int invalidate_range_start_trampoline(struct ib_umem *item, u64 start,</span>
<span class="quote">&gt;  static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  						    struct mm_struct *mm,</span>
<span class="quote">&gt;  						    unsigned long start,</span>
<span class="quote">&gt; -						    unsigned long end)</span>
<span class="quote">&gt; +						    unsigned long end,</span>
<span class="quote">&gt; +						    enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -217,7 +219,8 @@ static int invalidate_range_end_trampoline(struct ib_umem *item, u64 start,</span>
<span class="quote">&gt;  static void ib_umem_notifier_invalidate_range_end(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  						  struct mm_struct *mm,</span>
<span class="quote">&gt;  						  unsigned long start,</span>
<span class="quote">&gt; -						  unsigned long end)</span>
<span class="quote">&gt; +						  unsigned long end,</span>
<span class="quote">&gt; +						  enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; index 3465faf..4aa4de6 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; @@ -384,7 +384,8 @@ static int mn_clear_flush_young(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mn_invalidate_page(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  			       struct mm_struct *mm,</span>
<span class="quote">&gt; -			       unsigned long address)</span>
<span class="quote">&gt; +			       unsigned long address,</span>
<span class="quote">&gt; +			       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	__mn_flush_page(mn, address);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/drivers/misc/sgi-gru/grutlbpurge.c b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="quote">&gt; index 2129274..e67fed1 100644</span>
<span class="quote">&gt; --- a/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="quote">&gt; +++ b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="quote">&gt; @@ -221,7 +221,8 @@ void gru_flush_all_tlb(struct gru_state *gru)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static void gru_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				       struct mm_struct *mm,</span>
<span class="quote">&gt; -				       unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +				       unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +				       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,</span>
<span class="quote">&gt;  						 ms_notifier);</span>
<span class="quote">&gt; @@ -235,7 +236,8 @@ static void gru_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void gru_invalidate_range_end(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				     struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt; -				     unsigned long end)</span>
<span class="quote">&gt; +				     unsigned long end,</span>
<span class="quote">&gt; +				     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,</span>
<span class="quote">&gt;  						 ms_notifier);</span>
<span class="quote">&gt; @@ -248,7 +250,8 @@ static void gru_invalidate_range_end(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void gru_invalidate_page(struct mmu_notifier *mn, struct mm_struct *mm,</span>
<span class="quote">&gt; -				unsigned long address)</span>
<span class="quote">&gt; +				unsigned long address,</span>
<span class="quote">&gt; +				enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,</span>
<span class="quote">&gt;  						 ms_notifier);</span>
<span class="quote">&gt; diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c</span>
<span class="quote">&gt; index 8927485..46bc610 100644</span>
<span class="quote">&gt; --- a/drivers/xen/gntdev.c</span>
<span class="quote">&gt; +++ b/drivers/xen/gntdev.c</span>
<span class="quote">&gt; @@ -467,7 +467,9 @@ static void unmap_if_in_range(struct grant_map *map,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mn_invl_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				struct mm_struct *mm,</span>
<span class="quote">&gt; -				unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +				unsigned long start,</span>
<span class="quote">&gt; +				unsigned long end,</span>
<span class="quote">&gt; +				enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gntdev_priv *priv = container_of(mn, struct gntdev_priv, mn);</span>
<span class="quote">&gt;  	struct grant_map *map;</span>
<span class="quote">&gt; @@ -484,9 +486,10 @@ static void mn_invl_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mn_invl_page(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  			 struct mm_struct *mm,</span>
<span class="quote">&gt; -			 unsigned long address)</span>
<span class="quote">&gt; +			 unsigned long address,</span>
<span class="quote">&gt; +			 enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE);</span>
<span class="quote">&gt; +	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE, event);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mn_release(struct mmu_notifier *mn,</span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 6dee68d..58e2390 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -934,11 +934,13 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt;  				downgrade_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range_start(mm, 0, -1);</span>
<span class="quote">&gt; +			mmu_notifier_invalidate_range_start(mm, 0,</span>
<span class="quote">&gt; +							    -1, MMU_ISDIRTY);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		walk_page_range(0, ~0UL, &amp;clear_refs_walk);</span>
<span class="quote">&gt;  		if (type == CLEAR_REFS_SOFT_DIRTY)</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="quote">&gt; +			mmu_notifier_invalidate_range_end(mm, 0,</span>
<span class="quote">&gt; +							  -1, MMU_ISDIRTY);</span>
<span class="quote">&gt;  		flush_tlb_mm(mm);</span>
<span class="quote">&gt;  		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  out_mm:</span>
<span class="quote">&gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; index 61cd67f..8b11b1b 100644</span>
<span class="quote">&gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; @@ -9,6 +9,70 @@</span>
<span class="quote">&gt;  struct mmu_notifier;</span>
<span class="quote">&gt;  struct mmu_notifier_ops;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* MMU Events report fine-grained information to the callback routine, allowing</span>
<span class="quote">&gt; + * the event listener to make a more informed decision as to what action to</span>
<span class="quote">&gt; + * take. The event types are:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_FORK when a process is forking and as a results various vma needs to</span>
<span class="quote">&gt; + *     be write protected to allow for COW.</span>

Make that:

- MMU_FORK: a process is forking. This will lead to vmas getting 
write-protected, in order to set up COW.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + *   - MMU_HSPLIT huge page split, the memory is the same only the page table</span>
<span class="quote">&gt; + *     structure is updated (level added or removed).</span>

Make that (depending on the name change ideas mentioned later):

- MMU_PAGE_SPLIT: the pages don&#39;t move, nor does their content change, but 
the page table structure is updated (levels added or removed).
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + *   - MMU_ISDIRTY need to update the dirty bit of the page table so proper</span>
<span class="quote">&gt; + *     dirty accounting can happen.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_MIGRATE: memory is migrating from one page to another, thus all write</span>
<span class="quote">&gt; + *     access must stop after invalidate_range_start callback returns.</span>
<span class="quote">&gt; + *     Furthermore, no read access should be allowed either, as a new page can</span>
<span class="quote">&gt; + *     be remapped with write access before the invalidate_range_end callback</span>
<span class="quote">&gt; + *     happens and thus any read access to old page might read stale data. There</span>
<span class="quote">&gt; + *     are several sources for this event, including:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *         - A page moving to swap (various reasons, including page reclaim),</span>
<span class="quote">&gt; + *         - An mremap syscall,</span>
<span class="quote">&gt; + *         - migration for NUMA reasons,</span>
<span class="quote">&gt; + *         - balancing the memory pool,</span>
<span class="quote">&gt; + *         - write fault on COW page,</span>
<span class="quote">&gt; + *         - and more that are not listed here.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_MPROT: memory access protection is changing. Refer to the vma to get</span>
<span class="quote">&gt; + *     the new access protection. All memory access are still valid until the</span>
<span class="quote">&gt; + *     invalidate_range_end callback.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_MUNLOCK: unlock memory. Content of page table stays the same but</span>
<span class="quote">&gt; + *     page are unlocked.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_MUNMAP: the range is being unmapped (outcome of a munmap syscall or</span>
<span class="quote">&gt; + *     process destruction). However, access is still allowed, up until the</span>
<span class="quote">&gt; + *     invalidate_range_free_pages callback. This also implies that secondary</span>
<span class="quote">&gt; + *     page table can be trimmed, because the address range is no longer valid.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_WRITE_BACK: memory is being written back to disk, all write accesses</span>
<span class="quote">&gt; + *     must stop after invalidate_range_start callback returns. Read access are</span>
<span class="quote">&gt; + *     still allowed.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   - MMU_WRITE_PROTECT: memory is being write protected (ie should be mapped</span>
<span class="quote">&gt; + *     read only no matter what the vma memory protection allows). All write</span>
<span class="quote">&gt; + *     accesses must stop after invalidate_range_start callback returns. Read</span>
<span class="quote">&gt; + *     access are still allowed.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * If in doubt when adding a new notifier caller, please use MMU_MIGRATE,</span>
<span class="quote">&gt; + * because it will always lead to reasonable behavior, but will not allow the</span>
<span class="quote">&gt; + * listener a chance to optimize its events.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +enum mmu_event {</span>
<span class="quote">&gt; +	MMU_FORK = 0,</span>
<span class="quote">&gt; +	MMU_HSPLIT,</span>

Let&#39;s rename MMU_HSPLIT to one of the following, take your pick:

MMU_HUGE_PAGE_SPLIT (too long, but you can&#39;t possibly misunderstand it)
MMU_PAGE_SPLIT (my favorite: only huge pages are ever split, so it works)
MMU_HUGE_SPLIT (ugly, but still hard to misunderstand)
<span class="quote">

&gt; +	MMU_ISDIRTY,</span>

This MMU_ISDIRTY seems like a problem to me. First of all, it looks 
backwards: the only place that invokes it is the clear_refs_write() 
routine, for the soft-dirty tracking feature. And in that case, the pages 
are *not* being made dirty! Rather, the kernel is actually making the 
pages non-writable, in order to be able to trap the subsequent page fault 
and figure out if the page is in active use.

So, given that there is only one call site, and that call site should 
actually be setting MMU_WRITE_PROTECT instead (I think), let&#39;s just delete 
MMU_ISDIRTY.

Come to think about it, there is no callback possible for &quot;a page became 
dirty&quot;, anyway. Because the dirty and accessed bits are actually set by 
the hardware, and software is generally unable to know the current state.
So MMU_ISDIRTY just seems inappropriate to me, across the board.

I&#39;ll take a look at the corresponding HMM_ISDIRTY, too.
<span class="quote">
&gt; +	MMU_MIGRATE,</span>
<span class="quote">&gt; +	MMU_MPROT,</span>

The MMU_PROT also looks questionable. Short answer: probably better to 
read the protection, and pass either MMU_WRITE_PROTECT, MMU_READ_WRITE 
(that&#39;s a new item, of course), or MMU_UNMAP.

Here&#39;s why: the call site knows the protection, but by the time it filters 
down to HMM (in later patches), that information is lost, and HMM ends up 
doing (ouch!) another find_vma() call in order to retrieve it--and then 
translates it into only three possible things:

// hmm_mmu_mprot_to_etype() sets one of these:

   HMM_MUNMAP
   HMM_WRITE_PROTECT
   HMM_NONE
<span class="quote">

&gt; +	MMU_MUNLOCK,</span>

I think MMU_UNLOCK would be clearer. We already know the scope, so the 
extra &quot;M&quot; isn&#39;t adding anything.
<span class="quote">
&gt; +	MMU_MUNMAP,</span>

Same thing here: MMU_UNMAP seems better.
<span class="quote">
&gt; +	MMU_WRITE_BACK,</span>
<span class="quote">&gt; +	MMU_WRITE_PROTECT,</span>

We may have to add MMU_READ_WRITE (and maybe another one, I haven&#39;t 
bottomed out on that), if you agree with the above approach of 
always sending a precise event, instead of &quot;protection changed&quot;.

That&#39;s all I saw. This is not a complicated patch, even though it&#39;s 
touching a lot of files, and I think everything else is correct.

thanks,
John Hubbard
<span class="quote">
&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MMU_NOTIFIER</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -82,7 +146,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt;  	void (*change_pte)(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  			   struct mm_struct *mm,</span>
<span class="quote">&gt;  			   unsigned long address,</span>
<span class="quote">&gt; -			   pte_t pte);</span>
<span class="quote">&gt; +			   pte_t pte,</span>
<span class="quote">&gt; +			   enum mmu_event event);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Before this is invoked any secondary MMU is still ok to</span>
<span class="quote">&gt; @@ -93,7 +158,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	void (*invalidate_page)(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				struct mm_struct *mm,</span>
<span class="quote">&gt; -				unsigned long address);</span>
<span class="quote">&gt; +				unsigned long address,</span>
<span class="quote">&gt; +				enum mmu_event event);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * invalidate_range_start() and invalidate_range_end() must be</span>
<span class="quote">&gt; @@ -140,10 +206,14 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	void (*invalidate_range_start)(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				       struct mm_struct *mm,</span>
<span class="quote">&gt; -				       unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +				       unsigned long start,</span>
<span class="quote">&gt; +				       unsigned long end,</span>
<span class="quote">&gt; +				       enum mmu_event event);</span>
<span class="quote">&gt;  	void (*invalidate_range_end)(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  				     struct mm_struct *mm,</span>
<span class="quote">&gt; -				     unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +				     unsigned long start,</span>
<span class="quote">&gt; +				     unsigned long end,</span>
<span class="quote">&gt; +				     enum mmu_event event);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * invalidate_range() is either called between</span>
<span class="quote">&gt; @@ -206,13 +276,20 @@ extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,</span>
<span class="quote">&gt;  extern int __mmu_notifier_test_young(struct mm_struct *mm,</span>
<span class="quote">&gt;  				     unsigned long address);</span>
<span class="quote">&gt;  extern void __mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="quote">&gt; -				      unsigned long address, pte_t pte);</span>
<span class="quote">&gt; +				      unsigned long address,</span>
<span class="quote">&gt; +				      pte_t pte,</span>
<span class="quote">&gt; +				      enum mmu_event event);</span>
<span class="quote">&gt;  extern void __mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
<span class="quote">&gt; -					  unsigned long address);</span>
<span class="quote">&gt; +					  unsigned long address,</span>
<span class="quote">&gt; +					  enum mmu_event event);</span>
<span class="quote">&gt;  extern void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +						  unsigned long start,</span>
<span class="quote">&gt; +						  unsigned long end,</span>
<span class="quote">&gt; +						  enum mmu_event event);</span>
<span class="quote">&gt;  extern void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +						unsigned long start,</span>
<span class="quote">&gt; +						unsigned long end,</span>
<span class="quote">&gt; +						enum mmu_event event);</span>
<span class="quote">&gt;  extern void __mmu_notifier_invalidate_range(struct mm_struct *mm,</span>
<span class="quote">&gt;  				  unsigned long start, unsigned long end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -240,31 +317,38 @@ static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="quote">&gt; -					   unsigned long address, pte_t pte)</span>
<span class="quote">&gt; +					   unsigned long address,</span>
<span class="quote">&gt; +					   pte_t pte,</span>
<span class="quote">&gt; +					   enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (mm_has_notifiers(mm))</span>
<span class="quote">&gt; -		__mmu_notifier_change_pte(mm, address, pte);</span>
<span class="quote">&gt; +		__mmu_notifier_change_pte(mm, address, pte, event);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
<span class="quote">&gt; -					  unsigned long address)</span>
<span class="quote">&gt; +						unsigned long address,</span>
<span class="quote">&gt; +						enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (mm_has_notifiers(mm))</span>
<span class="quote">&gt; -		__mmu_notifier_invalidate_page(mm, address);</span>
<span class="quote">&gt; +		__mmu_notifier_invalidate_page(mm, address, event);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +						       unsigned long start,</span>
<span class="quote">&gt; +						       unsigned long end,</span>
<span class="quote">&gt; +						       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (mm_has_notifiers(mm))</span>
<span class="quote">&gt; -		__mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="quote">&gt; +		__mmu_notifier_invalidate_range_start(mm, start, end, event);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +						     unsigned long start,</span>
<span class="quote">&gt; +						     unsigned long end,</span>
<span class="quote">&gt; +						     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (mm_has_notifiers(mm))</span>
<span class="quote">&gt; -		__mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; +		__mmu_notifier_invalidate_range_end(mm, start, end, event);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,</span>
<span class="quote">&gt; @@ -359,13 +443,13 @@ static inline void mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
<span class="quote">&gt;   * old page would remain mapped readonly in the secondary MMUs after the new</span>
<span class="quote">&gt;   * page is already writable by some CPU through the primary MMU.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define set_pte_at_notify(__mm, __address, __ptep, __pte)		\</span>
<span class="quote">&gt; +#define set_pte_at_notify(__mm, __address, __ptep, __pte, __event)	\</span>
<span class="quote">&gt;  ({									\</span>
<span class="quote">&gt;  	struct mm_struct *___mm = __mm;					\</span>
<span class="quote">&gt;  	unsigned long ___address = __address;				\</span>
<span class="quote">&gt;  	pte_t ___pte = __pte;						\</span>
<span class="quote">&gt;  									\</span>
<span class="quote">&gt; -	mmu_notifier_change_pte(___mm, ___address, ___pte);		\</span>
<span class="quote">&gt; +	mmu_notifier_change_pte(___mm, ___address, ___pte, __event);	\</span>
<span class="quote">&gt;  	set_pte_at(___mm, ___address, __ptep, ___pte);			\</span>
<span class="quote">&gt;  })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -393,22 +477,29 @@ static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="quote">&gt; -					   unsigned long address, pte_t pte)</span>
<span class="quote">&gt; +					   unsigned long address,</span>
<span class="quote">&gt; +					   pte_t pte,</span>
<span class="quote">&gt; +					   enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
<span class="quote">&gt; -					  unsigned long address)</span>
<span class="quote">&gt; +						unsigned long address,</span>
<span class="quote">&gt; +						enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +						       unsigned long start,</span>
<span class="quote">&gt; +						       unsigned long end,</span>
<span class="quote">&gt; +						       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +						     unsigned long start,</span>
<span class="quote">&gt; +						     unsigned long end,</span>
<span class="quote">&gt; +						     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="quote">&gt; index cb346f2..802828a 100644</span>
<span class="quote">&gt; --- a/kernel/events/uprobes.c</span>
<span class="quote">&gt; +++ b/kernel/events/uprobes.c</span>
<span class="quote">&gt; @@ -176,7 +176,8 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;  	/* For try_to_free_swap() and munlock_vma_page() below */</span>
<span class="quote">&gt;  	lock_page(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	err = -EAGAIN;</span>
<span class="quote">&gt;  	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);</span>
<span class="quote">&gt;  	if (!ptep)</span>
<span class="quote">&gt; @@ -194,7 +195,9 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt;  	ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; -	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +	set_pte_at_notify(mm, addr, ptep,</span>
<span class="quote">&gt; +			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="quote">&gt; +			  MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page_remove_rmap(page);</span>
<span class="quote">&gt;  	if (!page_mapped(page))</span>
<span class="quote">&gt; @@ -208,7 +211,8 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;  	err = 0;</span>
<span class="quote">&gt;   unlock:</span>
<span class="quote">&gt;  	mem_cgroup_cancel_charge(kpage, memcg);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	unlock_page(page);</span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index cb8904c..41c342c 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1024,7 +1024,8 @@ static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = haddr;</span>
<span class="quote">&gt;  	mmun_end   = haddr + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					    MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*pmd, orig_pmd)))</span>
<span class="quote">&gt; @@ -1058,7 +1059,8 @@ static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
<span class="quote">&gt;  	page_remove_rmap(page);</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret |= VM_FAULT_WRITE;</span>
<span class="quote">&gt;  	put_page(page);</span>
<span class="quote">&gt; @@ -1068,7 +1070,8 @@ out:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  out_free_pages:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {</span>
<span class="quote">&gt;  		memcg = (void *)page_private(pages[i]);</span>
<span class="quote">&gt;  		set_page_private(pages[i], 0);</span>
<span class="quote">&gt; @@ -1160,7 +1163,8 @@ alloc:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = haddr;</span>
<span class="quote">&gt;  	mmun_end   = haddr + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					    MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(ptl);</span>
<span class="quote">&gt;  	if (page)</span>
<span class="quote">&gt; @@ -1192,7 +1196,8 @@ alloc:</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  out_mn:</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  out_unlock:</span>
<span class="quote">&gt; @@ -1646,7 +1651,8 @@ static int __split_huge_page_splitting(struct page *page,</span>
<span class="quote">&gt;  	const unsigned long mmun_start = address;</span>
<span class="quote">&gt;  	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_HSPLIT);</span>
<span class="quote">&gt;  	pmd = page_check_address_pmd(page, mm, address,</span>
<span class="quote">&gt;  			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &amp;ptl);</span>
<span class="quote">&gt;  	if (pmd) {</span>
<span class="quote">&gt; @@ -1662,7 +1668,8 @@ static int __split_huge_page_splitting(struct page *page,</span>
<span class="quote">&gt;  		ret = 1;</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_HSPLIT);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2526,7 +2533,8 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = address;</span>
<span class="quote">&gt;  	mmun_end   = address + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	pmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * After this gup_fast can&#39;t run anymore. This also removes</span>
<span class="quote">&gt; @@ -2536,7 +2544,8 @@ static void collapse_huge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	_pmd = pmdp_collapse_flush(vma, address, pmd);</span>
<span class="quote">&gt;  	spin_unlock(pmd_ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(pte_ptl);</span>
<span class="quote">&gt;  	isolated = __collapse_huge_page_isolate(vma, address, pte);</span>
<span class="quote">&gt; @@ -2933,24 +2942,28 @@ void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	mmun_start = haddr;</span>
<span class="quote">&gt;  	mmun_end   = haddr + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +						  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt;  		__split_huge_zero_page_pmd(vma, haddr, pmd);</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +						  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	page = pmd_page(*pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="quote">&gt;  	get_page(page);</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	split_huge_page(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 54f129d..19da310 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -2670,7 +2670,8 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt;  	mmun_start = vma-&gt;vm_start;</span>
<span class="quote">&gt;  	mmun_end = vma-&gt;vm_end;</span>
<span class="quote">&gt;  	if (cow)</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_start(src, mmun_start,</span>
<span class="quote">&gt; +						    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (addr = vma-&gt;vm_start; addr &lt; vma-&gt;vm_end; addr += sz) {</span>
<span class="quote">&gt;  		spinlock_t *src_ptl, *dst_ptl;</span>
<span class="quote">&gt; @@ -2724,7 +2725,8 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (cow)</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(src, mmun_start,</span>
<span class="quote">&gt; +						  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2750,7 +2752,8 @@ void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	BUG_ON(end &amp; ~huge_page_mask(h));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	tlb_start_vma(tlb, vma);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	address = start;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	for (; address &lt; end; address += sz) {</span>
<span class="quote">&gt; @@ -2824,7 +2827,8 @@ unlock:</span>
<span class="quote">&gt;  		if (address &lt; end &amp;&amp; !ref_page)</span>
<span class="quote">&gt;  			goto again;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	tlb_end_vma(tlb, vma);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -3003,8 +3007,8 @@ retry_avoidcopy:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = address &amp; huge_page_mask(h);</span>
<span class="quote">&gt;  	mmun_end = mmun_start + huge_page_size(h);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					    MMU_MIGRATE);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Retake the page table lock to check for racing updates</span>
<span class="quote">&gt;  	 * before the page tables are altered</span>
<span class="quote">&gt; @@ -3025,7 +3029,8 @@ retry_avoidcopy:</span>
<span class="quote">&gt;  		new_page = old_page;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					  MMU_MIGRATE);</span>
<span class="quote">&gt;  out_release_all:</span>
<span class="quote">&gt;  	page_cache_release(new_page);</span>
<span class="quote">&gt;  out_release_old:</span>
<span class="quote">&gt; @@ -3493,7 +3498,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	BUG_ON(address &gt;= end);</span>
<span class="quote">&gt;  	flush_cache_range(vma, address, end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MPROT);</span>
<span class="quote">&gt;  	i_mmap_lock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt;  	for (; address &lt; end; address += huge_page_size(h)) {</span>
<span class="quote">&gt;  		spinlock_t *ptl;</span>
<span class="quote">&gt; @@ -3543,7 +3548,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	flush_tlb_range(vma, start, end);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt;  	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MPROT);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return pages &lt;&lt; h-&gt;order;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; index bc7be0e..76f167c 100644</span>
<span class="quote">&gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; @@ -872,7 +872,8 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = addr;</span>
<span class="quote">&gt;  	mmun_end   = addr + PAGE_SIZE;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					    MMU_WRITE_PROTECT);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);</span>
<span class="quote">&gt;  	if (!ptep)</span>
<span class="quote">&gt; @@ -904,7 +905,7 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  		if (pte_dirty(entry))</span>
<span class="quote">&gt;  			set_page_dirty(page);</span>
<span class="quote">&gt;  		entry = pte_mkclean(pte_wrprotect(entry));</span>
<span class="quote">&gt; -		set_pte_at_notify(mm, addr, ptep, entry);</span>
<span class="quote">&gt; +		set_pte_at_notify(mm, addr, ptep, entry, MMU_WRITE_PROTECT);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	*orig_pte = *ptep;</span>
<span class="quote">&gt;  	err = 0;</span>
<span class="quote">&gt; @@ -912,7 +913,8 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  out_unlock:</span>
<span class="quote">&gt;  	pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;  out_mn:</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					  MMU_WRITE_PROTECT);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -948,7 +950,8 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = addr;</span>
<span class="quote">&gt;  	mmun_end   = addr + PAGE_SIZE;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					    MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptep = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="quote">&gt;  	if (!pte_same(*ptep, orig_pte)) {</span>
<span class="quote">&gt; @@ -961,7 +964,9 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt;  	ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; -	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +	set_pte_at_notify(mm, addr, ptep,</span>
<span class="quote">&gt; +			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="quote">&gt; +			  MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page_remove_rmap(page);</span>
<span class="quote">&gt;  	if (!page_mapped(page))</span>
<span class="quote">&gt; @@ -971,7 +976,8 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  	pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;  	err = 0;</span>
<span class="quote">&gt;  out_mn:</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="quote">&gt; +					  MMU_MIGRATE);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index 22e8f0c..b90ba3d 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -405,9 +405,9 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="quote">&gt;  	update_hiwater_rss(mm);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MUNMAP);</span>
<span class="quote">&gt;  	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MUNMAP);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index d1fa0c1..9300fad 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -1048,7 +1048,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  	mmun_end   = end;</span>
<span class="quote">&gt;  	if (is_cow)</span>
<span class="quote">&gt;  		mmu_notifier_invalidate_range_start(src_mm, mmun_start,</span>
<span class="quote">&gt; -						    mmun_end);</span>
<span class="quote">&gt; +						    mmun_end, MMU_FORK);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret = 0;</span>
<span class="quote">&gt;  	dst_pgd = pgd_offset(dst_mm, addr);</span>
<span class="quote">&gt; @@ -1065,7 +1065,8 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  	} while (dst_pgd++, src_pgd++, addr = next, addr != end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (is_cow)</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(src_mm, mmun_start,</span>
<span class="quote">&gt; +						  mmun_end, MMU_FORK);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1335,10 +1336,12 @@ void unmap_vmas(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, start_addr,</span>
<span class="quote">&gt; +					    end_addr, MMU_MUNMAP);</span>
<span class="quote">&gt;  	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end_addr; vma = vma-&gt;vm_next)</span>
<span class="quote">&gt;  		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, start_addr,</span>
<span class="quote">&gt; +					  end_addr, MMU_MUNMAP);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; @@ -1360,10 +1363,10 @@ void zap_page_range(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  	lru_add_drain();</span>
<span class="quote">&gt;  	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="quote">&gt;  	update_hiwater_rss(mm);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next)</span>
<span class="quote">&gt;  		unmap_single_vma(&amp;tlb, vma, start, end, details);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1386,9 +1389,9 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr</span>
<span class="quote">&gt;  	lru_add_drain();</span>
<span class="quote">&gt;  	tlb_gather_mmu(&amp;tlb, mm, address, end);</span>
<span class="quote">&gt;  	update_hiwater_rss(mm);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, address, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, address, end, MMU_MUNMAP);</span>
<span class="quote">&gt;  	unmap_single_vma(&amp;tlb, vma, address, end, details);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, address, end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, address, end, MMU_MUNMAP);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, address, end);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2086,7 +2089,8 @@ static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &amp;memcg))</span>
<span class="quote">&gt;  		goto oom_free_new;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Re-check the pte - we dropped the lock</span>
<span class="quote">&gt; @@ -2119,7 +2123,7 @@ static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		 * mmu page tables (such as kvm shadow page tables), we want the</span>
<span class="quote">&gt;  		 * new page to be mapped directly into the secondary page table.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		set_pte_at_notify(mm, address, page_table, entry);</span>
<span class="quote">&gt; +		set_pte_at_notify(mm, address, page_table, entry, MMU_MIGRATE);</span>
<span class="quote">&gt;  		update_mmu_cache(vma, address, page_table);</span>
<span class="quote">&gt;  		if (old_page) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt; @@ -2158,7 +2162,8 @@ static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		page_cache_release(new_page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pte_unmap_unlock(page_table, ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	if (old_page) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Don&#39;t let another task, with possibly unlocked vma,</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 236ee25..ad9a55a 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1759,12 +1759,14 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	WARN_ON(PageLRU(new_page));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Recheck the target PMD */</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="quote">&gt;  fail_putback:</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +						  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Reverse changes made by migrate_page_copy() */</span>
<span class="quote">&gt;  		if (TestClearPageActive(new_page))</span>
<span class="quote">&gt; @@ -1818,7 +1820,8 @@ fail_putback:</span>
<span class="quote">&gt;  	page_remove_rmap(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Take an &quot;isolate&quot; reference and put new page on the LRU. */</span>
<span class="quote">&gt;  	get_page(new_page);</span>
<span class="quote">&gt; diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="quote">&gt; index 3b9b3d0..e51ea02 100644</span>
<span class="quote">&gt; --- a/mm/mmu_notifier.c</span>
<span class="quote">&gt; +++ b/mm/mmu_notifier.c</span>
<span class="quote">&gt; @@ -142,8 +142,10 @@ int __mmu_notifier_test_young(struct mm_struct *mm,</span>
<span class="quote">&gt;  	return young;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt; -			       pte_t pte)</span>
<span class="quote">&gt; +void __mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="quote">&gt; +			       unsigned long address,</span>
<span class="quote">&gt; +			       pte_t pte,</span>
<span class="quote">&gt; +			       enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mmu_notifier *mn;</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt; @@ -151,13 +153,14 @@ void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  	id = srcu_read_lock(&amp;srcu);</span>
<span class="quote">&gt;  	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {</span>
<span class="quote">&gt;  		if (mn-&gt;ops-&gt;change_pte)</span>
<span class="quote">&gt; -			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte);</span>
<span class="quote">&gt; +			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte, event);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	srcu_read_unlock(&amp;srcu, id);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void __mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
<span class="quote">&gt; -					  unsigned long address)</span>
<span class="quote">&gt; +				    unsigned long address,</span>
<span class="quote">&gt; +				    enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mmu_notifier *mn;</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt; @@ -165,13 +168,16 @@ void __mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	id = srcu_read_lock(&amp;srcu);</span>
<span class="quote">&gt;  	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {</span>
<span class="quote">&gt;  		if (mn-&gt;ops-&gt;invalidate_page)</span>
<span class="quote">&gt; -			mn-&gt;ops-&gt;invalidate_page(mn, mm, address);</span>
<span class="quote">&gt; +			mn-&gt;ops-&gt;invalidate_page(mn, mm, address, event);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	srcu_read_unlock(&amp;srcu, id);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +					   unsigned long start,</span>
<span class="quote">&gt; +					   unsigned long end,</span>
<span class="quote">&gt; +					   enum mmu_event event)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mmu_notifier *mn;</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt; @@ -179,14 +185,17 @@ void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
<span class="quote">&gt;  	id = srcu_read_lock(&amp;srcu);</span>
<span class="quote">&gt;  	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {</span>
<span class="quote">&gt;  		if (mn-&gt;ops-&gt;invalidate_range_start)</span>
<span class="quote">&gt; -			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start, end);</span>
<span class="quote">&gt; +			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start,</span>
<span class="quote">&gt; +							end, event);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	srcu_read_unlock(&amp;srcu, id);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range_start);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
<span class="quote">&gt; -				  unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +					 unsigned long start,</span>
<span class="quote">&gt; +					 unsigned long end,</span>
<span class="quote">&gt; +					 enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mmu_notifier *mn;</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt; @@ -204,7 +213,8 @@ void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
<span class="quote">&gt;  		if (mn-&gt;ops-&gt;invalidate_range)</span>
<span class="quote">&gt;  			mn-&gt;ops-&gt;invalidate_range(mn, mm, start, end);</span>
<span class="quote">&gt;  		if (mn-&gt;ops-&gt;invalidate_range_end)</span>
<span class="quote">&gt; -			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start, end);</span>
<span class="quote">&gt; +			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start,</span>
<span class="quote">&gt; +						      end, event);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	srcu_read_unlock(&amp;srcu, id);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index e7d6f11..a57e8af 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -155,7 +155,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		/* invoke the mmu notifier if the pmd is populated */</span>
<span class="quote">&gt;  		if (!mni_start) {</span>
<span class="quote">&gt;  			mni_start = addr;</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range_start(mm, mni_start, end);</span>
<span class="quote">&gt; +			mmu_notifier_invalidate_range_start(mm, mni_start,</span>
<span class="quote">&gt; +							    end, MMU_MPROT);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (pmd_trans_huge(*pmd)) {</span>
<span class="quote">&gt; @@ -183,7 +184,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	} while (pmd++, addr = next, addr != end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (mni_start)</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range_end(mm, mni_start, end);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, mni_start, end,</span>
<span class="quote">&gt; +						  MMU_MPROT);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (nr_huge_updates)</span>
<span class="quote">&gt;  		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);</span>
<span class="quote">&gt; diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="quote">&gt; index a7c93ec..72051cf 100644</span>
<span class="quote">&gt; --- a/mm/mremap.c</span>
<span class="quote">&gt; +++ b/mm/mremap.c</span>
<span class="quote">&gt; @@ -176,7 +176,8 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmun_start = old_addr;</span>
<span class="quote">&gt;  	mmun_end   = old_end;</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start,</span>
<span class="quote">&gt; +					    mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for (; old_addr &lt; old_end; old_addr += extent, new_addr += extent) {</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt; @@ -228,7 +229,8 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (likely(need_flush))</span>
<span class="quote">&gt;  		flush_tlb_range(vma, old_end-len, old_addr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; +	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start,</span>
<span class="quote">&gt; +					  mmun_end, MMU_MIGRATE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return len + old_addr - old_end;	/* how much done */</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 9c04594..74c51e0 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -915,7 +915,7 @@ static int page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	pte_unmap_unlock(pte, ptl);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (ret) {</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_page(mm, address);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_page(mm, address, MMU_WRITE_BACK);</span>
<span class="quote">&gt;  		(*cleaned)++;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt; @@ -1338,7 +1338,7 @@ discard:</span>
<span class="quote">&gt;  out_unmap:</span>
<span class="quote">&gt;  	pte_unmap_unlock(pte, ptl);</span>
<span class="quote">&gt;  	if (ret != SWAP_FAIL &amp;&amp; !(flags &amp; TTU_MUNLOCK))</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_page(mm, address);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_page(mm, address, MMU_MIGRATE);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; index f202c40..d0b1060 100644</span>
<span class="quote">&gt; --- a/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; +++ b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; @@ -260,7 +260,8 @@ static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  					     struct mm_struct *mm,</span>
<span class="quote">&gt; -					     unsigned long address)</span>
<span class="quote">&gt; +					     unsigned long address,</span>
<span class="quote">&gt; +					     enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm *kvm = mmu_notifier_to_kvm(mn);</span>
<span class="quote">&gt;  	int need_tlb_flush, idx;</span>
<span class="quote">&gt; @@ -302,7 +303,8 @@ static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  					struct mm_struct *mm,</span>
<span class="quote">&gt;  					unsigned long address,</span>
<span class="quote">&gt; -					pte_t pte)</span>
<span class="quote">&gt; +					pte_t pte,</span>
<span class="quote">&gt; +					enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm *kvm = mmu_notifier_to_kvm(mn);</span>
<span class="quote">&gt;  	int idx;</span>
<span class="quote">&gt; @@ -318,7 +320,8 @@ static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  						    struct mm_struct *mm,</span>
<span class="quote">&gt;  						    unsigned long start,</span>
<span class="quote">&gt; -						    unsigned long end)</span>
<span class="quote">&gt; +						    unsigned long end,</span>
<span class="quote">&gt; +						    enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm *kvm = mmu_notifier_to_kvm(mn);</span>
<span class="quote">&gt;  	int need_tlb_flush = 0, idx;</span>
<span class="quote">&gt; @@ -344,7 +347,8 @@ static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,</span>
<span class="quote">&gt;  						  struct mm_struct *mm,</span>
<span class="quote">&gt;  						  unsigned long start,</span>
<span class="quote">&gt; -						  unsigned long end)</span>
<span class="quote">&gt; +						  unsigned long end,</span>
<span class="quote">&gt; +						  enum mmu_event event)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kvm *kvm = mmu_notifier_to_kvm(mn);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 1.9.3</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>

thanks,
John H.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 1, 2015, 7:03 p.m.</div>
<pre class="content">
On Fri, May 29, 2015 at 08:43:59PM -0700, John Hubbard wrote:
<span class="quote">&gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The event information will be useful for new user of mmu_notifier API.</span>
<span class="quote">&gt; &gt; The event argument differentiate between a vma disappearing, a page</span>
<span class="quote">&gt; &gt; being write protected or simply a page being unmaped. This allow new</span>
<span class="quote">&gt; &gt; user to take different path for different event for instance on unmap</span>
<span class="quote">&gt; &gt; the resource used to track a vma are still valid and should stay around.</span>
<span class="quote">&gt; &gt; While if the event is saying that a vma is being destroy it means that any</span>
<span class="quote">&gt; &gt; resources used to track this vma can be free.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt;   - renamed action into event (updated commit message too).</span>
<span class="quote">&gt; &gt;   - simplified the event names and clarified their usage</span>
<span class="quote">&gt; &gt;     also documenting what exceptation the listener can have in</span>
<span class="quote">&gt; &gt;     respect to each event.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Changed since v2:</span>
<span class="quote">&gt; &gt;   - Avoid crazy name.</span>
<span class="quote">&gt; &gt;   - Do not move code that do not need to move.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Changed since v3:</span>
<span class="quote">&gt; &gt;   - Separate hugue page split from mlock/munlock and softdirty.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do we care about fixing up patch comments? If so:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; s/hugue/huge/</span>

I am noting them down and will go over them.


[...]
<span class="quote">&gt; &gt; +	MMU_HSPLIT,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let&#39;s rename MMU_HSPLIT to one of the following, take your pick:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; MMU_HUGE_PAGE_SPLIT (too long, but you can&#39;t possibly misunderstand it)</span>
<span class="quote">&gt; MMU_PAGE_SPLIT (my favorite: only huge pages are ever split, so it works)</span>
<span class="quote">&gt; MMU_HUGE_SPLIT (ugly, but still hard to misunderstand)</span>

I will go with MMU_HUGE_PAGE_SPLIT 


[...]
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	MMU_ISDIRTY,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This MMU_ISDIRTY seems like a problem to me. First of all, it looks </span>
<span class="quote">&gt; backwards: the only place that invokes it is the clear_refs_write() </span>
<span class="quote">&gt; routine, for the soft-dirty tracking feature. And in that case, the pages </span>
<span class="quote">&gt; are *not* being made dirty! Rather, the kernel is actually making the </span>
<span class="quote">&gt; pages non-writable, in order to be able to trap the subsequent page fault </span>
<span class="quote">&gt; and figure out if the page is in active use.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, given that there is only one call site, and that call site should </span>
<span class="quote">&gt; actually be setting MMU_WRITE_PROTECT instead (I think), let&#39;s just delete </span>
<span class="quote">&gt; MMU_ISDIRTY.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Come to think about it, there is no callback possible for &quot;a page became </span>
<span class="quote">&gt; dirty&quot;, anyway. Because the dirty and accessed bits are actually set by </span>
<span class="quote">&gt; the hardware, and software is generally unable to know the current state.</span>
<span class="quote">&gt; So MMU_ISDIRTY just seems inappropriate to me, across the board.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll take a look at the corresponding HMM_ISDIRTY, too.</span>

Ok i need to rename that one to CLEAR_SOFT_DIRTY, the idea is that
for HMM i would rather not write protect the memory for the device
and just rely on the regular and conservative dirtying of page. The
soft dirty is really for migrating a process where you first clear
the soft dirty bit, then copy memory while process still running,
then freeze process an only copy memory that was dirtied since
first copy. Point being that adding soft dirty to HMM is something
that can be done down the road. We should have enough bit inside
the device page table for that.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; +	MMU_MIGRATE,</span>
<span class="quote">&gt; &gt; +	MMU_MPROT,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The MMU_PROT also looks questionable. Short answer: probably better to </span>
<span class="quote">&gt; read the protection, and pass either MMU_WRITE_PROTECT, MMU_READ_WRITE </span>
<span class="quote">&gt; (that&#39;s a new item, of course), or MMU_UNMAP.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here&#39;s why: the call site knows the protection, but by the time it filters </span>
<span class="quote">&gt; down to HMM (in later patches), that information is lost, and HMM ends up </span>
<span class="quote">&gt; doing (ouch!) another find_vma() call in order to retrieve it--and then </span>
<span class="quote">&gt; translates it into only three possible things:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; // hmm_mmu_mprot_to_etype() sets one of these:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    HMM_MUNMAP</span>
<span class="quote">&gt;    HMM_WRITE_PROTECT</span>
<span class="quote">&gt;    HMM_NONE</span>

Linus complained of my previous version where i differenciated the
kind of protection change that was happening, hence why i only pass
down mprot.
<span class="quote">

&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	MMU_MUNLOCK,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think MMU_UNLOCK would be clearer. We already know the scope, so the </span>
<span class="quote">&gt; extra &quot;M&quot; isn&#39;t adding anything.</span>

I named it that way so it matches syscall name munlock(). I think
it is clearer to use MUNLOCK, or maybe SYSCALL_MUNLOCK
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +	MMU_MUNMAP,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same thing here: MMU_UNMAP seems better.</span>

Well same idea here.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; +	MMU_WRITE_BACK,</span>
<span class="quote">&gt; &gt; +	MMU_WRITE_PROTECT,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We may have to add MMU_READ_WRITE (and maybe another one, I haven&#39;t </span>
<span class="quote">&gt; bottomed out on that), if you agree with the above approach of </span>
<span class="quote">&gt; always sending a precise event, instead of &quot;protection changed&quot;.</span>

I think Linus point made sense last time, but i would need to read
again the thread. The idea of that patch is really to provide context
information on what kind of CPU page table changes is happening and
why.

In that respect i should probably change MMU_WRITE_PROTECT to 
MMU_KSM_WRITE_PROTECT.


Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101231">John Hubbard</a> - June 1, 2015, 11:10 p.m.</div>
<pre class="content">
On Mon, 1 Jun 2015, Jerome Glisse wrote:
<span class="quote">
&gt; On Fri, May 29, 2015 at 08:43:59PM -0700, John Hubbard wrote:</span>
<span class="quote">&gt; &gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The event information will be useful for new user of mmu_notifier API.</span>
<span class="quote">&gt; &gt; &gt; The event argument differentiate between a vma disappearing, a page</span>
<span class="quote">&gt; &gt; &gt; being write protected or simply a page being unmaped. This allow new</span>
<span class="quote">&gt; &gt; &gt; user to take different path for different event for instance on unmap</span>
<span class="quote">&gt; &gt; &gt; the resource used to track a vma are still valid and should stay around.</span>
<span class="quote">&gt; &gt; &gt; While if the event is saying that a vma is being destroy it means that any</span>
<span class="quote">&gt; &gt; &gt; resources used to track this vma can be free.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt; &gt;   - renamed action into event (updated commit message too).</span>
<span class="quote">&gt; &gt; &gt;   - simplified the event names and clarified their usage</span>
<span class="quote">&gt; &gt; &gt;     also documenting what exceptation the listener can have in</span>
<span class="quote">&gt; &gt; &gt;     respect to each event.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Changed since v2:</span>
<span class="quote">&gt; &gt; &gt;   - Avoid crazy name.</span>
<span class="quote">&gt; &gt; &gt;   - Do not move code that do not need to move.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Changed since v3:</span>
<span class="quote">&gt; &gt; &gt;   - Separate hugue page split from mlock/munlock and softdirty.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do we care about fixing up patch comments? If so:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; s/hugue/huge/</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am noting them down and will go over them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; +	MMU_HSPLIT,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Let&#39;s rename MMU_HSPLIT to one of the following, take your pick:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; MMU_HUGE_PAGE_SPLIT (too long, but you can&#39;t possibly misunderstand it)</span>
<span class="quote">&gt; &gt; MMU_PAGE_SPLIT (my favorite: only huge pages are ever split, so it works)</span>
<span class="quote">&gt; &gt; MMU_HUGE_SPLIT (ugly, but still hard to misunderstand)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I will go with MMU_HUGE_PAGE_SPLIT </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +	MMU_ISDIRTY,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This MMU_ISDIRTY seems like a problem to me. First of all, it looks </span>
<span class="quote">&gt; &gt; backwards: the only place that invokes it is the clear_refs_write() </span>
<span class="quote">&gt; &gt; routine, for the soft-dirty tracking feature. And in that case, the pages </span>
<span class="quote">&gt; &gt; are *not* being made dirty! Rather, the kernel is actually making the </span>
<span class="quote">&gt; &gt; pages non-writable, in order to be able to trap the subsequent page fault </span>
<span class="quote">&gt; &gt; and figure out if the page is in active use.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So, given that there is only one call site, and that call site should </span>
<span class="quote">&gt; &gt; actually be setting MMU_WRITE_PROTECT instead (I think), let&#39;s just delete </span>
<span class="quote">&gt; &gt; MMU_ISDIRTY.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Come to think about it, there is no callback possible for &quot;a page became </span>
<span class="quote">&gt; &gt; dirty&quot;, anyway. Because the dirty and accessed bits are actually set by </span>
<span class="quote">&gt; &gt; the hardware, and software is generally unable to know the current state.</span>
<span class="quote">&gt; &gt; So MMU_ISDIRTY just seems inappropriate to me, across the board.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;ll take a look at the corresponding HMM_ISDIRTY, too.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok i need to rename that one to CLEAR_SOFT_DIRTY, the idea is that</span>
<span class="quote">&gt; for HMM i would rather not write protect the memory for the device</span>
<span class="quote">&gt; and just rely on the regular and conservative dirtying of page. The</span>
<span class="quote">&gt; soft dirty is really for migrating a process where you first clear</span>
<span class="quote">&gt; the soft dirty bit, then copy memory while process still running,</span>
<span class="quote">&gt; then freeze process an only copy memory that was dirtied since</span>
<span class="quote">&gt; first copy. Point being that adding soft dirty to HMM is something</span>
<span class="quote">&gt; that can be done down the road. We should have enough bit inside</span>
<span class="quote">&gt; the device page table for that.</span>
<span class="quote">&gt; </span>

Yes, I think renaming it to CLEAR_SOFT_DIRTY will definitely allow more 
accurate behavior in response to these events.

Looking ahead, a couple things:

1. This mechanism is also used for general memory utilization tracking (I 
see that Vladimir DavyDov has an &quot;idle memory tracking&quot; proposal that 
assumes this works, for example: https://lwn.net/Articles/642202/ and 
https://lkml.org/lkml/2015/5/12/449).

2. It seems hard to avoid the need to eventually just write protect the 
page, whether it is on the CPU or the remote device, if things like device 
drivers or user space need to track write accesses to a virtual address. 
Either you write protect the page, and trap the page faults, or you wait 
until later and read the dirty bit (indirectly, via something like 
unmap_mapping_range). Or did you have something else in mind?

Anyway, none of that needs to hold up this part of the patchset, because 
the renaming fixes things up for the future code to do the right thing.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +	MMU_MIGRATE,</span>
<span class="quote">&gt; &gt; &gt; +	MMU_MPROT,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The MMU_PROT also looks questionable. Short answer: probably better to </span>
<span class="quote">&gt; &gt; read the protection, and pass either MMU_WRITE_PROTECT, MMU_READ_WRITE </span>
<span class="quote">&gt; &gt; (that&#39;s a new item, of course), or MMU_UNMAP.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Here&#39;s why: the call site knows the protection, but by the time it filters </span>
<span class="quote">&gt; &gt; down to HMM (in later patches), that information is lost, and HMM ends up </span>
<span class="quote">&gt; &gt; doing (ouch!) another find_vma() call in order to retrieve it--and then </span>
<span class="quote">&gt; &gt; translates it into only three possible things:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; // hmm_mmu_mprot_to_etype() sets one of these:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;    HMM_MUNMAP</span>
<span class="quote">&gt; &gt;    HMM_WRITE_PROTECT</span>
<span class="quote">&gt; &gt;    HMM_NONE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Linus complained of my previous version where i differenciated the</span>
<span class="quote">&gt; kind of protection change that was happening, hence why i only pass</span>
<span class="quote">&gt; down mprot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +	MMU_MUNLOCK,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think MMU_UNLOCK would be clearer. We already know the scope, so the </span>
<span class="quote">&gt; &gt; extra &quot;M&quot; isn&#39;t adding anything.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I named it that way so it matches syscall name munlock(). I think</span>
<span class="quote">&gt; it is clearer to use MUNLOCK, or maybe SYSCALL_MUNLOCK</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +	MMU_MUNMAP,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Same thing here: MMU_UNMAP seems better.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well same idea here.</span>

OK, sure.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; +	MMU_WRITE_BACK,</span>
<span class="quote">&gt; &gt; &gt; +	MMU_WRITE_PROTECT,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We may have to add MMU_READ_WRITE (and maybe another one, I haven&#39;t </span>
<span class="quote">&gt; &gt; bottomed out on that), if you agree with the above approach of </span>
<span class="quote">&gt; &gt; always sending a precise event, instead of &quot;protection changed&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think Linus point made sense last time, but i would need to read</span>
<span class="quote">&gt; again the thread. The idea of that patch is really to provide context</span>
<span class="quote">&gt; information on what kind of CPU page table changes is happening and</span>
<span class="quote">&gt; why.</span>
<span class="quote">&gt;</span>

Shoot, I tried to find that conversation, but my search foo is too weak. 
If you have a link to that thread, I&#39;d appreciate it, so I can refresh my 
memory.

I was hoping to re-read it and see if anything has changed. It&#39;s not 
really a huge problem to call find_vma() again, but I do want to be sure 
that there&#39;s a good reason for doing so.
 
Otherwise, I&#39;ll just rely on your memory that Linus preferred your current 
approach, and call it good, then.
<span class="quote">
&gt; In that respect i should probably change MMU_WRITE_PROTECT to </span>
<span class="quote">&gt; MMU_KSM_WRITE_PROTECT.</span>
<span class="quote">&gt; </span>

Yes, that might help clarify to the reader, because otherwise it&#39;s not 
always obvious why we have &quot;MPROT&quot; and &quot;WRITE_PROTECT&quot; (which seems at 
first like merely a subset of MPROT).

thanks,
john h
<span class="quote">
&gt; </span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt; Jérôme</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 3, 2015, 4:07 p.m.</div>
<pre class="content">
On Mon, Jun 01, 2015 at 04:10:46PM -0700, John Hubbard wrote:
<span class="quote">&gt; On Mon, 1 Jun 2015, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; On Fri, May 29, 2015 at 08:43:59PM -0700, John Hubbard wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

[...]
<span class="quote">&gt; &gt; &gt; &gt; +	MMU_ISDIRTY,</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This MMU_ISDIRTY seems like a problem to me. First of all, it looks </span>
<span class="quote">&gt; &gt; &gt; backwards: the only place that invokes it is the clear_refs_write() </span>
<span class="quote">&gt; &gt; &gt; routine, for the soft-dirty tracking feature. And in that case, the pages </span>
<span class="quote">&gt; &gt; &gt; are *not* being made dirty! Rather, the kernel is actually making the </span>
<span class="quote">&gt; &gt; &gt; pages non-writable, in order to be able to trap the subsequent page fault </span>
<span class="quote">&gt; &gt; &gt; and figure out if the page is in active use.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; So, given that there is only one call site, and that call site should </span>
<span class="quote">&gt; &gt; &gt; actually be setting MMU_WRITE_PROTECT instead (I think), let&#39;s just delete </span>
<span class="quote">&gt; &gt; &gt; MMU_ISDIRTY.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Come to think about it, there is no callback possible for &quot;a page became </span>
<span class="quote">&gt; &gt; &gt; dirty&quot;, anyway. Because the dirty and accessed bits are actually set by </span>
<span class="quote">&gt; &gt; &gt; the hardware, and software is generally unable to know the current state.</span>
<span class="quote">&gt; &gt; &gt; So MMU_ISDIRTY just seems inappropriate to me, across the board.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I&#39;ll take a look at the corresponding HMM_ISDIRTY, too.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ok i need to rename that one to CLEAR_SOFT_DIRTY, the idea is that</span>
<span class="quote">&gt; &gt; for HMM i would rather not write protect the memory for the device</span>
<span class="quote">&gt; &gt; and just rely on the regular and conservative dirtying of page. The</span>
<span class="quote">&gt; &gt; soft dirty is really for migrating a process where you first clear</span>
<span class="quote">&gt; &gt; the soft dirty bit, then copy memory while process still running,</span>
<span class="quote">&gt; &gt; then freeze process an only copy memory that was dirtied since</span>
<span class="quote">&gt; &gt; first copy. Point being that adding soft dirty to HMM is something</span>
<span class="quote">&gt; &gt; that can be done down the road. We should have enough bit inside</span>
<span class="quote">&gt; &gt; the device page table for that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, I think renaming it to CLEAR_SOFT_DIRTY will definitely allow more </span>
<span class="quote">&gt; accurate behavior in response to these events.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Looking ahead, a couple things:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1. This mechanism is also used for general memory utilization tracking (I </span>
<span class="quote">&gt; see that Vladimir DavyDov has an &quot;idle memory tracking&quot; proposal that </span>
<span class="quote">&gt; assumes this works, for example: https://lwn.net/Articles/642202/ and </span>
<span class="quote">&gt; https://lkml.org/lkml/2015/5/12/449).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 2. It seems hard to avoid the need to eventually just write protect the </span>
<span class="quote">&gt; page, whether it is on the CPU or the remote device, if things like device </span>
<span class="quote">&gt; drivers or user space need to track write accesses to a virtual address. </span>
<span class="quote">&gt; Either you write protect the page, and trap the page faults, or you wait </span>
<span class="quote">&gt; until later and read the dirty bit (indirectly, via something like </span>
<span class="quote">&gt; unmap_mapping_range). Or did you have something else in mind?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway, none of that needs to hold up this part of the patchset, because </span>
<span class="quote">&gt; the renaming fixes things up for the future code to do the right thing.</span>

I will go over Vladimir patchset it was on my radar but haven&#39;t had yet a
chance to go over it. We will likely need to do the write protect for device
too. But as you said this is not an issue that this patch need a fix for,
only HMM would need to change. I will do that.


[...]
<span class="quote">&gt; &gt; &gt; We may have to add MMU_READ_WRITE (and maybe another one, I haven&#39;t </span>
<span class="quote">&gt; &gt; &gt; bottomed out on that), if you agree with the above approach of </span>
<span class="quote">&gt; &gt; &gt; always sending a precise event, instead of &quot;protection changed&quot;.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think Linus point made sense last time, but i would need to read</span>
<span class="quote">&gt; &gt; again the thread. The idea of that patch is really to provide context</span>
<span class="quote">&gt; &gt; information on what kind of CPU page table changes is happening and</span>
<span class="quote">&gt; &gt; why.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Shoot, I tried to find that conversation, but my search foo is too weak. </span>
<span class="quote">&gt; If you have a link to that thread, I&#39;d appreciate it, so I can refresh my </span>
<span class="quote">&gt; memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I was hoping to re-read it and see if anything has changed. It&#39;s not </span>
<span class="quote">&gt; really a huge problem to call find_vma() again, but I do want to be sure </span>
<span class="quote">&gt; that there&#39;s a good reason for doing so.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; Otherwise, I&#39;ll just rely on your memory that Linus preferred your current </span>
<span class="quote">&gt; approach, and call it good, then.</span>

http://lkml.iu.edu/hypermail/linux/kernel/1406.3/04880.html

I am working on doing some of the changes discussed so far, i will push my
tree to git://people.freedesktop.org/~glisse/linux hmm branch once i am done.

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101231">John Hubbard</a> - June 3, 2015, 11:02 p.m.</div>
<pre class="content">
On Wed, 3 Jun 2015, Jerome Glisse wrote:
<span class="quote">&gt; On Mon, Jun 01, 2015 at 04:10:46PM -0700, John Hubbard wrote:</span>
<span class="quote">&gt; &gt; On Mon, 1 Jun 2015, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; &gt; On Fri, May 29, 2015 at 08:43:59PM -0700, John Hubbard wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; We may have to add MMU_READ_WRITE (and maybe another one, I haven&#39;t </span>
<span class="quote">&gt; &gt; &gt; &gt; bottomed out on that), if you agree with the above approach of </span>
<span class="quote">&gt; &gt; &gt; &gt; always sending a precise event, instead of &quot;protection changed&quot;.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I think Linus point made sense last time, but i would need to read</span>
<span class="quote">&gt; &gt; &gt; again the thread. The idea of that patch is really to provide context</span>
<span class="quote">&gt; &gt; &gt; information on what kind of CPU page table changes is happening and</span>
<span class="quote">&gt; &gt; &gt; why.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Shoot, I tried to find that conversation, but my search foo is too weak. </span>
<span class="quote">&gt; &gt; If you have a link to that thread, I&#39;d appreciate it, so I can refresh my </span>
<span class="quote">&gt; &gt; memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I was hoping to re-read it and see if anything has changed. It&#39;s not </span>
<span class="quote">&gt; &gt; really a huge problem to call find_vma() again, but I do want to be sure </span>
<span class="quote">&gt; &gt; that there&#39;s a good reason for doing so.</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; Otherwise, I&#39;ll just rely on your memory that Linus preferred your current </span>
<span class="quote">&gt; &gt; approach, and call it good, then.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; http://lkml.iu.edu/hypermail/linux/kernel/1406.3/04880.html</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am working on doing some of the changes discussed so far, i will push my</span>
<span class="quote">&gt; tree to git://people.freedesktop.org/~glisse/linux hmm branch once i am done.</span>


Aha, OK, that was back when you were passing around the vma. But now, 
you&#39;re not doing that anymore. It&#39;s just: mm*, range* (start, end, 
event_type), and sometimes page* and exclude*). So I think it&#39;s still 
reasonable to either pass down pure vma flags, or else add in new event 
types, in order to avoid having to lookup the vma later.

We could still get NAK&#39;d for adding ugly new event types, but if you&#39;re 
going to add the event types at all, let&#39;s make them complete, so that we 
really *earn* the NAK. :)
<span class="quote">
&gt; </span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt; Jérôme</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 4039ede..452e9b1 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -132,7 +132,8 @@</span> <span class="p_context"> restart:</span>
 static void i915_gem_userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 						       struct mm_struct *mm,
 						       unsigned long start,
<span class="p_del">-						       unsigned long end)</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 	struct i915_mmu_notifier *mn = container_of(_mn, struct i915_mmu_notifier, mn);
 	struct interval_tree_node *it = NULL;
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_mn.c b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">index eef006c..3a9615b 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_mn.c</span>
<span class="p_chunk">@@ -121,7 +121,8 @@</span> <span class="p_context"> static void radeon_mn_release(struct mmu_notifier *mn,</span>
 static void radeon_mn_invalidate_range_start(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
 					     unsigned long start,
<span class="p_del">-					     unsigned long end)</span>
<span class="p_add">+					     unsigned long end,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct radeon_mn *rmn = container_of(mn, struct radeon_mn, mn);
 	struct interval_tree_node *it;
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index 40becdb..6ed69fa 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -165,7 +165,8 @@</span> <span class="p_context"> static int invalidate_page_trampoline(struct ib_umem *item, u64 start,</span>
 
 static void ib_umem_notifier_invalidate_page(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
<span class="p_del">-					     unsigned long address)</span>
<span class="p_add">+					     unsigned long address,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_chunk">@@ -192,7 +193,8 @@</span> <span class="p_context"> static int invalidate_range_start_trampoline(struct ib_umem *item, u64 start,</span>
 static void ib_umem_notifier_invalidate_range_start(struct mmu_notifier *mn,
 						    struct mm_struct *mm,
 						    unsigned long start,
<span class="p_del">-						    unsigned long end)</span>
<span class="p_add">+						    unsigned long end,</span>
<span class="p_add">+						    enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_chunk">@@ -217,7 +219,8 @@</span> <span class="p_context"> static int invalidate_range_end_trampoline(struct ib_umem *item, u64 start,</span>
 static void ib_umem_notifier_invalidate_range_end(struct mmu_notifier *mn,
 						  struct mm_struct *mm,
 						  unsigned long start,
<span class="p_del">-						  unsigned long end)</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event)</span>
 {
 	struct ib_ucontext *context = container_of(mn, struct ib_ucontext, mn);
 
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">index 3465faf..4aa4de6 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_chunk">@@ -384,7 +384,8 @@</span> <span class="p_context"> static int mn_clear_flush_young(struct mmu_notifier *mn,</span>
 
 static void mn_invalidate_page(struct mmu_notifier *mn,
 			       struct mm_struct *mm,
<span class="p_del">-			       unsigned long address)</span>
<span class="p_add">+			       unsigned long address,</span>
<span class="p_add">+			       enum mmu_event event)</span>
 {
 	__mn_flush_page(mn, address);
 }
<span class="p_header">diff --git a/drivers/misc/sgi-gru/grutlbpurge.c b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_header">index 2129274..e67fed1 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-gru/grutlbpurge.c</span>
<span class="p_chunk">@@ -221,7 +221,8 @@</span> <span class="p_context"> void gru_flush_all_tlb(struct gru_state *gru)</span>
  */
 static void gru_invalidate_range_start(struct mmu_notifier *mn,
 				       struct mm_struct *mm,
<span class="p_del">-				       unsigned long start, unsigned long end)</span>
<span class="p_add">+				       unsigned long start, unsigned long end,</span>
<span class="p_add">+				       enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_chunk">@@ -235,7 +236,8 @@</span> <span class="p_context"> static void gru_invalidate_range_start(struct mmu_notifier *mn,</span>
 
 static void gru_invalidate_range_end(struct mmu_notifier *mn,
 				     struct mm_struct *mm, unsigned long start,
<span class="p_del">-				     unsigned long end)</span>
<span class="p_add">+				     unsigned long end,</span>
<span class="p_add">+				     enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_chunk">@@ -248,7 +250,8 @@</span> <span class="p_context"> static void gru_invalidate_range_end(struct mmu_notifier *mn,</span>
 }
 
 static void gru_invalidate_page(struct mmu_notifier *mn, struct mm_struct *mm,
<span class="p_del">-				unsigned long address)</span>
<span class="p_add">+				unsigned long address,</span>
<span class="p_add">+				enum mmu_event event)</span>
 {
 	struct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,
 						 ms_notifier);
<span class="p_header">diff --git a/drivers/xen/gntdev.c b/drivers/xen/gntdev.c</span>
<span class="p_header">index 8927485..46bc610 100644</span>
<span class="p_header">--- a/drivers/xen/gntdev.c</span>
<span class="p_header">+++ b/drivers/xen/gntdev.c</span>
<span class="p_chunk">@@ -467,7 +467,9 @@</span> <span class="p_context"> static void unmap_if_in_range(struct grant_map *map,</span>
 
 static void mn_invl_range_start(struct mmu_notifier *mn,
 				struct mm_struct *mm,
<span class="p_del">-				unsigned long start, unsigned long end)</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end,</span>
<span class="p_add">+				enum mmu_event event)</span>
 {
 	struct gntdev_priv *priv = container_of(mn, struct gntdev_priv, mn);
 	struct grant_map *map;
<span class="p_chunk">@@ -484,9 +486,10 @@</span> <span class="p_context"> static void mn_invl_range_start(struct mmu_notifier *mn,</span>
 
 static void mn_invl_page(struct mmu_notifier *mn,
 			 struct mm_struct *mm,
<span class="p_del">-			 unsigned long address)</span>
<span class="p_add">+			 unsigned long address,</span>
<span class="p_add">+			 enum mmu_event event)</span>
 {
<span class="p_del">-	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE);</span>
<span class="p_add">+	mn_invl_range_start(mn, mm, address, address + PAGE_SIZE, event);</span>
 }
 
 static void mn_release(struct mmu_notifier *mn,
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 6dee68d..58e2390 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -934,11 +934,13 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 				downgrade_write(&amp;mm-&gt;mmap_sem);
 				break;
 			}
<span class="p_del">-			mmu_notifier_invalidate_range_start(mm, 0, -1);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_start(mm, 0,</span>
<span class="p_add">+							    -1, MMU_ISDIRTY);</span>
 		}
 		walk_page_range(0, ~0UL, &amp;clear_refs_walk);
 		if (type == CLEAR_REFS_SOFT_DIRTY)
<span class="p_del">-			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_end(mm, 0,</span>
<span class="p_add">+							  -1, MMU_ISDIRTY);</span>
 		flush_tlb_mm(mm);
 		up_read(&amp;mm-&gt;mmap_sem);
 out_mm:
<span class="p_header">diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="p_header">index 61cd67f..8b11b1b 100644</span>
<span class="p_header">--- a/include/linux/mmu_notifier.h</span>
<span class="p_header">+++ b/include/linux/mmu_notifier.h</span>
<span class="p_chunk">@@ -9,6 +9,70 @@</span> <span class="p_context"></span>
 struct mmu_notifier;
 struct mmu_notifier_ops;
 
<span class="p_add">+/* MMU Events report fine-grained information to the callback routine, allowing</span>
<span class="p_add">+ * the event listener to make a more informed decision as to what action to</span>
<span class="p_add">+ * take. The event types are:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_FORK when a process is forking and as a results various vma needs to</span>
<span class="p_add">+ *     be write protected to allow for COW.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_HSPLIT huge page split, the memory is the same only the page table</span>
<span class="p_add">+ *     structure is updated (level added or removed).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_ISDIRTY need to update the dirty bit of the page table so proper</span>
<span class="p_add">+ *     dirty accounting can happen.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MIGRATE: memory is migrating from one page to another, thus all write</span>
<span class="p_add">+ *     access must stop after invalidate_range_start callback returns.</span>
<span class="p_add">+ *     Furthermore, no read access should be allowed either, as a new page can</span>
<span class="p_add">+ *     be remapped with write access before the invalidate_range_end callback</span>
<span class="p_add">+ *     happens and thus any read access to old page might read stale data. There</span>
<span class="p_add">+ *     are several sources for this event, including:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *         - A page moving to swap (various reasons, including page reclaim),</span>
<span class="p_add">+ *         - An mremap syscall,</span>
<span class="p_add">+ *         - migration for NUMA reasons,</span>
<span class="p_add">+ *         - balancing the memory pool,</span>
<span class="p_add">+ *         - write fault on COW page,</span>
<span class="p_add">+ *         - and more that are not listed here.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MPROT: memory access protection is changing. Refer to the vma to get</span>
<span class="p_add">+ *     the new access protection. All memory access are still valid until the</span>
<span class="p_add">+ *     invalidate_range_end callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MUNLOCK: unlock memory. Content of page table stays the same but</span>
<span class="p_add">+ *     page are unlocked.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_MUNMAP: the range is being unmapped (outcome of a munmap syscall or</span>
<span class="p_add">+ *     process destruction). However, access is still allowed, up until the</span>
<span class="p_add">+ *     invalidate_range_free_pages callback. This also implies that secondary</span>
<span class="p_add">+ *     page table can be trimmed, because the address range is no longer valid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_WRITE_BACK: memory is being written back to disk, all write accesses</span>
<span class="p_add">+ *     must stop after invalidate_range_start callback returns. Read access are</span>
<span class="p_add">+ *     still allowed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   - MMU_WRITE_PROTECT: memory is being write protected (ie should be mapped</span>
<span class="p_add">+ *     read only no matter what the vma memory protection allows). All write</span>
<span class="p_add">+ *     accesses must stop after invalidate_range_start callback returns. Read</span>
<span class="p_add">+ *     access are still allowed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If in doubt when adding a new notifier caller, please use MMU_MIGRATE,</span>
<span class="p_add">+ * because it will always lead to reasonable behavior, but will not allow the</span>
<span class="p_add">+ * listener a chance to optimize its events.</span>
<span class="p_add">+ */</span>
<span class="p_add">+enum mmu_event {</span>
<span class="p_add">+	MMU_FORK = 0,</span>
<span class="p_add">+	MMU_HSPLIT,</span>
<span class="p_add">+	MMU_ISDIRTY,</span>
<span class="p_add">+	MMU_MIGRATE,</span>
<span class="p_add">+	MMU_MPROT,</span>
<span class="p_add">+	MMU_MUNLOCK,</span>
<span class="p_add">+	MMU_MUNMAP,</span>
<span class="p_add">+	MMU_WRITE_BACK,</span>
<span class="p_add">+	MMU_WRITE_PROTECT,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MMU_NOTIFIER
 
 /*
<span class="p_chunk">@@ -82,7 +146,8 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	void (*change_pte)(struct mmu_notifier *mn,
 			   struct mm_struct *mm,
 			   unsigned long address,
<span class="p_del">-			   pte_t pte);</span>
<span class="p_add">+			   pte_t pte,</span>
<span class="p_add">+			   enum mmu_event event);</span>
 
 	/*
 	 * Before this is invoked any secondary MMU is still ok to
<span class="p_chunk">@@ -93,7 +158,8 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	 */
 	void (*invalidate_page)(struct mmu_notifier *mn,
 				struct mm_struct *mm,
<span class="p_del">-				unsigned long address);</span>
<span class="p_add">+				unsigned long address,</span>
<span class="p_add">+				enum mmu_event event);</span>
 
 	/*
 	 * invalidate_range_start() and invalidate_range_end() must be
<span class="p_chunk">@@ -140,10 +206,14 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	 */
 	void (*invalidate_range_start)(struct mmu_notifier *mn,
 				       struct mm_struct *mm,
<span class="p_del">-				       unsigned long start, unsigned long end);</span>
<span class="p_add">+				       unsigned long start,</span>
<span class="p_add">+				       unsigned long end,</span>
<span class="p_add">+				       enum mmu_event event);</span>
 	void (*invalidate_range_end)(struct mmu_notifier *mn,
 				     struct mm_struct *mm,
<span class="p_del">-				     unsigned long start, unsigned long end);</span>
<span class="p_add">+				     unsigned long start,</span>
<span class="p_add">+				     unsigned long end,</span>
<span class="p_add">+				     enum mmu_event event);</span>
 
 	/*
 	 * invalidate_range() is either called between
<span class="p_chunk">@@ -206,13 +276,20 @@</span> <span class="p_context"> extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,</span>
 extern int __mmu_notifier_test_young(struct mm_struct *mm,
 				     unsigned long address);
 extern void __mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-				      unsigned long address, pte_t pte);</span>
<span class="p_add">+				      unsigned long address,</span>
<span class="p_add">+				      pte_t pte,</span>
<span class="p_add">+				      enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address);</span>
<span class="p_add">+					  unsigned long address,</span>
<span class="p_add">+					  enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end);</span>
<span class="p_add">+						  unsigned long start,</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end);</span>
<span class="p_add">+						unsigned long start,</span>
<span class="p_add">+						unsigned long end,</span>
<span class="p_add">+						enum mmu_event event);</span>
 extern void __mmu_notifier_invalidate_range(struct mm_struct *mm,
 				  unsigned long start, unsigned long end);
 
<span class="p_chunk">@@ -240,31 +317,38 @@</span> <span class="p_context"> static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
 }
 
 static inline void mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-					   unsigned long address, pte_t pte)</span>
<span class="p_add">+					   unsigned long address,</span>
<span class="p_add">+					   pte_t pte,</span>
<span class="p_add">+					   enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_change_pte(mm, address, pte);</span>
<span class="p_add">+		__mmu_notifier_change_pte(mm, address, pte, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+						unsigned long address,</span>
<span class="p_add">+						enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		__mmu_notifier_invalidate_page(mm, address, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						       unsigned long start,</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+		__mmu_notifier_invalidate_range_start(mm, start, end, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						     unsigned long start,</span>
<span class="p_add">+						     unsigned long end,</span>
<span class="p_add">+						     enum mmu_event event)</span>
 {
 	if (mm_has_notifiers(mm))
<span class="p_del">-		__mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+		__mmu_notifier_invalidate_range_end(mm, start, end, event);</span>
 }
 
 static inline void mmu_notifier_invalidate_range(struct mm_struct *mm,
<span class="p_chunk">@@ -359,13 +443,13 @@</span> <span class="p_context"> static inline void mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
  * old page would remain mapped readonly in the secondary MMUs after the new
  * page is already writable by some CPU through the primary MMU.
  */
<span class="p_del">-#define set_pte_at_notify(__mm, __address, __ptep, __pte)		\</span>
<span class="p_add">+#define set_pte_at_notify(__mm, __address, __ptep, __pte, __event)	\</span>
 ({									\
 	struct mm_struct *___mm = __mm;					\
 	unsigned long ___address = __address;				\
 	pte_t ___pte = __pte;						\
 									\
<span class="p_del">-	mmu_notifier_change_pte(___mm, ___address, ___pte);		\</span>
<span class="p_add">+	mmu_notifier_change_pte(___mm, ___address, ___pte, __event);	\</span>
 	set_pte_at(___mm, ___address, __ptep, ___pte);			\
 })
 
<span class="p_chunk">@@ -393,22 +477,29 @@</span> <span class="p_context"> static inline int mmu_notifier_test_young(struct mm_struct *mm,</span>
 }
 
 static inline void mmu_notifier_change_pte(struct mm_struct *mm,
<span class="p_del">-					   unsigned long address, pte_t pte)</span>
<span class="p_add">+					   unsigned long address,</span>
<span class="p_add">+					   pte_t pte,</span>
<span class="p_add">+					   enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+						unsigned long address,</span>
<span class="p_add">+						enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						       unsigned long start,</span>
<span class="p_add">+						       unsigned long end,</span>
<span class="p_add">+						       enum mmu_event event)</span>
 {
 }
 
 static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+						     unsigned long start,</span>
<span class="p_add">+						     unsigned long end,</span>
<span class="p_add">+						     enum mmu_event event)</span>
 {
 }
 
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index cb346f2..802828a 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -176,7 +176,8 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 	/* For try_to_free_swap() and munlock_vma_page() below */
 	lock_page(page);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	err = -EAGAIN;
 	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);
 	if (!ptep)
<span class="p_chunk">@@ -194,7 +195,9 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
 	ptep_clear_flush_notify(vma, addr, ptep);
<span class="p_del">-	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	set_pte_at_notify(mm, addr, ptep,</span>
<span class="p_add">+			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="p_add">+			  MMU_MIGRATE);</span>
 
 	page_remove_rmap(page);
 	if (!page_mapped(page))
<span class="p_chunk">@@ -208,7 +211,8 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 	err = 0;
  unlock:
 	mem_cgroup_cancel_charge(kpage, memcg);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	unlock_page(page);
 	return err;
 }
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index cb8904c..41c342c 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1024,7 +1024,8 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, orig_pmd)))
<span class="p_chunk">@@ -1058,7 +1059,8 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 	page_remove_rmap(page);
 	spin_unlock(ptl);
 
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	ret |= VM_FAULT_WRITE;
 	put_page(page);
<span class="p_chunk">@@ -1068,7 +1070,8 @@</span> <span class="p_context"> out:</span>
 
 out_free_pages:
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	for (i = 0; i &lt; HPAGE_PMD_NR; i++) {
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
<span class="p_chunk">@@ -1160,7 +1163,8 @@</span> <span class="p_context"> alloc:</span>
 
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	spin_lock(ptl);
 	if (page)
<span class="p_chunk">@@ -1192,7 +1196,8 @@</span> <span class="p_context"> alloc:</span>
 	}
 	spin_unlock(ptl);
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 out:
 	return ret;
 out_unlock:
<span class="p_chunk">@@ -1646,7 +1651,8 @@</span> <span class="p_context"> static int __split_huge_page_splitting(struct page *page,</span>
 	const unsigned long mmun_start = address;
 	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_HSPLIT);</span>
 	pmd = page_check_address_pmd(page, mm, address,
 			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &amp;ptl);
 	if (pmd) {
<span class="p_chunk">@@ -1662,7 +1668,8 @@</span> <span class="p_context"> static int __split_huge_page_splitting(struct page *page,</span>
 		ret = 1;
 		spin_unlock(ptl);
 	}
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_HSPLIT);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -2526,7 +2533,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 
 	mmun_start = address;
 	mmun_end   = address + HPAGE_PMD_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	pmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */
 	/*
 	 * After this gup_fast can&#39;t run anymore. This also removes
<span class="p_chunk">@@ -2536,7 +2544,8 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	 */
 	_pmd = pmdp_collapse_flush(vma, address, pmd);
 	spin_unlock(pmd_ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	spin_lock(pte_ptl);
 	isolated = __collapse_huge_page_isolate(vma, address, pte);
<span class="p_chunk">@@ -2933,24 +2942,28 @@</span> <span class="p_context"> void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
 	mmun_start = haddr;
 	mmun_end   = haddr + HPAGE_PMD_SIZE;
 again:
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_trans_huge(*pmd))) {
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 		return;
 	}
 	if (is_huge_zero_pmd(*pmd)) {
 		__split_huge_zero_page_pmd(vma, haddr, pmd);
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 		return;
 	}
 	page = pmd_page(*pmd);
 	VM_BUG_ON_PAGE(!page_count(page), page);
 	get_page(page);
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	split_huge_page(page);
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 54f129d..19da310 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -2670,7 +2670,8 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 	mmun_start = vma-&gt;vm_start;
 	mmun_end = vma-&gt;vm_end;
 	if (cow)
<span class="p_del">-		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(src, mmun_start,</span>
<span class="p_add">+						    mmun_end, MMU_MIGRATE);</span>
 
 	for (addr = vma-&gt;vm_start; addr &lt; vma-&gt;vm_end; addr += sz) {
 		spinlock_t *src_ptl, *dst_ptl;
<span class="p_chunk">@@ -2724,7 +2725,8 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 	}
 
 	if (cow)
<span class="p_del">-		mmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(src, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -2750,7 +2752,8 @@</span> <span class="p_context"> void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	BUG_ON(end &amp; ~huge_page_mask(h));
 
 	tlb_start_vma(tlb, vma);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	address = start;
 again:
 	for (; address &lt; end; address += sz) {
<span class="p_chunk">@@ -2824,7 +2827,8 @@</span> <span class="p_context"> unlock:</span>
 		if (address &lt; end &amp;&amp; !ref_page)
 			goto again;
 	}
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	tlb_end_vma(tlb, vma);
 }
 
<span class="p_chunk">@@ -3003,8 +3007,8 @@</span> <span class="p_context"> retry_avoidcopy:</span>
 
 	mmun_start = address &amp; huge_page_mask(h);
 	mmun_end = mmun_start + huge_page_size(h);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_del">-</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 	/*
 	 * Retake the page table lock to check for racing updates
 	 * before the page tables are altered
<span class="p_chunk">@@ -3025,7 +3029,8 @@</span> <span class="p_context"> retry_avoidcopy:</span>
 		new_page = old_page;
 	}
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_MIGRATE);</span>
 out_release_all:
 	page_cache_release(new_page);
 out_release_old:
<span class="p_chunk">@@ -3493,7 +3498,7 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 	BUG_ON(address &gt;= end);
 	flush_cache_range(vma, address, end);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MPROT);</span>
 	i_mmap_lock_write(vma-&gt;vm_file-&gt;f_mapping);
 	for (; address &lt; end; address += huge_page_size(h)) {
 		spinlock_t *ptl;
<span class="p_chunk">@@ -3543,7 +3548,7 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 	flush_tlb_range(vma, start, end);
 	mmu_notifier_invalidate_range(mm, start, end);
 	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MPROT);</span>
 
 	return pages &lt;&lt; h-&gt;order;
 }
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index bc7be0e..76f167c 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -872,7 +872,8 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	mmun_start = addr;
 	mmun_end   = addr + PAGE_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_WRITE_PROTECT);</span>
 
 	ptep = page_check_address(page, mm, addr, &amp;ptl, 0);
 	if (!ptep)
<span class="p_chunk">@@ -904,7 +905,7 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 		if (pte_dirty(entry))
 			set_page_dirty(page);
 		entry = pte_mkclean(pte_wrprotect(entry));
<span class="p_del">-		set_pte_at_notify(mm, addr, ptep, entry);</span>
<span class="p_add">+		set_pte_at_notify(mm, addr, ptep, entry, MMU_WRITE_PROTECT);</span>
 	}
 	*orig_pte = *ptep;
 	err = 0;
<span class="p_chunk">@@ -912,7 +913,8 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 out_unlock:
 	pte_unmap_unlock(ptep, ptl);
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_WRITE_PROTECT);</span>
 out:
 	return err;
 }
<span class="p_chunk">@@ -948,7 +950,8 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	mmun_start = addr;
 	mmun_end   = addr + PAGE_SIZE;
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					    MMU_MIGRATE);</span>
 
 	ptep = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);
 	if (!pte_same(*ptep, orig_pte)) {
<span class="p_chunk">@@ -961,7 +964,9 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
 	ptep_clear_flush_notify(vma, addr, ptep);
<span class="p_del">-	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));</span>
<span class="p_add">+	set_pte_at_notify(mm, addr, ptep,</span>
<span class="p_add">+			  mk_pte(kpage, vma-&gt;vm_page_prot),</span>
<span class="p_add">+			  MMU_MIGRATE);</span>
 
 	page_remove_rmap(page);
 	if (!page_mapped(page))
<span class="p_chunk">@@ -971,7 +976,8 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 	pte_unmap_unlock(ptep, ptl);
 	err = 0;
 out_mn:
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end,</span>
<span class="p_add">+					  MMU_MIGRATE);</span>
 out:
 	return err;
 }
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 22e8f0c..b90ba3d 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -405,9 +405,9 @@</span> <span class="p_context"> static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
 	tlb_gather_mmu(&amp;tlb, mm, start, end);
 	update_hiwater_rss(mm);
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MUNMAP);</span>
 	madvise_free_page_range(&amp;tlb, vma, start, end);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MUNMAP);</span>
 	tlb_finish_mmu(&amp;tlb, start, end);
 
 	return 0;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index d1fa0c1..9300fad 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1048,7 +1048,7 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 	mmun_end   = end;
 	if (is_cow)
 		mmu_notifier_invalidate_range_start(src_mm, mmun_start,
<span class="p_del">-						    mmun_end);</span>
<span class="p_add">+						    mmun_end, MMU_FORK);</span>
 
 	ret = 0;
 	dst_pgd = pgd_offset(dst_mm, addr);
<span class="p_chunk">@@ -1065,7 +1065,8 @@</span> <span class="p_context"> int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
 
 	if (is_cow)
<span class="p_del">-		mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(src_mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_FORK);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1335,10 +1336,12 @@</span> <span class="p_context"> void unmap_vmas(struct mmu_gather *tlb,</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start_addr,</span>
<span class="p_add">+					    end_addr, MMU_MUNMAP);</span>
 	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end_addr; vma = vma-&gt;vm_next)
 		unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start_addr,</span>
<span class="p_add">+					  end_addr, MMU_MUNMAP);</span>
 }
 
 /**
<span class="p_chunk">@@ -1360,10 +1363,10 @@</span> <span class="p_context"> void zap_page_range(struct vm_area_struct *vma, unsigned long start,</span>
 	lru_add_drain();
 	tlb_gather_mmu(&amp;tlb, mm, start, end);
 	update_hiwater_rss(mm);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, start, end, MMU_MIGRATE);</span>
 	for ( ; vma &amp;&amp; vma-&gt;vm_start &lt; end; vma = vma-&gt;vm_next)
 		unmap_single_vma(&amp;tlb, vma, start, end, details);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, start, end, MMU_MIGRATE);</span>
 	tlb_finish_mmu(&amp;tlb, start, end);
 }
 
<span class="p_chunk">@@ -1386,9 +1389,9 @@</span> <span class="p_context"> static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr</span>
 	lru_add_drain();
 	tlb_gather_mmu(&amp;tlb, mm, address, end);
 	update_hiwater_rss(mm);
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, address, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, address, end, MMU_MUNMAP);</span>
 	unmap_single_vma(&amp;tlb, vma, address, end, details);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, address, end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, address, end, MMU_MUNMAP);</span>
 	tlb_finish_mmu(&amp;tlb, address, end);
 }
 
<span class="p_chunk">@@ -2086,7 +2089,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &amp;memcg))
 		goto oom_free_new;
 
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 
 	/*
 	 * Re-check the pte - we dropped the lock
<span class="p_chunk">@@ -2119,7 +2123,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * mmu page tables (such as kvm shadow page tables), we want the
 		 * new page to be mapped directly into the secondary page table.
 		 */
<span class="p_del">-		set_pte_at_notify(mm, address, page_table, entry);</span>
<span class="p_add">+		set_pte_at_notify(mm, address, page_table, entry, MMU_MIGRATE);</span>
 		update_mmu_cache(vma, address, page_table);
 		if (old_page) {
 			/*
<span class="p_chunk">@@ -2158,7 +2162,8 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		page_cache_release(new_page);
 
 	pte_unmap_unlock(page_table, ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 	if (old_page) {
 		/*
 		 * Don&#39;t let another task, with possibly unlocked vma,
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 236ee25..ad9a55a 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1759,12 +1759,14 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	WARN_ON(PageLRU(new_page));
 
 	/* Recheck the target PMD */
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {
 fail_putback:
 		spin_unlock(ptl);
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+						  mmun_end, MMU_MIGRATE);</span>
 
 		/* Reverse changes made by migrate_page_copy() */
 		if (TestClearPageActive(new_page))
<span class="p_chunk">@@ -1818,7 +1820,8 @@</span> <span class="p_context"> fail_putback:</span>
 	page_remove_rmap(page);
 
 	spin_unlock(ptl);
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	/* Take an &quot;isolate&quot; reference and put new page on the LRU. */
 	get_page(new_page);
<span class="p_header">diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="p_header">index 3b9b3d0..e51ea02 100644</span>
<span class="p_header">--- a/mm/mmu_notifier.c</span>
<span class="p_header">+++ b/mm/mmu_notifier.c</span>
<span class="p_chunk">@@ -142,8 +142,10 @@</span> <span class="p_context"> int __mmu_notifier_test_young(struct mm_struct *mm,</span>
 	return young;
 }
 
<span class="p_del">-void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
<span class="p_del">-			       pte_t pte)</span>
<span class="p_add">+void __mmu_notifier_change_pte(struct mm_struct *mm,</span>
<span class="p_add">+			       unsigned long address,</span>
<span class="p_add">+			       pte_t pte,</span>
<span class="p_add">+			       enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -151,13 +153,14 @@</span> <span class="p_context"> void __mmu_notifier_change_pte(struct mm_struct *mm, unsigned long address,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;change_pte)
<span class="p_del">-			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte);</span>
<span class="p_add">+			mn-&gt;ops-&gt;change_pte(mn, mm, address, pte, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 
 void __mmu_notifier_invalidate_page(struct mm_struct *mm,
<span class="p_del">-					  unsigned long address)</span>
<span class="p_add">+				    unsigned long address,</span>
<span class="p_add">+				    enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -165,13 +168,16 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_page(struct mm_struct *mm,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;invalidate_page)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_page(mn, mm, address);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_page(mn, mm, address, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 
 void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+					   unsigned long start,</span>
<span class="p_add">+					   unsigned long end,</span>
<span class="p_add">+					   enum mmu_event event)</span>
<span class="p_add">+</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -179,14 +185,17 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_range_start(struct mm_struct *mm,</span>
 	id = srcu_read_lock(&amp;srcu);
 	hlist_for_each_entry_rcu(mn, &amp;mm-&gt;mmu_notifier_mm-&gt;list, hlist) {
 		if (mn-&gt;ops-&gt;invalidate_range_start)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start, end);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_range_start(mn, mm, start,</span>
<span class="p_add">+							end, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
 EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range_start);
 
 void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,
<span class="p_del">-				  unsigned long start, unsigned long end)</span>
<span class="p_add">+					 unsigned long start,</span>
<span class="p_add">+					 unsigned long end,</span>
<span class="p_add">+					 enum mmu_event event)</span>
 {
 	struct mmu_notifier *mn;
 	int id;
<span class="p_chunk">@@ -204,7 +213,8 @@</span> <span class="p_context"> void __mmu_notifier_invalidate_range_end(struct mm_struct *mm,</span>
 		if (mn-&gt;ops-&gt;invalidate_range)
 			mn-&gt;ops-&gt;invalidate_range(mn, mm, start, end);
 		if (mn-&gt;ops-&gt;invalidate_range_end)
<span class="p_del">-			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start, end);</span>
<span class="p_add">+			mn-&gt;ops-&gt;invalidate_range_end(mn, mm, start,</span>
<span class="p_add">+						      end, event);</span>
 	}
 	srcu_read_unlock(&amp;srcu, id);
 }
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index e7d6f11..a57e8af 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -155,7 +155,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		/* invoke the mmu notifier if the pmd is populated */
 		if (!mni_start) {
 			mni_start = addr;
<span class="p_del">-			mmu_notifier_invalidate_range_start(mm, mni_start, end);</span>
<span class="p_add">+			mmu_notifier_invalidate_range_start(mm, mni_start,</span>
<span class="p_add">+							    end, MMU_MPROT);</span>
 		}
 
 		if (pmd_trans_huge(*pmd)) {
<span class="p_chunk">@@ -183,7 +184,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 	} while (pmd++, addr = next, addr != end);
 
 	if (mni_start)
<span class="p_del">-		mmu_notifier_invalidate_range_end(mm, mni_start, end);</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, mni_start, end,</span>
<span class="p_add">+						  MMU_MPROT);</span>
 
 	if (nr_huge_updates)
 		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index a7c93ec..72051cf 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -176,7 +176,8 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 
 	mmun_start = old_addr;
 	mmun_end   = old_end;
<span class="p_del">-	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(vma-&gt;vm_mm, mmun_start,</span>
<span class="p_add">+					    mmun_end, MMU_MIGRATE);</span>
 
 	for (; old_addr &lt; old_end; old_addr += extent, new_addr += extent) {
 		cond_resched();
<span class="p_chunk">@@ -228,7 +229,8 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 	if (likely(need_flush))
 		flush_tlb_range(vma, old_end-len, old_addr);
 
<span class="p_del">-	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start, mmun_end);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, mmun_start,</span>
<span class="p_add">+					  mmun_end, MMU_MIGRATE);</span>
 
 	return len + old_addr - old_end;	/* how much done */
 }
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 9c04594..74c51e0 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -915,7 +915,7 @@</span> <span class="p_context"> static int page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
 	pte_unmap_unlock(pte, ptl);
 
 	if (ret) {
<span class="p_del">-		mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		mmu_notifier_invalidate_page(mm, address, MMU_WRITE_BACK);</span>
 		(*cleaned)++;
 	}
 out:
<span class="p_chunk">@@ -1338,7 +1338,7 @@</span> <span class="p_context"> discard:</span>
 out_unmap:
 	pte_unmap_unlock(pte, ptl);
 	if (ret != SWAP_FAIL &amp;&amp; !(flags &amp; TTU_MUNLOCK))
<span class="p_del">-		mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+		mmu_notifier_invalidate_page(mm, address, MMU_MIGRATE);</span>
 out:
 	return ret;
 
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index f202c40..d0b1060 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -260,7 +260,8 @@</span> <span class="p_context"> static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)</span>
 
 static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
 					     struct mm_struct *mm,
<span class="p_del">-					     unsigned long address)</span>
<span class="p_add">+					     unsigned long address,</span>
<span class="p_add">+					     enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int need_tlb_flush, idx;
<span class="p_chunk">@@ -302,7 +303,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,
 					struct mm_struct *mm,
 					unsigned long address,
<span class="p_del">-					pte_t pte)</span>
<span class="p_add">+					pte_t pte,</span>
<span class="p_add">+					enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int idx;
<span class="p_chunk">@@ -318,7 +320,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_change_pte(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
 						    struct mm_struct *mm,
 						    unsigned long start,
<span class="p_del">-						    unsigned long end)</span>
<span class="p_add">+						    unsigned long end,</span>
<span class="p_add">+						    enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int need_tlb_flush = 0, idx;
<span class="p_chunk">@@ -344,7 +347,8 @@</span> <span class="p_context"> static void kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,</span>
 static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
 						  struct mm_struct *mm,
 						  unsigned long start,
<span class="p_del">-						  unsigned long end)</span>
<span class="p_add">+						  unsigned long end,</span>
<span class="p_add">+						  enum mmu_event event)</span>
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



