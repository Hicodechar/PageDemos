
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] x86/arch_prctl: add ARCH_SET_{COMPAT,NATIVE} to change compatible mode - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] x86/arch_prctl: add ARCH_SET_{COMPAT,NATIVE} to change compatible mode</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 8, 2016, 1:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;5707B70F.9080402@virtuozzo.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8783691/mbox/"
   >mbox</a>
|
   <a href="/patch/8783691/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8783691/">/patch/8783691/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id EE655C0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Apr 2016 13:51:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 0233F202E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Apr 2016 13:51:56 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id F305D202B8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Apr 2016 13:51:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756495AbcDHNvW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 8 Apr 2016 09:51:22 -0400
Received: from mail-am1on0143.outbound.protection.outlook.com
	([157.56.112.143]:30318
	&quot;EHLO emea01-am1-obe.outbound.protection.outlook.com&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1751757AbcDHNvT (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 8 Apr 2016 09:51:19 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=virtuozzo.com;
	s=selector1;
	h=From:To:Date:Subject:Message-ID:Content-Type:MIME-Version; 
	bh=RgwGfZU1H8SnRWnefw+511/3Ds0FBQjXyjQvzbiyMVw=;
	b=fq6ROn1OHCXyOLQG9etSp2eqhSWSj+2QbwjcqGCZ0jWL0BgNP47v+abtqBaRStVAOuP29KDx47eNxtnpndr7OgkVbcz8psfoX747XbrsZ8UPZtXE1LwtMfd2eZ1czDSzq54nls7C9bmA4tGtJ46NV3twDYzFg16hO5+ryNq/6fI=
Authentication-Results: virtuozzo.com; dkim=none (message not signed)
	header.d=none;virtuozzo.com; dmarc=none action=none
	header.from=virtuozzo.com;
Received: from [10.30.26.154] (195.214.232.10) by
	VI1PR08MB0990.eurprd08.prod.outlook.com (10.166.143.152) with
	Microsoft SMTP
	Server (TLS) id 15.1.453.26; Fri, 8 Apr 2016 13:51:12 +0000
Subject: Re: [PATCH 1/2] x86/arch_prctl: add ARCH_SET_{COMPAT,NATIVE} to
	change compatible mode
To: Andy Lutomirski &lt;luto@amacapital.net&gt;
References: &lt;1459960170-4454-1-git-send-email-dsafonov@virtuozzo.com&gt;
	&lt;1459960170-4454-2-git-send-email-dsafonov@virtuozzo.com&gt;
	&lt;CALCETrUhLVmHtEnt-RFMdPJ3E2T_798iWvkM8ciw=pydWCWjbw@mail.gmail.com&gt;
	&lt;57064E6C.2030202@virtuozzo.com&gt;
	&lt;CALCETrVNTC9DmKbrYvEDLiAa3s+ZQNyCChk7Y1F6cw6Qv3OuJg@mail.gmail.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Dmitry Safonov &lt;0x7f454c46@gmail.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Shuah Khan &lt;shuahkh@osg.samsung.com&gt;,
	Borislav Petkov &lt;bp@alien8.de&gt;, X86 ML &lt;x86@kernel.org&gt;,
	&lt;khorenko@virtuozzo.com&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	&lt;xemul@virtuozzo.com&gt;, &lt;linux-kselftest@vger.kernel.org&gt;,
	Cyrill Gorcunov &lt;gorcunov@openvz.org&gt;,
	&quot;linux-kernel@vger.kernel.org&quot; &lt;linux-kernel@vger.kernel.org&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
From: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
Message-ID: &lt;5707B70F.9080402@virtuozzo.com&gt;
Date: Fri, 8 Apr 2016 16:50:07 +0300
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:38.0) Gecko/20100101
	Thunderbird/38.7.0
MIME-Version: 1.0
In-Reply-To: &lt;CALCETrVNTC9DmKbrYvEDLiAa3s+ZQNyCChk7Y1F6cw6Qv3OuJg@mail.gmail.com&gt;
Content-Type: multipart/mixed;
	boundary=&quot;------------020505090309040806040800&quot;
X-Originating-IP: [195.214.232.10]
X-ClientProxiedBy: HE1PR02CA0080.eurprd02.prod.outlook.com (10.163.170.48) To
	VI1PR08MB0990.eurprd08.prod.outlook.com (10.166.143.152)
X-MS-Office365-Filtering-Correlation-Id: b26d995a-f009-41d5-34f0-08d35fb4da53
X-Microsoft-Exchange-Diagnostics: 1; VI1PR08MB0990;
	2:ijIJk+trwreElkax/Yatoi8QSLSZo9N7ezyISMo1p1GPz7Jty+Rz3O1lMOCArBDML40on4E+LTjH+hDvOOxpZpCXktMej5UytZwHH4VTAVkHpXlNkqclmmxtBcRIgCAbTrem6NDGtbrHJhC/q6HnRweNWFldzvEs/HUdr5/4aGtVaSAzXLAzzQKDMP4oUgpW;
	3:AjoApHws+mFrw/Ybv4B2buB74eQ8ZV6T5lyU9sVxC1MihxP3PJCk74ARbIw6AFRNUoAHDKCOHKRWd4X5loSwZNrfhXpq5MtHb6JljhYQiykoO742sq1xQ3AsPTtywS6l
X-Microsoft-Antispam: UriScan:;BCL:0;PCL:0;RULEID:;SRVR:VI1PR08MB0990;
X-Microsoft-Exchange-Diagnostics: 1; VI1PR08MB0990;
	25:tZT2lB+ArlKZWACH/FAVAD5tF+MMTLInFuJdHFH+eMfiUTu2whmOiD7dseySaWr/TXFO+upXWNmdblVob02Ft2HLP+w9ZaOQXUEVjCNNuv4aDllFaZLvYrvsZzTc8E4JXv47NaoVvevNQ4ZIN4knR77IPr2IrsiMaoKdlyVrLBHb5Xy+FDNI/TH6o2kmcEfSgfso/TH8VKXw0esJMSABzXEL3cNCSIlg5OrZSpdIq5rCt/FSI94xMNhkSgAYoQ9VM5lWbnGCiRXSK24gZRkswjmY24K32GD1ik2I8dKsR2KbA9vqRxj3uchWkrFh/jbIx1l4wgkNMeIBZKav2CFcRUyXiweg3hv7dUcYgrcxg77FB5VTBE5AdSPPvQRVDMiF0cz+ztB52vUuW3MaltMIDF8Jx4NrwcHipGqajb+c5yf4ddH7NonIRh/7ZP9HXK7eNeMRfO2B/7zRAdvU8VSY/mYf5ZY3ZeP4t8GWaF0AW1eGWiccu3PSORnCeUc89ot4HtrQ4j906s1C5//OfRXcCGYMQN8LAthu9gcuKxQFn/4ipdu90BIF+W7TnUEmmi/ecM8EDfna4R+324Jx8Ft3u4hpHpEY3LqRKahUSalmEda/E+Lhzr0g5psO9/18UVX4qCR1fUVtsKoA6hvA8kAU4Rf4QxK/VS8lhIZ/ZjO1thbpjyV2vM5VEybmhfSeHTHBDLlBJb3J7qwzLnTAIqmnKfS95ErU/9kDR/bEbWFnLS8=
X-Microsoft-Antispam-PRVS: &lt;VI1PR08MB09904669CBA3437B82B2897FD1910@VI1PR08MB0990.eurprd08.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:;
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(102415293)(102615271)(6040074)(601004)(2401047)(5005006)(8121501046)(10201501046)(3002001)(6041046)(6043046);
	SRVR:VI1PR08MB0990; BCL:0; PCL:0; RULEID:; SRVR:VI1PR08MB0990;
X-Microsoft-Exchange-Diagnostics: 1; VI1PR08MB0990;
	4:suxH9dehaAHS9kmd4mxjDc63TM3HFvkrXpBY98AcYmvp8m58ThLyLZ/gM3yJCb6Gj4gIX1oAsus1+p7/6dFzVbudIqgi9sdpHZ/1ut2OvhCCbuHN2kBZIAGaoZtOIC+6UvVMSLLYeTYqqj1OA3J6UNnT2cpvn+nnegKJPxDNnFylSSmoqwIhj0TMcviGfpYXmK3CNCffu5XXsmW2e0UN3yLOD+YRNMFC0i3SQo3H6ajwwIQzOjZ6Cz3AvuuRgtJA16Uh6j5xqz2hjgCf1KC1qfYWsLJP7q1IW5i0xmV1EXFha8tbOHy4du8PYtdYJR20iwOZGRAHsR6pZTd+j7rzR4eK0I1uweI80uqg4ddv13/NKVyN4gHweJG1XvgoMuAlyjDxBUbh6Fi0+I32uL0IFg9EDIjKMxLtqsc4HD1uY3vopQLAdmWWLQEzutBtqgHXYG87IwzT0EKP8dij52Irng==
X-Forefront-PRVS: 0906E83A25
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10019020)(4630300001)(6049001)(6009001)(76104003)(377454003)(57704003)(24454002)(80316001)(77096005)(59896002)(93886004)(64126003)(5004730100002)(2950100001)(65956001)(65806001)(66066001)(84326002)(512874002)(83506001)(189998001)(110136002)(2906002)(42186005)(76176999)(5890100001)(36756003)(5008740100001)(50986999)(54356999)(92566002)(86362001)(4610100001)(4001350100001)(65816999)(568964002)(164054004)(33656002)(2476003)(4326007)(1096002)(81166005)(270700001)(6116002)(3846002)(87266999)(586003)(142933001);
	DIR:OUT; SFP:1102; SCL:1; SRVR:VI1PR08MB0990; H:[10.30.26.154];
	FPR:; SPF:None; MLV:sfv; LANG:en; 
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; VI1PR08MB0990;
	23:Fd2UEW3gdLN0AbX4gaPJgLIr8CPBf3wrr6Ej2D3BZ?=
	=?us-ascii?Q?yizfosz4lh/0cXGcBFjnu3vPitLpRlMevqbl2e4cBItjR7j72bTWaAngo5PK?=
	=?us-ascii?Q?FZCEDYd4LIrG+KiThrk24wFPcDByVcP6U6BEV9gKPHKOdVnU6SkcSbh0kh4y?=
	=?us-ascii?Q?AaGcoM8Z6Hb3ic3Q7O0HkV3bj8kA6dxrJuI885EVTHM8BFfN/EKm19l+AriH?=
	=?us-ascii?Q?g0mD/EcB9Z8PBd8VYj7IythfqjiVMgvH20e8PZTDl8wVhzn1zCfQDcL5QIeJ?=
	=?us-ascii?Q?tLGzHNQc07YRGRVL+1fTCgJPeXMvd/4ruD93k587jJylzS7eh/Y+QxAsum3P?=
	=?us-ascii?Q?itocTTfbrPP2VHyDEZo0cVgTBO3svnxeHaVeU3hWufEX3bB/TwcRNVy7qUkw?=
	=?us-ascii?Q?nROD/wx4SaGY2DfJwjlRgRCIW6M2BMK52sIScqruTwhYhPIVT4kc5UgjMaA8?=
	=?us-ascii?Q?ckkTjwnJAb+HsTi8T2avGmmIEyejPBKjkhS6pFHRNYJ0isDAxKygSk78SpI5?=
	=?us-ascii?Q?W9vvEQBRTPWyEGPmP3mQC3tpzxNzikpSBRdtS5r2/Jjo8AwlmRj//9tT5MFT?=
	=?us-ascii?Q?P/AdROXeqR/KYkf1jSVhlGHxxtjPV6IH51PtKbEhvm9P0J5DrEitBnQYWNdo?=
	=?us-ascii?Q?MlwV2OXK4ie7teyeeNHny3Ch3AAdFIrjuewD+Io6W4UuQ50Q4tPUxftg8T96?=
	=?us-ascii?Q?iH57elF0sIjtwvrZzz5e+aYRXg8ab1MxbsL9TCGHzrvQ2EkHn908oZXVjHrY?=
	=?us-ascii?Q?eJKFv736j2itDvYGYDVV6LukHd7xEY0TPDOml9fuwo+RWSCqjDbx6OQ68Tdt?=
	=?us-ascii?Q?ZCS/Ys0eumFdNAXUjADeDkrXYl2sBO6wxtj7Sl0tbgcRzy2vrLzcm57nWy3t?=
	=?us-ascii?Q?ndEx08n9HlP6zIm9l7HaZHJtK579jj21SxaAtHFn/S18zxPI2RCEixvUztPE?=
	=?us-ascii?Q?qvkmxTJp25n6CLqUv5MsORsa/dCHI3meFMRzO1AK3yrVTmCqlaXKjTFK+Dsx?=
	=?us-ascii?Q?2ucLWBacY+yJN5SHjN82mdnzN/uGPMPvFDT7RgZbznDH8h6EQtYVdXkKAHso?=
	=?us-ascii?Q?kf6FnxCYTGbU5C1lKbNxf+EgfFMZSuJM71gNyv5IsZyNUDhBWl2+E2Cia0RF?=
	=?us-ascii?Q?g2VWdRpbuDgINxz/Q4wTNF0pf6UvaFnAa31ZJJ/3kVmrhSIoL27HdmPsdRDZ?=
	=?us-ascii?Q?VpnBzknhcOs6xGBpYIhiWImQntojK9DE7ZDHdx1Ns1u7EFdFVNuu0gZlCe9y?=
	=?us-ascii?Q?ZrsNBH4gXbDvwqRRGQ=3D?=
X-Microsoft-Exchange-Diagnostics: 1; VI1PR08MB0990;
	5:7ZBI+v8sCOhBJri74Go8mSUCfULpKrpYAmHDFrugWwQDkFBCS+ijqp6qU9nZQPSy1M2rAdF9ZbtzZsGA+XHEyzOw8Nb1BekDHKBUq3osm+C1x788q5g0FENiNIpaD/VvJwAzwyhogMwCI28gXmrk1A==;
	24:JIWU62qKz6ujfyrumPMdI0+7SwIfxSwMsXmExnd+q9f0TASe9iRdFnZUyBsH/LhXCanpz7v7cuDZoWSe6zgXKHirNe+SPizhP+gnQ212JPw=;
	20:0//NTP4692iB0TBJSUC1FRctplq7xYnwbYVbrpi6mzc220Jz/CJcEPcdIfovO2etIhn41jEzjoegc3IlM5kcB0z2c5jzvC5xdRjy5GKt9T/tVyJq7DROHugBSunrDot3mcQjh0Lj/4NyL5MM+aVyKeMpKMfBlkwMYwAe23cuEyw=
SpamDiagnosticOutput: 1:23
SpamDiagnosticMetadata: NSPM
X-OriginatorOrg: virtuozzo.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 08 Apr 2016 13:51:12.9134
	(UTC)
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-Transport-CrossTenantHeadersStamped: VI1PR08MB0990
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.0 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI, RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 8, 2016, 1:50 p.m.</div>
<pre class="content">
On 04/07/2016 05:39 PM, Andy Lutomirski wrote:
<span class="quote">&gt; For 32-bit, the vdso *must* exist in memory at the address that the</span>
<span class="quote">&gt; kernel thinks it&#39;s at.  Even if you had a pure 32-bit restore stub,</span>
<span class="quote">&gt; you would still need vdso remap, because there&#39;s a chance the vdso</span>
<span class="quote">&gt; could land at an unusable address, say one page off from where you</span>
<span class="quote">&gt; want it.  You couldn&#39;t map a wrapper because there wouldn&#39;t be any</span>
<span class="quote">&gt; space for it without moving the real vdso out of the way.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Remember, you *cannot* mremap() the 32-bit vdso because you will</span>
<span class="quote">&gt; crash.  It works by luck for 64-bit, but it&#39;s plausible that we&#39;d want</span>
<span class="quote">&gt; to change that some day.  (I have awful patches that speed a bunch of</span>
<span class="quote">&gt; things up at the cost of a vdso trampoline for 64-bit code and a bunch</span>
<span class="quote">&gt; of other hacks.  Those patches will never go in for real, but</span>
<span class="quote">&gt; something else might want the ability to use 64-bit vdso trampolines.)</span>
Hello again,
what do you think about attached patch?
I think it should fix landing problem for i386 vdso mremap.
It does not touch fast syscall path, so there should be no
speed regression.
<span class="quote">&gt;&gt; I did remapping for vdso as blob for native x86_64 task differs</span>
<span class="quote">&gt;&gt; to compatible task. So it&#39;s just changing blobs, address value</span>
<span class="quote">&gt;&gt; is there for convenience - I may omit it and just remap</span>
<span class="quote">&gt;&gt; different vdso blob at the same place where was previous vdso.</span>
<span class="quote">&gt;&gt; I&#39;m not sure, why do we need possibility to map 64-bit vdso blob</span>
<span class="quote">&gt;&gt; on native 32-bit builds?</span>
<span class="quote">&gt; That would fail, but I think the API should exist.  But a native</span>
<span class="quote">&gt; 32-bit program should be able to remap the 32-bit vdso.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; IOW, I think you should be able to do, roughly:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; map_new_vdso(VDSO_32BIT, addr);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; on any kernel.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Am I making sense?</span>
I will still work for this interface - just wanted that
usuall mremap to work on vdso mappings.

Thanks,
Dmitry.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 8, 2016, 3:56 p.m.</div>
<pre class="content">
On Fri, Apr 8, 2016 at 6:50 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:
<span class="quote">&gt; On 04/07/2016 05:39 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For 32-bit, the vdso *must* exist in memory at the address that the</span>
<span class="quote">&gt;&gt; kernel thinks it&#39;s at.  Even if you had a pure 32-bit restore stub,</span>
<span class="quote">&gt;&gt; you would still need vdso remap, because there&#39;s a chance the vdso</span>
<span class="quote">&gt;&gt; could land at an unusable address, say one page off from where you</span>
<span class="quote">&gt;&gt; want it.  You couldn&#39;t map a wrapper because there wouldn&#39;t be any</span>
<span class="quote">&gt;&gt; space for it without moving the real vdso out of the way.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Remember, you *cannot* mremap() the 32-bit vdso because you will</span>
<span class="quote">&gt;&gt; crash.  It works by luck for 64-bit, but it&#39;s plausible that we&#39;d want</span>
<span class="quote">&gt;&gt; to change that some day.  (I have awful patches that speed a bunch of</span>
<span class="quote">&gt;&gt; things up at the cost of a vdso trampoline for 64-bit code and a bunch</span>
<span class="quote">&gt;&gt; of other hacks.  Those patches will never go in for real, but</span>
<span class="quote">&gt;&gt; something else might want the ability to use 64-bit vdso trampolines.)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hello again,</span>
<span class="quote">&gt; what do you think about attached patch?</span>
<span class="quote">&gt; I think it should fix landing problem for i386 vdso mremap.</span>
<span class="quote">&gt; It does not touch fast syscall path, so there should be no</span>
<span class="quote">&gt; speed regression.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I did remapping for vdso as blob for native x86_64 task differs</span>
<span class="quote">&gt;&gt;&gt; to compatible task. So it&#39;s just changing blobs, address value</span>
<span class="quote">&gt;&gt;&gt; is there for convenience - I may omit it and just remap</span>
<span class="quote">&gt;&gt;&gt; different vdso blob at the same place where was previous vdso.</span>
<span class="quote">&gt;&gt;&gt; I&#39;m not sure, why do we need possibility to map 64-bit vdso blob</span>
<span class="quote">&gt;&gt;&gt; on native 32-bit builds?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That would fail, but I think the API should exist.  But a native</span>
<span class="quote">&gt;&gt; 32-bit program should be able to remap the 32-bit vdso.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; IOW, I think you should be able to do, roughly:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; map_new_vdso(VDSO_32BIT, addr);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; on any kernel.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Am I making sense?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I will still work for this interface - just wanted that</span>
<span class="quote">&gt; usuall mremap to work on vdso mappings.</span>

For this thing:

+    /* Fixing userspace landing - look at do_fast_syscall_32 */
+    if (current_thread_info()-&gt;status &amp; TS_COMPAT)
+        regs-&gt;ip = (unsigned long)current-&gt;mm-&gt;context.vdso +
+            vdso_image_32.sym_int80_landing_pad;

Either check that ip was where you expected it or simply remove this
code -- user programs that are mremapping the vdso are already playing
with fire and can just use int $0x80 to do it.

Other than that, it looks generally sane.  The .mremap hook didn&#39;t
exist last time I looked at this :)

The main downside of your approach is that it doesn&#39;t allow switching
between the 32-bit, 64-bit, and x32 images.  Also, it requires
awareness of how vvar and vdso line up, whereas a dedicated API could
do the whole thing.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Dmitry.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 8, 2016, 4:18 p.m.</div>
<pre class="content">
On 04/08/2016 06:56 PM, Andy Lutomirski wrote:
<span class="quote">&gt; On Fri, Apr 8, 2016 at 6:50 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Hello again,</span>
<span class="quote">&gt;&gt; what do you think about attached patch?</span>
<span class="quote">&gt;&gt; I think it should fix landing problem for i386 vdso mremap.</span>
<span class="quote">&gt;&gt; It does not touch fast syscall path, so there should be no</span>
<span class="quote">&gt;&gt; speed regression.</span>
<span class="quote">&gt; For this thing:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +    /* Fixing userspace landing - look at do_fast_syscall_32 */</span>
<span class="quote">&gt; +    if (current_thread_info()-&gt;status &amp; TS_COMPAT)</span>
<span class="quote">&gt; +        regs-&gt;ip = (unsigned long)current-&gt;mm-&gt;context.vdso +</span>
<span class="quote">&gt; +            vdso_image_32.sym_int80_landing_pad;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Either check that ip was where you expected it</span>
And if it&#39;s not there - return error?
<span class="quote">&gt;   or simply remove this</span>
<span class="quote">&gt; code -- user programs that are mremapping the vdso are already playing</span>
<span class="quote">&gt; with fire and can just use int $0x80 to do it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Other than that, it looks generally sane.  The .mremap hook didn&#39;t</span>
<span class="quote">&gt; exist last time I looked at this :)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The main downside of your approach is that it doesn&#39;t allow switching</span>
<span class="quote">&gt; between the 32-bit, 64-bit, and x32 images.  Also, it requires</span>
<span class="quote">&gt; awareness of how vvar and vdso line up, whereas a dedicated API could</span>
<span class="quote">&gt; do the whole thing.</span>
Yes, I&#39;m working on it. This patch will only allow moving vdso
image with general mremap - so I could use arch_prctl for
that API, as for native i386 one may move vdso with mremap
and cannot map any other vdso blobs.
Does it sound fine?

So, I have some difficulties with removing TIF_IA32 flag:
it&#39;s checked by perf for interpreting stack frames/instructions
and may be checked out of syscall executing (when tracing
page fault events, for example). I doubt, is it sane to remove
TS_COMPAT instead, leaving TIF_IA32, as for some cases
we need to know if task is compatible outside of syscall&#39;s path?
And the comment in asm/syscall.h says:
<span class="quote"> &gt;  * TIF_IA32 tasks should always have TS_COMPAT set at</span>
<span class="quote"> &gt;  * system call time.</span>
that means, that TS_COMPAT is always set on TIF_IA32, so
is meaningless.
What do you think?

Thanks,
Dmitry.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 8, 2016, 8:44 p.m.</div>
<pre class="content">
On Apr 8, 2016 9:20 AM, &quot;Dmitry Safonov&quot; &lt;dsafonov@virtuozzo.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; On 04/08/2016 06:56 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Fri, Apr 8, 2016 at 6:50 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Hello again,</span>
<span class="quote">&gt;&gt;&gt; what do you think about attached patch?</span>
<span class="quote">&gt;&gt;&gt; I think it should fix landing problem for i386 vdso mremap.</span>
<span class="quote">&gt;&gt;&gt; It does not touch fast syscall path, so there should be no</span>
<span class="quote">&gt;&gt;&gt; speed regression.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For this thing:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +    /* Fixing userspace landing - look at do_fast_syscall_32 */</span>
<span class="quote">&gt;&gt; +    if (current_thread_info()-&gt;status &amp; TS_COMPAT)</span>
<span class="quote">&gt;&gt; +        regs-&gt;ip = (unsigned long)current-&gt;mm-&gt;context.vdso +</span>
<span class="quote">&gt;&gt; +            vdso_image_32.sym_int80_landing_pad;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Either check that ip was where you expected it</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And if it&#39;s not there - return error?</span>

No, just leave IP unchanged.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;   or simply remove this</span>
<span class="quote">&gt;&gt; code -- user programs that are mremapping the vdso are already playing</span>
<span class="quote">&gt;&gt; with fire and can just use int $0x80 to do it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Other than that, it looks generally sane.  The .mremap hook didn&#39;t</span>
<span class="quote">&gt;&gt; exist last time I looked at this :)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The main downside of your approach is that it doesn&#39;t allow switching</span>
<span class="quote">&gt;&gt; between the 32-bit, 64-bit, and x32 images.  Also, it requires</span>
<span class="quote">&gt;&gt; awareness of how vvar and vdso line up, whereas a dedicated API could</span>
<span class="quote">&gt;&gt; do the whole thing.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, I&#39;m working on it. This patch will only allow moving vdso</span>
<span class="quote">&gt; image with general mremap - so I could use arch_prctl for</span>
<span class="quote">&gt; that API, as for native i386 one may move vdso with mremap</span>
<span class="quote">&gt; and cannot map any other vdso blobs.</span>
<span class="quote">&gt; Does it sound fine?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, I have some difficulties with removing TIF_IA32 flag:</span>
<span class="quote">&gt; it&#39;s checked by perf for interpreting stack frames/instructions</span>
<span class="quote">&gt; and may be checked out of syscall executing (when tracing</span>
<span class="quote">&gt; page fault events, for example).</span>

Feel free to ask for help on some of these details.  user_64bit_mode
will be helpful too.
<span class="quote">
&gt; I doubt, is it sane to remove</span>
<span class="quote">&gt; TS_COMPAT instead, leaving TIF_IA32, as for some cases</span>
<span class="quote">&gt; we need to know if task is compatible outside of syscall&#39;s path?</span>

No.  TS_COMPAT is important, and it&#39;s also better behaved than
TIF_IA32 -- it has a very specific meaning: &quot;am I currently executing
a 32-bit syscall&quot;.
<span class="quote">
&gt; And the comment in asm/syscall.h says:</span>
<span class="quote">&gt; &gt;  * TIF_IA32 tasks should always have TS_COMPAT set at</span>
<span class="quote">&gt; &gt;  * system call time.</span>
<span class="quote">&gt; that means, that TS_COMPAT is always set on TIF_IA32, so</span>
<span class="quote">&gt; is meaningless.</span>
<span class="quote">&gt; What do you think?</span>

The comment is wrong :). TS_COMPAT is true on int80 or 32-bit vdso
syscall entries and is false otherwise.  64-bit tasks can use int80
and, with your patches, will be able to use the 32-bit vdso entry as
well.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Dmitry.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137481">Dmitry Safonov</a> - April 9, 2016, 8:06 a.m.</div>
<pre class="content">
2016-04-08 23:44 GMT+03:00 Andy Lutomirski &lt;luto@amacapital.net&gt;:
<span class="quote">&gt; On Apr 8, 2016 9:20 AM, &quot;Dmitry Safonov&quot; &lt;dsafonov@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And if it&#39;s not there - return error?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No, just leave IP unchanged.</span>

Ok, will resend with this fixup.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Feel free to ask for help on some of these details.  user_64bit_mode</span>
<span class="quote">&gt; will be helpful too.</span>

Thanks.
<span class="quote">
&gt;&gt; I doubt, is it sane to remove</span>
<span class="quote">&gt;&gt; TS_COMPAT instead, leaving TIF_IA32, as for some cases</span>
<span class="quote">&gt;&gt; we need to know if task is compatible outside of syscall&#39;s path?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No.  TS_COMPAT is important, and it&#39;s also better behaved than</span>
<span class="quote">&gt; TIF_IA32 -- it has a very specific meaning: &quot;am I currently executing</span>
<span class="quote">&gt; a 32-bit syscall&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The comment is wrong :). TS_COMPAT is true on int80 or 32-bit vdso</span>
<span class="quote">&gt; syscall entries and is false otherwise.  64-bit tasks can use int80</span>
<span class="quote">&gt; and, with your patches, will be able to use the 32-bit vdso entry as</span>
<span class="quote">&gt; well.</span>
<span class="quote">&gt;</span>

Oh, yes, I see what you pointing, thanks, will work on it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 13, 2016, 4:55 p.m.</div>
<pre class="content">
On 04/08/2016 11:44 PM, Andy Lutomirski wrote:
<span class="quote">&gt; Feel free to ask for help on some of these details.  user_64bit_mode</span>
<span class="quote">&gt; will be helpful too.</span>
Hello again,

here are some questions on  TIF_IA32 removal:
- in function intel_pmu_pebs_fixup_ip: there is need to
know if process was it native/compat mode for instruction
interpreter for IP + one instruction fixup. There are
registers, but they are from PEBS, which does not contain
segment descriptors (even for PEBSv3). Other values
are from interrupt regs (look at setup_pebs_sample_data).
So, I guess, we may use user_64bit_mode on interrupt
register set, which will be racy with changing task&#39;s mode,
but quite ok?
- the same with LBR branching: I may got cs value for
user_64bit_mode or all registers set from intel_pmu_handle_irq
and pass it through intel_pmu_lbr_read =&gt; intel_pmu_lbr_filter
to branch_type for instruction decoder, which may
missinterpret opcode for the same racy-mode-switching app.
Is it also fine?
- for coredumping/ptracing, I will change test_thread_flag(TIF_IA32)
by user_64bit_mode(task_pt_regs()) - that looks/should be simple.
It&#39;s also valid as at the moment of coredump or of
PTRACE_GETREGSET task isn&#39;t running.
- I do not know what to do with uprobes - as you noted,
the way it cheks ia32_compat is buggy even now: task that
switches CS to __USER32_CS or back to __USER_CS will have
lousy inserted uprobe in mm.
So, how do we know on insert-time, with which descriptor
will be program on uprobed code?
- for MPX, I guess, tracking which syscall called
mpx_enable_management will work, at least it may be
documented, that before switching, one need to disable mpx.
- perf_reg_abi everywhere is used with current, so it&#39;s
also simple-switching to user_64bit_mode(task_pt_regs(current)).

For the conclusion:
I will send those patches, but I do not know what to do with
uprobes tracing. Could you give an advice what to do with
that?
It seems like, if I do those things, I will only need a way to
change vdso blob, without swapping some compatible flags,
as 64-bit tasks will differ from 32-bit only by the way they
execute syscalls.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 14, 2016, 6:27 p.m.</div>
<pre class="content">
On Wed, Apr 13, 2016 at 9:55 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:
<span class="quote">&gt; On 04/08/2016 11:44 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Feel free to ask for help on some of these details.  user_64bit_mode</span>
<span class="quote">&gt;&gt; will be helpful too.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hello again,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; here are some questions on  TIF_IA32 removal:</span>
<span class="quote">&gt; - in function intel_pmu_pebs_fixup_ip: there is need to</span>
<span class="quote">&gt; know if process was it native/compat mode for instruction</span>
<span class="quote">&gt; interpreter for IP + one instruction fixup. There are</span>
<span class="quote">&gt; registers, but they are from PEBS, which does not contain</span>
<span class="quote">&gt; segment descriptors (even for PEBSv3). Other values</span>
<span class="quote">&gt; are from interrupt regs (look at setup_pebs_sample_data).</span>
<span class="quote">&gt; So, I guess, we may use user_64bit_mode on interrupt</span>
<span class="quote">&gt; register set, which will be racy with changing task&#39;s mode,</span>
<span class="quote">&gt; but quite ok?</span>

Here&#39;s my understanding:

We don&#39;t actually know the mode, and there&#39;s no way we could get it
exactly.  User code could have changed the mode between when the PEBS
event was written and when we got the interrupt, and there&#39;s no way
for us to tell.

The regs passed to the interrupt aren&#39;t particularly helpful -- if we
get the overflow event from kernel mode, the regs will be kernel regs,
not user regs.

What we can do is to the the regs returned by perf_get_regs_user,
which I imagine perf is already doing.  Peter, is this the case?

If necessary, starting in 4.6, I could make the regs-&gt;cs part of
perf_get_regs_user be correct no matter what -- the only funny cases
left are NMI-in-system-call-prologue (there can&#39;t be intervening
interrupts any more other than MCE, and I don&#39;t think we really care
if we report correct PEBS results if we take a machine check in the
middle).
<span class="quote">
&gt; - the same with LBR branching: I may got cs value for</span>
<span class="quote">&gt; user_64bit_mode or all registers set from intel_pmu_handle_irq</span>
<span class="quote">&gt; and pass it through intel_pmu_lbr_read =&gt; intel_pmu_lbr_filter</span>
<span class="quote">&gt; to branch_type for instruction decoder, which may</span>
<span class="quote">&gt; missinterpret opcode for the same racy-mode-switching app.</span>
<span class="quote">&gt; Is it also fine?</span>

Same thing, I think.
<span class="quote">
&gt; - for coredumping/ptracing, I will change test_thread_flag(TIF_IA32)</span>
<span class="quote">&gt; by user_64bit_mode(task_pt_regs()) - that looks/should be simple.</span>
<span class="quote">&gt; It&#39;s also valid as at the moment of coredump or of</span>
<span class="quote">&gt; PTRACE_GETREGSET task isn&#39;t running.</span>

Please cc Oleg Nesterov on that one.
<span class="quote">
&gt; - I do not know what to do with uprobes - as you noted,</span>
<span class="quote">&gt; the way it cheks ia32_compat is buggy even now: task that</span>
<span class="quote">&gt; switches CS to __USER32_CS or back to __USER_CS will have</span>
<span class="quote">&gt; lousy inserted uprobe in mm.</span>

I have no idea, but I&#39;ll look at your patch and maybe have an idea.
Oleg Nesterov might be a good person to ask about that, too.
<span class="quote">
&gt; So, how do we know on insert-time, with which descriptor</span>
<span class="quote">&gt; will be program on uprobed code?</span>
<span class="quote">&gt; - for MPX, I guess, tracking which syscall called</span>
<span class="quote">&gt; mpx_enable_management will work, at least it may be</span>
<span class="quote">&gt; documented, that before switching, one need to disable mpx.</span>

You already have to disable MPX before switching because of hardware
issues, so I wouldn&#39;t worry about it.
<span class="quote">
&gt; - perf_reg_abi everywhere is used with current, so it&#39;s</span>
<span class="quote">&gt; also simple-switching to user_64bit_mode(task_pt_regs(current)).</span>

perf_reg_abi should be better -- see the fancy code in
perf_regs_get_user, which is where it comes from these days, I think.
<span class="quote">
&gt;</span>
<span class="quote">&gt; For the conclusion:</span>
<span class="quote">&gt; I will send those patches, but I do not know what to do with</span>
<span class="quote">&gt; uprobes tracing. Could you give an advice what to do with</span>
<span class="quote">&gt; that?</span>
<span class="quote">&gt; It seems like, if I do those things, I will only need a way to</span>
<span class="quote">&gt; change vdso blob, without swapping some compatible flags,</span>
<span class="quote">&gt; as 64-bit tasks will differ from 32-bit only by the way they</span>
<span class="quote">&gt; execute syscalls.</span>

Fantastic!

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - April 20, 2016, 11:04 a.m.</div>
<pre class="content">
On Thu, Apr 14, 2016 at 11:27:35AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; On Wed, Apr 13, 2016 at 9:55 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt; &gt; On 04/08/2016 11:44 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Feel free to ask for help on some of these details.  user_64bit_mode</span>
<span class="quote">&gt; &gt;&gt; will be helpful too.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Hello again,</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; here are some questions on  TIF_IA32 removal:</span>
<span class="quote">&gt; &gt; - in function intel_pmu_pebs_fixup_ip: there is need to</span>
<span class="quote">&gt; &gt; know if process was it native/compat mode for instruction</span>
<span class="quote">&gt; &gt; interpreter for IP + one instruction fixup. There are</span>
<span class="quote">&gt; &gt; registers, but they are from PEBS, which does not contain</span>
<span class="quote">&gt; &gt; segment descriptors (even for PEBSv3). Other values</span>
<span class="quote">&gt; &gt; are from interrupt regs (look at setup_pebs_sample_data).</span>
<span class="quote">&gt; &gt; So, I guess, we may use user_64bit_mode on interrupt</span>
<span class="quote">&gt; &gt; register set, which will be racy with changing task&#39;s mode,</span>
<span class="quote">&gt; &gt; but quite ok?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here&#39;s my understanding:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We don&#39;t actually know the mode, and there&#39;s no way we could get it</span>
<span class="quote">&gt; exactly.  User code could have changed the mode between when the PEBS</span>
<span class="quote">&gt; event was written and when we got the interrupt, and there&#39;s no way</span>
<span class="quote">&gt; for us to tell.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The regs passed to the interrupt aren&#39;t particularly helpful -- if we</span>
<span class="quote">&gt; get the overflow event from kernel mode, the regs will be kernel regs,</span>
<span class="quote">&gt; not user regs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What we can do is to the the regs returned by perf_get_regs_user,</span>
<span class="quote">&gt; which I imagine perf is already doing.  Peter, is this the case?</span>

*confused*, how is perf_get_regs_user() connected to the PEBS fixup?

Ah, you want to use perf_get_regs_user() instead of task_pt_regs()
because of how an NMI during interrupt entry would mess up the
task_pt_regs() contents.

At that point you can use regs_user-&gt;abi, right?
<span class="quote">
&gt; If necessary, starting in 4.6, I could make the regs-&gt;cs part of</span>
<span class="quote">&gt; perf_get_regs_user be correct no matter what -- the only funny cases</span>
<span class="quote">&gt; left are NMI-in-system-call-prologue (there can&#39;t be intervening</span>
<span class="quote">&gt; interrupts any more other than MCE, and I don&#39;t think we really care</span>
<span class="quote">&gt; if we report correct PEBS results if we take a machine check in the</span>
<span class="quote">&gt; middle).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; - the same with LBR branching: I may got cs value for</span>
<span class="quote">&gt; &gt; user_64bit_mode or all registers set from intel_pmu_handle_irq</span>
<span class="quote">&gt; &gt; and pass it through intel_pmu_lbr_read =&gt; intel_pmu_lbr_filter</span>
<span class="quote">&gt; &gt; to branch_type for instruction decoder, which may</span>
<span class="quote">&gt; &gt; missinterpret opcode for the same racy-mode-switching app.</span>
<span class="quote">&gt; &gt; Is it also fine?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same thing, I think.</span>

Yep, whatever works for PEBS should also work for the LBR case. Both can
handle an occasional failed decode. Esp. if userspace is doing daft
things like changing the mode, you get to keep whatever pieces result
from that.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 20, 2016, 3:40 p.m.</div>
<pre class="content">
On Wed, Apr 20, 2016 at 4:04 AM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Thu, Apr 14, 2016 at 11:27:35AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Wed, Apr 13, 2016 at 9:55 AM, Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; On 04/08/2016 11:44 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Feel free to ask for help on some of these details.  user_64bit_mode</span>
<span class="quote">&gt;&gt; &gt;&gt; will be helpful too.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Hello again,</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; here are some questions on  TIF_IA32 removal:</span>
<span class="quote">&gt;&gt; &gt; - in function intel_pmu_pebs_fixup_ip: there is need to</span>
<span class="quote">&gt;&gt; &gt; know if process was it native/compat mode for instruction</span>
<span class="quote">&gt;&gt; &gt; interpreter for IP + one instruction fixup. There are</span>
<span class="quote">&gt;&gt; &gt; registers, but they are from PEBS, which does not contain</span>
<span class="quote">&gt;&gt; &gt; segment descriptors (even for PEBSv3). Other values</span>
<span class="quote">&gt;&gt; &gt; are from interrupt regs (look at setup_pebs_sample_data).</span>
<span class="quote">&gt;&gt; &gt; So, I guess, we may use user_64bit_mode on interrupt</span>
<span class="quote">&gt;&gt; &gt; register set, which will be racy with changing task&#39;s mode,</span>
<span class="quote">&gt;&gt; &gt; but quite ok?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Here&#39;s my understanding:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; We don&#39;t actually know the mode, and there&#39;s no way we could get it</span>
<span class="quote">&gt;&gt; exactly.  User code could have changed the mode between when the PEBS</span>
<span class="quote">&gt;&gt; event was written and when we got the interrupt, and there&#39;s no way</span>
<span class="quote">&gt;&gt; for us to tell.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The regs passed to the interrupt aren&#39;t particularly helpful -- if we</span>
<span class="quote">&gt;&gt; get the overflow event from kernel mode, the regs will be kernel regs,</span>
<span class="quote">&gt;&gt; not user regs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What we can do is to the the regs returned by perf_get_regs_user,</span>
<span class="quote">&gt;&gt; which I imagine perf is already doing.  Peter, is this the case?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; *confused*, how is perf_get_regs_user() connected to the PEBS fixup?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ah, you want to use perf_get_regs_user() instead of task_pt_regs()</span>
<span class="quote">&gt; because of how an NMI during interrupt entry would mess up the</span>
<span class="quote">&gt; task_pt_regs() contents.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; At that point you can use regs_user-&gt;abi, right?</span>

Yes, exactly.

Do LBR, PEBS, and similar report user regs or do they merely want to
know the instruction format?  If the latter, I could whip up a tiny
function to do just that (like perf_get_regs_user but just for ABI --
it would be simpler).

[merging some emails]
<span class="quote">
&gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - interrupt/NMI regs</span>
<span class="quote">&gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>

Could you point me to this bit of the code?
<span class="quote">
&gt;</span>
<span class="quote">&gt; to call perf_callchain_user(), which then, ands up calling</span>
<span class="quote">&gt; perf_callchain_user32() which is expected to NO-OP for 64bit userspace.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; If it&#39;s the perf_get_regs_user, then this should be okay, but passing</span>
<span class="quote">&gt;&gt; in the ABI field directly would be even nicer.  If they&#39;re coming from</span>
<span class="quote">&gt;&gt; the overflow interrupt&#39;s regs or current_pt_regs(), could we change</span>
<span class="quote">&gt;&gt; that?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It might also be nice to make sure that we call perf_get_regs_user</span>
<span class="quote">&gt;&gt; exactly once per overflow interrupt -- i.e. we could push it into the</span>
<span class="quote">&gt;&gt; main code rather than the regs sampling code.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The risk there is that we might not need the user regs at all to handle</span>
<span class="quote">&gt; the overflow thingy, so doing it unconditionally would be unwanted.</span>

One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --
certainly much better then one per PEBS record.  One call to get user
ABI per overflow would be even less bad, but at that point, folding it
in to the PEBS code wouldn&#39;t be so bad either.

If I&#39;m understanding this right (a big, big if), if we get a PEBS
overflow while running in user mode, we&#39;ll dump out the user regs (and
call perf_get_regs_user) and all the PEBS entries (subject to
exclude_kernel and with all the decoding magic).  So, in that case, we
call perf_get_user_regs.

If we get a PEBS overflow while running in kernel mode, we&#39;ll report
the kernel regs (if !exclude_kernel) and report the PEBS data as well.
If any of those records are in user mode, then, ideally, we&#39;d invoke
perf_get_regs_user or similar *once* to get the ABI.  Although, if we
can get the user ABI efficiently enough, then maybe we don&#39;t care if
we call it once per PEBS record.

On x86, the only weird cases are NMIs or MCEs that land in the
syscall, syscall32, and sysenter prologues (easy to handle fully
correctly if we care because the IP that we interrupted tells us the
ABI) and the bullshit SYSENTER+TF thing.  Even the latter isn&#39;t so
hard to get right.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - April 20, 2016, 7:05 p.m.</div>
<pre class="content">
On Wed, Apr 20, 2016 at 08:40:23AM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; Do LBR, PEBS, and similar report user regs or do they merely want to</span>
<span class="quote">&gt; know the instruction format?  If the latter, I could whip up a tiny</span>
<span class="quote">&gt; function to do just that (like perf_get_regs_user but just for ABI --</span>
<span class="quote">&gt; it would be simpler).</span>

Just the instruction format, nothing else.
<span class="quote">
&gt; &gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt; &gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt; &gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  - interrupt/NMI regs</span>
<span class="quote">&gt; &gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you point me to this bit of the code?</span>

kernel/events/callchain.c:198
<span class="quote">
&gt; &gt; to call perf_callchain_user(), which then, ands up calling</span>
<span class="quote">&gt; &gt; perf_callchain_user32() which is expected to NO-OP for 64bit userspace.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; If it&#39;s the perf_get_regs_user, then this should be okay, but passing</span>
<span class="quote">&gt; &gt;&gt; in the ABI field directly would be even nicer.  If they&#39;re coming from</span>
<span class="quote">&gt; &gt;&gt; the overflow interrupt&#39;s regs or current_pt_regs(), could we change</span>
<span class="quote">&gt; &gt;&gt; that?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; It might also be nice to make sure that we call perf_get_regs_user</span>
<span class="quote">&gt; &gt;&gt; exactly once per overflow interrupt -- i.e. we could push it into the</span>
<span class="quote">&gt; &gt;&gt; main code rather than the regs sampling code.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The risk there is that we might not need the user regs at all to handle</span>
<span class="quote">&gt; &gt; the overflow thingy, so doing it unconditionally would be unwanted.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --</span>
<span class="quote">&gt; certainly much better then one per PEBS record.  One call to get user</span>
<span class="quote">&gt; ABI per overflow would be even less bad, but at that point, folding it</span>
<span class="quote">&gt; in to the PEBS code wouldn&#39;t be so bad either.</span>

Right; although note that the whole fixup_ip() thing requires a single
record per interrupt (for we need the LBR state for each record in order
to rewind).

Also, HSW+ PEBS doesn&#39;t do the fixup anymore.
<span class="quote">
&gt; If I&#39;m understanding this right (a big, big if), if we get a PEBS</span>
<span class="quote">&gt; overflow while running in user mode, we&#39;ll dump out the user regs (and</span>
<span class="quote">&gt; call perf_get_regs_user) and all the PEBS entries (subject to</span>
<span class="quote">&gt; exclude_kernel and with all the decoding magic).  So, in that case, we</span>
<span class="quote">&gt; call perf_get_user_regs.</span>

We only dump user regs if PERF_SAMPLE_REGS_USER, and in case we hit
userspace userspace with the interrupt we use the interrupt regs; see
perf_sample_regs_user().
<span class="quote">
&gt; If we get a PEBS overflow while running in kernel mode, we&#39;ll report</span>
<span class="quote">&gt; the kernel regs (if !exclude_kernel) and report the PEBS data as well.</span>
<span class="quote">&gt; If any of those records are in user mode, then, ideally, we&#39;d invoke</span>
<span class="quote">&gt; perf_get_regs_user or similar *once* to get the ABI.  Although, if we</span>
<span class="quote">&gt; can get the user ABI efficiently enough, then maybe we don&#39;t care if</span>
<span class="quote">&gt; we call it once per PEBS record.</span>

Right, if we interrupt kernel mode, we&#39;ll call perf_get_regs_user() if
PERF_SAMPLE_REGS_USER (| PERF_SAMPLE_STACK_USER).

The problem here is that the overflow stuff is designed for a single
&#39;event&#39; per interrupt, so passing it data for multiple events is
somewhat icky.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 21, 2016, 7:39 p.m.</div>
<pre class="content">
On Wed, Apr 20, 2016 at 12:05 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Wed, Apr 20, 2016 at 08:40:23AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; Do LBR, PEBS, and similar report user regs or do they merely want to</span>
<span class="quote">&gt;&gt; know the instruction format?  If the latter, I could whip up a tiny</span>
<span class="quote">&gt;&gt; function to do just that (like perf_get_regs_user but just for ABI --</span>
<span class="quote">&gt;&gt; it would be simpler).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Just the instruction format, nothing else.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt;&gt; &gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt;&gt; &gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  - interrupt/NMI regs</span>
<span class="quote">&gt;&gt; &gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Could you point me to this bit of the code?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; kernel/events/callchain.c:198</span>

But that only applies to the callchain code, right?  AFAICS the PEBS
code is invoked through the x86_pmu NMI handler and always gets the
IRQ regs.  Except for this case:

static inline void intel_pmu_drain_pebs_buffer(void)
{
    struct pt_regs regs;

    x86_pmu.drain_pebs(&amp;regs);
}

which seems a bit confused.

I don&#39;t suppose we could arrange to pass something consistent into the
PEBS handlers...

Or is the PEBS code being called from the callchain code somehow?

I haven&#39;t dug in to the LBR code much.
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt; One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --</span>
<span class="quote">&gt;&gt; certainly much better then one per PEBS record.  One call to get user</span>
<span class="quote">&gt;&gt; ABI per overflow would be even less bad, but at that point, folding it</span>
<span class="quote">&gt;&gt; in to the PEBS code wouldn&#39;t be so bad either.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right; although note that the whole fixup_ip() thing requires a single</span>
<span class="quote">&gt; record per interrupt (for we need the LBR state for each record in order</span>
<span class="quote">&gt; to rewind).</span>

So do earlier PEBS events not get rewound?  Or so we just program the
thing to only ever give us one event at a time?
<span class="quote">
&gt;</span>
<span class="quote">&gt; Also, HSW+ PEBS doesn&#39;t do the fixup anymore.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; If I&#39;m understanding this right (a big, big if), if we get a PEBS</span>
<span class="quote">&gt;&gt; overflow while running in user mode, we&#39;ll dump out the user regs (and</span>
<span class="quote">&gt;&gt; call perf_get_regs_user) and all the PEBS entries (subject to</span>
<span class="quote">&gt;&gt; exclude_kernel and with all the decoding magic).  So, in that case, we</span>
<span class="quote">&gt;&gt; call perf_get_user_regs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We only dump user regs if PERF_SAMPLE_REGS_USER, and in case we hit</span>
<span class="quote">&gt; userspace userspace with the interrupt we use the interrupt regs; see</span>
<span class="quote">&gt; perf_sample_regs_user().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; If we get a PEBS overflow while running in kernel mode, we&#39;ll report</span>
<span class="quote">&gt;&gt; the kernel regs (if !exclude_kernel) and report the PEBS data as well.</span>
<span class="quote">&gt;&gt; If any of those records are in user mode, then, ideally, we&#39;d invoke</span>
<span class="quote">&gt;&gt; perf_get_regs_user or similar *once* to get the ABI.  Although, if we</span>
<span class="quote">&gt;&gt; can get the user ABI efficiently enough, then maybe we don&#39;t care if</span>
<span class="quote">&gt;&gt; we call it once per PEBS record.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right, if we interrupt kernel mode, we&#39;ll call perf_get_regs_user() if</span>
<span class="quote">&gt; PERF_SAMPLE_REGS_USER (| PERF_SAMPLE_STACK_USER).</span>

But not in get_perf_callchain.  So we&#39;ll show the correct user *regs*
but not the current user callchain under some conditions, AFAICS.
<span class="quote">
&gt;</span>
<span class="quote">&gt; The problem here is that the overflow stuff is designed for a single</span>
<span class="quote">&gt; &#39;event&#39; per interrupt, so passing it data for multiple events is</span>
<span class="quote">&gt; somewhat icky.</span>

It also seems that there&#39;s a certain amount of confusion as to exactly
what &quot;regs&quot; means in various contexts.  Or at least I&#39;m confused by
it.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - April 21, 2016, 8:12 p.m.</div>
<pre class="content">
On Thu, Apr 21, 2016 at 12:39:42PM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; On Wed, Apr 20, 2016 at 12:05 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt; &gt; On Wed, Apr 20, 2016 at 08:40:23AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">
&gt; &gt;&gt; &gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  - interrupt/NMI regs</span>
<span class="quote">&gt; &gt;&gt; &gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Could you point me to this bit of the code?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; kernel/events/callchain.c:198</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But that only applies to the callchain code, right? </span>

Yes, which is what I thought you were after..
<span class="quote">
&gt; AFAICS the PEBS</span>
<span class="quote">&gt; code is invoked through the x86_pmu NMI handler and always gets the</span>
<span class="quote">&gt; IRQ regs.  Except for this case:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static inline void intel_pmu_drain_pebs_buffer(void)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;     struct pt_regs regs;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     x86_pmu.drain_pebs(&amp;regs);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; which seems a bit confused.</span>

Yes, so that only gets used with &#39;large&#39; pebs, which requires no other
flags than PERF_FRERERUNNING_FLAGS, which precludes the regs set from
being used.

Could definitely use a comment.
<span class="quote">
&gt; I don&#39;t suppose we could arrange to pass something consistent into the</span>
<span class="quote">&gt; PEBS handlers...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or is the PEBS code being called from the callchain code somehow?</span>

No. I think we were/are slightly talking past one another.
<span class="quote">
&gt; &gt;&gt; One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --</span>
<span class="quote">&gt; &gt;&gt; certainly much better then one per PEBS record.  One call to get user</span>
<span class="quote">&gt; &gt;&gt; ABI per overflow would be even less bad, but at that point, folding it</span>
<span class="quote">&gt; &gt;&gt; in to the PEBS code wouldn&#39;t be so bad either.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Right; although note that the whole fixup_ip() thing requires a single</span>
<span class="quote">&gt; &gt; record per interrupt (for we need the LBR state for each record in order</span>
<span class="quote">&gt; &gt; to rewind).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So do earlier PEBS events not get rewound?  Or so we just program the</span>
<span class="quote">&gt; thing to only ever give us one event at a time?</span>

The latter; we program PEBS such that it can hold but a single record
and thereby assure we get an interrupt for each record.
<span class="quote">
&gt; &gt; The problem here is that the overflow stuff is designed for a single</span>
<span class="quote">&gt; &gt; &#39;event&#39; per interrupt, so passing it data for multiple events is</span>
<span class="quote">&gt; &gt; somewhat icky.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It also seems that there&#39;s a certain amount of confusion as to exactly</span>
<span class="quote">&gt; what &quot;regs&quot; means in various contexts.  Or at least I&#39;m confused by</span>
<span class="quote">&gt; it.</span>

Yes, there&#39;s too much regs.

Typically &#39;regs&#39; is the &#39;interrrupt&#39;/&#39;event&#39; regs, that is the register
set at eventing time. For sampling hardware PMUs this is NMI/IRQ like
things, for software events this ends up being
perf_arch_fetch_caller_regs().

Then there&#39;s PERF_SAMPLE_REGS_USER|PERF_SAMPLE_STACK_USER, which, for
each event with it set, use perf_get_regs_user() to dump the thing into
our ringbuffer as part of the event record.

And then there&#39;s the callchain code, which first unwinds kernel space if
the &#39;interrupt&#39;/&#39;event&#39; reg set points into the kernel, and then uses
task_pt_regs() (which I think we agree should be perf_get_regs_user())
to obtain the user regs to continue with the user stack unwind.

Finally there&#39;s PERF_SAMPLE_REGS_INTR, which dumps whatever
&#39;interrupt/event&#39; regs we get into the ringbuffer sample record.


Did that help? Or did I confuse you moar?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 21, 2016, 11:27 p.m.</div>
<pre class="content">
On Thu, Apr 21, 2016 at 1:12 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">&gt; On Thu, Apr 21, 2016 at 12:39:42PM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Wed, Apr 20, 2016 at 12:05 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; On Wed, Apr 20, 2016 at 08:40:23AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;  - interrupt/NMI regs</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Could you point me to this bit of the code?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; kernel/events/callchain.c:198</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But that only applies to the callchain code, right?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, which is what I thought you were after..</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; AFAICS the PEBS</span>
<span class="quote">&gt;&gt; code is invoked through the x86_pmu NMI handler and always gets the</span>
<span class="quote">&gt;&gt; IRQ regs.  Except for this case:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; static inline void intel_pmu_drain_pebs_buffer(void)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;     struct pt_regs regs;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;     x86_pmu.drain_pebs(&amp;regs);</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; which seems a bit confused.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, so that only gets used with &#39;large&#39; pebs, which requires no other</span>
<span class="quote">&gt; flags than PERF_FRERERUNNING_FLAGS, which precludes the regs set from</span>
<span class="quote">&gt; being used.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Could definitely use a comment.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; I don&#39;t suppose we could arrange to pass something consistent into the</span>
<span class="quote">&gt;&gt; PEBS handlers...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Or is the PEBS code being called from the callchain code somehow?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No. I think we were/are slightly talking past one another.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --</span>
<span class="quote">&gt;&gt; &gt;&gt; certainly much better then one per PEBS record.  One call to get user</span>
<span class="quote">&gt;&gt; &gt;&gt; ABI per overflow would be even less bad, but at that point, folding it</span>
<span class="quote">&gt;&gt; &gt;&gt; in to the PEBS code wouldn&#39;t be so bad either.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Right; although note that the whole fixup_ip() thing requires a single</span>
<span class="quote">&gt;&gt; &gt; record per interrupt (for we need the LBR state for each record in order</span>
<span class="quote">&gt;&gt; &gt; to rewind).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So do earlier PEBS events not get rewound?  Or so we just program the</span>
<span class="quote">&gt;&gt; thing to only ever give us one event at a time?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The latter; we program PEBS such that it can hold but a single record</span>
<span class="quote">&gt; and thereby assure we get an interrupt for each record.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; The problem here is that the overflow stuff is designed for a single</span>
<span class="quote">&gt;&gt; &gt; &#39;event&#39; per interrupt, so passing it data for multiple events is</span>
<span class="quote">&gt;&gt; &gt; somewhat icky.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It also seems that there&#39;s a certain amount of confusion as to exactly</span>
<span class="quote">&gt;&gt; what &quot;regs&quot; means in various contexts.  Or at least I&#39;m confused by</span>
<span class="quote">&gt;&gt; it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, there&#39;s too much regs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Typically &#39;regs&#39; is the &#39;interrrupt&#39;/&#39;event&#39; regs, that is the register</span>
<span class="quote">&gt; set at eventing time. For sampling hardware PMUs this is NMI/IRQ like</span>
<span class="quote">&gt; things, for software events this ends up being</span>
<span class="quote">&gt; perf_arch_fetch_caller_regs().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Then there&#39;s PERF_SAMPLE_REGS_USER|PERF_SAMPLE_STACK_USER, which, for</span>
<span class="quote">&gt; each event with it set, use perf_get_regs_user() to dump the thing into</span>
<span class="quote">&gt; our ringbuffer as part of the event record.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And then there&#39;s the callchain code, which first unwinds kernel space if</span>
<span class="quote">&gt; the &#39;interrupt&#39;/&#39;event&#39; reg set points into the kernel, and then uses</span>
<span class="quote">&gt; task_pt_regs() (which I think we agree should be perf_get_regs_user())</span>
<span class="quote">&gt; to obtain the user regs to continue with the user stack unwind.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Finally there&#39;s PERF_SAMPLE_REGS_INTR, which dumps whatever</span>
<span class="quote">&gt; &#39;interrupt/event&#39; regs we get into the ringbuffer sample record.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Did that help? Or did I confuse you moar?</span>
<span class="quote">&gt;</span>

I think I&#39;m starting to get it.  What if we rearrange slightly, like this:

perf_sample_data already has a struct perf_regs in it.  We could add a
flags field to the first chunk of perf_sample_data:

u64 sample_flags;

perf_sample_data_init sets sample_flags to zero.

Now we rename perf_sample_regs_user to __perf_sample_regs_user and
make it non-static.  We also teach it to set do data-&gt;sample_flags |=
PERF_SAMPLE_FLAGS_HAS_REGS_USER.  We add:

static void perf_fetch_regs_user(struct perf_sample_data *data, struct
pt_regs *interrupt_regs)
{
  if (data-&gt;sample_flags &amp; PERF_SAMPLE_FLAGS_HAS_REGS_USER)
    return;

  __perf_sample_regs_user(&amp;data-&gt;regs_user, interrupt_regs,
&amp;data-&gt;regs_user_copy);
}

(Hmm.  This only really works well if we can guarantee that
interrupt_regs remains valid for the life of the perf_sample_data
object.  Could we perhaps move the interrupt_regs pointer *into*
perf_sample_data and stop passing it all over the place?)

We change all the callers of perf_sample_regs_user to use
perf_fetch_regs_user instead.

Now we teach the PEBS fixup code to call perf_fetch_regs_user if it
sees a user IP.  Then it can use regs_user-&gt;abi instead of TIF_IA32,
and my original goal of nuking TIF_IA32 can proceed apace.  (Keep in
mind that, if the interrupt refers to user mode, this is very fast.
If the interrupt refers to kernel mode, it&#39;s slower, but that&#39;s
comparatively rare and it&#39;s the case where we actually care.)

There might be one or two other tweaks needed, but I think this should
mostly do the trick.

What do you think?  If you like it, I can probably find some time to
give it a shot, but I don&#39;t guarantee that I won&#39;t miss some subtlety
in its interaction with the rest of the event output code.

On a vaguely related note, why is the big prebs-to-pt_regs copy
conditional on (sample_type &amp; PERF_SAMPLE_REGS_INTR)?  I bet it would
be faster to make it unconditional, because you could avoid copying
over the entire pt_regs struct if PERF_SAMPLE_REGS_INTR isn&#39;t set.

--Andy
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - April 21, 2016, 11:46 p.m.</div>
<pre class="content">
On Thu, Apr 21, 2016 at 4:27 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">&gt; On Thu, Apr 21, 2016 at 1:12 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt;&gt; On Thu, Apr 21, 2016 at 12:39:42PM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; On Wed, Apr 20, 2016 at 12:05 PM, Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt; On Wed, Apr 20, 2016 at 08:40:23AM -0700, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;&gt; Peter, I got lost in the code that calls this.  Are regs coming from</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;&gt; the overflow interrupt&#39;s regs, current_pt_regs(), or</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;&gt; perf_get_regs_user?</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt; So get_perf_callchain() will get regs from:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;  - interrupt/NMI regs</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;  - perf_arch_fetch_caller_regs()</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt; And when user &amp;&amp; !user_mode(), we&#39;ll use:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; &gt;  - task_pt_regs() (which arguably should maybe be perf_get_regs_user())</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; Could you point me to this bit of the code?</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; kernel/events/callchain.c:198</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; But that only applies to the callchain code, right?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Yes, which is what I thought you were after..</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; AFAICS the PEBS</span>
<span class="quote">&gt;&gt;&gt; code is invoked through the x86_pmu NMI handler and always gets the</span>
<span class="quote">&gt;&gt;&gt; IRQ regs.  Except for this case:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; static inline void intel_pmu_drain_pebs_buffer(void)</span>
<span class="quote">&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt;     struct pt_regs regs;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;     x86_pmu.drain_pebs(&amp;regs);</span>
<span class="quote">&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; which seems a bit confused.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Yes, so that only gets used with &#39;large&#39; pebs, which requires no other</span>
<span class="quote">&gt;&gt; flags than PERF_FRERERUNNING_FLAGS, which precludes the regs set from</span>
<span class="quote">&gt;&gt; being used.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Could definitely use a comment.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I don&#39;t suppose we could arrange to pass something consistent into the</span>
<span class="quote">&gt;&gt;&gt; PEBS handlers...</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Or is the PEBS code being called from the callchain code somehow?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; No. I think we were/are slightly talking past one another.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; One call to perf_get_user_regs per interrupt shouldn&#39;t be too bad --</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; certainly much better then one per PEBS record.  One call to get user</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; ABI per overflow would be even less bad, but at that point, folding it</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; in to the PEBS code wouldn&#39;t be so bad either.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; Right; although note that the whole fixup_ip() thing requires a single</span>
<span class="quote">&gt;&gt;&gt; &gt; record per interrupt (for we need the LBR state for each record in order</span>
<span class="quote">&gt;&gt;&gt; &gt; to rewind).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So do earlier PEBS events not get rewound?  Or so we just program the</span>
<span class="quote">&gt;&gt;&gt; thing to only ever give us one event at a time?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The latter; we program PEBS such that it can hold but a single record</span>
<span class="quote">&gt;&gt; and thereby assure we get an interrupt for each record.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; The problem here is that the overflow stuff is designed for a single</span>
<span class="quote">&gt;&gt;&gt; &gt; &#39;event&#39; per interrupt, so passing it data for multiple events is</span>
<span class="quote">&gt;&gt;&gt; &gt; somewhat icky.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It also seems that there&#39;s a certain amount of confusion as to exactly</span>
<span class="quote">&gt;&gt;&gt; what &quot;regs&quot; means in various contexts.  Or at least I&#39;m confused by</span>
<span class="quote">&gt;&gt;&gt; it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Yes, there&#39;s too much regs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Typically &#39;regs&#39; is the &#39;interrrupt&#39;/&#39;event&#39; regs, that is the register</span>
<span class="quote">&gt;&gt; set at eventing time. For sampling hardware PMUs this is NMI/IRQ like</span>
<span class="quote">&gt;&gt; things, for software events this ends up being</span>
<span class="quote">&gt;&gt; perf_arch_fetch_caller_regs().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Then there&#39;s PERF_SAMPLE_REGS_USER|PERF_SAMPLE_STACK_USER, which, for</span>
<span class="quote">&gt;&gt; each event with it set, use perf_get_regs_user() to dump the thing into</span>
<span class="quote">&gt;&gt; our ringbuffer as part of the event record.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And then there&#39;s the callchain code, which first unwinds kernel space if</span>
<span class="quote">&gt;&gt; the &#39;interrupt&#39;/&#39;event&#39; reg set points into the kernel, and then uses</span>
<span class="quote">&gt;&gt; task_pt_regs() (which I think we agree should be perf_get_regs_user())</span>
<span class="quote">&gt;&gt; to obtain the user regs to continue with the user stack unwind.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Finally there&#39;s PERF_SAMPLE_REGS_INTR, which dumps whatever</span>
<span class="quote">&gt;&gt; &#39;interrupt/event&#39; regs we get into the ringbuffer sample record.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Did that help? Or did I confuse you moar?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think I&#39;m starting to get it.  What if we rearrange slightly, like this:</span>
<span class="quote">&gt;</span>

I started fiddling to see what&#39;s involved, then I got to this:

    if (sample_type &amp; PERF_SAMPLE_REGS_INTR) {
        u64 abi = data-&gt;regs_intr.abi;
        /*
         * If there are no regs to dump, notice it through
         * first u64 being zero (PERF_SAMPLE_REGS_ABI_NONE).
         */
        perf_output_put(handle, abi);

        if (abi) {
            u64 mask = event-&gt;attr.sample_regs_intr;

            perf_output_sample_regs(handle,
                        data-&gt;regs_intr.regs,
                        mask);
        }
    }

regs_intr.abi comes from perf_regs_abi(current), which, on x86_64 or
arm64, may indicate 32-bit regs, but the actual regs are always
64-bit.  Am I just confused or is this a bug?

--Andy
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
From ffba027156d7194a07ccccd80ece9be19107e4e6 Mon Sep 17 00:00:00 2001
From: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
Date: Fri, 8 Apr 2016 16:27:18 +0300
Subject: [PATCH] x86/vdso: add mremap hook to vm_special_mapping

This patch adds possibility for userspace 32-bit applications
to move vdso mapping. Previously, when userspace app called
mremap for vdso, in return path it would land on previous
address of vdso page, resulting in segmentation violation.
Now it lands fine and returns to userspace with remapped vdso.

There is still problem for remapping vdso in glibc applications:
linker relocates addresses for syscalls on vdso page, so
you need to relink with the new addresses. Or the next syscall
through glibc may fail:
  Program received signal SIGSEGV, Segmentation fault.
  #0  0xf7fd9b80 in __kernel_vsyscall ()
  #1  0xf7ec8238 in _exit () from /usr/lib32/libc.so.6

Signed-off-by: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
<span class="p_del">---</span>
 arch/x86/entry/vdso/vma.c | 29 ++++++++++++++++++++++++-----
 include/linux/mm_types.h  |  3 +++
 mm/mmap.c                 | 10 ++++++++++
 3 files changed, 37 insertions(+), 5 deletions(-)

<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 10f704584922..ed6c9fa64531 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/random.h&gt;
 #include &lt;linux/elf.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/ptrace.h&gt;</span>
 #include &lt;asm/pvclock.h&gt;
 #include &lt;asm/vgtod.h&gt;
 #include &lt;asm/proto.h&gt;
<span class="p_chunk">@@ -98,10 +99,22 @@</span> <span class="p_context"> static int vdso_fault(const struct vm_special_mapping *sm,</span>
 	return 0;
 }
 
<span class="p_del">-static const struct vm_special_mapping text_mapping = {</span>
<span class="p_del">-	.name = &quot;[vdso]&quot;,</span>
<span class="p_del">-	.fault = vdso_fault,</span>
<span class="p_del">-};</span>
<span class="p_add">+static int vdso_mremap(const struct vm_special_mapping *sm,</span>
<span class="p_add">+		      struct vm_area_struct *new_vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pt_regs *regs = current_pt_regs();</span>
<span class="p_add">+</span>
<span class="p_add">+	new_vma-&gt;vm_mm-&gt;context.vdso = (void __user *)new_vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)</span>
<span class="p_add">+	/* Fixing userspace landing - look at do_fast_syscall_32 */</span>
<span class="p_add">+	if (current_thread_info()-&gt;status &amp; TS_COMPAT)</span>
<span class="p_add">+		regs-&gt;ip = (unsigned long)current-&gt;mm-&gt;context.vdso +</span>
<span class="p_add">+			vdso_image_32.sym_int80_landing_pad;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
 static int vvar_fault(const struct vm_special_mapping *sm,
 		      struct vm_area_struct *vma, struct vm_fault *vmf)
<span class="p_chunk">@@ -162,6 +175,12 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 	struct vm_area_struct *vma;
 	unsigned long addr, text_start;
 	int ret = 0;
<span class="p_add">+</span>
<span class="p_add">+	static const struct vm_special_mapping vdso_mapping = {</span>
<span class="p_add">+		.name = &quot;[vdso]&quot;,</span>
<span class="p_add">+		.fault = vdso_fault,</span>
<span class="p_add">+		.mremap = vdso_mremap,</span>
<span class="p_add">+	};</span>
 	static const struct vm_special_mapping vvar_mapping = {
 		.name = &quot;[vvar]&quot;,
 		.fault = vvar_fault,
<span class="p_chunk">@@ -195,7 +214,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 				       image-&gt;size,
 				       VM_READ|VM_EXEC|
 				       VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
<span class="p_del">-				       &amp;text_mapping);</span>
<span class="p_add">+				       &amp;vdso_mapping);</span>
 
 	if (IS_ERR(vma)) {
 		ret = PTR_ERR(vma);
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index c2d75b4fa86c..4d16ab9287af 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -586,6 +586,9 @@</span> <span class="p_context"> struct vm_special_mapping {</span>
 	int (*fault)(const struct vm_special_mapping *sm,
 		     struct vm_area_struct *vma,
 		     struct vm_fault *vmf);
<span class="p_add">+</span>
<span class="p_add">+	int (*mremap)(const struct vm_special_mapping *sm,</span>
<span class="p_add">+		     struct vm_area_struct *new_vma);</span>
 };
 
 enum tlb_flush_reason {
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index bd2e1a533bc1..ba71658dd1a1 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2930,9 +2930,19 @@</span> <span class="p_context"> static const char *special_mapping_name(struct vm_area_struct *vma)</span>
 	return ((struct vm_special_mapping *)vma-&gt;vm_private_data)-&gt;name;
 }
 
<span class="p_add">+static int special_mapping_mremap(struct vm_area_struct *new_vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_special_mapping *sm = new_vma-&gt;vm_private_data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sm-&gt;mremap)</span>
<span class="p_add">+		return sm-&gt;mremap(sm, new_vma);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct vm_operations_struct special_mapping_vmops = {
 	.close = special_mapping_close,
 	.fault = special_mapping_fault,
<span class="p_add">+	.mremap = special_mapping_mremap,</span>
 	.name = special_mapping_name,
 };
 
<span class="p_del">-- </span>
2.8.0


</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



