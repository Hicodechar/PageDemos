
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>KVM: x86: remove code for lazy FPU handling - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    KVM: x86: remove code for lazy FPU handling</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 16, 2017, 9:33 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1487237595-61928-1-git-send-email-pbonzini@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9576725/mbox/"
   >mbox</a>
|
   <a href="/patch/9576725/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9576725/">/patch/9576725/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E299E600C5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Feb 2017 09:33:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D9F5A2858D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Feb 2017 09:33:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id CD18D285AB; Thu, 16 Feb 2017 09:33:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DFF752858D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Feb 2017 09:33:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754217AbdBPJd2 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Feb 2017 04:33:28 -0500
Received: from mail-wr0-f196.google.com ([209.85.128.196]:36172 &quot;EHLO
	mail-wr0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753941AbdBPJdV (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Feb 2017 04:33:21 -0500
Received: by mail-wr0-f196.google.com with SMTP id z61so520107wrc.3;
	Thu, 16 Feb 2017 01:33:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:from:to:subject:date:message-id;
	bh=AmPZ4uCc/o41hdJG1xQGM3m5VT9khsASIsvvMLrVNmc=;
	b=OL3Atfvt3mWpbum2fi1o+hGnJMtUIo4VccJ4fKTy+aUnpUUH02qozt+IT38WbOWkSb
	i+ZJJT3uU62g2d4BcuoyQgf6fazo/X3yMEnVwIHu/pz5bNjUyb+S+dfUgMWFijYw4xPx
	XVbaeL1n65F6sXTpQX8LpeP9iiAb2A/JS9Y1eyU35a7/wt6DHTrKvf7B8ovI+2QULqa0
	OfsCSh9RP56ZpJHP9NW2bTjI6KtfF0xH+pTNb09kCvw1gU04f7NZJBonWnQfShLY2bvu
	NWeC4fGE5IXlxuVBsGBe2hFlFnJwRHKDgab1S7pNl80I0lN1e/wvC2zaJCbdJG5uzINf
	8B6g==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:from:to:subject:date:message-id;
	bh=AmPZ4uCc/o41hdJG1xQGM3m5VT9khsASIsvvMLrVNmc=;
	b=GcEyK+JmyvxSXVKXcWFeltfwd+Jd5nMCTMPf8xMgcnQbxwVg2/ImXkF3OWRz6t34P1
	T7YF+cjq5CbzC8McERsEQutSt46OCWWnrKvBGsrVGYtGQ3YluKe2EOS5l9bgNytfD7rC
	vfvl0ulPOlm9wuVWT1imGs9d7YaJjfomImCYnQXOio9mQJlBxxO3So1vSQGnsQs2CJx6
	MDn36vep+oZ80EpcZRYwPJRNjKnKpGSiuDgVikZAYmytxYuJgX1bH2xBHWApVbbdHD2Q
	jG+8IAD+ECtmiaWbgKJ+WtEiY+WO62A4nRlUTRm11y3HMopL1WWoeijYw5ni7P3U7HPW
	cfxA==
X-Gm-Message-State: AMke39m5VvoQkBslj8xr+HRxlOo83tV/FH0CaeKProGXa1cBWF7ojZivjo3pNdeEQihIgQ==
X-Received: by 10.223.133.226 with SMTP id 31mr1207949wru.137.1487237599367; 
	Thu, 16 Feb 2017 01:33:19 -0800 (PST)
Received: from 640k.lan (94-39-187-56.adsl-ull.clienti.tiscali.it.
	[94.39.187.56]) by smtp.gmail.com with ESMTPSA id
	z134sm2722239wmc.20.2017.02.16.01.33.17
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 16 Feb 2017 01:33:18 -0800 (PST)
From: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
To: linux-kernel@vger.kernel.org, kvm@vger.kernel.org
Subject: [PATCH] KVM: x86: remove code for lazy FPU handling
Date: Thu, 16 Feb 2017 10:33:15 +0100
Message-Id: &lt;1487237595-61928-1-git-send-email-pbonzini@redhat.com&gt;
X-Mailer: git-send-email 1.8.3.1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - Feb. 16, 2017, 9:33 a.m.</div>
<pre class="content">
The FPU is always active now when running KVM.
<span class="signed-off-by">
Signed-off-by: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
---
 arch/x86/include/asm/kvm_host.h |   3 --
 arch/x86/kvm/cpuid.c            |   2 -
 arch/x86/kvm/svm.c              |  43 ++-------------
 arch/x86/kvm/vmx.c              | 112 ++++++----------------------------------
 arch/x86/kvm/x86.c              |   7 +--
 include/linux/kvm_host.h        |   1 -
 6 files changed, 19 insertions(+), 149 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=96171">David Matlack</a> - Feb. 16, 2017, 9:23 p.m.</div>
<pre class="content">
On Thu, Feb 16, 2017 at 1:33 AM, Paolo Bonzini &lt;pbonzini@redhat.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; The FPU is always active now when running KVM.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="reviewed-by">
Reviewed-by: David Matlack &lt;dmatlack@google.com&gt;</span>

Glad to see this cleanup! Thanks for doing it.
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/kvm_host.h |   3 --</span>
<span class="quote">&gt;  arch/x86/kvm/cpuid.c            |   2 -</span>
<span class="quote">&gt;  arch/x86/kvm/svm.c              |  43 ++-------------</span>
<span class="quote">&gt;  arch/x86/kvm/vmx.c              | 112 ++++++----------------------------------</span>
<span class="quote">&gt;  arch/x86/kvm/x86.c              |   7 +--</span>
<span class="quote">&gt;  include/linux/kvm_host.h        |   1 -</span>
<span class="quote">&gt;  6 files changed, 19 insertions(+), 149 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; index e4f13e714bcf..74ef58c8ff53 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; @@ -55,7 +55,6 @@</span>
<span class="quote">&gt;  #define KVM_REQ_TRIPLE_FAULT      10</span>
<span class="quote">&gt;  #define KVM_REQ_MMU_SYNC          11</span>
<span class="quote">&gt;  #define KVM_REQ_CLOCK_UPDATE      12</span>
<span class="quote">&gt; -#define KVM_REQ_DEACTIVATE_FPU    13</span>
<span class="quote">&gt;  #define KVM_REQ_EVENT             14</span>
<span class="quote">&gt;  #define KVM_REQ_APF_HALT          15</span>
<span class="quote">&gt;  #define KVM_REQ_STEAL_UPDATE      16</span>
<span class="quote">&gt; @@ -936,8 +935,6 @@ struct kvm_x86_ops {</span>
<span class="quote">&gt;         unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;         void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);</span>
<span class="quote">&gt;         u32 (*get_pkru)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt; -       void (*fpu_activate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt; -       void (*fpu_deactivate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         void (*tlb_flush)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; index c0e2036217ad..1d155cc56629 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; @@ -123,8 +123,6 @@ int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;         if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))</span>
<span class="quote">&gt;                 best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * The existing code assumes virtual address is 48-bit in the canonical</span>
<span class="quote">&gt;          * address checks; exit if it is ever changed.</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; index 4e5905a1ce70..d1efe2c62b3f 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; @@ -1157,7 +1157,6 @@ static void init_vmcb(struct vcpu_svm *svm)</span>
<span class="quote">&gt;         struct vmcb_control_area *control = &amp;svm-&gt;vmcb-&gt;control;</span>
<span class="quote">&gt;         struct vmcb_save_area *save = &amp;svm-&gt;vmcb-&gt;save;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;         svm-&gt;vcpu.arch.hflags = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         set_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt; @@ -1899,15 +1898,12 @@ static void update_cr0_intercept(struct vcpu_svm *svm)</span>
<span class="quote">&gt;         ulong gcr0 = svm-&gt;vcpu.arch.cr0;</span>
<span class="quote">&gt;         u64 *hcr0 = &amp;svm-&gt;vmcb-&gt;save.cr0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!svm-&gt;vcpu.fpu_active)</span>
<span class="quote">&gt; -               *hcr0 |= SVM_CR0_SELECTIVE_MASK;</span>
<span class="quote">&gt; -       else</span>
<span class="quote">&gt; -               *hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt; -                       | (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt; +       *hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt; +               | (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         mark_dirty(svm-&gt;vmcb, VMCB_CR);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (gcr0 == *hcr0 &amp;&amp; svm-&gt;vcpu.fpu_active) {</span>
<span class="quote">&gt; +       if (gcr0 == *hcr0) {</span>
<span class="quote">&gt;                 clr_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;                 clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);</span>
<span class="quote">&gt;         } else {</span>
<span class="quote">&gt; @@ -1938,8 +1934,6 @@ static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;         if (!npt_enabled)</span>
<span class="quote">&gt;                 cr0 |= X86_CR0_PG | X86_CR0_WP;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -               cr0 |= X86_CR0_TS;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * re-enable caching here because the QEMU bios</span>
<span class="quote">&gt;          * does not do it - this results in some delay at</span>
<span class="quote">&gt; @@ -2158,22 +2152,6 @@ static int ac_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;         return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static void svm_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       clr_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt; -       update_cr0_intercept(svm);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static int nm_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       svm_fpu_activate(&amp;svm-&gt;vcpu);</span>
<span class="quote">&gt; -       return 1;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static bool is_erratum_383(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         int err, i;</span>
<span class="quote">&gt; @@ -2571,9 +2549,6 @@ static int nested_svm_exit_special(struct vcpu_svm *svm)</span>
<span class="quote">&gt;                 if (!npt_enabled &amp;&amp; svm-&gt;apf_reason == 0)</span>
<span class="quote">&gt;                         return NESTED_EXIT_HOST;</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt; -       case SVM_EXIT_EXCP_BASE + NM_VECTOR:</span>
<span class="quote">&gt; -               nm_interception(svm);</span>
<span class="quote">&gt; -               break;</span>
<span class="quote">&gt;         default:</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt; @@ -4018,7 +3993,6 @@ static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {</span>
<span class="quote">&gt;         [SVM_EXIT_EXCP_BASE + BP_VECTOR]        = bp_interception,</span>
<span class="quote">&gt;         [SVM_EXIT_EXCP_BASE + UD_VECTOR]        = ud_interception,</span>
<span class="quote">&gt;         [SVM_EXIT_EXCP_BASE + PF_VECTOR]        = pf_interception,</span>
<span class="quote">&gt; -       [SVM_EXIT_EXCP_BASE + NM_VECTOR]        = nm_interception,</span>
<span class="quote">&gt;         [SVM_EXIT_EXCP_BASE + MC_VECTOR]        = mc_interception,</span>
<span class="quote">&gt;         [SVM_EXIT_EXCP_BASE + AC_VECTOR]        = ac_interception,</span>
<span class="quote">&gt;         [SVM_EXIT_INTR]                         = intr_interception,</span>
<span class="quote">&gt; @@ -5072,14 +5046,6 @@ static bool svm_has_wbinvd_exit(void)</span>
<span class="quote">&gt;         return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static void svm_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       set_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt; -       update_cr0_intercept(svm);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #define PRE_EX(exit)  { .exit_code = (exit), \</span>
<span class="quote">&gt;                         .stage = X86_ICPT_PRE_EXCEPT, }</span>
<span class="quote">&gt;  #define POST_EX(exit) { .exit_code = (exit), \</span>
<span class="quote">&gt; @@ -5340,9 +5306,6 @@ static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         .get_pkru = svm_get_pkru,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       .fpu_activate = svm_fpu_activate,</span>
<span class="quote">&gt; -       .fpu_deactivate = svm_fpu_deactivate,</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         .tlb_flush = svm_flush_tlb,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         .run = svm_vcpu_run,</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; index 0e0b5d09597e..9856b73a21ad 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; @@ -1856,7 +1856,7 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;         u32 eb;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |</span>
<span class="quote">&gt; -            (1u &lt;&lt; NM_VECTOR) | (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt; +            (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;         if ((vcpu-&gt;guest_debug &amp;</span>
<span class="quote">&gt;              (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==</span>
<span class="quote">&gt;             (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))</span>
<span class="quote">&gt; @@ -1865,8 +1865,6 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;                 eb = ~0;</span>
<span class="quote">&gt;         if (enable_ept)</span>
<span class="quote">&gt;                 eb &amp;= ~(1u &lt;&lt; PF_VECTOR); /* bypass_guest_pf = 0 */</span>
<span class="quote">&gt; -       if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -               eb &amp;= ~(1u &lt;&lt; NM_VECTOR);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* When we are running a nested L2 guest and L1 specified for it a</span>
<span class="quote">&gt;          * certain exception bitmap, we must trap the same exceptions and pass</span>
<span class="quote">&gt; @@ -2340,25 +2338,6 @@ static void vmx_vcpu_put(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static void vmx_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       ulong cr0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -               return;</span>
<span class="quote">&gt; -       vcpu-&gt;fpu_active = 1;</span>
<span class="quote">&gt; -       cr0 = vmcs_readl(GUEST_CR0);</span>
<span class="quote">&gt; -       cr0 &amp;= ~(X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -       cr0 |= kvm_read_cr0_bits(vcpu, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -       vmcs_writel(GUEST_CR0, cr0);</span>
<span class="quote">&gt; -       update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -       vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt; -       if (is_guest_mode(vcpu))</span>
<span class="quote">&gt; -               vcpu-&gt;arch.cr0_guest_owned_bits &amp;=</span>
<span class="quote">&gt; -                       ~get_vmcs12(vcpu)-&gt;cr0_guest_host_mask;</span>
<span class="quote">&gt; -       vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -2377,33 +2356,6 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)</span>
<span class="quote">&gt;                 (fields-&gt;cr4_read_shadow &amp; fields-&gt;cr4_guest_host_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static void vmx_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       /* Note that there is no vcpu-&gt;fpu_active = 0 here. The caller must</span>
<span class="quote">&gt; -        * set this *before* calling this function.</span>
<span class="quote">&gt; -        */</span>
<span class="quote">&gt; -       vmx_decache_cr0_guest_bits(vcpu);</span>
<span class="quote">&gt; -       vmcs_set_bits(GUEST_CR0, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -       update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -       vcpu-&gt;arch.cr0_guest_owned_bits = 0;</span>
<span class="quote">&gt; -       vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt; -       if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt; -               /*</span>
<span class="quote">&gt; -                * L1&#39;s specified read shadow might not contain the TS bit,</span>
<span class="quote">&gt; -                * so now that we turned on shadowing of this bit, we need to</span>
<span class="quote">&gt; -                * set this bit of the shadow. Like in nested_vmx_run we need</span>
<span class="quote">&gt; -                * nested_read_cr0(vmcs12), but vmcs12-&gt;guest_cr0 is not yet</span>
<span class="quote">&gt; -                * up-to-date here because we just decached cr0.TS (and we&#39;ll</span>
<span class="quote">&gt; -                * only update vmcs12-&gt;guest_cr0 on nested exit).</span>
<span class="quote">&gt; -                */</span>
<span class="quote">&gt; -               struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</span>
<span class="quote">&gt; -               vmcs12-&gt;guest_cr0 = (vmcs12-&gt;guest_cr0 &amp; ~X86_CR0_TS) |</span>
<span class="quote">&gt; -                       (vcpu-&gt;arch.cr0 &amp; X86_CR0_TS);</span>
<span class="quote">&gt; -               vmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));</span>
<span class="quote">&gt; -       } else</span>
<span class="quote">&gt; -               vmcs_writel(CR0_READ_SHADOW, vcpu-&gt;arch.cr0);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         unsigned long rflags, save_rflags;</span>
<span class="quote">&gt; @@ -4232,9 +4184,6 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;         if (enable_ept)</span>
<span class="quote">&gt;                 ept_update_paging_mode_cr0(&amp;hw_cr0, cr0, vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -               hw_cr0 |= X86_CR0_TS | X86_CR0_MP;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         vmcs_writel(CR0_READ_SHADOW, cr0);</span>
<span class="quote">&gt;         vmcs_writel(GUEST_CR0, hw_cr0);</span>
<span class="quote">&gt;         vcpu-&gt;arch.cr0 = cr0;</span>
<span class="quote">&gt; @@ -5321,7 +5270,9 @@ static int vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
<span class="quote">&gt;         /* 22.2.1, 20.8.1 */</span>
<span class="quote">&gt;         vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);</span>
<span class="quote">&gt; +       vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt; +       vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;         set_cr4_guest_host_mask(vmx);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (vmx_xsaves_supported())</span>
<span class="quote">&gt; @@ -5425,7 +5376,7 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
<span class="quote">&gt;         vmx_set_cr0(vcpu, cr0); /* enter rmode */</span>
<span class="quote">&gt;         vmx_set_cr4(vcpu, 0);</span>
<span class="quote">&gt;         vmx_set_efer(vcpu, 0);</span>
<span class="quote">&gt; -       vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;         update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         vpid_sync_context(vmx-&gt;vpid);</span>
<span class="quote">&gt; @@ -5698,11 +5649,6 @@ static int handle_exception(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;         if (is_nmi(intr_info))</span>
<span class="quote">&gt;                 return 1;  /* already handled by vmx_vcpu_run() */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (is_no_device(intr_info)) {</span>
<span class="quote">&gt; -               vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt; -               return 1;</span>
<span class="quote">&gt; -       }</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         if (is_invalid_opcode(intr_info)) {</span>
<span class="quote">&gt;                 if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;                         kvm_queue_exception(vcpu, UD_VECTOR);</span>
<span class="quote">&gt; @@ -5892,22 +5838,6 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)</span>
<span class="quote">&gt;                 return kvm_set_cr4(vcpu, val);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -/* called to set cr0 as appropriate for clts instruction exit. */</span>
<span class="quote">&gt; -static void handle_clts(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -       if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt; -               /*</span>
<span class="quote">&gt; -                * We get here when L2 did CLTS, and L1 didn&#39;t shadow CR0.TS</span>
<span class="quote">&gt; -                * but we did (!fpu_active). We need to keep GUEST_CR0.TS on,</span>
<span class="quote">&gt; -                * just pretend it&#39;s off (also in arch.cr0 for fpu_activate).</span>
<span class="quote">&gt; -                */</span>
<span class="quote">&gt; -               vmcs_writel(CR0_READ_SHADOW,</span>
<span class="quote">&gt; -                       vmcs_readl(CR0_READ_SHADOW) &amp; ~X86_CR0_TS);</span>
<span class="quote">&gt; -               vcpu-&gt;arch.cr0 &amp;= ~X86_CR0_TS;</span>
<span class="quote">&gt; -       } else</span>
<span class="quote">&gt; -               vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         unsigned long exit_qualification, val;</span>
<span class="quote">&gt; @@ -5953,9 +5883,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;                 break;</span>
<span class="quote">&gt;         case 2: /* clts */</span>
<span class="quote">&gt; -               handle_clts(vcpu);</span>
<span class="quote">&gt; +               WARN_ONCE(1, &quot;Guest should always own CR0.TS&quot;);</span>
<span class="quote">&gt; +               vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;                 trace_kvm_cr_write(0, kvm_read_cr0(vcpu));</span>
<span class="quote">&gt; -               vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;                 return kvm_skip_emulated_instruction(vcpu);</span>
<span class="quote">&gt;         case 1: /*mov from cr*/</span>
<span class="quote">&gt;                 switch (cr) {</span>
<span class="quote">&gt; @@ -10349,8 +10279,8 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt; -        * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, with possibly a modified</span>
<span class="quote">&gt; -        * TS bit (for lazy fpu) and bits which we consider mandatory enabled.</span>
<span class="quote">&gt; +        * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, possibly modifying those</span>
<span class="quote">&gt; +        * bits which we consider mandatory enabled.</span>
<span class="quote">&gt;          * The CR0_READ_SHADOW is what L2 should have expected to read given</span>
<span class="quote">&gt;          * the specifications by L1; It&#39;s not enough to take</span>
<span class="quote">&gt;          * vmcs12-&gt;cr0_read_shadow because on our cr0_guest_host_mask we we</span>
<span class="quote">&gt; @@ -10963,24 +10893,15 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;         vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Note that calling vmx_set_cr0 is important, even if cr0 hasn&#39;t</span>
<span class="quote">&gt; -        * actually changed, because it depends on the current state of</span>
<span class="quote">&gt; -        * fpu_active (which may have changed).</span>
<span class="quote">&gt; -        * Note that vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt; +        * actually changed, because vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt; +        *</span>
<span class="quote">&gt; +        * CR0_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt; +        * (KVM doesn&#39;t change it);</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; +       vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;         vmx_set_cr0(vcpu, vmcs12-&gt;host_cr0);</span>
<span class="quote">&gt; -       /*</span>
<span class="quote">&gt; -        * If we did fpu_activate()/fpu_deactivate() during L2&#39;s run, we need</span>
<span class="quote">&gt; -        * to apply the same changes to L1&#39;s vmcs. We just set cr0 correctly,</span>
<span class="quote">&gt; -        * but we also need to update cr0_guest_host_mask and exception_bitmap.</span>
<span class="quote">&gt; -        */</span>
<span class="quote">&gt; -       update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -       vcpu-&gt;arch.cr0_guest_owned_bits = (vcpu-&gt;fpu_active ? X86_CR0_TS : 0);</span>
<span class="quote">&gt; -       vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       /*</span>
<span class="quote">&gt; -        * Note that CR4_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt; -        * (KVM doesn&#39;t change it)- no reason to call set_cr4_guest_host_mask();</span>
<span class="quote">&gt; -        */</span>
<span class="quote">&gt; +       /* Same as above - no reason to call set_cr4_guest_host_mask().  */</span>
<span class="quote">&gt;         vcpu-&gt;arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);</span>
<span class="quote">&gt;         kvm_set_cr4(vcpu, vmcs12-&gt;host_cr4);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -11609,9 +11530,6 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         .get_pkru = vmx_get_pkru,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       .fpu_activate = vmx_fpu_activate,</span>
<span class="quote">&gt; -       .fpu_deactivate = vmx_fpu_deactivate,</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         .tlb_flush = vmx_flush_tlb,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         .run = vmx_vcpu_run,</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; index 8d3047c8cce7..c48404017e4f 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; @@ -6751,10 +6751,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;                         r = 0;</span>
<span class="quote">&gt;                         goto out;</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt; -               if (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {</span>
<span class="quote">&gt; -                       vcpu-&gt;fpu_active = 0;</span>
<span class="quote">&gt; -                       kvm_x86_ops-&gt;fpu_deactivate(vcpu);</span>
<span class="quote">&gt; -               }</span>
<span class="quote">&gt;                 if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {</span>
<span class="quote">&gt;                         /* Page is swapped out. Do synthetic halt */</span>
<span class="quote">&gt;                         vcpu-&gt;arch.apf.halted = true;</span>
<span class="quote">&gt; @@ -6856,8 +6852,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;         preempt_disable();</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         kvm_x86_ops-&gt;prepare_guest_switch(vcpu);</span>
<span class="quote">&gt; -       if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -               kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt; +       kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /*</span>
<span class="quote">&gt;          * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt</span>
<span class="quote">&gt; diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="quote">&gt; index 2db458ee94b0..8d69d5150748 100644</span>
<span class="quote">&gt; --- a/include/linux/kvm_host.h</span>
<span class="quote">&gt; +++ b/include/linux/kvm_host.h</span>
<span class="quote">&gt; @@ -221,7 +221,6 @@ struct kvm_vcpu {</span>
<span class="quote">&gt;         struct mutex mutex;</span>
<span class="quote">&gt;         struct kvm_run *run;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       int fpu_active;</span>
<span class="quote">&gt;         int guest_fpu_loaded, guest_xcr0_loaded;</span>
<span class="quote">&gt;         struct swait_queue_head wq;</span>
<span class="quote">&gt;         struct pid *pid;</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 1.8.3.1</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68131">Bandan Das</a> - Feb. 17, 2017, 12:45 a.m.</div>
<pre class="content">
Paolo Bonzini &lt;pbonzini@redhat.com&gt; writes:
<span class="quote">
&gt; The FPU is always active now when running KVM.</span>

The lazy code was a performance optimization, correct ?
Is this just dormant code and being removed ? Maybe
mentioning the reasoning in a little more detail is a good
idea.

The removal itself looks clean. I was really hoping that you
would have forgotten removing &quot;fpu_active&quot; from struct kvm_vcpu()
but you hadn&#39;t ;)

Bandan
<span class="quote">
&gt; Signed-off-by: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/kvm_host.h |   3 --</span>
<span class="quote">&gt;  arch/x86/kvm/cpuid.c            |   2 -</span>
<span class="quote">&gt;  arch/x86/kvm/svm.c              |  43 ++-------------</span>
<span class="quote">&gt;  arch/x86/kvm/vmx.c              | 112 ++++++----------------------------------</span>
<span class="quote">&gt;  arch/x86/kvm/x86.c              |   7 +--</span>
<span class="quote">&gt;  include/linux/kvm_host.h        |   1 -</span>
<span class="quote">&gt;  6 files changed, 19 insertions(+), 149 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; index e4f13e714bcf..74ef58c8ff53 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt; @@ -55,7 +55,6 @@</span>
<span class="quote">&gt;  #define KVM_REQ_TRIPLE_FAULT      10</span>
<span class="quote">&gt;  #define KVM_REQ_MMU_SYNC          11</span>
<span class="quote">&gt;  #define KVM_REQ_CLOCK_UPDATE      12</span>
<span class="quote">&gt; -#define KVM_REQ_DEACTIVATE_FPU    13</span>
<span class="quote">&gt;  #define KVM_REQ_EVENT             14</span>
<span class="quote">&gt;  #define KVM_REQ_APF_HALT          15</span>
<span class="quote">&gt;  #define KVM_REQ_STEAL_UPDATE      16</span>
<span class="quote">&gt; @@ -936,8 +935,6 @@ struct kvm_x86_ops {</span>
<span class="quote">&gt;  	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);</span>
<span class="quote">&gt;  	u32 (*get_pkru)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt; -	void (*fpu_activate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt; -	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	void (*tlb_flush)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; index c0e2036217ad..1d155cc56629 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt; @@ -123,8 +123,6 @@ int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))</span>
<span class="quote">&gt;  		best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * The existing code assumes virtual address is 48-bit in the canonical</span>
<span class="quote">&gt;  	 * address checks; exit if it is ever changed.</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; index 4e5905a1ce70..d1efe2c62b3f 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt; @@ -1157,7 +1157,6 @@ static void init_vmcb(struct vcpu_svm *svm)</span>
<span class="quote">&gt;  	struct vmcb_control_area *control = &amp;svm-&gt;vmcb-&gt;control;</span>
<span class="quote">&gt;  	struct vmcb_save_area *save = &amp;svm-&gt;vmcb-&gt;save;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;  	svm-&gt;vcpu.arch.hflags = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	set_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt; @@ -1899,15 +1898,12 @@ static void update_cr0_intercept(struct vcpu_svm *svm)</span>
<span class="quote">&gt;  	ulong gcr0 = svm-&gt;vcpu.arch.cr0;</span>
<span class="quote">&gt;  	u64 *hcr0 = &amp;svm-&gt;vmcb-&gt;save.cr0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!svm-&gt;vcpu.fpu_active)</span>
<span class="quote">&gt; -		*hcr0 |= SVM_CR0_SELECTIVE_MASK;</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt; -			| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt; +	*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt; +		| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mark_dirty(svm-&gt;vmcb, VMCB_CR);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (gcr0 == *hcr0 &amp;&amp; svm-&gt;vcpu.fpu_active) {</span>
<span class="quote">&gt; +	if (gcr0 == *hcr0) {</span>
<span class="quote">&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; @@ -1938,8 +1934,6 @@ static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;  	if (!npt_enabled)</span>
<span class="quote">&gt;  		cr0 |= X86_CR0_PG | X86_CR0_WP;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -		cr0 |= X86_CR0_TS;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * re-enable caching here because the QEMU bios</span>
<span class="quote">&gt;  	 * does not do it - this results in some delay at</span>
<span class="quote">&gt; @@ -2158,22 +2152,6 @@ static int ac_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void svm_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	clr_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static int nm_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	svm_fpu_activate(&amp;svm-&gt;vcpu);</span>
<span class="quote">&gt; -	return 1;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static bool is_erratum_383(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int err, i;</span>
<span class="quote">&gt; @@ -2571,9 +2549,6 @@ static int nested_svm_exit_special(struct vcpu_svm *svm)</span>
<span class="quote">&gt;  		if (!npt_enabled &amp;&amp; svm-&gt;apf_reason == 0)</span>
<span class="quote">&gt;  			return NESTED_EXIT_HOST;</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt; -	case SVM_EXIT_EXCP_BASE + NM_VECTOR:</span>
<span class="quote">&gt; -		nm_interception(svm);</span>
<span class="quote">&gt; -		break;</span>
<span class="quote">&gt;  	default:</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -4018,7 +3993,6 @@ static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {</span>
<span class="quote">&gt;  	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,</span>
<span class="quote">&gt;  	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,</span>
<span class="quote">&gt;  	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,</span>
<span class="quote">&gt; -	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,</span>
<span class="quote">&gt;  	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,</span>
<span class="quote">&gt;  	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,</span>
<span class="quote">&gt;  	[SVM_EXIT_INTR]				= intr_interception,</span>
<span class="quote">&gt; @@ -5072,14 +5046,6 @@ static bool svm_has_wbinvd_exit(void)</span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void svm_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	set_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #define PRE_EX(exit)  { .exit_code = (exit), \</span>
<span class="quote">&gt;  			.stage = X86_ICPT_PRE_EXCEPT, }</span>
<span class="quote">&gt;  #define POST_EX(exit) { .exit_code = (exit), \</span>
<span class="quote">&gt; @@ -5340,9 +5306,6 @@ static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	.get_pkru = svm_get_pkru,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	.fpu_activate = svm_fpu_activate,</span>
<span class="quote">&gt; -	.fpu_deactivate = svm_fpu_deactivate,</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	.tlb_flush = svm_flush_tlb,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	.run = svm_vcpu_run,</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; index 0e0b5d09597e..9856b73a21ad 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; @@ -1856,7 +1856,7 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	u32 eb;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |</span>
<span class="quote">&gt; -	     (1u &lt;&lt; NM_VECTOR) | (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt; +	     (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;  	if ((vcpu-&gt;guest_debug &amp;</span>
<span class="quote">&gt;  	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==</span>
<span class="quote">&gt;  	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))</span>
<span class="quote">&gt; @@ -1865,8 +1865,6 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  		eb = ~0;</span>
<span class="quote">&gt;  	if (enable_ept)</span>
<span class="quote">&gt;  		eb &amp;= ~(1u &lt;&lt; PF_VECTOR); /* bypass_guest_pf = 0 */</span>
<span class="quote">&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -		eb &amp;= ~(1u &lt;&lt; NM_VECTOR);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* When we are running a nested L2 guest and L1 specified for it a</span>
<span class="quote">&gt;  	 * certain exception bitmap, we must trap the same exceptions and pass</span>
<span class="quote">&gt; @@ -2340,25 +2338,6 @@ static void vmx_vcpu_put(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void vmx_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	ulong cr0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -	vcpu-&gt;fpu_active = 1;</span>
<span class="quote">&gt; -	cr0 = vmcs_readl(GUEST_CR0);</span>
<span class="quote">&gt; -	cr0 &amp;= ~(X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -	cr0 |= kvm_read_cr0_bits(vcpu, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -	vmcs_writel(GUEST_CR0, cr0);</span>
<span class="quote">&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt; -	if (is_guest_mode(vcpu))</span>
<span class="quote">&gt; -		vcpu-&gt;arch.cr0_guest_owned_bits &amp;=</span>
<span class="quote">&gt; -			~get_vmcs12(vcpu)-&gt;cr0_guest_host_mask;</span>
<span class="quote">&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -2377,33 +2356,6 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)</span>
<span class="quote">&gt;  		(fields-&gt;cr4_read_shadow &amp; fields-&gt;cr4_guest_host_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void vmx_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	/* Note that there is no vcpu-&gt;fpu_active = 0 here. The caller must</span>
<span class="quote">&gt; -	 * set this *before* calling this function.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	vmx_decache_cr0_guest_bits(vcpu);</span>
<span class="quote">&gt; -	vmcs_set_bits(GUEST_CR0, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = 0;</span>
<span class="quote">&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * L1&#39;s specified read shadow might not contain the TS bit,</span>
<span class="quote">&gt; -		 * so now that we turned on shadowing of this bit, we need to</span>
<span class="quote">&gt; -		 * set this bit of the shadow. Like in nested_vmx_run we need</span>
<span class="quote">&gt; -		 * nested_read_cr0(vmcs12), but vmcs12-&gt;guest_cr0 is not yet</span>
<span class="quote">&gt; -		 * up-to-date here because we just decached cr0.TS (and we&#39;ll</span>
<span class="quote">&gt; -		 * only update vmcs12-&gt;guest_cr0 on nested exit).</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</span>
<span class="quote">&gt; -		vmcs12-&gt;guest_cr0 = (vmcs12-&gt;guest_cr0 &amp; ~X86_CR0_TS) |</span>
<span class="quote">&gt; -			(vcpu-&gt;arch.cr0 &amp; X86_CR0_TS);</span>
<span class="quote">&gt; -		vmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));</span>
<span class="quote">&gt; -	} else</span>
<span class="quote">&gt; -		vmcs_writel(CR0_READ_SHADOW, vcpu-&gt;arch.cr0);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long rflags, save_rflags;</span>
<span class="quote">&gt; @@ -4232,9 +4184,6 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;  	if (enable_ept)</span>
<span class="quote">&gt;  		ept_update_paging_mode_cr0(&amp;hw_cr0, cr0, vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -		hw_cr0 |= X86_CR0_TS | X86_CR0_MP;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	vmcs_writel(CR0_READ_SHADOW, cr0);</span>
<span class="quote">&gt;  	vmcs_writel(GUEST_CR0, hw_cr0);</span>
<span class="quote">&gt;  	vcpu-&gt;arch.cr0 = cr0;</span>
<span class="quote">&gt; @@ -5321,7 +5270,9 @@ static int vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
<span class="quote">&gt;  	/* 22.2.1, 20.8.1 */</span>
<span class="quote">&gt;  	vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);</span>
<span class="quote">&gt; +	vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt; +	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	set_cr4_guest_host_mask(vmx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (vmx_xsaves_supported())</span>
<span class="quote">&gt; @@ -5425,7 +5376,7 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
<span class="quote">&gt;  	vmx_set_cr0(vcpu, cr0); /* enter rmode */</span>
<span class="quote">&gt;  	vmx_set_cr4(vcpu, 0);</span>
<span class="quote">&gt;  	vmx_set_efer(vcpu, 0);</span>
<span class="quote">&gt; -	vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	vpid_sync_context(vmx-&gt;vpid);</span>
<span class="quote">&gt; @@ -5698,11 +5649,6 @@ static int handle_exception(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	if (is_nmi(intr_info))</span>
<span class="quote">&gt;  		return 1;  /* already handled by vmx_vcpu_run() */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (is_no_device(intr_info)) {</span>
<span class="quote">&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	if (is_invalid_opcode(intr_info)) {</span>
<span class="quote">&gt;  		if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;  			kvm_queue_exception(vcpu, UD_VECTOR);</span>
<span class="quote">&gt; @@ -5892,22 +5838,6 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)</span>
<span class="quote">&gt;  		return kvm_set_cr4(vcpu, val);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* called to set cr0 as appropriate for clts instruction exit. */</span>
<span class="quote">&gt; -static void handle_clts(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * We get here when L2 did CLTS, and L1 didn&#39;t shadow CR0.TS</span>
<span class="quote">&gt; -		 * but we did (!fpu_active). We need to keep GUEST_CR0.TS on,</span>
<span class="quote">&gt; -		 * just pretend it&#39;s off (also in arch.cr0 for fpu_activate).</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		vmcs_writel(CR0_READ_SHADOW,</span>
<span class="quote">&gt; -			vmcs_readl(CR0_READ_SHADOW) &amp; ~X86_CR0_TS);</span>
<span class="quote">&gt; -		vcpu-&gt;arch.cr0 &amp;= ~X86_CR0_TS;</span>
<span class="quote">&gt; -	} else</span>
<span class="quote">&gt; -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long exit_qualification, val;</span>
<span class="quote">&gt; @@ -5953,9 +5883,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt;  	case 2: /* clts */</span>
<span class="quote">&gt; -		handle_clts(vcpu);</span>
<span class="quote">&gt; +		WARN_ONCE(1, &quot;Guest should always own CR0.TS&quot;);</span>
<span class="quote">&gt; +		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;  		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));</span>
<span class="quote">&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;  		return kvm_skip_emulated_instruction(vcpu);</span>
<span class="quote">&gt;  	case 1: /*mov from cr*/</span>
<span class="quote">&gt;  		switch (cr) {</span>
<span class="quote">&gt; @@ -10349,8 +10279,8 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, with possibly a modified</span>
<span class="quote">&gt; -	 * TS bit (for lazy fpu) and bits which we consider mandatory enabled.</span>
<span class="quote">&gt; +	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, possibly modifying those</span>
<span class="quote">&gt; +	 * bits which we consider mandatory enabled.</span>
<span class="quote">&gt;  	 * The CR0_READ_SHADOW is what L2 should have expected to read given</span>
<span class="quote">&gt;  	 * the specifications by L1; It&#39;s not enough to take</span>
<span class="quote">&gt;  	 * vmcs12-&gt;cr0_read_shadow because on our cr0_guest_host_mask we we</span>
<span class="quote">&gt; @@ -10963,24 +10893,15 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;  	vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Note that calling vmx_set_cr0 is important, even if cr0 hasn&#39;t</span>
<span class="quote">&gt; -	 * actually changed, because it depends on the current state of</span>
<span class="quote">&gt; -	 * fpu_active (which may have changed).</span>
<span class="quote">&gt; -	 * Note that vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt; +	 * actually changed, because vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * CR0_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt; +	 * (KVM doesn&#39;t change it);</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; +	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;  	vmx_set_cr0(vcpu, vmcs12-&gt;host_cr0);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * If we did fpu_activate()/fpu_deactivate() during L2&#39;s run, we need</span>
<span class="quote">&gt; -	 * to apply the same changes to L1&#39;s vmcs. We just set cr0 correctly,</span>
<span class="quote">&gt; -	 * but we also need to update cr0_guest_host_mask and exception_bitmap.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = (vcpu-&gt;fpu_active ? X86_CR0_TS : 0);</span>
<span class="quote">&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Note that CR4_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt; -	 * (KVM doesn&#39;t change it)- no reason to call set_cr4_guest_host_mask();</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; +	/* Same as above - no reason to call set_cr4_guest_host_mask().  */</span>
<span class="quote">&gt;  	vcpu-&gt;arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);</span>
<span class="quote">&gt;  	kvm_set_cr4(vcpu, vmcs12-&gt;host_cr4);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -11609,9 +11530,6 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	.get_pkru = vmx_get_pkru,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	.fpu_activate = vmx_fpu_activate,</span>
<span class="quote">&gt; -	.fpu_deactivate = vmx_fpu_deactivate,</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	.tlb_flush = vmx_flush_tlb,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	.run = vmx_vcpu_run,</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; index 8d3047c8cce7..c48404017e4f 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt; @@ -6751,10 +6751,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  			r = 0;</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		if (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {</span>
<span class="quote">&gt; -			vcpu-&gt;fpu_active = 0;</span>
<span class="quote">&gt; -			kvm_x86_ops-&gt;fpu_deactivate(vcpu);</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt;  		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {</span>
<span class="quote">&gt;  			/* Page is swapped out. Do synthetic halt */</span>
<span class="quote">&gt;  			vcpu-&gt;arch.apf.halted = true;</span>
<span class="quote">&gt; @@ -6856,8 +6852,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	kvm_x86_ops-&gt;prepare_guest_switch(vcpu);</span>
<span class="quote">&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt; -		kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt; +	kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt</span>
<span class="quote">&gt; diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="quote">&gt; index 2db458ee94b0..8d69d5150748 100644</span>
<span class="quote">&gt; --- a/include/linux/kvm_host.h</span>
<span class="quote">&gt; +++ b/include/linux/kvm_host.h</span>
<span class="quote">&gt; @@ -221,7 +221,6 @@ struct kvm_vcpu {</span>
<span class="quote">&gt;  	struct mutex mutex;</span>
<span class="quote">&gt;  	struct kvm_run *run;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	int fpu_active;</span>
<span class="quote">&gt;  	int guest_fpu_loaded, guest_xcr0_loaded;</span>
<span class="quote">&gt;  	struct swait_queue_head wq;</span>
<span class="quote">&gt;  	struct pid *pid;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - Feb. 17, 2017, 9:37 a.m.</div>
<pre class="content">
On 17/02/2017 01:45, Bandan Das wrote:
<span class="quote">&gt; Paolo Bonzini &lt;pbonzini@redhat.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; The FPU is always active now when running KVM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The lazy code was a performance optimization, correct ?</span>
<span class="quote">&gt; Is this just dormant code and being removed ? Maybe</span>
<span class="quote">&gt; mentioning the reasoning in a little more detail is a good</span>
<span class="quote">&gt; idea.</span>

Lazy FPU support was removed completely from arch/x86.  Apparently,
things such as SSE-optimized mem* and str* functions made it much less
useful.  At this point the KVM code is unnecessary too.

Paolo
<span class="quote">
&gt; The removal itself looks clean. I was really hoping that you</span>
<span class="quote">&gt; would have forgotten removing &quot;fpu_active&quot; from struct kvm_vcpu()</span>
<span class="quote">&gt; but you hadn&#39;t ;)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Bandan</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/kvm_host.h |   3 --</span>
<span class="quote">&gt;&gt;  arch/x86/kvm/cpuid.c            |   2 -</span>
<span class="quote">&gt;&gt;  arch/x86/kvm/svm.c              |  43 ++-------------</span>
<span class="quote">&gt;&gt;  arch/x86/kvm/vmx.c              | 112 ++++++----------------------------------</span>
<span class="quote">&gt;&gt;  arch/x86/kvm/x86.c              |   7 +--</span>
<span class="quote">&gt;&gt;  include/linux/kvm_host.h        |   1 -</span>
<span class="quote">&gt;&gt;  6 files changed, 19 insertions(+), 149 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt; index e4f13e714bcf..74ef58c8ff53 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt; @@ -55,7 +55,6 @@</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_TRIPLE_FAULT      10</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_MMU_SYNC          11</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_CLOCK_UPDATE      12</span>
<span class="quote">&gt;&gt; -#define KVM_REQ_DEACTIVATE_FPU    13</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_EVENT             14</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_APF_HALT          15</span>
<span class="quote">&gt;&gt;  #define KVM_REQ_STEAL_UPDATE      16</span>
<span class="quote">&gt;&gt; @@ -936,8 +935,6 @@ struct kvm_x86_ops {</span>
<span class="quote">&gt;&gt;  	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;  	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);</span>
<span class="quote">&gt;&gt;  	u32 (*get_pkru)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt; -	void (*fpu_activate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt; -	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	void (*tlb_flush)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt; index c0e2036217ad..1d155cc56629 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt; @@ -123,8 +123,6 @@ int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  	if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))</span>
<span class="quote">&gt;&gt;  		best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * The existing code assumes virtual address is 48-bit in the canonical</span>
<span class="quote">&gt;&gt;  	 * address checks; exit if it is ever changed.</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt; index 4e5905a1ce70..d1efe2c62b3f 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt; @@ -1157,7 +1157,6 @@ static void init_vmcb(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;  	struct vmcb_control_area *control = &amp;svm-&gt;vmcb-&gt;control;</span>
<span class="quote">&gt;&gt;  	struct vmcb_save_area *save = &amp;svm-&gt;vmcb-&gt;save;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;&gt;  	svm-&gt;vcpu.arch.hflags = 0;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	set_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;&gt; @@ -1899,15 +1898,12 @@ static void update_cr0_intercept(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;  	ulong gcr0 = svm-&gt;vcpu.arch.cr0;</span>
<span class="quote">&gt;&gt;  	u64 *hcr0 = &amp;svm-&gt;vmcb-&gt;save.cr0;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (!svm-&gt;vcpu.fpu_active)</span>
<span class="quote">&gt;&gt; -		*hcr0 |= SVM_CR0_SELECTIVE_MASK;</span>
<span class="quote">&gt;&gt; -	else</span>
<span class="quote">&gt;&gt; -		*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt;&gt; -			| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;&gt; +	*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt;&gt; +		| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	mark_dirty(svm-&gt;vmcb, VMCB_CR);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (gcr0 == *hcr0 &amp;&amp; svm-&gt;vcpu.fpu_active) {</span>
<span class="quote">&gt;&gt; +	if (gcr0 == *hcr0) {</span>
<span class="quote">&gt;&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);</span>
<span class="quote">&gt;&gt;  	} else {</span>
<span class="quote">&gt;&gt; @@ -1938,8 +1934,6 @@ static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;&gt;  	if (!npt_enabled)</span>
<span class="quote">&gt;&gt;  		cr0 |= X86_CR0_PG | X86_CR0_WP;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt; -		cr0 |= X86_CR0_TS;</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * re-enable caching here because the QEMU bios</span>
<span class="quote">&gt;&gt;  	 * does not do it - this results in some delay at</span>
<span class="quote">&gt;&gt; @@ -2158,22 +2152,6 @@ static int ac_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;  	return 1;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -static void svm_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	clr_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -static int nm_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	svm_fpu_activate(&amp;svm-&gt;vcpu);</span>
<span class="quote">&gt;&gt; -	return 1;</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  static bool is_erratum_383(void)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	int err, i;</span>
<span class="quote">&gt;&gt; @@ -2571,9 +2549,6 @@ static int nested_svm_exit_special(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;  		if (!npt_enabled &amp;&amp; svm-&gt;apf_reason == 0)</span>
<span class="quote">&gt;&gt;  			return NESTED_EXIT_HOST;</span>
<span class="quote">&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt; -	case SVM_EXIT_EXCP_BASE + NM_VECTOR:</span>
<span class="quote">&gt;&gt; -		nm_interception(svm);</span>
<span class="quote">&gt;&gt; -		break;</span>
<span class="quote">&gt;&gt;  	default:</span>
<span class="quote">&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt; @@ -4018,7 +3993,6 @@ static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,</span>
<span class="quote">&gt;&gt; -	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,</span>
<span class="quote">&gt;&gt;  	[SVM_EXIT_INTR]				= intr_interception,</span>
<span class="quote">&gt;&gt; @@ -5072,14 +5046,6 @@ static bool svm_has_wbinvd_exit(void)</span>
<span class="quote">&gt;&gt;  	return true;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -static void svm_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	set_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt;&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  #define PRE_EX(exit)  { .exit_code = (exit), \</span>
<span class="quote">&gt;&gt;  			.stage = X86_ICPT_PRE_EXCEPT, }</span>
<span class="quote">&gt;&gt;  #define POST_EX(exit) { .exit_code = (exit), \</span>
<span class="quote">&gt;&gt; @@ -5340,9 +5306,6 @@ static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	.get_pkru = svm_get_pkru,</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	.fpu_activate = svm_fpu_activate,</span>
<span class="quote">&gt;&gt; -	.fpu_deactivate = svm_fpu_deactivate,</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	.tlb_flush = svm_flush_tlb,</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	.run = svm_vcpu_run,</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt; index 0e0b5d09597e..9856b73a21ad 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt; @@ -1856,7 +1856,7 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  	u32 eb;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |</span>
<span class="quote">&gt;&gt; -	     (1u &lt;&lt; NM_VECTOR) | (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;&gt; +	     (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;&gt;  	if ((vcpu-&gt;guest_debug &amp;</span>
<span class="quote">&gt;&gt;  	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==</span>
<span class="quote">&gt;&gt;  	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))</span>
<span class="quote">&gt;&gt; @@ -1865,8 +1865,6 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  		eb = ~0;</span>
<span class="quote">&gt;&gt;  	if (enable_ept)</span>
<span class="quote">&gt;&gt;  		eb &amp;= ~(1u &lt;&lt; PF_VECTOR); /* bypass_guest_pf = 0 */</span>
<span class="quote">&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt; -		eb &amp;= ~(1u &lt;&lt; NM_VECTOR);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/* When we are running a nested L2 guest and L1 specified for it a</span>
<span class="quote">&gt;&gt;  	 * certain exception bitmap, we must trap the same exceptions and pass</span>
<span class="quote">&gt;&gt; @@ -2340,25 +2338,6 @@ static void vmx_vcpu_put(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -static void vmx_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	ulong cr0;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt; -		return;</span>
<span class="quote">&gt;&gt; -	vcpu-&gt;fpu_active = 1;</span>
<span class="quote">&gt;&gt; -	cr0 = vmcs_readl(GUEST_CR0);</span>
<span class="quote">&gt;&gt; -	cr0 &amp;= ~(X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt; -	cr0 |= kvm_read_cr0_bits(vcpu, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt; -	vmcs_writel(GUEST_CR0, cr0);</span>
<span class="quote">&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt; -	if (is_guest_mode(vcpu))</span>
<span class="quote">&gt;&gt; -		vcpu-&gt;arch.cr0_guest_owned_bits &amp;=</span>
<span class="quote">&gt;&gt; -			~get_vmcs12(vcpu)-&gt;cr0_guest_host_mask;</span>
<span class="quote">&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; @@ -2377,33 +2356,6 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)</span>
<span class="quote">&gt;&gt;  		(fields-&gt;cr4_read_shadow &amp; fields-&gt;cr4_guest_host_mask);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -static void vmx_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	/* Note that there is no vcpu-&gt;fpu_active = 0 here. The caller must</span>
<span class="quote">&gt;&gt; -	 * set this *before* calling this function.</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; -	vmx_decache_cr0_guest_bits(vcpu);</span>
<span class="quote">&gt;&gt; -	vmcs_set_bits(GUEST_CR0, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = 0;</span>
<span class="quote">&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * L1&#39;s specified read shadow might not contain the TS bit,</span>
<span class="quote">&gt;&gt; -		 * so now that we turned on shadowing of this bit, we need to</span>
<span class="quote">&gt;&gt; -		 * set this bit of the shadow. Like in nested_vmx_run we need</span>
<span class="quote">&gt;&gt; -		 * nested_read_cr0(vmcs12), but vmcs12-&gt;guest_cr0 is not yet</span>
<span class="quote">&gt;&gt; -		 * up-to-date here because we just decached cr0.TS (and we&#39;ll</span>
<span class="quote">&gt;&gt; -		 * only update vmcs12-&gt;guest_cr0 on nested exit).</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</span>
<span class="quote">&gt;&gt; -		vmcs12-&gt;guest_cr0 = (vmcs12-&gt;guest_cr0 &amp; ~X86_CR0_TS) |</span>
<span class="quote">&gt;&gt; -			(vcpu-&gt;arch.cr0 &amp; X86_CR0_TS);</span>
<span class="quote">&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));</span>
<span class="quote">&gt;&gt; -	} else</span>
<span class="quote">&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW, vcpu-&gt;arch.cr0);</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	unsigned long rflags, save_rflags;</span>
<span class="quote">&gt;&gt; @@ -4232,9 +4184,6 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;&gt;  	if (enable_ept)</span>
<span class="quote">&gt;&gt;  		ept_update_paging_mode_cr0(&amp;hw_cr0, cr0, vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt; -		hw_cr0 |= X86_CR0_TS | X86_CR0_MP;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	vmcs_writel(CR0_READ_SHADOW, cr0);</span>
<span class="quote">&gt;&gt;  	vmcs_writel(GUEST_CR0, hw_cr0);</span>
<span class="quote">&gt;&gt;  	vcpu-&gt;arch.cr0 = cr0;</span>
<span class="quote">&gt;&gt; @@ -5321,7 +5270,9 @@ static int vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
<span class="quote">&gt;&gt;  	/* 22.2.1, 20.8.1 */</span>
<span class="quote">&gt;&gt;  	vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);</span>
<span class="quote">&gt;&gt; +	vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt; +	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	set_cr4_guest_host_mask(vmx);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	if (vmx_xsaves_supported())</span>
<span class="quote">&gt;&gt; @@ -5425,7 +5376,7 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
<span class="quote">&gt;&gt;  	vmx_set_cr0(vcpu, cr0); /* enter rmode */</span>
<span class="quote">&gt;&gt;  	vmx_set_cr4(vcpu, 0);</span>
<span class="quote">&gt;&gt;  	vmx_set_efer(vcpu, 0);</span>
<span class="quote">&gt;&gt; -	vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	vpid_sync_context(vmx-&gt;vpid);</span>
<span class="quote">&gt;&gt; @@ -5698,11 +5649,6 @@ static int handle_exception(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  	if (is_nmi(intr_info))</span>
<span class="quote">&gt;&gt;  		return 1;  /* already handled by vmx_vcpu_run() */</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	if (is_no_device(intr_info)) {</span>
<span class="quote">&gt;&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt; -		return 1;</span>
<span class="quote">&gt;&gt; -	}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	if (is_invalid_opcode(intr_info)) {</span>
<span class="quote">&gt;&gt;  		if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt;  			kvm_queue_exception(vcpu, UD_VECTOR);</span>
<span class="quote">&gt;&gt; @@ -5892,22 +5838,6 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)</span>
<span class="quote">&gt;&gt;  		return kvm_set_cr4(vcpu, val);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -/* called to set cr0 as appropriate for clts instruction exit. */</span>
<span class="quote">&gt;&gt; -static void handle_clts(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt; -{</span>
<span class="quote">&gt;&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt; -		 * We get here when L2 did CLTS, and L1 didn&#39;t shadow CR0.TS</span>
<span class="quote">&gt;&gt; -		 * but we did (!fpu_active). We need to keep GUEST_CR0.TS on,</span>
<span class="quote">&gt;&gt; -		 * just pretend it&#39;s off (also in arch.cr0 for fpu_activate).</span>
<span class="quote">&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW,</span>
<span class="quote">&gt;&gt; -			vmcs_readl(CR0_READ_SHADOW) &amp; ~X86_CR0_TS);</span>
<span class="quote">&gt;&gt; -		vcpu-&gt;arch.cr0 &amp;= ~X86_CR0_TS;</span>
<span class="quote">&gt;&gt; -	} else</span>
<span class="quote">&gt;&gt; -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;&gt; -}</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	unsigned long exit_qualification, val;</span>
<span class="quote">&gt;&gt; @@ -5953,9 +5883,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt;  	case 2: /* clts */</span>
<span class="quote">&gt;&gt; -		handle_clts(vcpu);</span>
<span class="quote">&gt;&gt; +		WARN_ONCE(1, &quot;Guest should always own CR0.TS&quot;);</span>
<span class="quote">&gt;&gt; +		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;&gt;  		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));</span>
<span class="quote">&gt;&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt;  		return kvm_skip_emulated_instruction(vcpu);</span>
<span class="quote">&gt;&gt;  	case 1: /*mov from cr*/</span>
<span class="quote">&gt;&gt;  		switch (cr) {</span>
<span class="quote">&gt;&gt; @@ -10349,8 +10279,8 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt; -	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, with possibly a modified</span>
<span class="quote">&gt;&gt; -	 * TS bit (for lazy fpu) and bits which we consider mandatory enabled.</span>
<span class="quote">&gt;&gt; +	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, possibly modifying those</span>
<span class="quote">&gt;&gt; +	 * bits which we consider mandatory enabled.</span>
<span class="quote">&gt;&gt;  	 * The CR0_READ_SHADOW is what L2 should have expected to read given</span>
<span class="quote">&gt;&gt;  	 * the specifications by L1; It&#39;s not enough to take</span>
<span class="quote">&gt;&gt;  	 * vmcs12-&gt;cr0_read_shadow because on our cr0_guest_host_mask we we</span>
<span class="quote">&gt;&gt; @@ -10963,24 +10893,15 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;&gt;  	vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);</span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * Note that calling vmx_set_cr0 is important, even if cr0 hasn&#39;t</span>
<span class="quote">&gt;&gt; -	 * actually changed, because it depends on the current state of</span>
<span class="quote">&gt;&gt; -	 * fpu_active (which may have changed).</span>
<span class="quote">&gt;&gt; -	 * Note that vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt;&gt; +	 * actually changed, because vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt; +	 * CR0_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt;&gt; +	 * (KVM doesn&#39;t change it);</span>
<span class="quote">&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt; +	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt;  	vmx_set_cr0(vcpu, vmcs12-&gt;host_cr0);</span>
<span class="quote">&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt; -	 * If we did fpu_activate()/fpu_deactivate() during L2&#39;s run, we need</span>
<span class="quote">&gt;&gt; -	 * to apply the same changes to L1&#39;s vmcs. We just set cr0 correctly,</span>
<span class="quote">&gt;&gt; -	 * but we also need to update cr0_guest_host_mask and exception_bitmap.</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = (vcpu-&gt;fpu_active ? X86_CR0_TS : 0);</span>
<span class="quote">&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt; -	 * Note that CR4_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt;&gt; -	 * (KVM doesn&#39;t change it)- no reason to call set_cr4_guest_host_mask();</span>
<span class="quote">&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt; +	/* Same as above - no reason to call set_cr4_guest_host_mask().  */</span>
<span class="quote">&gt;&gt;  	vcpu-&gt;arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);</span>
<span class="quote">&gt;&gt;  	kvm_set_cr4(vcpu, vmcs12-&gt;host_cr4);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; @@ -11609,9 +11530,6 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	.get_pkru = vmx_get_pkru,</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	.fpu_activate = vmx_fpu_activate,</span>
<span class="quote">&gt;&gt; -	.fpu_deactivate = vmx_fpu_deactivate,</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	.tlb_flush = vmx_flush_tlb,</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	.run = vmx_vcpu_run,</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt; index 8d3047c8cce7..c48404017e4f 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt; @@ -6751,10 +6751,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  			r = 0;</span>
<span class="quote">&gt;&gt;  			goto out;</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt; -		if (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {</span>
<span class="quote">&gt;&gt; -			vcpu-&gt;fpu_active = 0;</span>
<span class="quote">&gt;&gt; -			kvm_x86_ops-&gt;fpu_deactivate(vcpu);</span>
<span class="quote">&gt;&gt; -		}</span>
<span class="quote">&gt;&gt;  		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {</span>
<span class="quote">&gt;&gt;  			/* Page is swapped out. Do synthetic halt */</span>
<span class="quote">&gt;&gt;  			vcpu-&gt;arch.apf.halted = true;</span>
<span class="quote">&gt;&gt; @@ -6856,8 +6852,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;  	preempt_disable();</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	kvm_x86_ops-&gt;prepare_guest_switch(vcpu);</span>
<span class="quote">&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt; -		kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;&gt; +	kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;  	 * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt; index 2db458ee94b0..8d69d5150748 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt; @@ -221,7 +221,6 @@ struct kvm_vcpu {</span>
<span class="quote">&gt;&gt;  	struct mutex mutex;</span>
<span class="quote">&gt;&gt;  	struct kvm_run *run;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	int fpu_active;</span>
<span class="quote">&gt;&gt;  	int guest_fpu_loaded, guest_xcr0_loaded;</span>
<span class="quote">&gt;&gt;  	struct swait_queue_head wq;</span>
<span class="quote">&gt;&gt;  	struct pid *pid;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68131">Bandan Das</a> - Feb. 17, 2017, 5:21 p.m.</div>
<pre class="content">
Paolo Bonzini &lt;pbonzini@redhat.com&gt; writes:
<span class="quote">
&gt; On 17/02/2017 01:45, Bandan Das wrote:</span>
<span class="quote">&gt;&gt; Paolo Bonzini &lt;pbonzini@redhat.com&gt; writes:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; The FPU is always active now when running KVM.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The lazy code was a performance optimization, correct ?</span>
<span class="quote">&gt;&gt; Is this just dormant code and being removed ? Maybe</span>
<span class="quote">&gt;&gt; mentioning the reasoning in a little more detail is a good</span>
<span class="quote">&gt;&gt; idea.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Lazy FPU support was removed completely from arch/x86.  Apparently,</span>
<span class="quote">&gt; things such as SSE-optimized mem* and str* functions made it much less</span>
<span class="quote">&gt; useful.  At this point the KVM code is unnecessary too.</span>

Thanks! If it&#39;s not too late, please include the above in the commit
message.

Bandan
<span class="quote">
&gt; Paolo</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; The removal itself looks clean. I was really hoping that you</span>
<span class="quote">&gt;&gt; would have forgotten removing &quot;fpu_active&quot; from struct kvm_vcpu()</span>
<span class="quote">&gt;&gt; but you hadn&#39;t ;)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Bandan</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/include/asm/kvm_host.h |   3 --</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/kvm/cpuid.c            |   2 -</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/kvm/svm.c              |  43 ++-------------</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/kvm/vmx.c              | 112 ++++++----------------------------------</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/kvm/x86.c              |   7 +--</span>
<span class="quote">&gt;&gt;&gt;  include/linux/kvm_host.h        |   1 -</span>
<span class="quote">&gt;&gt;&gt;  6 files changed, 19 insertions(+), 149 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; index e4f13e714bcf..74ef58c8ff53 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; @@ -55,7 +55,6 @@</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_TRIPLE_FAULT      10</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_MMU_SYNC          11</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_CLOCK_UPDATE      12</span>
<span class="quote">&gt;&gt;&gt; -#define KVM_REQ_DEACTIVATE_FPU    13</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_EVENT             14</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_APF_HALT          15</span>
<span class="quote">&gt;&gt;&gt;  #define KVM_REQ_STEAL_UPDATE      16</span>
<span class="quote">&gt;&gt;&gt; @@ -936,8 +935,6 @@ struct kvm_x86_ops {</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt;  	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);</span>
<span class="quote">&gt;&gt;&gt;  	u32 (*get_pkru)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	void (*fpu_activate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	void (*tlb_flush)(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt;&gt; index c0e2036217ad..1d155cc56629 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/kvm/cpuid.c</span>
<span class="quote">&gt;&gt;&gt; @@ -123,8 +123,6 @@ int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  	if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))</span>
<span class="quote">&gt;&gt;&gt;  		best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * The existing code assumes virtual address is 48-bit in the canonical</span>
<span class="quote">&gt;&gt;&gt;  	 * address checks; exit if it is ever changed.</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt;&gt; index 4e5905a1ce70..d1efe2c62b3f 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/kvm/svm.c</span>
<span class="quote">&gt;&gt;&gt; @@ -1157,7 +1157,6 @@ static void init_vmcb(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;&gt;  	struct vmcb_control_area *control = &amp;svm-&gt;vmcb-&gt;control;</span>
<span class="quote">&gt;&gt;&gt;  	struct vmcb_save_area *save = &amp;svm-&gt;vmcb-&gt;save;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;&gt;&gt;  	svm-&gt;vcpu.arch.hflags = 0;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	set_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;&gt;&gt; @@ -1899,15 +1898,12 @@ static void update_cr0_intercept(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;&gt;  	ulong gcr0 = svm-&gt;vcpu.arch.cr0;</span>
<span class="quote">&gt;&gt;&gt;  	u64 *hcr0 = &amp;svm-&gt;vmcb-&gt;save.cr0;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (!svm-&gt;vcpu.fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		*hcr0 |= SVM_CR0_SELECTIVE_MASK;</span>
<span class="quote">&gt;&gt;&gt; -	else</span>
<span class="quote">&gt;&gt;&gt; -		*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt;&gt;&gt; -			| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;&gt;&gt; +	*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="quote">&gt;&gt;&gt; +		| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	mark_dirty(svm-&gt;vmcb, VMCB_CR);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (gcr0 == *hcr0 &amp;&amp; svm-&gt;vcpu.fpu_active) {</span>
<span class="quote">&gt;&gt;&gt; +	if (gcr0 == *hcr0) {</span>
<span class="quote">&gt;&gt;&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_READ);</span>
<span class="quote">&gt;&gt;&gt;  		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);</span>
<span class="quote">&gt;&gt;&gt;  	} else {</span>
<span class="quote">&gt;&gt;&gt; @@ -1938,8 +1934,6 @@ static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;&gt;&gt;  	if (!npt_enabled)</span>
<span class="quote">&gt;&gt;&gt;  		cr0 |= X86_CR0_PG | X86_CR0_WP;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		cr0 |= X86_CR0_TS;</span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * re-enable caching here because the QEMU bios</span>
<span class="quote">&gt;&gt;&gt;  	 * does not do it - this results in some delay at</span>
<span class="quote">&gt;&gt;&gt; @@ -2158,22 +2152,6 @@ static int ac_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;&gt;  	return 1;</span>
<span class="quote">&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -static void svm_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt; -	clr_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt; -	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="quote">&gt;&gt;&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt; -static int nm_interception(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	svm_fpu_activate(&amp;svm-&gt;vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	return 1;</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  static bool is_erratum_383(void)</span>
<span class="quote">&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;  	int err, i;</span>
<span class="quote">&gt;&gt;&gt; @@ -2571,9 +2549,6 @@ static int nested_svm_exit_special(struct vcpu_svm *svm)</span>
<span class="quote">&gt;&gt;&gt;  		if (!npt_enabled &amp;&amp; svm-&gt;apf_reason == 0)</span>
<span class="quote">&gt;&gt;&gt;  			return NESTED_EXIT_HOST;</span>
<span class="quote">&gt;&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt;&gt; -	case SVM_EXIT_EXCP_BASE + NM_VECTOR:</span>
<span class="quote">&gt;&gt;&gt; -		nm_interception(svm);</span>
<span class="quote">&gt;&gt;&gt; -		break;</span>
<span class="quote">&gt;&gt;&gt;  	default:</span>
<span class="quote">&gt;&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt; @@ -4018,7 +3993,6 @@ static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,</span>
<span class="quote">&gt;&gt;&gt; -	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,</span>
<span class="quote">&gt;&gt;&gt;  	[SVM_EXIT_INTR]				= intr_interception,</span>
<span class="quote">&gt;&gt;&gt; @@ -5072,14 +5046,6 @@ static bool svm_has_wbinvd_exit(void)</span>
<span class="quote">&gt;&gt;&gt;  	return true;</span>
<span class="quote">&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -static void svm_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt; -	set_exception_intercept(svm, NM_VECTOR);</span>
<span class="quote">&gt;&gt;&gt; -	update_cr0_intercept(svm);</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  #define PRE_EX(exit)  { .exit_code = (exit), \</span>
<span class="quote">&gt;&gt;&gt;  			.stage = X86_ICPT_PRE_EXCEPT, }</span>
<span class="quote">&gt;&gt;&gt;  #define POST_EX(exit) { .exit_code = (exit), \</span>
<span class="quote">&gt;&gt;&gt; @@ -5340,9 +5306,6 @@ static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	.get_pkru = svm_get_pkru,</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	.fpu_activate = svm_fpu_activate,</span>
<span class="quote">&gt;&gt;&gt; -	.fpu_deactivate = svm_fpu_deactivate,</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  	.tlb_flush = svm_flush_tlb,</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	.run = svm_vcpu_run,</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt;&gt; index 0e0b5d09597e..9856b73a21ad 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt;&gt;&gt; @@ -1856,7 +1856,7 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  	u32 eb;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |</span>
<span class="quote">&gt;&gt;&gt; -	     (1u &lt;&lt; NM_VECTOR) | (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;&gt;&gt; +	     (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="quote">&gt;&gt;&gt;  	if ((vcpu-&gt;guest_debug &amp;</span>
<span class="quote">&gt;&gt;&gt;  	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==</span>
<span class="quote">&gt;&gt;&gt;  	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))</span>
<span class="quote">&gt;&gt;&gt; @@ -1865,8 +1865,6 @@ static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  		eb = ~0;</span>
<span class="quote">&gt;&gt;&gt;  	if (enable_ept)</span>
<span class="quote">&gt;&gt;&gt;  		eb &amp;= ~(1u &lt;&lt; PF_VECTOR); /* bypass_guest_pf = 0 */</span>
<span class="quote">&gt;&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		eb &amp;= ~(1u &lt;&lt; NM_VECTOR);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	/* When we are running a nested L2 guest and L1 specified for it a</span>
<span class="quote">&gt;&gt;&gt;  	 * certain exception bitmap, we must trap the same exceptions and pass</span>
<span class="quote">&gt;&gt;&gt; @@ -2340,25 +2338,6 @@ static void vmx_vcpu_put(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -static void vmx_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	ulong cr0;</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		return;</span>
<span class="quote">&gt;&gt;&gt; -	vcpu-&gt;fpu_active = 1;</span>
<span class="quote">&gt;&gt;&gt; -	cr0 = vmcs_readl(GUEST_CR0);</span>
<span class="quote">&gt;&gt;&gt; -	cr0 &amp;= ~(X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt;&gt; -	cr0 |= kvm_read_cr0_bits(vcpu, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt;&gt; -	vmcs_writel(GUEST_CR0, cr0);</span>
<span class="quote">&gt;&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt;&gt; -	if (is_guest_mode(vcpu))</span>
<span class="quote">&gt;&gt;&gt; -		vcpu-&gt;arch.cr0_guest_owned_bits &amp;=</span>
<span class="quote">&gt;&gt;&gt; -			~get_vmcs12(vcpu)-&gt;cr0_guest_host_mask;</span>
<span class="quote">&gt;&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;&gt; @@ -2377,33 +2356,6 @@ static inline unsigned long nested_read_cr4(struct vmcs12 *fields)</span>
<span class="quote">&gt;&gt;&gt;  		(fields-&gt;cr4_read_shadow &amp; fields-&gt;cr4_guest_host_mask);</span>
<span class="quote">&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -static void vmx_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	/* Note that there is no vcpu-&gt;fpu_active = 0 here. The caller must</span>
<span class="quote">&gt;&gt;&gt; -	 * set this *before* calling this function.</span>
<span class="quote">&gt;&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt;&gt; -	vmx_decache_cr0_guest_bits(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	vmcs_set_bits(GUEST_CR0, X86_CR0_TS | X86_CR0_MP);</span>
<span class="quote">&gt;&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = 0;</span>
<span class="quote">&gt;&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt;&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt;&gt; -		 * L1&#39;s specified read shadow might not contain the TS bit,</span>
<span class="quote">&gt;&gt;&gt; -		 * so now that we turned on shadowing of this bit, we need to</span>
<span class="quote">&gt;&gt;&gt; -		 * set this bit of the shadow. Like in nested_vmx_run we need</span>
<span class="quote">&gt;&gt;&gt; -		 * nested_read_cr0(vmcs12), but vmcs12-&gt;guest_cr0 is not yet</span>
<span class="quote">&gt;&gt;&gt; -		 * up-to-date here because we just decached cr0.TS (and we&#39;ll</span>
<span class="quote">&gt;&gt;&gt; -		 * only update vmcs12-&gt;guest_cr0 on nested exit).</span>
<span class="quote">&gt;&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt;&gt; -		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -		vmcs12-&gt;guest_cr0 = (vmcs12-&gt;guest_cr0 &amp; ~X86_CR0_TS) |</span>
<span class="quote">&gt;&gt;&gt; -			(vcpu-&gt;arch.cr0 &amp; X86_CR0_TS);</span>
<span class="quote">&gt;&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));</span>
<span class="quote">&gt;&gt;&gt; -	} else</span>
<span class="quote">&gt;&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW, vcpu-&gt;arch.cr0);</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long rflags, save_rflags;</span>
<span class="quote">&gt;&gt;&gt; @@ -4232,9 +4184,6 @@ static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
<span class="quote">&gt;&gt;&gt;  	if (enable_ept)</span>
<span class="quote">&gt;&gt;&gt;  		ept_update_paging_mode_cr0(&amp;hw_cr0, cr0, vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (!vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		hw_cr0 |= X86_CR0_TS | X86_CR0_MP;</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  	vmcs_writel(CR0_READ_SHADOW, cr0);</span>
<span class="quote">&gt;&gt;&gt;  	vmcs_writel(GUEST_CR0, hw_cr0);</span>
<span class="quote">&gt;&gt;&gt;  	vcpu-&gt;arch.cr0 = cr0;</span>
<span class="quote">&gt;&gt;&gt; @@ -5321,7 +5270,9 @@ static int vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
<span class="quote">&gt;&gt;&gt;  	/* 22.2.1, 20.8.1 */</span>
<span class="quote">&gt;&gt;&gt;  	vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);</span>
<span class="quote">&gt;&gt;&gt; +	vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt;&gt; +	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;  	set_cr4_guest_host_mask(vmx);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	if (vmx_xsaves_supported())</span>
<span class="quote">&gt;&gt;&gt; @@ -5425,7 +5376,7 @@ static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
<span class="quote">&gt;&gt;&gt;  	vmx_set_cr0(vcpu, cr0); /* enter rmode */</span>
<span class="quote">&gt;&gt;&gt;  	vmx_set_cr4(vcpu, 0);</span>
<span class="quote">&gt;&gt;&gt;  	vmx_set_efer(vcpu, 0);</span>
<span class="quote">&gt;&gt;&gt; -	vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;  	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	vpid_sync_context(vmx-&gt;vpid);</span>
<span class="quote">&gt;&gt;&gt; @@ -5698,11 +5649,6 @@ static int handle_exception(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  	if (is_nmi(intr_info))</span>
<span class="quote">&gt;&gt;&gt;  		return 1;  /* already handled by vmx_vcpu_run() */</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (is_no_device(intr_info)) {</span>
<span class="quote">&gt;&gt;&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -		return 1;</span>
<span class="quote">&gt;&gt;&gt; -	}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  	if (is_invalid_opcode(intr_info)) {</span>
<span class="quote">&gt;&gt;&gt;  		if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt;&gt;  			kvm_queue_exception(vcpu, UD_VECTOR);</span>
<span class="quote">&gt;&gt;&gt; @@ -5892,22 +5838,6 @@ static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)</span>
<span class="quote">&gt;&gt;&gt;  		return kvm_set_cr4(vcpu, val);</span>
<span class="quote">&gt;&gt;&gt;  }</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -/* called to set cr0 as appropriate for clts instruction exit. */</span>
<span class="quote">&gt;&gt;&gt; -static void handle_clts(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt; -{</span>
<span class="quote">&gt;&gt;&gt; -	if (is_guest_mode(vcpu)) {</span>
<span class="quote">&gt;&gt;&gt; -		/*</span>
<span class="quote">&gt;&gt;&gt; -		 * We get here when L2 did CLTS, and L1 didn&#39;t shadow CR0.TS</span>
<span class="quote">&gt;&gt;&gt; -		 * but we did (!fpu_active). We need to keep GUEST_CR0.TS on,</span>
<span class="quote">&gt;&gt;&gt; -		 * just pretend it&#39;s off (also in arch.cr0 for fpu_activate).</span>
<span class="quote">&gt;&gt;&gt; -		 */</span>
<span class="quote">&gt;&gt;&gt; -		vmcs_writel(CR0_READ_SHADOW,</span>
<span class="quote">&gt;&gt;&gt; -			vmcs_readl(CR0_READ_SHADOW) &amp; ~X86_CR0_TS);</span>
<span class="quote">&gt;&gt;&gt; -		vcpu-&gt;arch.cr0 &amp;= ~X86_CR0_TS;</span>
<span class="quote">&gt;&gt;&gt; -	} else</span>
<span class="quote">&gt;&gt;&gt; -		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;&gt;&gt; -}</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  {</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long exit_qualification, val;</span>
<span class="quote">&gt;&gt;&gt; @@ -5953,9 +5883,9 @@ static int handle_cr(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;&gt;  		break;</span>
<span class="quote">&gt;&gt;&gt;  	case 2: /* clts */</span>
<span class="quote">&gt;&gt;&gt; -		handle_clts(vcpu);</span>
<span class="quote">&gt;&gt;&gt; +		WARN_ONCE(1, &quot;Guest should always own CR0.TS&quot;);</span>
<span class="quote">&gt;&gt;&gt; +		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="quote">&gt;&gt;&gt;  		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));</span>
<span class="quote">&gt;&gt;&gt; -		vmx_fpu_activate(vcpu);</span>
<span class="quote">&gt;&gt;&gt;  		return kvm_skip_emulated_instruction(vcpu);</span>
<span class="quote">&gt;&gt;&gt;  	case 1: /*mov from cr*/</span>
<span class="quote">&gt;&gt;&gt;  		switch (cr) {</span>
<span class="quote">&gt;&gt;&gt; @@ -10349,8 +10279,8 @@ static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
<span class="quote">&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt; -	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, with possibly a modified</span>
<span class="quote">&gt;&gt;&gt; -	 * TS bit (for lazy fpu) and bits which we consider mandatory enabled.</span>
<span class="quote">&gt;&gt;&gt; +	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, possibly modifying those</span>
<span class="quote">&gt;&gt;&gt; +	 * bits which we consider mandatory enabled.</span>
<span class="quote">&gt;&gt;&gt;  	 * The CR0_READ_SHADOW is what L2 should have expected to read given</span>
<span class="quote">&gt;&gt;&gt;  	 * the specifications by L1; It&#39;s not enough to take</span>
<span class="quote">&gt;&gt;&gt;  	 * vmcs12-&gt;cr0_read_shadow because on our cr0_guest_host_mask we we</span>
<span class="quote">&gt;&gt;&gt; @@ -10963,24 +10893,15 @@ static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;&gt;&gt;  	vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);</span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * Note that calling vmx_set_cr0 is important, even if cr0 hasn&#39;t</span>
<span class="quote">&gt;&gt;&gt; -	 * actually changed, because it depends on the current state of</span>
<span class="quote">&gt;&gt;&gt; -	 * fpu_active (which may have changed).</span>
<span class="quote">&gt;&gt;&gt; -	 * Note that vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt;&gt;&gt; +	 * actually changed, because vmx_set_cr0 refers to efer set above.</span>
<span class="quote">&gt;&gt;&gt; +	 *</span>
<span class="quote">&gt;&gt;&gt; +	 * CR0_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt;&gt;&gt; +	 * (KVM doesn&#39;t change it);</span>
<span class="quote">&gt;&gt;&gt;  	 */</span>
<span class="quote">&gt;&gt;&gt; +	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="quote">&gt;&gt;&gt;  	vmx_set_cr0(vcpu, vmcs12-&gt;host_cr0);</span>
<span class="quote">&gt;&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt;&gt; -	 * If we did fpu_activate()/fpu_deactivate() during L2&#39;s run, we need</span>
<span class="quote">&gt;&gt;&gt; -	 * to apply the same changes to L1&#39;s vmcs. We just set cr0 correctly,</span>
<span class="quote">&gt;&gt;&gt; -	 * but we also need to update cr0_guest_host_mask and exception_bitmap.</span>
<span class="quote">&gt;&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt;&gt; -	update_exception_bitmap(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	vcpu-&gt;arch.cr0_guest_owned_bits = (vcpu-&gt;fpu_active ? X86_CR0_TS : 0);</span>
<span class="quote">&gt;&gt;&gt; -	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	/*</span>
<span class="quote">&gt;&gt;&gt; -	 * Note that CR4_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="quote">&gt;&gt;&gt; -	 * (KVM doesn&#39;t change it)- no reason to call set_cr4_guest_host_mask();</span>
<span class="quote">&gt;&gt;&gt; -	 */</span>
<span class="quote">&gt;&gt;&gt; +	/* Same as above - no reason to call set_cr4_guest_host_mask().  */</span>
<span class="quote">&gt;&gt;&gt;  	vcpu-&gt;arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);</span>
<span class="quote">&gt;&gt;&gt;  	kvm_set_cr4(vcpu, vmcs12-&gt;host_cr4);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; @@ -11609,9 +11530,6 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	.get_pkru = vmx_get_pkru,</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	.fpu_activate = vmx_fpu_activate,</span>
<span class="quote">&gt;&gt;&gt; -	.fpu_deactivate = vmx_fpu_deactivate,</span>
<span class="quote">&gt;&gt;&gt; -</span>
<span class="quote">&gt;&gt;&gt;  	.tlb_flush = vmx_flush_tlb,</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	.run = vmx_vcpu_run,</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt;&gt; index 8d3047c8cce7..c48404017e4f 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/kvm/x86.c</span>
<span class="quote">&gt;&gt;&gt; @@ -6751,10 +6751,6 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  			r = 0;</span>
<span class="quote">&gt;&gt;&gt;  			goto out;</span>
<span class="quote">&gt;&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;&gt; -		if (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {</span>
<span class="quote">&gt;&gt;&gt; -			vcpu-&gt;fpu_active = 0;</span>
<span class="quote">&gt;&gt;&gt; -			kvm_x86_ops-&gt;fpu_deactivate(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -		}</span>
<span class="quote">&gt;&gt;&gt;  		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {</span>
<span class="quote">&gt;&gt;&gt;  			/* Page is swapped out. Do synthetic halt */</span>
<span class="quote">&gt;&gt;&gt;  			vcpu-&gt;arch.apf.halted = true;</span>
<span class="quote">&gt;&gt;&gt; @@ -6856,8 +6852,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
<span class="quote">&gt;&gt;&gt;  	preempt_disable();</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	kvm_x86_ops-&gt;prepare_guest_switch(vcpu);</span>
<span class="quote">&gt;&gt;&gt; -	if (vcpu-&gt;fpu_active)</span>
<span class="quote">&gt;&gt;&gt; -		kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;&gt;&gt; +	kvm_load_guest_fpu(vcpu);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt</span>
<span class="quote">&gt;&gt;&gt; diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; index 2db458ee94b0..8d69d5150748 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/include/linux/kvm_host.h</span>
<span class="quote">&gt;&gt;&gt; @@ -221,7 +221,6 @@ struct kvm_vcpu {</span>
<span class="quote">&gt;&gt;&gt;  	struct mutex mutex;</span>
<span class="quote">&gt;&gt;&gt;  	struct kvm_run *run;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	int fpu_active;</span>
<span class="quote">&gt;&gt;&gt;  	int guest_fpu_loaded, guest_xcr0_loaded;</span>
<span class="quote">&gt;&gt;&gt;  	struct swait_queue_head wq;</span>
<span class="quote">&gt;&gt;&gt;  	struct pid *pid;</span>
<span class="quote">&gt;&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="p_header">index e4f13e714bcf..74ef58c8ff53 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -55,7 +55,6 @@</span> <span class="p_context"></span>
 #define KVM_REQ_TRIPLE_FAULT      10
 #define KVM_REQ_MMU_SYNC          11
 #define KVM_REQ_CLOCK_UPDATE      12
<span class="p_del">-#define KVM_REQ_DEACTIVATE_FPU    13</span>
 #define KVM_REQ_EVENT             14
 #define KVM_REQ_APF_HALT          15
 #define KVM_REQ_STEAL_UPDATE      16
<span class="p_chunk">@@ -936,8 +935,6 @@</span> <span class="p_context"> struct kvm_x86_ops {</span>
 	unsigned long (*get_rflags)(struct kvm_vcpu *vcpu);
 	void (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);
 	u32 (*get_pkru)(struct kvm_vcpu *vcpu);
<span class="p_del">-	void (*fpu_activate)(struct kvm_vcpu *vcpu);</span>
<span class="p_del">-	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);</span>
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="p_header">index c0e2036217ad..1d155cc56629 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.c</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.c</span>
<span class="p_chunk">@@ -123,8 +123,6 @@</span> <span class="p_context"> int kvm_update_cpuid(struct kvm_vcpu *vcpu)</span>
 	if (best &amp;&amp; (best-&gt;eax &amp; (F(XSAVES) | F(XSAVEC))))
 		best-&gt;ebx = xstate_required_size(vcpu-&gt;arch.xcr0, true);
 
<span class="p_del">-	kvm_x86_ops-&gt;fpu_activate(vcpu);</span>
<span class="p_del">-</span>
 	/*
 	 * The existing code assumes virtual address is 48-bit in the canonical
 	 * address checks; exit if it is ever changed.
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index 4e5905a1ce70..d1efe2c62b3f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -1157,7 +1157,6 @@</span> <span class="p_context"> static void init_vmcb(struct vcpu_svm *svm)</span>
 	struct vmcb_control_area *control = &amp;svm-&gt;vmcb-&gt;control;
 	struct vmcb_save_area *save = &amp;svm-&gt;vmcb-&gt;save;
 
<span class="p_del">-	svm-&gt;vcpu.fpu_active = 1;</span>
 	svm-&gt;vcpu.arch.hflags = 0;
 
 	set_cr_intercept(svm, INTERCEPT_CR0_READ);
<span class="p_chunk">@@ -1899,15 +1898,12 @@</span> <span class="p_context"> static void update_cr0_intercept(struct vcpu_svm *svm)</span>
 	ulong gcr0 = svm-&gt;vcpu.arch.cr0;
 	u64 *hcr0 = &amp;svm-&gt;vmcb-&gt;save.cr0;
 
<span class="p_del">-	if (!svm-&gt;vcpu.fpu_active)</span>
<span class="p_del">-		*hcr0 |= SVM_CR0_SELECTIVE_MASK;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="p_del">-			| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
<span class="p_add">+	*hcr0 = (*hcr0 &amp; ~SVM_CR0_SELECTIVE_MASK)</span>
<span class="p_add">+		| (gcr0 &amp; SVM_CR0_SELECTIVE_MASK);</span>
 
 	mark_dirty(svm-&gt;vmcb, VMCB_CR);
 
<span class="p_del">-	if (gcr0 == *hcr0 &amp;&amp; svm-&gt;vcpu.fpu_active) {</span>
<span class="p_add">+	if (gcr0 == *hcr0) {</span>
 		clr_cr_intercept(svm, INTERCEPT_CR0_READ);
 		clr_cr_intercept(svm, INTERCEPT_CR0_WRITE);
 	} else {
<span class="p_chunk">@@ -1938,8 +1934,6 @@</span> <span class="p_context"> static void svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
 	if (!npt_enabled)
 		cr0 |= X86_CR0_PG | X86_CR0_WP;
 
<span class="p_del">-	if (!vcpu-&gt;fpu_active)</span>
<span class="p_del">-		cr0 |= X86_CR0_TS;</span>
 	/*
 	 * re-enable caching here because the QEMU bios
 	 * does not do it - this results in some delay at
<span class="p_chunk">@@ -2158,22 +2152,6 @@</span> <span class="p_context"> static int ac_interception(struct vcpu_svm *svm)</span>
 	return 1;
 }
 
<span class="p_del">-static void svm_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="p_del">-</span>
<span class="p_del">-	clr_exception_intercept(svm, NM_VECTOR);</span>
<span class="p_del">-</span>
<span class="p_del">-	svm-&gt;vcpu.fpu_active = 1;</span>
<span class="p_del">-	update_cr0_intercept(svm);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int nm_interception(struct vcpu_svm *svm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	svm_fpu_activate(&amp;svm-&gt;vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static bool is_erratum_383(void)
 {
 	int err, i;
<span class="p_chunk">@@ -2571,9 +2549,6 @@</span> <span class="p_context"> static int nested_svm_exit_special(struct vcpu_svm *svm)</span>
 		if (!npt_enabled &amp;&amp; svm-&gt;apf_reason == 0)
 			return NESTED_EXIT_HOST;
 		break;
<span class="p_del">-	case SVM_EXIT_EXCP_BASE + NM_VECTOR:</span>
<span class="p_del">-		nm_interception(svm);</span>
<span class="p_del">-		break;</span>
 	default:
 		break;
 	}
<span class="p_chunk">@@ -4018,7 +3993,6 @@</span> <span class="p_context"> static int (*const svm_exit_handlers[])(struct vcpu_svm *svm) = {</span>
 	[SVM_EXIT_EXCP_BASE + BP_VECTOR]	= bp_interception,
 	[SVM_EXIT_EXCP_BASE + UD_VECTOR]	= ud_interception,
 	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,
<span class="p_del">-	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,</span>
 	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,
 	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,
 	[SVM_EXIT_INTR]				= intr_interception,
<span class="p_chunk">@@ -5072,14 +5046,6 @@</span> <span class="p_context"> static bool svm_has_wbinvd_exit(void)</span>
 	return true;
 }
 
<span class="p_del">-static void svm_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vcpu_svm *svm = to_svm(vcpu);</span>
<span class="p_del">-</span>
<span class="p_del">-	set_exception_intercept(svm, NM_VECTOR);</span>
<span class="p_del">-	update_cr0_intercept(svm);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define PRE_EX(exit)  { .exit_code = (exit), \
 			.stage = X86_ICPT_PRE_EXCEPT, }
 #define POST_EX(exit) { .exit_code = (exit), \
<span class="p_chunk">@@ -5340,9 +5306,6 @@</span> <span class="p_context"> static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
 
 	.get_pkru = svm_get_pkru,
 
<span class="p_del">-	.fpu_activate = svm_fpu_activate,</span>
<span class="p_del">-	.fpu_deactivate = svm_fpu_deactivate,</span>
<span class="p_del">-</span>
 	.tlb_flush = svm_flush_tlb,
 
 	.run = svm_vcpu_run,
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 0e0b5d09597e..9856b73a21ad 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -1856,7 +1856,7 @@</span> <span class="p_context"> static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
 	u32 eb;
 
 	eb = (1u &lt;&lt; PF_VECTOR) | (1u &lt;&lt; UD_VECTOR) | (1u &lt;&lt; MC_VECTOR) |
<span class="p_del">-	     (1u &lt;&lt; NM_VECTOR) | (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
<span class="p_add">+	     (1u &lt;&lt; DB_VECTOR) | (1u &lt;&lt; AC_VECTOR);</span>
 	if ((vcpu-&gt;guest_debug &amp;
 	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==
 	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))
<span class="p_chunk">@@ -1865,8 +1865,6 @@</span> <span class="p_context"> static void update_exception_bitmap(struct kvm_vcpu *vcpu)</span>
 		eb = ~0;
 	if (enable_ept)
 		eb &amp;= ~(1u &lt;&lt; PF_VECTOR); /* bypass_guest_pf = 0 */
<span class="p_del">-	if (vcpu-&gt;fpu_active)</span>
<span class="p_del">-		eb &amp;= ~(1u &lt;&lt; NM_VECTOR);</span>
 
 	/* When we are running a nested L2 guest and L1 specified for it a
 	 * certain exception bitmap, we must trap the same exceptions and pass
<span class="p_chunk">@@ -2340,25 +2338,6 @@</span> <span class="p_context"> static void vmx_vcpu_put(struct kvm_vcpu *vcpu)</span>
 	}
 }
 
<span class="p_del">-static void vmx_fpu_activate(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	ulong cr0;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (vcpu-&gt;fpu_active)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	vcpu-&gt;fpu_active = 1;</span>
<span class="p_del">-	cr0 = vmcs_readl(GUEST_CR0);</span>
<span class="p_del">-	cr0 &amp;= ~(X86_CR0_TS | X86_CR0_MP);</span>
<span class="p_del">-	cr0 |= kvm_read_cr0_bits(vcpu, X86_CR0_TS | X86_CR0_MP);</span>
<span class="p_del">-	vmcs_writel(GUEST_CR0, cr0);</span>
<span class="p_del">-	update_exception_bitmap(vcpu);</span>
<span class="p_del">-	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="p_del">-	if (is_guest_mode(vcpu))</span>
<span class="p_del">-		vcpu-&gt;arch.cr0_guest_owned_bits &amp;=</span>
<span class="p_del">-			~get_vmcs12(vcpu)-&gt;cr0_guest_host_mask;</span>
<span class="p_del">-	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);
 
 /*
<span class="p_chunk">@@ -2377,33 +2356,6 @@</span> <span class="p_context"> static inline unsigned long nested_read_cr4(struct vmcs12 *fields)</span>
 		(fields-&gt;cr4_read_shadow &amp; fields-&gt;cr4_guest_host_mask);
 }
 
<span class="p_del">-static void vmx_fpu_deactivate(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Note that there is no vcpu-&gt;fpu_active = 0 here. The caller must</span>
<span class="p_del">-	 * set this *before* calling this function.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	vmx_decache_cr0_guest_bits(vcpu);</span>
<span class="p_del">-	vmcs_set_bits(GUEST_CR0, X86_CR0_TS | X86_CR0_MP);</span>
<span class="p_del">-	update_exception_bitmap(vcpu);</span>
<span class="p_del">-	vcpu-&gt;arch.cr0_guest_owned_bits = 0;</span>
<span class="p_del">-	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
<span class="p_del">-	if (is_guest_mode(vcpu)) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * L1&#39;s specified read shadow might not contain the TS bit,</span>
<span class="p_del">-		 * so now that we turned on shadowing of this bit, we need to</span>
<span class="p_del">-		 * set this bit of the shadow. Like in nested_vmx_run we need</span>
<span class="p_del">-		 * nested_read_cr0(vmcs12), but vmcs12-&gt;guest_cr0 is not yet</span>
<span class="p_del">-		 * up-to-date here because we just decached cr0.TS (and we&#39;ll</span>
<span class="p_del">-		 * only update vmcs12-&gt;guest_cr0 on nested exit).</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		struct vmcs12 *vmcs12 = get_vmcs12(vcpu);</span>
<span class="p_del">-		vmcs12-&gt;guest_cr0 = (vmcs12-&gt;guest_cr0 &amp; ~X86_CR0_TS) |</span>
<span class="p_del">-			(vcpu-&gt;arch.cr0 &amp; X86_CR0_TS);</span>
<span class="p_del">-		vmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		vmcs_writel(CR0_READ_SHADOW, vcpu-&gt;arch.cr0);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)
 {
 	unsigned long rflags, save_rflags;
<span class="p_chunk">@@ -4232,9 +4184,6 @@</span> <span class="p_context"> static void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)</span>
 	if (enable_ept)
 		ept_update_paging_mode_cr0(&amp;hw_cr0, cr0, vcpu);
 
<span class="p_del">-	if (!vcpu-&gt;fpu_active)</span>
<span class="p_del">-		hw_cr0 |= X86_CR0_TS | X86_CR0_MP;</span>
<span class="p_del">-</span>
 	vmcs_writel(CR0_READ_SHADOW, cr0);
 	vmcs_writel(GUEST_CR0, hw_cr0);
 	vcpu-&gt;arch.cr0 = cr0;
<span class="p_chunk">@@ -5321,7 +5270,9 @@</span> <span class="p_context"> static int vmx_vcpu_setup(struct vcpu_vmx *vmx)</span>
 	/* 22.2.1, 20.8.1 */
 	vm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);
 
<span class="p_del">-	vmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);</span>
<span class="p_add">+	vmx-&gt;vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
<span class="p_add">+	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);</span>
<span class="p_add">+</span>
 	set_cr4_guest_host_mask(vmx);
 
 	if (vmx_xsaves_supported())
<span class="p_chunk">@@ -5425,7 +5376,7 @@</span> <span class="p_context"> static void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)</span>
 	vmx_set_cr0(vcpu, cr0); /* enter rmode */
 	vmx_set_cr4(vcpu, 0);
 	vmx_set_efer(vcpu, 0);
<span class="p_del">-	vmx_fpu_activate(vcpu);</span>
<span class="p_add">+</span>
 	update_exception_bitmap(vcpu);
 
 	vpid_sync_context(vmx-&gt;vpid);
<span class="p_chunk">@@ -5698,11 +5649,6 @@</span> <span class="p_context"> static int handle_exception(struct kvm_vcpu *vcpu)</span>
 	if (is_nmi(intr_info))
 		return 1;  /* already handled by vmx_vcpu_run() */
 
<span class="p_del">-	if (is_no_device(intr_info)) {</span>
<span class="p_del">-		vmx_fpu_activate(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	if (is_invalid_opcode(intr_info)) {
 		if (is_guest_mode(vcpu)) {
 			kvm_queue_exception(vcpu, UD_VECTOR);
<span class="p_chunk">@@ -5892,22 +5838,6 @@</span> <span class="p_context"> static int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)</span>
 		return kvm_set_cr4(vcpu, val);
 }
 
<span class="p_del">-/* called to set cr0 as appropriate for clts instruction exit. */</span>
<span class="p_del">-static void handle_clts(struct kvm_vcpu *vcpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (is_guest_mode(vcpu)) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We get here when L2 did CLTS, and L1 didn&#39;t shadow CR0.TS</span>
<span class="p_del">-		 * but we did (!fpu_active). We need to keep GUEST_CR0.TS on,</span>
<span class="p_del">-		 * just pretend it&#39;s off (also in arch.cr0 for fpu_activate).</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		vmcs_writel(CR0_READ_SHADOW,</span>
<span class="p_del">-			vmcs_readl(CR0_READ_SHADOW) &amp; ~X86_CR0_TS);</span>
<span class="p_del">-		vcpu-&gt;arch.cr0 &amp;= ~X86_CR0_TS;</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static int handle_cr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification, val;
<span class="p_chunk">@@ -5953,9 +5883,9 @@</span> <span class="p_context"> static int handle_cr(struct kvm_vcpu *vcpu)</span>
 		}
 		break;
 	case 2: /* clts */
<span class="p_del">-		handle_clts(vcpu);</span>
<span class="p_add">+		WARN_ONCE(1, &quot;Guest should always own CR0.TS&quot;);</span>
<span class="p_add">+		vmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));</span>
 		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
<span class="p_del">-		vmx_fpu_activate(vcpu);</span>
 		return kvm_skip_emulated_instruction(vcpu);
 	case 1: /*mov from cr*/
 		switch (cr) {
<span class="p_chunk">@@ -10349,8 +10279,8 @@</span> <span class="p_context"> static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,</span>
 	}
 
 	/*
<span class="p_del">-	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, with possibly a modified</span>
<span class="p_del">-	 * TS bit (for lazy fpu) and bits which we consider mandatory enabled.</span>
<span class="p_add">+	 * This sets GUEST_CR0 to vmcs12-&gt;guest_cr0, possibly modifying those</span>
<span class="p_add">+	 * bits which we consider mandatory enabled.</span>
 	 * The CR0_READ_SHADOW is what L2 should have expected to read given
 	 * the specifications by L1; It&#39;s not enough to take
 	 * vmcs12-&gt;cr0_read_shadow because on our cr0_guest_host_mask we we
<span class="p_chunk">@@ -10963,24 +10893,15 @@</span> <span class="p_context"> static void load_vmcs12_host_state(struct kvm_vcpu *vcpu,</span>
 	vmx_set_rflags(vcpu, X86_EFLAGS_FIXED);
 	/*
 	 * Note that calling vmx_set_cr0 is important, even if cr0 hasn&#39;t
<span class="p_del">-	 * actually changed, because it depends on the current state of</span>
<span class="p_del">-	 * fpu_active (which may have changed).</span>
<span class="p_del">-	 * Note that vmx_set_cr0 refers to efer set above.</span>
<span class="p_add">+	 * actually changed, because vmx_set_cr0 refers to efer set above.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * CR0_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="p_add">+	 * (KVM doesn&#39;t change it);</span>
 	 */
<span class="p_add">+	vcpu-&gt;arch.cr0_guest_owned_bits = X86_CR0_TS;</span>
 	vmx_set_cr0(vcpu, vmcs12-&gt;host_cr0);
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If we did fpu_activate()/fpu_deactivate() during L2&#39;s run, we need</span>
<span class="p_del">-	 * to apply the same changes to L1&#39;s vmcs. We just set cr0 correctly,</span>
<span class="p_del">-	 * but we also need to update cr0_guest_host_mask and exception_bitmap.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	update_exception_bitmap(vcpu);</span>
<span class="p_del">-	vcpu-&gt;arch.cr0_guest_owned_bits = (vcpu-&gt;fpu_active ? X86_CR0_TS : 0);</span>
<span class="p_del">-	vmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu-&gt;arch.cr0_guest_owned_bits);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Note that CR4_GUEST_HOST_MASK is already set in the original vmcs01</span>
<span class="p_del">-	 * (KVM doesn&#39;t change it)- no reason to call set_cr4_guest_host_mask();</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	/* Same as above - no reason to call set_cr4_guest_host_mask().  */</span>
 	vcpu-&gt;arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);
 	kvm_set_cr4(vcpu, vmcs12-&gt;host_cr4);
 
<span class="p_chunk">@@ -11609,9 +11530,6 @@</span> <span class="p_context"> static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
 
 	.get_pkru = vmx_get_pkru,
 
<span class="p_del">-	.fpu_activate = vmx_fpu_activate,</span>
<span class="p_del">-	.fpu_deactivate = vmx_fpu_deactivate,</span>
<span class="p_del">-</span>
 	.tlb_flush = vmx_flush_tlb,
 
 	.run = vmx_vcpu_run,
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index 8d3047c8cce7..c48404017e4f 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -6751,10 +6751,6 @@</span> <span class="p_context"> static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
 			r = 0;
 			goto out;
 		}
<span class="p_del">-		if (kvm_check_request(KVM_REQ_DEACTIVATE_FPU, vcpu)) {</span>
<span class="p_del">-			vcpu-&gt;fpu_active = 0;</span>
<span class="p_del">-			kvm_x86_ops-&gt;fpu_deactivate(vcpu);</span>
<span class="p_del">-		}</span>
 		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
 			/* Page is swapped out. Do synthetic halt */
 			vcpu-&gt;arch.apf.halted = true;
<span class="p_chunk">@@ -6856,8 +6852,7 @@</span> <span class="p_context"> static int vcpu_enter_guest(struct kvm_vcpu *vcpu)</span>
 	preempt_disable();
 
 	kvm_x86_ops-&gt;prepare_guest_switch(vcpu);
<span class="p_del">-	if (vcpu-&gt;fpu_active)</span>
<span class="p_del">-		kvm_load_guest_fpu(vcpu);</span>
<span class="p_add">+	kvm_load_guest_fpu(vcpu);</span>
 
 	/*
 	 * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt
<span class="p_header">diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="p_header">index 2db458ee94b0..8d69d5150748 100644</span>
<span class="p_header">--- a/include/linux/kvm_host.h</span>
<span class="p_header">+++ b/include/linux/kvm_host.h</span>
<span class="p_chunk">@@ -221,7 +221,6 @@</span> <span class="p_context"> struct kvm_vcpu {</span>
 	struct mutex mutex;
 	struct kvm_run *run;
 
<span class="p_del">-	int fpu_active;</span>
 	int guest_fpu_loaded, guest_xcr0_loaded;
 	struct swait_queue_head wq;
 	struct pid *pid;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



