
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6v3,02/12] mm: migrate: support non-lru movable page migration - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6v3,02/12] mm: migrate: support non-lru movable page migration</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 31, 2016, 12:01 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160531000117.GB18314@bbox&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9143039/mbox/"
   >mbox</a>
|
   <a href="/patch/9143039/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9143039/">/patch/9143039/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	CA71860754 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 May 2016 00:01:05 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B92E927F17
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 May 2016 00:01:05 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AB40E28183; Tue, 31 May 2016 00:01:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B46A227F17
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 31 May 2016 00:01:03 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1162198AbcEaAAs (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 30 May 2016 20:00:48 -0400
Received: from LGEAMRELO12.lge.com ([156.147.23.52]:54346 &quot;EHLO
	lgeamrelo12.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1161860AbcEaAAp (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 30 May 2016 20:00:45 -0400
Received: from unknown (HELO lgeamrelo01.lge.com) (156.147.1.125)
	by 156.147.23.52 with ESMTP; 31 May 2016 09:00:41 +0900
X-Original-SENDERIP: 156.147.1.125
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO LGEAEXHB02Q.LGE.NET) (165.244.98.150)
	by 156.147.1.125 with ESMTP; 31 May 2016 09:00:41 +0900
X-Original-SENDERIP: 165.244.98.150
X-Original-MAILFROM: minchan@kernel.org
Received: from lgekrmhub08.lge.com (10.185.110.18) by lgeaexhb02q.lge.net
	(165.244.98.150) with Microsoft SMTP Server id 8.3.264.0;
	Tue, 31 May 2016 09:00:39 +0900
Received: from lgemrelse6q.lge.com ([156.147.1.121])          by
	lgekrmhub08.lge.com (Lotus Domino Release 8.5.3FP6) with ESMTP id
	2016053109003883-242419 ;          Tue, 31 May 2016 09:00:38 +0900 
Received: from unknown (HELO bbox) (10.177.223.161)	by 156.147.1.121 with
	ESMTP; 31 May 2016 09:00:39 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
Date: Tue, 31 May 2016 09:01:17 +0900
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
CC: &lt;linux-mm@kvack.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Rafael Aquini &lt;aquini@redhat.com&gt;,
	&lt;virtualization@lists.linux-foundation.org&gt;,
	Jonathan Corbet &lt;corbet@lwn.net&gt;,
	John Einar Reitan &lt;john.reitan@foss.arm.com&gt;,
	&lt;dri-devel@lists.freedesktop.org&gt;,
	Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;,
	Gioh Kim &lt;gi-oh.kim@profitbricks.com&gt;
Subject: [PATCH v6v3 02/12] mm: migrate: support non-lru movable page
	migration
Message-ID: &lt;20160531000117.GB18314@bbox&gt;
References: &lt;1463754225-31311-1-git-send-email-minchan@kernel.org&gt;
	&lt;1463754225-31311-3-git-send-email-minchan@kernel.org&gt;
	&lt;20160530013926.GB8683@bbox&gt;
MIME-Version: 1.0
In-Reply-To: &lt;20160530013926.GB8683@bbox&gt;
User-Agent: Mutt/1.5.21 (2010-09-15)
X-MIMETrack: Itemize by SMTP Server on LGEKRMHUB08/LGE/LG Group(Release
	8.5.3FP6|November 21, 2013) at 2016/05/31 09:00:38,
	Serialize by Router on LGEKRMHUB08/LGE/LG Group(Release
	8.5.3FP6|November 21, 2013) at 2016/05/31 09:00:39,
	Serialize complete at 2016/05/31 09:00:39
Content-Type: text/plain; charset=&quot;us-ascii&quot;
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - May 31, 2016, 12:01 a.m.</div>
<pre class="content">
Per Vlastimi&#39;s review comment.

Thanks for the detail review, Vlastimi!
If you have another concern, feel free to say.
After I resolve all thing, I will send v7 rebased on recent mmotm.

From b14aaeeeeb2d3ac0702c7b2eec36409d74406d43 Mon Sep 17 00:00:00 2001
<span class="from">From: Minchan Kim &lt;minchan@kernel.org&gt;</span>
Date: Fri, 8 Apr 2016 10:34:49 +0900
Subject: [PATCH] mm: migrate: support non-lru movable page migration

We have allowed migration for only LRU pages until now and it was
enough to make high-order pages. But recently, embedded system(e.g.,
webOS, android) uses lots of non-movable pages(e.g., zram, GPU memory)
so we have seen several reports about troubles of small high-order
allocation. For fixing the problem, there were several efforts
(e,g,. enhance compaction algorithm, SLUB fallback to 0-order page,
reserved memory, vmalloc and so on) but if there are lots of
non-movable pages in system, their solutions are void in the long run.

So, this patch is to support facility to change non-movable pages
with movable. For the feature, this patch introduces functions related
to migration to address_space_operations as well as some page flags.

If a driver want to make own pages movable, it should define three functions
which are function pointers of struct address_space_operations.

1. bool (*isolate_page) (struct page *page, isolate_mode_t mode);

What VM expects on isolate_page function of driver is to return *true*
if driver isolates page successfully. On returing true, VM marks the page
as PG_isolated so concurrent isolation in several CPUs skip the page
for isolation. If a driver cannot isolate the page, it should return *false*.

Once page is successfully isolated, VM uses page.lru fields so driver
shouldn&#39;t expect to preserve values in that fields.

2. int (*migratepage) (struct address_space *mapping,
		struct page *newpage, struct page *oldpage, enum migrate_mode);

After isolation, VM calls migratepage of driver with isolated page.
The function of migratepage is to move content of the old page to new page
and set up fields of struct page newpage. Keep in mind that you should
indicate to the VM the oldpage is no longer movable via __ClearPageMovable()
under page_lock if you migrated the oldpage successfully and returns 0.
If driver cannot migrate the page at the moment, driver can return -EAGAIN.
On -EAGAIN, VM will retry page migration in a short time because VM interprets
-EAGAIN as &quot;temporal migration failure&quot;. On returning any error except -EAGAIN,
VM will give up the page migration without retrying in this time.

Driver shouldn&#39;t touch page.lru field VM using in the functions.

3. void (*putback_page)(struct page *);

If migration fails on isolated page, VM should return the isolated page
to the driver so VM calls driver&#39;s putback_page with migration failed page.
In this function, driver should put the isolated page back to the own data
structure.

4. non-lru movable page flags

There are two page flags for supporting non-lru movable page.

* PG_movable

Driver should use the below function to make page movable under page_lock.

	void __SetPageMovable(struct page *page, struct address_space *mapping)

It needs argument of address_space for registering migration family functions
which will be called by VM. Exactly speaking, PG_movable is not a real flag of
struct page. Rather than, VM reuses page-&gt;mapping&#39;s lower bits to represent it.

	#define PAGE_MAPPING_MOVABLE 0x2
	page-&gt;mapping = page-&gt;mapping | PAGE_MAPPING_MOVABLE;

so driver shouldn&#39;t access page-&gt;mapping directly. Instead, driver should
use page_mapping which mask off the low two bits of page-&gt;mapping so it can get
right struct address_space.

For testing of non-lru movable page, VM supports __PageMovable function.
However, it doesn&#39;t guarantee to identify non-lru movable page because
page-&gt;mapping field is unified with other variables in struct page.
As well, if driver releases the page after isolation by VM, page-&gt;mapping
doesn&#39;t have stable value although it has PAGE_MAPPING_MOVABLE
(Look at __ClearPageMovable). But __PageMovable is cheap to catch whether
page is LRU or non-lru movable once the page has been isolated. Because
LRU pages never can have PAGE_MAPPING_MOVABLE in page-&gt;mapping. It is also
good for just peeking to test non-lru movable pages before more expensive
checking with lock_page in pfn scanning to select victim.

For guaranteeing non-lru movable page, VM provides PageMovable function.
Unlike __PageMovable, PageMovable functions validates page-&gt;mapping and
mapping-&gt;a_ops-&gt;isolate_page under lock_page. The lock_page prevents sudden
destroying of page-&gt;mapping.

Driver using __SetPageMovable should clear the flag via __ClearMovablePage
under page_lock before the releasing the page.

* PG_isolated

To prevent concurrent isolation among several CPUs, VM marks isolated page
as PG_isolated under lock_page. So if a CPU encounters PG_isolated non-lru
movable page, it can skip it. Driver doesn&#39;t need to manipulate the flag
because VM will set/clear it automatically. Keep in mind that if driver
sees PG_isolated page, it means the page have been isolated by VM so it
shouldn&#39;t touch page.lru field.
PG_isolated is alias with PG_reclaim flag so driver shouldn&#39;t use the flag
for own purpose.

Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;
Cc: Mel Gorman &lt;mgorman@suse.de&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Rafael Aquini &lt;aquini@redhat.com&gt;
Cc: virtualization@lists.linux-foundation.org
Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;
Cc: John Einar Reitan &lt;john.reitan@foss.arm.com&gt;
Cc: dri-devel@lists.freedesktop.org
Cc: Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;
<span class="signed-off-by">Signed-off-by: Gioh Kim &lt;gi-oh.kim@profitbricks.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 Documentation/filesystems/Locking |   4 +
 Documentation/filesystems/vfs.txt |  11 +++
 Documentation/vm/page_migration   | 107 ++++++++++++++++++++-
 include/linux/compaction.h        |  17 ++++
 include/linux/fs.h                |   2 +
 include/linux/ksm.h               |   3 +-
 include/linux/migrate.h           |   2 +
 include/linux/mm.h                |   1 +
 include/linux/page-flags.h        |  33 +++++--
 mm/compaction.c                   |  85 +++++++++++++----
 mm/ksm.c                          |   4 +-
 mm/migrate.c                      | 192 ++++++++++++++++++++++++++++++++++----
 mm/page_alloc.c                   |   2 +-
 mm/util.c                         |   6 +-
 14 files changed, 417 insertions(+), 52 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - May 31, 2016, 7:52 a.m.</div>
<pre class="content">
On 05/31/2016 02:01 AM, Minchan Kim wrote:
<span class="quote">&gt; Per Vlastimi&#39;s review comment.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thanks for the detail review, Vlastimi!</span>
<span class="quote">&gt; If you have another concern, feel free to say.</span>

I don&#39;t for now :)

[...]
<span class="quote">
&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Rafael Aquini &lt;aquini@redhat.com&gt;</span>
<span class="quote">&gt; Cc: virtualization@lists.linux-foundation.org</span>
<span class="quote">&gt; Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;</span>
<span class="quote">&gt; Cc: John Einar Reitan &lt;john.reitan@foss.arm.com&gt;</span>
<span class="quote">&gt; Cc: dri-devel@lists.freedesktop.org</span>
<span class="quote">&gt; Cc: Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Gioh Kim &lt;gi-oh.kim@profitbricks.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - May 31, 2016, 11:05 p.m.</div>
<pre class="content">
On Tue, May 31, 2016 at 09:52:48AM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 05/31/2016 02:01 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;Per Vlastimi&#39;s review comment.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Thanks for the detail review, Vlastimi!</span>
<span class="quote">&gt; &gt;If you have another concern, feel free to say.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t for now :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt;Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; &gt;Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;</span>
<span class="quote">&gt; &gt;Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; &gt;Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt;Cc: Rafael Aquini &lt;aquini@redhat.com&gt;</span>
<span class="quote">&gt; &gt;Cc: virtualization@lists.linux-foundation.org</span>
<span class="quote">&gt; &gt;Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;</span>
<span class="quote">&gt; &gt;Cc: John Einar Reitan &lt;john.reitan@foss.arm.com&gt;</span>
<span class="quote">&gt; &gt;Cc: dri-devel@lists.freedesktop.org</span>
<span class="quote">&gt; &gt;Cc: Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;</span>
<span class="quote">&gt; &gt;Signed-off-by: Gioh Kim &lt;gi-oh.kim@profitbricks.com&gt;</span>
<span class="quote">&gt; &gt;Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>

Thanks for the review, Vlastimil!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - June 13, 2016, 9:38 a.m.</div>
<pre class="content">
On 05/31/2016 05:31 AM, Minchan Kim wrote:
<span class="quote">&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>

Hello Minchan,

I might be missing something here but does this implementation support the
scenario where these non LRU pages owned by the driver mapped as PTE into
process page table ? Because the &quot;goto out_unlock_both&quot; statement above
skips all the PTE unmap, putting a migration PTE and removing the migration
PTE steps.

Regards
Anshuman
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 15, 2016, 2:32 a.m.</div>
<pre class="content">
Hi,

On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt; &gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt; &gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt; &gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt; &gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;  		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt; &gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt; &gt; +		goto out_unlock_both;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hello Minchan,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt; PTE steps.</span>

You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,
it&#39;s my TODO after landing this work.

Could you share your usecase?

It would be helpful for merging when I wll send patchset.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - June 15, 2016, 6:45 a.m.</div>
<pre class="content">
On 06/15/2016 08:02 AM, Minchan Kim wrote:
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; &gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  </span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  	}</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt;  </span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +	}</span>
<span class="quote">&gt;&gt;&gt; &gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Hello Minchan,</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt;&gt; &gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt;&gt; &gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt;&gt; &gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt;&gt; &gt; PTE steps.</span>
<span class="quote">&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you share your usecase?</span>

Sure.

My driver has privately managed non LRU pages which gets mapped into user space
process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates
the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing
to note here is that the page-&gt;mapping eventually points to struct address_space
(file-&gt;f_mapping) which belongs to the character device file (created using mknod)
which we are using for establishing the mmap() regions in the user space.

Now as per this new framework, all the page&#39;s are to be made __SetPageMovable before
passing the list down to migrate_pages(). Now __SetPageMovable() takes *new* struct
address_space as an argument and replaces the existing page-&gt;mapping. Now thats the
problem, we have lost all our connection to the existing file RMAP information. This
stands as a problem when we try to migrate these non LRU pages which are PTE mapped.
The rmap_walk_file() never finds them in the VMA, skips all the migrate PTE steps and
then the migration eventually fails.

Seems like assigning a new struct address_space to the page through __SetPageMovable()
is the source of the problem. Can it take the existing (file-&gt;f_mapping) as an argument
in there ? Sure, but then can we override file system generic -&gt;isolate(), -&gt;putback(),
-&gt;migratepages() functions ? I dont think so. I am sure, there must be some work around
to fix this problem for the driver. But we need to rethink this framework from supporting
these mapped non LRU pages point of view.

I might be missing something here, feel free to point out.

- Anshuman
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 16, 2016, 12:26 a.m.</div>
<pre class="content">
On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; Hi,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  	}</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +	}</span>
<span class="quote">&gt; &gt;&gt;&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; Hello Minchan,</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt; &gt;&gt; &gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt; &gt;&gt; &gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt; &gt;&gt; &gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt; &gt;&gt; &gt; PTE steps.</span>
<span class="quote">&gt; &gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt; &gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Could you share your usecase?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure.</span>

Thanks a lot!
<span class="quote">
&gt; </span>
<span class="quote">&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>

Hmm, page_add_file_rmap is not exported function. How does your driver can use it?
Do you use vm_insert_pfn?
What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?

I want to make dummy driver to simulate your case.
It would be very helpful to implement/test pte-mapped non-lru page
migration feature. That&#39;s why I ask now.
<span class="quote">
&gt; to note here is that the page-&gt;mapping eventually points to struct address_space</span>
<span class="quote">&gt; (file-&gt;f_mapping) which belongs to the character device file (created using mknod)</span>
<span class="quote">&gt; which we are using for establishing the mmap() regions in the user space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now as per this new framework, all the page&#39;s are to be made __SetPageMovable before</span>
<span class="quote">&gt; passing the list down to migrate_pages(). Now __SetPageMovable() takes *new* struct</span>
<span class="quote">&gt; address_space as an argument and replaces the existing page-&gt;mapping. Now thats the</span>
<span class="quote">&gt; problem, we have lost all our connection to the existing file RMAP information. This</span>

We could change __SetPageMovable doesn&#39;t need mapping argument.
Instead, it just marks PAGE_MAPPING_MOVABLE into page-&gt;mapping.
For that, user should take care of setting page-&gt;mapping earlier than
marking the flag.
<span class="quote">
&gt; stands as a problem when we try to migrate these non LRU pages which are PTE mapped.</span>
<span class="quote">&gt; The rmap_walk_file() never finds them in the VMA, skips all the migrate PTE steps and</span>
<span class="quote">&gt; then the migration eventually fails.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Seems like assigning a new struct address_space to the page through __SetPageMovable()</span>
<span class="quote">&gt; is the source of the problem. Can it take the existing (file-&gt;f_mapping) as an argument</span>
We can set existing file-&gt;f_mapping under the page_lock.
<span class="quote">
&gt; in there ? Sure, but then can we override file system generic -&gt;isolate(), -&gt;putback(),</span>

I don&#39;t get it. Why does it override file system generic functions?
<span class="quote">
&gt; -&gt;migratepages() functions ? I dont think so. I am sure, there must be some work around</span>
<span class="quote">&gt; to fix this problem for the driver. But we need to rethink this framework from supporting</span>
<span class="quote">&gt; these mapped non LRU pages point of view.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I might be missing something here, feel free to point out.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - Anshuman</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - June 16, 2016, 3:42 a.m.</div>
<pre class="content">
On 06/16/2016 05:56 AM, Minchan Kim wrote:
<span class="quote">&gt; On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; PTE steps.</span>
<span class="quote">&gt;&gt;&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt;&gt;&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Could you share your usecase?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks a lot!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt;&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt;&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, page_add_file_rmap is not exported function. How does your driver can use it?</span>

Its not using the function directly, I just re-iterated the sequence of functions
above. (do_set_pte -&gt; page_add_file_rmap) gets called after we grab the page from
driver through (__do_fault-&gt;vma-&gt;vm_ops-&gt;fault()).
<span class="quote">
&gt; Do you use vm_insert_pfn?</span>
<span class="quote">&gt; What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?</span>

I dont use vm_insert_pfn(). Here is the sequence of events how the user space
VMA gets the non LRU pages from the driver.

- Driver registers a character device with &#39;struct file_operations&#39; binding
- Then the &#39;fops-&gt;mmap()&#39; just binds the incoming &#39;struct vma&#39; with a &#39;struct
  vm_operations_struct&#39; which provides the &#39;vmops-&gt;fault()&#39; routine which
  basically traps all page faults on the VMA and provides one page at a time
  through a driver specific allocation routine which hands over non LRU pages

The VMA is not anything special as such. Its what we get when we try to do a
simple mmap() on a file descriptor pointing to a character device. I can
figure out all the VM_* flags it holds after creation.
<span class="quote">
&gt; </span>
<span class="quote">&gt; I want to make dummy driver to simulate your case.</span>

Sure. I hope the above mentioned steps will help you but in case you need more
information, please do let me know.
<span class="quote">
&gt; It would be very helpful to implement/test pte-mapped non-lru page</span>
<span class="quote">&gt; migration feature. That&#39;s why I ask now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; to note here is that the page-&gt;mapping eventually points to struct address_space</span>
<span class="quote">&gt;&gt; (file-&gt;f_mapping) which belongs to the character device file (created using mknod)</span>
<span class="quote">&gt;&gt; which we are using for establishing the mmap() regions in the user space.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Now as per this new framework, all the page&#39;s are to be made __SetPageMovable before</span>
<span class="quote">&gt;&gt; passing the list down to migrate_pages(). Now __SetPageMovable() takes *new* struct</span>
<span class="quote">&gt;&gt; address_space as an argument and replaces the existing page-&gt;mapping. Now thats the</span>
<span class="quote">&gt;&gt; problem, we have lost all our connection to the existing file RMAP information. This</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We could change __SetPageMovable doesn&#39;t need mapping argument.</span>
<span class="quote">&gt; Instead, it just marks PAGE_MAPPING_MOVABLE into page-&gt;mapping.</span>
<span class="quote">&gt; For that, user should take care of setting page-&gt;mapping earlier than</span>
<span class="quote">&gt; marking the flag.</span>

Sounds like a good idea, that way we dont loose the reverse mapping information.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; stands as a problem when we try to migrate these non LRU pages which are PTE mapped.</span>
<span class="quote">&gt;&gt; The rmap_walk_file() never finds them in the VMA, skips all the migrate PTE steps and</span>
<span class="quote">&gt;&gt; then the migration eventually fails.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Seems like assigning a new struct address_space to the page through __SetPageMovable()</span>
<span class="quote">&gt;&gt; is the source of the problem. Can it take the existing (file-&gt;f_mapping) as an argument</span>
<span class="quote">
&gt; We can set existing file-&gt;f_mapping under the page_lock.</span>

Thats another option along with what you mentioned above.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; in there ? Sure, but then can we override file system generic -&gt;isolate(), -&gt;putback(),</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t get it. Why does it override file system generic functions?</span>

Sure it does not, it was just an wild idea to over come the problem.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 16, 2016, 5:37 a.m.</div>
<pre class="content">
On Thu, Jun 16, 2016 at 09:12:07AM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 06/16/2016 05:56 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; Hi,</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  	}</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; PTE steps.</span>
<span class="quote">&gt; &gt;&gt;&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt; &gt;&gt;&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Could you share your usecase?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Sure.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Thanks a lot!</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt; &gt;&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt; &gt;&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hmm, page_add_file_rmap is not exported function. How does your driver can use it?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Its not using the function directly, I just re-iterated the sequence of functions</span>
<span class="quote">&gt; above. (do_set_pte -&gt; page_add_file_rmap) gets called after we grab the page from</span>
<span class="quote">&gt; driver through (__do_fault-&gt;vma-&gt;vm_ops-&gt;fault()).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Do you use vm_insert_pfn?</span>
<span class="quote">&gt; &gt; What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I dont use vm_insert_pfn(). Here is the sequence of events how the user space</span>
<span class="quote">&gt; VMA gets the non LRU pages from the driver.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - Driver registers a character device with &#39;struct file_operations&#39; binding</span>
<span class="quote">&gt; - Then the &#39;fops-&gt;mmap()&#39; just binds the incoming &#39;struct vma&#39; with a &#39;struct</span>
<span class="quote">&gt;   vm_operations_struct&#39; which provides the &#39;vmops-&gt;fault()&#39; routine which</span>
<span class="quote">&gt;   basically traps all page faults on the VMA and provides one page at a time</span>
<span class="quote">&gt;   through a driver specific allocation routine which hands over non LRU pages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The VMA is not anything special as such. Its what we get when we try to do a</span>
<span class="quote">&gt; simple mmap() on a file descriptor pointing to a character device. I can</span>
<span class="quote">&gt; figure out all the VM_* flags it holds after creation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I want to make dummy driver to simulate your case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure. I hope the above mentioned steps will help you but in case you need more</span>
<span class="quote">&gt; information, please do let me know.</span>

I got understood now. :)
I will test it with dummy driver and will Cc&#39;ed when I send a patch.

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - June 27, 2016, 5:51 a.m.</div>
<pre class="content">
On 06/16/2016 11:07 AM, Minchan Kim wrote:
<span class="quote">&gt; On Thu, Jun 16, 2016 at 09:12:07AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 06/16/2016 05:56 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt; On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; PTE steps.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Could you share your usecase?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Sure.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Thanks a lot!</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt;&gt;&gt;&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt;&gt;&gt;&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Hmm, page_add_file_rmap is not exported function. How does your driver can use it?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Its not using the function directly, I just re-iterated the sequence of functions</span>
<span class="quote">&gt;&gt; above. (do_set_pte -&gt; page_add_file_rmap) gets called after we grab the page from</span>
<span class="quote">&gt;&gt; driver through (__do_fault-&gt;vma-&gt;vm_ops-&gt;fault()).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Do you use vm_insert_pfn?</span>
<span class="quote">&gt;&gt;&gt; What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I dont use vm_insert_pfn(). Here is the sequence of events how the user space</span>
<span class="quote">&gt;&gt; VMA gets the non LRU pages from the driver.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - Driver registers a character device with &#39;struct file_operations&#39; binding</span>
<span class="quote">&gt;&gt; - Then the &#39;fops-&gt;mmap()&#39; just binds the incoming &#39;struct vma&#39; with a &#39;struct</span>
<span class="quote">&gt;&gt;   vm_operations_struct&#39; which provides the &#39;vmops-&gt;fault()&#39; routine which</span>
<span class="quote">&gt;&gt;   basically traps all page faults on the VMA and provides one page at a time</span>
<span class="quote">&gt;&gt;   through a driver specific allocation routine which hands over non LRU pages</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The VMA is not anything special as such. Its what we get when we try to do a</span>
<span class="quote">&gt;&gt; simple mmap() on a file descriptor pointing to a character device. I can</span>
<span class="quote">&gt;&gt; figure out all the VM_* flags it holds after creation.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I want to make dummy driver to simulate your case.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure. I hope the above mentioned steps will help you but in case you need more</span>
<span class="quote">&gt;&gt; information, please do let me know.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I got understood now. :)</span>
<span class="quote">&gt; I will test it with dummy driver and will Cc&#39;ed when I send a patch.</span>

Hello Minchan,

Do you have any updates on this ? The V7 of the series still has this limitation.
Did you get a chance to test the driver out ? I am still concerned about how to
handle the struct address_space override problem within the struct page.

- Anshuman
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 28, 2016, 6:39 a.m.</div>
<pre class="content">
On Mon, Jun 27, 2016 at 11:21:01AM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 06/16/2016 11:07 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; On Thu, Jun 16, 2016 at 09:12:07AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt; On 06/16/2016 05:56 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; Hi,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	}</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; PTE steps.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;&gt; Could you share your usecase?</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; Sure.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Thanks a lot!</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Hmm, page_add_file_rmap is not exported function. How does your driver can use it?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Its not using the function directly, I just re-iterated the sequence of functions</span>
<span class="quote">&gt; &gt;&gt; above. (do_set_pte -&gt; page_add_file_rmap) gets called after we grab the page from</span>
<span class="quote">&gt; &gt;&gt; driver through (__do_fault-&gt;vma-&gt;vm_ops-&gt;fault()).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Do you use vm_insert_pfn?</span>
<span class="quote">&gt; &gt;&gt;&gt; What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I dont use vm_insert_pfn(). Here is the sequence of events how the user space</span>
<span class="quote">&gt; &gt;&gt; VMA gets the non LRU pages from the driver.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; - Driver registers a character device with &#39;struct file_operations&#39; binding</span>
<span class="quote">&gt; &gt;&gt; - Then the &#39;fops-&gt;mmap()&#39; just binds the incoming &#39;struct vma&#39; with a &#39;struct</span>
<span class="quote">&gt; &gt;&gt;   vm_operations_struct&#39; which provides the &#39;vmops-&gt;fault()&#39; routine which</span>
<span class="quote">&gt; &gt;&gt;   basically traps all page faults on the VMA and provides one page at a time</span>
<span class="quote">&gt; &gt;&gt;   through a driver specific allocation routine which hands over non LRU pages</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The VMA is not anything special as such. Its what we get when we try to do a</span>
<span class="quote">&gt; &gt;&gt; simple mmap() on a file descriptor pointing to a character device. I can</span>
<span class="quote">&gt; &gt;&gt; figure out all the VM_* flags it holds after creation.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I want to make dummy driver to simulate your case.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Sure. I hope the above mentioned steps will help you but in case you need more</span>
<span class="quote">&gt; &gt;&gt; information, please do let me know.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I got understood now. :)</span>
<span class="quote">&gt; &gt; I will test it with dummy driver and will Cc&#39;ed when I send a patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hello Minchan,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you have any updates on this ? The V7 of the series still has this limitation.</span>
<span class="quote">&gt; Did you get a chance to test the driver out ? I am still concerned about how to</span>
<span class="quote">&gt; handle the struct address_space override problem within the struct page.</span>

Hi Anshuman,

Slow but I am working on that. :) However, as I said, I want to do it
after soft landing of current non-lru-no-mapped page migration to solve
current real field issues.

About the overriding problem of non-lru-mapped-page, I implemented dummy
driver as miscellaneous device and in test_mmap(file_operations.mmap),
I changed a_ops with my address_space_operations.

int test_mmap(struct file *filp, struct vm_area_struct *vma)
{
        filp-&gt;f_mapping-&gt;a_ops = &amp;test_aops;
        vma-&gt;vm_ops = &amp;test_vm_ops;
        vma-&gt;vm_private_data = filp-&gt;private_data;
        return 0;
}

test_aops should have *set_page_dirty* overriding.

static int test_set_pag_dirty(struct page *page)
{
        if (!PageDirty(page))
                SetPageDirty*page);
        return 0;
}

Otherwise, it goes BUG_ON during radix tree operation because
currently try_to_unmap is designed for file-lru pages which lives
in page cache so it propagates page table dirty bit to PG_dirty flag
of struct page by set_page_dirty. And set_page_dirty want to mark
dirty tag in radix tree node but it&#39;s character driver so the page
cache doesn&#39;t have it. That&#39;s why we encounter BUG_ON in radix tree
operation. Anyway, to test, I implemented set_page_dirty in my dummy
driver.

With only that, it doesn&#39;t work because I need to modify migrate.c to
work non-lru-mapped-page and changing PG_isolated flag which is
override of PG_reclaim which is cleared in set_page_dirty.

With that, it seems to work. But I&#39;m not saying it&#39;s right model now
for device drivers. In runtime, replacing filp-&gt;f_mapping-&gt;a_ops with
custom a_ops of own driver seems to be hacky to me.
So, I&#39;m considering now new pseudo fs &quot;movable_inode&quot; which will
support 

struct file *movable_inode_getfile(const char *name,
                        const struct file_operations *fop,
                        const struct address_space_operations *a_ops)
{
        struct path path;
        struct qstr this;
        struct inode *inode;
        struct super_block *sb;

        this.name = name;
        this.len = strlen(name);
        this.hash = 0;
        sb = movable_mnt.mnt_sb;
        patch.denty = d_alloc_pseudo(movable_inode_mnt-&gt;mnt_sb, &amp;this);
        patch.mnt = mntget(movable_inode_mnt);
        
        inode = new_inode(sb);
        ..
        ..
        inode-&gt;i_mapping-&gt;a_ops = a_ops;
        d_instantiate(path.dentry, inode);

        return alloc_file(&amp;path, FMODE_WRITE | FMODE_READ, f_op);
}

And in our driver, we can change vma-&gt;vm_file with new one.

int test_mmap(struct file *filp, struct vm_area_structd *vma)
{
        struct file *newfile = movable_inode_getfile(&quot;[test&quot;],
                                filep-&gt;f_op, &amp;test_aops);
        vma-&gt;vm_file = newfile;
        ..
        ..
}

When I read mmap_region in mm/mmap.c, it&#39;s reasonable usecase
which dirver&#39;s mmap changes vma-&gt;vm_file with own file.

Anyway, it needs many subtle changes in mm/vfs/driver side so
need to review from each maintainers related subsystem so I
want to not be hurry.

Thanks.
<span class="quote">
&gt; </span>
<span class="quote">&gt; - Anshuman</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - June 30, 2016, 5:56 a.m.</div>
<pre class="content">
On 06/28/2016 12:09 PM, Minchan Kim wrote:
<span class="quote">&gt; On Mon, Jun 27, 2016 at 11:21:01AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 06/16/2016 11:07 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt; On Thu, Jun 16, 2016 at 09:12:07AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 06/16/2016 05:56 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Wed, Jun 15, 2016 at 12:15:04PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On 06/15/2016 08:02 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Mon, Jun 13, 2016 at 03:08:19PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On 05/31/2016 05:31 AM, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -791,6 +921,7 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int rc = -EAGAIN;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	int page_was_mapped = 0;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	struct anon_vma *anon_vma = NULL;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	bool is_lru = !__PageMovable(page);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	if (!trylock_page(page)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		if (!force || mode == MIGRATE_ASYNC)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -871,6 +1002,11 @@ static int __unmap_and_move(struct page *page, struct page *newpage,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	if (unlikely(!is_lru)) {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		rc = move_to_new_page(newpage, page, mode);</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +		goto out_unlock_both;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; I might be missing something here but does this implementation support the</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; scenario where these non LRU pages owned by the driver mapped as PTE into</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; process page table ? Because the &quot;goto out_unlock_both&quot; statement above</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; skips all the PTE unmap, putting a migration PTE and removing the migration</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; PTE steps.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; You&#39;re right. Unfortunately, it doesn&#39;t support right now but surely,</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; it&#39;s my TODO after landing this work.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; Could you share your usecase?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Sure.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Thanks a lot!</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; My driver has privately managed non LRU pages which gets mapped into user space</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; process page table through f_ops-&gt;mmap() and vmops-&gt;fault() which then updates</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; the file RMAP (page-&gt;mapping-&gt;i_mmap) through page_add_file_rmap(page). One thing</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Hmm, page_add_file_rmap is not exported function. How does your driver can use it?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Its not using the function directly, I just re-iterated the sequence of functions</span>
<span class="quote">&gt;&gt;&gt;&gt; above. (do_set_pte -&gt; page_add_file_rmap) gets called after we grab the page from</span>
<span class="quote">&gt;&gt;&gt;&gt; driver through (__do_fault-&gt;vma-&gt;vm_ops-&gt;fault()).</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Do you use vm_insert_pfn?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; What type your vma is? VM_PFNMMAP or VM_MIXEDMAP?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; I dont use vm_insert_pfn(). Here is the sequence of events how the user space</span>
<span class="quote">&gt;&gt;&gt;&gt; VMA gets the non LRU pages from the driver.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; - Driver registers a character device with &#39;struct file_operations&#39; binding</span>
<span class="quote">&gt;&gt;&gt;&gt; - Then the &#39;fops-&gt;mmap()&#39; just binds the incoming &#39;struct vma&#39; with a &#39;struct</span>
<span class="quote">&gt;&gt;&gt;&gt;   vm_operations_struct&#39; which provides the &#39;vmops-&gt;fault()&#39; routine which</span>
<span class="quote">&gt;&gt;&gt;&gt;   basically traps all page faults on the VMA and provides one page at a time</span>
<span class="quote">&gt;&gt;&gt;&gt;   through a driver specific allocation routine which hands over non LRU pages</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; The VMA is not anything special as such. Its what we get when we try to do a</span>
<span class="quote">&gt;&gt;&gt;&gt; simple mmap() on a file descriptor pointing to a character device. I can</span>
<span class="quote">&gt;&gt;&gt;&gt; figure out all the VM_* flags it holds after creation.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I want to make dummy driver to simulate your case.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Sure. I hope the above mentioned steps will help you but in case you need more</span>
<span class="quote">&gt;&gt;&gt;&gt; information, please do let me know.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I got understood now. :)</span>
<span class="quote">&gt;&gt;&gt; I will test it with dummy driver and will Cc&#39;ed when I send a patch.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hello Minchan,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Do you have any updates on this ? The V7 of the series still has this limitation.</span>
<span class="quote">&gt;&gt; Did you get a chance to test the driver out ? I am still concerned about how to</span>
<span class="quote">&gt;&gt; handle the struct address_space override problem within the struct page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Anshuman,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Slow but I am working on that. :) However, as I said, I want to do it</span>

I really appreciate. Was just curious about the problem and any potential
solution we can look into.
<span class="quote">
&gt; after soft landing of current non-lru-no-mapped page migration to solve</span>
<span class="quote">&gt; current real field issues.</span>

yeah it makes sense.
<span class="quote">
&gt; </span>
<span class="quote">&gt; About the overriding problem of non-lru-mapped-page, I implemented dummy</span>
<span class="quote">&gt; driver as miscellaneous device and in test_mmap(file_operations.mmap),</span>
<span class="quote">&gt; I changed a_ops with my address_space_operations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; int test_mmap(struct file *filp, struct vm_area_struct *vma)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         filp-&gt;f_mapping-&gt;a_ops = &amp;test_aops;</span>
<span class="quote">&gt;         vma-&gt;vm_ops = &amp;test_vm_ops;</span>
<span class="quote">&gt;         vma-&gt;vm_private_data = filp-&gt;private_data;</span>
<span class="quote">&gt;         return 0;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>

Okay.
<span class="quote">
&gt; test_aops should have *set_page_dirty* overriding.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static int test_set_pag_dirty(struct page *page)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         if (!PageDirty(page))</span>
<span class="quote">&gt;                 SetPageDirty*page);</span>
<span class="quote">&gt;         return 0;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Otherwise, it goes BUG_ON during radix tree operation because</span>
<span class="quote">&gt; currently try_to_unmap is designed for file-lru pages which lives</span>
<span class="quote">&gt; in page cache so it propagates page table dirty bit to PG_dirty flag</span>
<span class="quote">&gt; of struct page by set_page_dirty. And set_page_dirty want to mark</span>
<span class="quote">&gt; dirty tag in radix tree node but it&#39;s character driver so the page</span>
<span class="quote">&gt; cache doesn&#39;t have it. That&#39;s why we encounter BUG_ON in radix tree</span>
<span class="quote">&gt; operation. Anyway, to test, I implemented set_page_dirty in my dummy</span>
<span class="quote">&gt; driver.</span>

Okay and the above test_set_page_dirty() example is sufficient ?
<span class="quote">
&gt; </span>
<span class="quote">&gt; With only that, it doesn&#39;t work because I need to modify migrate.c to</span>
<span class="quote">&gt; work non-lru-mapped-page and changing PG_isolated flag which is</span>
<span class="quote">&gt; override of PG_reclaim which is cleared in set_page_dirty.</span>

Got it, so what changes you did ? Implemented PG_isolated differently
not by overriding PG_reclaim or something else ? Yes set_page_dirty
indeed clears the PG_reclaim flag.
<span class="quote">
&gt; </span>
<span class="quote">&gt; With that, it seems to work. But I&#39;m not saying it&#39;s right model now</span>

So the mapped pages migration was successful ? Even after overloading
filp-&gt;f_mapping-&gt;a_ops = &amp;test_aops, we still have the RMAP information
intact with filp-&gt;f_mappinp pointed interval tree. But would really like
to see the code changes.
<span class="quote">
&gt; for device drivers. In runtime, replacing filp-&gt;f_mapping-&gt;a_ops with</span>
<span class="quote">&gt; custom a_ops of own driver seems to be hacky to me.</span>

Yeah I thought so.
<span class="quote">
&gt; So, I&#39;m considering now new pseudo fs &quot;movable_inode&quot; which will</span>
<span class="quote">&gt; support </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; struct file *movable_inode_getfile(const char *name,</span>
<span class="quote">&gt;                         const struct file_operations *fop,</span>
<span class="quote">&gt;                         const struct address_space_operations *a_ops)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         struct path path;</span>
<span class="quote">&gt;         struct qstr this;</span>
<span class="quote">&gt;         struct inode *inode;</span>
<span class="quote">&gt;         struct super_block *sb;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         this.name = name;</span>
<span class="quote">&gt;         this.len = strlen(name);</span>
<span class="quote">&gt;         this.hash = 0;</span>
<span class="quote">&gt;         sb = movable_mnt.mnt_sb;</span>
<span class="quote">&gt;         patch.denty = d_alloc_pseudo(movable_inode_mnt-&gt;mnt_sb, &amp;this);</span>
<span class="quote">&gt;         patch.mnt = mntget(movable_inode_mnt);</span>
<span class="quote">&gt;         </span>
<span class="quote">&gt;         inode = new_inode(sb);</span>
<span class="quote">&gt;         ..</span>
<span class="quote">&gt;         ..</span>
<span class="quote">&gt;         inode-&gt;i_mapping-&gt;a_ops = a_ops;</span>
<span class="quote">&gt;         d_instantiate(path.dentry, inode);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;         return alloc_file(&amp;path, FMODE_WRITE | FMODE_READ, f_op);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And in our driver, we can change vma-&gt;vm_file with new one.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; int test_mmap(struct file *filp, struct vm_area_structd *vma)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         struct file *newfile = movable_inode_getfile(&quot;[test&quot;],</span>
<span class="quote">&gt;                                 filep-&gt;f_op, &amp;test_aops);</span>
<span class="quote">&gt;         vma-&gt;vm_file = newfile;</span>
<span class="quote">&gt;         ..</span>
<span class="quote">&gt;         ..</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When I read mmap_region in mm/mmap.c, it&#39;s reasonable usecase</span>
<span class="quote">&gt; which dirver&#39;s mmap changes vma-&gt;vm_file with own file.</span>

I will look into these details.
<span class="quote">
&gt; Anyway, it needs many subtle changes in mm/vfs/driver side so</span>
<span class="quote">&gt; need to review from each maintainers related subsystem so I</span>
<span class="quote">&gt; want to not be hurry.</span>

Sure, makes sense. Mean while it will be really great if you could share
your code changes as described above, so that I can try them out.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 30, 2016, 6:18 a.m.</div>
<pre class="content">
On Thu, Jun 30, 2016 at 11:26:45AM +0530, Anshuman Khandual wrote:

&lt;snip&gt;
<span class="quote">
&gt; &gt;&gt; Did you get a chance to test the driver out ? I am still concerned about how to</span>
<span class="quote">&gt; &gt;&gt; handle the struct address_space override problem within the struct page.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hi Anshuman,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Slow but I am working on that. :) However, as I said, I want to do it</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I really appreciate. Was just curious about the problem and any potential</span>
<span class="quote">&gt; solution we can look into.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; after soft landing of current non-lru-no-mapped page migration to solve</span>
<span class="quote">&gt; &gt; current real field issues.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; yeah it makes sense.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; About the overriding problem of non-lru-mapped-page, I implemented dummy</span>
<span class="quote">&gt; &gt; driver as miscellaneous device and in test_mmap(file_operations.mmap),</span>
<span class="quote">&gt; &gt; I changed a_ops with my address_space_operations.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; int test_mmap(struct file *filp, struct vm_area_struct *vma)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt;         filp-&gt;f_mapping-&gt;a_ops = &amp;test_aops;</span>
<span class="quote">&gt; &gt;         vma-&gt;vm_ops = &amp;test_vm_ops;</span>
<span class="quote">&gt; &gt;         vma-&gt;vm_private_data = filp-&gt;private_data;</span>
<span class="quote">&gt; &gt;         return 0;</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Okay.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; test_aops should have *set_page_dirty* overriding.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; static int test_set_pag_dirty(struct page *page)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt;         if (!PageDirty(page))</span>
<span class="quote">&gt; &gt;                 SetPageDirty*page);</span>
<span class="quote">&gt; &gt;         return 0;</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Otherwise, it goes BUG_ON during radix tree operation because</span>
<span class="quote">&gt; &gt; currently try_to_unmap is designed for file-lru pages which lives</span>
<span class="quote">&gt; &gt; in page cache so it propagates page table dirty bit to PG_dirty flag</span>
<span class="quote">&gt; &gt; of struct page by set_page_dirty. And set_page_dirty want to mark</span>
<span class="quote">&gt; &gt; dirty tag in radix tree node but it&#39;s character driver so the page</span>
<span class="quote">&gt; &gt; cache doesn&#39;t have it. That&#39;s why we encounter BUG_ON in radix tree</span>
<span class="quote">&gt; &gt; operation. Anyway, to test, I implemented set_page_dirty in my dummy</span>
<span class="quote">&gt; &gt; driver.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Okay and the above test_set_page_dirty() example is sufficient ?</span>

I guess just return 0 is sufficeint without any dirting a page.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; With only that, it doesn&#39;t work because I need to modify migrate.c to</span>
<span class="quote">&gt; &gt; work non-lru-mapped-page and changing PG_isolated flag which is</span>
<span class="quote">&gt; &gt; override of PG_reclaim which is cleared in set_page_dirty.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Got it, so what changes you did ? Implemented PG_isolated differently</span>
<span class="quote">&gt; not by overriding PG_reclaim or something else ? Yes set_page_dirty</span>
<span class="quote">&gt; indeed clears the PG_reclaim flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; With that, it seems to work. But I&#39;m not saying it&#39;s right model now</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the mapped pages migration was successful ? Even after overloading</span>
<span class="quote">&gt; filp-&gt;f_mapping-&gt;a_ops = &amp;test_aops, we still have the RMAP information</span>
<span class="quote">&gt; intact with filp-&gt;f_mappinp pointed interval tree. But would really like</span>
<span class="quote">&gt; to see the code changes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; for device drivers. In runtime, replacing filp-&gt;f_mapping-&gt;a_ops with</span>
<span class="quote">&gt; &gt; custom a_ops of own driver seems to be hacky to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah I thought so.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; So, I&#39;m considering now new pseudo fs &quot;movable_inode&quot; which will</span>
<span class="quote">&gt; &gt; support </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; struct file *movable_inode_getfile(const char *name,</span>
<span class="quote">&gt; &gt;                         const struct file_operations *fop,</span>
<span class="quote">&gt; &gt;                         const struct address_space_operations *a_ops)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt;         struct path path;</span>
<span class="quote">&gt; &gt;         struct qstr this;</span>
<span class="quote">&gt; &gt;         struct inode *inode;</span>
<span class="quote">&gt; &gt;         struct super_block *sb;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;         this.name = name;</span>
<span class="quote">&gt; &gt;         this.len = strlen(name);</span>
<span class="quote">&gt; &gt;         this.hash = 0;</span>
<span class="quote">&gt; &gt;         sb = movable_mnt.mnt_sb;</span>
<span class="quote">&gt; &gt;         patch.denty = d_alloc_pseudo(movable_inode_mnt-&gt;mnt_sb, &amp;this);</span>
<span class="quote">&gt; &gt;         patch.mnt = mntget(movable_inode_mnt);</span>
<span class="quote">&gt; &gt;         </span>
<span class="quote">&gt; &gt;         inode = new_inode(sb);</span>
<span class="quote">&gt; &gt;         ..</span>
<span class="quote">&gt; &gt;         ..</span>
<span class="quote">&gt; &gt;         inode-&gt;i_mapping-&gt;a_ops = a_ops;</span>
<span class="quote">&gt; &gt;         d_instantiate(path.dentry, inode);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;         return alloc_file(&amp;path, FMODE_WRITE | FMODE_READ, f_op);</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And in our driver, we can change vma-&gt;vm_file with new one.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; int test_mmap(struct file *filp, struct vm_area_structd *vma)</span>
<span class="quote">&gt; &gt; {</span>
<span class="quote">&gt; &gt;         struct file *newfile = movable_inode_getfile(&quot;[test&quot;],</span>
<span class="quote">&gt; &gt;                                 filep-&gt;f_op, &amp;test_aops);</span>
<span class="quote">&gt; &gt;         vma-&gt;vm_file = newfile;</span>
<span class="quote">&gt; &gt;         ..</span>
<span class="quote">&gt; &gt;         ..</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When I read mmap_region in mm/mmap.c, it&#39;s reasonable usecase</span>
<span class="quote">&gt; &gt; which dirver&#39;s mmap changes vma-&gt;vm_file with own file.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I will look into these details.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Anyway, it needs many subtle changes in mm/vfs/driver side so</span>
<span class="quote">&gt; &gt; need to review from each maintainers related subsystem so I</span>
<span class="quote">&gt; &gt; want to not be hurry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, makes sense. Mean while it will be really great if you could share</span>
<span class="quote">&gt; your code changes as described above, so that I can try them out.</span>
<span class="quote">&gt; </span>

It&#39;s almost done for draft version and I&#39;m doing stress test now and
fortunately, doesn&#39;t see the problem until now.

I will send you when I&#39;m ready.

Thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/filesystems/Locking b/Documentation/filesystems/Locking</span>
<span class="p_header">index af7c030a0368..3991a976cf43 100644</span>
<span class="p_header">--- a/Documentation/filesystems/Locking</span>
<span class="p_header">+++ b/Documentation/filesystems/Locking</span>
<span class="p_chunk">@@ -195,7 +195,9 @@</span> <span class="p_context"> unlocks and drops the reference.</span>
 	int (*releasepage) (struct page *, int);
 	void (*freepage)(struct page *);
 	int (*direct_IO)(struct kiocb *, struct iov_iter *iter);
<span class="p_add">+	bool (*isolate_page) (struct page *, isolate_mode_t);</span>
 	int (*migratepage)(struct address_space *, struct page *, struct page *);
<span class="p_add">+	void (*putback_page) (struct page *);</span>
 	int (*launder_page)(struct page *);
 	int (*is_partially_uptodate)(struct page *, unsigned long, unsigned long);
 	int (*error_remove_page)(struct address_space *, struct page *);
<span class="p_chunk">@@ -219,7 +221,9 @@</span> <span class="p_context"> invalidatepage:		yes</span>
 releasepage:		yes
 freepage:		yes
 direct_IO:
<span class="p_add">+isolate_page:		yes</span>
 migratepage:		yes (both)
<span class="p_add">+putback_page:		yes</span>
 launder_page:		yes
 is_partially_uptodate:	yes
 error_remove_page:	yes
<span class="p_header">diff --git a/Documentation/filesystems/vfs.txt b/Documentation/filesystems/vfs.txt</span>
<span class="p_header">index 19366fef2652..9d4ae317fdcb 100644</span>
<span class="p_header">--- a/Documentation/filesystems/vfs.txt</span>
<span class="p_header">+++ b/Documentation/filesystems/vfs.txt</span>
<span class="p_chunk">@@ -591,9 +591,14 @@</span> <span class="p_context"> struct address_space_operations {</span>
 	int (*releasepage) (struct page *, int);
 	void (*freepage)(struct page *);
 	ssize_t (*direct_IO)(struct kiocb *, struct iov_iter *iter);
<span class="p_add">+	/* isolate a page for migration */</span>
<span class="p_add">+	bool (*isolate_page) (struct page *, isolate_mode_t);</span>
 	/* migrate the contents of a page to the specified target */
 	int (*migratepage) (struct page *, struct page *);
<span class="p_add">+	/* put migration-failed page back to right list */</span>
<span class="p_add">+	void (*putback_page) (struct page *);</span>
 	int (*launder_page) (struct page *);
<span class="p_add">+</span>
 	int (*is_partially_uptodate) (struct page *, unsigned long,
 					unsigned long);
 	void (*is_dirty_writeback) (struct page *, bool *, bool *);
<span class="p_chunk">@@ -739,6 +744,10 @@</span> <span class="p_context"> struct address_space_operations {</span>
         and transfer data directly between the storage and the
         application&#39;s address space.
 
<span class="p_add">+  isolate_page: Called by the VM when isolating a movable non-lru page.</span>
<span class="p_add">+	If page is successfully isolated, VM marks the page as PG_isolated</span>
<span class="p_add">+	via __SetPageIsolated.</span>
<span class="p_add">+</span>
   migrate_page:  This is used to compact the physical memory usage.
         If the VM wants to relocate a page (maybe off a memory card
         that is signalling imminent failure) it will pass a new page
<span class="p_chunk">@@ -746,6 +755,8 @@</span> <span class="p_context"> struct address_space_operations {</span>
 	transfer any private data across and update any references
         that it has to the page.
 
<span class="p_add">+  putback_page: Called by the VM when isolated page&#39;s migration fails.</span>
<span class="p_add">+</span>
   launder_page: Called before freeing a page - it writes back the dirty page. To
   	prevent redirtying the page, it is kept locked during the whole
 	operation.
<span class="p_header">diff --git a/Documentation/vm/page_migration b/Documentation/vm/page_migration</span>
<span class="p_header">index fea5c0864170..18d37c7ac50b 100644</span>
<span class="p_header">--- a/Documentation/vm/page_migration</span>
<span class="p_header">+++ b/Documentation/vm/page_migration</span>
<span class="p_chunk">@@ -142,5 +142,110 @@</span> <span class="p_context"> is increased so that the page cannot be freed while page migration occurs.</span>
 20. The new page is moved to the LRU and can be scanned by the swapper
     etc again.
 
<span class="p_del">-Christoph Lameter, May 8, 2006.</span>
<span class="p_add">+C. Non-LRU page migration</span>
<span class="p_add">+-------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+Although original migration aimed for reducing the latency of memory access</span>
<span class="p_add">+for NUMA, compaction who want to create high-order page is also main customer.</span>
<span class="p_add">+</span>
<span class="p_add">+Current problem of the implementation is that it is designed to migrate only</span>
<span class="p_add">+*LRU* pages. However, there are potential non-lru pages which can be migrated</span>
<span class="p_add">+in drivers, for example, zsmalloc, virtio-balloon pages.</span>
<span class="p_add">+</span>
<span class="p_add">+For virtio-balloon pages, some parts of migration code path have been hooked</span>
<span class="p_add">+up and added virtio-balloon specific functions to intercept migration logics.</span>
<span class="p_add">+It&#39;s too specific to a driver so other drivers who want to make their pages</span>
<span class="p_add">+movable would have to add own specific hooks in migration path.</span>
<span class="p_add">+</span>
<span class="p_add">+To overclome the problem, VM supports non-LRU page migration which provides</span>
<span class="p_add">+generic functions for non-LRU movable pages without driver specific hooks</span>
<span class="p_add">+migration path.</span>
<span class="p_add">+</span>
<span class="p_add">+If a driver want to make own pages movable, it should define three functions</span>
<span class="p_add">+which are function pointers of struct address_space_operations.</span>
<span class="p_add">+</span>
<span class="p_add">+1. bool (*isolate_page) (struct page *page, isolate_mode_t mode);</span>
<span class="p_add">+</span>
<span class="p_add">+What VM expects on isolate_page function of driver is to return *true*</span>
<span class="p_add">+if driver isolates page successfully. On returing true, VM marks the page</span>
<span class="p_add">+as PG_isolated so concurrent isolation in several CPUs skip the page</span>
<span class="p_add">+for isolation. If a driver cannot isolate the page, it should return *false*.</span>
<span class="p_add">+</span>
<span class="p_add">+Once page is successfully isolated, VM uses page.lru fields so driver</span>
<span class="p_add">+shouldn&#39;t expect to preserve values in that fields.</span>
<span class="p_add">+</span>
<span class="p_add">+2. int (*migratepage) (struct address_space *mapping,</span>
<span class="p_add">+		struct page *newpage, struct page *oldpage, enum migrate_mode);</span>
<span class="p_add">+</span>
<span class="p_add">+After isolation, VM calls migratepage of driver with isolated page.</span>
<span class="p_add">+The function of migratepage is to move content of the old page to new page</span>
<span class="p_add">+and set up fields of struct page newpage. Keep in mind that you should</span>
<span class="p_add">+indicate to the VM the oldpage is no longer movable via __ClearPageMovable()</span>
<span class="p_add">+under page_lock if you migrated the oldpage successfully and returns 0.</span>
<span class="p_add">+If driver cannot migrate the page at the moment, driver can return -EAGAIN.</span>
<span class="p_add">+On -EAGAIN, VM will retry page migration in a short time because VM interprets</span>
<span class="p_add">+-EAGAIN as &quot;temporal migration failure&quot;. On returning any error except -EAGAIN,</span>
<span class="p_add">+VM will give up the page migration without retrying in this time.</span>
<span class="p_add">+</span>
<span class="p_add">+Driver shouldn&#39;t touch page.lru field VM using in the functions.</span>
<span class="p_add">+</span>
<span class="p_add">+3. void (*putback_page)(struct page *);</span>
<span class="p_add">+</span>
<span class="p_add">+If migration fails on isolated page, VM should return the isolated page</span>
<span class="p_add">+to the driver so VM calls driver&#39;s putback_page with migration failed page.</span>
<span class="p_add">+In this function, driver should put the isolated page back to the own data</span>
<span class="p_add">+structure.</span>
 
<span class="p_add">+4. non-lru movable page flags</span>
<span class="p_add">+</span>
<span class="p_add">+There are two page flags for supporting non-lru movable page.</span>
<span class="p_add">+</span>
<span class="p_add">+* PG_movable</span>
<span class="p_add">+</span>
<span class="p_add">+Driver should use the below function to make page movable under page_lock.</span>
<span class="p_add">+</span>
<span class="p_add">+	void __SetPageMovable(struct page *page, struct address_space *mapping)</span>
<span class="p_add">+</span>
<span class="p_add">+It needs argument of address_space for registering migration family functions</span>
<span class="p_add">+which will be called by VM. Exactly speaking, PG_movable is not a real flag of</span>
<span class="p_add">+struct page. Rather than, VM reuses page-&gt;mapping&#39;s lower bits to represent it.</span>
<span class="p_add">+</span>
<span class="p_add">+	#define PAGE_MAPPING_MOVABLE 0x2</span>
<span class="p_add">+	page-&gt;mapping = page-&gt;mapping | PAGE_MAPPING_MOVABLE;</span>
<span class="p_add">+</span>
<span class="p_add">+so driver shouldn&#39;t access page-&gt;mapping directly. Instead, driver should</span>
<span class="p_add">+use page_mapping which mask off the low two bits of page-&gt;mapping under</span>
<span class="p_add">+page lock so it can get right struct address_space.</span>
<span class="p_add">+</span>
<span class="p_add">+For testing of non-lru movable page, VM supports __PageMovable function.</span>
<span class="p_add">+However, it doesn&#39;t guarantee to identify non-lru movable page because</span>
<span class="p_add">+page-&gt;mapping field is unified with other variables in struct page.</span>
<span class="p_add">+As well, if driver releases the page after isolation by VM, page-&gt;mapping</span>
<span class="p_add">+doesn&#39;t have stable value although it has PAGE_MAPPING_MOVABLE</span>
<span class="p_add">+(Look at __ClearPageMovable). But __PageMovable is cheap to catch whether</span>
<span class="p_add">+page is LRU or non-lru movable once the page has been isolated. Because</span>
<span class="p_add">+LRU pages never can have PAGE_MAPPING_MOVABLE in page-&gt;mapping. It is also</span>
<span class="p_add">+good for just peeking to test non-lru movable pages before more expensive</span>
<span class="p_add">+checking with lock_page in pfn scanning to select victim.</span>
<span class="p_add">+</span>
<span class="p_add">+For guaranteeing non-lru movable page, VM provides PageMovable function.</span>
<span class="p_add">+Unlike __PageMovable, PageMovable functions validates page-&gt;mapping and</span>
<span class="p_add">+mapping-&gt;a_ops-&gt;isolate_page under lock_page. The lock_page prevents sudden</span>
<span class="p_add">+destroying of page-&gt;mapping.</span>
<span class="p_add">+</span>
<span class="p_add">+Driver using __SetPageMovable should clear the flag via __ClearMovablePage</span>
<span class="p_add">+under page_lock before the releasing the page.</span>
<span class="p_add">+</span>
<span class="p_add">+* PG_isolated</span>
<span class="p_add">+</span>
<span class="p_add">+To prevent concurrent isolation among several CPUs, VM marks isolated page</span>
<span class="p_add">+as PG_isolated under lock_page. So if a CPU encounters PG_isolated non-lru</span>
<span class="p_add">+movable page, it can skip it. Driver doesn&#39;t need to manipulate the flag</span>
<span class="p_add">+because VM will set/clear it automatically. Keep in mind that if driver</span>
<span class="p_add">+sees PG_isolated page, it means the page have been isolated by VM so it</span>
<span class="p_add">+shouldn&#39;t touch page.lru field.</span>
<span class="p_add">+PG_isolated is alias with PG_reclaim flag so driver shouldn&#39;t use the flag</span>
<span class="p_add">+for own purpose.</span>
<span class="p_add">+</span>
<span class="p_add">+Christoph Lameter, May 8, 2006.</span>
<span class="p_add">+Minchan Kim, Mar 28, 2016.</span>
<span class="p_header">diff --git a/include/linux/compaction.h b/include/linux/compaction.h</span>
<span class="p_header">index a58c852a268f..c6b47c861cea 100644</span>
<span class="p_header">--- a/include/linux/compaction.h</span>
<span class="p_header">+++ b/include/linux/compaction.h</span>
<span class="p_chunk">@@ -54,6 +54,9 @@</span> <span class="p_context"> enum compact_result {</span>
 struct alloc_context; /* in mm/internal.h */
 
 #ifdef CONFIG_COMPACTION
<span class="p_add">+extern int PageMovable(struct page *page);</span>
<span class="p_add">+extern void __SetPageMovable(struct page *page, struct address_space *mapping);</span>
<span class="p_add">+extern void __ClearPageMovable(struct page *page);</span>
 extern int sysctl_compact_memory;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos);
<span class="p_chunk">@@ -151,6 +154,19 @@</span> <span class="p_context"> extern void kcompactd_stop(int nid);</span>
 extern void wakeup_kcompactd(pg_data_t *pgdat, int order, int classzone_idx);
 
 #else
<span class="p_add">+static inline int PageMovable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void __SetPageMovable(struct page *page,</span>
<span class="p_add">+			struct address_space *mapping)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __ClearPageMovable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline enum compact_result try_to_compact_pages(gfp_t gfp_mask,
 			unsigned int order, int alloc_flags,
 			const struct alloc_context *ac,
<span class="p_chunk">@@ -212,6 +228,7 @@</span> <span class="p_context"> static inline void wakeup_kcompactd(pg_data_t *pgdat, int order, int classzone_i</span>
 #endif /* CONFIG_COMPACTION */
 
 #if defined(CONFIG_COMPACTION) &amp;&amp; defined(CONFIG_SYSFS) &amp;&amp; defined(CONFIG_NUMA)
<span class="p_add">+struct node;</span>
 extern int compaction_register_node(struct node *node);
 extern void compaction_unregister_node(struct node *node);
 
<span class="p_header">diff --git a/include/linux/fs.h b/include/linux/fs.h</span>
<span class="p_header">index 0cfdf2aec8f7..39ef97414033 100644</span>
<span class="p_header">--- a/include/linux/fs.h</span>
<span class="p_header">+++ b/include/linux/fs.h</span>
<span class="p_chunk">@@ -402,6 +402,8 @@</span> <span class="p_context"> struct address_space_operations {</span>
 	 */
 	int (*migratepage) (struct address_space *,
 			struct page *, struct page *, enum migrate_mode);
<span class="p_add">+	bool (*isolate_page)(struct page *, isolate_mode_t);</span>
<span class="p_add">+	void (*putback_page)(struct page *);</span>
 	int (*launder_page) (struct page *);
 	int (*is_partially_uptodate) (struct page *, unsigned long,
 					unsigned long);
<span class="p_header">diff --git a/include/linux/ksm.h b/include/linux/ksm.h</span>
<span class="p_header">index 7ae216a39c9e..481c8c4627ca 100644</span>
<span class="p_header">--- a/include/linux/ksm.h</span>
<span class="p_header">+++ b/include/linux/ksm.h</span>
<span class="p_chunk">@@ -43,8 +43,7 @@</span> <span class="p_context"> static inline struct stable_node *page_stable_node(struct page *page)</span>
 static inline void set_page_stable_node(struct page *page,
 					struct stable_node *stable_node)
 {
<span class="p_del">-	page-&gt;mapping = (void *)stable_node +</span>
<span class="p_del">-				(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);</span>
<span class="p_add">+	page-&gt;mapping = (void *)((unsigned long)stable_node | PAGE_MAPPING_KSM);</span>
 }
 
 /*
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index 9b50325e4ddf..404fbfefeb33 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -37,6 +37,8 @@</span> <span class="p_context"> extern int migrate_page(struct address_space *,</span>
 			struct page *, struct page *, enum migrate_mode);
 extern int migrate_pages(struct list_head *l, new_page_t new, free_page_t free,
 		unsigned long private, enum migrate_mode mode, int reason);
<span class="p_add">+extern bool isolate_movable_page(struct page *page, isolate_mode_t mode);</span>
<span class="p_add">+extern void putback_movable_page(struct page *page);</span>
 
 extern int migrate_prep(void);
 extern int migrate_prep_local(void);
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index a00ec816233a..33eaec57e997 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1035,6 +1035,7 @@</span> <span class="p_context"> static inline pgoff_t page_file_index(struct page *page)</span>
 }
 
 bool page_mapped(struct page *page);
<span class="p_add">+struct address_space *page_mapping(struct page *page);</span>
 
 /*
  * Return true only if the page has been allocated with
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index e5a32445f930..f36dbb3a3060 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -129,6 +129,9 @@</span> <span class="p_context"> enum pageflags {</span>
 
 	/* Compound pages. Stored in first tail page&#39;s flags */
 	PG_double_map = PG_private_2,
<span class="p_add">+</span>
<span class="p_add">+	/* non-lru isolated movable page */</span>
<span class="p_add">+	PG_isolated = PG_reclaim,</span>
 };
 
 #ifndef __GENERATING_BOUNDS_H
<span class="p_chunk">@@ -357,29 +360,37 @@</span> <span class="p_context"> PAGEFLAG(Idle, idle, PF_ANY)</span>
  * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.
  *
  * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,
<span class="p_del">- * the PAGE_MAPPING_KSM bit may be set along with the PAGE_MAPPING_ANON bit;</span>
<span class="p_del">- * and then page-&gt;mapping points, not to an anon_vma, but to a private</span>
<span class="p_add">+ * the PAGE_MAPPING_MOVABLE bit may be set along with the PAGE_MAPPING_ANON</span>
<span class="p_add">+ * bit; and then page-&gt;mapping points, not to an anon_vma, but to a private</span>
  * structure which KSM associates with that merged page.  See ksm.h.
  *
<span class="p_del">- * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is currently never used.</span>
<span class="p_add">+ * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is used for non-lru movable</span>
<span class="p_add">+ * page and then page-&gt;mapping points a struct address_space.</span>
  *
  * Please note that, confusingly, &quot;page_mapping&quot; refers to the inode
  * address_space which maps the page from disk; whereas &quot;page_mapped&quot;
  * refers to user virtual address space into which the page is mapped.
  */
<span class="p_del">-#define PAGE_MAPPING_ANON	1</span>
<span class="p_del">-#define PAGE_MAPPING_KSM	2</span>
<span class="p_del">-#define PAGE_MAPPING_FLAGS	(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM)</span>
<span class="p_add">+#define PAGE_MAPPING_ANON	0x1</span>
<span class="p_add">+#define PAGE_MAPPING_MOVABLE	0x2</span>
<span class="p_add">+#define PAGE_MAPPING_KSM	(PAGE_MAPPING_ANON | PAGE_MAPPING_MOVABLE)</span>
<span class="p_add">+#define PAGE_MAPPING_FLAGS	(PAGE_MAPPING_ANON | PAGE_MAPPING_MOVABLE)</span>
 
<span class="p_del">-static __always_inline int PageAnonHead(struct page *page)</span>
<span class="p_add">+static __always_inline int PageMappingFlags(struct page *page)</span>
 {
<span class="p_del">-	return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_ANON) != 0;</span>
<span class="p_add">+	return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_FLAGS) != 0;</span>
 }
 
 static __always_inline int PageAnon(struct page *page)
 {
 	page = compound_head(page);
<span class="p_del">-	return PageAnonHead(page);</span>
<span class="p_add">+	return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_ANON) != 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline int __PageMovable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_FLAGS) ==</span>
<span class="p_add">+				PAGE_MAPPING_MOVABLE;</span>
 }
 
 #ifdef CONFIG_KSM
<span class="p_chunk">@@ -393,7 +404,7 @@</span> <span class="p_context"> static __always_inline int PageKsm(struct page *page)</span>
 {
 	page = compound_head(page);
 	return ((unsigned long)page-&gt;mapping &amp; PAGE_MAPPING_FLAGS) ==
<span class="p_del">-				(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);</span>
<span class="p_add">+				PAGE_MAPPING_KSM;</span>
 }
 #else
 TESTPAGEFLAG_FALSE(Ksm)
<span class="p_chunk">@@ -641,6 +652,8 @@</span> <span class="p_context"> static inline void __ClearPageBalloon(struct page *page)</span>
 	atomic_set(&amp;page-&gt;_mapcount, -1);
 }
 
<span class="p_add">+__PAGEFLAG(Isolated, isolated, PF_ANY);</span>
<span class="p_add">+</span>
 /*
  * If network-based swap is enabled, sl*b must keep track of whether pages
  * were allocated from pfmemalloc reserves.
<span class="p_header">diff --git a/mm/compaction.c b/mm/compaction.c</span>
<span class="p_header">index 1427366ad673..a680b52e190b 100644</span>
<span class="p_header">--- a/mm/compaction.c</span>
<span class="p_header">+++ b/mm/compaction.c</span>
<span class="p_chunk">@@ -81,6 +81,44 @@</span> <span class="p_context"> static inline bool migrate_async_suitable(int migratetype)</span>
 
 #ifdef CONFIG_COMPACTION
 
<span class="p_add">+int PageMovable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	if (!__PageMovable(page))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	mapping = page_mapping(page);</span>
<span class="p_add">+	if (mapping &amp;&amp; mapping-&gt;a_ops &amp;&amp; mapping-&gt;a_ops-&gt;isolate_page)</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(PageMovable);</span>
<span class="p_add">+</span>
<span class="p_add">+void __SetPageMovable(struct page *page, struct address_space *mapping)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE((unsigned long)mapping &amp; PAGE_MAPPING_MOVABLE, page);</span>
<span class="p_add">+	page-&gt;mapping = (void *)((unsigned long)mapping | PAGE_MAPPING_MOVABLE);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__SetPageMovable);</span>
<span class="p_add">+</span>
<span class="p_add">+void __ClearPageMovable(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageMovable(page), page);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Clear registered address_space val with keeping PAGE_MAPPING_MOVABLE</span>
<span class="p_add">+	 * flag so that VM can catch up released page by driver after isolation.</span>
<span class="p_add">+	 * With it, VM migration doesn&#39;t try to put it back.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	page-&gt;mapping = (void *)((unsigned long)page-&gt;mapping &amp;</span>
<span class="p_add">+				PAGE_MAPPING_MOVABLE);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(__ClearPageMovable);</span>
<span class="p_add">+</span>
 /* Do not skip compaction more than 64 times */
 #define COMPACT_MAX_DEFER_SHIFT 6
 
<span class="p_chunk">@@ -735,21 +773,6 @@</span> <span class="p_context"> isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,</span>
 		}
 
 		/*
<span class="p_del">-		 * Check may be lockless but that&#39;s ok as we recheck later.</span>
<span class="p_del">-		 * It&#39;s possible to migrate LRU pages and balloon pages</span>
<span class="p_del">-		 * Skip any other type of page</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		is_lru = PageLRU(page);</span>
<span class="p_del">-		if (!is_lru) {</span>
<span class="p_del">-			if (unlikely(balloon_page_movable(page))) {</span>
<span class="p_del">-				if (balloon_page_isolate(page)) {</span>
<span class="p_del">-					/* Successfully isolated */</span>
<span class="p_del">-					goto isolate_success;</span>
<span class="p_del">-				}</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
 		 * Regardless of being on LRU, compound pages such as THP and
 		 * hugetlbfs are not to be compacted. We can potentially save
 		 * a lot of iterations if we skip them at once. The check is
<span class="p_chunk">@@ -765,8 +788,38 @@</span> <span class="p_context"> isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,</span>
 			goto isolate_fail;
 		}
 
<span class="p_del">-		if (!is_lru)</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Check may be lockless but that&#39;s ok as we recheck later.</span>
<span class="p_add">+		 * It&#39;s possible to migrate LRU and non-lru movable pages.</span>
<span class="p_add">+		 * Skip any other type of page</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		is_lru = PageLRU(page);</span>
<span class="p_add">+		if (!is_lru) {</span>
<span class="p_add">+			if (unlikely(balloon_page_movable(page))) {</span>
<span class="p_add">+				if (balloon_page_isolate(page)) {</span>
<span class="p_add">+					/* Successfully isolated */</span>
<span class="p_add">+					goto isolate_success;</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * __PageMovable can return false positive so we need</span>
<span class="p_add">+			 * to verify it under page_lock.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (unlikely(__PageMovable(page)) &amp;&amp;</span>
<span class="p_add">+					!PageIsolated(page)) {</span>
<span class="p_add">+				if (locked) {</span>
<span class="p_add">+					spin_unlock_irqrestore(&amp;zone-&gt;lru_lock,</span>
<span class="p_add">+									flags);</span>
<span class="p_add">+					locked = false;</span>
<span class="p_add">+				}</span>
<span class="p_add">+</span>
<span class="p_add">+				if (isolate_movable_page(page, isolate_mode))</span>
<span class="p_add">+					goto isolate_success;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			goto isolate_fail;
<span class="p_add">+		}</span>
 
 		/*
 		 * Migration will fail if an anonymous page is pinned in memory,
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 4786b4150f62..35b8aef867a9 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -532,8 +532,8 @@</span> <span class="p_context"> static struct page *get_ksm_page(struct stable_node *stable_node, bool lock_it)</span>
 	void *expected_mapping;
 	unsigned long kpfn;
 
<span class="p_del">-	expected_mapping = (void *)stable_node +</span>
<span class="p_del">-				(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);</span>
<span class="p_add">+	expected_mapping = (void *)((unsigned long)stable_node |</span>
<span class="p_add">+					PAGE_MAPPING_KSM);</span>
 again:
 	kpfn = READ_ONCE(stable_node-&gt;kpfn);
 	page = pfn_to_page(kpfn);
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 2666f28b5236..60abcf379b51 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -31,6 +31,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/security.h&gt;
 #include &lt;linux/backing-dev.h&gt;
<span class="p_add">+#include &lt;linux/compaction.h&gt;</span>
 #include &lt;linux/syscalls.h&gt;
 #include &lt;linux/hugetlb.h&gt;
 #include &lt;linux/hugetlb_cgroup.h&gt;
<span class="p_chunk">@@ -73,6 +74,81 @@</span> <span class="p_context"> int migrate_prep_local(void)</span>
 	return 0;
 }
 
<span class="p_add">+bool isolate_movable_page(struct page *page, isolate_mode_t mode)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Avoid burning cycles with pages that are yet under __free_pages(),</span>
<span class="p_add">+	 * or just got freed under us.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * In case we &#39;win&#39; a race for a movable page being freed under us and</span>
<span class="p_add">+	 * raise its refcount preventing __free_pages() from doing its job</span>
<span class="p_add">+	 * the put_page() at the end of this block will take care of</span>
<span class="p_add">+	 * release this page, thus avoiding a nasty leakage.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(!get_page_unless_zero(page)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check PageMovable before holding a PG_lock because page&#39;s owner</span>
<span class="p_add">+	 * assumes anybody doesn&#39;t touch PG_lock of newly allocated page</span>
<span class="p_add">+	 * so unconditionally grapping the lock ruins page&#39;s owner side.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(!__PageMovable(page)))</span>
<span class="p_add">+		goto out_putpage;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * As movable pages are not isolated from LRU lists, concurrent</span>
<span class="p_add">+	 * compaction threads can race against page migration functions</span>
<span class="p_add">+	 * as well as race against the releasing a page.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * In order to avoid having an already isolated movable page</span>
<span class="p_add">+	 * being (wrongly) re-isolated while it is under migration,</span>
<span class="p_add">+	 * or to avoid attempting to isolate pages being released,</span>
<span class="p_add">+	 * lets be sure we have the page lock</span>
<span class="p_add">+	 * before proceeding with the movable page isolation steps.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(!trylock_page(page)))</span>
<span class="p_add">+		goto out_putpage;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!PageMovable(page) || PageIsolated(page))</span>
<span class="p_add">+		goto out_no_isolated;</span>
<span class="p_add">+</span>
<span class="p_add">+	mapping = page_mapping(page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!mapping, page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mapping-&gt;a_ops-&gt;isolate_page(page, mode))</span>
<span class="p_add">+		goto out_no_isolated;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Driver shouldn&#39;t use PG_isolated bit of page-&gt;flags */</span>
<span class="p_add">+	WARN_ON_ONCE(PageIsolated(page));</span>
<span class="p_add">+	__SetPageIsolated(page);</span>
<span class="p_add">+	unlock_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+</span>
<span class="p_add">+out_no_isolated:</span>
<span class="p_add">+	unlock_page(page);</span>
<span class="p_add">+out_putpage:</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* It should be called on page which is PG_movable */</span>
<span class="p_add">+void putback_movable_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageMovable(page), page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageIsolated(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping-&gt;a_ops-&gt;putback_page(page);</span>
<span class="p_add">+	__ClearPageIsolated(page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Put previously isolated pages back onto the appropriate lists
  * from where they were once taken off for compaction/migration.
<span class="p_chunk">@@ -94,10 +170,25 @@</span> <span class="p_context"> void putback_movable_pages(struct list_head *l)</span>
 		list_del(&amp;page-&gt;lru);
 		dec_zone_page_state(page, NR_ISOLATED_ANON +
 				page_is_file_cache(page));
<span class="p_del">-		if (unlikely(isolated_balloon_page(page)))</span>
<span class="p_add">+		if (unlikely(isolated_balloon_page(page))) {</span>
 			balloon_page_putback(page);
<span class="p_del">-		else</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We isolated non-lru movable page so here we can use</span>
<span class="p_add">+		 * __PageMovable because LRU page&#39;s mapping cannot have</span>
<span class="p_add">+		 * PAGE_MAPPING_MOVABLE.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		} else if (unlikely(__PageMovable(page))) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageIsolated(page), page);</span>
<span class="p_add">+			lock_page(page);</span>
<span class="p_add">+			if (PageMovable(page))</span>
<span class="p_add">+				putback_movable_page(page);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				__ClearPageIsolated(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		} else {</span>
 			putback_lru_page(page);
<span class="p_add">+		}</span>
 	}
 }
 
<span class="p_chunk">@@ -592,7 +683,7 @@</span> <span class="p_context"> void migrate_page_copy(struct page *newpage, struct page *page)</span>
  ***********************************************************/
 
 /*
<span class="p_del">- * Common logic to directly migrate a single page suitable for</span>
<span class="p_add">+ * Common logic to directly migrate a single LRU page suitable for</span>
  * pages that do not use PagePrivate/PagePrivate2.
  *
  * Pages are locked upon entry and exit.
<span class="p_chunk">@@ -755,33 +846,72 @@</span> <span class="p_context"> static int move_to_new_page(struct page *newpage, struct page *page,</span>
 				enum migrate_mode mode)
 {
 	struct address_space *mapping;
<span class="p_del">-	int rc;</span>
<span class="p_add">+	int rc = -EAGAIN;</span>
<span class="p_add">+	bool is_lru = !__PageMovable(page);</span>
 
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE(!PageLocked(newpage), newpage);
 
 	mapping = page_mapping(page);
<span class="p_del">-	if (!mapping)</span>
<span class="p_del">-		rc = migrate_page(mapping, newpage, page, mode);</span>
<span class="p_del">-	else if (mapping-&gt;a_ops-&gt;migratepage)</span>
<span class="p_add">+</span>
<span class="p_add">+	if (likely(is_lru)) {</span>
<span class="p_add">+		if (!mapping)</span>
<span class="p_add">+			rc = migrate_page(mapping, newpage, page, mode);</span>
<span class="p_add">+		else if (mapping-&gt;a_ops-&gt;migratepage)</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Most pages have a mapping and most filesystems</span>
<span class="p_add">+			 * provide a migratepage callback. Anonymous pages</span>
<span class="p_add">+			 * are part of swap space which also has its own</span>
<span class="p_add">+			 * migratepage callback. This is the most common path</span>
<span class="p_add">+			 * for page migration.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			rc = mapping-&gt;a_ops-&gt;migratepage(mapping, newpage,</span>
<span class="p_add">+							page, mode);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			rc = fallback_migrate_page(mapping, newpage,</span>
<span class="p_add">+							page, mode);</span>
<span class="p_add">+	} else {</span>
 		/*
<span class="p_del">-		 * Most pages have a mapping and most filesystems provide a</span>
<span class="p_del">-		 * migratepage callback. Anonymous pages are part of swap</span>
<span class="p_del">-		 * space which also has its own migratepage callback. This</span>
<span class="p_del">-		 * is the most common path for page migration.</span>
<span class="p_add">+		 * In case of non-lru page, it could be released after</span>
<span class="p_add">+		 * isolation step. In that case, we shouldn&#39;t try migration.</span>
 		 */
<span class="p_del">-		rc = mapping-&gt;a_ops-&gt;migratepage(mapping, newpage, page, mode);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		rc = fallback_migrate_page(mapping, newpage, page, mode);</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!PageIsolated(page), page);</span>
<span class="p_add">+		if (!PageMovable(page)) {</span>
<span class="p_add">+			rc = MIGRATEPAGE_SUCCESS;</span>
<span class="p_add">+			__ClearPageIsolated(page);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		rc = mapping-&gt;a_ops-&gt;migratepage(mapping, newpage,</span>
<span class="p_add">+						page, mode);</span>
<span class="p_add">+		WARN_ON_ONCE(rc == MIGRATEPAGE_SUCCESS &amp;&amp;</span>
<span class="p_add">+			!PageIsolated(page));</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * When successful, old pagecache page-&gt;mapping must be cleared before
 	 * page is freed; but stats require that PageAnon be left as PageAnon.
 	 */
 	if (rc == MIGRATEPAGE_SUCCESS) {
<span class="p_del">-		if (!PageAnon(page))</span>
<span class="p_add">+		if (__PageMovable(page)) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageIsolated(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We clear PG_movable under page_lock so any compactor</span>
<span class="p_add">+			 * cannot try to migrate this page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			__ClearPageIsolated(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Anonymous and movable page-&gt;mapping will be cleard by</span>
<span class="p_add">+		 * free_pages_prepare so don&#39;t reset it here for keeping</span>
<span class="p_add">+		 * the type to work PageAnon, for example.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!PageMappingFlags(page))</span>
 			page-&gt;mapping = NULL;
 	}
<span class="p_add">+out:</span>
 	return rc;
 }
 
<span class="p_chunk">@@ -791,6 +921,7 @@</span> <span class="p_context"> static int __unmap_and_move(struct page *page, struct page *newpage,</span>
 	int rc = -EAGAIN;
 	int page_was_mapped = 0;
 	struct anon_vma *anon_vma = NULL;
<span class="p_add">+	bool is_lru = !__PageMovable(page);</span>
 
 	if (!trylock_page(page)) {
 		if (!force || mode == MIGRATE_ASYNC)
<span class="p_chunk">@@ -871,6 +1002,11 @@</span> <span class="p_context"> static int __unmap_and_move(struct page *page, struct page *newpage,</span>
 		goto out_unlock_both;
 	}
 
<span class="p_add">+	if (unlikely(!is_lru)) {</span>
<span class="p_add">+		rc = move_to_new_page(newpage, page, mode);</span>
<span class="p_add">+		goto out_unlock_both;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Corner case handling:
 	 * 1. When a new swap-cache page is read into, it is added to the LRU
<span class="p_chunk">@@ -920,7 +1056,8 @@</span> <span class="p_context"> static int __unmap_and_move(struct page *page, struct page *newpage,</span>
 	 * list in here.
 	 */
 	if (rc == MIGRATEPAGE_SUCCESS) {
<span class="p_del">-		if (unlikely(__is_movable_balloon_page(newpage)))</span>
<span class="p_add">+		if (unlikely(__is_movable_balloon_page(newpage) ||</span>
<span class="p_add">+				__PageMovable(newpage)))</span>
 			put_page(newpage);
 		else
 			putback_lru_page(newpage);
<span class="p_chunk">@@ -961,6 +1098,12 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 		/* page was freed from under us. So we are done. */
 		ClearPageActive(page);
 		ClearPageUnevictable(page);
<span class="p_add">+		if (unlikely(__PageMovable(page))) {</span>
<span class="p_add">+			lock_page(page);</span>
<span class="p_add">+			if (!PageMovable(page))</span>
<span class="p_add">+				__ClearPageIsolated(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+		}</span>
 		if (put_new_page)
 			put_new_page(newpage, private);
 		else
<span class="p_chunk">@@ -1010,8 +1153,21 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 				num_poisoned_pages_inc();
 		}
 	} else {
<span class="p_del">-		if (rc != -EAGAIN)</span>
<span class="p_del">-			putback_lru_page(page);</span>
<span class="p_add">+		if (rc != -EAGAIN) {</span>
<span class="p_add">+			if (likely(!__PageMovable(page))) {</span>
<span class="p_add">+				putback_lru_page(page);</span>
<span class="p_add">+				goto put_new;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			lock_page(page);</span>
<span class="p_add">+			if (PageMovable(page))</span>
<span class="p_add">+				putback_movable_page(page);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				__ClearPageIsolated(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+put_new:</span>
 		if (put_new_page)
 			put_new_page(newpage, private);
 		else
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index d27e8b968ac3..4d0ba8f6cfee 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1014,7 +1014,7 @@</span> <span class="p_context"> static __always_inline bool free_pages_prepare(struct page *page,</span>
 			(page + i)-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP;
 		}
 	}
<span class="p_del">-	if (PageAnonHead(page))</span>
<span class="p_add">+	if (PageMappingFlags(page))</span>
 		page-&gt;mapping = NULL;
 	if (check_free)
 		bad += free_pages_check(page);
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index 917e0e3d0f8e..b756ee36f7f0 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -399,10 +399,12 @@</span> <span class="p_context"> struct address_space *page_mapping(struct page *page)</span>
 	}
 
 	mapping = page-&gt;mapping;
<span class="p_del">-	if ((unsigned long)mapping &amp; PAGE_MAPPING_FLAGS)</span>
<span class="p_add">+	if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)</span>
 		return NULL;
<span class="p_del">-	return mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);</span>
 }
<span class="p_add">+EXPORT_SYMBOL(page_mapping);</span>
 
 /* Slow path of page_mapcount() for compound pages */
 int __page_mapcount(struct page *page)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



